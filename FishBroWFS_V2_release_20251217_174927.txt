================================================================================
FishBroWFS_V2 Release Package
Generated: 2025-12-17 17:49:27
================================================================================

DIRECTORY STRUCTURE
--------------------------------------------------------------------------------
FishBroWFS_V2/
    â”œâ”€â”€ GM_Huang/
    â”‚   â”œâ”€â”€ clean_repo_caches.py
    â”‚   â””â”€â”€ release_tool.py
    â”œâ”€â”€ docs/
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ perf_direct.py
    â”‚   â””â”€â”€ perf_grid.py
    â”œâ”€â”€ src/
    â”‚   â””â”€â”€ FishBroWFS_V2/
    â”‚       â”œâ”€â”€ data/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â””â”€â”€ layout.py
    â”‚       â”œâ”€â”€ engine/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ constants.py
    â”‚       â”‚   â”œâ”€â”€ constitution.py
    â”‚       â”‚   â”œâ”€â”€ engine_jit.py
    â”‚       â”‚   â”œâ”€â”€ matcher_core.py
    â”‚       â”‚   â””â”€â”€ types.py
    â”‚       â”œâ”€â”€ indicators/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â””â”€â”€ numba_indicators.py
    â”‚       â”œâ”€â”€ perf/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â””â”€â”€ profile_report.py
    â”‚       â”œâ”€â”€ pipeline/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ metrics_schema.py
    â”‚       â”‚   â”œâ”€â”€ param_sort.py
    â”‚       â”‚   â””â”€â”€ runner_grid.py
    â”‚       â”œâ”€â”€ stage0/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ ma_proxy.py
    â”‚       â”‚   â””â”€â”€ proxies.py
    â”‚       â”œâ”€â”€ strategy/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ kernel.py
    â”‚       â”‚   â””â”€â”€ runner_single.py
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â””â”€â”€ version.py
    â””â”€â”€ tests/
        â”œâ”€â”€ conftest.py
        â”œâ”€â”€ test_baseline_lock.py
        â”œâ”€â”€ test_data_layout.py
        â”œâ”€â”€ test_engine_constitution.py
        â”œâ”€â”€ test_engine_gaps_and_priority.py
        â”œâ”€â”€ test_engine_jit_active_book_contract.py
        â”œâ”€â”€ test_golden_kernel_verification.py
        â”œâ”€â”€ test_grid_runner_smoke.py
        â”œâ”€â”€ test_indicators_consistency.py
        â”œâ”€â”€ test_perf_evidence_chain.py
        â”œâ”€â”€ test_perf_grid_profile_report.py
        â”œâ”€â”€ test_runner_grid_perf_observability.py
        â”œâ”€â”€ test_stage0_contract.py
        â”œâ”€â”€ test_stage0_ma_proxy.py
        â”œâ”€â”€ test_stage0_proxies.py
        â”œâ”€â”€ test_stage0_proxy_rank_corr.py
        â”œâ”€â”€ test_stage2_params_influence.py
        â””â”€â”€ test_vectorization_parity.py

================================================================================
PYTHON FILES AND CODE
================================================================================


================================================================================
FILE: GM_Huang/clean_repo_caches.py
================================================================================

#!/usr/bin/env python3
from __future__ import annotations

import os
from pathlib import Path


def _is_under(path: Path, parent: Path) -> bool:
    try:
        path.resolve().relative_to(parent.resolve())
        return True
    except Exception:
        return False


def clean_repo_caches(repo_root: Path, dry_run: bool = False) -> tuple[int, int]:
    """
    Remove Python bytecode caches inside repo_root:
      - __pycache__ directories
      - *.pyc, *.pyo
    Does NOT touch anything outside repo_root.
    """
    removed_dirs = 0
    removed_files = 0

    for p in repo_root.rglob("__pycache__"):
        if not p.is_dir():
            continue
        if not _is_under(p, repo_root):
            continue
        if dry_run:
            print(f"[DRY] rmdir: {p}")
        else:
            for child in p.rglob("*"):
                try:
                    if child.is_file() or child.is_symlink():
                        child.unlink(missing_ok=True)
                        removed_files += 1
                except Exception:
                    pass
            try:
                p.rmdir()
                removed_dirs += 1
            except Exception:
                pass

    for ext in ("*.pyc", "*.pyo"):
        for p in repo_root.rglob(ext):
            if not p.is_file() and not p.is_symlink():
                continue
            if not _is_under(p, repo_root):
                continue
            if dry_run:
                print(f"[DRY] rm: {p}")
            else:
                try:
                    p.unlink(missing_ok=True)
                    removed_files += 1
                except Exception:
                    pass

    return removed_dirs, removed_files


def main() -> None:
    repo_root = Path(__file__).resolve().parents[1]
    dry_run = os.environ.get("FISHBRO_DRY_RUN", "").strip() == "1"
    removed_dirs, removed_files = clean_repo_caches(repo_root, dry_run=dry_run)

    if dry_run:
        print("[DRY] Done.")
        return

    print(f"Cleaned {removed_dirs} __pycache__ directories and {removed_files} bytecode files.")


if __name__ == "__main__":
    main()


================================================================================
FILE: GM_Huang/release_tool.py
================================================================================

#!/usr/bin/env python3
"""
Release tool for FishBroWFS_V2.

Generates release packages (txt or zip) excluding sensitive information like .git
"""

from __future__ import annotations

import os
import zipfile
from datetime import datetime
from pathlib import Path


def should_exclude(path: Path, repo_root: Path) -> bool:
    """
    Check if a path should be excluded from release.
    
    Excludes:
    - .git directory and all its contents
    - __pycache__ directories
    - .pyc, .pyo files
    - Common build/test artifacts
    """
    path_str = str(path)
    path_parts = path.parts
    
    # Exclude .git directory
    if '.git' in path_parts:
        return True
    
    # Exclude cache directories
    if '__pycache__' in path_parts:
        return True
    
    # Exclude bytecode files
    if path.suffix in ('.pyc', '.pyo'):
        return True
    
    # Exclude common build/test artifacts
    exclude_names = {
        '.pytest_cache', '.mypy_cache', '.ruff_cache',
        '.coverage', 'htmlcov', '.tox', 'dist', 'build',
        '*.egg-info', '.eggs'
    }
    
    for name in exclude_names:
        if name in path_parts or path.name.startswith(name.replace('*', '')):
            return True
    
    # Exclude GM_Huang itself from the release (optional, but makes sense)
    # Actually, let's include it since it's part of the project structure
    
    return False


def get_python_files(repo_root: Path) -> list[Path]:
    """Get all Python files in the repository, excluding sensitive paths."""
    python_files = []
    
    for py_file in repo_root.rglob('*.py'):
        if not should_exclude(py_file, repo_root):
            python_files.append(py_file)
    
    return sorted(python_files)


def get_directory_structure(repo_root: Path) -> str:
    """Generate a text representation of directory structure."""
    lines = []
    
    def walk_tree(directory: Path, prefix: str = '', is_last: bool = True):
        """Recursively walk directory tree and build structure."""
        if should_exclude(directory, repo_root):
            return
        
        # Skip if it's the repo root itself
        if directory == repo_root:
            lines.append(f"{directory.name}/")
        else:
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            lines.append(f"{prefix}{connector}{directory.name}/")
        
        # Get subdirectories and files
        try:
            items = sorted([p for p in directory.iterdir() 
                          if not should_exclude(p, repo_root)])
            dirs = [p for p in items if p.is_dir()]
            files = [p for p in items if p.is_file() and p.suffix == '.py']
            
            # Process directories
            for i, item in enumerate(dirs):
                is_last_item = (i == len(dirs) - 1) and len(files) == 0
                extension = "    " if is_last else "â”‚   "
                walk_tree(item, prefix + extension, is_last_item)
            
            # Process Python files
            for i, file in enumerate(files):
                is_last_item = i == len(files) - 1
                connector = "â””â”€â”€ " if is_last_item else "â”œâ”€â”€ "
                lines.append(f"{prefix}{'    ' if is_last else 'â”‚   '}{connector}{file.name}")
        except PermissionError:
            pass
    
    walk_tree(repo_root)
    return "\n".join(lines)


def generate_release_txt(repo_root: Path, output_path: Path) -> None:
    """Generate a text file with directory structure and Python code."""
    print(f"Generating release TXT: {output_path}")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        # Header
        f.write("=" * 80 + "\n")
        f.write(f"FishBroWFS_V2 Release Package\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")
        
        # Directory structure
        f.write("DIRECTORY STRUCTURE\n")
        f.write("-" * 80 + "\n")
        f.write(get_directory_structure(repo_root))
        f.write("\n\n")
        
        # Python files and their content
        f.write("=" * 80 + "\n")
        f.write("PYTHON FILES AND CODE\n")
        f.write("=" * 80 + "\n\n")
        
        python_files = get_python_files(repo_root)
        
        for py_file in python_files:
            relative_path = py_file.relative_to(repo_root)
            f.write(f"\n{'=' * 80}\n")
            f.write(f"FILE: {relative_path}\n")
            f.write(f"{'=' * 80}\n\n")
            
            try:
                content = py_file.read_text(encoding='utf-8')
                f.write(content)
                if not content.endswith('\n'):
                    f.write('\n')
            except Exception as e:
                f.write(f"[ERROR: Could not read file: {e}]\n")
            
            f.write("\n")
    
    print(f"âœ“ Release TXT generated: {output_path}")


def generate_release_zip(repo_root: Path, output_path: Path) -> None:
    """Generate a zip file of the project, excluding sensitive information."""
    print(f"Generating release ZIP: {output_path}")
    
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        python_files = get_python_files(repo_root)
        
        # Also include non-Python files that are important
        important_extensions = {'.toml', '.txt', '.md', '.yml', '.yaml'}
        important_files = []
        
        for ext in important_extensions:
            for file in repo_root.rglob(f'*{ext}'):
                if not should_exclude(file, repo_root):
                    important_files.append(file)
        
        all_files = sorted(set(python_files + important_files))
        
        for file_path in all_files:
            relative_path = file_path.relative_to(repo_root)
            zipf.write(file_path, relative_path)
            print(f"  Added: {relative_path}")
    
    print(f"âœ“ Release ZIP generated: {output_path}")
    print(f"  Total files: {len(all_files)}")


def main() -> None:
    """Main entry point."""
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python release_tool.py [txt|zip]")
        sys.exit(1)
    
    mode = sys.argv[1].lower()
    
    # Get repo root (parent of GM_Huang)
    script_dir = Path(__file__).resolve().parent
    repo_root = script_dir.parent
    
    # Generate output filename with timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    project_name = repo_root.name
    
    if mode == 'txt':
        output_path = repo_root / f"{project_name}_release_{timestamp}.txt"
        generate_release_txt(repo_root, output_path)
    elif mode == 'zip':
        output_path = repo_root / f"{project_name}_release_{timestamp}.zip"
        generate_release_zip(repo_root, output_path)
    else:
        print(f"Unknown mode: {mode}. Use 'txt' or 'zip'")
        sys.exit(1)


if __name__ == "__main__":
    main()


================================================================================
FILE: scripts/perf_direct.py
================================================================================

#!/usr/bin/env python3
"""
FishBro WFS - Direct Engine Benchmark
ç”¨é€”: ç¹éæ‰€æœ‰ Harness/Subprocess è¤‡é›œåº¦ï¼Œç›´æ¥ import engine æ¸¬é€Ÿ
"""
import sys
import time
import gc
import numpy as np
from pathlib import Path

# 1. å¼·åˆ¶è¨­å®šè·¯å¾‘ (æŒ‡å‘ src)
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

print(f"python_path: {sys.path[0]}")

try:
    # Correct src-based package name in this repo:
    # src/FishBroWFS_V2/pipeline/runner_grid.py
    from FishBroWFS_V2.pipeline.runner_grid import run_grid  # type: ignore
    print("âœ… Engine imported successfully (FishBroWFS_V2.pipeline.runner_grid).")
except ImportError as e:
    print(f"âŒ FATAL: Cannot import engine: {e}")
    sys.exit(1)

# 2. è¨­å®šè¦æ¨¡ (å°è¦æ¨¡ Smoke Test)
BARS = 20_000
PARAMS = 5_000
HOT_RUNS = 5

def generate_data(n_bars, n_params):
    print(f"generating data: {n_bars} bars, {n_params} params...")
    rng = np.random.default_rng(42)
    
    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10
    # ä½¿ç”¨ np.abs é¿å… AttributeError
    high = close + np.abs(rng.standard_normal(n_bars)) * 5
    low = close - np.abs(rng.standard_normal(n_bars)) * 5
    open_ = (high + low) / 2 + rng.standard_normal(n_bars)
    
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    # Generate Params (runner_grid contract: params_matrix must be (n, >=3))
    w1 = rng.integers(10, 100, size=n_params)
    w2 = rng.integers(5, 50, size=n_params)
    w3 = rng.integers(2, 30, size=n_params)
    params = np.column_stack((w1, w2, w3))
    
    # Layout check
    data_arrays = [open_, high, low, close, params]
    final_arrays = []
    for arr in data_arrays:
        arr = arr.astype(np.float64)
        if not arr.flags['C_CONTIGUOUS']:
            arr = np.ascontiguousarray(arr)
        final_arrays.append(arr)
        
    return final_arrays[0], final_arrays[1], final_arrays[2], final_arrays[3], final_arrays[4]

def main():
    opens, highs, lows, closes, params = generate_data(BARS, PARAMS)
    
    print("-" * 40)
    print(f"Start Benchmark: {BARS} bars x {PARAMS} params")
    print("-" * 40)

    # COLD RUN
    print("ğŸ¥¶ Cold run (compiling)...", end="", flush=True)
    t0 = time.perf_counter()
    _ = run_grid(
        open_=opens,
        high=highs,
        low=lows,
        close=closes,
        params_matrix=params,
        commission=0.0,
        slip=0.0,
        sort_params=False,
    )
    print(f" Done in {time.perf_counter() - t0:.4f}s")

    # HOT RUNS
    times = []
    print(f"ğŸ”¥ Hot runs ({HOT_RUNS} times, GC off)...")
    gc.disable()
    for i in range(HOT_RUNS):
        t_start = time.perf_counter()
        _ = run_grid(
            open_=opens,
            high=highs,
            low=lows,
            close=closes,
            params_matrix=params,
            commission=0.0,
            slip=0.0,
            sort_params=False,
        )
        dt = time.perf_counter() - t_start
        times.append(dt)
        print(f"   Run {i+1}: {dt:.4f}s")
    gc.enable()
    
    min_time = min(times)
    total_ops = BARS * PARAMS
    tput = total_ops / min_time
    
    print("-" * 40)
    print(f"MIN TIME:   {min_time:.4f}s")
    print(f"THROUGHPUT: {int(tput):,} pair-bars/sec")
    print("-" * 40)

if __name__ == "__main__":
    main()


================================================================================
FILE: scripts/perf_grid.py
================================================================================

#!/usr/bin/env python3
"""
FishBro WFS Perf Harness (Red Team Spec v1.0)
ç‹€æ…‹: âœ… File-based IPC / JIT-First / Observable
ç”¨é€”: é‡æ¸¬ JIT Grid Runner çš„ç©©æ…‹ååé‡ (Steady-state Throughput)

ä¿®æ­£ç´€éŒ„:
- v1.1: ä¿®å¾© numpy generator abs éŒ¯èª¤
- v1.2: Hotfix: è§£æ±º subprocess Import Errorï¼Œå¼·åˆ¶æ³¨å…¥ PYTHONPATH ä¸¦å¢å¼· debug info
"""
import os
import sys
import time
import gc
import json
import cProfile
import argparse
import subprocess
import tempfile
import statistics
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional

import numpy as np

from FishBroWFS_V2.perf.profile_report import _format_profile_report

# ==========================================
# 1. é…ç½®èˆ‡å¸¸æ•¸ (Tiers)
# ==========================================

@dataclass
class PerfConfig:
    name: str
    n_bars: int
    n_params: int
    hot_runs: int
    timeout: int
    disable_jit: bool
    sort_params: bool

# Warmup Tier: Ensure cold can complete within timeout to get hot (steady-state) numbers
TIER_JIT_BARS = int(os.environ.get("FISHBRO_PERF_BARS", "200000"))
TIER_JIT_PARAMS = int(os.environ.get("FISHBRO_PERF_PARAMS", "10000"))
TIER_JIT_HOT_RUNS = int(os.environ.get("FISHBRO_PERF_HOTRUNS", "5"))
TIER_JIT_TIMEOUT = int(os.environ.get("FISHBRO_PERF_TIMEOUT_S", "600"))

# Stress Tier: Optional, for extreme throughput testing (requires larger timeout or skip-cold)
TIER_STRESS_BARS = int(os.environ.get("FISHBRO_PERF_STRESS_BARS", "200000"))
TIER_STRESS_PARAMS = int(os.environ.get("FISHBRO_PERF_STRESS_PARAMS", "10000"))

TIER_TOY_BARS = 2_000
TIER_TOY_PARAMS = 10
TIER_TOY_HOT_RUNS = 1
TIER_TOY_TIMEOUT = 60

# Warmup compile tier (for skip-cold mode)
TIER_WARMUP_COMPILE_BARS = 2_000
TIER_WARMUP_COMPILE_PARAMS = 200

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

# ==========================================
# 2. è³‡æ–™ç”Ÿæˆ (Deterministic)
# ==========================================

def generate_synthetic_data(n_bars: int, seed: int = 42) -> Dict[str, np.ndarray]:
    rng = np.random.default_rng(seed)
    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10
    high = close + np.abs(rng.standard_normal(n_bars)) * 5
    low = close - np.abs(rng.standard_normal(n_bars)) * 5
    open_ = (high + low) / 2 + rng.standard_normal(n_bars)
    
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    data = {
        "open": open_.astype(np.float64),
        "high": high.astype(np.float64),
        "low": low.astype(np.float64),
        "close": close.astype(np.float64),
    }
    
    for k, v in data.items():
        if not v.flags['C_CONTIGUOUS']:
            data[k] = np.ascontiguousarray(v)
    return data

def generate_params(n_params: int, seed: int = 999) -> np.ndarray:
    rng = np.random.default_rng(seed)
    w1 = rng.integers(10, 100, size=n_params)
    w2 = rng.integers(5, 50, size=n_params)
    # runner_grid contract: params_matrix must be (n, >=3)
    # Provide a minimal 3-column schema for perf harness.
    w3 = rng.integers(2, 30, size=n_params)
    params = np.column_stack((w1, w2, w3)).astype(np.float64)
    if not params.flags['C_CONTIGUOUS']:
        params = np.ascontiguousarray(params)
    return params

# ==========================================
# 3. Worker é‚è¼¯ (Child Process)
# ==========================================

def worker_log(msg: str):
    print(f"[worker] {msg}", flush=True)


def _env_flag(name: str) -> bool:
    return os.environ.get(name, "").strip() == "1"


def _env_int(name: str, default: int) -> int:
    try:
        return int(os.environ.get(name, str(default)))
    except Exception:
        return default


# NOTE: _format_profile_report moved to src/FishBroWFS_V2/perf/profile_report.py

def _run_microbench_numba_indicators(closes: np.ndarray, hot_runs: int) -> Dict[str, Any]:
    """
    Perf-only microbench:
      - Prove Numba is active in worker process.
      - Measure pure numeric kernels (no Python object loop) baseline.
    """
    try:
        import numba as nb  # type: ignore
    except Exception:  # pragma: no cover
        return {"microbench": "numba_missing"}

    from FishBroWFS_V2.indicators import numba_indicators as ni  # type: ignore

    # Use a fixed window; keep deterministic and cheap.
    length = 14
    x = np.ascontiguousarray(closes, dtype=np.float64)

    # Warmup compile (first call triggers compilation if JIT enabled).
    _ = ni.rolling_max(x, length)

    # Hot runs
    times: List[float] = []
    for _i in range(max(1, hot_runs)):
        t0 = time.perf_counter()
        _ = ni.rolling_max(x, length)
        times.append(time.perf_counter() - t0)

    best = min(times) if times else 0.0
    n = int(x.shape[0])
    # rolling_max visits each element once -> treat as "ops" ~= n
    tput = (n / best) if best > 0 else 0.0
    return {
        "microbench": "rolling_max",
        "n": n,
        "best_s": best,
        "ops_per_s": tput,
        "nb_disable_jit": int(getattr(nb.config, "DISABLE_JIT", -1)),
    }


def run_worker(
    npz_path: str,
    hot_runs: int,
    skip_cold: bool = False,
    warmup_bars: int = 0,
    warmup_params: int = 0,
    microbench: bool = False,
):
    try:
        worker_log(f"Starting. Loading input: {npz_path}")
        
        with np.load(npz_path, allow_pickle=False) as data:
            opens = data['open']
            highs = data['high']
            lows = data['low']
            closes = data['close']
            params = data['params']
            
        worker_log(f"Data loaded. Bars: {len(opens)}, Params: {len(params)}")

        if microbench:
            worker_log("MICROBENCH enabled: running numba indicator microbench.")
            res = _run_microbench_numba_indicators(closes, hot_runs=hot_runs)
            print("__RESULT_JSON_START__")
            print(json.dumps({"mode": "microbench", "result": res}))
            print("__RESULT_JSON_END__")
            return
        
        try:
            # Phase 3B Grid Runner (correct target)
            # src/FishBroWFS_V2/pipeline/runner_grid.py
            from FishBroWFS_V2.pipeline.runner_grid import run_grid  # type: ignore
            worker_log("Grid runner imported successfully (FishBroWFS_V2.pipeline.runner_grid).")
            # Enable runner_grid observability payload in returned dict (timings + jit truth + counts).
            os.environ["FISHBRO_PROFILE_GRID"] = "1"

            # ---- JIT truth report (perf-only) ----
            worker_log(f"ENV NUMBA_DISABLE_JIT={os.environ.get('NUMBA_DISABLE_JIT','')!r}")
            try:
                import numba as _nb  # type: ignore
                worker_log(f"Numba present. nb.config.DISABLE_JIT={getattr(_nb.config,'DISABLE_JIT',None)!r}")
            except Exception as _e:
                worker_log(f"Numba import failed: {_e!r}")

            # run_grid itself might be Python; report what it is.
            worker_log(f"run_grid type={type(run_grid)} has_signatures={hasattr(run_grid,'signatures')}")
            if hasattr(run_grid, "signatures"):
                worker_log(f"run_grid.signatures(before)={getattr(run_grid,'signatures',None)!r}")
            # --------------------------------------
        except ImportError as e:
            worker_log(f"FATAL: Import grid runner failed: {e!r}")
            
            # --- DEBUG INFO ---
            worker_log(f"Current sys.path: {sys.path}")
            src_path = Path(__file__).resolve().parent.parent / "src"
            if src_path.exists():
                worker_log(f"Listing {src_path}:")
                try:
                    for p in src_path.iterdir():
                        worker_log(f" - {p.name}")
                        if p.is_dir() and (p / "__init__.py").exists():
                             worker_log(f"   (package content): {[sub.name for sub in p.iterdir()]}")
                except Exception as ex:
                    worker_log(f"   Error listing dir: {ex}")
            else:
                worker_log(f"Src path not found at: {src_path}")
            # ------------------
            sys.exit(1)
        
        # Warmup run (perf-only): compile/JIT on a tiny slice so the real run measures steady-state.
        # IMPORTANT: respect CLI-provided warmup_{bars,params}. If 0, fall back to defaults.
        if warmup_bars and warmup_bars > 0:
            wb = min(int(warmup_bars), len(opens))
        else:
            wb = min(2000, len(opens))

        if warmup_params and warmup_params > 0:
            wp = min(int(warmup_params), len(params))
        else:
            wp = min(200, len(params))
        if wb >= 10 and wp >= 10:
            worker_log(f"Starting WARMUP run (bars={wb}, params={wp})...")
            _ = run_grid(
                open_=opens[:wb],
                high=highs[:wb],
                low=lows[:wb],
                close=closes[:wb],
                params_matrix=params[:wp],
                commission=0.0,
                slip=0.0,
                sort_params=False,
            )
            worker_log("WARMUP finished.")
            if hasattr(run_grid, "signatures"):
                worker_log(f"run_grid.signatures(after)={getattr(run_grid,'signatures',None)!r}")
        
        lane_sort = os.environ.get("FISHBRO_PERF_LANE_SORT", "0").strip() == "1"
        lane_id = os.environ.get("FISHBRO_PERF_LANE_ID", "?").strip()
        do_profile = _env_flag("FISHBRO_PERF_PROFILE")
        topn = _env_int("FISHBRO_PERF_PROFILE_TOP", 40)
        mode = os.environ.get("FISHBRO_PERF_PROFILE_MODE", "").strip()
        jit_enabled = os.environ.get("NUMBA_DISABLE_JIT", "").strip() != "1"
        cold_time = 0.0
        if skip_cold:
            # Skip-cold mode: warmup already done, skip full cold run
            worker_log("Skip-cold mode: skipping full cold run (warmup already completed)")
        else:
            # Full cold run
            worker_log("Starting COLD run...")
            t0 = time.perf_counter()
            _ = run_grid(
                open_=opens,
                high=highs,
                low=lows,
                close=closes,
                params_matrix=params,
                commission=0.0,
                slip=0.0,
                sort_params=lane_sort,
            )
            cold_time = time.perf_counter() - t0
            worker_log(f"COLD run finished: {cold_time:.4f}s")
        
        worker_log(f"Starting {hot_runs} HOT runs (GC disabled)...")
        hot_times = []
        last_out: Optional[Dict[str, Any]] = None
        gc.disable()
        try:
            for i in range(hot_runs):
                t_start = time.perf_counter()
                if do_profile and i == 0:
                    pr = cProfile.Profile()
                    pr.enable()
                    last_out = run_grid(
                        open_=opens,
                        high=highs,
                        low=lows,
                        close=closes,
                        params_matrix=params,
                        commission=0.0,
                        slip=0.0,
                        sort_params=lane_sort,
                    )
                    pr.disable()
                    print(
                        _format_profile_report(
                            lane_id=lane_id,
                            n_bars=int(len(opens)),
                            n_params=int(len(params)),
                            jit_enabled=bool(jit_enabled),
                            sort_params=bool(lane_sort),
                            topn=int(topn),
                            mode=mode,
                            pr=pr,
                        ),
                        end="",
                    )
                else:
                    last_out = run_grid(
                        open_=opens,
                        high=highs,
                        low=lows,
                        close=closes,
                        params_matrix=params,
                        commission=0.0,
                        slip=0.0,
                        sort_params=lane_sort,
                    )
                t_end = time.perf_counter()
                hot_times.append(t_end - t_start)
        finally:
            gc.enable()
        
        avg_hot = statistics.mean(hot_times) if hot_times else 0.0
        min_hot = min(hot_times) if hot_times else 0.0
        
        result = {
            "cold_time": cold_time,
            "hot_times": hot_times,
            "avg_hot_time": avg_hot,
            "min_hot_time": min_hot,
            "n_bars": len(opens),
            "n_params": len(params),
            "throughput": (len(opens) * len(params)) / min_hot if min_hot > 0 else 0,
        }

        # Attach runner_grid observability payload (timings + jit truth + counts)
        if isinstance(last_out, dict) and "perf" in last_out:
            result["perf"] = last_out["perf"]
        
        print(f"__RESULT_JSON_START__")
        print(json.dumps(result))
        print(f"__RESULT_JSON_END__")
        
    except Exception as e:
        worker_log(f"CRASH: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

# ==========================================
# 4. Controller é‚è¼¯ (Host Process)
# ==========================================

def run_lane(
    lane_id: int,
    cfg: PerfConfig,
    tmp_dir: str,
    ohlc_data: Dict[str, np.ndarray],
    microbench: bool = False,
) -> Dict[str, Any]:
    print(f"\n>>> Running Lane {lane_id}: {cfg.name}")
    print(f"    Config: Bars={cfg.n_bars}, Params={cfg.n_params}, JIT={not cfg.disable_jit}, Sort={cfg.sort_params}")
    
    params = generate_params(cfg.n_params)
    # Do not pre-sort here; sorting behavior must be owned by runner_grid(sort_params=...).
    # For no-sort lane, we shuffle to simulate random access order.
    if not cfg.sort_params:
        np.random.shuffle(params)
        print("    Params shuffled (random access simulation).")
    else:
        print("    Params left unsorted; runner_grid(sort_params=True) will apply cache-friendly sort.")
        
    npz_path = os.path.join(tmp_dir, f"input_lane_{lane_id}.npz")
    np.savez_compressed(
        npz_path, 
        open=ohlc_data["open"][:cfg.n_bars],
        high=ohlc_data["high"][:cfg.n_bars],
        low=ohlc_data["low"][:cfg.n_bars],
        close=ohlc_data["close"][:cfg.n_bars],
        params=params
    )
    
    env = os.environ.copy()
    
    # é—œéµä¿®æ­£: å¼·åˆ¶æ³¨å…¥ PYTHONPATH ç¢ºä¿å­é€²ç¨‹çœ‹å¾—åˆ° src
    src_path = str(PROJECT_ROOT / "src")
    if "PYTHONPATH" in env:
        env["PYTHONPATH"] = f"{src_path}:{env['PYTHONPATH']}"
    else:
        env["PYTHONPATH"] = src_path
        
    if cfg.disable_jit:
        env["NUMBA_DISABLE_JIT"] = "1"
    else:
        env.pop("NUMBA_DISABLE_JIT", None)
        
    # Build worker command
    cmd = [
        sys.executable,
        __file__,
        "--worker",
        "--input",
        npz_path,
        "--hot-runs",
        str(cfg.hot_runs),
    ]
    if microbench:
        cmd.append("--microbench")
    # Pass lane sort flag to worker via env (avoid CLI churn)
    env["FISHBRO_PERF_LANE_SORT"] = "1" if cfg.sort_params else "0"
    env["FISHBRO_PERF_LANE_ID"] = str(lane_id)
    
    # Add skip-cold and warmup params if needed
    skip_cold = os.environ.get("FISHBRO_PERF_SKIP_COLD", "").lower() == "true"
    if skip_cold:
        cmd.extend(["--skip-cold"])
        warmup_bars = int(os.environ.get("FISHBRO_PERF_WARMUP_BARS", str(TIER_WARMUP_COMPILE_BARS)))
        warmup_params = int(os.environ.get("FISHBRO_PERF_WARMUP_PARAMS", str(TIER_WARMUP_COMPILE_PARAMS)))
        cmd.extend(["--warmup-bars", str(warmup_bars), "--warmup-params", str(warmup_params)])
    
    try:
        proc = subprocess.run(
            cmd,
            env=env,
            capture_output=True,
            text=True,
            timeout=cfg.timeout,
            check=True
        )
        
        stdout = proc.stdout
        # Print worker stdout (includes JIT truth report)
        print(stdout, end="")
        
        result_json = None
        lines = stdout.splitlines()
        capture = False
        json_str = ""
        
        for line in lines:
            if line.strip() == "__RESULT_JSON_END__":
                capture = False
            if capture:
                json_str += line
            if line.strip() == "__RESULT_JSON_START__":
                capture = True
                
        if json_str:
            result_json = json.loads(json_str)
            
            # Phase 3.0-C: FAIL-FAST defense - detect fallback to object mode
            strict_arrays = os.environ.get("FISHBRO_PERF_STRICT_ARRAYS", "1").strip() == "1"
            if strict_arrays and isinstance(result_json, dict):
                perf = result_json.get("perf")
                if isinstance(perf, dict):
                    intent_mode = perf.get("intent_mode")
                    if intent_mode != "arrays":
                        # Handle None or any non-"arrays" value
                        intent_mode_str = str(intent_mode) if intent_mode is not None else "None"
                        error_msg = (
                            f"ERROR: intent_mode expected 'arrays' but got '{intent_mode_str}' (lane {lane_id})\n"
                            f"This indicates the kernel fell back to object mode, which is a performance regression.\n"
                            f"To disable this check, set FISHBRO_PERF_STRICT_ARRAYS=0"
                        )
                        print(f"âŒ {error_msg}", file=sys.stderr)
                        raise RuntimeError(error_msg)
            
            return result_json
        else:
            print("âŒ Error: Worker finished but no JSON result found.")
            print("--- Worker Stdout ---")
            print(stdout)
            print("--- Worker Stderr ---")
            print(proc.stderr)
            return {}
            
    except subprocess.TimeoutExpired as e:
        print(f"âŒ Error: Lane {lane_id} Timeout ({cfg.timeout}s).")
        if e.stdout: print(e.stdout)
        if e.stderr: print(e.stderr)
        return {}
    except subprocess.CalledProcessError as e:
        print(f"âŒ Error: Lane {lane_id} Crashed (Exit {e.returncode}).")
        print("--- Worker Stdout ---")
        print(e.stdout)
        print("--- Worker Stderr ---")
        print(e.stderr)
        return {}
    except Exception as e:
        print(f"âŒ Error: System error {e}")
        return {}

def print_report(results: List[Dict[str, Any]]):
    print("\n\n=== FishBro WFS Perf Harness Report ===")
    print("| Lane | Mode | Sort | Bars | Params | Cold(s) | Hot(s) | Tput (Ops/s) | Speedup |")
    print("|---|---|---|---|---|---|---|---|---|")
    
    jit_no_sort_tput = 0
    for r in results:
        if not r: continue
        lane_id = r['lane_id']
        name = r['name']
        bars = r['res'].get('n_bars', 0)
        params = r['res'].get('n_params', 0)
        cold = r['res'].get('cold_time', 0)
        hot = r['res'].get('min_hot_time', 0)
        tput = r['res'].get('throughput', 0)
        
        if lane_id == 3:
            jit_no_sort_tput = tput
            speedup = "1.0x (Base)"
        elif jit_no_sort_tput > 0 and tput > 0:
            ratio = tput / jit_no_sort_tput
            speedup = f"{ratio:.2f}x"
        else:
            speedup = "-"
            
        mode = "Py" if r.get("disable_jit", False) else "JIT"
        sort = "Yes" if r.get("sort_params", False) else "No"
        print(f"| {lane_id} | {mode} | {sort} | {bars} | {params} | {cold:.4f} | {hot:.4f} | {int(tput):,} | {speedup} |")
    print("\nNote: Tput = (Bars * Params) / Min Hot Run Time")

def run_matcherbench() -> None:
    """
    Matcher-only microbenchmark.
    Purpose:
      - Measure true throughput of cursor-based matcher kernel
      - Avoid runner_grid / Python orchestration overhead
    """
    from FishBroWFS_V2.engine.engine_jit import simulate
    from FishBroWFS_V2.engine.types import (
        BarArrays,
        OrderIntent,
        OrderKind,
        OrderRole,
        Side,
    )

    # ---- config (safe defaults) ----
    n_bars = int(os.environ.get("FISHBRO_MB_BARS", "20000"))
    intents_per_bar = int(os.environ.get("FISHBRO_MB_INTENTS_PER_BAR", "2"))
    hot_runs = int(os.environ.get("FISHBRO_MB_HOTRUNS", "3"))

    print(
        f"[matcherbench] bars={n_bars}, intents_per_bar={intents_per_bar}, hot_runs={hot_runs}"
    )

    # ---- synthetic OHLC ----
    rng = np.random.default_rng(42)
    close = 10000 + np.cumsum(rng.standard_normal(n_bars))
    high = close + 5.0
    low = close - 5.0
    open_ = (high + low) * 0.5

    bars = BarArrays(
        open=open_.astype(np.float64),
        high=high.astype(np.float64),
        low=low.astype(np.float64),
        close=close.astype(np.float64),
    )

    # ---- generate intents: created_bar = t-1 ----
    intents = []
    oid = 1
    for t in range(1, n_bars):
        for _ in range(intents_per_bar):
            # ENTRY
            intents.append(
                OrderIntent(
                    order_id=oid,
                    created_bar=t - 1,
                    role=OrderRole.ENTRY,
                    kind=OrderKind.STOP,
                    side=Side.BUY,
                    price=float(high[t - 1]),
                    qty=1,
                )
            )
            oid += 1
            # EXIT
            intents.append(
                OrderIntent(
                    order_id=oid,
                    created_bar=t - 1,
                    role=OrderRole.EXIT,
                    kind=OrderKind.STOP,
                    side=Side.SELL,
                    price=float(low[t - 1]),
                    qty=1,
                )
            )
            oid += 1

    print(f"[matcherbench] total_intents={len(intents)}")

    # ---- warmup (compile) ----
    simulate(bars, intents)

    # ---- hot runs ----
    times = []
    gc.disable()
    try:
        for _ in range(hot_runs):
            t0 = time.perf_counter()
            fills = simulate(bars, intents)
            dt = time.perf_counter() - t0
            times.append(dt)
    finally:
        gc.enable()

    best = min(times)
    bars_per_s = n_bars / best
    intents_scanned = len(intents)
    intents_per_s = intents_scanned / best
    fills_per_s = len(fills) / best

    print("\n=== MATCHERBENCH RESULT ===")
    print(f"best_time_s      : {best:.6f}")
    print(f"bars_per_sec     : {bars_per_s:,.0f}")
    print(f"intents_per_sec  : {intents_per_s:,.0f}")
    print(f"fills_per_sec    : {fills_per_s:,.0f}")


def main():
    parser = argparse.ArgumentParser(description="FishBro WFS Perf Harness")
    parser.add_argument("--worker", action="store_true", help="Run as worker")
    parser.add_argument("--input", type=str, help="Path to input NPZ")
    parser.add_argument("--hot-runs", type=int, default=5, help="Hot runs")
    parser.add_argument("--skip-cold", action="store_true", help="Skip full cold run, use warmup compile instead")
    parser.add_argument("--warmup-bars", type=int, default=0, help="Warmup compile bars (for skip-cold)")
    parser.add_argument("--warmup-params", type=int, default=0, help="Warmup compile params (for skip-cold)")
    parser.add_argument("--microbench", action="store_true", help="Run microbench only (numba indicator baseline)")
    parser.add_argument("--include-python-baseline", action="store_true", help="Include Toy Tier")
    parser.add_argument(
        "--matcherbench",
        action="store_true",
        help="Benchmark matcher kernel only (engine_jit.simulate), no runner_grid",
    )
    parser.add_argument("--stress-tier", action="store_true", help="Use stress tier (200kÃ—10k) instead of warmup tier")
    args = parser.parse_args()
    
    if args.matcherbench:
        run_matcherbench()
        return

    if args.worker:
        if not args.input: sys.exit(1)
        run_worker(
            args.input,
            args.hot_runs,
            args.skip_cold,
            args.warmup_bars,
            args.warmup_params,
            args.microbench,
        )
        return

    print("Initializing Perf Harness...")
    lanes_cfg: List[PerfConfig] = []
    
    # Select tier based on stress-tier flag
    if args.stress_tier:
        jit_bars = TIER_STRESS_BARS
        jit_params = TIER_STRESS_PARAMS
        print(f"Using STRESS tier: {jit_bars:,} bars Ã— {jit_params:,} params")
    else:
        jit_bars = TIER_JIT_BARS
        jit_params = TIER_JIT_PARAMS
        print(f"Using WARMUP tier: {jit_bars:,} bars Ã— {jit_params:,} params")
    
    if args.include_python_baseline:
        lanes_cfg.append(PerfConfig("Lane 1 (Py, No Sort)", TIER_TOY_BARS, TIER_TOY_PARAMS, TIER_TOY_HOT_RUNS, TIER_TOY_TIMEOUT, True, False))
        lanes_cfg.append(PerfConfig("Lane 2 (Py, Sort)", TIER_TOY_BARS, TIER_TOY_PARAMS, TIER_TOY_HOT_RUNS, TIER_TOY_TIMEOUT, True, True))
        
    lanes_cfg.append(PerfConfig("Lane 3 (JIT, No Sort)", jit_bars, jit_params, TIER_JIT_HOT_RUNS, TIER_JIT_TIMEOUT, False, False))
    lanes_cfg.append(PerfConfig("Lane 4 (JIT, Sort)", jit_bars, jit_params, TIER_JIT_HOT_RUNS, TIER_JIT_TIMEOUT, False, True))
    
    max_bars = max(c.n_bars for c in lanes_cfg)
    print(f"Generating synthetic data (Max Bars: {max_bars})...")
    ohlc_data = generate_synthetic_data(max_bars)
    
    results = []
    try:
        with tempfile.TemporaryDirectory() as tmp_dir:
            print(f"Created temp dir for IPC: {tmp_dir}")
            for i, cfg in enumerate(lanes_cfg):
                lane_id = i + 1
                if not args.include_python_baseline: lane_id += 2 
                res = run_lane(lane_id, cfg, tmp_dir, ohlc_data, microbench=args.microbench)
                if res:
                    results.append(
                        {
                            "lane_id": lane_id,
                            "name": cfg.name,
                            "res": res,
                            "disable_jit": cfg.disable_jit,
                            "sort_params": cfg.sort_params,
                        }
                    )
                else: results.append({})
                
        print_report(results)
    except RuntimeError as e:
        # Phase 3.0-C: FAIL-FAST - exit with non-zero code on intent_mode violation
        print(f"\nâŒ FAIL-FAST triggered: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()


================================================================================
FILE: src/FishBroWFS_V2/__init__.py
================================================================================




================================================================================
FILE: src/FishBroWFS_V2/data/__init__.py
================================================================================




================================================================================
FILE: src/FishBroWFS_V2/data/layout.py
================================================================================

import numpy as np
from FishBroWFS_V2.engine.types import BarArrays


def ensure_float64_contiguous(x: np.ndarray) -> np.ndarray:
    arr = np.asarray(x, dtype=np.float64)
    if not arr.flags["C_CONTIGUOUS"]:
        arr = np.ascontiguousarray(arr)
    return arr


def normalize_bars(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
) -> BarArrays:
    arrays = [open_, high, low, close]
    for a in arrays:
        if np.isnan(a).any():
            raise ValueError("NaN detected in input data")

    o = ensure_float64_contiguous(open_)
    h = ensure_float64_contiguous(high)
    l = ensure_float64_contiguous(low)
    c = ensure_float64_contiguous(close)

    return BarArrays(open=o, high=h, low=l, close=c)



================================================================================
FILE: src/FishBroWFS_V2/engine/__init__.py
================================================================================




================================================================================
FILE: src/FishBroWFS_V2/engine/constants.py
================================================================================

"""
Engine integer constants (hot-path friendly).

These constants are used in array/SoA pathways to avoid Enum.value lookups in tight loops.
"""

ROLE_EXIT = 0
ROLE_ENTRY = 1

KIND_STOP = 0
KIND_LIMIT = 1

SIDE_SELL = -1
SIDE_BUY = 1




================================================================================
FILE: src/FishBroWFS_V2/engine/constitution.py
================================================================================

"""
Engine Constitution v1.1 (FROZEN)

Activation:
- Orders are created at Bar[T] close and become active at Bar[T+1].

STOP fills (Open==price is treated as GAP branch):
Buy Stop @ S:
- if Open >= S: fill = Open
- elif High >= S: fill = S
Sell Stop @ S:
- if Open <= S: fill = Open
- elif Low <= S: fill = S

LIMIT fills (Open==price is treated as GAP branch):
Buy Limit @ L:
- if Open <= L: fill = Open
- elif Low <= L: fill = L
Sell Limit @ L:
- if Open >= L: fill = Open
- elif High >= L: fill = L

Priority:
- STOP wins over LIMIT (risk-first pessimism).

Same-bar In/Out:
- If entry and exit are both triggerable in the same bar, execute Entry then Exit.

Same-kind tie rule:
- If multiple orders of the same role are triggerable in the same bar, execute EXIT-first.
- Within the same role+kind, use deterministic order: smaller order_id first.
"""

NEXT_BAR_ACTIVE = True
PRIORITY_STOP_OVER_LIMIT = True
SAME_BAR_ENTRY_THEN_EXIT = True
SAME_KIND_TIE_EXIT_FIRST = True



================================================================================
FILE: src/FishBroWFS_V2/engine/engine_jit.py
================================================================================

from __future__ import annotations

from dataclasses import asdict
from typing import Iterable, List, Tuple

import numpy as np

# Engine JIT matcher kernel contract:
# - Complexity target: O(B + I + A), where:
#     B = bars, I = intents, A = per-bar active-book scan.
# - Forbidden: scanning all intents per bar (O(B*I)).
# - Extension point: ttl_bars (0=GTC, 1=one-shot next-bar-only, future: >1).

try:
    import numba as nb
except Exception:  # pragma: no cover
    nb = None  # type: ignore

from FishBroWFS_V2.engine.types import (
    BarArrays,
    Fill,
    OrderIntent,
    OrderKind,
    OrderRole,
    Side,
)
from FishBroWFS_V2.engine.matcher_core import simulate as simulate_py
from FishBroWFS_V2.engine.constants import (
    KIND_LIMIT,
    KIND_STOP,
    ROLE_ENTRY,
    ROLE_EXIT,
    SIDE_BUY,
    SIDE_SELL,
)

STATUS_OK = 0
STATUS_ERROR_UNSORTED = 1

# JIT truth (debug/perf observability)
JIT_PATH_USED_LAST = False
JIT_KERNEL_SIGNATURES_LAST = None  # type: ignore


def get_jit_truth() -> dict:
    """
    Debug helper: returns whether the last simulate() call used the JIT kernel,
    and (if available) the kernel signatures snapshot.
    """
    return {
        "jit_path_used": bool(JIT_PATH_USED_LAST),
        "kernel_signatures": JIT_KERNEL_SIGNATURES_LAST,
    }


def _to_int(x) -> int:
    # Enum values are int/str; we convert deterministically.
    if isinstance(x, Side):
        return int(x.value)
    if isinstance(x, OrderRole):
        # EXIT first tie-break relies on role; map explicitly.
        return 0 if x == OrderRole.EXIT else 1
    if isinstance(x, OrderKind):
        return 0 if x == OrderKind.STOP else 1
    return int(x)


def _to_kind_int(k: OrderKind) -> int:
    return 0 if k == OrderKind.STOP else 1


def _to_role_int(r: OrderRole) -> int:
    return 0 if r == OrderRole.EXIT else 1


def _to_side_int(s: Side) -> int:
    return int(s.value)


def _kind_from_int(v: int) -> OrderKind:
    return OrderKind.STOP if v == 0 else OrderKind.LIMIT


def _role_from_int(v: int) -> OrderRole:
    return OrderRole.EXIT if v == 0 else OrderRole.ENTRY


def _side_from_int(v: int) -> Side:
    return Side.BUY if v == 1 else Side.SELL


def _pack_intents(intents: Iterable[OrderIntent]):
    """
    Pack intents into plain arrays for numba.

    Fields:
      order_id: int64
      created_bar: int64
      role: int8 (0=EXIT,1=ENTRY)
      kind: int8 (0=STOP,1=LIMIT)
      side: int8 (1=BUY,-1=SELL)
      price: float64
      qty: int64
    """
    it = list(intents)
    n = len(it)
    order_id = np.empty(n, dtype=np.int64)
    created_bar = np.empty(n, dtype=np.int64)
    role = np.empty(n, dtype=np.int8)
    kind = np.empty(n, dtype=np.int8)
    side = np.empty(n, dtype=np.int8)
    price = np.empty(n, dtype=np.float64)
    qty = np.empty(n, dtype=np.int64)

    for i, x in enumerate(it):
        order_id[i] = int(x.order_id)
        created_bar[i] = int(x.created_bar)
        role[i] = np.int8(_to_role_int(x.role))
        kind[i] = np.int8(_to_kind_int(x.kind))
        side[i] = np.int8(_to_side_int(x.side))
        price[i] = float(x.price)
        qty[i] = int(x.qty)

    return order_id, created_bar, role, kind, side, price, qty


def _sort_packed_by_created_bar(
    packed: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Sort packed intent arrays by (created_bar, order_id).

    Why:
      - Cursor + active-book kernel requires activate_bar=(created_bar+1) and order_id to be non-decreasing.
      - Determinism is preserved because selection is still based on (kind priority, order_id).
    """
    order_id, created_bar, role, kind, side, price, qty = packed
    # lexsort uses last key as primary -> (created_bar primary, order_id secondary)
    idx = np.lexsort((order_id, created_bar))
    return (
        order_id[idx],
        created_bar[idx],
        role[idx],
        kind[idx],
        side[idx],
        price[idx],
        qty[idx],
    )


def simulate(
    bars: BarArrays,
    intents: Iterable[OrderIntent],
) -> List[Fill]:
    """
    Phase 2A: JIT accelerated matcher.

    Kill switch:
      - If numba is unavailable OR NUMBA_DISABLE_JIT=1, fall back to Python reference.
    """
    global JIT_PATH_USED_LAST, JIT_KERNEL_SIGNATURES_LAST

    if nb is None:
        JIT_PATH_USED_LAST = False
        JIT_KERNEL_SIGNATURES_LAST = None
        return simulate_py(bars, intents)

    # If numba is disabled, keep behavior stable.
    # Numba respects NUMBA_DISABLE_JIT; but we short-circuit to be safe.
    import os

    if os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        JIT_PATH_USED_LAST = False
        JIT_KERNEL_SIGNATURES_LAST = None
        return simulate_py(bars, intents)

    packed = _sort_packed_by_created_bar(_pack_intents(intents))
    status, fills_arr = _simulate_kernel(
        bars.open,
        bars.high,
        bars.low,
        packed[0],
        packed[1],
        packed[2],
        packed[3],
        packed[4],
        packed[5],
        packed[6],
        np.int64(1),  # ttl_bars=1 keeps Phase-2 semantics (next-bar only)
    )
    if int(status) != STATUS_OK:
        JIT_PATH_USED_LAST = True
        raise RuntimeError(f"engine_jit kernel error: status={int(status)}")

    # record JIT truth (best-effort)
    JIT_PATH_USED_LAST = True
    try:
        sigs = getattr(_simulate_kernel, "signatures", None)
        if sigs is not None:
            JIT_KERNEL_SIGNATURES_LAST = list(sigs)
        else:
            JIT_KERNEL_SIGNATURES_LAST = None
    except Exception:
        JIT_KERNEL_SIGNATURES_LAST = None

    # Convert to Fill objects (drop unused capacity)
    out: List[Fill] = []
    m = fills_arr.shape[0]
    for i in range(m):
        row = fills_arr[i]
        out.append(
            Fill(
                bar_index=int(row[0]),
                role=_role_from_int(int(row[1])),
                kind=_kind_from_int(int(row[2])),
                side=_side_from_int(int(row[3])),
                price=float(row[4]),
                qty=int(row[5]),
                order_id=int(row[6]),
            )
        )
    return out


def simulate_arrays(
    bars: BarArrays,
    *,
    order_id: np.ndarray,
    created_bar: np.ndarray,
    role: np.ndarray,
    kind: np.ndarray,
    side: np.ndarray,
    price: np.ndarray,
    qty: np.ndarray,
    ttl_bars: int = 1,
) -> List[Fill]:
    """
    Array/SoA entry point: bypass OrderIntent objects and _pack_intents hot-path.

    Arrays must be 1D and same length. Dtypes are expected:
      order_id,int64 ; created_bar,int64 ; role,int8 ; kind,int8 ; side,int8 ; price,float64 ; qty,int64

    ttl_bars:
      1 => one-shot next-bar-only (Phase 2 semantics)
      0 => GTC extension point (debug/tests)
    """
    global JIT_PATH_USED_LAST, JIT_KERNEL_SIGNATURES_LAST

    # Normalize/ensure arrays are numpy with the expected dtypes (cold path).
    oid = np.asarray(order_id, dtype=np.int64)
    cb = np.asarray(created_bar, dtype=np.int64)
    rl = np.asarray(role, dtype=np.int8)
    kd = np.asarray(kind, dtype=np.int8)
    sd = np.asarray(side, dtype=np.int8)
    px = np.asarray(price, dtype=np.float64)
    qy = np.asarray(qty, dtype=np.int64)

    if nb is None:
        JIT_PATH_USED_LAST = False
        JIT_KERNEL_SIGNATURES_LAST = None
        intents: List[OrderIntent] = []
        n = int(oid.shape[0])
        for i in range(n):
            r = OrderRole.EXIT if int(rl[i]) == ROLE_EXIT else OrderRole.ENTRY
            k = OrderKind.STOP if int(kd[i]) == KIND_STOP else OrderKind.LIMIT
            s = Side.BUY if int(sd[i]) == SIDE_BUY else Side.SELL
            intents.append(
                OrderIntent(
                    order_id=int(oid[i]),
                    created_bar=int(cb[i]),
                    role=r,
                    kind=k,
                    side=s,
                    price=float(px[i]),
                    qty=int(qy[i]),
                )
            )
        return simulate_py(bars, intents)

    import os

    if os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        JIT_PATH_USED_LAST = False
        JIT_KERNEL_SIGNATURES_LAST = None
        intents: List[OrderIntent] = []
        n = int(oid.shape[0])
        for i in range(n):
            r = OrderRole.EXIT if int(rl[i]) == ROLE_EXIT else OrderRole.ENTRY
            k = OrderKind.STOP if int(kd[i]) == KIND_STOP else OrderKind.LIMIT
            s = Side.BUY if int(sd[i]) == SIDE_BUY else Side.SELL
            intents.append(
                OrderIntent(
                    order_id=int(oid[i]),
                    created_bar=int(cb[i]),
                    role=r,
                    kind=k,
                    side=s,
                    price=float(px[i]),
                    qty=int(qy[i]),
                )
            )
        return simulate_py(bars, intents)

    packed = _sort_packed_by_created_bar((oid, cb, rl, kd, sd, px, qy))
    status, fills_arr = _simulate_kernel(
        bars.open,
        bars.high,
        bars.low,
        packed[0],
        packed[1],
        packed[2],
        packed[3],
        packed[4],
        packed[5],
        packed[6],
        np.int64(ttl_bars),
    )
    if int(status) != STATUS_OK:
        JIT_PATH_USED_LAST = True
        raise RuntimeError(f"engine_jit kernel error: status={int(status)}")

    JIT_PATH_USED_LAST = True
    try:
        sigs = getattr(_simulate_kernel, "signatures", None)
        if sigs is not None:
            JIT_KERNEL_SIGNATURES_LAST = list(sigs)
        else:
            JIT_KERNEL_SIGNATURES_LAST = None
    except Exception:
        JIT_KERNEL_SIGNATURES_LAST = None

    out: List[Fill] = []
    m = fills_arr.shape[0]
    for i in range(m):
        row = fills_arr[i]
        out.append(
            Fill(
                bar_index=int(row[0]),
                role=_role_from_int(int(row[1])),
                kind=_kind_from_int(int(row[2])),
                side=_side_from_int(int(row[3])),
                price=float(row[4]),
                qty=int(row[5]),
                order_id=int(row[6]),
            )
        )
    return out


def _simulate_with_ttl(bars: BarArrays, intents: Iterable[OrderIntent], ttl_bars: int) -> List[Fill]:
    """
    Internal helper (tests/dev): run JIT matcher with a custom ttl_bars.
    ttl_bars=0 => GTC, ttl_bars=1 => one-shot next-bar-only (default).
    """
    if nb is None:
        return simulate_py(bars, intents)

    import os

    if os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        return simulate_py(bars, intents)

    packed = _sort_packed_by_created_bar(_pack_intents(intents))
    status, fills_arr = _simulate_kernel(
        bars.open,
        bars.high,
        bars.low,
        packed[0],
        packed[1],
        packed[2],
        packed[3],
        packed[4],
        packed[5],
        packed[6],
        np.int64(ttl_bars),
    )
    if int(status) != STATUS_OK:
        raise RuntimeError(f"engine_jit kernel error: status={int(status)}")

    out: List[Fill] = []
    m = fills_arr.shape[0]
    for i in range(m):
        row = fills_arr[i]
        out.append(
            Fill(
                bar_index=int(row[0]),
                role=_role_from_int(int(row[1])),
                kind=_kind_from_int(int(row[2])),
                side=_side_from_int(int(row[3])),
                price=float(row[4]),
                qty=int(row[5]),
                order_id=int(row[6]),
            )
        )
    return out


# ----------------------------
# Numba Kernel
# ----------------------------

if nb is not None:

    @nb.njit(cache=False)
    def _stop_fill(side: int, stop_price: float, o: float, h: float, l: float) -> float:
        # returns nan if no fill
        if side == 1:  # BUY
            if o >= stop_price:
                return o
            if h >= stop_price:
                return stop_price
            return np.nan
        else:  # SELL
            if o <= stop_price:
                return o
            if l <= stop_price:
                return stop_price
            return np.nan

    @nb.njit(cache=False)
    def _limit_fill(side: int, limit_price: float, o: float, h: float, l: float) -> float:
        # returns nan if no fill
        if side == 1:  # BUY
            if o <= limit_price:
                return o
            if l <= limit_price:
                return limit_price
            return np.nan
        else:  # SELL
            if o >= limit_price:
                return o
            if h >= limit_price:
                return limit_price
            return np.nan

    @nb.njit(cache=False)
    def _fill_price(kind: int, side: int, px: float, o: float, h: float, l: float) -> float:
        # kind: 0=STOP, 1=LIMIT
        if kind == 0:
            return _stop_fill(side, px, o, h, l)
        return _limit_fill(side, px, o, h, l)

    @nb.njit(cache=False)
    def _simulate_kernel(
        open_: np.ndarray,
        high: np.ndarray,
        low: np.ndarray,
        order_id: np.ndarray,
        created_bar: np.ndarray,
        role: np.ndarray,
        kind: np.ndarray,
        side: np.ndarray,
        price: np.ndarray,
        qty: np.ndarray,
        ttl_bars: np.int64,
    ):
        """
        Cursor + Active Book kernel (O(B + I + A)).

        Output columns (float64):
          0 bar_index
          1 role_int (0=EXIT,1=ENTRY)
          2 kind_int (0=STOP,1=LIMIT)
          3 side_int (1=BUY,-1=SELL)
          4 fill_price
          5 qty
          6 order_id

        Assumption:
          - intents are sorted by (created_bar, order_id) before calling this kernel.
        """
        n_bars = open_.shape[0]
        n_intents = order_id.shape[0]

        max_fills = n_bars * 2
        out = np.empty((max_fills, 7), dtype=np.float64)
        out_n = 0

        # -------------------------
        # Fail-fast monotonicity check (activate_bar, order_id)
        # -------------------------
        prev_activate = np.int64(-1)
        prev_order = np.int64(-1)
        for i in range(n_intents):
            a = np.int64(created_bar[i]) + np.int64(1)
            o = np.int64(order_id[i])
            if a < prev_activate or (a == prev_activate and o < prev_order):
                return np.int64(STATUS_ERROR_UNSORTED), out[:0]
            prev_activate = a
            prev_order = o

        # Active Book (indices into intent arrays)
        active_indices = np.empty(n_intents, dtype=np.int64)
        active_count = np.int64(0)
        global_cursor = np.int64(0)

        pos = np.int64(0)  # 0 flat, 1 long, -1 short

        for t in range(n_bars):
            o = float(open_[t])
            h = float(high[t])
            l = float(low[t])

            # Step A â€” Injection (cursor inject intents activating at this bar)
            while global_cursor < n_intents:
                a = np.int64(created_bar[global_cursor]) + np.int64(1)
                if a == np.int64(t):
                    active_indices[active_count] = global_cursor
                    active_count += np.int64(1)
                    global_cursor += np.int64(1)
                    continue
                if a > np.int64(t):
                    break
                # a < t should not happen if monotonicity check passed
                return np.int64(STATUS_ERROR_UNSORTED), out[:0]

            # Step B â€” Pass 1 (ENTRY scan, best-pick, swap-remove)
            # Deterministic selection: STOP(0) before LIMIT(1), then order_id asc.
            if pos == 0 and active_count > 0:
                best_k = np.int64(-1)
                best_kind = np.int64(99)
                best_oid = np.int64(2**62)
                best_fp = np.nan

                k = np.int64(0)
                while k < active_count:
                    idx = active_indices[k]
                    if np.int64(role[idx]) != np.int64(1):  # ENTRY
                        k += np.int64(1)
                        continue

                    kk = np.int64(kind[idx])
                    oo = np.int64(order_id[idx])
                    if kk < best_kind or (kk == best_kind and oo < best_oid):
                        fp = _fill_price(int(kk), int(side[idx]), float(price[idx]), o, h, l)
                        if not np.isnan(fp):
                            best_k = k
                            best_kind = kk
                            best_oid = oo
                            best_fp = fp
                    k += np.int64(1)

                if best_k != np.int64(-1):
                    idx = active_indices[best_k]
                    out[out_n, 0] = float(t)
                    out[out_n, 1] = float(role[idx])
                    out[out_n, 2] = float(kind[idx])
                    out[out_n, 3] = float(side[idx])
                    out[out_n, 4] = float(best_fp)
                    out[out_n, 5] = float(qty[idx])
                    out[out_n, 6] = float(order_id[idx])
                    out_n += 1

                    pos = np.int64(1) if np.int64(side[idx]) == np.int64(1) else np.int64(-1)

                    # swap-remove filled intent
                    active_indices[best_k] = active_indices[active_count - 1]
                    active_count -= np.int64(1)

            # Step C â€” Pass 2 (EXIT scan, best-pick, swap-remove)
            # Deterministic selection: STOP(0) before LIMIT(1), then order_id asc.
            if pos != 0 and active_count > 0:
                best_k = np.int64(-1)
                best_kind = np.int64(99)
                best_oid = np.int64(2**62)
                best_fp = np.nan

                k = np.int64(0)
                while k < active_count:
                    idx = active_indices[k]
                    if np.int64(role[idx]) != np.int64(0):  # EXIT
                        k += np.int64(1)
                        continue

                    s = np.int64(side[idx])
                    # long exits are SELL(-1), short exits are BUY(1)
                    if pos == np.int64(1) and s != np.int64(-1):
                        k += np.int64(1)
                        continue
                    if pos == np.int64(-1) and s != np.int64(1):
                        k += np.int64(1)
                        continue

                    kk = np.int64(kind[idx])
                    oo = np.int64(order_id[idx])
                    if kk < best_kind or (kk == best_kind and oo < best_oid):
                        fp = _fill_price(int(kk), int(s), float(price[idx]), o, h, l)
                        if not np.isnan(fp):
                            best_k = k
                            best_kind = kk
                            best_oid = oo
                            best_fp = fp
                    k += np.int64(1)

                if best_k != np.int64(-1):
                    idx = active_indices[best_k]
                    out[out_n, 0] = float(t)
                    out[out_n, 1] = float(role[idx])
                    out[out_n, 2] = float(kind[idx])
                    out[out_n, 3] = float(side[idx])
                    out[out_n, 4] = float(best_fp)
                    out[out_n, 5] = float(qty[idx])
                    out[out_n, 6] = float(order_id[idx])
                    out_n += 1

                    pos = np.int64(0)

                    # swap-remove filled intent
                    active_indices[best_k] = active_indices[active_count - 1]
                    active_count -= np.int64(1)

            # Step D â€” Housekeeping (TTL/GTC extension point)
            if ttl_bars > np.int64(0) and active_count > 0:
                k = np.int64(0)
                while k < active_count:
                    idx = active_indices[k]
                    activate_bar = np.int64(created_bar[idx]) + np.int64(1)
                    expire_bar = activate_bar + (ttl_bars - np.int64(1))
                    if np.int64(t) > expire_bar:
                        active_indices[k] = active_indices[active_count - 1]
                        active_count -= np.int64(1)
                        continue
                    k += np.int64(1)

        return np.int64(STATUS_OK), out[:out_n]



================================================================================
FILE: src/FishBroWFS_V2/engine/matcher_core.py
================================================================================

from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable, List, Optional, Tuple

import numpy as np

from FishBroWFS_V2.engine.types import (
    BarArrays,
    Fill,
    OrderIntent,
    OrderKind,
    OrderRole,
    Side,
)


@dataclass
class PositionState:
    """
    Minimal single-position state for Phase 1 tests.
    pos: 0 = flat, 1 = long, -1 = short
    """
    pos: int = 0


def _is_active(intent: OrderIntent, bar_index: int) -> bool:
    return bar_index == intent.created_bar + 1


def _stop_fill_price(side: Side, stop_price: float, o: float, h: float, l: float) -> Optional[float]:
    # Open==price goes to GAP branch by definition.
    if side == Side.BUY:
        if o >= stop_price:
            return o
        if h >= stop_price:
            return stop_price
        return None
    else:
        if o <= stop_price:
            return o
        if l <= stop_price:
            return stop_price
        return None


def _limit_fill_price(side: Side, limit_price: float, o: float, h: float, l: float) -> Optional[float]:
    # Open==price goes to GAP branch by definition.
    if side == Side.BUY:
        if o <= limit_price:
            return o
        if l <= limit_price:
            return limit_price
        return None
    else:
        if o >= limit_price:
            return o
        if h >= limit_price:
            return limit_price
        return None


def _intent_fill_price(intent: OrderIntent, o: float, h: float, l: float) -> Optional[float]:
    if intent.kind == OrderKind.STOP:
        return _stop_fill_price(intent.side, intent.price, o, h, l)
    return _limit_fill_price(intent.side, intent.price, o, h, l)


def _sort_key(intent: OrderIntent) -> Tuple[int, int, int]:
    """
    Deterministic priority:
    1) Role: EXIT first when selecting within same-stage bucket.
    2) Kind: STOP before LIMIT.
    3) order_id: ascending.
    Note: Entry-vs-Exit ordering is handled at a higher level (Entry then Exit).
    """
    role_rank = 0 if intent.role == OrderRole.EXIT else 1
    kind_rank = 0 if intent.kind == OrderKind.STOP else 1
    return (role_rank, kind_rank, intent.order_id)


def simulate(
    bars: BarArrays,
    intents: Iterable[OrderIntent],
) -> List[Fill]:
    """
    Phase 1 slow reference matcher.

    Rules enforced:
    - next-bar active only (bar_index == created_bar + 1)
    - STOP/LIMIT gap behavior at Open
    - STOP over LIMIT
    - Same-bar Entry then Exit
    - Same-kind tie: EXIT-first, order_id ascending
    """
    o = bars.open
    h = bars.high
    l = bars.low
    n = int(o.shape[0])

    intents_list = list(intents)
    fills: List[Fill] = []
    state = PositionState(pos=0)

    for t in range(n):
        ot = float(o[t])
        ht = float(h[t])
        lt = float(l[t])

        active = [x for x in intents_list if _is_active(x, t)]
        if not active:
            continue

        # Partition by role for same-bar entry then exit.
        entry_intents = [x for x in active if x.role == OrderRole.ENTRY]
        exit_intents = [x for x in active if x.role == OrderRole.EXIT]

        # Stage 1: ENTRY stage
        if entry_intents:
            # Among entries: STOP before LIMIT, then order_id.
            entry_sorted = sorted(entry_intents, key=lambda x: (0 if x.kind == OrderKind.STOP else 1, x.order_id))
            for it in entry_sorted:
                if state.pos != 0:
                    break  # single-position only
                px = _intent_fill_price(it, ot, ht, lt)
                if px is None:
                    continue
                fills.append(
                    Fill(
                        bar_index=t,
                        role=it.role,
                        kind=it.kind,
                        side=it.side,
                        price=float(px),
                        qty=int(it.qty),
                        order_id=int(it.order_id),
                    )
                )
                # Apply position change
                if it.side == Side.BUY:
                    state.pos = 1
                else:
                    state.pos = -1
                break  # at most one entry fill per bar in Phase 1 reference

        # Stage 2: EXIT stage (after entry)
        if exit_intents and state.pos != 0:
            # Same-kind tie rule: EXIT-first already, and STOP before LIMIT, then order_id
            exit_sorted = sorted(exit_intents, key=_sort_key)
            for it in exit_sorted:
                # Only allow exits that reduce/close current position in this minimal model:
                # long exits are SELL, short exits are BUY.
                if state.pos == 1 and it.side != Side.SELL:
                    continue
                if state.pos == -1 and it.side != Side.BUY:
                    continue

                px = _intent_fill_price(it, ot, ht, lt)
                if px is None:
                    continue
                fills.append(
                    Fill(
                        bar_index=t,
                        role=it.role,
                        kind=it.kind,
                        side=it.side,
                        price=float(px),
                        qty=int(it.qty),
                        order_id=int(it.order_id),
                    )
                )
                state.pos = 0
                break  # at most one exit fill per bar in Phase 1 reference

    return fills



================================================================================
FILE: src/FishBroWFS_V2/engine/types.py
================================================================================

from __future__ import annotations

from dataclasses import dataclass
from enum import Enum
from typing import Optional

import numpy as np


@dataclass(frozen=True)
class BarArrays:
    open: np.ndarray
    high: np.ndarray
    low: np.ndarray
    close: np.ndarray


class Side(int, Enum):
    BUY = 1
    SELL = -1


class OrderKind(str, Enum):
    STOP = "STOP"
    LIMIT = "LIMIT"


class OrderRole(str, Enum):
    ENTRY = "ENTRY"
    EXIT = "EXIT"


@dataclass(frozen=True)
class OrderIntent:
    """
    Order intent created at bar `created_bar` and becomes active at bar `created_bar + 1`.
    Deterministic ordering is controlled via `order_id` (smaller = earlier).
    """
    order_id: int
    created_bar: int
    role: OrderRole
    kind: OrderKind
    side: Side
    price: float
    qty: int = 1


@dataclass(frozen=True)
class Fill:
    bar_index: int
    role: OrderRole
    kind: OrderKind
    side: Side
    price: float
    qty: int
    order_id: int



================================================================================
FILE: src/FishBroWFS_V2/indicators/__init__.py
================================================================================




================================================================================
FILE: src/FishBroWFS_V2/indicators/numba_indicators.py
================================================================================

from __future__ import annotations

import numpy as np

try:
    import numba as nb
except Exception:  # pragma: no cover
    nb = None  # type: ignore


# ----------------------------
# Rolling Max / Min
# ----------------------------
# Design choice (v1):
# - Simple loop scan for window <= ~50 is cache-friendly and predictable.
# - Correctness first; no deque optimization in v1.


if nb is not None:

    @nb.njit(cache=False)
    def rolling_max(arr: np.ndarray, window: int) -> np.ndarray:
        n = arr.shape[0]
        out = np.full(n, np.nan, dtype=np.float64)
        if window <= 0:
            return out
        for i in range(n):
            start = i - window + 1
            if start < 0:
                start = 0
            m = arr[start]
            for j in range(start + 1, i + 1):
                v = arr[j]
                if v > m:
                    m = v
            out[i] = m
        return out

    @nb.njit(cache=False)
    def rolling_min(arr: np.ndarray, window: int) -> np.ndarray:
        n = arr.shape[0]
        out = np.full(n, np.nan, dtype=np.float64)
        if window <= 0:
            return out
        for i in range(n):
            start = i - window + 1
            if start < 0:
                start = 0
            m = arr[start]
            for j in range(start + 1, i + 1):
                v = arr[j]
                if v < m:
                    m = v
            out[i] = m
        return out

else:
    # Fallback pure-python (used only if numba unavailable)
    def rolling_max(arr: np.ndarray, window: int) -> np.ndarray:  # type: ignore
        n = arr.shape[0]
        out = np.full(n, np.nan, dtype=np.float64)
        if window <= 0:
            return out
        for i in range(n):
            start = max(0, i - window + 1)
            out[i] = np.max(arr[start : i + 1])
        return out

    def rolling_min(arr: np.ndarray, window: int) -> np.ndarray:  # type: ignore
        n = arr.shape[0]
        out = np.full(n, np.nan, dtype=np.float64)
        if window <= 0:
            return out
        for i in range(n):
            start = max(0, i - window + 1)
            out[i] = np.min(arr[start : i + 1])
        return out


# ----------------------------
# ATR (Wilder's RMA)
# ----------------------------
# Definition:
# TR[t] = max(high[t]-low[t], abs(high[t]-close[t-1]), abs(low[t]-close[t-1]))
# ATR[t] = (ATR[t-1]*(n-1) + TR[t]) / n
# Notes:
# - Recursive; must keep state.
# - First ATR uses simple average of first n TRs.


if nb is not None:

    @nb.njit(cache=False)
    def atr_wilder(high: np.ndarray, low: np.ndarray, close: np.ndarray, window: int) -> np.ndarray:
        n = high.shape[0]
        out = np.full(n, np.nan, dtype=np.float64)
        if window <= 0 or n == 0:
            return out
        if window > n:
            return out

        # TR computation
        tr = np.empty(n, dtype=np.float64)
        tr[0] = high[0] - low[0]
        for i in range(1, n):
            a = high[i] - low[i]
            b = abs(high[i] - close[i - 1])
            c = abs(low[i] - close[i - 1])
            tr[i] = a if a >= b and a >= c else (b if b >= c else c)

        # initial ATR: simple average of first window TRs
        s = 0.0
        end = window if window < n else n
        for i in range(end):
            s += tr[i]
        # here window <= n guaranteed
        out[end - 1] = s / window

        # Wilder smoothing
        for i in range(window, n):
            out[i] = (out[i - 1] * (window - 1) + tr[i]) / window

        return out

else:
    def atr_wilder(high: np.ndarray, low: np.ndarray, close: np.ndarray, window: int) -> np.ndarray:  # type: ignore
        n = high.shape[0]
        out = np.full(n, np.nan, dtype=np.float64)
        if window <= 0 or n == 0:
            return out
        if window > n:
            return out

        tr = np.empty(n, dtype=np.float64)
        tr[0] = high[0] - low[0]
        for i in range(1, n):
            tr[i] = max(
                high[i] - low[i],
                abs(high[i] - close[i - 1]),
                abs(low[i] - close[i - 1]),
            )

        end = min(window, n)
        # window <= n guaranteed
        out[end - 1] = np.mean(tr[:end])
        for i in range(window, n):
            out[i] = (out[i - 1] * (window - 1) + tr[i]) / window
        return out



================================================================================
FILE: src/FishBroWFS_V2/perf/__init__.py
================================================================================

"""
Performance profiling utilities.
"""


================================================================================
FILE: src/FishBroWFS_V2/perf/profile_report.py
================================================================================

from __future__ import annotations

import cProfile
import io
import os
import pstats


def _format_profile_report(
    lane_id: str,
    n_bars: int,
    n_params: int,
    jit_enabled: bool,
    sort_params: bool,
    topn: int,
    mode: str,
    pr: cProfile.Profile,
) -> str:
    """
    Format a deterministic profile report string for perf harness.

    Contract:
    - Always includes __PROFILE_START__/__PROFILE_END__ markers.
    - Always includes the 'pstats sort: cumtime' header even if no stats exist.
    - Must not throw when the profile has no collected stats (empty Profile).
    """
    s = io.StringIO()
    s.write("__PROFILE_START__\n")
    s.write(f"lane_id={lane_id}\n")
    s.write(f"bars={n_bars} params={n_params}\n")
    s.write(f"jit_enabled={jit_enabled} sort_params={sort_params}\n")
    s.write(f"pid={os.getpid()}\n")
    if mode is not None:
        s.write(f"mode={mode}\n")
    s.write("\n")

    # Always emit the headers so tests can rely on markers/labels.
    s.write(f"== pstats sort: cumtime (top {topn}) ==\n")
    try:
        ps = pstats.Stats(pr, stream=s).strip_dirs()
        ps.sort_stats("cumtime")
        ps.print_stats(topn)
    except TypeError:
        s.write("(no profile stats collected)\n")

    s.write("\n\n")
    s.write(f"== pstats sort: tottime (top {topn}) ==\n")
    try:
        ps = pstats.Stats(pr, stream=s).strip_dirs()
        ps.sort_stats("tottime")
        ps.print_stats(topn)
    except TypeError:
        s.write("(no profile stats collected)\n")

    s.write("\n\n__PROFILE_END__\n")
    return s.getvalue()


================================================================================
FILE: src/FishBroWFS_V2/pipeline/__init__.py
================================================================================




================================================================================
FILE: src/FishBroWFS_V2/pipeline/metrics_schema.py
================================================================================

from __future__ import annotations

"""
Metrics column schema (single source of truth).

Defines the column order for metrics arrays returned by run_grid().
"""

# Column indices for metrics array (n_params, 3)
METRICS_COL_NET_PROFIT = 0
METRICS_COL_TRADES = 1
METRICS_COL_MAX_DD = 2

# Column names (for documentation/debugging)
METRICS_COLUMN_NAMES = ["net_profit", "trades", "max_dd"]

# Number of columns
METRICS_N_COLUMNS = 3


================================================================================
FILE: src/FishBroWFS_V2/pipeline/param_sort.py
================================================================================

from __future__ import annotations

import numpy as np


def sort_params_cache_friendly(params: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    """
    Cache-friendly sorting for parameter matrix.

    params: shape (n, k) float64.
      Convention (Phase 3B v1):
        col0 = channel_len
        col1 = atr_len
        col2 = stop_mult

    Returns:
      sorted_params: params reordered (view/copy depending on numpy)
      order: indices such that sorted_params = params[order]
    """
    if params.ndim != 2 or params.shape[1] < 3:
        raise ValueError("params must be (n, >=3) array")

    # Primary: channel_len (int-like)
    # Secondary: atr_len (int-like)
    # Tertiary: stop_mult
    ch = params[:, 0]
    atr = params[:, 1]
    sm = params[:, 2]

    order = np.lexsort((sm, atr, ch))
    return params[order], order



================================================================================
FILE: src/FishBroWFS_V2/pipeline/runner_grid.py
================================================================================

from __future__ import annotations

from typing import Dict, Tuple

import numpy as np
import os
import time

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.types import BarArrays, Fill, OrderIntent, OrderKind, OrderRole, Side
from FishBroWFS_V2.pipeline.metrics_schema import (
    METRICS_COL_MAX_DD,
    METRICS_COL_NET_PROFIT,
    METRICS_COL_TRADES,
    METRICS_N_COLUMNS,
)
from FishBroWFS_V2.pipeline.param_sort import sort_params_cache_friendly
from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, run_kernel


def _max_drawdown(equity: np.ndarray) -> float:
    """
    Vectorized max drawdown on an equity curve.
    Handles empty arrays gracefully.
    """
    if equity.size == 0:
        return 0.0
    peak = np.maximum.accumulate(equity)
    dd = equity - peak
    mdd = float(np.min(dd))  # negative or 0
    return mdd


def _ensure_contiguous_bars(bars: BarArrays) -> BarArrays:
    if bars.open.flags["C_CONTIGUOUS"] and bars.high.flags["C_CONTIGUOUS"] and bars.low.flags["C_CONTIGUOUS"] and bars.close.flags["C_CONTIGUOUS"]:
        return bars
    return BarArrays(
        open=np.ascontiguousarray(bars.open, dtype=np.float64),
        high=np.ascontiguousarray(bars.high, dtype=np.float64),
        low=np.ascontiguousarray(bars.low, dtype=np.float64),
        close=np.ascontiguousarray(bars.close, dtype=np.float64),
    )


def run_grid(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
    sort_params: bool = True,
    force_close_last: bool = False,
    return_debug: bool = False,
) -> Dict[str, object]:
    """
    Phase 3B v1: Dynamic Grid Runner (homology locked).

    params_matrix: shape (n, >=3) float64
      col0 channel_len (int-like)
      col1 atr_len (int-like)
      col2 stop_mult (float)

    Args:
        force_close_last: If True, force close any open positions at the last bar
            using close[-1] as exit price. This ensures trades > 0 when fills exist.

    Returns:
      dict with:
        - metrics: np.ndarray shape (n, 3) float64 columns:
            [net_profit, trades, max_dd] (see pipeline.metrics_schema for column indices)
        - order: np.ndarray indices mapping output rows back to original params (or identity)
    """
    profile = os.environ.get("FISHBRO_PROFILE_GRID", "").strip() == "1"
    sim_only = os.environ.get("FISHBRO_PERF_SIM_ONLY", "").strip() == "1"
    t0 = time.perf_counter()

    bars = _ensure_contiguous_bars(normalize_bars(open_, high, low, close))
    t_prep1 = time.perf_counter()

    if params_matrix.ndim != 2 or params_matrix.shape[1] < 3:
        raise ValueError("params_matrix must be (n, >=3)")

    pm = np.asarray(params_matrix, dtype=np.float64)
    if sort_params:
        pm_sorted, order = sort_params_cache_friendly(pm)
    else:
        pm_sorted = pm
        order = np.arange(pm.shape[0], dtype=np.int64)
    t_sort = time.perf_counter()

    n = pm_sorted.shape[0]
    metrics = np.zeros((n, METRICS_N_COLUMNS), dtype=np.float64)
    
    # Debug arrays: per-param first trade snapshot (only if return_debug=True)
    if return_debug:
        debug_fills_first = np.full((n, 6), np.nan, dtype=np.float64)
        # Columns: entry_bar, entry_price, exit_bar, exit_price, net_profit, trades
    else:
        debug_fills_first = None

    # Initialize result dict early (minimal structure)
    perf: Dict[str, object] = {}
    result: Dict[str, object] = {"metrics": metrics, "order": order, "perf": perf}

    if sim_only:
        # Debug mode: bypass strategy/orchestration and only benchmark matcher simulate.
        # This provides A/B evidence: if sim-only is fast, bottleneck is in kernel (indicators/intents).
        from FishBroWFS_V2.engine import engine_jit

        intents_per_bar = int(os.environ.get("FISHBRO_SIM_ONLY_INTENTS_PER_BAR", "2"))
        intents: list[OrderIntent] = []
        oid = 1
        nbars = int(bars.open.shape[0])
        for t in range(1, nbars):
            for _ in range(intents_per_bar):
                intents.append(
                    OrderIntent(
                        order_id=oid,
                        created_bar=t - 1,
                        role=OrderRole.ENTRY,
                        kind=OrderKind.STOP,
                        side=Side.BUY,
                        price=float(bars.high[t - 1]),
                        qty=1,
                    )
                )
                oid += 1
                intents.append(
                    OrderIntent(
                        order_id=oid,
                        created_bar=t - 1,
                        role=OrderRole.EXIT,
                        kind=OrderKind.STOP,
                        side=Side.SELL,
                        price=float(bars.low[t - 1]),
                        qty=1,
                    )
                )
                oid += 1

        t_sim0 = time.perf_counter()
        _fills = engine_jit.simulate(bars, intents)
        t_sim1 = time.perf_counter()
        jt = engine_jit.get_jit_truth()
        numba_env = os.environ.get("NUMBA_DISABLE_JIT", "")
        sigs = jt.get("kernel_signatures") or []
        perf = {
            "t_features": float(t_prep1 - t0),
            "t_indicators": None,
            "t_intent_gen": None,
            "t_simulate": float(t_sim1 - t_sim0),
            "simulate_impl": "jit" if jt.get("jit_path_used") else "py",
            "jit_path_used": bool(jt.get("jit_path_used")),
            "simulate_signatures_count": int(len(sigs)),
            "numba_disable_jit_env": str(numba_env),
            "intents_total": int(len(intents)),
            "intents_per_bar_avg": float(len(intents) / float(max(1, bars.open.shape[0]))),
            "fills_total": int(len(_fills)),
            "intent_mode": "objects",
        }
        result["perf"] = perf
        if return_debug and debug_fills_first is not None:
            result["debug_fills_first"] = debug_fills_first
        return result

    # Homology: only call run_kernel, never compute strategy/metrics here.
    # Perf observability is env-gated so default usage stays unchanged.
    t_ind = 0.0
    t_intgen = 0.0
    t_sim = 0.0
    intents_total = 0
    fills_total = 0
    any_profile_missing = False
    intent_mode: str | None = None
    for i in range(n):
        ch = int(pm_sorted[i, 0])
        atr = int(pm_sorted[i, 1])
        sm = float(pm_sorted[i, 2])

        if profile:
            # enable kernel profiling (cheap, coarse) so we can attribute time and counts
            os.environ["FISHBRO_PROFILE_KERNEL"] = "1"
        out = run_kernel(
            bars,
            DonchianAtrParams(channel_len=ch, atr_len=atr, stop_mult=sm),
            commission=float(commission),
            slip=float(slip),
            order_qty=int(order_qty),
            return_debug=return_debug,
        )
        obs = out.get("_obs", None)  # type: ignore
        if isinstance(obs, dict):
            # Phase 3.0-B: Trust kernel's evidence fields, do not recompute
            if intent_mode is None and isinstance(obs.get("intent_mode"), str):
                intent_mode = str(obs.get("intent_mode"))
            # Use intents_total directly from kernel (Source of Truth), not recompute from entry+exit
            intents_total += int(obs.get("intents_total", 0))
            fills_total += int(obs.get("fills_total", 0))

        if profile:
            kp = out.get("_profile", None)  # type: ignore
            if not isinstance(kp, dict):
                any_profile_missing = True
                continue
            t_ind += float(kp.get("indicators_s", 0.0))
            # include both entry+exit intent generation as "intent generation"
            t_intgen += float(kp.get("intent_gen_s", 0.0)) + float(kp.get("exit_intent_gen_s", 0.0))
            t_sim += float(kp.get("simulate_entry_s", 0.0)) + float(kp.get("simulate_exit_s", 0.0))

        m = out["metrics"]
        
        # Collect debug data if requested
        if return_debug:
            debug_info = out.get("_debug", {})
            entry_bar = debug_info.get("entry_bar", -1)
            entry_price = debug_info.get("entry_price", np.nan)
            exit_bar = debug_info.get("exit_bar", -1)
            exit_price = debug_info.get("exit_price", np.nan)
            
            # Handle force_close_last: update exit_bar/exit_price if forced close
            # (will be updated below if force_close_last triggers)
        
        # Handle force_close_last: if still in position, force close at last bar
        if force_close_last:
            fills = out.get("fills", [])
            if isinstance(fills, list) and len(fills) > 0:
                # Count entry and exit fills
                entry_fills = [f for f in fills if f.role == OrderRole.ENTRY and f.side == Side.BUY]
                exit_fills = [f for f in fills if f.role == OrderRole.EXIT and f.side == Side.SELL]
                
                # If there are unpaired entries, force close at last bar
                if len(entry_fills) > len(exit_fills):
                    n_unpaired = len(entry_fills) - len(exit_fills)
                    last_bar_idx = int(bars.open.shape[0] - 1)
                    last_close_price = float(bars.close[last_bar_idx])
                    
                    # Create forced exit fills for unpaired entries
                    # Use entry prices from the unpaired entries
                    unpaired_entry_prices = [float(f.price) for f in entry_fills[-n_unpaired:]]
                    
                    # Calculate additional pnl from forced closes
                    forced_pnl = []
                    costs_per_trade = (float(commission) + float(slip)) * 2.0
                    for entry_price in unpaired_entry_prices:
                        # PnL = (exit_price - entry_price) * qty - costs
                        trade_pnl = (last_close_price - entry_price) * float(order_qty) - costs_per_trade
                        forced_pnl.append(trade_pnl)
                    
                    # Update metrics with forced closes
                    original_net_profit = float(m["net_profit"])
                    original_trades = int(m["trades"])
                    
                    # Add forced close trades
                    new_net_profit = original_net_profit + sum(forced_pnl)
                    new_trades = original_trades + n_unpaired
                    
                    # Update debug exit info for force_close_last
                    if return_debug and n_unpaired > 0:
                        exit_bar = last_bar_idx
                        exit_price = last_close_price
                    
                    # Recalculate equity and max_dd
                    forced_pnl_arr = np.asarray(forced_pnl, dtype=np.float64)
                    if original_trades > 0 and "equity" in out:
                        original_equity = out["equity"]
                        if isinstance(original_equity, np.ndarray) and original_equity.size > 0:
                            # Append forced pnl to existing equity curve
                            # Start from last equity value
                            start_equity = float(original_equity[-1])
                            forced_equity = np.cumsum(forced_pnl_arr) + start_equity
                            new_equity = np.concatenate([original_equity, forced_equity])
                        else:
                            # No previous equity array, start from 0
                            new_equity = np.cumsum(forced_pnl_arr)
                    else:
                        # No previous trades, start from 0
                        new_equity = np.cumsum(forced_pnl_arr)
                    
                    new_max_dd = _max_drawdown(new_equity)
                    
                    metrics[i, METRICS_COL_NET_PROFIT] = new_net_profit
                    metrics[i, METRICS_COL_TRADES] = new_trades
                    metrics[i, METRICS_COL_MAX_DD] = new_max_dd
                    
                    # Update debug with final metrics after force_close_last
                    if return_debug:
                        debug_fills_first[i, 0] = entry_bar
                        debug_fills_first[i, 1] = entry_price
                        debug_fills_first[i, 2] = exit_bar
                        debug_fills_first[i, 3] = exit_price
                        debug_fills_first[i, 4] = new_net_profit
                        debug_fills_first[i, 5] = float(new_trades)
                else:
                    # No unpaired entries, use original metrics
                    metrics[i, METRICS_COL_NET_PROFIT] = float(m["net_profit"])
                    metrics[i, METRICS_COL_TRADES] = float(m["trades"])
                    metrics[i, METRICS_COL_MAX_DD] = float(m["max_dd"])
                    
                    # Store debug data
                    if return_debug:
                        debug_fills_first[i, 0] = entry_bar
                        debug_fills_first[i, 1] = entry_price
                        debug_fills_first[i, 2] = exit_bar
                        debug_fills_first[i, 3] = exit_price
                        debug_fills_first[i, 4] = float(m["net_profit"])
                        debug_fills_first[i, 5] = float(m["trades"])
            else:
                # No fills, use original metrics
                metrics[i, METRICS_COL_NET_PROFIT] = float(m["net_profit"])
                metrics[i, METRICS_COL_TRADES] = int(m["trades"])
                metrics[i, METRICS_COL_MAX_DD] = float(m["max_dd"])
                
                # Store debug data (no fills case)
                if return_debug:
                    debug_fills_first[i, 0] = entry_bar
                    debug_fills_first[i, 1] = entry_price
                    debug_fills_first[i, 2] = exit_bar
                    debug_fills_first[i, 3] = exit_price
                    debug_fills_first[i, 4] = float(m["net_profit"])
                    debug_fills_first[i, 5] = float(m["trades"])
        else:
            # Zero-trade safe: kernel guarantees valid numbers (0.0/0)
            metrics[i, METRICS_COL_NET_PROFIT] = float(m["net_profit"])
            metrics[i, METRICS_COL_TRADES] = int(m["trades"])
            metrics[i, METRICS_COL_MAX_DD] = float(m["max_dd"])
            
            # Store debug data
            if return_debug:
                debug_fills_first[i, 0] = entry_bar
                debug_fills_first[i, 1] = entry_price
                debug_fills_first[i, 2] = exit_bar
                debug_fills_first[i, 3] = exit_price
                debug_fills_first[i, 4] = float(m["net_profit"])
                debug_fills_first[i, 5] = float(m["trades"])

    # Phase 3.0-E: Ensure intent_mode is never None
    # If no kernel results (n == 0), default to "arrays" (default kernel path)
    # Otherwise, intent_mode should have been set from first kernel result
    if intent_mode is None:
        # Edge case: n == 0 (no params) - use default "arrays" since run_kernel defaults to array path
        intent_mode = "arrays"

    if not profile:
        # Return minimal perf with evidence fields only
        perf = {
            "intent_mode": intent_mode,
            "intents_total": int(intents_total),
            "fills_total": int(fills_total),
        }
        result["perf"] = perf
        if return_debug and debug_fills_first is not None:
            result["debug_fills_first"] = debug_fills_first
        return result

    from FishBroWFS_V2.engine import engine_jit

    jt = engine_jit.get_jit_truth()
    numba_env = os.environ.get("NUMBA_DISABLE_JIT", "")
    sigs = jt.get("kernel_signatures") or []

    # Best-effort: avoid leaking this env to callers
    try:
        del os.environ["FISHBRO_PROFILE_KERNEL"]
    except KeyError:
        pass

    # Phase 3.0-E: Ensure intent_mode is never None
    # If no kernel results (n == 0), default to "arrays" (default kernel path)
    # Otherwise, intent_mode should have been set from first kernel result
    if intent_mode is None:
        # Edge case: n == 0 (no params) - use default "arrays" since run_kernel defaults to array path
        intent_mode = "arrays"

    perf = {
        "t_features": float(t_prep1 - t0),
        # current architecture: indicators are computed inside run_kernel per param
        "t_indicators": None if any_profile_missing else float(t_ind),
        "t_intent_gen": None if any_profile_missing else float(t_intgen),
        "t_simulate": None if any_profile_missing else float(t_sim),
        "simulate_impl": "jit" if jt.get("jit_path_used") else "py",
        "jit_path_used": bool(jt.get("jit_path_used")),
        "simulate_signatures_count": int(len(sigs)),
        "numba_disable_jit_env": str(numba_env),
        # Phase 3.0-B: Use kernel's evidence fields directly (Source of Truth), not recomputed
        "intent_mode": intent_mode,
        "intents_total": int(intents_total),
        "fills_total": int(fills_total),
        "intents_per_bar_avg": float(intents_total / float(max(1, bars.open.shape[0]))),
    }

    result["perf"] = perf
    if return_debug and debug_fills_first is not None:
        result["debug_fills_first"] = debug_fills_first
    return result



================================================================================
FILE: src/FishBroWFS_V2/stage0/__init__.py
================================================================================

"""
Stage 0 Funnel (Vector/Proxy Filter)

Design goal:
  - Extremely cheap scoring/ranking for massive parameter grids.
  - No matcher, no orders, no fills, no state machine.
  - Must be vectorizable / nopython friendly.
"""

from .ma_proxy import stage0_score_ma_proxy
from .proxies import trend_proxy, vol_proxy, activity_proxy




================================================================================
FILE: src/FishBroWFS_V2/stage0/ma_proxy.py
================================================================================

from __future__ import annotations

"""
Stage 0 v0: MA Directional Efficiency Proxy

This module intentionally does NOT depend on:
  - engine/* (matcher, fills, intents)
  - strategy/kernel
  - pipeline/runner_grid

It is a cheap scoring function to rank massive parameter grids before Stage 2.

Proxy idea (directional efficiency):
  dir[t] = sign(SMA_fast[t] - SMA_slow[t])
  ret[t] = close[t] - close[t-1]
  score = sum(dir[t] * ret[t]) / (std(ret) + eps)

Notes:
  - This is NOT a backtest. No orders, no fills, no costs.
  - Recall > precision. False negatives are acceptable at Stage 0.
"""

from typing import Tuple

import numpy as np
import os

try:
    import numba as nb
except Exception:  # pragma: no cover
    nb = None  # type: ignore


def _validate_inputs(close: np.ndarray, params_matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    c = np.asarray(close, dtype=np.float64)
    if c.ndim != 1:
        raise ValueError("close must be 1D")
    pm = np.asarray(params_matrix, dtype=np.float64)
    if pm.ndim != 2:
        raise ValueError("params_matrix must be 2D")
    if pm.shape[1] < 2:
        raise ValueError("params_matrix must have at least 2 columns: fast, slow")
    if c.shape[0] < 3:
        raise ValueError("close must have at least 3 bars for Stage0 scoring")
    if not c.flags["C_CONTIGUOUS"]:
        c = np.ascontiguousarray(c)
    if not pm.flags["C_CONTIGUOUS"]:
        pm = np.ascontiguousarray(pm)
    return c, pm


def stage0_score_ma_proxy(close: np.ndarray, params_matrix: np.ndarray) -> np.ndarray:
    """
    Compute Stage 0 proxy scores for a parameter matrix.

    Args:
        close: float64 1D array (n_bars,)
        params_matrix: float64 2D array (n_params, >=2)
            - col0: fast_len
            - col1: slow_len
            - additional columns allowed and ignored by v0

    Returns:
        scores: float64 1D array (n_params,) where higher is better
    """
    c, pm = _validate_inputs(close, params_matrix)

    # If numba is available and JIT is not disabled, use nopython kernel.
    if nb is not None and os.environ.get("NUMBA_DISABLE_JIT", "").strip() != "1":
        return _stage0_kernel(c, pm)

    # Fallback: pure numpy/python (correctness only, not intended for scale).
    ret = c[1:] - c[:-1]
    denom = np.std(ret) + 1e-12
    scores = np.empty(pm.shape[0], dtype=np.float64)
    for i in range(pm.shape[0]):
        fast = int(pm[i, 0])
        slow = int(pm[i, 1])
        if fast <= 0 or slow <= 0 or fast >= c.shape[0] or slow >= c.shape[0]:
            scores[i] = -np.inf
            continue
        f = _sma_py(c, fast)
        s = _sma_py(c, slow)
        # Skip NaN warmup region: SMA length L is valid from index (L-1) onward.
        # Here we conservatively start at max(fast, slow) to ensure both are non-NaN.
        start = max(fast, slow)
        acc = 0.0
        for t in range(start, c.shape[0]):
            d = np.sign(f[t] - s[t])
            acc += d * ret[t - 1]
        scores[i] = acc / denom
    return scores


def _sma_py(x: np.ndarray, length: int) -> np.ndarray:
    n = x.shape[0]
    out = np.full(n, np.nan, dtype=np.float64)
    if length <= 0:
        return out
    csum = np.cumsum(x, dtype=np.float64)
    for i in range(n):
        j = i - length + 1
        if j < 0:
            continue
        total = csum[i] - (csum[j - 1] if j > 0 else 0.0)
        out[i] = total / float(length)
    return out


if nb is not None:

    @nb.njit(cache=False)
    def _sma_nb(x: np.ndarray, length: int) -> np.ndarray:
        n = x.shape[0]
        out = np.empty(n, dtype=np.float64)
        for i in range(n):
            out[i] = np.nan
        if length <= 0:
            return out
        csum = np.empty(n, dtype=np.float64)
        acc = 0.0
        for i in range(n):
            acc += float(x[i])
            csum[i] = acc
        for i in range(n):
            j = i - length + 1
            if j < 0:
                continue
            total = csum[i] - (csum[j - 1] if j > 0 else 0.0)
            out[i] = total / float(length)
        return out

    @nb.njit(cache=False)
    def _sign_nb(v: float) -> float:
        if v > 0.0:
            return 1.0
        if v < 0.0:
            return -1.0
        return 0.0

    @nb.njit(cache=False)
    def _std_nb(x: np.ndarray) -> float:
        # simple two-pass std for stability
        n = x.shape[0]
        if n <= 1:
            return 0.0
        mu = 0.0
        for i in range(n):
            mu += float(x[i])
        mu /= float(n)
        var = 0.0
        for i in range(n):
            d = float(x[i]) - mu
            var += d * d
        var /= float(n)
        return np.sqrt(var)

    @nb.njit(cache=False)
    def _stage0_kernel(close: np.ndarray, params_matrix: np.ndarray) -> np.ndarray:
        n = close.shape[0]
        n_params = params_matrix.shape[0]

        # ret[t] = close[t] - close[t-1] for t in [1..n-1]
        ret = np.empty(n - 1, dtype=np.float64)
        for t in range(1, n):
            ret[t - 1] = float(close[t]) - float(close[t - 1])

        denom = _std_nb(ret) + 1e-12
        scores = np.empty(n_params, dtype=np.float64)

        for i in range(n_params):
            fast = int(params_matrix[i, 0])
            slow = int(params_matrix[i, 1])

            # invalid lengths => hard reject
            if fast <= 0 or slow <= 0 or fast >= n or slow >= n:
                scores[i] = -np.inf
                continue

            f = _sma_nb(close, fast)
            s = _sma_nb(close, slow)

            start = fast if fast > slow else slow
            acc = 0.0
            for t in range(start, n):
                d = _sign_nb(f[t] - s[t])
                acc += d * ret[t - 1]

            scores[i] = acc / denom

        return scores




================================================================================
FILE: src/FishBroWFS_V2/stage0/proxies.py
================================================================================

from __future__ import annotations

"""
Stage 0 v1 Trinity: Trend + Volatility + Activity Proxies

This module provides three proxy scoring functions for ranking parameter grids
before full backtest (Stage 2). These are NOT backtests - they are cheap heuristics.

Proxy Contract:
  - Stage0 is ranking proxy, NOT equal to backtest
  - NaN/warmup rules: start = max(required_lookbacks)
  - Correlation contract: Spearman Ï â‰¥ 0.4 (enforced by tests)

Design:
  - All proxies return float64 (n_params,) scores where higher is better
  - Input: OHLC arrays (np.ndarray), params: float64 2D array (n_params, k)
  - Must provide *_py (pure Python) and *_nb (Numba njit) versions
  - Wrapper functions select nb/py based on NUMBA_DISABLE_JIT kill-switch
"""

from typing import Tuple

import numpy as np
import os

try:
    import numba as nb
except Exception:  # pragma: no cover
    nb = None  # type: ignore

from FishBroWFS_V2.indicators.numba_indicators import atr_wilder


def _validate_inputs(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Validate and ensure contiguous arrays."""
    o = np.asarray(open_, dtype=np.float64)
    h = np.asarray(high, dtype=np.float64)
    l = np.asarray(low, dtype=np.float64)
    c = np.asarray(close, dtype=np.float64)
    pm = np.asarray(params_matrix, dtype=np.float64)

    if o.ndim != 1 or h.ndim != 1 or l.ndim != 1 or c.ndim != 1:
        raise ValueError("OHLC arrays must be 1D")
    if pm.ndim != 2:
        raise ValueError("params_matrix must be 2D")
    if not (o.shape[0] == h.shape[0] == l.shape[0] == c.shape[0]):
        raise ValueError("OHLC arrays must have same length")

    if not o.flags["C_CONTIGUOUS"]:
        o = np.ascontiguousarray(o)
    if not h.flags["C_CONTIGUOUS"]:
        h = np.ascontiguousarray(h)
    if not l.flags["C_CONTIGUOUS"]:
        l = np.ascontiguousarray(l)
    if not c.flags["C_CONTIGUOUS"]:
        c = np.ascontiguousarray(c)
    if not pm.flags["C_CONTIGUOUS"]:
        pm = np.ascontiguousarray(pm)

    return o, h, l, c, pm


# ============================================================================
# Proxy #1: Trend Proxy (MA / slope)
# ============================================================================


def trend_proxy_py(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """
    Trend proxy: mean(sign(sma_fast - sma_slow)) or mean((sma_fast - sma_slow) / close)

    Args:
        open_, high, low, close: float64 1D arrays (n_bars,)
        params_matrix: float64 2D array (n_params, >=2)
            - col0: fast_len
            - col1: slow_len

    Returns:
        scores: float64 1D array (n_params,)
    """
    o, h, l, c, pm = _validate_inputs(open_, high, low, close, params_matrix)
    n = c.shape[0]
    n_params = pm.shape[0]

    if pm.shape[1] < 2:
        raise ValueError("params_matrix must have at least 2 columns: fast_len, slow_len")

    scores = np.empty(n_params, dtype=np.float64)

    for i in range(n_params):
        fast = int(pm[i, 0])
        slow = int(pm[i, 1])

        # Invalid params: return -inf
        if fast <= 0 or slow <= 0 or fast >= n or slow >= n:
            scores[i] = -np.inf
            continue

        # Compute SMAs
        sma_fast = _sma_py(c, fast)
        sma_slow = _sma_py(c, slow)

        # Warmup: start at max(fast, slow)
        start = max(fast, slow)
        if start >= n:
            scores[i] = -np.inf
            continue

        # Compute trend score: mean((sma_fast - sma_slow) / close)
        acc = 0.0
        count = 0
        for t in range(start, n):
            diff = sma_fast[t] - sma_slow[t]
            if not np.isnan(diff) and c[t] > 0:
                acc += diff / c[t]
                count += 1

        if count == 0:
            scores[i] = -np.inf
        else:
            scores[i] = acc / count

    return scores


def trend_proxy_nb(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """Numba version of trend_proxy."""
    if nb is None:  # pragma: no cover
        raise RuntimeError("numba not available")
    return _trend_proxy_kernel(open_, high, low, close, params_matrix)


def trend_proxy(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """Wrapper: select nb/py based on NUMBA_DISABLE_JIT."""
    if nb is not None and os.environ.get("NUMBA_DISABLE_JIT", "").strip() != "1":
        return trend_proxy_nb(open_, high, low, close, params_matrix)
    return trend_proxy_py(open_, high, low, close, params_matrix)


# ============================================================================
# Proxy #2: Volatility Proxy (ATR / Range)
# ============================================================================


def vol_proxy_py(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """
    Volatility proxy: effective stop distance = ATR(atr_len) * stop_mult.
    
    Score prefers moderate stop distance (avoids extremely tiny or huge stops).

    Args:
        open_, high, low, close: float64 1D arrays (n_bars,)
        params_matrix: float64 2D array (n_params, >=2)
            - col0: atr_len
            - col1: stop_mult

    Returns:
        scores: float64 1D array (n_params,)
    """
    o, h, l, c, pm = _validate_inputs(open_, high, low, close, params_matrix)
    n = c.shape[0]
    n_params = pm.shape[0]

    if pm.shape[1] < 2:
        raise ValueError("params_matrix must have at least 2 columns: atr_len, stop_mult")

    scores = np.empty(n_params, dtype=np.float64)

    for i in range(n_params):
        atr_len = int(pm[i, 0])
        stop_mult = float(pm[i, 1])

        # Invalid params: return -inf
        if atr_len <= 0 or atr_len >= n or stop_mult <= 0.0:
            scores[i] = -np.inf
            continue

        # Compute ATR using Wilder's method
        atr = atr_wilder(h, l, c, atr_len)

        # Warmup: start at atr_len
        start = max(atr_len, 1)
        if start >= n:
            scores[i] = -np.inf
            continue

        # Compute stop distance: ATR * stop_mult
        stop_dist_sum = 0.0
        stop_dist_count = 0
        for t in range(start, n):
            if not np.isnan(atr[t]) and atr[t] > 0:
                stop_dist = atr[t] * stop_mult
                stop_dist_sum += stop_dist
                stop_dist_count += 1

        if stop_dist_count == 0:
            scores[i] = -np.inf
        else:
            stop_dist_mean = stop_dist_sum / float(stop_dist_count)
            # Score: -log1p(stop_mean) - penalize larger stops; deterministic; no target/median
            scores[i] = -np.log1p(stop_dist_mean)

    return scores


def vol_proxy_nb(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """Numba version of vol_proxy."""
    if nb is None:  # pragma: no cover
        raise RuntimeError("numba not available")
    return _vol_proxy_kernel(open_, high, low, close, params_matrix)


def vol_proxy(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """Wrapper: select nb/py based on NUMBA_DISABLE_JIT."""
    if nb is not None and os.environ.get("NUMBA_DISABLE_JIT", "").strip() != "1":
        return vol_proxy_nb(open_, high, low, close, params_matrix)
    return vol_proxy_py(open_, high, low, close, params_matrix)


# ============================================================================
# Proxy #3: Activity Proxy (Trade Count / trigger density)
# ============================================================================


def activity_proxy_py(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """
    Activity proxy: channel breakout trigger count.
    
    Counts crossings where close[t-1] <= channel_hi[t-1] and close[t] > channel_hi[t].
    Aligned with Stage2 kernel which uses channel breakout entry.

    Args:
        open_, high, low, close: float64 1D arrays (n_bars,)
        params_matrix: float64 2D array (n_params, >=1)
            - col0: channel_len
            - col1: atr_len (not used, kept for compatibility)

    Returns:
        scores: float64 1D array (n_params,)
    """
    o, h, l, c, pm = _validate_inputs(open_, high, low, close, params_matrix)
    n = c.shape[0]
    n_params = pm.shape[0]

    if pm.shape[1] < 1:
        raise ValueError("params_matrix must have at least 1 column: channel_len")

    scores = np.empty(n_params, dtype=np.float64)

    for i in range(n_params):
        channel_len = int(pm[i, 0])

        # Invalid params: return -inf
        if channel_len <= 0 or channel_len >= n:
            scores[i] = -np.inf
            continue

        # Compute channel_hi = rolling_max(high, channel_len)
        channel_hi = np.full(n, np.nan, dtype=np.float64)
        for t in range(n):
            start_idx = max(0, t - channel_len + 1)
            window_high = h[start_idx : t + 1]
            if window_high.size > 0:
                channel_hi[t] = np.max(window_high)

        # Warmup: start at channel_len
        start = channel_len
        if start >= n - 1:
            scores[i] = -np.inf
            continue

        # Count breakout triggers: high[t] > ch[t-1] AND high[t-1] <= ch[t-1]
        # Compare to previous channel high to avoid equality lock
        # Start from start+1 to ensure we have t-1 available
        triggers = 0
        for t in range(start + 1, n):
            if np.isnan(channel_hi[t-1]):
                continue
            # Trigger when high crosses above previous channel high
            if high[t] > channel_hi[t-1] and high[t-1] <= channel_hi[t-1]:
                triggers += 1

        n_effective = n - start
        if n_effective == 0:
            scores[i] = -np.inf
        else:
            # Activity score: raw count of triggers (or triggers per bar)
            # Using raw count for simplicity and robustness
            scores[i] = float(triggers)

    return scores


def activity_proxy_nb(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """Numba version of activity_proxy."""
    if nb is None:  # pragma: no cover
        raise RuntimeError("numba not available")
    return _activity_proxy_kernel(open_, high, low, close, params_matrix)


def activity_proxy(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """Wrapper: select nb/py based on NUMBA_DISABLE_JIT."""
    if nb is not None and os.environ.get("NUMBA_DISABLE_JIT", "").strip() != "1":
        return activity_proxy_nb(open_, high, low, close, params_matrix)
    return activity_proxy_py(open_, high, low, close, params_matrix)


# ============================================================================
# Helper functions (SMA)
# ============================================================================


def _sma_py(x: np.ndarray, length: int) -> np.ndarray:
    """Simple Moving Average (pure Python)."""
    n = x.shape[0]
    out = np.full(n, np.nan, dtype=np.float64)
    if length <= 0:
        return out
    csum = np.cumsum(x, dtype=np.float64)
    for i in range(n):
        j = i - length + 1
        if j < 0:
            continue
        total = csum[i] - (csum[j - 1] if j > 0 else 0.0)
        out[i] = total / float(length)
    return out


# ============================================================================
# Numba kernels
# ============================================================================

if nb is not None:

    @nb.njit(cache=False)
    def _sma_nb(x: np.ndarray, length: int) -> np.ndarray:
        """Simple Moving Average (Numba)."""
        n = x.shape[0]
        out = np.empty(n, dtype=np.float64)
        for i in range(n):
            out[i] = np.nan
        if length <= 0:
            return out
        csum = np.empty(n, dtype=np.float64)
        acc = 0.0
        for i in range(n):
            acc += float(x[i])
            csum[i] = acc
        for i in range(n):
            j = i - length + 1
            if j < 0:
                continue
            total = csum[i] - (csum[j - 1] if j > 0 else 0.0)
            out[i] = total / float(length)
        return out

    @nb.njit(cache=False)
    def _trend_proxy_kernel(
        open_: np.ndarray,
        high: np.ndarray,
        low: np.ndarray,
        close: np.ndarray,
        params_matrix: np.ndarray,
    ) -> np.ndarray:
        """Numba kernel for trend proxy."""
        n = close.shape[0]
        n_params = params_matrix.shape[0]
        scores = np.empty(n_params, dtype=np.float64)

        for i in range(n_params):
            fast = int(params_matrix[i, 0])
            slow = int(params_matrix[i, 1])

            if fast <= 0 or slow <= 0 or fast >= n or slow >= n:
                scores[i] = -np.inf
                continue

            sma_fast = _sma_nb(close, fast)
            sma_slow = _sma_nb(close, slow)

            start = fast if fast > slow else slow
            if start >= n:
                scores[i] = -np.inf
                continue

            acc = 0.0
            count = 0
            for t in range(start, n):
                diff = sma_fast[t] - sma_slow[t]
                if not np.isnan(diff) and close[t] > 0.0:
                    acc += diff / close[t]
                    count += 1

            if count == 0:
                scores[i] = -np.inf
            else:
                scores[i] = acc / float(count)

        return scores

    @nb.njit(cache=False)
    def _atr_wilder_nb(high: np.ndarray, low: np.ndarray, close: np.ndarray, window: int) -> np.ndarray:
        """ATR Wilder (Numba version, inline for njit compatibility)."""
        n = high.shape[0]
        out = np.empty(n, dtype=np.float64)
        for i in range(n):
            out[i] = np.nan

        if window <= 0 or n == 0 or window > n:
            return out

        tr = np.empty(n, dtype=np.float64)
        tr[0] = high[0] - low[0]
        for i in range(1, n):
            a = high[i] - low[i]
            b = abs(high[i] - close[i - 1])
            c = abs(low[i] - close[i - 1])
            tr[i] = a if a >= b and a >= c else (b if b >= c else c)

        s = 0.0
        end = window if window < n else n
        for i in range(end):
            s += tr[i]
        out[end - 1] = s / float(window)

        for i in range(window, n):
            out[i] = (out[i - 1] * float(window - 1) + tr[i]) / float(window)

        return out

    @nb.njit(cache=False)
    def _rolling_max_nb(arr: np.ndarray, window: int) -> np.ndarray:
        """Rolling maximum (Numba, inline for njit compatibility)."""
        n = arr.shape[0]
        out = np.empty(n, dtype=np.float64)
        for i in range(n):
            out[i] = np.nan
        if window <= 0:
            return out
        for i in range(n):
            start = i - window + 1
            if start < 0:
                start = 0
            m = arr[start]
            for j in range(start + 1, i + 1):
                v = arr[j]
                if v > m:
                    m = v
            out[i] = m
        return out

    @nb.njit(cache=False)
    def _vol_proxy_kernel(
        open_: np.ndarray,
        high: np.ndarray,
        low: np.ndarray,
        close: np.ndarray,
        params_matrix: np.ndarray,
    ) -> np.ndarray:
        """Numba kernel for vol proxy with stop_mult."""
        n = close.shape[0]
        n_params = params_matrix.shape[0]
        scores = np.empty(n_params, dtype=np.float64)

        for i in range(n_params):
            atr_len = int(params_matrix[i, 0])
            stop_mult = float(params_matrix[i, 1])

            if atr_len <= 0 or atr_len >= n or stop_mult <= 0.0:
                scores[i] = -np.inf
                continue

            atr = _atr_wilder_nb(high, low, close, atr_len)

            start = atr_len if atr_len > 1 else 1
            if start >= n:
                scores[i] = -np.inf
                continue

            # Compute stop distance: ATR * stop_mult
            stop_dist_sum = 0.0
            stop_dist_count = 0
            for t in range(start, n):
                if not np.isnan(atr[t]) and atr[t] > 0.0:
                    stop_dist = atr[t] * stop_mult
                    stop_dist_sum += stop_dist
                    stop_dist_count += 1

            if stop_dist_count == 0:
                scores[i] = -np.inf
            else:
                stop_dist_mean = stop_dist_sum / float(stop_dist_count)
                # Score: -log1p(stop_mean) - penalize larger stops; deterministic; no target/median
                scores[i] = -np.log1p(stop_dist_mean)

        return scores

    @nb.njit(cache=False)
    def _sign_nb(v: float) -> float:
        """Sign function (Numba)."""
        if v > 0.0:
            return 1.0
        if v < 0.0:
            return -1.0
        return 0.0

    @nb.njit(cache=False)
    def _activity_proxy_kernel(
        open_: np.ndarray,
        high: np.ndarray,
        low: np.ndarray,
        close: np.ndarray,
        params_matrix: np.ndarray,
    ) -> np.ndarray:
        """Numba kernel for activity proxy: channel breakout triggers."""
        n = close.shape[0]
        n_params = params_matrix.shape[0]
        scores = np.empty(n_params, dtype=np.float64)

        for i in range(n_params):
            channel_len = int(params_matrix[i, 0])

            if channel_len <= 0 or channel_len >= n:
                scores[i] = -np.inf
                continue

            # Compute channel_hi = rolling_max(high, channel_len)
            channel_hi = _rolling_max_nb(high, channel_len)

            start = channel_len
            if start >= n - 1:
                scores[i] = -np.inf
                continue

            # Count breakout triggers: high[t] > ch[t-1] AND high[t-1] <= ch[t-1]
            # Compare to previous channel high to avoid equality lock
            # Start from start+1 to ensure we have t-1 available
            triggers = 0
            for t in range(start + 1, n):
                if np.isnan(channel_hi[t-1]):
                    continue
                # Trigger when high crosses above previous channel high
                if high[t] > channel_hi[t-1] and high[t-1] <= channel_hi[t-1]:
                    triggers += 1

            n_effective = n - start
            if n_effective == 0:
                scores[i] = -np.inf
            else:
                # Activity score: raw count of triggers (or triggers per bar)
                # Using raw count for simplicity and robustness
                scores[i] = float(triggers)

        return scores


================================================================================
FILE: src/FishBroWFS_V2/strategy/__init__.py
================================================================================




================================================================================
FILE: src/FishBroWFS_V2/strategy/kernel.py
================================================================================

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Tuple

import numpy as np
import os
import time

from FishBroWFS_V2.engine.constants import KIND_STOP, ROLE_ENTRY, ROLE_EXIT, SIDE_BUY, SIDE_SELL
from FishBroWFS_V2.engine.engine_jit import simulate as simulate_matcher
from FishBroWFS_V2.engine.engine_jit import simulate_arrays as simulate_matcher_arrays
from FishBroWFS_V2.engine.types import BarArrays, Fill, OrderIntent, OrderKind, OrderRole, Side
from FishBroWFS_V2.indicators.numba_indicators import rolling_max, atr_wilder


@dataclass(frozen=True)
class DonchianAtrParams:
    channel_len: int
    atr_len: int
    stop_mult: float


def _max_drawdown(equity: np.ndarray) -> float:
    """
    Vectorized max drawdown on an equity curve.
    Handles empty arrays gracefully.
    """
    if equity.size == 0:
        return 0.0
    peak = np.maximum.accumulate(equity)
    dd = equity - peak
    mdd = float(np.min(dd))  # negative or 0
    return mdd


def run_kernel_object_mode(
    bars: BarArrays,
    params: DonchianAtrParams,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
) -> Dict[str, object]:
    """
    Golden Kernel (GKV): single-source-of-truth kernel for Phase 3A and future Phase 3B.

    Strategy (minimal):
      - Entry: Buy Stop at Donchian High (rolling max of HIGH over channel_len) at bar close -> next bar active.
      - Exit: Sell Stop at (entry_fill_price - stop_mult * ATR_wilder) active from next bar after entry_fill.

    Costs:
      - commission (absolute per trade)
      - slip (absolute per trade)
      Costs are applied on each round-trip fill (entry and exit each incur cost).

    Returns:
      dict with:
        - fills: List[Fill]
        - pnl: np.ndarray (float64, per-round-trip pnl, can be empty)
        - equity: np.ndarray (float64, cumsum of pnl, can be empty)
        - metrics: dict (net_profit, trades, max_dd)
    """
    profile = os.environ.get("FISHBRO_PROFILE_KERNEL", "").strip() == "1"
    t0 = time.perf_counter() if profile else 0.0

    # --- Compute indicators (kernel level; wrapper must ensure contiguous arrays) ---
    ch = int(params.channel_len)
    atr_n = int(params.atr_len)
    stop_mult = float(params.stop_mult)

    if ch <= 0 or atr_n <= 0:
        # invalid params -> zero trades, deterministic
        pnl = np.empty(0, dtype=np.float64)
        equity = np.empty(0, dtype=np.float64)
        # Evidence fields (Source of Truth) - Phase 3.0-A: must not be null
        # Red Team requirement: if fallback to objects mode, must leave fingerprint
        return {
            "fills": [],
            "pnl": pnl,
            "equity": equity,
            "metrics": {"net_profit": 0.0, "trades": 0, "max_dd": 0.0},
            "_obs": {
                "intent_mode": "objects",
                "intents_total": 0,
                "fills_total": 0,
            },
        }

    donch_hi = rolling_max(bars.high, ch)  # includes current bar
    atr = atr_wilder(bars.high, bars.low, bars.close, atr_n)
    t_ind = time.perf_counter() if profile else 0.0

    # --- Build order intents (next-bar active) ---
    intents: List[OrderIntent] = []
    oid = 1

    # We create entry intents for each bar t where indicator exists:
    # created_bar=t, active at t+1. price=donch_hi[t]
    n = int(bars.open.shape[0])
    for t in range(n):
        px = float(donch_hi[t])
        if np.isnan(px):
            continue
        intents.append(
            OrderIntent(
                order_id=oid,
                created_bar=t,
                role=OrderRole.ENTRY,
                kind=OrderKind.STOP,
                side=Side.BUY,
                price=px,
                qty=order_qty,
            )
        )
        oid += 1
    t_intents = time.perf_counter() if profile else 0.0

    # Run matcher (JIT or python via kill-switch)
    fills: List[Fill] = simulate_matcher(bars, intents)
    t_sim1 = time.perf_counter() if profile else 0.0

    # --- Convert fills -> round-trip pnl (vectorized style, no python trade loops as truth) ---
    # For this minimal kernel we assume:
    # - Only LONG trades (BUY entry, SELL exit) will be produced once we add exits.
    # Phase 3A GKV: We implement exits by post-processing: when entry fills, schedule a sell stop from next bar.
    # To preserve Homology, we do a second matcher pass with generated exit intents.
    # This keeps all fill semantics inside the matcher (constitution).
    exit_intents: List[OrderIntent] = []
    for f in fills:
        if f.role != OrderRole.ENTRY:
            continue
        # exit stop price = entry_price - stop_mult * atr at entry bar
        ebar = int(f.bar_index)
        if ebar < 0 or ebar >= n:
            continue
        a = float(atr[ebar])
        if np.isnan(a):
            continue
        stop_px = float(f.price - stop_mult * a)
        exit_intents.append(
            OrderIntent(
                order_id=oid,
                created_bar=ebar,  # active next bar
                role=OrderRole.EXIT,
                kind=OrderKind.STOP,
                side=Side.SELL,
                price=stop_px,
                qty=order_qty,
            )
        )
        oid += 1
    t_exit_intents = time.perf_counter() if profile else 0.0

    if exit_intents:
        fills2 = simulate_matcher(bars, exit_intents)
        t_sim2 = time.perf_counter() if profile else 0.0
        fills_all = fills + fills2
        # deterministic order: sort by (bar_index, role(ENTRY first), kind, order_id)
        fills_all.sort(key=lambda x: (x.bar_index, 0 if x.role == OrderRole.ENTRY else 1, 0 if x.kind == OrderKind.STOP else 1, x.order_id))
    else:
        fills_all = fills
        t_sim2 = t_sim1 if profile else 0.0

    # Extract entry/exit prices for round trips (vectorized with arrays, but pairing is minimal 1:1 sequential)
    # Pairing rule for GKV v1: take fills in chronological order, pair BUY(ENTRY) then SELL(EXIT).
    entry_prices = []
    exit_prices = []
    for f in fills_all:
        if f.role == OrderRole.ENTRY and f.side == Side.BUY:
            entry_prices.append(float(f.price))
        elif f.role == OrderRole.EXIT and f.side == Side.SELL:
            exit_prices.append(float(f.price))

    k = min(len(entry_prices), len(exit_prices))
    if k == 0:
        pnl = np.empty(0, dtype=np.float64)
        equity = np.empty(0, dtype=np.float64)
        metrics = {"net_profit": 0.0, "trades": 0, "max_dd": 0.0}
        # Evidence fields (Source of Truth) - Phase 3.0-A: must not be null
        # Red Team requirement: if fallback to objects mode, must leave fingerprint
        intents_total = int(len(intents) + len(exit_intents))
        fills_total = int(len(fills_all))
        return {
            "fills": fills_all,
            "pnl": pnl,
            "equity": equity,
            "metrics": metrics,
            "_obs": {
                "intent_mode": "objects",
                "intents_total": intents_total,
                "fills_total": fills_total,
            },
        }

    ep = np.asarray(entry_prices[:k], dtype=np.float64)
    xp = np.asarray(exit_prices[:k], dtype=np.float64)

    # Costs applied per fill (entry + exit)
    costs = (float(commission) + float(slip)) * 2.0
    pnl = (xp - ep) * float(order_qty) - costs
    equity = np.cumsum(pnl)

    metrics = {
        "net_profit": float(np.sum(pnl)) if pnl.size else 0.0,
        "trades": int(pnl.size),
        "max_dd": _max_drawdown(equity),
    }
    out = {"fills": fills_all, "pnl": pnl, "equity": equity, "metrics": metrics}

    # Evidence fields (Source of Truth) - Phase 3.0-A
    # Red Team requirement: if fallback to objects mode, must leave fingerprint
    intents_total = int(len(intents) + len(exit_intents))  # Total intents (entry + exit, merged)
    fills_total = int(len(fills_all))  # fills_all is List[Fill], use len()
    
    # Always-on observability payload (no timing assumptions).
    out["_obs"] = {
        "intent_mode": "objects",
        "intents_total": intents_total,
        "fills_total": fills_total,
        "entry_intents": int(len(intents)),
        "exit_intents": int(len(exit_intents)),
    }

    if profile:
        out["_profile"] = {
            "intent_mode": "objects",
            "indicators_s": float(t_ind - t0),
            "intent_gen_s": float(t_intents - t_ind),
            "simulate_entry_s": float(t_sim1 - t_intents),
            "exit_intent_gen_s": float(t_exit_intents - t_sim1),
            "simulate_exit_s": float(t_sim2 - t_exit_intents),
            "kernel_total_s": float(t_sim2 - t0),
            "entry_intents": int(len(intents)),
            "exit_intents": int(len(exit_intents)),
        }
    return out


def run_kernel_arrays(
    bars: BarArrays,
    params: DonchianAtrParams,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
    return_debug: bool = False,
) -> Dict[str, object]:
    """
    Array/SoA intent mode: generates intents as arrays and calls engine_jit.simulate_arrays().
    This avoids OrderIntent object construction in the hot path.
    """
    profile = os.environ.get("FISHBRO_PROFILE_KERNEL", "").strip() == "1"
    t0 = time.perf_counter() if profile else 0.0

    ch = int(params.channel_len)
    atr_n = int(params.atr_len)
    stop_mult = float(params.stop_mult)

    if ch <= 0 or atr_n <= 0:
        pnl = np.empty(0, dtype=np.float64)
        equity = np.empty(0, dtype=np.float64)
        # Evidence fields (Source of Truth) - Phase 3.0-A: must not be null
        return {
            "fills": [],
            "pnl": pnl,
            "equity": equity,
            "metrics": {"net_profit": 0.0, "trades": 0, "max_dd": 0.0},
            "_obs": {
                "intent_mode": "arrays",
                "intents_total": 0,
                "fills_total": 0,
            },
        }

    donch_hi = rolling_max(bars.high, ch)
    atr = atr_wilder(bars.high, bars.low, bars.close, atr_n)
    t_ind = time.perf_counter() if profile else 0.0

    # Debug instrumentation: track first entry/exit per param (only if return_debug=True)
    if return_debug:
        dbg_entry_bar = -1
        dbg_entry_price = np.nan
        dbg_exit_bar = -1
        dbg_exit_price = np.nan
    else:
        dbg_entry_bar = None
        dbg_entry_price = None
        dbg_exit_bar = None
        dbg_exit_price = None

    # ---- ENTRY intents (vectorized) ----
    valid = ~np.isnan(donch_hi)
    created_bar = np.flatnonzero(valid).astype(np.int64)
    price = donch_hi[valid].astype(np.float64, copy=False)
    n_entry = int(created_bar.shape[0])

    order_id = np.arange(1, n_entry + 1, dtype=np.int64)
    role = np.full(n_entry, ROLE_ENTRY, dtype=np.int8)
    kind = np.full(n_entry, KIND_STOP, dtype=np.int8)
    side = np.full(n_entry, SIDE_BUY, dtype=np.int8)
    qty = np.full(n_entry, int(order_qty), dtype=np.int64)
    t_intents = time.perf_counter() if profile else 0.0

    fills: List[Fill] = simulate_matcher_arrays(
        bars,
        order_id=order_id,
        created_bar=created_bar,
        role=role,
        kind=kind,
        side=side,
        price=price,
        qty=qty,
        ttl_bars=1,
    )
    t_sim1 = time.perf_counter() if profile else 0.0

    # Capture first entry fill for debug
    if return_debug and len(fills) > 0:
        first_entry = None
        for f in fills:
            if f.role == OrderRole.ENTRY and f.side == Side.BUY:
                first_entry = f
                break
        if first_entry is not None:
            dbg_entry_bar = int(first_entry.bar_index)
            dbg_entry_price = float(first_entry.price)

    # ---- EXIT intents (depends on entry fills; keep SoA, no OrderIntent objects) ----
    exit_created = np.empty(len(fills), dtype=np.int64)
    exit_price = np.empty(len(fills), dtype=np.float64)
    keep = 0
    for f in fills:
        if f.role != OrderRole.ENTRY:
            continue
        ebar = int(f.bar_index)
        if ebar < 0 or ebar >= int(bars.open.shape[0]):
            continue
        a = float(atr[ebar])
        if np.isnan(a):
            continue
        exit_created[keep] = np.int64(ebar)
        exit_price[keep] = float(f.price - stop_mult * a)
        keep += 1

    exit_intents_count = int(keep)
    t_exit_intents = time.perf_counter() if profile else 0.0

    if exit_intents_count > 0:
        exit_created = exit_created[:exit_intents_count]
        exit_price = exit_price[:exit_intents_count]
        exit_order_id = np.arange(n_entry + 1, n_entry + 1 + exit_intents_count, dtype=np.int64)
        exit_role = np.full(exit_intents_count, ROLE_EXIT, dtype=np.int8)
        exit_kind = np.full(exit_intents_count, KIND_STOP, dtype=np.int8)
        exit_side = np.full(exit_intents_count, SIDE_SELL, dtype=np.int8)
        exit_qty = np.full(exit_intents_count, int(order_qty), dtype=np.int64)
        fills2 = simulate_matcher_arrays(
            bars,
            order_id=exit_order_id,
            created_bar=exit_created,
            role=exit_role,
            kind=exit_kind,
            side=exit_side,
            price=exit_price,
            qty=exit_qty,
            ttl_bars=1,
        )
        t_sim2 = time.perf_counter() if profile else 0.0
        fills_all = fills + fills2
        fills_all.sort(
            key=lambda x: (
                x.bar_index,
                0 if x.role == OrderRole.ENTRY else 1,
                0 if x.kind == OrderKind.STOP else 1,
                x.order_id,
            )
        )
    else:
        fills_all = fills
        t_sim2 = t_sim1 if profile else 0.0

    # Capture first exit fill for debug (or force_close_last exit)
    if return_debug and len(fills_all) > 0:
        first_exit = None
        for f in fills_all:
            if f.role == OrderRole.EXIT and f.side == Side.SELL:
                first_exit = f
                break
        if first_exit is not None:
            dbg_exit_bar = int(first_exit.bar_index)
            dbg_exit_price = float(first_exit.price)

    entry_prices = []
    exit_prices = []
    for f in fills_all:
        if f.role == OrderRole.ENTRY and f.side == Side.BUY:
            entry_prices.append(float(f.price))
        elif f.role == OrderRole.EXIT and f.side == Side.SELL:
            exit_prices.append(float(f.price))

    k = min(len(entry_prices), len(exit_prices))
    if k == 0:
        pnl = np.empty(0, dtype=np.float64)
        equity = np.empty(0, dtype=np.float64)
        metrics = {"net_profit": 0.0, "trades": 0, "max_dd": 0.0}
        # Evidence fields (Source of Truth) - Phase 3.0-A: must not be null
        intents_total = int(n_entry + exit_intents_count)
        fills_total = int(len(fills_all))
        result = {
            "fills": fills_all,
            "pnl": pnl,
            "equity": equity,
            "metrics": metrics,
            "_obs": {
                "intent_mode": "arrays",
                "intents_total": intents_total,
                "fills_total": fills_total,
            },
        }
        if return_debug:
            result["_debug"] = {
                "entry_bar": dbg_entry_bar,
                "entry_price": dbg_entry_price,
                "exit_bar": dbg_exit_bar,
                "exit_price": dbg_exit_price,
            }
        return result

    ep = np.asarray(entry_prices[:k], dtype=np.float64)
    xp = np.asarray(exit_prices[:k], dtype=np.float64)

    costs = (float(commission) + float(slip)) * 2.0
    pnl = (xp - ep) * float(order_qty) - costs
    equity = np.cumsum(pnl)

    metrics = {
        "net_profit": float(np.sum(pnl)) if pnl.size else 0.0,
        "trades": int(pnl.size),
        "max_dd": _max_drawdown(equity),
    }
    out = {"fills": fills_all, "pnl": pnl, "equity": equity, "metrics": metrics}

    # Evidence fields (Source of Truth) - Phase 3.0-A
    intents_total = int(n_entry + exit_intents_count)  # Total intents (entry + exit, merged)
    fills_total = int(len(fills_all))  # fills_all is List[Fill], use len()
    
    out["_obs"] = {
        "intent_mode": "arrays",
        "intents_total": intents_total,
        "fills_total": fills_total,
        "entry_intents": int(n_entry),
        "exit_intents": int(exit_intents_count),
    }
    if return_debug:
        out["_debug"] = {
            "entry_bar": dbg_entry_bar,
            "entry_price": dbg_entry_price,
            "exit_bar": dbg_exit_bar,
            "exit_price": dbg_exit_price,
        }
    if profile:
        out["_profile"] = {
            "intent_mode": "arrays",
            "indicators_s": float(t_ind - t0),
            "intent_gen_s": float(t_intents - t_ind),
            "simulate_entry_s": float(t_sim1 - t_intents),
            "exit_intent_gen_s": float(t_exit_intents - t_sim1),
            "simulate_exit_s": float(t_sim2 - t_exit_intents),
            "kernel_total_s": float(t_sim2 - t0),
            "entry_intents": int(n_entry),
            "exit_intents": int(exit_intents_count),
        }
    return out


def run_kernel(
    bars: BarArrays,
    params: DonchianAtrParams,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
    return_debug: bool = False,
) -> Dict[str, object]:
    # Default to arrays path for perf; object mode remains as a correctness reference.
    mode = os.environ.get("FISHBRO_KERNEL_INTENT_MODE", "").strip().lower()
    if mode == "objects":
        return run_kernel_object_mode(
            bars,
            params,
            commission=commission,
            slip=slip,
            order_qty=order_qty,
        )
    return run_kernel_arrays(
        bars,
        params,
        commission=commission,
        slip=slip,
        order_qty=order_qty,
        return_debug=return_debug,
    )



================================================================================
FILE: src/FishBroWFS_V2/strategy/runner_single.py
================================================================================

from __future__ import annotations

from typing import Dict

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.types import BarArrays
from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, run_kernel


def run_single(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params: DonchianAtrParams,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
) -> Dict[str, object]:
    """
    Wrapper for Phase 3A (GKV): ensure memory layout + call kernel once.
    """
    bars: BarArrays = normalize_bars(open_, high, low, close)

    # Boundary Layout Check: enforce contiguous arrays before entering kernel.
    if not bars.open.flags["C_CONTIGUOUS"]:
        bars = BarArrays(
            open=np.ascontiguousarray(bars.open, dtype=np.float64),
            high=np.ascontiguousarray(bars.high, dtype=np.float64),
            low=np.ascontiguousarray(bars.low, dtype=np.float64),
            close=np.ascontiguousarray(bars.close, dtype=np.float64),
        )

    return run_kernel(bars, params, commission=commission, slip=slip, order_qty=order_qty)



================================================================================
FILE: src/FishBroWFS_V2/version.py
================================================================================

__version__ = "0.1.0"



================================================================================
FILE: tests/conftest.py
================================================================================

"""
Pytest configuration and fixtures.

Ensures PYTHONPATH is set correctly for imports.
"""
from __future__ import annotations

import sys
from pathlib import Path

# Add src/ to Python path if not already present
# This ensures tests can import FishBroWFS_V2 without manual PYTHONPATH setup
repo_root = Path(__file__).parent.parent
src_path = repo_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))


================================================================================
FILE: tests/test_baseline_lock.py
================================================================================

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.engine_jit import simulate as simulate_jit
from FishBroWFS_V2.engine.matcher_core import simulate as simulate_py
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def _fills_to_matrix(fills):
    # Columns: bar_index, role, kind, side, price, qty, order_id
    m = np.empty((len(fills), 7), dtype=np.float64)
    for i, f in enumerate(fills):
        m[i, 0] = float(f.bar_index)
        m[i, 1] = 0.0 if f.role == OrderRole.EXIT else 1.0
        m[i, 2] = 0.0 if f.kind == OrderKind.STOP else 1.0
        m[i, 3] = float(int(f.side.value))
        m[i, 4] = float(f.price)
        m[i, 5] = float(f.qty)
        m[i, 6] = float(f.order_id)
    return m


def test_gate_a_jit_matches_python_reference():
    # Two bars so we can test next-bar active + entry then exit.
    bars = normalize_bars(
        np.array([100.0, 100.0], dtype=np.float64),
        np.array([120.0, 120.0], dtype=np.float64),
        np.array([90.0, 80.0], dtype=np.float64),
        np.array([110.0, 90.0], dtype=np.float64),
    )

    intents = [
        # Entry active on bar0
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),
        # Exit active on bar0 (same bar), should execute after entry
        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),
        # Entry created on bar0 -> active on bar1
        OrderIntent(order_id=3, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=110.0),
    ]

    py = simulate_py(bars, intents)
    jit = simulate_jit(bars, intents)

    m_py = _fills_to_matrix(py)
    m_jit = _fills_to_matrix(jit)

    assert m_py.shape == m_jit.shape
    # Event-level exactness except price tolerance
    np.testing.assert_array_equal(m_py[:, [0, 1, 2, 3, 5, 6]], m_jit[:, [0, 1, 2, 3, 5, 6]])
    np.testing.assert_allclose(m_py[:, 4], m_jit[:, 4], rtol=0.0, atol=1e-9)



================================================================================
FILE: tests/test_data_layout.py
================================================================================

import numpy as np
import pytest
from FishBroWFS_V2.data.layout import normalize_bars


def test_normalize_bars_dtype_and_contiguous():
    o = np.arange(10, dtype=np.float32)[::2]
    h = o + 1
    l = o - 1
    c = o + 0.5

    bars = normalize_bars(o, h, l, c)

    for arr in (bars.open, bars.high, bars.low, bars.close):
        assert arr.dtype == np.float64
        assert arr.flags["C_CONTIGUOUS"]


def test_normalize_bars_reject_nan():
    o = np.array([1.0, np.nan])
    h = np.array([1.0, 2.0])
    l = np.array([0.5, 1.5])
    c = np.array([0.8, 1.8])

    with pytest.raises(ValueError):
        normalize_bars(o, h, l, c)



================================================================================
FILE: tests/test_engine_constitution.py
================================================================================

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.matcher_core import simulate
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def _bars1(o, h, l, c):
    return normalize_bars(
        np.array([o], dtype=np.float64),
        np.array([h], dtype=np.float64),
        np.array([l], dtype=np.float64),
        np.array([c], dtype=np.float64),
    )


def _bars2(o0, h0, l0, c0, o1, h1, l1, c1):
    return normalize_bars(
        np.array([o0, o1], dtype=np.float64),
        np.array([h0, h1], dtype=np.float64),
        np.array([l0, l1], dtype=np.float64),
        np.array([c0, c1], dtype=np.float64),
    )


def test_tc01_buy_stop_normal():
    bars = _bars1(90, 105, 90, 100)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 100.0


def test_tc02_buy_stop_gap_up_fill_open():
    bars = _bars1(105, 110, 105, 108)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 105.0


def test_tc03_sell_stop_gap_down_fill_open():
    bars = _bars1(90, 95, 80, 85)
    intents = [
        # Exit a long position requires SELL stop; we will enter long first in same bar is not allowed here,
        # so we simulate already-in-position by forcing an entry earlier: created_bar=-2 triggers at -1 (ignored),
        # Instead: use two bars and enter on bar0, exit on bar1.
    ]
    bars2 = _bars2(
        100, 100, 100, 100,   # bar0: enter long at 100 (buy stop gap/normal both ok)
        90, 95, 80, 85        # bar1: exit stop triggers gap down open
    )
    intents2 = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=100.0),
    ]
    fills = simulate(bars2, intents2)
    assert len(fills) == 2
    # second fill is the exit
    assert fills[1].price == 90.0


def test_tc08_next_bar_active_not_same_bar():
    # bar0 has high 105 which would hit stop 102, but order created at bar0 must not fill at bar0.
    # bar1 hits again, should fill at bar1.
    bars = _bars2(
        100, 105, 95, 100,
        100, 105, 95, 100,
    )
    intents = [
        OrderIntent(order_id=1, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].bar_index == 1
    assert fills[0].price == 102.0


def test_tc09_open_equals_stop_gap_branch_but_same_price():
    bars = _bars1(100, 100, 90, 95)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 100.0


def test_tc10_no_fill_when_not_touched():
    bars = _bars1(90, 95, 90, 92)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert fills == []



================================================================================
FILE: tests/test_engine_gaps_and_priority.py
================================================================================

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.matcher_core import simulate
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def _bars1(o, h, l, c):
    return normalize_bars(
        np.array([o], dtype=np.float64),
        np.array([h], dtype=np.float64),
        np.array([l], dtype=np.float64),
        np.array([c], dtype=np.float64),
    )


def test_tc04_buy_limit_gap_down_better_fill_open():
    bars = _bars1(90, 95, 85, 92)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 90.0


def test_tc05_sell_limit_gap_up_better_fill_open():
    bars = _bars1(105, 110, 100, 108)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.SELL, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 105.0


def test_tc06_priority_stop_wins_over_limit_on_exit():
    # First enter long on this same bar, then exit on next bar where both stop and limit are triggerable.
    # Bar0: enter long at 100 (buy stop hits)
    # Bar1: both exit stop 90 and exit limit 110 are touchable (high=110, low=80), STOP must win (fill=90)
    bars = normalize_bars(
        np.array([100, 100], dtype=np.float64),
        np.array([110, 110], dtype=np.float64),
        np.array([90, 80], dtype=np.float64),
        np.array([100, 90], dtype=np.float64),
    )

    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=90.0),
        OrderIntent(order_id=3, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.LIMIT, side=Side.SELL, price=110.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 2
    # Second fill is exit; STOP wins -> 90
    assert fills[1].kind == OrderKind.STOP
    assert fills[1].price == 90.0


def test_tc07_same_bar_entry_then_exit():
    # Same bar allows Entry then Exit.
    # Bar: O=100 H=120 L=90 C=110
    # Entry: Buy Stop 105 -> fills at 105 (since open 100 < 105 and high 120 >= 105)
    # Exit: Sell Stop 95 -> after entry, low 90 <= 95 -> fills at 95
    bars = _bars1(100, 120, 90, 110)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),
        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 2
    assert fills[0].price == 105.0
    assert fills[1].price == 95.0



================================================================================
FILE: tests/test_engine_jit_active_book_contract.py
================================================================================

from __future__ import annotations

import os

import numpy as np
import pytest

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.engine_jit import _simulate_with_ttl, simulate as simulate_jit
from FishBroWFS_V2.engine.matcher_core import simulate as simulate_py
from FishBroWFS_V2.engine.types import Fill, OrderIntent, OrderKind, OrderRole, Side


def _assert_fills_equal(a: list[Fill], b: list[Fill]) -> None:
    assert len(a) == len(b)
    for fa, fb in zip(a, b):
        assert fa.bar_index == fb.bar_index
        assert fa.role == fb.role
        assert fa.kind == fb.kind
        assert fa.side == fb.side
        assert fa.qty == fb.qty
        assert fa.order_id == fb.order_id
        assert abs(fa.price - fb.price) <= 1e-9


def test_jit_sorted_invariance_matches_python() -> None:
    # Bars: 3 bars, deterministic highs/lows for STOP triggers
    bars = normalize_bars(
        np.array([100.0, 100.0, 100.0], dtype=np.float64),
        np.array([110.0, 110.0, 110.0], dtype=np.float64),
        np.array([90.0, 90.0, 90.0], dtype=np.float64),
        np.array([100.0, 100.0, 100.0], dtype=np.float64),
    )

    # Intents across multiple activate bars (created_bar = t-1)
    intents = [
        # activate on bar0 (created -1)
        OrderIntent(3, -1, OrderRole.EXIT, OrderKind.STOP, Side.SELL, 95.0, 1),
        OrderIntent(2, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 105.0, 1),
        # activate on bar1 (created 0)
        OrderIntent(6, 0, OrderRole.EXIT, OrderKind.LIMIT, Side.SELL, 110.0, 1),
        OrderIntent(5, 0, OrderRole.ENTRY, OrderKind.LIMIT, Side.BUY, 99.0, 1),
        # activate on bar2 (created 1)
        OrderIntent(9, 1, OrderRole.EXIT, OrderKind.STOP, Side.SELL, 90.0, 1),
        OrderIntent(8, 1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),
    ]

    shuffled = list(intents)
    rng = np.random.default_rng(123)
    rng.shuffle(shuffled)

    # JIT simulate sorts internally for cursor+book; it must be invariant to input ordering.
    jit_a = simulate_jit(bars, shuffled)
    jit_b = simulate_jit(bars, intents)
    _assert_fills_equal(jit_a, jit_b)

    # Also must match Python reference semantics.
    py = simulate_py(bars, shuffled)
    _assert_fills_equal(jit_a, py)


def test_one_bar_max_one_entry_one_exit_defense() -> None:
    # Single bar is enough: created_bar=-1 activates on bar 0.
    bars = normalize_bars(
        np.array([100.0], dtype=np.float64),
        np.array([120.0], dtype=np.float64),
        np.array([80.0], dtype=np.float64),
        np.array([110.0], dtype=np.float64),
    )

    # Same activate bar contains Entry1, Exit1, Entry2.
    intents = [
        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 105.0, 1),
        OrderIntent(2, -1, OrderRole.EXIT, OrderKind.STOP, Side.SELL, 95.0, 1),
        OrderIntent(3, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 110.0, 1),
    ]

    fills = simulate_jit(bars, intents)
    assert len(fills) == 2
    assert fills[0].order_id == 1
    assert fills[1].order_id == 2


def test_ttl_one_shot_vs_gtc_extension_point() -> None:
    # Skip if JIT is disabled; ttl=0 is a JIT-only extension behavior.
    import FishBroWFS_V2.engine.engine_jit as ej

    if ej.nb is None or os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        pytest.skip("numba not available or disabled; ttl=0 extension tested only under JIT")

    # Bar0: stop not touched, Bar1: stop touched
    bars = normalize_bars(
        np.array([90.0, 90.0], dtype=np.float64),
        np.array([99.0, 110.0], dtype=np.float64),
        np.array([90.0, 90.0], dtype=np.float64),
        np.array([95.0, 100.0], dtype=np.float64),
    )
    intents = [
        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),
    ]

    # ttl=1 (default semantics): active only on bar0 -> no fill
    fills_ttl1 = simulate_jit(bars, intents)
    assert fills_ttl1 == []

    # ttl=0 (GTC extension): order stays in book and can fill on bar1
    fills_gtc = _simulate_with_ttl(bars, intents, ttl_bars=0)
    assert len(fills_gtc) == 1
    assert fills_gtc[0].bar_index == 1
    assert abs(fills_gtc[0].price - 100.0) <= 1e-9




================================================================================
FILE: tests/test_golden_kernel_verification.py
================================================================================

import numpy as np

from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, run_kernel, _max_drawdown
from FishBroWFS_V2.engine.types import BarArrays


def _bars():
    # Small synthetic OHLC series
    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)
    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)
    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)
    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)
    return BarArrays(open=o, high=h, low=l, close=c)


def test_no_trade_case_does_not_crash_and_returns_zero_metrics():
    bars = _bars()
    params = DonchianAtrParams(channel_len=99999, atr_len=3, stop_mult=2.0)

    out = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)
    pnl = out["pnl"]
    equity = out["equity"]
    metrics = out["metrics"]

    assert isinstance(pnl, np.ndarray)
    assert pnl.size == 0
    assert isinstance(equity, np.ndarray)
    assert equity.size == 0
    assert metrics["net_profit"] == 0.0
    assert metrics["trades"] == 0
    assert metrics["max_dd"] == 0.0


def test_vectorized_metrics_are_self_consistent():
    bars = _bars()
    params = DonchianAtrParams(channel_len=2, atr_len=2, stop_mult=1.0)

    out = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)
    pnl = out["pnl"]
    equity = out["equity"]
    metrics = out["metrics"]

    # If zero trades, still must be consistent
    if pnl.size == 0:
        assert metrics["net_profit"] == 0.0
        assert metrics["trades"] == 0
        assert metrics["max_dd"] == 0.0
        return

    # Vectorized checks
    np.testing.assert_allclose(equity, np.cumsum(pnl), rtol=0.0, atol=0.0)
    assert metrics["trades"] == int(pnl.size)
    assert metrics["net_profit"] == float(np.sum(pnl))
    assert metrics["max_dd"] == _max_drawdown(equity)


def test_costs_are_parameterized_not_hardcoded():
    bars = _bars()
    params = DonchianAtrParams(channel_len=2, atr_len=2, stop_mult=1.0)

    out0 = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)
    out1 = run_kernel(bars, params, commission=1.25, slip=0.75, order_qty=1)

    pnl0 = out0["pnl"]
    pnl1 = out1["pnl"]

    # Either both empty or both non-empty; if empty, pass
    if pnl0.size == 0:
        assert pnl1.size == 0
        return

    # Costs increase => pnl decreases by 2*(commission+slip) per trade
    per_trade_delta = 2.0 * (1.25 + 0.75)
    np.testing.assert_allclose(pnl1, pnl0 - per_trade_delta, rtol=0.0, atol=1e-12)



================================================================================
FILE: tests/test_grid_runner_smoke.py
================================================================================

import numpy as np

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def _ohlc():
    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)
    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)
    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)
    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)
    return o, h, l, c


def test_grid_runner_smoke_shapes_and_no_crash():
    o, h, l, c = _ohlc()

    # params: [channel_len, atr_len, stop_mult]
    params = np.array(
        [
            [2, 2, 1.0],
            [3, 2, 1.5],
            [99999, 3, 2.0],  # should produce 0 trades
            [2, 99999, 2.0],  # atr_len > n should be safe (atr_wilder returns all-NaN -> kernel => 0 trades)
        ],
        dtype=np.float64,
    )

    out = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=True)
    m = out["metrics"]
    order = out["order"]

    assert isinstance(m, np.ndarray)
    assert m.shape == (params.shape[0], 3)
    assert isinstance(order, np.ndarray)
    assert order.shape == (params.shape[0],)
    assert set(order.tolist()) == set(range(params.shape[0]))
    # Optional stronger assertion: at least one row should have 0 trades due to atr_len > n
    assert np.any(m[:, 1] == 0.0)


def test_grid_runner_sorting_toggle():
    o, h, l, c = _ohlc()
    params = np.array(
        [
            [3, 2, 1.5],
            [2, 2, 1.0],
            [2, 3, 2.0],
        ],
        dtype=np.float64,
    )

    out_sorted = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=True)
    out_unsorted = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=False)

    assert out_sorted["metrics"].shape == out_unsorted["metrics"].shape == (3, 3)
    assert out_sorted["order"].shape == out_unsorted["order"].shape == (3,)
    # unsorted order should be identity
    np.testing.assert_array_equal(out_unsorted["order"], np.array([0, 1, 2], dtype=np.int64))



================================================================================
FILE: tests/test_indicators_consistency.py
================================================================================

import numpy as np

from FishBroWFS_V2.indicators.numba_indicators import (
    rolling_max,
    rolling_min,
    atr_wilder,
)


def _py_rolling_max(arr, window):
    out = np.full(len(arr), np.nan, dtype=np.float64)
    for i in range(len(arr)):
        start = max(0, i - window + 1)
        out[i] = np.max(arr[start : i + 1])
    return out


def _py_rolling_min(arr, window):
    out = np.full(len(arr), np.nan, dtype=np.float64)
    for i in range(len(arr)):
        start = max(0, i - window + 1)
        out[i] = np.min(arr[start : i + 1])
    return out


def _py_atr_wilder(high, low, close, window):
    n = len(high)
    out = np.full(n, np.nan, dtype=np.float64)
    if window > n:
        return out
    tr = np.empty(n, dtype=np.float64)
    tr[0] = high[0] - low[0]
    for i in range(1, n):
        tr[i] = max(
            high[i] - low[i],
            abs(high[i] - close[i - 1]),
            abs(low[i] - close[i - 1]),
        )
    end = window
    out[end - 1] = np.mean(tr[:end])
    for i in range(window, n):
        out[i] = (out[i - 1] * (window - 1) + tr[i]) / window
    return out


def test_rolling_max_min_consistency():
    arr = np.array([1.0, 3.0, 2.0, 5.0, 4.0], dtype=np.float64)
    w = 3

    mx_py = _py_rolling_max(arr, w)
    mn_py = _py_rolling_min(arr, w)

    mx = rolling_max(arr, w)
    mn = rolling_min(arr, w)

    np.testing.assert_allclose(mx, mx_py, rtol=0.0, atol=0.0)
    np.testing.assert_allclose(mn, mn_py, rtol=0.0, atol=0.0)


def test_atr_wilder_consistency():
    high = np.array([10, 11, 12, 11, 13, 14], dtype=np.float64)
    low = np.array([9, 9, 10, 9, 11, 12], dtype=np.float64)
    close = np.array([9.5, 10.5, 11.0, 10.0, 12.0, 13.0], dtype=np.float64)
    w = 3

    atr_py = _py_atr_wilder(high, low, close, w)
    atr = atr_wilder(high, low, close, w)

    np.testing.assert_allclose(atr, atr_py, rtol=0.0, atol=1e-12)


def test_atr_wilder_window_gt_n_returns_all_nan():
    high = np.array([10, 11], dtype=np.float64)
    low = np.array([9, 10], dtype=np.float64)
    close = np.array([9.5, 10.5], dtype=np.float64)
    atr = atr_wilder(high, low, close, 999)
    assert atr.shape == (2,)
    assert np.all(np.isnan(atr))



================================================================================
FILE: tests/test_perf_evidence_chain.py
================================================================================

from __future__ import annotations

import numpy as np

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_perf_evidence_chain_exists() -> None:
    """
    Phase 3.0-D: Contract Test - Evidence Chain Existence
    
    Purpose: Lock down that evidence fields always exist and are non-null.
    This test only verifies evidence existence, not timing or strategy quality.
    """
    # Use minimal data: bars=50, params=3
    n_bars = 50
    n_params = 3
    
    # Generate synthetic OHLC data
    rng = np.random.default_rng(42)
    close = 100.0 + np.cumsum(rng.standard_normal(n_bars)).astype(np.float64)
    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
    open_ = (high + low) / 2 + rng.standard_normal(n_bars) * 0.5
    
    # Ensure high >= max(open, close) and low <= min(open, close)
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    # Generate minimal params: [channel_len, atr_len, stop_mult]
    params = np.array(
        [
            [10, 5, 1.0],
            [15, 7, 1.5],
            [20, 10, 2.0],
        ],
        dtype=np.float64,
    )
    
    # Run grid runner (array path)
    # Note: perf field is always present in runner output (Phase 3.0-B)
    out = run_grid(
        open_=open_,
        high=high,
        low=low,
        close=close,
        params_matrix=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        sort_params=False,
    )
    
    # Verify perf field exists
    assert "perf" in out, "perf field must exist in runner output"
    perf = out["perf"]
    assert isinstance(perf, dict), "perf must be a dict"
    
    # Phase 3.0-D: Assert evidence fields exist and are non-null
    # 1. intent_mode must be "arrays"
    assert "intent_mode" in perf, "intent_mode must exist in perf"
    assert perf["intent_mode"] == "arrays", (
        f"intent_mode expected 'arrays' but got '{perf['intent_mode']}'"
    )
    
    # 2. intents_total must exist, be non-null, and > 0
    assert "intents_total" in perf, "intents_total must exist in perf"
    assert perf["intents_total"] is not None, "intents_total must not be None"
    assert isinstance(perf["intents_total"], (int, np.integer)), (
        f"intents_total must be an integer, got {type(perf['intents_total'])}"
    )
    assert int(perf["intents_total"]) > 0, (
        f"intents_total must be > 0, got {perf['intents_total']}"
    )
    
    # 3. fills_total must exist and be non-null (can be 0, but not None)
    assert "fills_total" in perf, "fills_total must exist in perf"
    assert perf["fills_total"] is not None, "fills_total must not be None"
    assert isinstance(perf["fills_total"], (int, np.integer)), (
        f"fills_total must be an integer, got {type(perf['fills_total'])}"
    )
    # fills_total can be 0 (no trades), but must not be None


================================================================================
FILE: tests/test_perf_grid_profile_report.py
================================================================================

from __future__ import annotations

import cProfile

from FishBroWFS_V2.perf.profile_report import _format_profile_report


def test_profile_report_markers_present() -> None:
    pr = cProfile.Profile()
    pr.enable()
    _ = sum(range(10_000))  # tiny workload, deterministic
    pr.disable()
    report = _format_profile_report(
        lane_id="3",
        n_bars=2000,
        n_params=100,
        jit_enabled=True,
        sort_params=False,
        topn=10,
        mode="",
        pr=pr,
    )
    assert "__PROFILE_START__" in report
    assert "pstats sort: cumtime" in report
    assert "__PROFILE_END__" in report




================================================================================
FILE: tests/test_runner_grid_perf_observability.py
================================================================================

from __future__ import annotations

import numpy as np

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_run_grid_perf_fields_present_and_non_negative(monkeypatch) -> None:
    # Enable perf observability.
    monkeypatch.setenv("FISHBRO_PROFILE_GRID", "1")

    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)
    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)
    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)
    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)

    params = np.array([[2, 2, 1.0], [3, 2, 1.5]], dtype=np.float64)
    out = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=False)

    assert "perf" in out
    perf = out["perf"]
    assert isinstance(perf, dict)

    for k in ("t_features", "t_indicators", "t_intent_gen", "t_simulate"):
        assert k in perf
        # allow None (JSON null) when measurement is unavailable; never assume 0 is meaningful
        if perf[k] is not None:
            assert float(perf[k]) >= 0.0

    assert "simulate_impl" in perf
    assert perf["simulate_impl"] in ("jit", "py")

    assert "intents_total" in perf
    if perf["intents_total"] is not None:
        assert int(perf["intents_total"]) >= 0

    # Perf harness hook: confirm we can observe intent mode when profiling is enabled.
    assert "intent_mode" in perf
    if perf["intent_mode"] is not None:
        assert perf["intent_mode"] in ("arrays", "objects")




================================================================================
FILE: tests/test_stage0_contract.py
================================================================================

from __future__ import annotations

"""
Stage 0 Contract Tests

Stage 0 must remain a "vector/proxy" layer:
  - MUST NOT import engine/matcher/strategy kernel/pipeline grid runner.
  - MUST NOT create OrderIntent/Fill objects.

These tests are intentionally strict: they prevent "silent scope creep"
that would destroy throughput and blur semantics.
"""

import ast
from pathlib import Path


def _read(path: Path) -> str:
    return path.read_text(encoding="utf-8")


def test_stage0_does_not_import_engine_or_runner_grid() -> None:
    root = Path(__file__).resolve().parent.parent
    p = root / "src" / "FishBroWFS_V2" / "stage0" / "ma_proxy.py"
    code = _read(p)
    tree = ast.parse(code)

    banned_prefixes = (
        "FishBroWFS_V2.engine",
        "FishBroWFS_V2.strategy",
        "FishBroWFS_V2.pipeline",
    )

    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for a in node.names:
                name = a.name
                assert not name.startswith(banned_prefixes), f"banned import: {name}"
        if isinstance(node, ast.ImportFrom):
            mod = node.module or ""
            assert not mod.startswith(banned_prefixes), f"banned import-from: {mod}"


def test_stage0_file_exists() -> None:
    root = Path(__file__).resolve().parent.parent
    p = root / "src" / "FishBroWFS_V2" / "stage0" / "ma_proxy.py"
    assert p.exists(), "Stage0 module must exist"




================================================================================
FILE: tests/test_stage0_ma_proxy.py
================================================================================

from __future__ import annotations

import numpy as np

from FishBroWFS_V2.stage0.ma_proxy import stage0_score_ma_proxy


def test_stage0_scores_shape_and_ordering_trend_series() -> None:
    # Simple upward trend: MA(5)-MA(20) should be mostly positive => positive score
    n = 500
    close = np.linspace(100.0, 200.0, n, dtype=np.float64)

    params = np.array(
        [
            [5.0, 20.0, 0.0],
            [20.0, 5.0, 0.0],  # inverted => should score worse
            [1.0, 2.0, 0.0],
        ],
        dtype=np.float64,
    )

    scores = stage0_score_ma_proxy(close, params)
    assert scores.shape == (3,)
    assert np.isfinite(scores[0])
    assert np.isfinite(scores[1])
    assert np.isfinite(scores[2])
    assert scores[0] > scores[1]


def test_stage0_rejects_invalid_lengths() -> None:
    close = np.linspace(100.0, 101.0, 50, dtype=np.float64)
    params = np.array([[0.0, 10.0], [10.0, 0.0], [1000.0, 5.0]], dtype=np.float64)
    scores = stage0_score_ma_proxy(close, params)
    assert scores.shape == (3,)
    assert scores[0] == -np.inf
    assert scores[1] == -np.inf
    assert scores[2] == -np.inf




================================================================================
FILE: tests/test_stage0_proxies.py
================================================================================

from __future__ import annotations

import numpy as np
import pytest

from FishBroWFS_V2.stage0.proxies import (
    activity_proxy,
    activity_proxy_nb,
    activity_proxy_py,
    trend_proxy,
    trend_proxy_nb,
    trend_proxy_py,
    vol_proxy,
    vol_proxy_nb,
    vol_proxy_py,
)

try:
    import numba as nb

    NUMBA_AVAILABLE = nb is not None
except Exception:
    NUMBA_AVAILABLE = False


def _generate_ohlc_trend(n: int, seed: int = 42) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Generate upward trend OHLC data."""
    rng = np.random.default_rng(seed)
    close = np.linspace(100.0, 200.0, n, dtype=np.float64)
    noise = rng.standard_normal(n) * 2.0
    close = close + noise
    high = close + np.abs(rng.standard_normal(n)) * 1.0
    low = close - np.abs(rng.standard_normal(n)) * 1.0
    open_ = (high + low) / 2 + rng.standard_normal(n) * 0.5
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    return open_, high, low, close


def _generate_ohlc_sine(n: int, seed: int = 999) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Generate oscillating (sine wave) OHLC data."""
    rng = np.random.default_rng(seed)
    t = np.linspace(0, 4 * np.pi, n)
    close = 100.0 + 20.0 * np.sin(t) + rng.standard_normal(n) * 1.0
    high = close + np.abs(rng.standard_normal(n)) * 1.0
    low = close - np.abs(rng.standard_normal(n)) * 1.0
    open_ = (high + low) / 2 + rng.standard_normal(n) * 0.5
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    return open_, high, low, close


# ============================================================================
# Parity Tests (nb vs py)
# ============================================================================


def test_trend_proxy_parity() -> None:
    """Test parity between Numba and Python versions of trend_proxy."""
    if not NUMBA_AVAILABLE:
        pytest.skip("Numba not available")

    open_, high, low, close = _generate_ohlc_trend(500, seed=42)

    # Generate random params
    rng = np.random.default_rng(123)
    n_params = 200
    params = np.empty((n_params, 2), dtype=np.float64)
    params[:, 0] = rng.integers(5, 50, size=n_params)  # fast
    params[:, 1] = rng.integers(20, 100, size=n_params)  # slow

    scores_nb = trend_proxy_nb(open_, high, low, close, params)
    scores_py = trend_proxy_py(open_, high, low, close, params)

    assert scores_nb.shape == scores_py.shape == (n_params,)

    # Check finite scores match
    finite_mask = np.isfinite(scores_py)
    assert np.all(np.isfinite(scores_py[finite_mask]))
    assert np.allclose(scores_nb[finite_mask], scores_py[finite_mask], rtol=0, atol=1e-12)

    # Check -inf matches
    inf_mask = ~finite_mask
    assert np.all(np.isinf(scores_nb[inf_mask]))
    assert np.all(np.isinf(scores_py[inf_mask]))


def test_vol_proxy_parity() -> None:
    """Test parity between Numba and Python versions of vol_proxy."""
    if not NUMBA_AVAILABLE:
        pytest.skip("Numba not available")

    open_, high, low, close = _generate_ohlc_trend(500, seed=42)

    # Generate random params
    rng = np.random.default_rng(456)
    n_params = 200
    params = np.empty((n_params, 2), dtype=np.float64)
    params[:, 0] = rng.integers(5, 50, size=n_params)  # atr_len
    params[:, 1] = rng.uniform(0.2, 1.5, size=n_params)  # stop_mult

    scores_nb = vol_proxy_nb(open_, high, low, close, params)
    scores_py = vol_proxy_py(open_, high, low, close, params)

    assert scores_nb.shape == scores_py.shape == (n_params,)

    finite_mask = np.isfinite(scores_py)
    assert np.all(np.isfinite(scores_py[finite_mask]))
    assert np.allclose(scores_nb[finite_mask], scores_py[finite_mask], rtol=0, atol=1e-12)

    inf_mask = ~finite_mask
    assert np.all(np.isinf(scores_nb[inf_mask]))
    assert np.all(np.isinf(scores_py[inf_mask]))


def test_activity_proxy_parity() -> None:
    """Test parity between Numba and Python versions of activity_proxy."""
    if not NUMBA_AVAILABLE:
        pytest.skip("Numba not available")

    open_, high, low, close = _generate_ohlc_trend(500, seed=42)

    # Generate random params
    rng = np.random.default_rng(789)
    n_params = 200
    params = np.empty((n_params, 1), dtype=np.float64)
    params[:, 0] = rng.integers(5, 50, size=n_params)  # channel_len

    scores_nb = activity_proxy_nb(open_, high, low, close, params)
    scores_py = activity_proxy_py(open_, high, low, close, params)

    assert scores_nb.shape == scores_py.shape == (n_params,)

    finite_mask = np.isfinite(scores_py)
    assert np.all(np.isfinite(scores_py[finite_mask]))
    # Activity proxy uses log1p, so allow slightly larger tolerance
    assert np.allclose(scores_nb[finite_mask], scores_py[finite_mask], rtol=0, atol=1e-10)

    inf_mask = ~finite_mask
    assert np.all(np.isinf(scores_nb[inf_mask]))
    assert np.all(np.isinf(scores_py[inf_mask]))


# ============================================================================
# Semantic Tests
# ============================================================================


def test_trend_proxy_sanity_upward_trend() -> None:
    """Test that upward trend produces positive trend_score."""
    open_, high, low, close = _generate_ohlc_trend(500, seed=42)

    # Good params: fast < slow, reasonable values
    params_good = np.array([[10.0, 30.0], [15.0, 50.0]], dtype=np.float64)
    scores_good = trend_proxy(open_, high, low, close, params_good)

    # Bad params: inverted (fast >= slow)
    params_bad = np.array([[30.0, 10.0], [50.0, 15.0]], dtype=np.float64)
    scores_bad = trend_proxy(open_, high, low, close, params_bad)

    assert np.all(np.isfinite(scores_good))
    assert np.all(np.isfinite(scores_bad))

    # Good params should score better (or at least not worse) than inverted
    # In upward trend, fast < slow should give positive score
    assert scores_good[0] > 0.0 or scores_good[1] > 0.0


def test_activity_proxy_sanity_oscillation_vs_trend() -> None:
    """Test that oscillating sequence has higher activity than trend."""
    # Generate oscillating data
    open_sine, high_sine, low_sine, close_sine = _generate_ohlc_sine(500, seed=999)
    # Generate trend data
    open_trend, high_trend, low_trend, close_trend = _generate_ohlc_trend(500, seed=42)

    # Same params for both (channel_len only)
    params = np.array([[10.0], [15.0]], dtype=np.float64)

    scores_sine = activity_proxy(open_sine, high_sine, low_sine, close_sine, params)
    scores_trend = activity_proxy(open_trend, high_trend, low_trend, close_trend, params)

    assert np.all(np.isfinite(scores_sine))
    assert np.all(np.isfinite(scores_trend))

    # Oscillating sequence should have higher activity (more breakout triggers)
    assert np.mean(scores_sine) > np.mean(scores_trend)


def test_vol_proxy_sanity_positive_scores() -> None:
    """Test that vol_proxy returns finite scores for valid params."""
    open_, high, low, close = _generate_ohlc_trend(500, seed=42)

    params = np.array([[10.0, 0.5], [20.0, 1.0], [30.0, 1.5]], dtype=np.float64)  # [atr_len, stop_mult]
    scores = vol_proxy(open_, high, low, close, params)

    assert np.all(np.isfinite(scores))
    # Vol proxy scores are negative (-log1p(stop_mean)), but finite
    assert np.all(scores <= 0.0)  # Scores are negative (closer to 0 is better)


def test_proxies_reject_invalid_params() -> None:
    """Test that all proxies return -inf for invalid params."""
    open_, high, low, close = _generate_ohlc_trend(100, seed=42)

    # Invalid: too large
    params_invalid = np.array([[1000.0, 2000.0]], dtype=np.float64)

    scores_trend = trend_proxy(open_, high, low, close, params_invalid)
    params_activity_invalid = np.array([[1000.0]], dtype=np.float64)
    scores_activity = activity_proxy(open_, high, low, close, params_activity_invalid)

    assert np.all(np.isinf(scores_trend))
    assert np.all(np.isinf(scores_activity))
    assert np.all(scores_trend < 0)
    assert np.all(scores_activity < 0)

    # Invalid: zero or negative
    params_invalid2 = np.array([[0.0, 10.0], [-5.0, 10.0]], dtype=np.float64)

    scores_trend2 = trend_proxy(open_, high, low, close, params_invalid2)
    params_activity_invalid2 = np.array([[0.0], [-5.0]], dtype=np.float64)
    scores_activity2 = activity_proxy(open_, high, low, close, params_activity_invalid2)

    assert np.all(np.isinf(scores_trend2))
    assert np.all(np.isinf(scores_activity2))

    # Vol proxy: invalid
    params_vol_invalid = np.array([[1000.0, 0.5], [500.0, -1.0]], dtype=np.float64)  # [atr_len, stop_mult]
    scores_vol = vol_proxy(open_, high, low, close, params_vol_invalid)
    assert np.all(np.isinf(scores_vol))


================================================================================
FILE: tests/test_stage0_proxy_rank_corr.py
================================================================================

from __future__ import annotations

import os

import numpy as np
import pytest

from FishBroWFS_V2.pipeline.metrics_schema import (
    METRICS_COL_MAX_DD,
    METRICS_COL_NET_PROFIT,
    METRICS_COL_TRADES,
    METRICS_COLUMN_NAMES,
)
from FishBroWFS_V2.pipeline.runner_grid import run_grid
from FishBroWFS_V2.stage0.proxies import activity_proxy, trend_proxy, vol_proxy

try:
    import numba as nb
except Exception:
    nb = None  # type: ignore


def _rankdata(x: np.ndarray) -> np.ndarray:
    """
    Compute ranks for Spearman correlation (handles ties with average rank).

    Args:
        x: 1D array

    Returns:
        ranks: 1D array of ranks (1-indexed, ties get average rank)
    """
    n = x.shape[0]
    if n == 0:
        return np.empty(0, dtype=np.float64)

    # Get sorted indices
    sorted_indices = np.argsort(x, kind="stable")

    # Compute ranks
    ranks = np.empty(n, dtype=np.float64)
    i = 0
    while i < n:
        # Find all values equal to current value
        j = i
        while j < n - 1 and x[sorted_indices[j]] == x[sorted_indices[j + 1]]:
            j += 1

        # Average rank for this group
        avg_rank = (i + j + 2) / 2.0  # +2 because ranks are 1-indexed

        # Assign ranks
        for k in range(i, j + 1):
            ranks[sorted_indices[k]] = avg_rank

        i = j + 1

    return ranks


def _pearson_corr(x: np.ndarray, y: np.ndarray) -> float:
    """
    Compute Pearson correlation coefficient.

    Args:
        x, y: 1D arrays of same length

    Returns:
        correlation coefficient
    """
    n = x.shape[0]
    if n == 0 or n != y.shape[0]:
        raise ValueError("x and y must have same non-zero length")

    # Compute means
    mx = np.mean(x)
    my = np.mean(y)

    # Compute covariance and variances
    cov = np.sum((x - mx) * (y - my))
    var_x = np.sum((x - mx) ** 2)
    var_y = np.sum((y - my) ** 2)

    # Handle degenerate cases
    if var_x == 0.0 or var_y == 0.0:
        return 0.0

    return cov / np.sqrt(var_x * var_y)


def spearman_corr(x: np.ndarray, y: np.ndarray) -> float:
    """
    Compute Spearman rank correlation coefficient.

    Args:
        x, y: 1D arrays of same length

    Returns:
        Spearman correlation coefficient (rho)
    """
    rx = _rankdata(x)
    ry = _rankdata(y)
    return _pearson_corr(rx, ry)


def _generate_ohlc_for_corr(n: int, seed: int = 42) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Generate OHLC data with regime-switch + jumps for reliable breakout opportunities.
    
    Design:
    - Regime switches every ~250 bars: trending-up, trending-down, mean-reverting/chop
    - Gaussian noise with increased variance
    - Occasional jumps (p=0.01, Â±(2~4)*sigma shock)
    - Ensures high/low have clear intrabar range
    """
    rng = np.random.default_rng(seed)
    base_price = 100.0
    regime_period = 250
    
    # Generate regime sequence (0=trend-up, 1=trend-down, 2=chop)
    n_regimes = (n + regime_period - 1) // regime_period
    regime_seed = seed + 10000
    regime_rng = np.random.default_rng(regime_seed)
    regimes = regime_rng.integers(0, 3, size=n_regimes)
    
    # Generate close series
    close = np.empty(n, dtype=np.float64)
    close[0] = base_price
    
    sigma_base = 3.0  # Base noise sigma
    jump_prob = 0.01
    
    for t in range(1, n):
        regime_idx = t // regime_period
        regime = regimes[regime_idx] if regime_idx < len(regimes) else regimes[-1]
        
        # Trend component based on regime
        if regime == 0:  # Trending up
            trend_component = 0.05
        elif regime == 1:  # Trending down
            trend_component = -0.05
        else:  # Chop/mean-reverting
            trend_component = -0.01 * (close[t-1] - base_price) / 10.0
        
        # Gaussian noise
        noise = rng.standard_normal() * sigma_base
        
        # Occasional jump
        if rng.random() < jump_prob:
            jump_magnitude = rng.uniform(2.0, 4.0) * sigma_base
            jump_sign = 1.0 if rng.random() < 0.5 else -1.0
            noise += jump_sign * jump_magnitude
        
        close[t] = close[t-1] + trend_component + noise
    
    # Generate open (prev close with small gap)
    open_ = np.empty(n, dtype=np.float64)
    open_[0] = base_price
    for t in range(1, n):
        gap = rng.standard_normal() * 0.5
        open_[t] = close[t-1] + gap
    
    # Generate high/low with intrabar range
    high = np.empty(n, dtype=np.float64)
    low = np.empty(n, dtype=np.float64)
    base_range = 1.0
    
    for t in range(n):
        # Intrabar range based on noise magnitude
        noise_mag = abs(rng.standard_normal())
        intrabar_range = noise_mag * 2.0 + base_range
        
        # Ensure high >= max(open, close) and low <= min(open, close)
        max_oc = max(open_[t], close[t])
        min_oc = min(open_[t], close[t])
        
        high[t] = max_oc + intrabar_range * 0.5
        low[t] = min_oc - intrabar_range * 0.5
    
    return open_, high, low, close


@pytest.mark.slow
def test_stage0_proxy_spearman_correlation() -> None:
    """
    Test that Stage0 proxy scores have median Spearman Ï â‰¥ 0.4 with actual PnL.

    This test:
    1. Runs all seeds and computes rho for each non-degenerate seed
    2. Collects all rho values into a list
    3. Uses median rho as the contract (more stable than mean)
    4. Degenerate seeds are skipped but recorded for diagnostics
    5. If all seeds are degenerate, test fails with diagnostic info
    """
    # JIT requirement check: avoid degenerate samples in CI-safe / no-jit environments
    numba_disable_jit_env = os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1"
    numba_disable_jit_config = False
    if nb is not None:
        numba_disable_jit_config = getattr(nb.config, "DISABLE_JIT", 0) == 1

    if numba_disable_jit_env or numba_disable_jit_config:
        pytest.skip(
            "Spearman correlation test requires JIT-enabled Stage2; run without NUMBA_DISABLE_JIT=1\n"
            "Suggested command: PYTHONDONTWRITEBYTECODE=1 pytest -q -m slow -k spearman -vv"
        )

    SEEDS = [0, 1, 2, 3, 4, 5, 6, 7]
    MAX_TRIES = len(SEEDS)
    MIN_VALID = 4  # Hard gate: require at least 4 valid seeds
    n_bars = 1500
    n_params = 250

    # Track evidence for all seeds (including degenerate)
    seeds_tried = []
    pnl_unique_counts = []
    pnl_mins = []
    pnl_maxs = []
    trades_totals = []
    trades_unique_counts = []
    intent_modes = []
    intents_totals = []
    fills_totals = []
    # Collect rho values for non-degenerate seeds
    rho_values = []
    degenerate_seeds = []
    valid_seeds = []

    for seed in SEEDS:
        seeds_tried.append(seed)

        # Generate OHLC data with current seed
        open_, high, low, close = _generate_ohlc_for_corr(n_bars, seed=seed)

        # Generate random params with deterministic seed (seed + 1000 to avoid collision)
        rng = np.random.default_rng(seed + 1000)

        # Params for kernel: [channel_len, atr_len, stop_mult]
        params_kernel = np.empty((n_params, 3), dtype=np.float64)
        params_kernel[:, 0] = rng.integers(5, 40, size=n_params)  # channel_len (reduced range)
        params_kernel[:, 1] = rng.integers(5, 50, size=n_params)  # atr_len
        params_kernel[:, 2] = rng.uniform(0.2, 1.5, size=n_params)  # stop_mult (reduced range)

        # Params for proxies (aligned with Stage2 kernel params)
        # Trend: [fast, slow] where fast = max(2, floor(channel_len/3)), slow = channel_len
        params_trend = np.empty((n_params, 2), dtype=np.float64)
        params_trend[:, 0] = np.maximum(2, params_kernel[:, 0] // 3)  # fast
        params_trend[:, 1] = params_kernel[:, 0]  # slow = channel_len
        # Activity: [channel_len, atr_len] (atr_len kept for compatibility but not used)
        params_activity = params_kernel[:, :2].copy()
        # Vol: [atr_len, stop_mult]
        params_vol = params_kernel[:, 1:3].copy()

        # Compute proxy scores
        trend_scores = trend_proxy(open_, high, low, close, params_trend)
        vol_scores = vol_proxy(open_, high, low, close, params_vol)
        activity_scores = activity_proxy(open_, high, low, close, params_activity)

        # Filter out -inf scores (invalid params)
        valid_mask = np.isfinite(trend_scores) & np.isfinite(vol_scores) & np.isfinite(activity_scores)

        # Combined proxy score (weights: w1=1.0, w2=0.5, w3=1.0)
        # Adjusted weights: emphasize activity (often strongest for breakout strategies)
        proxy_scores = 1.0 * trend_scores + 0.5 * vol_scores + 1.0 * activity_scores

        # Run minimal backtest to get PnL
        from FishBroWFS_V2.pipeline.runner_grid import run_grid

        result = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_kernel,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=False,
            force_close_last=True,
        )

        metrics = result["metrics"]
        pnl = metrics[:, METRICS_COL_NET_PROFIT]  # net_profit column
        trades = metrics[:, METRICS_COL_TRADES]  # trades column

        # Extract perf diagnostic info
        perf = result.get("perf", {})
        intent_mode = perf.get("intent_mode")
        intents_total = perf.get("intents_total")
        fills_total = perf.get("fills_total")

        # Strict diagnostics when trades_sum == 0 (fills exist but trades/pnl = 0)
        trades_sum = float(np.sum(trades))
        if trades_sum == 0.0:
            # Dump metrics diagnostics
            diag_parts = [f"\n[DIAG] seed={seed}: trades_sum=0 but fills_total={fills_total}"]
            diag_parts.append(f"metrics.shape={metrics.shape}")
            diag_parts.append(f"metrics_column_names={METRICS_COLUMN_NAMES}")
            diag_parts.append(f"result.keys()={list(result.keys())}")
            if "metrics_columns" in result:
                diag_parts.append(f"result['metrics_columns']={result.get('metrics_columns')}")

            # First row of metrics
            if metrics.shape[0] > 0:
                n_cols_to_show = min(10, metrics.shape[1])
                diag_parts.append(f"metrics[0, :{n_cols_to_show}]={metrics[0, :n_cols_to_show].tolist()}")

            # Min/max of first few columns (with column names)
            n_cols_to_check = min(5, metrics.shape[1])
            for col_idx in range(n_cols_to_check):
                col_data = metrics[:, col_idx]
                col_name = METRICS_COLUMN_NAMES[col_idx] if col_idx < len(METRICS_COLUMN_NAMES) else f"col{col_idx}"
                diag_parts.append(
                    f"metrics[:, {col_idx}] ({col_name}): min={np.min(col_data):.6f}, max={np.max(col_data):.6f}"
                )

            # Inspect fills payload
            if "fills" in result:
                fills_list = result["fills"]
                if isinstance(fills_list, list):
                    diag_parts.append(f"fills (list): len={len(fills_list)}")
                    if len(fills_list) > 0:
                        diag_parts.append(f"fills[0]={repr(fills_list[0])} (type={type(fills_list[0])})")
                    if len(fills_list) > 1:
                        diag_parts.append(f"fills[1]={repr(fills_list[1])}")
                    if len(fills_list) > 2:
                        diag_parts.append(f"fills[2]={repr(fills_list[2])}")
            elif "fills_arr" in result:
                fills_arr = result["fills_arr"]
                diag_parts.append(f"fills_arr: shape={fills_arr.shape}, dtype={fills_arr.dtype}")
                if fills_arr.shape[0] > 0:
                    n_rows = min(5, fills_arr.shape[0])
                    diag_parts.append(f"fills_arr[:{n_rows}]=\n{fills_arr[:n_rows]}")
            elif "fills_array" in result:
                fills_array = result["fills_array"]
                diag_parts.append(f"fills_array: shape={fills_array.shape}, dtype={fills_array.dtype}")
                if fills_array.shape[0] > 0:
                    n_rows = min(5, fills_array.shape[0])
                    diag_parts.append(f"fills_array[:{n_rows}]=\n{fills_array[:n_rows]}")
            else:
                diag_parts.append("No 'fills', 'fills_arr', or 'fills_array' in result (perf only)")

            # Print diagnostics to stderr for visibility
            import sys

            print("\n".join(diag_parts), file=sys.stderr)

        # Check for degenerate cases
        pnl_unique = np.unique(pnl)
        pnl_unique_count = pnl_unique.size
        pnl_std = np.std(pnl)
        proxy_std = np.std(proxy_scores)

        # Record evidence (including perf diagnostics)
        pnl_unique_counts.append(pnl_unique_count)
        pnl_mins.append(float(np.min(pnl)))
        pnl_maxs.append(float(np.max(pnl)))
        trades_totals.append(float(np.sum(trades)))
        trades_unique_counts.append(np.unique(trades).size)
        intent_modes.append(intent_mode)
        intents_totals.append(intents_total)
        fills_totals.append(fills_total)

        # Check if this sample is degenerate and compute rho if non-degenerate
        is_degenerate = False
        if proxy_std == 0.0:
            is_degenerate = True
        elif pnl_unique_count < 2 or pnl_std == 0.0:
            is_degenerate = True
        else:
            # Filter out invalid proxy scores (-inf)
            # Combine proxy valid_mask with pnl finite check
            valid_mask_combined = valid_mask & np.isfinite(pnl)
            if np.sum(valid_mask_combined) < 10:
                is_degenerate = True
            else:
                proxy_valid = proxy_scores[valid_mask_combined]
                pnl_valid = pnl[valid_mask_combined]

                # Check again after filtering
                if np.std(pnl_valid) == 0.0 or np.unique(pnl_valid).size < 2:
                    is_degenerate = True
                else:
                    # Non-degenerate sample - compute Spearman correlation
                    rho = spearman_corr(proxy_valid, pnl_valid)
                    rho_values.append(rho)
                    valid_seeds.append(seed)
                    # Continue to next seed (collect all rho values)

        if is_degenerate:
            degenerate_seeds.append(seed)
            # Continue to next seed (skip degenerate, but diagnostics already recorded)

    # Check minimum valid seeds requirement
    if len(rho_values) < MIN_VALID:
        # Build detailed diagnostic message with per-seed info
        diag_lines = [
            f"Insufficient valid seeds: {len(rho_values)}/{MAX_TRIES} < MIN_VALID={MIN_VALID}",
            f"Valid seeds: {valid_seeds}",
            f"Degenerate seeds: {degenerate_seeds}",
            "",
            "Per-seed summary:",
        ]
        for i, seed in enumerate(seeds_tried):
            is_valid = seed in valid_seeds
            diag_lines.append(
                f"seed={seed} ({'VALID' if is_valid else 'DEGENERATE'}): "
                f"intent_mode={intent_modes[i]}, "
                f"intents_total={intents_totals[i]}, "
                f"fills_total={fills_totals[i]}, "
                f"trades_sum={trades_totals[i]}, "
                f"pnl_unique={pnl_unique_counts[i]}, "
                f"pnl_range=[{pnl_mins[i]:.4f}, {pnl_maxs[i]:.4f}], "
                f"trades_unique={trades_unique_counts[i]}"
            )
        if len(rho_values) > 0:
            diag_lines.append(f"rho_values (partial): {rho_values}")
        pytest.fail("\n".join(diag_lines))

    # Compute median and mean rho
    median_rho = float(np.median(rho_values))
    mean_rho = float(np.mean(rho_values))

    # Assert correlation contract using median (more stable than mean)
    # Only assert if we have enough valid seeds (already checked above)
    assert median_rho >= 0.4, (
        f"Median Spearman correlation {median_rho:.4f} < 0.4 threshold. "
        f"Mean rho={mean_rho:.4f}, "
        f"rho_values={rho_values}, "
        f"valid_seeds={valid_seeds} ({len(rho_values)}/{MAX_TRIES}), "
        f"degenerate_seeds={degenerate_seeds}"
    )


def test_spearman_corr_basic() -> None:
    """Basic test for Spearman correlation function."""
    # Perfect positive correlation
    x = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
    y = np.array([2.0, 4.0, 6.0, 8.0, 10.0])
    rho = spearman_corr(x, y)
    assert abs(rho - 1.0) < 1e-10

    # Perfect negative correlation
    y_neg = np.array([10.0, 8.0, 6.0, 4.0, 2.0])
    rho_neg = spearman_corr(x, y_neg)
    assert abs(rho_neg - (-1.0)) < 1e-10

    # No correlation (random)
    rng = np.random.default_rng(42)
    y_rand = rng.standard_normal(100)
    x_rand = rng.standard_normal(100)
    rho_rand = spearman_corr(x_rand, y_rand)
    assert abs(rho_rand) < 0.5  # Should be close to 0 for independent data


def test_spearman_corr_with_ties() -> None:
    """Test Spearman correlation with tied values."""
    # Test with ties
    x = np.array([1.0, 2.0, 2.0, 3.0, 4.0])
    y = np.array([2.0, 3.0, 4.0, 5.0, 6.0])
    rho = spearman_corr(x, y)
    # Should still be positive
    assert rho > 0.0

    # All same values (degenerate)
    x_same = np.array([1.0, 1.0, 1.0])
    y_same = np.array([2.0, 2.0, 2.0])
    rho_same = spearman_corr(x_same, y_same)
    # Should handle gracefully (0 or NaN)
    assert np.isfinite(rho_same) or np.isnan(rho_same)


================================================================================
FILE: tests/test_stage2_params_influence.py
================================================================================

from __future__ import annotations

import numpy as np
import pytest

from FishBroWFS_V2.pipeline.runner_grid import run_grid
from tests.test_stage0_proxy_rank_corr import _generate_ohlc_for_corr


def test_stage2_params_influence_extremes() -> None:
    """
    Contract test: params must influence outcome.
    
    Root cause fuse: if different params produce identical metrics,
    Stage2 is broken and Spearman correlation will be meaningless.
    """
    # Generate OHLC data using same generator as Spearman test
    n_bars = 1500
    seed = 0
    open_, high, low, close = _generate_ohlc_for_corr(n_bars, seed=seed)
    
    # Two extreme params that should produce different outcomes
    params = np.array([
        [5.0, 5.0, 0.2],   # A: short channel, short ATR, tight stop
        [39.0, 49.0, 1.5], # B: long channel, long ATR, wide stop
    ], dtype=np.float64)
    
    # Run grid with debug enabled
    result = run_grid(
        open_=open_,
        high=high,
        low=low,
        close=close,
        params_matrix=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        sort_params=False,
        force_close_last=True,
        return_debug=True,
    )
    
    metrics = result["metrics"]
    debug_fills_first = result.get("debug_fills_first")
    
    # Extract metrics for both params
    net_profit_a = float(metrics[0, 0])  # net_profit
    net_profit_b = float(metrics[1, 0])
    trades_a = int(metrics[0, 1])  # trades
    trades_b = int(metrics[1, 1])
    
    # Extract debug info
    if debug_fills_first is not None:
        entry_bar_a = int(debug_fills_first[0, 0])
        entry_price_a = float(debug_fills_first[0, 1])
        exit_bar_a = int(debug_fills_first[0, 2])
        exit_price_a = float(debug_fills_first[0, 3])
        
        entry_bar_b = int(debug_fills_first[1, 0])
        entry_price_b = float(debug_fills_first[1, 1])
        exit_bar_b = int(debug_fills_first[1, 2])
        exit_price_b = float(debug_fills_first[1, 3])
        
        debug_msg = (
            f"Param A [5, 5, 0.2]: entry_bar={entry_bar_a}, entry_price={entry_price_a:.4f}, "
            f"exit_bar={exit_bar_a}, exit_price={exit_price_a:.4f}, "
            f"net_profit={net_profit_a:.4f}, trades={trades_a}\n"
            f"Param B [39, 49, 1.5]: entry_bar={entry_bar_b}, entry_price={entry_price_b:.4f}, "
            f"exit_bar={exit_bar_b}, exit_price={exit_price_b:.4f}, "
            f"net_profit={net_profit_b:.4f}, trades={trades_b}"
        )
    else:
        debug_msg = (
            f"Param A [5, 5, 0.2]: net_profit={net_profit_a:.4f}, trades={trades_a}\n"
            f"Param B [39, 49, 1.5]: net_profit={net_profit_b:.4f}, trades={trades_b}"
        )
    
    # Assert at least one difference exists
    # This is the "root cause fuse" - if all identical, Stage2 is broken
    assert (
        entry_bar_a != entry_bar_b or
        abs(entry_price_a - entry_price_b) > 1e-6 or
        exit_bar_a != exit_bar_b or
        abs(exit_price_a - exit_price_b) > 1e-6 or
        abs(net_profit_a - net_profit_b) > 1e-6
    ), (
        f"Params A and B produced identical outcomes - Stage2 is broken!\n"
        f"{debug_msg}\n"
        f"This indicates params are not being used correctly in signal/stop calculation."
    )


================================================================================
FILE: tests/test_vectorization_parity.py
================================================================================

from __future__ import annotations

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.engine_jit import simulate_arrays
from FishBroWFS_V2.engine.types import Fill, OrderKind, OrderRole, Side
from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, run_kernel_arrays, run_kernel_object_mode


def _assert_fills_equal(a: list[Fill], b: list[Fill]) -> None:
    assert len(a) == len(b)
    for fa, fb in zip(a, b):
        assert fa.bar_index == fb.bar_index
        assert fa.role == fb.role
        assert fa.kind == fb.kind
        assert fa.side == fb.side
        assert fa.qty == fb.qty
        assert fa.order_id == fb.order_id
        assert abs(fa.price - fb.price) <= 1e-9


def test_strategy_object_vs_array_mode_parity() -> None:
    rng = np.random.default_rng(42)
    n = 300
    close = 100.0 + np.cumsum(rng.standard_normal(n)).astype(np.float64)
    high = close + 1.0
    low = close - 1.0
    open_ = (high + low) * 0.5

    bars = normalize_bars(open_, high, low, close)
    params = DonchianAtrParams(channel_len=20, atr_len=14, stop_mult=2.0)

    out_obj = run_kernel_object_mode(bars, params, commission=0.0, slip=0.0, order_qty=1)
    out_arr = run_kernel_arrays(bars, params, commission=0.0, slip=0.0, order_qty=1)

    _assert_fills_equal(out_obj["fills"], out_arr["fills"])  # type: ignore[arg-type]


def test_simulate_arrays_same_bar_entry_exit_parity() -> None:
    # Construct a same-bar entry then exit scenario (created_bar=-1 activates on bar0).
    bars = normalize_bars(
        np.array([100.0], dtype=np.float64),
        np.array([120.0], dtype=np.float64),
        np.array([80.0], dtype=np.float64),
        np.array([110.0], dtype=np.float64),
    )

    # ENTRY BUY STOP 105, EXIT SELL STOP 95, both active on bar0.
    order_id = np.array([1, 2], dtype=np.int64)
    created_bar = np.array([-1, -1], dtype=np.int64)
    role = np.array([1, 0], dtype=np.int8)  # ENTRY then EXIT (order_id tie-break handles)
    kind = np.array([0, 0], dtype=np.int8)  # STOP
    side = np.array([1, -1], dtype=np.int8)  # BUY, SELL
    price = np.array([105.0, 95.0], dtype=np.float64)
    qty = np.array([1, 1], dtype=np.int64)

    fills = simulate_arrays(
        bars,
        order_id=order_id,
        created_bar=created_bar,
        role=role,
        kind=kind,
        side=side,
        price=price,
        qty=qty,
        ttl_bars=1,
    )

    assert len(fills) == 2
    assert fills[0].role == OrderRole.ENTRY and fills[0].side == Side.BUY and fills[0].kind == OrderKind.STOP
    assert fills[1].role == OrderRole.EXIT and fills[1].side == Side.SELL and fills[1].kind == OrderKind.STOP



