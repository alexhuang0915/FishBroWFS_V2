================================================================================
FishBroWFS_V2 Release Package
Generated: 2025-12-19 01:00:45
================================================================================

DIRECTORY STRUCTURE
--------------------------------------------------------------------------------
FishBroWFS_V2/
    â”œâ”€â”€ GM_Huang/
    â”‚   â”œâ”€â”€ clean_repo_caches.py
    â”‚   â””â”€â”€ release_tool.py
    â”œâ”€â”€ configs/
    â”œâ”€â”€ docs/
    â”œâ”€â”€ outputs/
    â”‚   â”œâ”€â”€ default/
    â”‚   â”‚   â””â”€â”€ e6961792-ad83-4211-b65c-3821685eceb0/
    â”‚   â”‚       â””â”€â”€ logs/
    â”‚   â”œâ”€â”€ demo_20251218T163544Z/
    â”‚   â”œâ”€â”€ demo_20251218T163823Z/
    â”‚   â”œâ”€â”€ seasons/
    â”‚   â”‚   â””â”€â”€ 2026Q1/
    â”‚   â”‚       â”œâ”€â”€ governance/
    â”‚   â”‚       â”‚   â”œâ”€â”€ gov-20251218T095734Z-8f765e66/
    â”‚   â”‚       â”‚   â””â”€â”€ gov-20251218T121312Z-b22918b2/
    â”‚   â”‚       â””â”€â”€ runs/
    â”‚   â”‚           â”œâ”€â”€ stage0_coarse-20251218T093116Z-9a59d4a5/
    â”‚   â”‚           â”œâ”€â”€ stage0_coarse-20251218T093251Z-051a016e/
    â”‚   â”‚           â”œâ”€â”€ stage0_coarse-20251218T093257Z-471e326c/
    â”‚   â”‚           â”œâ”€â”€ stage0_coarse-20251218T093512Z-d3caa754/
    â”‚   â”‚           â”œâ”€â”€ stage1_topk-20251218T093512Z-943e749a/
    â”‚   â”‚           â””â”€â”€ stage2_confirm-20251218T093513Z-354cee6b/
    â”‚   â””â”€â”€ test_season/
    â”‚       â”œâ”€â”€ 00e23de3-4585-41c3-8673-593e23c4a9c4/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 06a339ae-8b5f-4935-ad44-c6504c4d07f9/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 0ee1dbe1-245d-44f4-8e43-3bd137e6f102/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 16f7ccb9-63bb-4f6e-9e93-e593e3132e0b/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 1ac8c7a3-c6fa-4f1e-a319-f6c5e397b04f/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 25dd660d-857b-4755-8233-7835e0095a33/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 2cf0024c-f949-4bee-80b9-7d996bf3364b/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 3c9da358-8313-49aa-a972-d34f98b36312/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 3ec6287e-4a68-4cbb-8838-63e2b4baaa0d/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 41003a18-a2a4-47af-bd47-de1f434b6dc7/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 419f7c4a-792f-4d56-9ec1-c30425ef4baf/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 4c577332-2fa5-40f6-b272-1c07b7a97063/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 4dc43aed-ddce-48c2-958c-f4786fd5ea60/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 5792f274-f865-4ddb-96cc-a584de623dde/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 61c8e9ff-8675-47e6-9764-c988b0fa49ff/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 64fa6f81-d2d5-488d-82f0-25a763e33ff6/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 876c80ff-d68d-4080-88fc-01bcc8b3e648/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ 99696f3b-d612-4c31-a69a-9b85f3bcdfa3/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ a767de80-1c76-40af-aff1-c3338586adb7/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ b9b74fc2-8ee5-4a7c-9d80-b77739f70cf5/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ beba7569-b592-4425-a831-c1d09361dde7/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ c2fbc7b4-8003-401f-b83c-01cf538356ce/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ c9b1e589-93da-4434-a9a0-02e314718160/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ e1251f92-eed8-4d76-b6f8-4b9d471856fd/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ e51d83ec-44eb-4c59-8e9b-0de69e6ffa4a/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â”œâ”€â”€ f13cabc2-49d2-44c8-9783-6a4839370403/
    â”‚       â”‚   â””â”€â”€ logs/
    â”‚       â””â”€â”€ f70358b9-73f2-4362-b8bf-f6fe672161b7/
    â”‚           â””â”€â”€ logs/
    â”œâ”€â”€ scripts/
    â”‚   â”œâ”€â”€ perf_direct.py
    â”‚   â”œâ”€â”€ perf_grid.py
    â”‚   â”œâ”€â”€ run_funnel.py
    â”‚   â”œâ”€â”€ run_governance.py
    â”‚   â””â”€â”€ upgrade_winners_v2.py
    â”œâ”€â”€ src/
    â”‚   â””â”€â”€ FishBroWFS_V2/
    â”‚       â”œâ”€â”€ config/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ constants.py
    â”‚       â”‚   â””â”€â”€ dtypes.py
    â”‚       â”œâ”€â”€ control/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ api.py
    â”‚       â”‚   â”œâ”€â”€ app_nicegui.py
    â”‚       â”‚   â”œâ”€â”€ jobs_db.py
    â”‚       â”‚   â”œâ”€â”€ paths.py
    â”‚       â”‚   â”œâ”€â”€ preflight.py
    â”‚       â”‚   â”œâ”€â”€ report_links.py
    â”‚       â”‚   â”œâ”€â”€ seed_demo_run.py
    â”‚       â”‚   â”œâ”€â”€ types.py
    â”‚       â”‚   â”œâ”€â”€ worker.py
    â”‚       â”‚   â””â”€â”€ worker_main.py
    â”‚       â”œâ”€â”€ core/
    â”‚       â”‚   â”œâ”€â”€ schemas/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ governance.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ manifest.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ oom_gate.py
    â”‚       â”‚   â”‚   â””â”€â”€ winners_v2.py
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ artifact_reader.py
    â”‚       â”‚   â”œâ”€â”€ artifact_status.py
    â”‚       â”‚   â”œâ”€â”€ artifacts.py
    â”‚       â”‚   â”œâ”€â”€ audit_schema.py
    â”‚       â”‚   â”œâ”€â”€ config_hash.py
    â”‚       â”‚   â”œâ”€â”€ config_snapshot.py
    â”‚       â”‚   â”œâ”€â”€ governance_schema.py
    â”‚       â”‚   â”œâ”€â”€ governance_writer.py
    â”‚       â”‚   â”œâ”€â”€ oom_cost_model.py
    â”‚       â”‚   â”œâ”€â”€ oom_gate.py
    â”‚       â”‚   â”œâ”€â”€ paths.py
    â”‚       â”‚   â”œâ”€â”€ run_id.py
    â”‚       â”‚   â”œâ”€â”€ winners_builder.py
    â”‚       â”‚   â””â”€â”€ winners_schema.py
    â”‚       â”œâ”€â”€ data/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â””â”€â”€ layout.py
    â”‚       â”œâ”€â”€ engine/
    â”‚       â”‚   â”œâ”€â”€ kernels/
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ cursor_kernel.py
    â”‚       â”‚   â”‚   â””â”€â”€ reference_kernel.py
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ constants.py
    â”‚       â”‚   â”œâ”€â”€ constitution.py
    â”‚       â”‚   â”œâ”€â”€ engine_jit.py
    â”‚       â”‚   â”œâ”€â”€ matcher_core.py
    â”‚       â”‚   â”œâ”€â”€ metrics_from_fills.py
    â”‚       â”‚   â”œâ”€â”€ order_id.py
    â”‚       â”‚   â”œâ”€â”€ simulate.py
    â”‚       â”‚   â””â”€â”€ types.py
    â”‚       â”œâ”€â”€ gui/
    â”‚       â”‚   â”œâ”€â”€ viewer/
    â”‚       â”‚   â”‚   â”œâ”€â”€ components/
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ evidence_panel.py
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ kpi_table.py
    â”‚       â”‚   â”‚   â”‚   â””â”€â”€ status_bar.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ pages/
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ artifacts.py
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ governance.py
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ kpi.py
    â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ overview.py
    â”‚       â”‚   â”‚   â”‚   â””â”€â”€ winners.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ app.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ json_pointer.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ kpi_registry.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ load_state.py
    â”‚       â”‚   â”‚   â”œâ”€â”€ page_scaffold.py
    â”‚       â”‚   â”‚   â””â”€â”€ schema.py
    â”‚       â”‚   â””â”€â”€ __init__.py
    â”‚       â”œâ”€â”€ indicators/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â””â”€â”€ numba_indicators.py
    â”‚       â”œâ”€â”€ perf/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ cost_model.py
    â”‚       â”‚   â”œâ”€â”€ profile_report.py
    â”‚       â”‚   â”œâ”€â”€ scenario_control.py
    â”‚       â”‚   â””â”€â”€ timers.py
    â”‚       â”œâ”€â”€ pipeline/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ funnel.py
    â”‚       â”‚   â”œâ”€â”€ funnel_plan.py
    â”‚       â”‚   â”œâ”€â”€ funnel_runner.py
    â”‚       â”‚   â”œâ”€â”€ funnel_schema.py
    â”‚       â”‚   â”œâ”€â”€ governance_eval.py
    â”‚       â”‚   â”œâ”€â”€ metrics_schema.py
    â”‚       â”‚   â”œâ”€â”€ param_sort.py
    â”‚       â”‚   â”œâ”€â”€ runner_adapter.py
    â”‚       â”‚   â”œâ”€â”€ runner_grid.py
    â”‚       â”‚   â”œâ”€â”€ stage0_runner.py
    â”‚       â”‚   â”œâ”€â”€ stage2_runner.py
    â”‚       â”‚   â””â”€â”€ topk.py
    â”‚       â”œâ”€â”€ stage0/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ ma_proxy.py
    â”‚       â”‚   â””â”€â”€ proxies.py
    â”‚       â”œâ”€â”€ strategy/
    â”‚       â”‚   â”œâ”€â”€ __init__.py
    â”‚       â”‚   â”œâ”€â”€ entry_builder_nb.py
    â”‚       â”‚   â”œâ”€â”€ kernel.py
    â”‚       â”‚   â””â”€â”€ runner_single.py
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â””â”€â”€ version.py
    â””â”€â”€ tests/
        â”œâ”€â”€ fixtures/
        â”‚   â””â”€â”€ artifacts/
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ conftest.py
        â”œâ”€â”€ test_artifact_contract.py
        â”œâ”€â”€ test_artifacts_winners_v2_written.py
        â”œâ”€â”€ test_audit_schema_contract.py
        â”œâ”€â”€ test_b5_query_params.py
        â”œâ”€â”€ test_baseline_lock.py
        â”œâ”€â”€ test_builder_sparse_contract.py
        â”œâ”€â”€ test_control_api_smoke.py
        â”œâ”€â”€ test_control_jobs_db.py
        â”œâ”€â”€ test_control_preflight.py
        â”œâ”€â”€ test_control_worker_integration.py
        â”œâ”€â”€ test_data_layout.py
        â”œâ”€â”€ test_dtype_compression_contract.py
        â”œâ”€â”€ test_engine_constitution.py
        â”œâ”€â”€ test_engine_gaps_and_priority.py
        â”œâ”€â”€ test_engine_jit_active_book_contract.py
        â”œâ”€â”€ test_entry_only_regression.py
        â”œâ”€â”€ test_funnel_contract.py
        â”œâ”€â”€ test_funnel_oom_integration.py
        â”œâ”€â”€ test_funnel_smoke_contract.py
        â”œâ”€â”€ test_funnel_topk_determinism.py
        â”œâ”€â”€ test_funnel_topk_no_human_contract.py
        â”œâ”€â”€ test_golden_kernel_verification.py
        â”œâ”€â”€ test_governance_accepts_winners_v2.py
        â”œâ”€â”€ test_governance_eval_rules.py
        â”œâ”€â”€ test_governance_schema_contract.py
        â”œâ”€â”€ test_governance_writer_contract.py
        â”œâ”€â”€ test_grid_runner_smoke.py
        â”œâ”€â”€ test_indicators_consistency.py
        â”œâ”€â”€ test_indicators_precompute_bit_exact.py
        â”œâ”€â”€ test_json_pointer.py
        â”œâ”€â”€ test_kernel_parity_contract.py
        â”œâ”€â”€ test_kpi_drilldown_no_raise.py
        â”œâ”€â”€ test_kpi_registry.py
        â”œâ”€â”€ test_no_ui_imports_anywhere.py
        â”œâ”€â”€ test_no_ui_namespace.py
        â”œâ”€â”€ test_oom_gate.py
        â”œâ”€â”€ test_oom_gate_contract.py
        â”œâ”€â”€ test_perf_breakdown_contract.py
        â”œâ”€â”€ test_perf_env_config_contract.py
        â”œâ”€â”€ test_perf_evidence_chain.py
        â”œâ”€â”€ test_perf_grid_profile_report.py
        â”œâ”€â”€ test_perf_obs_contract.py
        â”œâ”€â”€ test_perf_trigger_rate_contract.py
        â”œâ”€â”€ test_report_link_allows_minimal_artifacts.py
        â”œâ”€â”€ test_runner_adapter_contract.py
        â”œâ”€â”€ test_runner_adapter_input_coercion.py
        â”œâ”€â”€ test_runner_grid_perf_observability.py
        â”œâ”€â”€ test_seed_demo_run.py
        â”œâ”€â”€ test_sparse_intents_contract.py
        â”œâ”€â”€ test_sparse_intents_mvp_contract.py
        â”œâ”€â”€ test_stage0_contract.py
        â”œâ”€â”€ test_stage0_ma_proxy.py
        â”œâ”€â”€ test_stage0_no_pnl_contract.py
        â”œâ”€â”€ test_stage0_proxies.py
        â”œâ”€â”€ test_stage0_proxy_rank_corr.py
        â”œâ”€â”€ test_stage2_params_influence.py
        â”œâ”€â”€ test_trigger_rate_param_subsample_contract.py
        â”œâ”€â”€ test_ui_artifact_validation.py
        â”œâ”€â”€ test_vectorization_parity.py
        â”œâ”€â”€ test_viewer_entrypoint.py
        â”œâ”€â”€ test_viewer_load_state.py
        â”œâ”€â”€ test_viewer_no_ui_import.py
        â”œâ”€â”€ test_viewer_page_scaffold_no_raise.py
        â””â”€â”€ test_winners_schema_v2_contract.py

================================================================================
PYTHON FILES AND CODE
================================================================================


================================================================================
FILE: GM_Huang/clean_repo_caches.py
================================================================================

#!/usr/bin/env python3
from __future__ import annotations

import os
from pathlib import Path


def _is_under(path: Path, parent: Path) -> bool:
    try:
        path.resolve().relative_to(parent.resolve())
        return True
    except Exception:
        return False


def clean_repo_caches(repo_root: Path, dry_run: bool = False) -> tuple[int, int]:
    """
    Remove Python bytecode caches inside repo_root:
      - __pycache__ directories
      - *.pyc, *.pyo
    Does NOT touch anything outside repo_root.
    """
    removed_dirs = 0
    removed_files = 0

    for p in repo_root.rglob("__pycache__"):
        if not p.is_dir():
            continue
        if not _is_under(p, repo_root):
            continue
        if dry_run:
            print(f"[DRY] rmdir: {p}")
        else:
            for child in p.rglob("*"):
                try:
                    if child.is_file() or child.is_symlink():
                        child.unlink(missing_ok=True)
                        removed_files += 1
                except Exception:
                    pass
            try:
                p.rmdir()
                removed_dirs += 1
            except Exception:
                pass

    for ext in ("*.pyc", "*.pyo"):
        for p in repo_root.rglob(ext):
            if not p.is_file() and not p.is_symlink():
                continue
            if not _is_under(p, repo_root):
                continue
            if dry_run:
                print(f"[DRY] rm: {p}")
            else:
                try:
                    p.unlink(missing_ok=True)
                    removed_files += 1
                except Exception:
                    pass

    return removed_dirs, removed_files


def main() -> None:
    repo_root = Path(__file__).resolve().parents[1]
    dry_run = os.environ.get("FISHBRO_DRY_RUN", "").strip() == "1"
    removed_dirs, removed_files = clean_repo_caches(repo_root, dry_run=dry_run)

    if dry_run:
        print("[DRY] Done.")
        return

    print(f"Cleaned {removed_dirs} __pycache__ directories and {removed_files} bytecode files.")


if __name__ == "__main__":
    main()


================================================================================
FILE: GM_Huang/release_tool.py
================================================================================

#!/usr/bin/env python3
"""
Release tool for FishBroWFS_V2.

Generates release packages (txt or zip) excluding sensitive information like .git
"""

from __future__ import annotations

import os
import zipfile
from datetime import datetime
from pathlib import Path


def should_exclude(path: Path, repo_root: Path) -> bool:
    """
    Check if a path should be excluded from release.
    
    Excludes:
    - .git directory and all its contents
    - __pycache__ directories
    - .pyc, .pyo files
    - Common build/test artifacts
    """
    path_str = str(path)
    path_parts = path.parts
    
    # Exclude .git directory
    if '.git' in path_parts:
        return True
    
    # Exclude cache directories
    if '__pycache__' in path_parts:
        return True
    
    # Exclude bytecode files
    if path.suffix in ('.pyc', '.pyo'):
        return True
    
    # Exclude common build/test artifacts
    exclude_names = {
        '.pytest_cache', '.mypy_cache', '.ruff_cache',
        '.coverage', 'htmlcov', '.tox', 'dist', 'build',
        '*.egg-info', '.eggs'
    }
    
    for name in exclude_names:
        if name in path_parts or path.name.startswith(name.replace('*', '')):
            return True
    
    # Exclude GM_Huang itself from the release (optional, but makes sense)
    # Actually, let's include it since it's part of the project structure
    
    return False


def get_python_files(repo_root: Path) -> list[Path]:
    """Get all Python files in the repository, excluding sensitive paths."""
    python_files = []
    
    for py_file in repo_root.rglob('*.py'):
        if not should_exclude(py_file, repo_root):
            python_files.append(py_file)
    
    return sorted(python_files)


def get_directory_structure(repo_root: Path) -> str:
    """Generate a text representation of directory structure."""
    lines = []
    
    def walk_tree(directory: Path, prefix: str = '', is_last: bool = True):
        """Recursively walk directory tree and build structure."""
        if should_exclude(directory, repo_root):
            return
        
        # Skip if it's the repo root itself
        if directory == repo_root:
            lines.append(f"{directory.name}/")
        else:
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            lines.append(f"{prefix}{connector}{directory.name}/")
        
        # Get subdirectories and files
        try:
            items = sorted([p for p in directory.iterdir() 
                          if not should_exclude(p, repo_root)])
            dirs = [p for p in items if p.is_dir()]
            files = [p for p in items if p.is_file() and p.suffix == '.py']
            
            # Process directories
            for i, item in enumerate(dirs):
                is_last_item = (i == len(dirs) - 1) and len(files) == 0
                extension = "    " if is_last else "â”‚   "
                walk_tree(item, prefix + extension, is_last_item)
            
            # Process Python files
            for i, file in enumerate(files):
                is_last_item = i == len(files) - 1
                connector = "â””â”€â”€ " if is_last_item else "â”œâ”€â”€ "
                lines.append(f"{prefix}{'    ' if is_last else 'â”‚   '}{connector}{file.name}")
        except PermissionError:
            pass
    
    walk_tree(repo_root)
    return "\n".join(lines)


def generate_release_txt(repo_root: Path, output_path: Path) -> None:
    """Generate a text file with directory structure and Python code."""
    print(f"Generating release TXT: {output_path}")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        # Header
        f.write("=" * 80 + "\n")
        f.write(f"FishBroWFS_V2 Release Package\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("=" * 80 + "\n\n")
        
        # Directory structure
        f.write("DIRECTORY STRUCTURE\n")
        f.write("-" * 80 + "\n")
        f.write(get_directory_structure(repo_root))
        f.write("\n\n")
        
        # Python files and their content
        f.write("=" * 80 + "\n")
        f.write("PYTHON FILES AND CODE\n")
        f.write("=" * 80 + "\n\n")
        
        python_files = get_python_files(repo_root)
        
        for py_file in python_files:
            relative_path = py_file.relative_to(repo_root)
            f.write(f"\n{'=' * 80}\n")
            f.write(f"FILE: {relative_path}\n")
            f.write(f"{'=' * 80}\n\n")
            
            try:
                content = py_file.read_text(encoding='utf-8')
                f.write(content)
                if not content.endswith('\n'):
                    f.write('\n')
            except Exception as e:
                f.write(f"[ERROR: Could not read file: {e}]\n")
            
            f.write("\n")
    
    print(f"âœ“ Release TXT generated: {output_path}")


def generate_release_zip(repo_root: Path, output_path: Path) -> None:
    """Generate a zip file of the project, excluding sensitive information."""
    print(f"Generating release ZIP: {output_path}")
    
    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        python_files = get_python_files(repo_root)
        
        # Also include non-Python files that are important
        important_extensions = {'.toml', '.txt', '.md', '.yml', '.yaml'}
        important_files = []
        
        for ext in important_extensions:
            for file in repo_root.rglob(f'*{ext}'):
                if not should_exclude(file, repo_root):
                    important_files.append(file)
        
        all_files = sorted(set(python_files + important_files))
        
        for file_path in all_files:
            relative_path = file_path.relative_to(repo_root)
            zipf.write(file_path, relative_path)
            print(f"  Added: {relative_path}")
    
    print(f"âœ“ Release ZIP generated: {output_path}")
    print(f"  Total files: {len(all_files)}")


def main() -> None:
    """Main entry point."""
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python release_tool.py [txt|zip]")
        sys.exit(1)
    
    mode = sys.argv[1].lower()
    
    # Get repo root (parent of GM_Huang)
    script_dir = Path(__file__).resolve().parent
    repo_root = script_dir.parent
    
    # Generate output filename with timestamp
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    project_name = repo_root.name
    
    if mode == 'txt':
        output_path = repo_root / f"{project_name}_release_{timestamp}.txt"
        generate_release_txt(repo_root, output_path)
    elif mode == 'zip':
        output_path = repo_root / f"{project_name}_release_{timestamp}.zip"
        generate_release_zip(repo_root, output_path)
    else:
        print(f"Unknown mode: {mode}. Use 'txt' or 'zip'")
        sys.exit(1)


if __name__ == "__main__":
    main()



================================================================================
FILE: scripts/perf_direct.py
================================================================================

#!/usr/bin/env python3
"""
FishBro WFS - Direct Engine Benchmark
ç”¨é€”: ç¹žéŽæ‰€æœ‰ Harness/Subprocess è¤‡é›œåº¦ï¼Œç›´æŽ¥ import engine æ¸¬é€Ÿ
"""
import sys
import time
import gc
import numpy as np
from pathlib import Path

# 1. å¼·åˆ¶è¨­å®šè·¯å¾‘ (æŒ‡å‘ src)
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

print(f"python_path: {sys.path[0]}")

try:
    # Correct src-based package name in this repo:
    # src/FishBroWFS_V2/pipeline/runner_grid.py
    from FishBroWFS_V2.pipeline.runner_grid import run_grid  # type: ignore
    print("âœ… Engine imported successfully (FishBroWFS_V2.pipeline.runner_grid).")
except ImportError as e:
    print(f"âŒ FATAL: Cannot import engine: {e}")
    sys.exit(1)

# 2. è¨­å®šè¦æ¨¡ (å°è¦æ¨¡ Smoke Test)
BARS = 20_000
PARAMS = 5_000
HOT_RUNS = 5

def generate_data(n_bars, n_params):
    print(f"generating data: {n_bars} bars, {n_params} params...")
    rng = np.random.default_rng(42)
    
    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10
    # ä½¿ç”¨ np.abs é¿å… AttributeError
    high = close + np.abs(rng.standard_normal(n_bars)) * 5
    low = close - np.abs(rng.standard_normal(n_bars)) * 5
    open_ = (high + low) / 2 + rng.standard_normal(n_bars)
    
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    # Generate Params (runner_grid contract: params_matrix must be (n, >=3))
    w1 = rng.integers(10, 100, size=n_params)
    w2 = rng.integers(5, 50, size=n_params)
    w3 = rng.integers(2, 30, size=n_params)
    params = np.column_stack((w1, w2, w3))
    
    # Layout check
    data_arrays = [open_, high, low, close, params]
    final_arrays = []
    for arr in data_arrays:
        arr = arr.astype(np.float64)
        if not arr.flags['C_CONTIGUOUS']:
            arr = np.ascontiguousarray(arr)
        final_arrays.append(arr)
        
    return final_arrays[0], final_arrays[1], final_arrays[2], final_arrays[3], final_arrays[4]

def main():
    opens, highs, lows, closes, params = generate_data(BARS, PARAMS)
    
    print("-" * 40)
    print(f"Start Benchmark: {BARS} bars x {PARAMS} params")
    print("-" * 40)

    # COLD RUN
    print("ðŸ¥¶ Cold run (compiling)...", end="", flush=True)
    t0 = time.perf_counter()
    _ = run_grid(
        open_=opens,
        high=highs,
        low=lows,
        close=closes,
        params_matrix=params,
        commission=0.0,
        slip=0.0,
        sort_params=False,
    )
    print(f" Done in {time.perf_counter() - t0:.4f}s")

    # HOT RUNS
    times = []
    print(f"ðŸ”¥ Hot runs ({HOT_RUNS} times, GC off)...")
    gc.disable()
    for i in range(HOT_RUNS):
        t_start = time.perf_counter()
        _ = run_grid(
            open_=opens,
            high=highs,
            low=lows,
            close=closes,
            params_matrix=params,
            commission=0.0,
            slip=0.0,
            sort_params=False,
        )
        dt = time.perf_counter() - t_start
        times.append(dt)
        print(f"   Run {i+1}: {dt:.4f}s")
    gc.enable()
    
    min_time = min(times)
    total_ops = BARS * PARAMS
    tput = total_ops / min_time
    
    print("-" * 40)
    print(f"MIN TIME:   {min_time:.4f}s")
    print(f"THROUGHPUT: {int(tput):,} pair-bars/sec")
    print("-" * 40)

if __name__ == "__main__":
    main()


================================================================================
FILE: scripts/perf_grid.py
================================================================================

#!/usr/bin/env python3
"""
FishBro WFS Perf Harness (Red Team Spec v1.0)
ç‹€æ…‹: âœ… File-based IPC / JIT-First / Observable
ç”¨é€”: é‡æ¸¬ JIT Grid Runner çš„ç©©æ…‹åžåé‡ (Steady-state Throughput)

ä¿®æ­£ç´€éŒ„:
- v1.1: ä¿®å¾© numpy generator abs éŒ¯èª¤
- v1.2: Hotfix: è§£æ±º subprocess Import Errorï¼Œå¼·åˆ¶æ³¨å…¥ PYTHONPATH ä¸¦å¢žå¼· debug info
"""
import os
import sys
import time
import gc
import json
import cProfile
import argparse
import subprocess
import tempfile
import statistics
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional

import numpy as np

from FishBroWFS_V2.perf.cost_model import estimate_seconds
from FishBroWFS_V2.perf.profile_report import _format_profile_report

# ==========================================
# 1. é…ç½®èˆ‡å¸¸æ•¸ (Tiers)
# ==========================================

@dataclass
class PerfConfig:
    name: str
    n_bars: int
    n_params: int
    hot_runs: int
    timeout: int
    disable_jit: bool
    sort_params: bool

# Baseline Tier (default): Fast, suitable for commit-to-commit comparison
# Can be overridden via FISHBRO_PERF_BARS and FISHBRO_PERF_PARAMS env vars
TIER_JIT_BARS = int(os.environ.get("FISHBRO_PERF_BARS", "20000"))
TIER_JIT_PARAMS = int(os.environ.get("FISHBRO_PERF_PARAMS", "1000"))
TIER_JIT_HOT_RUNS = int(os.environ.get("FISHBRO_PERF_HOTRUNS", "5"))
TIER_JIT_TIMEOUT = int(os.environ.get("FISHBRO_PERF_TIMEOUT_S", "600"))

# Stress Tier: Optional, for extreme throughput testing (requires larger timeout or skip-cold)
TIER_STRESS_BARS = int(os.environ.get("FISHBRO_PERF_STRESS_BARS", "200000"))
TIER_STRESS_PARAMS = int(os.environ.get("FISHBRO_PERF_STRESS_PARAMS", "10000"))

TIER_TOY_BARS = 2_000
TIER_TOY_PARAMS = 10
TIER_TOY_HOT_RUNS = 1
TIER_TOY_TIMEOUT = 60

# Warmup compile tier (for skip-cold mode)
TIER_WARMUP_COMPILE_BARS = 2_000
TIER_WARMUP_COMPILE_PARAMS = 200

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

# ==========================================
# 2. è³‡æ–™ç”Ÿæˆ (Deterministic)
# ==========================================

def generate_synthetic_data(n_bars: int, seed: int = 42) -> Dict[str, np.ndarray]:
    """
    Generate synthetic OHLC data for perf harness.
    
    Uses float32 for Stage0/perf optimization (memory bandwidth reduction).
    """
    from FishBroWFS_V2.config.dtypes import PRICE_DTYPE_STAGE0
    
    rng = np.random.default_rng(seed)
    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10
    high = close + np.abs(rng.standard_normal(n_bars)) * 5
    low = close - np.abs(rng.standard_normal(n_bars)) * 5
    open_ = (high + low) / 2 + rng.standard_normal(n_bars)
    
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    # Use float32 for perf harness (Stage0 optimization)
    data = {
        "open": open_.astype(PRICE_DTYPE_STAGE0),
        "high": high.astype(PRICE_DTYPE_STAGE0),
        "low": low.astype(PRICE_DTYPE_STAGE0),
        "close": close.astype(PRICE_DTYPE_STAGE0),
    }
    
    for k, v in data.items():
        if not v.flags['C_CONTIGUOUS']:
            data[k] = np.ascontiguousarray(v, dtype=PRICE_DTYPE_STAGE0)
    return data

def generate_params(n_params: int, seed: int = 999) -> np.ndarray:
    """
    Generate parameter matrix for perf harness.
    
    Uses float32 for Stage0 optimization (memory bandwidth reduction).
    """
    from FishBroWFS_V2.config.dtypes import PRICE_DTYPE_STAGE0
    
    rng = np.random.default_rng(seed)
    w1 = rng.integers(10, 100, size=n_params)
    w2 = rng.integers(5, 50, size=n_params)
    # runner_grid contract: params_matrix must be (n, >=3)
    # Provide a minimal 3-column schema for perf harness.
    w3 = rng.integers(2, 30, size=n_params)
    params = np.column_stack((w1, w2, w3)).astype(PRICE_DTYPE_STAGE0)
    if not params.flags['C_CONTIGUOUS']:
        params = np.ascontiguousarray(params, dtype=PRICE_DTYPE_STAGE0)
    return params

# ==========================================
# 3. Worker é‚è¼¯ (Child Process)
# ==========================================

def worker_log(msg: str):
    print(f"[worker] {msg}", flush=True)


def _env_flag(name: str) -> bool:
    return os.environ.get(name, "").strip() == "1"


def _env_int(name: str, default: int) -> int:
    try:
        return int(os.environ.get(name, str(default)))
    except Exception:
        return default


def _env_float(name: str, default: float) -> float:
    try:
        return float(os.environ.get(name, str(default)))
    except Exception:
        return default


# NOTE: _format_profile_report moved to src/FishBroWFS_V2/perf/profile_report.py

def _run_microbench_numba_indicators(closes: np.ndarray, hot_runs: int) -> Dict[str, Any]:
    """
    Perf-only microbench:
      - Prove Numba is active in worker process.
      - Measure pure numeric kernels (no Python object loop) baseline.
    """
    try:
        import numba as nb  # type: ignore
    except Exception:  # pragma: no cover
        return {"microbench": "numba_missing"}

    from FishBroWFS_V2.indicators import numba_indicators as ni  # type: ignore

    # Use a fixed window; keep deterministic and cheap.
    length = 14
    x = np.ascontiguousarray(closes, dtype=np.float64)

    # Warmup compile (first call triggers compilation if JIT enabled).
    _ = ni.rolling_max(x, length)

    # Hot runs
    times: List[float] = []
    for _i in range(max(1, hot_runs)):
        t0 = time.perf_counter()
        _ = ni.rolling_max(x, length)
        times.append(time.perf_counter() - t0)

    best = min(times) if times else 0.0
    n = int(x.shape[0])
    # rolling_max visits each element once -> treat as "ops" ~= n
    tput = (n / best) if best > 0 else 0.0
    return {
        "microbench": "rolling_max",
        "n": n,
        "best_s": best,
        "ops_per_s": tput,
        "nb_disable_jit": int(getattr(nb.config, "DISABLE_JIT", -1)),
    }


def run_worker(
    npz_path: str,
    hot_runs: int,
    skip_cold: bool = False,
    warmup_bars: int = 0,
    warmup_params: int = 0,
    microbench: bool = False,
):
    try:
        # Stage P2-1.6: Parse trigger_rate env var
        trigger_rate = _env_float("FISHBRO_PERF_TRIGGER_RATE", 1.0)
        if trigger_rate < 0.0 or trigger_rate > 1.0:
            raise ValueError(f"FISHBRO_PERF_TRIGGER_RATE must be in [0, 1], got {trigger_rate}")
        worker_log(f"trigger_rate={trigger_rate}")
        
        worker_log(f"Starting. Loading input: {npz_path}")
        
        with np.load(npz_path, allow_pickle=False) as data:
            opens = data['open']
            highs = data['high']
            lows = data['low']
            closes = data['close']
            params = data['params']
            
        worker_log(f"Data loaded. Bars: {len(opens)}, Params: {len(params)}")

        if microbench:
            worker_log("MICROBENCH enabled: running numba indicator microbench.")
            res = _run_microbench_numba_indicators(closes, hot_runs=hot_runs)
            print("__RESULT_JSON_START__")
            print(json.dumps({"mode": "microbench", "result": res}))
            print("__RESULT_JSON_END__")
            return
        
        try:
            # Phase 3B Grid Runner (correct target)
            # src/FishBroWFS_V2/pipeline/runner_grid.py
            from FishBroWFS_V2.pipeline.runner_grid import run_grid  # type: ignore
            worker_log("Grid runner imported successfully (FishBroWFS_V2.pipeline.runner_grid).")
            # Enable runner_grid observability payload in returned dict (timings + jit truth + counts).
            os.environ["FISHBRO_PROFILE_GRID"] = "1"

            # ---- JIT truth report (perf-only) ----
            worker_log(f"ENV NUMBA_DISABLE_JIT={os.environ.get('NUMBA_DISABLE_JIT','')!r}")
            try:
                import numba as _nb  # type: ignore
                worker_log(f"Numba present. nb.config.DISABLE_JIT={getattr(_nb.config,'DISABLE_JIT',None)!r}")
            except Exception as _e:
                worker_log(f"Numba import failed: {_e!r}")

            # run_grid itself might be Python; report what it is.
            worker_log(f"run_grid type={type(run_grid)} has_signatures={hasattr(run_grid,'signatures')}")
            if hasattr(run_grid, "signatures"):
                worker_log(f"run_grid.signatures(before)={getattr(run_grid,'signatures',None)!r}")
            # --------------------------------------
        except ImportError as e:
            worker_log(f"FATAL: Import grid runner failed: {e!r}")
            
            # --- DEBUG INFO ---
            worker_log(f"Current sys.path: {sys.path}")
            src_path = Path(__file__).resolve().parent.parent / "src"
            if src_path.exists():
                worker_log(f"Listing {src_path}:")
                try:
                    for p in src_path.iterdir():
                        worker_log(f" - {p.name}")
                        if p.is_dir() and (p / "__init__.py").exists():
                             worker_log(f"   (package content): {[sub.name for sub in p.iterdir()]}")
                except Exception as ex:
                    worker_log(f"   Error listing dir: {ex}")
            else:
                worker_log(f"Src path not found at: {src_path}")
            # ------------------
            sys.exit(1)
        
        # Warmup run (perf-only): compile/JIT on a tiny slice so the real run measures steady-state.
        # IMPORTANT: respect CLI-provided warmup_{bars,params}. If 0, fall back to defaults.
        if warmup_bars and warmup_bars > 0:
            wb = min(int(warmup_bars), len(opens))
        else:
            wb = min(2000, len(opens))

        if warmup_params and warmup_params > 0:
            wp = min(int(warmup_params), len(params))
        else:
            wp = min(200, len(params))
        if wb >= 10 and wp >= 10:
            worker_log(f"Starting WARMUP run (bars={wb}, params={wp})...")
            _ = run_grid(
                open_=opens[:wb],
                high=highs[:wb],
                low=lows[:wb],
                close=closes[:wb],
                params_matrix=params[:wp],
                commission=0.0,
                slip=0.0,
                sort_params=False,
            )
            worker_log("WARMUP finished.")
            if hasattr(run_grid, "signatures"):
                worker_log(f"run_grid.signatures(after)={getattr(run_grid,'signatures',None)!r}")
        
        lane_sort = os.environ.get("FISHBRO_PERF_LANE_SORT", "0").strip() == "1"
        lane_id = os.environ.get("FISHBRO_PERF_LANE_ID", "?").strip()
        do_profile = _env_flag("FISHBRO_PERF_PROFILE")
        topn = _env_int("FISHBRO_PERF_PROFILE_TOP", 40)
        mode = os.environ.get("FISHBRO_PERF_PROFILE_MODE", "").strip()
        jit_enabled = os.environ.get("NUMBA_DISABLE_JIT", "").strip() != "1"
        cold_time = 0.0
        if skip_cold:
            # Skip-cold mode: warmup already done, skip full cold run
            worker_log("Skip-cold mode: skipping full cold run (warmup already completed)")
        else:
            # Full cold run
            worker_log("Starting COLD run...")
            t0 = time.perf_counter()
            _ = run_grid(
                open_=opens,
                high=highs,
                low=lows,
                close=closes,
                params_matrix=params,
                commission=0.0,
                slip=0.0,
                sort_params=lane_sort,
            )
            cold_time = time.perf_counter() - t0
            worker_log(f"COLD run finished: {cold_time:.4f}s")
        
        worker_log(f"Starting {hot_runs} HOT runs (GC disabled)...")
        hot_times = []
        last_out: Optional[Dict[str, Any]] = None
        gc.disable()
        try:
            for i in range(hot_runs):
                t_start = time.perf_counter()
                if do_profile and i == 0:
                    pr = cProfile.Profile()
                    pr.enable()
                    last_out = run_grid(
                        open_=opens,
                        high=highs,
                        low=lows,
                        close=closes,
                        params_matrix=params,
                        commission=0.0,
                        slip=0.0,
                        sort_params=lane_sort,
                    )
                    pr.disable()
                    print(
                        _format_profile_report(
                            lane_id=lane_id,
                            n_bars=int(len(opens)),
                            n_params=int(len(params)),
                            jit_enabled=bool(jit_enabled),
                            sort_params=bool(lane_sort),
                            topn=int(topn),
                            mode=mode,
                            pr=pr,
                        ),
                        end="",
                    )
                else:
                    last_out = run_grid(
                        open_=opens,
                        high=highs,
                        low=lows,
                        close=closes,
                        params_matrix=params,
                        commission=0.0,
                        slip=0.0,
                        sort_params=lane_sort,
                    )
                t_end = time.perf_counter()
                hot_times.append(t_end - t_start)
        finally:
            gc.enable()
        
        avg_hot = statistics.mean(hot_times) if hot_times else 0.0
        min_hot = min(hot_times) if hot_times else 0.0
        
        result = {
            "cold_time": cold_time,
            "hot_times": hot_times,
            "avg_hot_time": avg_hot,
            "min_hot_time": min_hot,
            "n_bars": len(opens),
            "n_params": len(params),
            "throughput": (len(opens) * len(params)) / min_hot if min_hot > 0 else 0,
        }

        # Attach runner_grid observability payload (timings + jit truth + counts)
        if isinstance(last_out, dict) and "perf" in last_out:
            result["perf"] = last_out["perf"]
            # Stage P2-1.6: Add trigger_rate_configured to perf dict
            if isinstance(result["perf"], dict):
                result["perf"]["trigger_rate_configured"] = float(trigger_rate)
        
        # Stage P2-1.8: Debug timing keys (only if PERF_DEBUG=1)
        if os.environ.get("PERF_DEBUG", "").strip() == "1":
            perf_keys = sorted(result.get("perf", {}).keys()) if isinstance(result.get("perf"), dict) else []
            worker_log(f"DEBUG: perf keys count={len(perf_keys)}, has t_total_kernel_s={'t_total_kernel_s' in perf_keys}")
            if len(perf_keys) > 0:
                worker_log(f"DEBUG: perf keys sample: {perf_keys[:20]}")
        
        print(f"__RESULT_JSON_START__")
        print(json.dumps(result))
        print(f"__RESULT_JSON_END__")
        
    except Exception as e:
        worker_log(f"CRASH: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

# ==========================================
# 4. Controller é‚è¼¯ (Host Process)
# ==========================================

def run_lane(
    lane_id: int,
    cfg: PerfConfig,
    tmp_dir: str,
    ohlc_data: Dict[str, np.ndarray],
    microbench: bool = False,
) -> Dict[str, Any]:
    print(f"\n>>> Running Lane {lane_id}: {cfg.name}")
    print(f"    Config: Bars={cfg.n_bars}, Params={cfg.n_params}, JIT={not cfg.disable_jit}, Sort={cfg.sort_params}")
    
    params = generate_params(cfg.n_params)
    # Do not pre-sort here; sorting behavior must be owned by runner_grid(sort_params=...).
    # For no-sort lane, we shuffle to simulate random access order.
    if not cfg.sort_params:
        np.random.shuffle(params)
        print("    Params shuffled (random access simulation).")
    else:
        print("    Params left unsorted; runner_grid(sort_params=True) will apply cache-friendly sort.")
        
    npz_path = os.path.join(tmp_dir, f"input_lane_{lane_id}.npz")
    np.savez_compressed(
        npz_path, 
        open=ohlc_data["open"][:cfg.n_bars],
        high=ohlc_data["high"][:cfg.n_bars],
        low=ohlc_data["low"][:cfg.n_bars],
        close=ohlc_data["close"][:cfg.n_bars],
        params=params
    )
    
    env = os.environ.copy()
    
    # é—œéµä¿®æ­£: å¼·åˆ¶æ³¨å…¥ PYTHONPATH ç¢ºä¿å­é€²ç¨‹çœ‹å¾—åˆ° src
    src_path = str(PROJECT_ROOT / "src")
    if "PYTHONPATH" in env:
        env["PYTHONPATH"] = f"{src_path}:{env['PYTHONPATH']}"
    else:
        env["PYTHONPATH"] = src_path
        
    if cfg.disable_jit:
        env["NUMBA_DISABLE_JIT"] = "1"
    else:
        env.pop("NUMBA_DISABLE_JIT", None)
    
    # Stage P2-1.6: Pass FISHBRO_PERF_TRIGGER_RATE to worker if set
    # (env.copy() already includes it, but we ensure it's explicitly passed)
    trigger_rate_env = os.environ.get("FISHBRO_PERF_TRIGGER_RATE")
    if trigger_rate_env:
        env["FISHBRO_PERF_TRIGGER_RATE"] = trigger_rate_env
        
    # Build worker command
    cmd = [
        sys.executable,
        __file__,
        "--worker",
        "--input",
        npz_path,
        "--hot-runs",
        str(cfg.hot_runs),
    ]
    if microbench:
        cmd.append("--microbench")
    # Pass lane sort flag to worker via env (avoid CLI churn)
    env["FISHBRO_PERF_LANE_SORT"] = "1" if cfg.sort_params else "0"
    env["FISHBRO_PERF_LANE_ID"] = str(lane_id)
    
    # Add skip-cold and warmup params if needed
    skip_cold = os.environ.get("FISHBRO_PERF_SKIP_COLD", "").lower() == "true"
    if skip_cold:
        cmd.extend(["--skip-cold"])
        warmup_bars = int(os.environ.get("FISHBRO_PERF_WARMUP_BARS", str(TIER_WARMUP_COMPILE_BARS)))
        warmup_params = int(os.environ.get("FISHBRO_PERF_WARMUP_PARAMS", str(TIER_WARMUP_COMPILE_PARAMS)))
        cmd.extend(["--warmup-bars", str(warmup_bars), "--warmup-params", str(warmup_params)])
    
    try:
        proc = subprocess.run(
            cmd,
            env=env,
            capture_output=True,
            text=True,
            timeout=cfg.timeout,
            check=True
        )
        
        stdout = proc.stdout
        # Print worker stdout (includes JIT truth report)
        print(stdout, end="")
        
        result_json = None
        lines = stdout.splitlines()
        capture = False
        json_str = ""
        
        for line in lines:
            if line.strip() == "__RESULT_JSON_END__":
                capture = False
            if capture:
                json_str += line
            if line.strip() == "__RESULT_JSON_START__":
                capture = True
                
        if json_str:
            result_json = json.loads(json_str)
            
            # Phase 3.0-C: FAIL-FAST defense - detect fallback to object mode
            strict_arrays = os.environ.get("FISHBRO_PERF_STRICT_ARRAYS", "1").strip() == "1"
            if strict_arrays and isinstance(result_json, dict):
                perf = result_json.get("perf")
                if isinstance(perf, dict):
                    intent_mode = perf.get("intent_mode")
                    if intent_mode != "arrays":
                        # Handle None or any non-"arrays" value
                        intent_mode_str = str(intent_mode) if intent_mode is not None else "None"
                        error_msg = (
                            f"ERROR: intent_mode expected 'arrays' but got '{intent_mode_str}' (lane {lane_id})\n"
                            f"This indicates the kernel fell back to object mode, which is a performance regression.\n"
                            f"To disable this check, set FISHBRO_PERF_STRICT_ARRAYS=0"
                        )
                        print(f"âŒ {error_msg}", file=sys.stderr)
                        raise RuntimeError(error_msg)
            
            return result_json
        else:
            print("âŒ Error: Worker finished but no JSON result found.")
            print("--- Worker Stdout ---")
            print(stdout)
            print("--- Worker Stderr ---")
            print(proc.stderr)
            return {}
            
    except subprocess.TimeoutExpired as e:
        print(f"âŒ Error: Lane {lane_id} Timeout ({cfg.timeout}s).")
        if e.stdout: print(e.stdout)
        if e.stderr: print(e.stderr)
        return {}
    except subprocess.CalledProcessError as e:
        print(f"âŒ Error: Lane {lane_id} Crashed (Exit {e.returncode}).")
        print("--- Worker Stdout ---")
        print(e.stdout)
        print("--- Worker Stderr ---")
        print(e.stderr)
        return {}
    except Exception as e:
        print(f"âŒ Error: System error {e}")
        return {}

def print_report(results: List[Dict[str, Any]]):
    print("\n\n=== FishBro WFS Perf Harness Report ===")
    print("| Lane | Mode | Sort | Bars | Params | Cold(s) | Hot(s) | Tput (Ops/s) | Speedup |")
    print("|---|---|---|---|---|---|---|---|---|")
    
    jit_no_sort_tput = 0
    for r in results:
        if not r or "res" not in r or "lane_id" not in r: continue
        lane_id = r.get('lane_id', 0)
        name = r.get('name', 'Unknown')
        bars = r['res'].get('n_bars', 0)
        params = r['res'].get('n_params', 0)
        cold = r['res'].get('cold_time', 0)
        hot = r['res'].get('min_hot_time', 0)
        tput = r['res'].get('throughput', 0)
        
        if lane_id == 3:
            jit_no_sort_tput = tput
            speedup = "1.0x (Base)"
        elif jit_no_sort_tput > 0 and tput > 0:
            ratio = tput / jit_no_sort_tput
            speedup = f"{ratio:.2f}x"
        else:
            speedup = "-"
            
        mode = "Py" if r.get("disable_jit", False) else "JIT"
        sort = "Yes" if r.get("sort_params", False) else "No"
        print(f"| {lane_id} | {mode} | {sort} | {bars} | {params} | {cold:.4f} | {hot:.4f} | {int(tput):,} | {speedup} |")
    print("\nNote: Tput = (Bars * Params) / Min Hot Run Time")
    
    # Phase 4 Stage E: Cost Model Output
    print("\n=== Cost Model (Predictable Cost Estimation) ===")
    for r in results:
        if not r or "res" not in r or "lane_id" not in r: continue
        lane_id = r.get('lane_id', 0)
        res = r.get('res', {})
        bars = res.get('n_bars', 0)
        params = res.get('n_params', 0)
        min_hot_time = res.get('min_hot_time', 0)
        
        if min_hot_time > 0 and params > 0:
            # Calculate cost per parameter (milliseconds)
            cost_ms_per_param = (min_hot_time / params) * 1000.0
            
            # Calculate params per second
            params_per_sec = params / min_hot_time
            
            # Estimate time for 50k params
            estimated_time_for_50k_params = estimate_seconds(
                bars=bars,
                params=50000,
                cost_ms_per_param=cost_ms_per_param,
            )
            
            # Output cost model fields (stdout)
            print(f"\nLane {lane_id} Cost Model:")
            print(f"  bars: {bars}")
            print(f"  params: {params}")
            print(f"  best_time_s: {min_hot_time:.6f}")
            print(f"  params_per_sec: {params_per_sec:,.2f}")
            print(f"  cost_ms_per_param: {cost_ms_per_param:.6f}")
            print(f"  estimated_time_for_50k_params: {estimated_time_for_50k_params:.2f}")
            
            # Stage P2-1.5: Entry Sparse Observability
            perf = res.get('perf', {})
            if isinstance(perf, dict):
                entry_valid_mask_sum = perf.get('entry_valid_mask_sum')
                entry_intents_total = perf.get('entry_intents_total')
                entry_intents_per_bar_avg = perf.get('entry_intents_per_bar_avg')
                intents_total_reported = perf.get('intents_total_reported')
                trigger_rate_configured = perf.get('trigger_rate_configured')
                
                # Always output if perf dict exists (fields should always be present)
                if entry_valid_mask_sum is not None or entry_intents_total is not None:
                    print(f"\nLane {lane_id} Entry Sparse Observability:")
                    # Stage P2-1.6: Display trigger_rate_configured
                    if trigger_rate_configured is not None:
                        print(f"  trigger_rate_configured: {trigger_rate_configured:.6f}")
                    print(f"  entry_valid_mask_sum: {entry_valid_mask_sum if entry_valid_mask_sum is not None else 0}")
                    print(f"  entry_intents_total: {entry_intents_total if entry_intents_total is not None else 0}")
                    if entry_intents_per_bar_avg is not None:
                        print(f"  entry_intents_per_bar_avg: {entry_intents_per_bar_avg:.6f}")
                    else:
                        # Calculate if missing
                        if entry_intents_total is not None and bars > 0:
                            print(f"  entry_intents_per_bar_avg: {entry_intents_total / bars:.6f}")
                    print(f"  intents_total_reported: {intents_total_reported if intents_total_reported is not None else perf.get('intents_total', 0)}")
                
                # Stage P2-3: Sparse Builder Scaling (for scaling verification)
                allowed_bars = perf.get('allowed_bars')
                selected_params = perf.get('selected_params')
                intents_generated = perf.get('intents_generated')
                
                if allowed_bars is not None or selected_params is not None or intents_generated is not None:
                    print(f"\nLane {lane_id} Sparse Builder Scaling:")
                    if allowed_bars is not None:
                        print(f"  allowed_bars: {allowed_bars:,}")
                    if selected_params is not None:
                        print(f"  selected_params: {selected_params:,}")
                    if intents_generated is not None:
                        print(f"  intents_generated: {intents_generated:,}")
                    # Calculate scaling ratio if both available
                    if allowed_bars is not None and intents_generated is not None and allowed_bars > 0:
                        scaling_ratio = intents_generated / allowed_bars
                        print(f"  scaling_ratio (intents/allowed): {scaling_ratio:.4f}")
    
    # Stage P2-1.8: Breakdown (Kernel Stage Timings)
    print("\n=== Breakdown (Kernel Stage Timings) ===")
    for r in results:
        if not r or "res" not in r or "lane_id" not in r: continue
        lane_id = r.get('lane_id', 0)
        res = r.get('res', {})
        perf = res.get('perf', {})
        
        if isinstance(perf, dict):
            trigger_rate = perf.get('trigger_rate_configured')
            t_ind_donchian = perf.get('t_ind_donchian_s')
            t_ind_atr = perf.get('t_ind_atr_s')
            t_build_entry = perf.get('t_build_entry_intents_s')
            t_sim_entry = perf.get('t_simulate_entry_s')
            t_calc_exits = perf.get('t_calc_exits_s')
            t_sim_exit = perf.get('t_simulate_exit_s')
            t_total_kernel = perf.get('t_total_kernel_s')
            
            print(f"\nLane {lane_id} Breakdown:")
            if trigger_rate is not None:
                print(f"  trigger_rate_configured: {trigger_rate:.6f}")
            
            # Helper to format timing with "(missing)" if None
            def fmt_time(key: str, val) -> str:
                if val is None:
                    return f"  {key}: (missing)"
                return f"  {key}: {val:.6f}"
            
            # Stage P2-2 Step A: Micro-profiling indicators
            print(fmt_time("t_ind_donchian_s", t_ind_donchian))
            print(fmt_time("t_ind_atr_s", t_ind_atr))
            print(fmt_time("t_build_entry_intents_s", t_build_entry))
            print(fmt_time("t_simulate_entry_s", t_sim_entry))
            print(fmt_time("t_calc_exits_s", t_calc_exits))
            print(fmt_time("t_simulate_exit_s", t_sim_exit))
            print(fmt_time("t_total_kernel_s", t_total_kernel))
            
            # Print percentages if t_total_kernel is available and > 0
            if t_total_kernel is not None and t_total_kernel > 0:
                def fmt_pct(key: str, val, total: float) -> str:
                    if val is None:
                        return f"    {key}: (missing)"
                    pct = (val / total) * 100.0
                    return f"    {key}: {pct:.1f}%"
                
                print("  Percentages:")
                print(fmt_pct("t_ind_donchian_s", t_ind_donchian, t_total_kernel))
                print(fmt_pct("t_ind_atr_s", t_ind_atr, t_total_kernel))
                print(fmt_pct("t_build_entry_intents_s", t_build_entry, t_total_kernel))
                print(fmt_pct("t_simulate_entry_s", t_sim_entry, t_total_kernel))
                print(fmt_pct("t_calc_exits_s", t_calc_exits, t_total_kernel))
                print(fmt_pct("t_simulate_exit_s", t_sim_exit, t_total_kernel))
            
            # Stage P2-2 Step A: Memoization potential assessment
            unique_ch = perf.get('unique_channel_len_count')
            unique_atr = perf.get('unique_atr_len_count')
            unique_pair = perf.get('unique_ch_atr_pair_count')
            
            if unique_ch is not None or unique_atr is not None or unique_pair is not None:
                print(f"\nLane {lane_id} Memoization Potential:")
                if unique_ch is not None:
                    print(f"  unique_channel_len_count: {unique_ch}")
                else:
                    print(f"  unique_channel_len_count: (missing)")
                if unique_atr is not None:
                    print(f"  unique_atr_len_count: {unique_atr}")
                else:
                    print(f"  unique_atr_len_count: (missing)")
                if unique_pair is not None:
                    print(f"  unique_ch_atr_pair_count: {unique_pair}")
                else:
                    print(f"  unique_ch_atr_pair_count: (missing)")
            
            # Stage P2-1.8: Display downstream counts
            entry_fills_total = perf.get('entry_fills_total')
            exit_intents_total = perf.get('exit_intents_total')
            exit_fills_total = perf.get('exit_fills_total')
            
            if entry_fills_total is not None or exit_intents_total is not None or exit_fills_total is not None:
                print(f"\nLane {lane_id} Downstream Observability:")
                if entry_fills_total is not None:
                    print(f"  entry_fills_total: {entry_fills_total}")
                else:
                    print(f"  entry_fills_total: (missing)")
                if exit_intents_total is not None:
                    print(f"  exit_intents_total: {exit_intents_total}")
                else:
                    print(f"  exit_intents_total: (missing)")
                if exit_fills_total is not None:
                    print(f"  exit_fills_total: {exit_fills_total}")
                else:
                    print(f"  exit_fills_total: (missing)")

def run_matcherbench() -> None:
    """
    Matcher-only microbenchmark.
    Purpose:
      - Measure true throughput of cursor-based matcher kernel
      - Avoid runner_grid / Python orchestration overhead
    """
    from FishBroWFS_V2.engine.engine_jit import simulate
    from FishBroWFS_V2.engine.types import (
        BarArrays,
        OrderIntent,
        OrderKind,
        OrderRole,
        Side,
    )

    # ---- config (safe defaults) ----
    n_bars = int(os.environ.get("FISHBRO_MB_BARS", "20000"))
    intents_per_bar = int(os.environ.get("FISHBRO_MB_INTENTS_PER_BAR", "2"))
    hot_runs = int(os.environ.get("FISHBRO_MB_HOTRUNS", "3"))

    print(
        f"[matcherbench] bars={n_bars}, intents_per_bar={intents_per_bar}, hot_runs={hot_runs}"
    )

    # ---- synthetic OHLC ----
    rng = np.random.default_rng(42)
    close = 10000 + np.cumsum(rng.standard_normal(n_bars))
    high = close + 5.0
    low = close - 5.0
    open_ = (high + low) * 0.5

    bars = BarArrays(
        open=open_.astype(np.float64),
        high=high.astype(np.float64),
        low=low.astype(np.float64),
        close=close.astype(np.float64),
    )

    # ---- generate intents: created_bar = t-1 ----
    intents = []
    oid = 1
    for t in range(1, n_bars):
        for _ in range(intents_per_bar):
            # ENTRY
            intents.append(
                OrderIntent(
                    order_id=oid,
                    created_bar=t - 1,
                    role=OrderRole.ENTRY,
                    kind=OrderKind.STOP,
                    side=Side.BUY,
                    price=float(high[t - 1]),
                    qty=1,
                )
            )
            oid += 1
            # EXIT
            intents.append(
                OrderIntent(
                    order_id=oid,
                    created_bar=t - 1,
                    role=OrderRole.EXIT,
                    kind=OrderKind.STOP,
                    side=Side.SELL,
                    price=float(low[t - 1]),
                    qty=1,
                )
            )
            oid += 1

    print(f"[matcherbench] total_intents={len(intents)}")

    # ---- warmup (compile) ----
    simulate(bars, intents)

    # ---- hot runs ----
    times = []
    gc.disable()
    try:
        for _ in range(hot_runs):
            t0 = time.perf_counter()
            fills = simulate(bars, intents)
            dt = time.perf_counter() - t0
            times.append(dt)
    finally:
        gc.enable()

    best = min(times)
    bars_per_s = n_bars / best
    intents_scanned = len(intents)
    intents_per_s = intents_scanned / best
    fills_per_s = len(fills) / best

    print("\n=== MATCHERBENCH RESULT ===")
    print(f"best_time_s      : {best:.6f}")
    print(f"bars_per_sec     : {bars_per_s:,.0f}")
    print(f"intents_per_sec  : {intents_per_s:,.0f}")
    print(f"fills_per_sec    : {fills_per_s:,.0f}")


def main():
    parser = argparse.ArgumentParser(description="FishBro WFS Perf Harness")
    parser.add_argument("--worker", action="store_true", help="Run as worker")
    parser.add_argument("--input", type=str, help="Path to input NPZ")
    parser.add_argument("--hot-runs", type=int, default=5, help="Hot runs")
    parser.add_argument("--skip-cold", action="store_true", help="Skip full cold run, use warmup compile instead")
    parser.add_argument("--warmup-bars", type=int, default=0, help="Warmup compile bars (for skip-cold)")
    parser.add_argument("--warmup-params", type=int, default=0, help="Warmup compile params (for skip-cold)")
    parser.add_argument("--microbench", action="store_true", help="Run microbench only (numba indicator baseline)")
    parser.add_argument("--include-python-baseline", action="store_true", help="Include Toy Tier")
    parser.add_argument(
        "--matcherbench",
        action="store_true",
        help="Benchmark matcher kernel only (engine_jit.simulate), no runner_grid",
    )
    parser.add_argument("--stress-tier", action="store_true", help="Use stress tier (200kÃ—10k) instead of warmup tier")
    args = parser.parse_args()
    
    if args.matcherbench:
        run_matcherbench()
        return

    if args.worker:
        if not args.input: sys.exit(1)
        run_worker(
            args.input,
            args.hot_runs,
            args.skip_cold,
            args.warmup_bars,
            args.warmup_params,
            args.microbench,
        )
        return

    print("Initializing Perf Harness...")
    
    # Stage P2-1.6: Parse and display trigger_rate in main process
    trigger_rate = _env_float("FISHBRO_PERF_TRIGGER_RATE", 1.0)
    if trigger_rate < 0.0 or trigger_rate > 1.0:
        raise ValueError(f"FISHBRO_PERF_TRIGGER_RATE must be in [0, 1], got {trigger_rate}")
    print(f"trigger_rate={trigger_rate}")
    
    lanes_cfg: List[PerfConfig] = []
    
    # Select tier based on stress-tier flag
    if args.stress_tier:
        jit_bars = TIER_STRESS_BARS
        jit_params = TIER_STRESS_PARAMS
        print(f"Using STRESS tier: {jit_bars:,} bars Ã— {jit_params:,} params")
    else:
        jit_bars = TIER_JIT_BARS
        jit_params = TIER_JIT_PARAMS
        print(f"Using WARMUP tier: {jit_bars:,} bars Ã— {jit_params:,} params")
    
    if args.include_python_baseline:
        lanes_cfg.append(PerfConfig("Lane 1 (Py, No Sort)", TIER_TOY_BARS, TIER_TOY_PARAMS, TIER_TOY_HOT_RUNS, TIER_TOY_TIMEOUT, True, False))
        lanes_cfg.append(PerfConfig("Lane 2 (Py, Sort)", TIER_TOY_BARS, TIER_TOY_PARAMS, TIER_TOY_HOT_RUNS, TIER_TOY_TIMEOUT, True, True))
        
    lanes_cfg.append(PerfConfig("Lane 3 (JIT, No Sort)", jit_bars, jit_params, TIER_JIT_HOT_RUNS, TIER_JIT_TIMEOUT, False, False))
    lanes_cfg.append(PerfConfig("Lane 4 (JIT, Sort)", jit_bars, jit_params, TIER_JIT_HOT_RUNS, TIER_JIT_TIMEOUT, False, True))
    
    max_bars = max(c.n_bars for c in lanes_cfg)
    print(f"Generating synthetic data (Max Bars: {max_bars})...")
    ohlc_data = generate_synthetic_data(max_bars)
    
    results = []
    try:
        with tempfile.TemporaryDirectory() as tmp_dir:
            print(f"Created temp dir for IPC: {tmp_dir}")
            for i, cfg in enumerate(lanes_cfg):
                lane_id = i + 1
                if not args.include_python_baseline: lane_id += 2 
                res = run_lane(lane_id, cfg, tmp_dir, ohlc_data, microbench=args.microbench)
                if res:
                    results.append(
                        {
                            "lane_id": lane_id,
                            "name": cfg.name,
                            "res": res,
                            "disable_jit": cfg.disable_jit,
                            "sort_params": cfg.sort_params,
                        }
                    )
                else: results.append({})
                
        print_report(results)
    except RuntimeError as e:
        # Phase 3.0-C: FAIL-FAST - exit with non-zero code on intent_mode violation
        print(f"\nâŒ FAIL-FAST triggered: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()


================================================================================
FILE: scripts/run_funnel.py
================================================================================

#!/usr/bin/env python3
"""
Funnel pipeline CLI entry point.

Reads config and runs funnel pipeline, outputting stage run directories.
"""

from __future__ import annotations

import json
import sys
from pathlib import Path

# Add src to path
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

from FishBroWFS_V2.pipeline.funnel_runner import run_funnel


def load_config(config_path: Path) -> dict:
    """
    Load configuration from JSON file.
    
    Args:
        config_path: Path to JSON config file
        
    Returns:
        Configuration dictionary
    """
    with open(config_path, "r", encoding="utf-8") as f:
        return json.load(f)


def main() -> int:
    """Main entry point."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Run funnel pipeline (Stage0 â†’ Stage1 â†’ Stage2)"
    )
    parser.add_argument(
        "--config",
        type=Path,
        required=True,
        help="Path to JSON configuration file",
    )
    parser.add_argument(
        "--outputs-root",
        type=Path,
        default=Path("outputs"),
        help="Root outputs directory (default: outputs)",
    )
    
    args = parser.parse_args()
    
    try:
        # Load config
        cfg = load_config(args.config)
        
        # Ensure outputs root exists
        args.outputs_root.mkdir(parents=True, exist_ok=True)
        
        # Run funnel
        result_index = run_funnel(cfg, args.outputs_root)
        
        # Print stage run directories (for tracking)
        print("Funnel pipeline completed successfully.")
        print("\nStage run directories:")
        for stage_idx in result_index.stages:
            print(f"  {stage_idx.stage.value}: {stage_idx.run_dir}")
            print(f"    run_id: {stage_idx.run_id}")
        
        return 0
        
    except Exception as e:
        print(f"ERROR: Funnel pipeline failed: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())


================================================================================
FILE: scripts/run_governance.py
================================================================================

#!/usr/bin/env python3
"""CLI entry point for governance evaluation.

Reads artifacts from three stage run directories and produces governance decisions.
"""

from __future__ import annotations

import argparse
import sys
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from FishBroWFS_V2.core.governance_writer import write_governance_artifacts
from FishBroWFS_V2.core.paths import get_run_dir
from FishBroWFS_V2.core.run_id import make_run_id
from FishBroWFS_V2.pipeline.governance_eval import evaluate_governance


def main() -> int:
    """Main CLI entry point."""
    parser = argparse.ArgumentParser(
        description="Evaluate governance rules on funnel stage artifacts",
    )
    parser.add_argument(
        "--stage0-dir",
        type=Path,
        required=True,
        help="Path to Stage0 run directory",
    )
    parser.add_argument(
        "--stage1-dir",
        type=Path,
        required=True,
        help="Path to Stage1 run directory",
    )
    parser.add_argument(
        "--stage2-dir",
        type=Path,
        required=True,
        help="Path to Stage2 run directory",
    )
    parser.add_argument(
        "--outputs-root",
        type=Path,
        required=True,
        help="Root outputs directory (e.g., outputs/)",
    )
    parser.add_argument(
        "--season",
        type=str,
        required=True,
        help="Season identifier",
    )
    
    args = parser.parse_args()
    
    # Validate stage directories exist
    if not args.stage0_dir.exists():
        print(f"Error: Stage0 directory does not exist: {args.stage0_dir}", file=sys.stderr)
        return 1
    if not args.stage1_dir.exists():
        print(f"Error: Stage1 directory does not exist: {args.stage1_dir}", file=sys.stderr)
        return 1
    if not args.stage2_dir.exists():
        print(f"Error: Stage2 directory does not exist: {args.stage2_dir}", file=sys.stderr)
        return 1
    
    # Evaluate governance
    try:
        report = evaluate_governance(
            stage0_dir=args.stage0_dir,
            stage1_dir=args.stage1_dir,
            stage2_dir=args.stage2_dir,
        )
    except Exception as e:
        print(f"Error evaluating governance: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1
    
    # Generate governance_id
    governance_id = make_run_id(prefix="gov")
    
    # Determine governance directory path
    # Format: outputs/seasons/{season}/governance/{governance_id}/
    governance_dir = args.outputs_root / "seasons" / args.season / "governance" / governance_id
    
    # Write artifacts
    try:
        write_governance_artifacts(governance_dir, report)
    except Exception as e:
        print(f"Error writing governance artifacts: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1
    
    # Output governance_dir path (stdout)
    print(str(governance_dir))
    
    return 0


if __name__ == "__main__":
    sys.exit(main())


================================================================================
FILE: scripts/upgrade_winners_v2.py
================================================================================

#!/usr/bin/env python3
from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from typing import Any, Dict

# --- Ensure src/ is on sys.path so `import FishBroWFS_V2` works even when running as a script.
REPO_ROOT = Path(__file__).resolve().parents[1]
SRC_DIR = REPO_ROOT / "src"
if str(SRC_DIR) not in sys.path:
    sys.path.insert(0, str(SRC_DIR))

from FishBroWFS_V2.core.winners_builder import build_winners_v2  # noqa: E402
from FishBroWFS_V2.core.winners_schema import is_winners_v2      # noqa: E402


def _read_json(path: Path) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _write_json(path: Path, obj: Dict[str, Any]) -> None:
    with open(path, "w", encoding="utf-8") as f:
        json.dump(obj, f, sort_keys=True, separators=(",", ":"), indent=2)
        f.write("\n")


def _read_required_artifacts(run_dir: Path) -> Dict[str, Dict[str, Any]]:
    manifest = _read_json(run_dir / "manifest.json")
    config_snapshot = _read_json(run_dir / "config_snapshot.json")
    metrics = _read_json(run_dir / "metrics.json")
    winners = _read_json(run_dir / "winners.json")
    return {
        "manifest": manifest,
        "config_snapshot": config_snapshot,
        "metrics": metrics,
        "winners": winners,
    }


def upgrade_one_run_dir(run_dir: Path, *, dry_run: bool) -> bool:
    winners_path = run_dir / "winners.json"
    if not winners_path.exists():
        return False

    data = _read_required_artifacts(run_dir)
    winners_data = data["winners"]

    if is_winners_v2(winners_data):
        return False

    manifest = data["manifest"]
    config_snapshot = data["config_snapshot"]
    metrics = data["metrics"]

    stage_name = metrics.get("stage_name") or config_snapshot.get("stage_name") or "unknown_stage"
    run_id = manifest.get("run_id", run_dir.name)

    legacy_topk = winners_data.get("topk", [])
    winners_v2 = build_winners_v2(
        stage_name=stage_name,
        run_id=run_id,
        manifest=manifest,
        config_snapshot=config_snapshot,
        legacy_topk=legacy_topk,
    )

    if dry_run:
        print(f"[DRY] would upgrade: {run_dir}")
        return True

    backup_path = run_dir / "winners_legacy.json"
    if not backup_path.exists():
        _write_json(backup_path, winners_data)

    _write_json(winners_path, winners_v2)
    print(f"[OK] upgraded: {run_dir}")
    return True


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--season", required=True)
    ap.add_argument("--outputs-root", required=True)
    ap.add_argument("--dry-run", action="store_true")
    args = ap.parse_args()

    outputs_root = Path(args.outputs_root)
    runs_dir = outputs_root / "seasons" / args.season / "runs"
    if not runs_dir.exists():
        raise SystemExit(f"runs dir not found: {runs_dir}")

    scanned = 0
    changed = 0

    for run_dir in sorted(p for p in runs_dir.iterdir() if p.is_dir()):
        scanned += 1
        try:
            if upgrade_one_run_dir(run_dir, dry_run=args.dry_run):
                changed += 1
        except FileNotFoundError as e:
            print(f"[SKIP] missing file in {run_dir}: {e}")
        except json.JSONDecodeError as e:
            print(f"[SKIP] bad json in {run_dir}: {e}")

    print(f"[DONE] scanned={scanned} changed={changed} dry_run={args.dry_run}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())


================================================================================
FILE: src/FishBroWFS_V2/__init__.py
================================================================================




================================================================================
FILE: src/FishBroWFS_V2/config/__init__.py
================================================================================

"""Configuration constants for FishBroWFS_V2."""


================================================================================
FILE: src/FishBroWFS_V2/config/constants.py
================================================================================

"""Phase 4 constants definition.

These constants define the core parameters for Phase 4 Funnel v1 pipeline.
"""

# Top-K selection parameter
TOPK_K: int = 20

# Stage0 proxy name (must match the proxy implementation name)
STAGE0_PROXY_NAME: str = "ma_proxy_v0"


================================================================================
FILE: src/FishBroWFS_V2/config/dtypes.py
================================================================================

"""Dtype configuration for memory optimization.

Centralized dtype definitions to avoid hardcoding throughout the codebase.
These dtypes are optimized for memory bandwidth while maintaining precision where needed.
"""

import numpy as np

# Stage0: Use float32 for price arrays to reduce memory bandwidth
PRICE_DTYPE_STAGE0 = np.float32

# Stage2: Keep float64 for final PnL accumulation (conservative)
PRICE_DTYPE_STAGE2 = np.float64

# Intent arrays: Use float64 for prices (strict parity), uint8 for enums
INTENT_PRICE_DTYPE = np.float64
INTENT_ENUM_DTYPE = np.uint8  # For role, kind, side

# Index arrays: Use int32 instead of int64 where possible
INDEX_DTYPE = np.int32  # For bar_index, param_id (if within int32 range)


================================================================================
FILE: src/FishBroWFS_V2/control/__init__.py
================================================================================

"""B5-C Mission Control - Job management and worker orchestration."""

from FishBroWFS_V2.control.types import JobRecord, JobSpec, JobStatus, StopMode

__all__ = ["JobRecord", "JobSpec", "JobStatus", "StopMode"]



================================================================================
FILE: src/FishBroWFS_V2/control/api.py
================================================================================

"""FastAPI endpoints for B5-C Mission Control."""

from __future__ import annotations

import os
import signal
import subprocess
import sys
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Any

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

from FishBroWFS_V2.control.jobs_db import (
    create_job,
    get_job,
    init_db,
    list_jobs,
    request_pause,
    request_stop,
)
from FishBroWFS_V2.control.paths import run_log_path
from FishBroWFS_V2.control.preflight import PreflightResult, run_preflight
from FishBroWFS_V2.control.types import JobRecord, JobSpec, StopMode

# Default DB path (can be overridden via environment)
DEFAULT_DB_PATH = Path("outputs/jobs.db")


def get_db_path() -> Path:
    """Get database path from environment or default."""
    db_path_str = os.getenv("JOBS_DB_PATH")
    if db_path_str:
        return Path(db_path_str)
    return DEFAULT_DB_PATH


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifespan context manager for startup/shutdown."""
    # startup
    db_path = get_db_path()
    init_db(db_path)
    yield
    # shutdown (currently empty)


app = FastAPI(title="B5-C Mission Control API", lifespan=lifespan)


@app.get("/health")
async def health() -> dict[str, str]:
    """Health check endpoint."""
    return {"status": "ok"}


@app.get("/jobs", response_model=list[JobRecord])
async def list_jobs_endpoint() -> list[JobRecord]:
    """List recent jobs."""
    db_path = get_db_path()
    return list_jobs(db_path)


@app.get("/jobs/{job_id}", response_model=JobRecord)
async def get_job_endpoint(job_id: str) -> JobRecord:
    """Get job by ID."""
    db_path = get_db_path()
    try:
        return get_job(db_path, job_id)
    except KeyError as e:
        raise HTTPException(status_code=404, detail=str(e))


class CreateJobRequest(BaseModel):
    """Request body for creating a job."""

    season: str
    dataset_id: str
    outputs_root: str
    config_snapshot: dict[str, Any]
    config_hash: str
    created_by: str = "b5c"


@app.post("/jobs")
async def create_job_endpoint(req: CreateJobRequest) -> dict[str, str]:
    """Create a new job."""
    db_path = get_db_path()
    spec = JobSpec(
        season=req.season,
        dataset_id=req.dataset_id,
        outputs_root=req.outputs_root,
        config_snapshot=req.config_snapshot,
        config_hash=req.config_hash,
        created_by=req.created_by,
    )
    job_id = create_job(db_path, spec)
    return {"job_id": job_id}


@app.post("/jobs/{job_id}/check", response_model=PreflightResult)
async def check_job_endpoint(job_id: str) -> PreflightResult:
    """Run preflight check for a job (does not write to DB)."""
    db_path = get_db_path()
    try:
        job = get_job(db_path, job_id)
        result = run_preflight(job.spec.config_snapshot)
        return result
    except KeyError as e:
        raise HTTPException(status_code=404, detail=str(e))


@app.post("/jobs/{job_id}/start")
async def start_job_endpoint(job_id: str) -> dict[str, str]:
    """Start a job (ensure worker is running)."""
    db_path = get_db_path()
    try:
        job = get_job(db_path, job_id)
        
        # If job is QUEUED, worker will pick it up
        # If worker not running, start it
        _ensure_worker_running(db_path)
        
        return {"status": "started", "job_id": job_id}
    except KeyError as e:
        raise HTTPException(status_code=404, detail=str(e))


class PauseRequest(BaseModel):
    """Request body for pause/unpause."""

    pause: bool


@app.post("/jobs/{job_id}/pause")
async def pause_job_endpoint(job_id: str, req: PauseRequest) -> dict[str, str]:
    """Pause/unpause a job."""
    db_path = get_db_path()
    try:
        request_pause(db_path, job_id, req.pause)
        return {"status": "paused" if req.pause else "unpaused", "job_id": job_id}
    except KeyError as e:
        raise HTTPException(status_code=404, detail=str(e))


class StopRequest(BaseModel):
    """Request body for stop."""

    mode: str  # "SOFT" or "KILL"


@app.post("/jobs/{job_id}/stop")
async def stop_job_endpoint(job_id: str, req: StopRequest) -> dict[str, str]:
    """Stop a job."""
    db_path = get_db_path()
    try:
        mode = StopMode(req.mode.upper())
        request_stop(db_path, job_id, mode)
        
        # If KILL, also kill the process
        if mode == StopMode.KILL:
            job = get_job(db_path, job_id)
            if job.pid:
                try:
                    os.kill(job.pid, signal.SIGTERM)
                except ProcessLookupError:
                    pass  # Process already dead
        
        return {"status": "stopped", "job_id": job_id, "mode": mode.value}
    except KeyError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))


@app.get("/jobs/{job_id}/log_tail")
async def get_log_tail_endpoint(job_id: str, n: int = 200) -> dict[str, Any]:
    """
    Return last n lines of worker.log for this job's current run_id.
    
    Args:
        job_id: Job ID
        n: Number of lines to return
        
    Returns:
        Dictionary with ok, log_path, lines, truncated
    """
    db_path = get_db_path()
    try:
        job = get_job(db_path, job_id)
    except KeyError as e:
        raise HTTPException(status_code=404, detail=str(e))
    
    # Get run_id from run_link or use job_id as fallback
    run_id = job.run_link.split("/")[-1] if job.run_link else job_id
    season = job.spec.season
    outputs_root = Path(job.spec.outputs_root)
    
    log_path = run_log_path(outputs_root, season, run_id)
    
    if not log_path.exists():
        return {
            "ok": True,
            "log_path": str(log_path),
            "lines": [],
            "truncated": False,
        }
    
    # Read last n lines
    try:
        with log_path.open("r", encoding="utf-8") as f:
            all_lines = f.readlines()
        
        lines = all_lines[-n:] if len(all_lines) > n else all_lines
        truncated = len(all_lines) > n
        
        return {
            "ok": True,
            "log_path": str(log_path),
            "lines": [line.rstrip("\n") for line in lines],
            "truncated": truncated,
        }
    except Exception as e:
        # Don't 500 on log read errors, but still return ok=True
        return {
            "ok": True,
            "log_path": str(log_path),
            "lines": [],
            "truncated": False,
            "error": str(e),
        }


@app.get("/jobs/{job_id}/report_link")
async def get_report_link_endpoint(job_id: str) -> dict[str, Any]:
    """
    Get report_link for a job.
    
    Phase 6 rule: Always return Viewer URL if run_id exists.
    Viewer will handle missing/invalid artifacts gracefully.
    
    Returns:
        - ok: Always True if job exists
        - report_link: Report link URL (always present if run_id exists)
    """
    from FishBroWFS_V2.control.report_links import build_report_link
    
    db_path = get_db_path()
    try:
        job = get_job(db_path, job_id)
        
        # If job has run_id, build report link (always return Viewer URL)
        if job.run_id:
            # Use job.season if available, otherwise default
            season = getattr(job, "season", "2026Q1") or "2026Q1"
            report_link = build_report_link(season=season, run_id=job.run_id)
            return {"ok": True, "report_link": report_link}
        
        # If no run_id, return empty string (never None)
        return {"ok": True, "report_link": ""}
    except KeyError as e:
        raise HTTPException(status_code=404, detail=str(e))


def _ensure_worker_running(db_path: Path) -> None:
    """
    Ensure worker process is running (start if not).
    
    Args:
        db_path: Path to SQLite database
    """
    # Check if worker is already running (simple check via pidfile)
    pidfile = db_path.parent / "worker.pid"
    if pidfile.exists():
        try:
            pid = int(pidfile.read_text().strip())
            # Check if process exists
            os.kill(pid, 0)
            return  # Worker already running
        except (OSError, ValueError):
            # Process dead, remove pidfile
            pidfile.unlink(missing_ok=True)
    
    # Start worker in background
    proc = subprocess.Popen(
        [sys.executable, "-m", "FishBroWFS_V2.control.worker_main", str(db_path)],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    
    # Write pidfile
    pidfile.write_text(str(proc.pid))



================================================================================
FILE: src/FishBroWFS_V2/control/app_nicegui.py
================================================================================

"""NiceGUI app for B5-C Mission Control."""

from __future__ import annotations

import json
import os
from pathlib import Path

import requests
from nicegui import ui

from FishBroWFS_V2.core.config_hash import stable_config_hash
from FishBroWFS_V2.core.config_snapshot import make_config_snapshot

# API base URL (default to localhost)
API_BASE = "http://localhost:8000"


def create_job_from_config(cfg: dict) -> str:
    """
    Create job from config dict.
    
    Args:
        cfg: Configuration dictionary
        
    Returns:
        Job ID
    """
    
    # Sanitize config
    cfg_snapshot = make_config_snapshot(cfg)
    config_hash = stable_config_hash(cfg_snapshot)
    
    # Prepare request
    req = {
        "season": cfg.get("season", "default"),
        "dataset_id": cfg.get("dataset_id", "default"),
        "outputs_root": str(Path("outputs").absolute()),
        "config_snapshot": cfg_snapshot,
        "config_hash": config_hash,
        "created_by": "b5c",
    }
    
    # POST to API
    resp = requests.post(f"{API_BASE}/jobs", json=req)
    resp.raise_for_status()
    return resp.json()["job_id"]


def get_preflight_result(job_id: str) -> dict:
    """Get preflight result for a job."""
    
    resp = requests.post(f"{API_BASE}/jobs/{job_id}/check")
    resp.raise_for_status()
    return resp.json()


def list_jobs_api() -> list[dict]:
    """List jobs from API."""
    
    resp = requests.get(f"{API_BASE}/jobs")
    resp.raise_for_status()
    return resp.json()


@ui.page("/")
def main_page() -> None:
    """Main B5-C Mission Control page."""
    ui.page_title("B5-C Mission Control")
    
    with ui.row().classes("w-full"):
        # Left: Job List
        with ui.column().classes("w-1/3"):
            ui.label("Job List").classes("text-xl font-bold")
            job_list = ui.column().classes("w-full")
            
            def refresh_job_list() -> None:
                """Refresh job list."""
                job_list.clear()
                try:
                    jobs = list_jobs_api()
                    for job in jobs[:50]:  # Limit to 50
                        status = job["status"]
                        status_color = {
                            "QUEUED": "blue",
                            "RUNNING": "green",
                            "PAUSED": "yellow",
                            "DONE": "gray",
                            "FAILED": "red",
                            "KILLED": "red",
                        }.get(status, "gray")
                        
                        with ui.card().classes("w-full mb-2"):
                            ui.label(f"Job: {job['job_id'][:8]}...").classes("font-mono")
                            ui.label(f"Status: {status}").classes(f"text-{status_color}-600")
                            ui.label(f"Season: {job['spec']['season']}").classes("text-sm")
                            ui.label(f"Dataset: {job['spec']['dataset_id']}").classes("text-sm")
                            
                            # Show Open Report and Open Outputs Folder for DONE jobs
                            if job["status"] == "DONE":
                                with ui.row().classes("w-full mt-2"):
                                    # Show Open Report button if run_id exists
                                    if job.get("run_id"):
                                        def get_report_url(jid: str = job["job_id"]) -> str | None:
                                            """Get report URL from API."""
                                            try:
                                                resp = requests.get(f"{API_BASE}/jobs/{jid}/report_link")
                                                resp.raise_for_status()
                                                data = resp.json()
                                                if data.get("ok") and data.get("report_link"):
                                                    b5_base = os.getenv("FISHBRO_B5_BASE_URL", "http://localhost:8502")
                                                    report_url = f"{b5_base}{data['report_link']}"
                                                    
                                                    # Dev mode assertion (can be disabled in production)
                                                    if os.getenv("FISHBRO_DEV_MODE", "0") == "1":
                                                        assert isinstance(report_url, str), f"report_url must be str, got {type(report_url)}"
                                                        assert report_url.startswith("http"), f"report_url must start with http, got {report_url}"
                                                    
                                                    return report_url
                                                return None
                                            except Exception as e:
                                                ui.notify(f"Error getting report link: {e}", type="negative")
                                                return None
                                        
                                        def open_report(jid: str = job["job_id"]) -> None:
                                            """Open report link."""
                                            report_url = get_report_url(jid)
                                            if report_url:
                                                # Use ui.navigate.to() for external URLs
                                                ui.navigate.to(report_url, new_tab=True)
                                            else:
                                                ui.notify("Report link not available", type="warning")
                                        
                                        ui.button("âœ… Open Report", on_click=lambda: open_report()).classes("bg-blue-500 text-white")
                                    
                                    # Show outputs folder path
                                    if job.get("spec", {}).get("outputs_root"):
                                        outputs_path = job["spec"]["outputs_root"]
                                        ui.label(f"ðŸ“ {outputs_path}").classes("text-xs text-gray-600 ml-2")
                except Exception as e:
                    ui.label(f"Error: {e}").classes("text-red-600")
            
            ui.button("Refresh", on_click=refresh_job_list)
            
            # Demo job button (DEV/demo only)
            def create_demo_job() -> None:
                """Create demo job for Viewer validation."""
                try:
                    from FishBroWFS_V2.control.seed_demo_run import main
                    run_id = main()
                    ui.notify(f"Demo job created: {run_id}", type="positive")
                    refresh_job_list()
                except Exception as e:
                    ui.notify(f"Error creating demo job: {e}", type="negative")
            
            ui.button("Create Demo Job", on_click=create_demo_job).classes("bg-purple-500 text-white mt-2")
            refresh_job_list()
        
        # Right: Config Composer + Control
        with ui.column().classes("w-2/3"):
            ui.label("Config Composer").classes("text-xl font-bold")
            
            # Config inputs
            season_input = ui.input("Season", value="default").classes("w-full")
            dataset_input = ui.input("Dataset ID", value="default").classes("w-full")
            outputs_root_input = ui.input("Outputs Root", value="outputs").classes("w-full")
            
            subsample_slider = ui.slider(
                min=0.01, max=1.0, value=0.1, step=0.01
            ).classes("w-full")
            ui.label().bind_text_from(subsample_slider, "value", lambda v: f"Subsample: {v:.2f}")
            
            mem_limit_input = ui.number("Memory Limit (MB)", value=6000.0).classes("w-full")
            allow_auto_switch = ui.switch("Allow Auto-Downsample", value=True).classes("w-full")
            
            # CHECK Panel
            ui.label("CHECK Panel").classes("text-xl font-bold mt-4")
            check_result = ui.column().classes("w-full")
            
            def run_check() -> None:
                """Run preflight check."""
                check_result.clear()
                try:
                    # Create temp job for check
                    cfg = {
                        "season": season_input.value,
                        "dataset_id": dataset_input.value,
                        "outputs_root": outputs_root_input.value,
                        "bars": 1000,  # Default
                        "params_total": 100,  # Default
                        "param_subsample_rate": subsample_slider.value,
                        "mem_limit_mb": mem_limit_input.value,
                        "allow_auto_downsample": allow_auto_switch.value,
                    }
                    
                    # Create job and check
                    job_id = create_job_from_config(cfg)
                    result = get_preflight_result(job_id)
                    
                    # Display result
                    action = result["action"]
                    action_color = {
                        "PASS": "green",
                        "BLOCK": "red",
                        "AUTO_DOWNSAMPLE": "yellow",
                    }.get(action, "gray")
                    
                    ui.label(f"Action: {action}").classes(f"text-{action_color}-600 font-bold")
                    ui.label(f"Reason: {result['reason']}")
                    ui.label(f"Estimated MB: {result['estimated_mb']:.2f}")
                    ui.label(f"Memory Limit MB: {result['mem_limit_mb']:.2f}")
                    ui.label(f"Ops Est: {result['estimates']['ops_est']:,}")
                    ui.label(f"Time Est (s): {result['estimates']['time_est_s']:.2f}")
                except Exception as e:
                    ui.label(f"Error: {e}").classes("text-red-600")
            
            ui.button("CHECK", on_click=run_check).classes("mt-2")
            
            # Control Buttons
            ui.label("Control").classes("text-xl font-bold mt-4")
            
            current_job_id = ui.label("No job selected").classes("font-mono text-sm")
            
            def start_job() -> None:
                """Start current job."""
                try:
                    # Get latest job
                    jobs = list_jobs_api()
                    if jobs:
                        job_id = jobs[0]["job_id"]
                        resp = requests.post(f"{API_BASE}/jobs/{job_id}/start")
                        resp.raise_for_status()
                        ui.notify("Job started")
                    else:
                        ui.notify("No jobs available", type="warning")
                except Exception as e:
                    ui.notify(f"Error: {e}", type="negative")
            
            def pause_job() -> None:
                """Pause current job."""
                try:
                    jobs = list_jobs_api()
                    if jobs:
                        job_id = jobs[0]["job_id"]
                        resp = requests.post(
                            f"{API_BASE}/jobs/{job_id}/pause", json={"pause": True}
                        )
                        resp.raise_for_status()
                        ui.notify("Job paused")
                except Exception as e:
                    ui.notify(f"Error: {e}", type="negative")
            
            def stop_job_soft() -> None:
                """Stop job (soft)."""
                try:
                    jobs = list_jobs_api()
                    if jobs:
                        job_id = jobs[0]["job_id"]
                        resp = requests.post(
                            f"{API_BASE}/jobs/{job_id}/stop", json={"mode": "SOFT"}
                        )
                        resp.raise_for_status()
                        ui.notify("Job stopped (soft)")
                except Exception as e:
                    ui.notify(f"Error: {e}", type="negative")
            
            def stop_job_kill() -> None:
                """Stop job (kill)."""
                try:
                    jobs = list_jobs_api()
                    if jobs:
                        job_id = jobs[0]["job_id"]
                        resp = requests.post(
                            f"{API_BASE}/jobs/{job_id}/stop", json={"mode": "KILL"}
                        )
                        resp.raise_for_status()
                        ui.notify("Job killed")
                except Exception as e:
                    ui.notify(f"Error: {e}", type="negative")
            
            with ui.row().classes("w-full"):
                ui.button("START", on_click=start_job).classes("bg-green-500")
                ui.button("PAUSE", on_click=pause_job).classes("bg-yellow-500")
                ui.button("STOP (soft)", on_click=stop_job_soft).classes("bg-orange-500")
                ui.button("STOP (kill)", on_click=stop_job_kill).classes("bg-red-500")
            
            # Log Panel
            ui.label("Live Log").classes("text-xl font-bold mt-4")
            log_textarea = ui.textarea("").classes("w-full h-64 font-mono text-sm").props("readonly")
            
            def refresh_log() -> None:
                """Refresh log tail."""
                try:
                    jobs = list_jobs_api()
                    if jobs:
                        job_id = jobs[0]["job_id"]
                        resp = requests.get(f"{API_BASE}/jobs/{job_id}/log_tail?n=200")
                        resp.raise_for_status()
                        data = resp.json()
                        if data["ok"]:
                            log_textarea.value = "\n".join(data["lines"])
                        else:
                            log_textarea.value = f"Error: {data.get('error', 'Unknown error')}"
                    else:
                        log_textarea.value = "No jobs available"
                except Exception as e:
                    log_textarea.value = f"Error: {e}"
            
            ui.button("Refresh Log", on_click=refresh_log).classes("mt-2")
            


if __name__ in {"__main__", "__mp_main__"}:
    ui.run(port=8080, title="B5-C Mission Control")



================================================================================
FILE: src/FishBroWFS_V2/control/jobs_db.py
================================================================================

"""SQLite jobs database - CRUD and state machine."""

from __future__ import annotations

import json
import sqlite3
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional
from uuid import uuid4

from FishBroWFS_V2.control.types import JobRecord, JobSpec, JobStatus, StopMode


def ensure_schema(conn: sqlite3.Connection) -> None:
    """
    Create tables or migrate schema in-place.
    
    Args:
        conn: SQLite connection
    """
    # Check existing columns
    cursor = conn.execute("PRAGMA table_info(jobs)")
    columns = [row[1] for row in cursor.fetchall()]
    
    # Add run_id column if missing
    if "run_id" not in columns:
        conn.execute("ALTER TABLE jobs ADD COLUMN run_id TEXT")
    
    # Add report_link column if missing
    if "report_link" not in columns:
        conn.execute("ALTER TABLE jobs ADD COLUMN report_link TEXT")
    
    conn.commit()


def init_db(db_path: Path) -> None:
    """
    Initialize jobs database schema.
    
    Args:
        db_path: Path to SQLite database file
    """
    db_path.parent.mkdir(parents=True, exist_ok=True)
    
    conn = sqlite3.connect(str(db_path))
    try:
        conn.execute("""
            CREATE TABLE IF NOT EXISTS jobs (
                job_id TEXT PRIMARY KEY,
                status TEXT NOT NULL,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                season TEXT NOT NULL,
                dataset_id TEXT NOT NULL,
                outputs_root TEXT NOT NULL,
                config_hash TEXT NOT NULL,
                config_snapshot_json TEXT NOT NULL,
                pid INTEGER NULL,
                run_id TEXT NULL,
                run_link TEXT NULL,
                report_link TEXT NULL,
                last_error TEXT NULL,
                requested_stop TEXT NULL,
                requested_pause INTEGER NOT NULL DEFAULT 0
            )
        """)
        conn.execute("CREATE INDEX IF NOT EXISTS idx_status ON jobs(status)")
        conn.execute("CREATE INDEX IF NOT EXISTS idx_created_at ON jobs(created_at DESC)")
        
        # Ensure schema is up to date (migration)
        ensure_schema(conn)
        
        conn.commit()
    finally:
        conn.close()


def _now_iso() -> str:
    """Get current UTC time as ISO8601 string."""
    return datetime.now(timezone.utc).isoformat()


def _validate_status_transition(old_status: JobStatus, new_status: JobStatus) -> None:
    """
    Validate status transition (state machine).
    
    Allowed transitions:
    - QUEUED â†’ RUNNING
    - RUNNING â†’ PAUSED (pause=1 and worker checkpoint)
    - PAUSED â†’ RUNNING (pause=0 and worker continues)
    - RUNNING/PAUSED â†’ DONE | FAILED | KILLED
    - QUEUED â†’ KILLED (cancel before running)
    
    Args:
        old_status: Current status
        new_status: Target status
        
    Raises:
        ValueError: If transition is not allowed
    """
    allowed = {
        JobStatus.QUEUED: {JobStatus.RUNNING, JobStatus.KILLED},
        JobStatus.RUNNING: {JobStatus.PAUSED, JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED},
        JobStatus.PAUSED: {JobStatus.RUNNING, JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED},
    }
    
    if old_status in allowed:
        if new_status not in allowed[old_status]:
            raise ValueError(
                f"Invalid status transition: {old_status} â†’ {new_status}. "
                f"Allowed: {allowed[old_status]}"
            )
    elif old_status in {JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED}:
        raise ValueError(f"Cannot transition from terminal status: {old_status}")


def create_job(db_path: Path, spec: JobSpec) -> str:
    """
    Create a new job record.
    
    Args:
        db_path: Path to SQLite database
        spec: Job specification
        
    Returns:
        Generated job_id
    """
    job_id = str(uuid4())
    now = _now_iso()
    
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        conn.execute("""
            INSERT INTO jobs (
                job_id, status, created_at, updated_at,
                season, dataset_id, outputs_root, config_hash,
                config_snapshot_json, requested_pause
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            job_id,
            JobStatus.QUEUED.value,
            now,
            now,
            spec.season,
            spec.dataset_id,
            spec.outputs_root,
            spec.config_hash,
            json.dumps(spec.config_snapshot),
            0,
        ))
        conn.commit()
    finally:
        conn.close()
    
    return job_id


def _row_to_record(row: tuple) -> JobRecord:
    """Convert database row to JobRecord."""
    # Handle schema versions:
    # - Old: 12 columns (before report_link)
    # - Middle: 13 columns (with report_link, before run_id)
    # - New: 14 columns (with run_id and report_link)
    if len(row) == 14:
        # New schema with run_id and report_link
        # Order: job_id, status, created_at, updated_at, season, dataset_id, outputs_root,
        #        config_hash, config_snapshot_json, pid, run_id, run_link, report_link, last_error
        (
            job_id,
            status,
            created_at,
            updated_at,
            season,
            dataset_id,
            outputs_root,
            config_hash,
            config_snapshot_json,
            pid,
            run_id,
            run_link,
            report_link,
            last_error,
        ) = row
    elif len(row) == 13:
        # Middle schema with report_link but no run_id
        (
            job_id,
            status,
            created_at,
            updated_at,
            season,
            dataset_id,
            outputs_root,
            config_hash,
            config_snapshot_json,
            pid,
            run_link,
            last_error,
            report_link,
        ) = row
        run_id = None
    else:
        # Old schema (backward compatibility)
        (
            job_id,
            status,
            created_at,
            updated_at,
            season,
            dataset_id,
            outputs_root,
            config_hash,
            config_snapshot_json,
            pid,
            run_link,
            last_error,
        ) = row
        run_id = None
        report_link = None
    
    spec = JobSpec(
        season=season,
        dataset_id=dataset_id,
        outputs_root=outputs_root,
        config_snapshot=json.loads(config_snapshot_json),
        config_hash=config_hash,
    )
    
    return JobRecord(
        job_id=job_id,
        status=JobStatus(status),
        created_at=created_at,
        updated_at=updated_at,
        spec=spec,
        pid=pid,
        run_id=run_id if run_id else None,
        run_link=run_link,
        report_link=report_link if report_link else None,
        last_error=last_error,
    )


def get_job(db_path: Path, job_id: str) -> JobRecord:
    """
    Get job record by ID.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
        
    Returns:
        JobRecord
        
    Raises:
        KeyError: If job not found
    """
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        cursor = conn.execute("""
            SELECT job_id, status, created_at, updated_at,
                   season, dataset_id, outputs_root, config_hash,
                   config_snapshot_json, pid, 
                   COALESCE(run_id, NULL) as run_id,
                   run_link,
                   COALESCE(report_link, NULL) as report_link,
                   last_error
            FROM jobs
            WHERE job_id = ?
        """, (job_id,))
        row = cursor.fetchone()
        if row is None:
            raise KeyError(f"Job not found: {job_id}")
        return _row_to_record(row)
    finally:
        conn.close()


def list_jobs(db_path: Path, *, limit: int = 50) -> list[JobRecord]:
    """
    List recent jobs.
    
    Args:
        db_path: Path to SQLite database
        limit: Maximum number of jobs to return
        
    Returns:
        List of JobRecord, ordered by created_at DESC
    """
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        cursor = conn.execute("""
            SELECT job_id, status, created_at, updated_at,
                   season, dataset_id, outputs_root, config_hash,
                   config_snapshot_json, pid,
                   COALESCE(run_id, NULL) as run_id,
                   run_link,
                   COALESCE(report_link, NULL) as report_link,
                   last_error
            FROM jobs
            ORDER BY created_at DESC
            LIMIT ?
        """, (limit,))
        return [_row_to_record(row) for row in cursor.fetchall()]
    finally:
        conn.close()


def request_pause(db_path: Path, job_id: str, pause: bool) -> None:
    """
    Request pause/unpause for a job.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
        pause: True to pause, False to unpause
        
    Raises:
        KeyError: If job not found
    """
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        cursor = conn.execute("SELECT status FROM jobs WHERE job_id = ?", (job_id,))
        if cursor.fetchone() is None:
            raise KeyError(f"Job not found: {job_id}")
        
        conn.execute("""
            UPDATE jobs
            SET requested_pause = ?, updated_at = ?
            WHERE job_id = ?
        """, (1 if pause else 0, _now_iso(), job_id))
        conn.commit()
    finally:
        conn.close()


def request_stop(db_path: Path, job_id: str, mode: StopMode) -> None:
    """
    Request stop for a job.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
        mode: Stop mode (SOFT or KILL)
        
    Raises:
        KeyError: If job not found
    """
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        cursor = conn.execute("SELECT status FROM jobs WHERE job_id = ?", (job_id,))
        row = cursor.fetchone()
        if row is None:
            raise KeyError(f"Job not found: {job_id}")
        
        old_status = JobStatus(row[0])
        
        # If QUEUED, immediately mark as KILLED
        if old_status == JobStatus.QUEUED:
            conn.execute("""
                UPDATE jobs
                SET status = ?, requested_stop = ?, updated_at = ?
                WHERE job_id = ?
            """, (JobStatus.KILLED.value, mode.value, _now_iso(), job_id))
        else:
            # Otherwise, set requested_stop flag (worker will handle)
            conn.execute("""
                UPDATE jobs
                SET requested_stop = ?, updated_at = ?
                WHERE job_id = ?
            """, (mode.value, _now_iso(), job_id))
        
        conn.commit()
    finally:
        conn.close()


def update_running(db_path: Path, job_id: str, *, pid: int) -> None:
    """
    Update job to RUNNING status with PID.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
        pid: Process ID
        
    Raises:
        KeyError: If job not found
        ValueError: If status transition is invalid
    """
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        cursor = conn.execute("SELECT status FROM jobs WHERE job_id = ?", (job_id,))
        row = cursor.fetchone()
        if row is None:
            raise KeyError(f"Job not found: {job_id}")
        
        old_status = JobStatus(row[0])
        _validate_status_transition(old_status, JobStatus.RUNNING)
        
        conn.execute("""
            UPDATE jobs
            SET status = ?, pid = ?, updated_at = ?
            WHERE job_id = ?
        """, (JobStatus.RUNNING.value, pid, _now_iso(), job_id))
        conn.commit()
    finally:
        conn.close()


def update_run_link(db_path: Path, job_id: str, *, run_link: str) -> None:
    """
    Update job run_link.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
        run_link: Run link path
    """
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        conn.execute("""
            UPDATE jobs
            SET run_link = ?, updated_at = ?
            WHERE job_id = ?
        """, (run_link, _now_iso(), job_id))
        conn.commit()
    finally:
        conn.close()


def set_report_link(db_path: Path, job_id: str, report_link: str) -> None:
    """
    Set report_link for a job.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
        report_link: Report link URL
    """
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        conn.execute("""
            UPDATE jobs
            SET report_link = ?, updated_at = ?
            WHERE job_id = ?
        """, (report_link, _now_iso(), job_id))
        conn.commit()
    finally:
        conn.close()


def mark_done(
    db_path: Path, 
    job_id: str, 
    *, 
    run_id: Optional[str] = None,
    report_link: Optional[str] = None
) -> None:
    """
    Mark job as DONE.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
        run_id: Optional final stage run_id
        report_link: Optional report link URL
        
    Raises:
        KeyError: If job not found
        ValueError: If status transition is invalid
    """
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        cursor = conn.execute("SELECT status FROM jobs WHERE job_id = ?", (job_id,))
        row = cursor.fetchone()
        if row is None:
            raise KeyError(f"Job not found: {job_id}")
        
        old_status = JobStatus(row[0])
        _validate_status_transition(old_status, JobStatus.DONE)
        
        # Always update run_id and report_link (even if None, to clear old values)
        conn.execute("""
            UPDATE jobs
            SET status = ?, updated_at = ?, run_id = ?, report_link = ?, last_error = NULL
            WHERE job_id = ?
        """, (JobStatus.DONE.value, _now_iso(), run_id, report_link, job_id))
        conn.commit()
    finally:
        conn.close()


def mark_failed(db_path: Path, job_id: str, *, error: str) -> None:
    """
    Mark job as FAILED with error message.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
        error: Error message
        
    Raises:
        KeyError: If job not found
        ValueError: If status transition is invalid
    """
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        cursor = conn.execute("SELECT status FROM jobs WHERE job_id = ?", (job_id,))
        row = cursor.fetchone()
        if row is None:
            raise KeyError(f"Job not found: {job_id}")
        
        old_status = JobStatus(row[0])
        _validate_status_transition(old_status, JobStatus.FAILED)
        
        conn.execute("""
            UPDATE jobs
            SET status = ?, last_error = ?, updated_at = ?
            WHERE job_id = ?
        """, (JobStatus.FAILED.value, error, _now_iso(), job_id))
        conn.commit()
    finally:
        conn.close()


def mark_killed(db_path: Path, job_id: str, *, error: str | None = None) -> None:
    """
    Mark job as KILLED.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
        error: Optional error message
    """
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        conn.execute("""
            UPDATE jobs
            SET status = ?, last_error = ?, updated_at = ?
            WHERE job_id = ?
        """, (JobStatus.KILLED.value, error, _now_iso(), job_id))
        conn.commit()
    finally:
        conn.close()


def get_requested_stop(db_path: Path, job_id: str) -> Optional[str]:
    """
    Get requested_stop value for a job.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
        
    Returns:
        Stop mode string or None
    """
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        cursor = conn.execute("SELECT requested_stop FROM jobs WHERE job_id = ?", (job_id,))
        row = cursor.fetchone()
        return row[0] if row and row[0] else None
    finally:
        conn.close()


def get_requested_pause(db_path: Path, job_id: str) -> bool:
    """
    Get requested_pause value for a job.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
        
    Returns:
        True if pause requested, False otherwise
    """
    conn = sqlite3.connect(str(db_path))
    try:
        ensure_schema(conn)
        cursor = conn.execute("SELECT requested_pause FROM jobs WHERE job_id = ?", (job_id,))
        row = cursor.fetchone()
        return bool(row[0]) if row else False
    finally:
        conn.close()


================================================================================
FILE: src/FishBroWFS_V2/control/paths.py
================================================================================

"""Path helpers for B5-C Mission Control."""

from __future__ import annotations

from pathlib import Path


def run_log_path(outputs_root: Path, season: str, run_id: str) -> Path:
    """
    Return outputs log path for a run (mkdir parents).
    
    Args:
        outputs_root: Root outputs directory
        season: Season identifier
        run_id: Run ID
        
    Returns:
        Path to log file: outputs/{season}/{run_id}/logs/worker.log
    """
    log_path = outputs_root / season / run_id / "logs" / "worker.log"
    log_path.parent.mkdir(parents=True, exist_ok=True)
    return log_path



================================================================================
FILE: src/FishBroWFS_V2/control/preflight.py
================================================================================

"""Preflight check - OOM gate and cost summary."""

from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Literal

from FishBroWFS_V2.core.oom_gate import decide_oom_action


@dataclass(frozen=True)
class PreflightResult:
    """Preflight check result."""

    action: Literal["PASS", "BLOCK", "AUTO_DOWNSAMPLE"]
    reason: str
    original_subsample: float
    final_subsample: float
    estimated_bytes: int
    estimated_mb: float
    mem_limit_mb: float
    mem_limit_bytes: int
    estimates: dict[str, Any]  # must include ops_est, time_est_s, mem_est_mb, ...


def run_preflight(cfg_snapshot: dict[str, Any]) -> PreflightResult:
    """
    Run preflight check (pure, no I/O).
    
    Returns what UI shows in CHECK panel.
    
    Args:
        cfg_snapshot: Sanitized config snapshot (no ndarrays)
        
    Returns:
        PreflightResult with OOM gate decision and estimates
    """
    # Extract mem_limit_mb from config (default: 6000 MB = 6GB)
    mem_limit_mb = float(cfg_snapshot.get("mem_limit_mb", 6000.0))
    
    # Run OOM gate decision
    gate_result = decide_oom_action(
        cfg_snapshot,
        mem_limit_mb=mem_limit_mb,
        allow_auto_downsample=cfg_snapshot.get("allow_auto_downsample", True),
        auto_downsample_step=cfg_snapshot.get("auto_downsample_step", 0.5),
        auto_downsample_min=cfg_snapshot.get("auto_downsample_min", 0.02),
        work_factor=cfg_snapshot.get("work_factor", 2.0),
    )
    
    return PreflightResult(
        action=gate_result["action"],
        reason=gate_result["reason"],
        original_subsample=gate_result["original_subsample"],
        final_subsample=gate_result["final_subsample"],
        estimated_bytes=gate_result["estimated_bytes"],
        estimated_mb=gate_result["estimated_mb"],
        mem_limit_mb=gate_result["mem_limit_mb"],
        mem_limit_bytes=gate_result["mem_limit_bytes"],
        estimates=gate_result["estimates"],
    )



================================================================================
FILE: src/FishBroWFS_V2/control/report_links.py
================================================================================

"""Report link generation for B5 viewer."""

from __future__ import annotations

import os
from pathlib import Path
from urllib.parse import urlencode

# Default outputs root (can be overridden via environment)
DEFAULT_OUTPUTS_ROOT = "outputs"


def get_outputs_root() -> Path:
    """Get outputs root from environment or default."""
    outputs_root_str = os.getenv("FISHBRO_OUTPUTS_ROOT", DEFAULT_OUTPUTS_ROOT)
    return Path(outputs_root_str)


def make_report_link(*, season: str, run_id: str) -> str:
    """
    Generate report link for B5 viewer.
    
    Args:
        season: Season identifier (e.g. "2026Q1")
        run_id: Run ID (e.g. "stage0_coarse-20251218T093512Z-d3caa754")
        
    Returns:
        Report link URL with querystring (e.g. "/?season=2026Q1&run_id=stage0_xxx")
    """
    # Test contract: link.startswith("/?")
    base = "/"
    qs = urlencode({"season": season, "run_id": run_id})
    return f"{base}?{qs}"


def is_report_ready(run_id: str) -> bool:
    """
    Check if report is ready (minimal artifacts exist).
    
    Phase 6 rule: Only check file existence, not content validity.
    Content validation is Viewer's responsibility.
    
    Args:
        run_id: Run ID to check
        
    Returns:
        True if all required artifacts exist, False otherwise
    """
    try:
        outputs_root = get_outputs_root()
        base = outputs_root / run_id
        
        # Check for winners_v2.json first, fallback to winners.json
        winners_v2_path = base / "winners_v2.json"
        winners_path = base / "winners.json"
        winners_exists = winners_v2_path.exists() or winners_path.exists()
        
        required = [
            base / "manifest.json",
            base / "governance.json",
        ]
        
        return winners_exists and all(p.exists() for p in required)
    except Exception:
        return False


def build_report_link(season: str, run_id: str) -> str:
    """
    Build report link for Viewer.
    
    Phase 6 rule: Always return Viewer URL, regardless of readiness.
    Viewer will display BLOCKED/MISSING status if artifacts are missing.
    
    Args:
        season: Season identifier (e.g. "2026Q1")
        run_id: Run ID
        
    Returns:
        Viewer URL with season and run_id query parameters (always a string, never None)
    """
    return f"/b5?season={season}&run_id={run_id}"


================================================================================
FILE: src/FishBroWFS_V2/control/seed_demo_run.py
================================================================================

"""Seed demo run for Viewer validation.

Creates a DONE job with minimal artifacts for Viewer testing.
Does NOT run engine - only writes files.
"""

from __future__ import annotations

import json
import os
import sqlite3
from datetime import datetime, timezone
from pathlib import Path
from uuid import uuid4

from FishBroWFS_V2.control.jobs_db import init_db
from FishBroWFS_V2.control.report_links import build_report_link
from FishBroWFS_V2.control.types import JobStatus
from FishBroWFS_V2.core.paths import ensure_run_dir

# Default DB path (same as api.py)
DEFAULT_DB_PATH = Path("outputs/jobs.db")


def get_db_path() -> Path:
    """Get database path from environment or default."""
    db_path_str = os.getenv("JOBS_DB_PATH")
    if db_path_str:
        return Path(db_path_str)
    return DEFAULT_DB_PATH


def main() -> str:
    """
    Create demo job with minimal artifacts.
    
    Returns:
        run_id of created demo job
        
    Contract:
        - Never raises exceptions
        - Does NOT import engine
        - Does NOT run backtest
        - Does NOT touch worker
        - Does NOT need dataset
    """
    try:
        # Generate run_id
        timestamp = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
        run_id = f"demo_{timestamp}"
        
        # Initialize DB if needed
        db_path = get_db_path()
        init_db(db_path)
        
        # Create outputs directory (use standard path structure: outputs/<season>/runs/<run_id>/)
        outputs_root = Path("outputs")
        season = "2026Q1"  # Default season for demo
        run_dir = ensure_run_dir(outputs_root, season, run_id)
        
        # Write minimal artifacts
        _write_manifest(run_dir, run_id, season)
        _write_winners_v2(run_dir)
        _write_governance(run_dir)
        _write_kpi(run_dir)
        
        # Create job record (status = DONE)
        _create_demo_job(db_path, run_id, season)
        
        return run_id
    
    except Exception as e:
        print(f"ERROR: Failed to create demo job: {e}")
        raise


def _write_manifest(run_dir: Path, run_id: str, season: str) -> None:
    """Write minimal manifest.json."""
    manifest = {
        "run_id": run_id,
        "season": season,
        "config_hash": "demo-config-hash",
        "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        "stages": [],
        "meta": {},
    }
    
    manifest_path = run_dir / "manifest.json"
    with manifest_path.open("w", encoding="utf-8") as f:
        json.dump(manifest, f, indent=2, sort_keys=True)


def _write_winners_v2(run_dir: Path) -> None:
    """Write minimal winners_v2.json."""
    winners_v2 = {
        "config_hash": "demo-config-hash",
        "schema_version": "v2",
        "run_id": "demo",
        "rows": [],
        "meta": {},
    }
    
    winners_path = run_dir / "winners_v2.json"
    with winners_path.open("w", encoding="utf-8") as f:
        json.dump(winners_v2, f, indent=2, sort_keys=True)


def _write_governance(run_dir: Path) -> None:
    """Write minimal governance.json."""
    governance = {
        "config_hash": "demo-config-hash",
        "schema_version": "v1",
        "run_id": "demo",
        "rows": [],
        "meta": {},
    }
    
    governance_path = run_dir / "governance.json"
    with governance_path.open("w", encoding="utf-8") as f:
        json.dump(governance, f, indent=2, sort_keys=True)


def _write_kpi(run_dir: Path) -> None:
    """Write kpi.json with KPI values aligned with Phase 6.1 registry."""
    kpi = {
        "net_profit": 123456,
        "max_drawdown": -0.18,
        "num_trades": 42,
        "final_score": 1.23,
    }
    
    kpi_path = run_dir / "kpi.json"
    with kpi_path.open("w", encoding="utf-8") as f:
        json.dump(kpi, f, indent=2, sort_keys=True)


def _create_demo_job(db_path: Path, run_id: str, season: str) -> None:
    """
    Create demo job record in database.
    
    Uses direct SQL to create job with DONE status and report_link.
    """
    job_id = str(uuid4())
    now = datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")
    
    # Generate report link
    report_link = build_report_link(season=season, run_id=run_id)
    
    conn = sqlite3.connect(str(db_path))
    try:
        # Ensure schema
        from FishBroWFS_V2.control.jobs_db import ensure_schema
        ensure_schema(conn)
        
        # Insert job with DONE status
        # Note: requested_pause is required (defaults to 0)
        conn.execute("""
            INSERT INTO jobs (
                job_id, status, created_at, updated_at,
                season, dataset_id, outputs_root, config_hash,
                config_snapshot_json, requested_pause, run_id, report_link
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            job_id,
            JobStatus.DONE.value,
            now,
            now,
            season,
            "demo_dataset",
            "outputs",
            "demo-config-hash",
            json.dumps({}),
            0,  # requested_pause
            run_id,
            report_link,
        ))
        
        conn.commit()
    finally:
        conn.close()


if __name__ == "__main__":
    run_id = main()
    print(f"Demo job created: {run_id}")
    print(f"Outputs: outputs/seasons/2026Q1/runs/{run_id}/")
    print(f"Report link: /b5?season=2026Q1&run_id={run_id}")


================================================================================
FILE: src/FishBroWFS_V2/control/types.py
================================================================================

"""Type definitions for B5-C Mission Control."""

from __future__ import annotations

from dataclasses import dataclass
from enum import StrEnum
from typing import Any, Literal, Optional


class JobStatus(StrEnum):
    """Job status state machine."""

    QUEUED = "QUEUED"
    RUNNING = "RUNNING"
    PAUSED = "PAUSED"
    DONE = "DONE"
    FAILED = "FAILED"
    KILLED = "KILLED"


class StopMode(StrEnum):
    """Stop request mode."""

    SOFT = "SOFT"
    KILL = "KILL"


@dataclass(frozen=True)
class JobSpec:
    """Job specification (input to create_job)."""

    season: str
    dataset_id: str
    outputs_root: str
    config_snapshot: dict[str, Any]  # sanitized; no ndarrays
    config_hash: str
    created_by: str = "b5c"


@dataclass(frozen=True)
class JobRecord:
    """Job record (returned from DB)."""

    job_id: str
    status: JobStatus
    created_at: str
    updated_at: str
    spec: JobSpec
    pid: Optional[int] = None
    run_id: Optional[str] = None  # Final stage run_id (e.g. stage2_confirm-xxx)
    run_link: Optional[str] = None  # e.g. outputs/.../stage0_run_id or final run index pointer
    report_link: Optional[str] = None  # Link to B5 report viewer
    last_error: Optional[str] = None



================================================================================
FILE: src/FishBroWFS_V2/control/worker.py
================================================================================

"""Worker - long-running task executor."""

from __future__ import annotations

import os
import signal
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional

# âœ… Module-level import for patch support
from FishBroWFS_V2.pipeline.funnel_runner import run_funnel

from FishBroWFS_V2.control.jobs_db import (
    get_job,
    get_requested_pause,
    get_requested_stop,
    mark_done,
    mark_failed,
    mark_killed,
    update_running,
    update_run_link,
)
from FishBroWFS_V2.control.paths import run_log_path
from FishBroWFS_V2.control.report_links import make_report_link
from FishBroWFS_V2.control.types import JobStatus, StopMode


def _append_log(log_path: Path, text: str) -> None:
    """
    Append text to log file.
    
    Args:
        log_path: Path to log file
        text: Text to append
    """
    log_path.parent.mkdir(parents=True, exist_ok=True)
    with log_path.open("a", encoding="utf-8") as f:
        f.write(text)
        if not text.endswith("\n"):
            f.write("\n")


def worker_loop(db_path: Path, *, poll_s: float = 0.5) -> None:
    """
    Worker loop: poll QUEUED jobs and execute them sequentially.
    
    Args:
        db_path: Path to SQLite database
        poll_s: Polling interval in seconds
    """
    while True:
        try:
            # Find QUEUED jobs
            from FishBroWFS_V2.control.jobs_db import list_jobs
            
            jobs = list_jobs(db_path, limit=100)
            queued_jobs = [j for j in jobs if j.status == JobStatus.QUEUED]
            
            if queued_jobs:
                # Process first QUEUED job
                job = queued_jobs[0]
                run_one_job(db_path, job.job_id)
            else:
                # No jobs, sleep
                time.sleep(poll_s)
        except KeyboardInterrupt:
            break
        except Exception as e:
            # Log error but continue loop
            print(f"Worker loop error: {e}")
            time.sleep(poll_s)


def run_one_job(db_path: Path, job_id: str) -> None:
    """
    Run a single job.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
    """
    log_path: Path | None = None
    try:
        job = get_job(db_path, job_id)
        
        # Check if already terminal
        if job.status in {JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED}:
            return
        
        # Update to RUNNING with current PID
        pid = os.getpid()
        update_running(db_path, job_id, pid=pid)
        
        # Log status update
        timestamp = datetime.now(timezone.utc).isoformat()
        outputs_root = Path(job.spec.outputs_root)
        season = job.spec.season
        
        # Initialize log_path early (use job_id as run_id fallback)
        log_path = run_log_path(outputs_root, season, job_id)
        
        # Check for KILL before starting
        stop_mode = get_requested_stop(db_path, job_id)
        if stop_mode == StopMode.KILL.value:
            _append_log(log_path, f"{timestamp} [job_id={job_id}] [status=KILLED] Killed before execution")
            mark_killed(db_path, job_id, error="Killed before execution")
            return
        
        outputs_root.mkdir(parents=True, exist_ok=True)
        
        # Reconstruct runtime config from snapshot
        cfg = dict(job.spec.config_snapshot)
        # Ensure required fields are present
        cfg["season"] = job.spec.season
        cfg["dataset_id"] = job.spec.dataset_id
        
        # Log job start
        _append_log(
            log_path,
            f"{timestamp} [job_id={job_id}] [status=RUNNING] Starting funnel execution"
        )
        
        # Check pause/stop before each stage
        _check_pause_stop(db_path, job_id)
        
        # Run funnel
        result = run_funnel(cfg, outputs_root)
        
        # Extract run_id and generate report_link
        run_id: Optional[str] = None
        report_link: Optional[str] = None
        
        if getattr(result, "stages", None) and result.stages:
            last = result.stages[-1]
            run_id = last.run_id
            report_link = make_report_link(season=job.spec.season, run_id=run_id)
            
            # Update run_link
            run_link = str(last.run_dir)
            update_run_link(db_path, job_id, run_link=run_link)
            
            # Log summary
            log_path = run_log_path(outputs_root, season, run_id)
            timestamp = datetime.now(timezone.utc).isoformat()
            _append_log(
                log_path,
                f"{timestamp} [job_id={job_id}] [status=DONE] Funnel completed: "
                f"run_id={run_id}, stage={last.stage.value}, run_dir={run_link}"
            )
        
        # Mark as done with run_id and report_link (both can be None if no stages)
        mark_done(db_path, job_id, run_id=run_id, report_link=report_link)
        
        # Log final status
        timestamp = datetime.now(timezone.utc).isoformat()
        if log_path:
            _append_log(log_path, f"{timestamp} [job_id={job_id}] [status=DONE] Job completed successfully")
        
    except KeyboardInterrupt:
        if log_path:
            timestamp = datetime.now(timezone.utc).isoformat()
            _append_log(log_path, f"{timestamp} [job_id={job_id}] [status=KILLED] Interrupted by user")
        mark_killed(db_path, job_id, error="Interrupted by user")
        raise
    except Exception as e:
        error_msg = str(e)[:500]  # Limit error message length
        if log_path:
            timestamp = datetime.now(timezone.utc).isoformat()
            _append_log(log_path, f"{timestamp} [job_id={job_id}] [status=FAILED] Error: {error_msg}")
        mark_failed(db_path, job_id, error=error_msg)
        raise


def _check_pause_stop(db_path: Path, job_id: str) -> None:
    """
    Check pause/stop flags and handle accordingly.
    
    Args:
        db_path: Path to SQLite database
        job_id: Job ID
        
    Raises:
        SystemExit: If KILL requested
    """
    stop_mode = get_requested_stop(db_path, job_id)
    if stop_mode == StopMode.KILL.value:
        # Get PID and kill process
        job = get_job(db_path, job_id)
        if job.pid:
            try:
                os.kill(job.pid, signal.SIGTERM)
            except ProcessLookupError:
                pass  # Process already dead
        mark_killed(db_path, job_id, error="Killed by user")
        raise SystemExit("Job killed")
    
    # Handle pause
    while get_requested_pause(db_path, job_id):
        time.sleep(0.5)
        # Re-check stop while paused
        stop_mode = get_requested_stop(db_path, job_id)
        if stop_mode == StopMode.KILL.value:
            job = get_job(db_path, job_id)
            if job.pid:
                try:
                    os.kill(job.pid, signal.SIGTERM)
                except ProcessLookupError:
                    pass
            mark_killed(db_path, job_id, error="Killed while paused")
            raise SystemExit("Job killed while paused")



================================================================================
FILE: src/FishBroWFS_V2/control/worker_main.py
================================================================================

"""Worker main entry point (for subprocess execution)."""

from __future__ import annotations

import sys
from pathlib import Path

from FishBroWFS_V2.control.worker import worker_loop

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python -m FishBroWFS_V2.control.worker_main <db_path>")
        sys.exit(1)
    
    db_path = Path(sys.argv[1])
    worker_loop(db_path)



================================================================================
FILE: src/FishBroWFS_V2/core/__init__.py
================================================================================

"""Core modules for audit and artifact management."""


================================================================================
FILE: src/FishBroWFS_V2/core/artifact_reader.py
================================================================================

"""Artifact reader for governance evaluation and Viewer.

Reads artifacts (manifest/metrics/winners/config_snapshot) from run directories.
Provides safe read functions that never raise exceptions (for Viewer use).
"""

from __future__ import annotations

import hashlib
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional

try:
    import yaml
    HAS_YAML = True
except ImportError:
    HAS_YAML = False


def read_manifest(run_dir: Path) -> Dict[str, Any]:
    """
    Read manifest.json from run directory.
    
    Args:
        run_dir: Path to run directory
        
    Returns:
        Manifest dict (AuditSchema as dict)
        
    Raises:
        FileNotFoundError: If manifest.json does not exist
        json.JSONDecodeError: If manifest.json is invalid JSON
    """
    manifest_path = run_dir / "manifest.json"
    if not manifest_path.exists():
        raise FileNotFoundError(f"manifest.json not found in {run_dir}")
    
    with manifest_path.open("r", encoding="utf-8") as f:
        return json.load(f)


def read_metrics(run_dir: Path) -> Dict[str, Any]:
    """
    Read metrics.json from run directory.
    
    Args:
        run_dir: Path to run directory
        
    Returns:
        Metrics dict
        
    Raises:
        FileNotFoundError: If metrics.json does not exist
        json.JSONDecodeError: If metrics.json is invalid JSON
    """
    metrics_path = run_dir / "metrics.json"
    if not metrics_path.exists():
        raise FileNotFoundError(f"metrics.json not found in {run_dir}")
    
    with metrics_path.open("r", encoding="utf-8") as f:
        return json.load(f)


def read_winners(run_dir: Path) -> Dict[str, Any]:
    """
    Read winners.json from run directory.
    
    Args:
        run_dir: Path to run directory
        
    Returns:
        Winners dict with schema {"topk": [...], "notes": {...}}
        
    Raises:
        FileNotFoundError: If winners.json does not exist
        json.JSONDecodeError: If winners.json is invalid JSON
    """
    winners_path = run_dir / "winners.json"
    if not winners_path.exists():
        raise FileNotFoundError(f"winners.json not found in {run_dir}")
    
    with winners_path.open("r", encoding="utf-8") as f:
        return json.load(f)


def read_config_snapshot(run_dir: Path) -> Dict[str, Any]:
    """
    Read config_snapshot.json from run directory.
    
    Args:
        run_dir: Path to run directory
        
    Returns:
        Config snapshot dict
        
    Raises:
        FileNotFoundError: If config_snapshot.json does not exist
        json.JSONDecodeError: If config_snapshot.json is invalid JSON
    """
    config_path = run_dir / "config_snapshot.json"
    if not config_path.exists():
        raise FileNotFoundError(f"config_snapshot.json not found in {run_dir}")
    
    with config_path.open("r", encoding="utf-8") as f:
        return json.load(f)


# ============================================================================
# Safe artifact reader (never raises) - for Viewer use
# ============================================================================

@dataclass(frozen=True)
class ReadMeta:
    """Metadata about the read operation."""
    source_path: str  # Absolute path to source file
    sha256: str  # SHA256 hash of file content
    mtime_s: float  # Modification time in seconds since epoch


@dataclass(frozen=True)
class ReadResult:
    """
    Result of reading an artifact file.
    
    Contains raw data (dict/list/str) and metadata.
    Upper layer uses pydantic for validation.
    """
    raw: Any  # dict/list/str - raw parsed data
    meta: ReadMeta


@dataclass(frozen=True)
class ReadError:
    """Error information for failed read operations."""
    error_code: str  # "FILE_NOT_FOUND", "UNSUPPORTED_FORMAT", "YAML_NOT_AVAILABLE", "JSON_DECODE_ERROR", "IO_ERROR"
    message: str
    source_path: str


@dataclass(frozen=True)
class SafeReadResult:
    """
    Safe read result that never raises.
    
    Either contains ReadResult (success) or ReadError (failure).
    """
    result: Optional[ReadResult] = None
    error: Optional[ReadError] = None
    
    @property
    def is_ok(self) -> bool:
        """Check if read was successful."""
        return self.result is not None and self.error is None
    
    @property
    def is_error(self) -> bool:
        """Check if read failed."""
        return self.error is not None


def _compute_sha256(file_path: Path) -> str:
    """Compute SHA256 hash of file content."""
    sha256_hash = hashlib.sha256()
    with file_path.open("rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            sha256_hash.update(chunk)
    return sha256_hash.hexdigest()


def read_artifact(file_path: Path | str) -> ReadResult:
    """
    Read artifact file (JSON/YAML/MD) and return ReadResult.
    
    Args:
        file_path: Path to artifact file
        
    Returns:
        ReadResult with raw data and metadata
        
    Raises:
        FileNotFoundError: If file does not exist
        ValueError: If file format is not supported
    """
    path = Path(file_path).resolve()
    
    if not path.exists():
        raise FileNotFoundError(f"Artifact file not found: {path}")
    
    # Get metadata
    mtime_s = path.stat().st_mtime
    sha256 = _compute_sha256(path)
    
    # Read based on extension
    suffix = path.suffix.lower()
    
    if suffix == ".json":
        with path.open("r", encoding="utf-8") as f:
            raw = json.load(f)
    elif suffix in (".yaml", ".yml"):
        if not HAS_YAML:
            raise ValueError(f"YAML support not available. Install pyyaml to read {path}")
        with path.open("r", encoding="utf-8") as f:
            raw = yaml.safe_load(f)
    elif suffix == ".md":
        with path.open("r", encoding="utf-8") as f:
            raw = f.read()  # Return as string for markdown
    else:
        raise ValueError(f"Unsupported file format: {suffix}. Supported: .json, .yaml, .yml, .md")
    
    meta = ReadMeta(
        source_path=str(path),
        sha256=sha256,
        mtime_s=mtime_s,
    )
    
    return ReadResult(raw=raw, meta=meta)


def try_read_artifact(file_path: Path | str) -> SafeReadResult:
    """
    Safe version of read_artifact that never raises.
    
    All Viewer code should use this function instead of read_artifact()
    to ensure no exceptions are thrown.
    
    Args:
        file_path: Path to artifact file
        
    Returns:
        SafeReadResult with either ReadResult (success) or ReadError (failure)
    """
    path = Path(file_path).resolve()
    
    # Check if file exists
    if not path.exists():
        return SafeReadResult(
            error=ReadError(
                error_code="FILE_NOT_FOUND",
                message=f"Artifact file not found: {path}",
                source_path=str(path),
            )
        )
    
    try:
        # Get metadata
        mtime_s = path.stat().st_mtime
        sha256 = _compute_sha256(path)
    except OSError as e:
        return SafeReadResult(
            error=ReadError(
                error_code="IO_ERROR",
                message=f"Failed to read file metadata: {e}",
                source_path=str(path),
            )
        )
    
    # Read based on extension
    suffix = path.suffix.lower()
    
    try:
        if suffix == ".json":
            with path.open("r", encoding="utf-8") as f:
                raw = json.load(f)
        elif suffix in (".yaml", ".yml"):
            if not HAS_YAML:
                return SafeReadResult(
                    error=ReadError(
                        error_code="YAML_NOT_AVAILABLE",
                        message=f"YAML support not available. Install pyyaml to read {path}",
                        source_path=str(path),
                    )
                )
            with path.open("r", encoding="utf-8") as f:
                raw = yaml.safe_load(f)
        elif suffix == ".md":
            with path.open("r", encoding="utf-8") as f:
                raw = f.read()  # Return as string for markdown
        else:
            return SafeReadResult(
                error=ReadError(
                    error_code="UNSUPPORTED_FORMAT",
                    message=f"Unsupported file format: {suffix}. Supported: .json, .yaml, .yml, .md",
                    source_path=str(path),
                )
            )
    except json.JSONDecodeError as e:
        return SafeReadResult(
            error=ReadError(
                error_code="JSON_DECODE_ERROR",
                message=f"JSON decode error: {e}",
                source_path=str(path),
            )
        )
    except OSError as e:
        return SafeReadResult(
            error=ReadError(
                error_code="IO_ERROR",
                message=f"Failed to read file: {e}",
                source_path=str(path),
            )
        )
    except Exception as e:
        return SafeReadResult(
            error=ReadError(
                error_code="UNKNOWN_ERROR",
                message=f"Unexpected error: {e}",
                source_path=str(path),
            )
        )
    
    meta = ReadMeta(
        source_path=str(path),
        sha256=sha256,
        mtime_s=mtime_s,
    )
    
    return SafeReadResult(result=ReadResult(raw=raw, meta=meta))


================================================================================
FILE: src/FishBroWFS_V2/core/artifact_status.py
================================================================================

"""Status determination for artifact validation.

Defines OK/MISSING/INVALID/DIRTY states with human-readable error messages.
"""

from __future__ import annotations

from dataclasses import dataclass
from enum import Enum
from typing import Optional

from pydantic import ValidationError


class ArtifactStatus(str, Enum):
    """Artifact validation status."""
    OK = "OK"
    MISSING = "MISSING"  # File does not exist
    INVALID = "INVALID"  # Pydantic validation error
    DIRTY = "DIRTY"  # config_hash mismatch


@dataclass(frozen=True)
class ValidationResult:
    """
    Result of artifact validation.
    
    Contains status and human-readable error message.
    """
    status: ArtifactStatus
    message: str = ""
    error_details: Optional[str] = None  # Detailed error for debugging


def _format_pydantic_error(e: ValidationError) -> str:
    """Format Pydantic ValidationError into readable string with field paths."""
    parts: list[str] = []
    for err in e.errors():
        loc = ".".join(str(x) for x in err.get("loc", []))
        msg = err.get("msg", "")
        typ = err.get("type", "")
        if loc:
            parts.append(f"{loc}: {msg} ({typ})")
        else:
            parts.append(f"{msg} ({typ})")
    return "ï¼›".join(parts) if parts else str(e)


def _extract_missing_field_names(e: ValidationError) -> list[str]:
    """Extract missing field names from ValidationError."""
    missing: set[str] = set()
    for err in e.errors():
        typ = str(err.get("type", "")).lower()
        msg = str(err.get("msg", "")).lower()
        if "missing" in typ or "required" in msg:
            loc = err.get("loc", ())
            # loc å¯èƒ½åƒ ("rows", 0, "net_profit") æˆ– ("config_hash",)
            if loc:
                leaf = str(loc[-1])
                # é¿å… leaf æ˜¯ index
                if not leaf.isdigit():
                    missing.add(leaf)
            # ä¹ŸæŠŠå®Œæ•´è·¯å¾‘æ”¶é€²ä¾†ï¼ˆå¯è®€æ€§æ›´å¥½ï¼‰
            loc_str = ".".join(str(x) for x in loc if not isinstance(x, int))
            if loc_str:
                missing.add(loc_str.split(".")[-1])  # leaf å†ä¿éšªä¸€æ¬¡
    return sorted(missing)


def validate_manifest_status(
    file_path: str,
    manifest_data: Optional[dict] = None,
    expected_config_hash: Optional[str] = None,
) -> ValidationResult:
    """
    Validate manifest.json status.
    
    Args:
        file_path: Path to manifest.json
        manifest_data: Parsed manifest data (if available)
        expected_config_hash: Expected config_hash (for DIRTY check)
        
    Returns:
        ValidationResult with status and message
    """
    from pathlib import Path
    from FishBroWFS_V2.core.schemas.manifest import RunManifest
    
    path = Path(file_path)
    
    # Check if file exists
    if not path.exists():
        return ValidationResult(
            status=ArtifactStatus.MISSING,
            message=f"manifest.json ä¸å­˜åœ¨: {file_path}",
        )
    
    # Try to parse with Pydantic
    if manifest_data is None:
        import json
        try:
            with path.open("r", encoding="utf-8") as f:
                manifest_data = json.load(f)
        except json.JSONDecodeError as e:
            return ValidationResult(
                status=ArtifactStatus.INVALID,
                message=f"manifest.json JSON æ ¼å¼éŒ¯èª¤: {e}",
                error_details=str(e),
            )
    
    try:
        manifest = RunManifest(**manifest_data)
    except Exception as e:
        # Extract missing field from Pydantic error
        error_msg = str(e)
        missing_fields = []
        if "field required" in error_msg.lower():
            # Try to extract field name from error
            import re
            matches = re.findall(r"Field required.*?['\"]([^'\"]+)['\"]", error_msg)
            if matches:
                missing_fields = matches
        
        if missing_fields:
            msg = f"manifest.json ç¼ºå°‘æ¬„ä½: {', '.join(missing_fields)}"
        else:
            msg = f"manifest.json é©—è­‰å¤±æ•—: {error_msg}"
        
        return ValidationResult(
            status=ArtifactStatus.INVALID,
            message=msg,
            error_details=error_msg,
        )
    
    # Check config_hash if expected is provided
    if expected_config_hash is not None and manifest.config_hash != expected_config_hash:
        return ValidationResult(
            status=ArtifactStatus.DIRTY,
            message=f"manifest.config_hash={manifest.config_hash} ä½†é æœŸå€¼ç‚º {expected_config_hash}",
        )
    
    return ValidationResult(status=ArtifactStatus.OK, message="manifest.json é©—è­‰é€šéŽ")


def validate_winners_v2_status(
    file_path: str,
    winners_data: Optional[dict] = None,
    expected_config_hash: Optional[str] = None,
    manifest_config_hash: Optional[str] = None,
) -> ValidationResult:
    """
    Validate winners_v2.json status.
    
    Args:
        file_path: Path to winners_v2.json
        winners_data: Parsed winners data (if available)
        expected_config_hash: Expected config_hash (for DIRTY check)
        manifest_config_hash: config_hash from manifest (for DIRTY check)
        
    Returns:
        ValidationResult with status and message
    """
    from pathlib import Path
    from FishBroWFS_V2.core.schemas.winners_v2 import WinnersV2
    
    path = Path(file_path)
    
    # Check if file exists
    if not path.exists():
        return ValidationResult(
            status=ArtifactStatus.MISSING,
            message=f"winners_v2.json ä¸å­˜åœ¨: {file_path}",
        )
    
    # Try to parse with Pydantic
    if winners_data is None:
        import json
        try:
            with path.open("r", encoding="utf-8") as f:
                winners_data = json.load(f)
        except json.JSONDecodeError as e:
            return ValidationResult(
                status=ArtifactStatus.INVALID,
                message=f"winners_v2.json JSON æ ¼å¼éŒ¯èª¤: {e}",
                error_details=str(e),
            )
    
    try:
        winners = WinnersV2(**winners_data)
        
        # Validate rows if present (Pydantic already validates required fields)
        # Additional checks for None values (defensive)
        for idx, row in enumerate(winners.rows):
            if row.net_profit is None:
                return ValidationResult(
                    status=ArtifactStatus.INVALID,
                    message=f"winners_v2.json ç¬¬ {idx} è¡Œ net_profit æ˜¯å¿…å¡«æ¬„ä½",
                    error_details=f"row[{idx}].net_profit is None",
                )
            if row.max_drawdown is None:
                return ValidationResult(
                    status=ArtifactStatus.INVALID,
                    message=f"winners_v2.json ç¬¬ {idx} è¡Œ max_drawdown æ˜¯å¿…å¡«æ¬„ä½",
                    error_details=f"row[{idx}].max_drawdown is None",
                )
            if row.trades is None:
                return ValidationResult(
                    status=ArtifactStatus.INVALID,
                    message=f"winners_v2.json ç¬¬ {idx} è¡Œ trades æ˜¯å¿…å¡«æ¬„ä½",
                    error_details=f"row[{idx}].trades is None",
                )
    except ValidationError as e:
        missing_fields = _extract_missing_field_names(e)
        missing_txt = f"ç¼ºå°‘æ¬„ä½: {', '.join(missing_fields)}ï¼›" if missing_fields else ""
        error_details = str(e) + "\nmissing_fields=" + ",".join(missing_fields) if missing_fields else str(e)
        return ValidationResult(
            status=ArtifactStatus.INVALID,
            message=f"winners_v2.json {missing_txt}schema é©—è­‰å¤±æ•—ï¼š{_format_pydantic_error(e)}",
            error_details=error_details,
        )
    except Exception as e:
        # Fallback for non-Pydantic errors
        return ValidationResult(
            status=ArtifactStatus.INVALID,
            message=f"winners_v2.json é©—è­‰å¤±æ•—: {e}",
            error_details=str(e),
        )
    
    # Check config_hash if expected/manifest is provided
    if expected_config_hash is not None:
        if winners.config_hash != expected_config_hash:
            return ValidationResult(
                status=ArtifactStatus.DIRTY,
                message=f"winners_v2.config_hash={winners.config_hash} ä½†é æœŸå€¼ç‚º {expected_config_hash}",
            )
    
    if manifest_config_hash is not None:
        if winners.config_hash != manifest_config_hash:
            return ValidationResult(
                status=ArtifactStatus.DIRTY,
                message=f"winners_v2.config_hash={winners.config_hash} ä½† manifest.config_hash={manifest_config_hash}",
            )
    
    return ValidationResult(status=ArtifactStatus.OK, message="winners_v2.json é©—è­‰é€šéŽ")


def validate_governance_status(
    file_path: str,
    governance_data: Optional[dict] = None,
    expected_config_hash: Optional[str] = None,
    manifest_config_hash: Optional[str] = None,
) -> ValidationResult:
    """
    Validate governance.json status.
    
    Args:
        file_path: Path to governance.json
        governance_data: Parsed governance data (if available)
        expected_config_hash: Expected config_hash (for DIRTY check)
        manifest_config_hash: config_hash from manifest (for DIRTY check)
        
    Returns:
        ValidationResult with status and message
    """
    from pathlib import Path
    from FishBroWFS_V2.core.schemas.governance import GovernanceReport
    
    path = Path(file_path)
    
    # Check if file exists
    if not path.exists():
        return ValidationResult(
            status=ArtifactStatus.MISSING,
            message=f"governance.json ä¸å­˜åœ¨: {file_path}",
        )
    
    # Try to parse with Pydantic
    if governance_data is None:
        import json
        try:
            with path.open("r", encoding="utf-8") as f:
                governance_data = json.load(f)
        except json.JSONDecodeError as e:
            return ValidationResult(
                status=ArtifactStatus.INVALID,
                message=f"governance.json JSON æ ¼å¼éŒ¯èª¤: {e}",
                error_details=str(e),
            )
    
    try:
        governance = GovernanceReport(**governance_data)
    except Exception as e:
        # Extract missing field from Pydantic error
        error_msg = str(e)
        missing_fields = []
        if "field required" in error_msg.lower():
            import re
            matches = re.findall(r"Field required.*?['\"]([^'\"]+)['\"]", error_msg)
            if matches:
                missing_fields = matches
        
        if missing_fields:
            msg = f"governance.json ç¼ºå°‘æ¬„ä½: {', '.join(missing_fields)}"
        else:
            msg = f"governance.json é©—è­‰å¤±æ•—: {error_msg}"
        
        return ValidationResult(
            status=ArtifactStatus.INVALID,
            message=msg,
            error_details=error_msg,
        )
    
    # Check config_hash if expected/manifest is provided
    if expected_config_hash is not None:
        if governance.config_hash != expected_config_hash:
            return ValidationResult(
                status=ArtifactStatus.DIRTY,
                message=f"governance.config_hash={governance.config_hash} ä½†é æœŸå€¼ç‚º {expected_config_hash}",
            )
    
    if manifest_config_hash is not None:
        if governance.config_hash != manifest_config_hash:
            return ValidationResult(
                status=ArtifactStatus.DIRTY,
                message=f"governance.config_hash={governance.config_hash} ä½† manifest.config_hash={manifest_config_hash}",
            )
    
    return ValidationResult(status=ArtifactStatus.OK, message="governance.json é©—è­‰é€šéŽ")


================================================================================
FILE: src/FishBroWFS_V2/core/artifacts.py
================================================================================

"""Artifact writer for unified run output.

Provides consistent artifact structure for all runs, with mandatory
subsample rate visibility.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict

from FishBroWFS_V2.core.winners_builder import build_winners_v2
from FishBroWFS_V2.core.winners_schema import is_winners_legacy, is_winners_v2


def _write_json(path: Path, obj: Any) -> None:
    """
    Write object to JSON file with fixed format.
    
    Uses sort_keys=True and fixed separators for reproducibility.
    
    Args:
        path: Path to JSON file
        obj: Object to serialize
    """
    path.write_text(
        json.dumps(obj, ensure_ascii=False, sort_keys=True, indent=2) + "\n",
        encoding="utf-8",
    )


def write_run_artifacts(
    run_dir: Path,
    manifest: Dict[str, Any],
    config_snapshot: Dict[str, Any],
    metrics: Dict[str, Any],
    winners: Dict[str, Any] | None = None,
) -> None:
    """
    Write all standard artifacts for a run.
    
    Creates the following files:
    - manifest.json: Full AuditSchema data
    - config_snapshot.json: Original/normalized config
    - metrics.json: Performance metrics
    - winners.json: Top-K results (fixed schema)
    - README.md: Human-readable summary
    - logs.txt: Execution logs (empty initially)
    
    Args:
        run_dir: Run directory path (will be created if needed)
        manifest: Manifest data (AuditSchema as dict)
        config_snapshot: Configuration snapshot
        metrics: Performance metrics (must include param_subsample_rate visibility)
        winners: Optional winners dict. If None, uses empty schema.
            Must follow schema: {"topk": [...], "notes": {"schema": "v1", ...}}
    """
    run_dir.mkdir(parents=True, exist_ok=True)
    
    # Write manifest.json (full AuditSchema)
    _write_json(run_dir / "manifest.json", manifest)
    
    # Write config_snapshot.json
    _write_json(run_dir / "config_snapshot.json", config_snapshot)
    
    # Write metrics.json (must include param_subsample_rate visibility)
    _write_json(run_dir / "metrics.json", metrics)
    
    # Write winners.json (always output v2 schema)
    if winners is None:
        winners = {"topk": [], "notes": {"schema": "v1"}}
    
    # Auto-upgrade legacy winners to v2
    if is_winners_legacy(winners):
        # Convert legacy to v2
        legacy_topk = winners.get("topk", [])
        run_id = manifest.get("run_id", "unknown")
        stage_name = metrics.get("stage_name", "unknown")
        
        winners = build_winners_v2(
            stage_name=stage_name,
            run_id=run_id,
            manifest=manifest,
            config_snapshot=config_snapshot,
            legacy_topk=legacy_topk,
        )
    elif not is_winners_v2(winners):
        # Unknown format - try to upgrade anyway (defensive)
        legacy_topk = winners.get("topk", [])
        if legacy_topk:
            run_id = manifest.get("run_id", "unknown")
            stage_name = metrics.get("stage_name", "unknown")
            
            winners = build_winners_v2(
                stage_name=stage_name,
                run_id=run_id,
                manifest=manifest,
                config_snapshot=config_snapshot,
                legacy_topk=legacy_topk,
            )
        else:
            # Empty topk - create minimal v2 structure
            from FishBroWFS_V2.core.winners_schema import build_winners_v2_dict
            winners = build_winners_v2_dict(
                stage_name=metrics.get("stage_name", "unknown"),
                run_id=manifest.get("run_id", "unknown"),
                topk=[],
            )
    
    _write_json(run_dir / "winners.json", winners)
    
    # Write README.md (human-readable summary)
    # Must prominently display param_subsample_rate
    readme_lines = [
        "# FishBroWFS_V2 Run",
        "",
        f"- run_id: {manifest.get('run_id')}",
        f"- git_sha: {manifest.get('git_sha')}",
        f"- param_subsample_rate: {manifest.get('param_subsample_rate')}",
        f"- season: {manifest.get('season')}",
        f"- dataset_id: {manifest.get('dataset_id')}",
        f"- bars: {manifest.get('bars')}",
        f"- params_total: {manifest.get('params_total')}",
        f"- params_effective: {manifest.get('params_effective')}",
        f"- config_hash: {manifest.get('config_hash')}",
    ]
    
    # Add OOM gate information if present in metrics
    if "oom_gate_action" in metrics:
        readme_lines.extend([
            "",
            "## OOM Gate",
            "",
            f"- action: {metrics.get('oom_gate_action')}",
            f"- reason: {metrics.get('oom_gate_reason')}",
            f"- mem_est_mb: {metrics.get('mem_est_mb', 0):.1f}",
            f"- mem_limit_mb: {metrics.get('mem_limit_mb', 0):.1f}",
            f"- ops_est: {metrics.get('ops_est', 0)}",
        ])
        
        # If auto-downsample occurred, show original and final
        if metrics.get("oom_gate_action") == "AUTO_DOWNSAMPLE":
            readme_lines.extend([
                f"- original_subsample: {metrics.get('oom_gate_original_subsample', 0)}",
                f"- final_subsample: {metrics.get('oom_gate_final_subsample', 0)}",
            ])
    
    readme = "\n".join(readme_lines)
    (run_dir / "README.md").write_text(readme, encoding="utf-8")
    
    # Write logs.txt (empty initially)
    (run_dir / "logs.txt").write_text("", encoding="utf-8")


================================================================================
FILE: src/FishBroWFS_V2/core/audit_schema.py
================================================================================

"""Audit schema for run tracking and reproducibility.

Single Source of Truth (SSOT) for audit data.
"""

from __future__ import annotations

from dataclasses import dataclass, asdict
from datetime import datetime, timezone
from typing import Any, Dict


@dataclass(frozen=True)
class AuditSchema:
    """
    Audit schema for run tracking.
    
    All fields are required and must be JSON-serializable.
    This is the Single Source of Truth (SSOT) for audit data.
    """
    run_id: str
    created_at: str  # ISO8601 with Z suffix (UTC)
    git_sha: str  # At least 12 chars
    dirty_repo: bool  # Whether repo has uncommitted changes
    param_subsample_rate: float  # Required, must be in [0.0, 1.0]
    config_hash: str  # Stable hash of config
    season: str  # Season identifier
    dataset_id: str  # Dataset identifier
    bars: int  # Number of bars processed
    params_total: int  # Total parameters before subsample
    params_effective: int  # Effective parameters after subsample (= int(params_total * param_subsample_rate))
    artifact_version: str = "v1"  # Artifact version
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)


def compute_params_effective(params_total: int, param_subsample_rate: float) -> int:
    """
    Compute effective parameters after subsample.
    
    Rounding rule: int(params_total * param_subsample_rate)
    This is locked in code/docs/tests - do not change.
    
    Args:
        params_total: Total parameters before subsample
        param_subsample_rate: Subsample rate in [0.0, 1.0]
        
    Returns:
        Effective parameters (integer, rounded down)
    """
    if not (0.0 <= param_subsample_rate <= 1.0):
        raise ValueError(f"param_subsample_rate must be in [0.0, 1.0], got {param_subsample_rate}")
    
    return int(params_total * param_subsample_rate)


================================================================================
FILE: src/FishBroWFS_V2/core/config_hash.py
================================================================================

"""Stable config hash computation.

Provides deterministic hash of configuration objects for reproducibility.
"""

from __future__ import annotations

import hashlib
import json
from typing import Any


def stable_config_hash(obj: Any) -> str:
    """
    Compute stable hash of configuration object.
    
    Uses JSON serialization with sorted keys and fixed separators
    to ensure cross-platform consistency.
    
    Args:
        obj: Configuration object (dict, list, etc.)
        
    Returns:
        Hex string hash (64 chars, SHA256)
    """
    s = json.dumps(
        obj,
        sort_keys=True,
        separators=(",", ":"),
        ensure_ascii=False,
    )
    return hashlib.sha256(s.encode("utf-8")).hexdigest()


================================================================================
FILE: src/FishBroWFS_V2/core/config_snapshot.py
================================================================================

"""Config snapshot sanitizer.

Creates JSON-serializable config snapshots by excluding large ndarrays
and converting numpy types to Python native types.
"""

from __future__ import annotations

from typing import Any, Dict

import numpy as np

# These keys will make artifacts garbage or directly crash JSON serialization
_DEFAULT_DROP_KEYS = {
    "open_",
    "open",
    "high",
    "low",
    "close",
    "volume",
    "params_matrix",
}


def _ndarray_meta(x: np.ndarray) -> Dict[str, Any]:
    """
    Create metadata dict for ndarray (shape and dtype only).
    
    Args:
        x: numpy array
        
    Returns:
        Metadata dictionary with shape and dtype
    """
    return {
        "__ndarray__": True,
        "shape": list(x.shape),
        "dtype": str(x.dtype),
    }


def make_config_snapshot(
    cfg: Dict[str, Any],
    drop_keys: set[str] | None = None,
) -> Dict[str, Any]:
    """
    Create sanitized config snapshot for JSON serialization and hashing.
    
    Rules (locked):
    - Must include: season, dataset_id, bars, params_total, param_subsample_rate,
      stage_name, topk, commission, slip, order_qty, config knobs...
    - Must exclude/replace: open_, high, low, close, params_matrix (ndarrays)
    - If metadata needed, only keep shape/dtype (no bytes hash to avoid cost)
    
    Args:
        cfg: Configuration dictionary (may contain ndarrays)
        drop_keys: Optional set of keys to drop. If None, uses default.
        
    Returns:
        Sanitized config dictionary (JSON-serializable)
    """
    drop = _DEFAULT_DROP_KEYS if drop_keys is None else drop_keys
    out: Dict[str, Any] = {}
    
    for k, v in cfg.items():
        if k in drop:
            # Don't keep raw data, only metadata (optional)
            if isinstance(v, np.ndarray):
                out[k + "_meta"] = _ndarray_meta(v)
            continue
        
        # numpy scalar -> python scalar
        if isinstance(v, (np.floating, np.integer)):
            out[k] = v.item()
        # ndarray (if slipped through) -> meta
        elif isinstance(v, np.ndarray):
            out[k + "_meta"] = _ndarray_meta(v)
        # Basic types: keep as-is
        elif isinstance(v, (str, int, float, bool)) or v is None:
            out[k] = v
        # list/tuple: conservative handling (avoid strange objects)
        elif isinstance(v, (list, tuple)):
            # Check if list contains only serializable types
            try:
                # Try to serialize to verify
                import json
                json.dumps(v)
                out[k] = v
            except (TypeError, ValueError):
                # If not serializable, convert to string representation
                out[k] = str(v)
        # Other types: convert to string (avoid JSON crash)
        else:
            out[k] = str(v)
    
    return out


================================================================================
FILE: src/FishBroWFS_V2/core/governance_schema.py
================================================================================

"""Governance schema for decision tracking and auditability.

Single Source of Truth (SSOT) for governance decisions.
"""

from __future__ import annotations

from dataclasses import dataclass, asdict
from enum import Enum
from typing import Any, Dict, List


class Decision(str, Enum):
    """Governance decision types."""
    KEEP = "KEEP"
    FREEZE = "FREEZE"
    DROP = "DROP"


@dataclass(frozen=True)
class EvidenceRef:
    """
    Reference to evidence used in governance decision.
    
    Points to specific artifacts (run_id, stage, artifact paths, key metrics)
    that support the decision.
    """
    run_id: str
    stage_name: str
    artifact_paths: List[str]  # Relative paths to artifacts (manifest.json, metrics.json, etc.)
    key_metrics: Dict[str, Any]  # Key metrics extracted from artifacts


@dataclass(frozen=True)
class GovernanceItem:
    """
    Governance decision for a single candidate.
    
    Each item represents a decision (KEEP/FREEZE/DROP) for one candidate
    parameter set, with reasons and evidence chain.
    """
    candidate_id: str  # Stable identifier: strategy_id:params_hash[:12]
    decision: Decision
    reasons: List[str]  # Human-readable reasons for decision
    evidence: List[EvidenceRef]  # Evidence chain supporting decision
    created_at: str  # ISO8601 with Z suffix (UTC)
    git_sha: str  # Git SHA at time of governance evaluation


@dataclass(frozen=True)
class GovernanceReport:
    """
    Complete governance report for a set of candidates.
    
    Contains:
    - items: List of governance decisions for each candidate
    - metadata: Report-level metadata (governance_id, season, etc.)
    """
    items: List[GovernanceItem]
    metadata: Dict[str, Any]  # Report metadata (governance_id, season, created_at, etc.)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            "items": [
                {
                    "candidate_id": item.candidate_id,
                    "decision": item.decision.value,
                    "reasons": item.reasons,
                    "evidence": [
                        {
                            "run_id": ev.run_id,
                            "stage_name": ev.stage_name,
                            "artifact_paths": ev.artifact_paths,
                            "key_metrics": ev.key_metrics,
                        }
                        for ev in item.evidence
                    ],
                    "created_at": item.created_at,
                    "git_sha": item.git_sha,
                }
                for item in self.items
            ],
            "metadata": self.metadata,
        }


================================================================================
FILE: src/FishBroWFS_V2/core/governance_writer.py
================================================================================

"""Governance writer for decision artifacts.

Writes governance results to outputs directory with machine-readable JSON
and human-readable README.
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict

from FishBroWFS_V2.core.governance_schema import GovernanceReport
from FishBroWFS_V2.core.run_id import make_run_id


def write_governance_artifacts(
    governance_dir: Path,
    report: GovernanceReport,
) -> None:
    """
    Write governance artifacts to directory.
    
    Creates:
    - governance.json: Machine-readable governance report
    - README.md: Human-readable summary
    - evidence_index.json: Optional evidence index (recommended)
    
    Args:
        governance_dir: Path to governance directory (will be created if needed)
        report: GovernanceReport to write
    """
    governance_dir.mkdir(parents=True, exist_ok=True)
    
    # Write governance.json (machine-readable SSOT)
    governance_dict = report.to_dict()
    governance_path = governance_dir / "governance.json"
    with governance_path.open("w", encoding="utf-8") as f:
        json.dump(
            governance_dict,
            f,
            ensure_ascii=False,
            sort_keys=True,
            indent=2,
        )
        f.write("\n")
    
    # Write README.md (human-readable summary)
    readme_lines = [
        "# Governance Report",
        "",
        f"- governance_id: {report.metadata.get('governance_id')}",
        f"- season: {report.metadata.get('season')}",
        f"- created_at: {report.metadata.get('created_at')}",
        f"- git_sha: {report.metadata.get('git_sha')}",
        "",
        "## Decision Summary",
        "",
    ]
    
    decisions = report.metadata.get("decisions", {})
    readme_lines.extend([
        f"- KEEP: {decisions.get('KEEP', 0)}",
        f"- FREEZE: {decisions.get('FREEZE', 0)}",
        f"- DROP: {decisions.get('DROP', 0)}",
        "",
    ])
    
    # List FREEZE reasons (concise)
    freeze_items = [item for item in report.items if item.decision.value == "FREEZE"]
    if freeze_items:
        readme_lines.extend([
            "## FREEZE Reasons",
            "",
        ])
        for item in freeze_items:
            reasons_str = "; ".join(item.reasons)
            readme_lines.append(f"- {item.candidate_id}: {reasons_str}")
        readme_lines.append("")
    
    # Subsample/params_effective summary
    readme_lines.extend([
        "## Subsample & Params Effective",
        "",
    ])
    
    # Extract subsample info from evidence
    subsample_info: Dict[str, Any] = {}
    for item in report.items:
        for ev in item.evidence:
            stage = ev.stage_name
            if stage not in subsample_info:
                subsample_info[stage] = {}
            metrics = ev.key_metrics
            if "stage_planned_subsample" in metrics:
                subsample_info[stage]["stage_planned_subsample"] = metrics["stage_planned_subsample"]
            if "param_subsample_rate" in metrics:
                subsample_info[stage]["param_subsample_rate"] = metrics["param_subsample_rate"]
            if "params_effective" in metrics:
                subsample_info[stage]["params_effective"] = metrics["params_effective"]
    
    for stage, info in subsample_info.items():
        readme_lines.append(f"### {stage}")
        if "stage_planned_subsample" in info:
            readme_lines.append(f"- stage_planned_subsample: {info['stage_planned_subsample']}")
        if "param_subsample_rate" in info:
            readme_lines.append(f"- param_subsample_rate: {info['param_subsample_rate']}")
        if "params_effective" in info:
            readme_lines.append(f"- params_effective: {info['params_effective']}")
        readme_lines.append("")
    
    readme = "\n".join(readme_lines)
    readme_path = governance_dir / "README.md"
    readme_path.write_text(readme, encoding="utf-8")
    
    # Write evidence_index.json (optional but recommended)
    evidence_index = {
        "governance_id": report.metadata.get("governance_id"),
        "evidence_by_candidate": {
            item.candidate_id: [
                {
                    "run_id": ev.run_id,
                    "stage_name": ev.stage_name,
                    "artifact_paths": ev.artifact_paths,
                }
                for ev in item.evidence
            ]
            for item in report.items
        },
    }
    evidence_index_path = governance_dir / "evidence_index.json"
    with evidence_index_path.open("w", encoding="utf-8") as f:
        json.dump(
            evidence_index,
            f,
            ensure_ascii=False,
            sort_keys=True,
            indent=2,
        )
        f.write("\n")


================================================================================
FILE: src/FishBroWFS_V2/core/oom_cost_model.py
================================================================================

"""OOM cost model for memory and computation estimation.

Provides conservative estimates for memory usage and operations
to enable OOM gate decisions before stage execution.
"""

from __future__ import annotations

from typing import Any, Dict

import numpy as np


def _bytes_of_array(a: Any) -> int:
    """
    Get bytes of numpy array.
    
    Args:
        a: Array-like object
        
    Returns:
        Number of bytes (0 if not ndarray)
    """
    if isinstance(a, np.ndarray):
        return int(a.nbytes)
    return 0


def estimate_memory_bytes(
    cfg: Dict[str, Any],
    work_factor: float = 2.0,
) -> int:
    """
    Estimate memory usage in bytes (conservative upper bound).
    
    Memory estimation includes:
    - Price arrays: open/high/low/close (if present)
    - Params matrix: params_total * param_dim * 8 bytes (if present)
    - Working buffers: conservative multiplier (work_factor)
    
    Note: This is a conservative estimate. Actual usage may be lower,
    but gate uses this to prevent OOM failures.
    
    Args:
        cfg: Configuration dictionary containing:
            - bars: Number of bars
            - params_total: Total parameters
            - param_subsample_rate: Subsample rate
            - open_, high, low, close: Optional OHLC arrays
            - params_matrix: Optional parameter matrix
        work_factor: Conservative multiplier for working buffers (default: 2.0)
        
    Returns:
        Estimated memory in bytes
    """
    mem = 0
    
    # Price arrays (if present)
    for k in ("open_", "open", "high", "low", "close"):
        mem += _bytes_of_array(cfg.get(k))
    
    # Params matrix
    mem += _bytes_of_array(cfg.get("params_matrix"))
    
    # Conservative working buffers
    # Note: This is a conservative multiplier to account for:
    # - Intermediate computation buffers
    # - Indicator arrays (donchian, ATR, etc.)
    # - Intent arrays
    # - Fill arrays
    mem = int(mem * float(work_factor))
    
    # Note: We do NOT reduce mem by subsample_rate here because:
    # 1. Some allocations are per-bar (not per-param)
    # 2. Working buffers may scale differently
    # 3. Conservative estimate is safer for OOM prevention
    
    return mem


def estimate_ops(cfg: Dict[str, Any]) -> int:
    """
    Estimate operations count (coarse approximation).
    
    Baseline: per-bar per-effective-param operations.
    This is a coarse estimate for cost tracking.
    
    Args:
        cfg: Configuration dictionary containing:
            - bars: Number of bars
            - params_total: Total parameters
            - param_subsample_rate: Subsample rate
            
    Returns:
        Estimated operations count
    """
    bars = int(cfg.get("bars", 0))
    params_total = int(cfg.get("params_total", 0))
    subsample_rate = float(cfg.get("param_subsample_rate", 1.0))
    
    # Effective params after subsample (floor rule)
    params_effective = int(params_total * subsample_rate)
    
    # Baseline: per-bar per-effective-param step (coarse)
    ops = int(bars * params_effective)
    
    return ops


def estimate_time_s(cfg: Dict[str, Any]) -> float | None:
    """
    Estimate execution time in seconds (optional).
    
    This is a placeholder for future time estimation.
    Currently returns None.
    
    Args:
        cfg: Configuration dictionary
        
    Returns:
        Estimated time in seconds (None if not available)
    """
    # Placeholder for future implementation
    return None


def summarize_estimates(cfg: Dict[str, Any]) -> Dict[str, Any]:
    """
    Summarize all estimates in a JSON-serializable dict.
    
    Args:
        cfg: Configuration dictionary
        
    Returns:
        Dictionary with estimates:
        - mem_est_bytes: Memory estimate in bytes
        - mem_est_mb: Memory estimate in MB
        - ops_est: Operations estimate
        - time_est_s: Time estimate in seconds (None if not available)
    """
    mem_b = estimate_memory_bytes(cfg)
    ops = estimate_ops(cfg)
    time_s = estimate_time_s(cfg)
    
    return {
        "mem_est_bytes": mem_b,
        "mem_est_mb": mem_b / (1024.0 * 1024.0),
        "ops_est": ops,
        "time_est_s": time_s,
    }


================================================================================
FILE: src/FishBroWFS_V2/core/oom_gate.py
================================================================================

"""OOM gate decision maker.

Pure functions for estimating memory usage and deciding PASS/BLOCK/AUTO_DOWNSAMPLE.
No engine dependencies, no file I/O - pure computation only.

This module provides two APIs:
1. New API (for B5-C): estimate_bytes(), decide_gate() with Pydantic schemas
2. Legacy API (for pipeline/tests): decide_oom_action() with dict I/O
"""

from __future__ import annotations

from typing import Any, Dict, Literal, Optional

import FishBroWFS_V2.core.oom_cost_model as oom_cost_model
from FishBroWFS_V2.core.schemas.oom_gate import OomGateDecision, OomGateInput

OomAction = Literal["PASS", "BLOCK", "AUTO_DOWNSAMPLE"]


def estimate_bytes(inp: OomGateInput) -> int:
    """
    Estimate memory usage in bytes.
    
    Formula (locked):
        estimated = bars * params * subsample * intents_per_bar * bytes_per_intent_est
    
    Args:
        inp: OomGateInput with bars, params, param_subsample_rate, etc.
        
    Returns:
        Estimated memory usage in bytes
    """
    estimated = (
        inp.bars
        * inp.params
        * inp.param_subsample_rate
        * inp.intents_per_bar
        * inp.bytes_per_intent_est
    )
    return int(estimated)


def decide_gate(inp: OomGateInput) -> OomGateDecision:
    """
    Decide OOM gate action: PASS, BLOCK, or AUTO_DOWNSAMPLE.
    
    Rules (locked):
    - PASS: estimated <= ram_budget * 0.6
    - BLOCK: estimated > ram_budget * 0.9
    - AUTO_DOWNSAMPLE: otherwise, recommended_rate = (ram_budget * 0.6) / (bars * params * intents_per_bar * bytes_per_intent_est)
    
    Args:
        inp: OomGateInput with configuration
        
    Returns:
        OomGateDecision with decision and recommendations
    """
    estimated = estimate_bytes(inp)
    ram_budget = inp.ram_budget_bytes
    
    # Thresholds (locked)
    pass_threshold = ram_budget * 0.6
    block_threshold = ram_budget * 0.9
    
    if estimated <= pass_threshold:
        return OomGateDecision(
            decision="PASS",
            estimated_bytes=estimated,
            ram_budget_bytes=ram_budget,
            recommended_subsample_rate=None,
            notes=f"Estimated {estimated:,} bytes <= {pass_threshold:,.0f} bytes (60% of budget)",
        )
    
    if estimated > block_threshold:
        return OomGateDecision(
            decision="BLOCK",
            estimated_bytes=estimated,
            ram_budget_bytes=ram_budget,
            recommended_subsample_rate=None,
            notes=f"Estimated {estimated:,} bytes > {block_threshold:,.0f} bytes (90% of budget) - BLOCKED",
        )
    
    # AUTO_DOWNSAMPLE: calculate recommended rate
    # recommended_rate = (ram_budget * 0.6) / (bars * params * intents_per_bar * bytes_per_intent_est)
    denominator = inp.bars * inp.params * inp.intents_per_bar * inp.bytes_per_intent_est
    if denominator > 0:
        recommended_rate = (ram_budget * 0.6) / denominator
        # Clamp to [0.0, 1.0]
        recommended_rate = max(0.0, min(1.0, recommended_rate))
    else:
        recommended_rate = 0.0
    
    return OomGateDecision(
        decision="AUTO_DOWNSAMPLE",
        estimated_bytes=estimated,
        ram_budget_bytes=ram_budget,
        recommended_subsample_rate=recommended_rate,
        notes=(
            f"Estimated {estimated:,} bytes between {pass_threshold:,.0f} and {block_threshold:,.0f} "
            f"- recommended subsample rate: {recommended_rate:.4f}"
        ),
    )


def _params_effective(params_total: int, rate: float) -> int:
    """Calculate effective params with floor rule (at least 1)."""
    return max(1, int(params_total * rate))


def _estimate_ops(cfg: dict, *, params_effective: int) -> int:
    """
    Safely estimate operations count.
    
    Priority:
    1. Use oom_cost_model.estimate_ops if available (most consistent)
    2. Fallback to deterministic formula
    
    Args:
        cfg: Configuration dictionary
        params_effective: Effective params count (already calculated)
        
    Returns:
        Estimated operations count
    """
    # If cost model has ops estimate, use it (most consistent)
    if hasattr(oom_cost_model, "estimate_ops"):
        return int(oom_cost_model.estimate_ops(cfg))
    if hasattr(oom_cost_model, "estimate_ops_est"):
        return int(oom_cost_model.estimate_ops_est(cfg))
    
    # Fallback: at least stable and monotonic
    bars = int(cfg.get("bars", 0))
    intents_per_bar = float(cfg.get("intents_per_bar", 2.0))
    return int(bars * params_effective * intents_per_bar)


def decide_oom_action(
    cfg: Dict[str, Any],
    *,
    mem_limit_mb: float,
    allow_auto_downsample: bool = True,
    auto_downsample_step: float = 0.5,
    auto_downsample_min: float = 0.02,
    work_factor: float = 2.0,
) -> Dict[str, Any]:
    """
    Backward-compatible OOM gate used by funnel_runner + contract tests.

    Returns a dict (schema-as-dict) consumed by pipeline and written to artifacts/README.
    This function MAY mutate cfg['param_subsample_rate'] when AUTO_DOWNSAMPLE.
    
    Uses estimate_memory_bytes() from oom_cost_model (tests monkeypatch this).
    Must use module import (oom_cost_model.estimate_memory_bytes) for monkeypatch to work.
    
    Args:
        cfg: Configuration dictionary with bars, params_total, param_subsample_rate, etc.
        mem_limit_mb: Memory limit in MB
        allow_auto_downsample: Whether to allow automatic downsample
        auto_downsample_step: Multiplier for each downsample step (default: 0.5)
        auto_downsample_min: Minimum subsample rate (default: 0.02)
        work_factor: Work factor for memory estimation (default: 2.0)
        
    Returns:
        Dictionary with action, reason, estimated_bytes, and metadata
    """
    bars = int(cfg.get("bars", 0))
    params_total = int(cfg.get("params_total", 0))
    original = float(cfg.get("param_subsample_rate", 1.0))
    mem_limit_bytes = int(mem_limit_mb * 1024.0 * 1024.0)

    def _result(action: str, reason: str, est_bytes: int, final_rate: float) -> Dict[str, Any]:
        """Helper to build consistent result dict."""
        params_eff = _params_effective(params_total, final_rate)
        ops_est = _estimate_ops(cfg, params_effective=params_eff)
        
        # Calculate time estimate from ops_est
        ops_per_sec_est = float(cfg.get("ops_per_sec_est", 2.0e7))
        time_est_s = float(ops_est) / ops_per_sec_est if ops_per_sec_est > 0 else 0.0
        
        estimates = {
            "mem_est_bytes": int(est_bytes),
            "mem_est_mb": float(est_bytes) / (1024.0 * 1024.0),
            "mem_limit_mb": float(mem_limit_mb),
            "mem_limit_bytes": int(mem_limit_bytes),
            "ops_est": int(ops_est),
            "time_est_s": float(time_est_s),
        }
        return {
            "action": action,
            "reason": reason,
            # âœ… tests/test_oom_gate.py needs this
            "estimated_bytes": int(est_bytes),
            "estimated_mb": float(est_bytes) / (1024.0 * 1024.0),
            # âœ… NEW: required by tests/test_oom_gate.py
            "mem_limit_mb": float(mem_limit_mb),
            "mem_limit_bytes": int(mem_limit_bytes),
            # Original subsample contract
            "original_subsample": float(original),
            "final_subsample": float(final_rate),
            # Funnel/README common fields (preserved)
            "params_total": int(params_total),
            "params_effective": int(params_eff),
            # âœ… funnel_runner/tests needs estimates.ops_est / estimates.mem_est_mb
            "estimates": estimates,
            # Other debug fields
            "allow_auto_downsample": bool(allow_auto_downsample),
            "auto_downsample_step": float(auto_downsample_step),
            "auto_downsample_min": float(auto_downsample_min),
            "work_factor": float(work_factor),
        }

    # invalid input â†’ BLOCKï¼ˆä½†ä»å›ž schemaï¼‰
    if bars <= 0 or params_total <= 0:
        return _result("BLOCK", "invalid_input", 0, original)

    # estimate at original
    est0 = int(oom_cost_model.estimate_memory_bytes(cfg, work_factor=work_factor))
    if est0 <= mem_limit_bytes:
        return _result("PASS", "pass_under_limit", est0, original)

    if not allow_auto_downsample:
        return _result("BLOCK", "block: over limit (auto-downsample disabled)", est0, original)

    # auto-downsample loop
    rate = original
    while rate > auto_downsample_min:
        rate = max(auto_downsample_min, rate * auto_downsample_step)
        cfg["param_subsample_rate"] = float(rate)  # NOTE: mutation expected by integration tests
        est = int(oom_cost_model.estimate_memory_bytes(cfg, work_factor=work_factor))
        if est <= mem_limit_bytes:
            return _result("AUTO_DOWNSAMPLE", "auto-downsample: over limit, reduced subsample", est, rate)

    # still over limit at min
    cfg["param_subsample_rate"] = float(auto_downsample_min)
    est_min = int(oom_cost_model.estimate_memory_bytes(cfg, work_factor=work_factor))
    return _result("BLOCK", "block: min_subsample still too large", est_min, auto_downsample_min)


================================================================================
FILE: src/FishBroWFS_V2/core/paths.py
================================================================================

"""Path management for artifact output.

Centralized contract for output directory structure.
"""

from __future__ import annotations

from pathlib import Path


def get_run_dir(outputs_root: Path, season: str, run_id: str) -> Path:
    """
    Get path for a specific run.
    
    Fixed path structure: outputs/seasons/{season}/runs/{run_id}/
    
    Args:
        outputs_root: Root outputs directory (e.g., Path("outputs"))
        season: Season identifier
        run_id: Run ID
        
    Returns:
        Path to run directory
    """
    return outputs_root / "seasons" / season / "runs" / run_id


def ensure_run_dir(outputs_root: Path, season: str, run_id: str) -> Path:
    """
    Ensure run directory exists and return its path.
    
    Args:
        outputs_root: Root outputs directory
        season: Season identifier
        run_id: Run ID
        
    Returns:
        Path to run directory (created if needed)
    """
    run_dir = get_run_dir(outputs_root, season, run_id)
    run_dir.mkdir(parents=True, exist_ok=True)
    return run_dir


================================================================================
FILE: src/FishBroWFS_V2/core/run_id.py
================================================================================

"""Run ID generation for audit trail.

Provides deterministic, sortable run IDs with timestamp and short token.
"""

from __future__ import annotations

import secrets
from datetime import datetime, timezone


def make_run_id(prefix: str | None = None) -> str:
    """
    Generate a sortable, readable run ID.
    
    Format: {prefix-}YYYYMMDDTHHMMSSZ-{token}
    - Timestamp ensures chronological ordering (UTC)
    - Short token (8 hex chars) provides uniqueness
    
    Args:
        prefix: Optional prefix string (e.g., "test", "prod")
        
    Returns:
        Run ID string, e.g., "20251218T135221Z-a1b2c3d4"
        or "test-20251218T135221Z-a1b2c3d4" if prefix provided
    """
    ts = datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")
    tok = secrets.token_hex(4)  # 8 hex chars
    
    if prefix:
        return f"{prefix}-{ts}-{tok}"
    else:
        return f"{ts}-{tok}"


================================================================================
FILE: src/FishBroWFS_V2/core/schemas/__init__.py
================================================================================

"""Schemas for core modules."""


================================================================================
FILE: src/FishBroWFS_V2/core/schemas/governance.py
================================================================================

"""Pydantic schema for governance.json validation.

Validates governance decisions with KEEP/DROP/FREEZE and evidence chain.
"""

from __future__ import annotations

from pydantic import BaseModel, ConfigDict, Field
from typing import Any, Dict, List, Optional, Literal


Decision = Literal["KEEP", "DROP", "FREEZE"]


class EvidenceLinkModel(BaseModel):
    """Evidence link model for governance."""
    source_path: str
    json_pointer: str
    note: str = ""


class GovernanceDecisionRow(BaseModel):
    """
    Governance decision row schema.
    
    Represents a single governance decision with rule_id and evidence chain.
    """
    strategy_id: str
    decision: Decision
    rule_id: str  # "R1"/"R2"/"R3"
    reason: str = ""
    run_id: str
    stage: str
    config_hash: Optional[str] = None
    
    evidence: List[EvidenceLinkModel] = Field(default_factory=list)
    metrics_snapshot: Dict[str, Any] = Field(default_factory=dict)
    
    # Additional fields from existing schema (for backward compatibility)
    candidate_id: Optional[str] = None
    reasons: Optional[List[str]] = None
    created_at: Optional[str] = None
    git_sha: Optional[str] = None
    
    model_config = ConfigDict(extra="allow")  # Allow extra fields for backward compatibility


class GovernanceReport(BaseModel):
    """
    Governance report schema.
    
    Validates governance.json structure with decision rows and metadata.
    Supports both items format and rows format.
    """
    config_hash: str  # Required top-level field for DIRTY check contract
    schema_version: Optional[str] = None
    run_id: str
    rows: List[GovernanceDecisionRow] = Field(default_factory=list)
    meta: Dict[str, Any] = Field(default_factory=dict)
    
    # Additional fields from existing schema (for backward compatibility)
    items: Optional[List[Dict[str, Any]]] = None
    metadata: Optional[Dict[str, Any]] = None
    
    model_config = ConfigDict(extra="allow")  # Allow extra fields for backward compatibility


================================================================================
FILE: src/FishBroWFS_V2/core/schemas/manifest.py
================================================================================

"""Pydantic schema for manifest.json validation.

Validates run manifest with stages and artifacts tracking.
"""

from __future__ import annotations

from pydantic import BaseModel, Field
from typing import Any, Dict, List, Optional


class ManifestStage(BaseModel):
    """Stage information in manifest."""
    name: str
    status: str  # e.g. "DONE"/"FAILED"/"ABORTED"
    started_at: Optional[str] = None
    finished_at: Optional[str] = None
    artifacts: Dict[str, str] = Field(default_factory=dict)  # filename -> relpath


class RunManifest(BaseModel):
    """
    Run manifest schema.
    
    Validates manifest.json structure with run metadata, config hash, and stages.
    """
    schema_version: Optional[str] = None  # For future versioning
    run_id: str
    season: str
    config_hash: str
    created_at: Optional[str] = None
    stages: List[ManifestStage] = Field(default_factory=list)
    meta: Dict[str, Any] = Field(default_factory=dict)
    
    # Additional fields from AuditSchema (for backward compatibility)
    git_sha: Optional[str] = None
    dirty_repo: Optional[bool] = None
    param_subsample_rate: Optional[float] = None
    dataset_id: Optional[str] = None
    bars: Optional[int] = None
    params_total: Optional[int] = None
    params_effective: Optional[int] = None
    artifact_version: Optional[str] = None


================================================================================
FILE: src/FishBroWFS_V2/core/schemas/oom_gate.py
================================================================================

"""Pydantic schemas for OOM gate input and output.

Locked schemas for PASS/BLOCK/AUTO_DOWNSAMPLE decisions.
"""

from __future__ import annotations

from pydantic import BaseModel, Field
from typing import Literal


class OomGateInput(BaseModel):
    """
    Input for OOM gate decision.
    
    All fields are required for memory estimation.
    """
    bars: int = Field(gt=0, description="Number of bars")
    params: int = Field(gt=0, description="Total number of parameters")
    param_subsample_rate: float = Field(gt=0.0, le=1.0, description="Subsample rate in [0.0, 1.0]")
    intents_per_bar: float = Field(default=2.0, ge=0.0, description="Estimated intents per bar")
    bytes_per_intent_est: int = Field(default=64, gt=0, description="Estimated bytes per intent")
    ram_budget_bytes: int = Field(default=6_000_000_000, gt=0, description="RAM budget in bytes (default: 6GB)")


class OomGateDecision(BaseModel):
    """
    OOM gate decision output.
    
    Contains decision (PASS/BLOCK/AUTO_DOWNSAMPLE) and recommendations.
    """
    decision: Literal["PASS", "BLOCK", "AUTO_DOWNSAMPLE"]
    estimated_bytes: int = Field(ge=0, description="Estimated memory usage in bytes")
    ram_budget_bytes: int = Field(gt=0, description="RAM budget in bytes")
    recommended_subsample_rate: float | None = Field(
        default=None,
        ge=0.0,
        le=1.0,
        description="Recommended subsample rate (only for AUTO_DOWNSAMPLE)"
    )
    notes: str = Field(default="", description="Human-readable notes about the decision")


================================================================================
FILE: src/FishBroWFS_V2/core/schemas/winners_v2.py
================================================================================

"""Pydantic schema for winners_v2.json validation.

Validates winners v2 structure with KPI metrics.
"""

from __future__ import annotations

from pydantic import BaseModel, ConfigDict, Field
from typing import Any, Dict, List, Optional


class WinnerRow(BaseModel):
    """
    Winner row schema.
    
    Represents a single winner with strategy info and KPI metrics.
    """
    strategy_id: str
    symbol: str
    timeframe: str
    params: Dict[str, Any] = Field(default_factory=dict)
    
    # Required KPI metrics
    net_profit: float
    max_drawdown: float
    trades: int
    
    # Optional metrics
    win_rate: Optional[float] = None
    sharpe: Optional[float] = None
    sqn: Optional[float] = None
    
    # Evidence links (if already present)
    evidence: Dict[str, str] = Field(default_factory=dict)  # pointers/paths if already present
    
    # Additional fields from v2 schema (for backward compatibility)
    candidate_id: Optional[str] = None
    score: Optional[float] = None
    metrics: Optional[Dict[str, Any]] = None
    source: Optional[Dict[str, Any]] = None


class WinnersV2(BaseModel):
    """
    Winners v2 schema.
    
    Validates winners_v2.json structure with rows and metadata.
    Supports both v2 format (with topk) and normalized format (with rows).
    """
    config_hash: str  # Required top-level field for DIRTY check contract
    schema_version: Optional[str] = None  # "v2" or "schema" field
    run_id: Optional[str] = None
    stage: Optional[str] = None  # stage_name
    rows: List[WinnerRow] = Field(default_factory=list)
    meta: Dict[str, Any] = Field(default_factory=dict)
    
    # Additional fields from v2 schema (for backward compatibility)
    schema_name: Optional[str] = Field(default=None, alias="schema")  # "v2" - renamed to avoid conflict
    stage_name: Optional[str] = None
    generated_at: Optional[str] = None
    topk: Optional[List[Dict[str, Any]]] = None
    notes: Optional[Dict[str, Any]] = None
    
    model_config = ConfigDict(extra="allow", populate_by_name=True)  # Allow extra fields and support alias


================================================================================
FILE: src/FishBroWFS_V2/core/winners_builder.py
================================================================================

"""Winners builder - converts legacy winners to v2 schema.

Builds v2 winners.json from legacy topk format with fallback strategies.
"""

from __future__ import annotations

from datetime import datetime, timezone
from typing import Any, Dict, List

from FishBroWFS_V2.core.winners_schema import WinnerItemV2, build_winners_v2_dict


def build_winners_v2(
    *,
    stage_name: str,
    run_id: str,
    manifest: Dict[str, Any],
    config_snapshot: Dict[str, Any],
    legacy_topk: List[Dict[str, Any]],
) -> Dict[str, Any]:
    """
    Build winners.json v2 from legacy topk format.
    
    Args:
        stage_name: Stage identifier
        run_id: Run ID
        manifest: Manifest dict (AuditSchema)
        config_snapshot: Config snapshot dict
        legacy_topk: Legacy topk list (old format items)
        
    Returns:
        Winners dict with v2 schema
    """
    # Extract strategy_id
    strategy_id = _extract_strategy_id(config_snapshot, manifest)
    
    # Extract symbol/timeframe
    symbol = _extract_symbol(config_snapshot)
    timeframe = _extract_timeframe(config_snapshot)
    
    # Build v2 items
    v2_items: List[WinnerItemV2] = []
    
    for legacy_item in legacy_topk:
        # Extract param_id (required for candidate_id generation)
        param_id = legacy_item.get("param_id")
        if param_id is None:
            # Skip items without param_id (should not happen, but be defensive)
            continue
        
        # Generate candidate_id (temporary: strategy_id:param_id)
        # Future: upgrade to strategy_id:params_hash[:12] when params are available
        candidate_id = f"{strategy_id}:{param_id}"
        
        # Extract params (fallback to empty dict)
        params = _extract_params(legacy_item, config_snapshot, param_id)
        
        # Extract score (priority: score/finalscore > net_profit > 0.0)
        score = _extract_score(legacy_item)
        
        # Build metrics (must include legacy fields for backward compatibility)
        metrics = {
            "net_profit": float(legacy_item.get("net_profit", 0.0)),
            "max_dd": float(legacy_item.get("max_dd", 0.0)),
            "trades": int(legacy_item.get("trades", 0)),
            "param_id": int(param_id),  # Keep for backward compatibility
        }
        
        # Add proxy_value if present (Stage0)
        if "proxy_value" in legacy_item:
            metrics["proxy_value"] = float(legacy_item["proxy_value"])
        
        # Build source metadata
        source = {
            "param_id": int(param_id),
            "run_id": run_id,
            "stage_name": stage_name,
        }
        
        # Create v2 item
        v2_item = WinnerItemV2(
            candidate_id=candidate_id,
            strategy_id=strategy_id,
            symbol=symbol,
            timeframe=timeframe,
            params=params,
            score=score,
            metrics=metrics,
            source=source,
        )
        
        v2_items.append(v2_item)
    
    # Build notes with candidate_id_mode info
    notes = {
        "candidate_id_mode": "strategy_id:param_id",  # Temporary mode
        "note": "candidate_id uses param_id temporarily; will upgrade to params_hash when params are available",
    }
    
    # Build v2 winners dict
    return build_winners_v2_dict(
        stage_name=stage_name,
        run_id=run_id,
        generated_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        topk=v2_items,
        notes=notes,
    )


def _extract_strategy_id(config_snapshot: Dict[str, Any], manifest: Dict[str, Any]) -> str:
    """
    Extract strategy_id from config_snapshot or manifest.
    
    Priority:
    1. config_snapshot.get("strategy_id")
    2. manifest.get("dataset_id") (fallback)
    3. "unknown" (final fallback)
    """
    if "strategy_id" in config_snapshot:
        return str(config_snapshot["strategy_id"])
    
    dataset_id = manifest.get("dataset_id")
    if dataset_id:
        return str(dataset_id)
    
    return "unknown"


def _extract_symbol(config_snapshot: Dict[str, Any]) -> str:
    """
    Extract symbol from config_snapshot.
    
    Returns "UNKNOWN" if not available.
    """
    return str(config_snapshot.get("symbol", "UNKNOWN"))


def _extract_timeframe(config_snapshot: Dict[str, Any]) -> str:
    """
    Extract timeframe from config_snapshot.
    
    Returns "UNKNOWN" if not available.
    """
    return str(config_snapshot.get("timeframe", "UNKNOWN"))


def _extract_params(
    legacy_item: Dict[str, Any],
    config_snapshot: Dict[str, Any],
    param_id: int,
) -> Dict[str, Any]:
    """
    Extract params from legacy_item or config_snapshot.
    
    Priority:
    1. legacy_item.get("params")
    2. config_snapshot.get("params_by_id", {}).get(param_id)
    3. config_snapshot.get("params_spec") (if available)
    4. {} (empty dict fallback)
    
    Returns empty dict {} if params are not available.
    """
    # Try legacy_item first
    if "params" in legacy_item:
        params = legacy_item["params"]
        if isinstance(params, dict):
            return params
    
    # Try config_snapshot params_by_id
    params_by_id = config_snapshot.get("params_by_id", {})
    if isinstance(params_by_id, dict) and param_id in params_by_id:
        params = params_by_id[param_id]
        if isinstance(params, dict):
            return params
    
    # Try config_snapshot params_spec (if available)
    params_spec = config_snapshot.get("params_spec")
    if isinstance(params_spec, dict):
        # Could extract from params_spec if it has param_id mapping
        # For now, return empty dict
        pass
    
    # Fallback: empty dict
    return {}


def _extract_score(legacy_item: Dict[str, Any]) -> float:
    """
    Extract score from legacy_item.
    
    Priority:
    1. legacy_item.get("score")
    2. legacy_item.get("finalscore")
    3. legacy_item.get("net_profit")
    4. legacy_item.get("proxy_value") (for Stage0)
    5. 0.0 (fallback)
    """
    if "score" in legacy_item:
        val = legacy_item["score"]
        if isinstance(val, (int, float)):
            return float(val)
    
    if "finalscore" in legacy_item:
        val = legacy_item["finalscore"]
        if isinstance(val, (int, float)):
            return float(val)
    
    if "net_profit" in legacy_item:
        val = legacy_item["net_profit"]
        if isinstance(val, (int, float)):
            return float(val)
    
    if "proxy_value" in legacy_item:
        val = legacy_item["proxy_value"]
        if isinstance(val, (int, float)):
            return float(val)
    
    return 0.0


================================================================================
FILE: src/FishBroWFS_V2/core/winners_schema.py
================================================================================

"""Winners schema v2 (SSOT).

Defines the v2 schema for winners.json with enhanced metadata.
"""

from __future__ import annotations

from dataclasses import dataclass, asdict
from datetime import datetime, timezone
from typing import Any, Dict, List


WINNERS_SCHEMA_VERSION = "v2"


@dataclass(frozen=True)
class WinnerItemV2:
    """
    Winner item in v2 schema.
    
    Each item represents a top-K candidate with complete metadata.
    """
    candidate_id: str  # Format: {strategy_id}:{param_id} (temporary) or {strategy_id}:{params_hash[:12]} (future)
    strategy_id: str  # Strategy identifier (e.g., "donchian_atr")
    symbol: str  # Symbol identifier (e.g., "CME.MNQ" or "UNKNOWN")
    timeframe: str  # Timeframe (e.g., "60m" or "UNKNOWN")
    params: Dict[str, Any]  # Parameters dict (may be empty {} if not available)
    score: float  # Ranking score (finalscore, net_profit, or proxy_value)
    metrics: Dict[str, Any]  # Performance metrics (must include legacy fields: net_profit, max_dd, trades, param_id)
    source: Dict[str, Any]  # Source metadata (param_id, run_id, stage_name)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)


def build_winners_v2_dict(
    *,
    stage_name: str,
    run_id: str,
    generated_at: str | None = None,
    topk: List[WinnerItemV2],
    notes: Dict[str, Any] | None = None,
) -> Dict[str, Any]:
    """
    Build winners.json v2 structure.
    
    Args:
        stage_name: Stage identifier
        run_id: Run ID
        generated_at: ISO8601 timestamp (defaults to now if None)
        topk: List of WinnerItemV2 items
        notes: Additional notes dict (will be merged with default notes)
        
    Returns:
        Winners dict with v2 schema
    """
    if generated_at is None:
        generated_at = datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")
    
    default_notes = {
        "schema": WINNERS_SCHEMA_VERSION,
    }
    
    if notes:
        default_notes.update(notes)
    
    return {
        "schema": WINNERS_SCHEMA_VERSION,
        "stage_name": stage_name,
        "generated_at": generated_at,
        "topk": [item.to_dict() for item in topk],
        "notes": default_notes,
    }


def is_winners_v2(winners: Dict[str, Any]) -> bool:
    """
    Check if winners dict is v2 schema.
    
    Args:
        winners: Winners dict
        
    Returns:
        True if v2 schema, False otherwise
    """
    # Check top-level schema field
    if winners.get("schema") == WINNERS_SCHEMA_VERSION:
        return True
    
    # Check notes.schema field (legacy check)
    notes = winners.get("notes", {})
    if isinstance(notes, dict) and notes.get("schema") == WINNERS_SCHEMA_VERSION:
        return True
    
    return False


def is_winners_legacy(winners: Dict[str, Any]) -> bool:
    """
    Check if winners dict is legacy (v1) schema.
    
    Args:
        winners: Winners dict
        
    Returns:
        True if legacy schema, False otherwise
    """
    # If it's v2, it's not legacy
    if is_winners_v2(winners):
        return False
    
    # Legacy format: {"topk": [...], "notes": {"schema": "v1"}} or just {"topk": [...]}
    if "topk" in winners:
        # Check if items have v2 structure (candidate_id, strategy_id, etc.)
        topk = winners.get("topk", [])
        if topk and isinstance(topk[0], dict):
            # If first item has candidate_id, it's v2
            if "candidate_id" in topk[0]:
                return False
        return True
    
    return False


================================================================================
FILE: src/FishBroWFS_V2/data/__init__.py
================================================================================




================================================================================
FILE: src/FishBroWFS_V2/data/layout.py
================================================================================

import numpy as np
from FishBroWFS_V2.engine.types import BarArrays


def ensure_float64_contiguous(x: np.ndarray) -> np.ndarray:
    arr = np.asarray(x, dtype=np.float64)
    if not arr.flags["C_CONTIGUOUS"]:
        arr = np.ascontiguousarray(arr)
    return arr


def normalize_bars(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
) -> BarArrays:
    arrays = [open_, high, low, close]
    for a in arrays:
        if np.isnan(a).any():
            raise ValueError("NaN detected in input data")

    o = ensure_float64_contiguous(open_)
    h = ensure_float64_contiguous(high)
    l = ensure_float64_contiguous(low)
    c = ensure_float64_contiguous(close)

    return BarArrays(open=o, high=h, low=l, close=c)



================================================================================
FILE: src/FishBroWFS_V2/engine/__init__.py
================================================================================

"""Engine module - unified simulate entry point."""

from FishBroWFS_V2.engine.simulate import simulate_run

__all__ = ["simulate_run"]


================================================================================
FILE: src/FishBroWFS_V2/engine/constants.py
================================================================================

"""
Engine integer constants (hot-path friendly).

These constants are used in array/SoA pathways to avoid Enum.value lookups in tight loops.
"""

ROLE_EXIT = 0
ROLE_ENTRY = 1

KIND_STOP = 0
KIND_LIMIT = 1

SIDE_SELL = -1
SIDE_BUY = 1




================================================================================
FILE: src/FishBroWFS_V2/engine/constitution.py
================================================================================

"""
Engine Constitution v1.1 (FROZEN)

Activation:
- Orders are created at Bar[T] close and become active at Bar[T+1].

STOP fills (Open==price is treated as GAP branch):
Buy Stop @ S:
- if Open >= S: fill = Open
- elif High >= S: fill = S
Sell Stop @ S:
- if Open <= S: fill = Open
- elif Low <= S: fill = S

LIMIT fills (Open==price is treated as GAP branch):
Buy Limit @ L:
- if Open <= L: fill = Open
- elif Low <= L: fill = L
Sell Limit @ L:
- if Open >= L: fill = Open
- elif High >= L: fill = L

Priority:
- STOP wins over LIMIT (risk-first pessimism).

Same-bar In/Out:
- If entry and exit are both triggerable in the same bar, execute Entry then Exit.

Same-kind tie rule:
- If multiple orders of the same role are triggerable in the same bar, execute EXIT-first.
- Within the same role+kind, use deterministic order: smaller order_id first.
"""

NEXT_BAR_ACTIVE = True
PRIORITY_STOP_OVER_LIMIT = True
SAME_BAR_ENTRY_THEN_EXIT = True
SAME_KIND_TIE_EXIT_FIRST = True



================================================================================
FILE: src/FishBroWFS_V2/engine/engine_jit.py
================================================================================

from __future__ import annotations

from dataclasses import asdict
from typing import Iterable, List, Tuple

import numpy as np

# Engine JIT matcher kernel contract:
# - Complexity target: O(B + I + A), where:
#     B = bars, I = intents, A = per-bar active-book scan.
# - Forbidden: scanning all intents per bar (O(B*I)).
# - Extension point: ttl_bars (0=GTC, 1=one-shot next-bar-only, future: >1).

try:
    import numba as nb
except Exception:  # pragma: no cover
    nb = None  # type: ignore

from FishBroWFS_V2.engine.types import (
    BarArrays,
    Fill,
    OrderIntent,
    OrderKind,
    OrderRole,
    Side,
)
from FishBroWFS_V2.engine.matcher_core import simulate as simulate_py
from FishBroWFS_V2.engine.constants import (
    KIND_LIMIT,
    KIND_STOP,
    ROLE_ENTRY,
    ROLE_EXIT,
    SIDE_BUY,
    SIDE_SELL,
)

# Side enum codes for uint8 encoding (avoid -1 cast deprecation)
SIDE_BUY_CODE = 1
SIDE_SELL_CODE = 255  # SIDE_SELL (-1) encoded as uint8

STATUS_OK = 0
STATUS_ERROR_UNSORTED = 1

# JIT truth (debug/perf observability)
JIT_PATH_USED_LAST = False
JIT_KERNEL_SIGNATURES_LAST = None  # type: ignore


def get_jit_truth() -> dict:
    """
    Debug helper: returns whether the last simulate() call used the JIT kernel,
    and (if available) the kernel signatures snapshot.
    """
    return {
        "jit_path_used": bool(JIT_PATH_USED_LAST),
        "kernel_signatures": JIT_KERNEL_SIGNATURES_LAST,
    }


def _to_int(x) -> int:
    # Enum values are int/str; we convert deterministically.
    if isinstance(x, Side):
        return int(x.value)
    if isinstance(x, OrderRole):
        # EXIT first tie-break relies on role; map explicitly.
        return 0 if x == OrderRole.EXIT else 1
    if isinstance(x, OrderKind):
        return 0 if x == OrderKind.STOP else 1
    return int(x)


def _to_kind_int(k: OrderKind) -> int:
    return 0 if k == OrderKind.STOP else 1


def _to_role_int(r: OrderRole) -> int:
    return 0 if r == OrderRole.EXIT else 1


def _to_side_int(s: Side) -> int:
    """
    Convert Side enum to integer code for uint8 encoding.
    
    Returns:
        SIDE_BUY_CODE (1) for Side.BUY
        SIDE_SELL_CODE (255) for Side.SELL (avoid -1 cast deprecation)
    """
    if s == Side.BUY:
        return SIDE_BUY_CODE
    elif s == Side.SELL:
        return SIDE_SELL_CODE
    else:
        raise ValueError(f"Unknown Side enum: {s}")


def _kind_from_int(v: int) -> OrderKind:
    """
    Decode kind enum from integer value (strict mode).
    
    Allowed values:
    - 0 (KIND_STOP) -> OrderKind.STOP
    - 1 (KIND_LIMIT) -> OrderKind.LIMIT
    
    Raises ValueError for any other value to catch silent corruption.
    """
    if v == KIND_STOP:  # 0
        return OrderKind.STOP
    elif v == KIND_LIMIT:  # 1
        return OrderKind.LIMIT
    else:
        raise ValueError(
            f"Invalid kind enum value: {v}. Allowed values are {KIND_STOP} (STOP) or {KIND_LIMIT} (LIMIT)"
        )


def _role_from_int(v: int) -> OrderRole:
    """
    Decode role enum from integer value (strict mode).
    
    Allowed values:
    - 0 (ROLE_EXIT) -> OrderRole.EXIT
    - 1 (ROLE_ENTRY) -> OrderRole.ENTRY
    
    Raises ValueError for any other value to catch silent corruption.
    """
    if v == ROLE_EXIT:  # 0
        return OrderRole.EXIT
    elif v == ROLE_ENTRY:  # 1
        return OrderRole.ENTRY
    else:
        raise ValueError(
            f"Invalid role enum value: {v}. Allowed values are {ROLE_EXIT} (EXIT) or {ROLE_ENTRY} (ENTRY)"
        )


def _side_from_int(v: int) -> Side:
    """
    Decode side enum from integer value (strict mode).
    
    Allowed values:
    - SIDE_BUY_CODE (1) -> Side.BUY
    - SIDE_SELL_CODE (255) -> Side.SELL
    
    Raises ValueError for any other value to catch silent corruption.
    """
    if v == SIDE_BUY_CODE:  # 1
        return Side.BUY
    elif v == SIDE_SELL_CODE:  # 255
        return Side.SELL
    else:
        raise ValueError(
            f"Invalid side enum value: {v}. Allowed values are {SIDE_BUY_CODE} (BUY) or {SIDE_SELL_CODE} (SELL)"
        )


def _pack_intents(intents: Iterable[OrderIntent]):
    """
    Pack intents into plain arrays for numba.

    Fields (optimized dtypes):
      order_id: int32 (INDEX_DTYPE)
      created_bar: int32 (INDEX_DTYPE)
      role: uint8 (INTENT_ENUM_DTYPE, 0=EXIT,1=ENTRY)
      kind: uint8 (INTENT_ENUM_DTYPE, 0=STOP,1=LIMIT)
      side: uint8 (INTENT_ENUM_DTYPE, SIDE_BUY_CODE=BUY, SIDE_SELL_CODE=SELL)
      price: float64 (INTENT_PRICE_DTYPE)
      qty: int32 (INDEX_DTYPE)
    """
    from FishBroWFS_V2.config.dtypes import (
        INDEX_DTYPE,
        INTENT_ENUM_DTYPE,
        INTENT_PRICE_DTYPE,
    )
    
    it = list(intents)
    n = len(it)
    order_id = np.empty(n, dtype=INDEX_DTYPE)
    created_bar = np.empty(n, dtype=INDEX_DTYPE)
    role = np.empty(n, dtype=INTENT_ENUM_DTYPE)
    kind = np.empty(n, dtype=INTENT_ENUM_DTYPE)
    side = np.empty(n, dtype=INTENT_ENUM_DTYPE)
    price = np.empty(n, dtype=INTENT_PRICE_DTYPE)
    qty = np.empty(n, dtype=INDEX_DTYPE)

    for i, x in enumerate(it):
        order_id[i] = int(x.order_id)
        created_bar[i] = int(x.created_bar)
        role[i] = INTENT_ENUM_DTYPE(_to_role_int(x.role))
        kind[i] = INTENT_ENUM_DTYPE(_to_kind_int(x.kind))
        side[i] = INTENT_ENUM_DTYPE(_to_side_int(x.side))
        price[i] = INTENT_PRICE_DTYPE(x.price)
        qty[i] = int(x.qty)

    return order_id, created_bar, role, kind, side, price, qty


def _sort_packed_by_created_bar(
    packed: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Sort packed intent arrays by (created_bar, order_id).

    Why:
      - Cursor + active-book kernel requires activate_bar=(created_bar+1) and order_id to be non-decreasing.
      - Determinism is preserved because selection is still based on (kind priority, order_id).
    """
    order_id, created_bar, role, kind, side, price, qty = packed
    # lexsort uses last key as primary -> (created_bar primary, order_id secondary)
    idx = np.lexsort((order_id, created_bar))
    return (
        order_id[idx],
        created_bar[idx],
        role[idx],
        kind[idx],
        side[idx],
        price[idx],
        qty[idx],
    )


def simulate(
    bars: BarArrays,
    intents: Iterable[OrderIntent],
) -> List[Fill]:
    """
    Phase 2A: JIT accelerated matcher.

    Kill switch:
      - If numba is unavailable OR NUMBA_DISABLE_JIT=1, fall back to Python reference.
    """
    global JIT_PATH_USED_LAST, JIT_KERNEL_SIGNATURES_LAST

    if nb is None:
        JIT_PATH_USED_LAST = False
        JIT_KERNEL_SIGNATURES_LAST = None
        return simulate_py(bars, intents)

    # If numba is disabled, keep behavior stable.
    # Numba respects NUMBA_DISABLE_JIT; but we short-circuit to be safe.
    import os

    if os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        JIT_PATH_USED_LAST = False
        JIT_KERNEL_SIGNATURES_LAST = None
        return simulate_py(bars, intents)

    packed = _sort_packed_by_created_bar(_pack_intents(intents))
    status, fills_arr = _simulate_kernel(
        bars.open,
        bars.high,
        bars.low,
        packed[0],
        packed[1],
        packed[2],
        packed[3],
        packed[4],
        packed[5],
        packed[6],
        np.int64(1),  # ttl_bars=1 keeps Phase-2 semantics (next-bar only)
    )
    if int(status) != STATUS_OK:
        JIT_PATH_USED_LAST = True
        raise RuntimeError(f"engine_jit kernel error: status={int(status)}")

    # record JIT truth (best-effort)
    JIT_PATH_USED_LAST = True
    try:
        sigs = getattr(_simulate_kernel, "signatures", None)
        if sigs is not None:
            JIT_KERNEL_SIGNATURES_LAST = list(sigs)
        else:
            JIT_KERNEL_SIGNATURES_LAST = None
    except Exception:
        JIT_KERNEL_SIGNATURES_LAST = None

    # Convert to Fill objects (drop unused capacity)
    out: List[Fill] = []
    m = fills_arr.shape[0]
    for i in range(m):
        row = fills_arr[i]
        out.append(
            Fill(
                bar_index=int(row[0]),
                role=_role_from_int(int(row[1])),
                kind=_kind_from_int(int(row[2])),
                side=_side_from_int(int(row[3])),
                price=float(row[4]),
                qty=int(row[5]),
                order_id=int(row[6]),
            )
        )
    return out


def simulate_arrays(
    bars: BarArrays,
    *,
    order_id: np.ndarray,
    created_bar: np.ndarray,
    role: np.ndarray,
    kind: np.ndarray,
    side: np.ndarray,
    price: np.ndarray,
    qty: np.ndarray,
    ttl_bars: int = 1,
) -> List[Fill]:
    """
    Array/SoA entry point: bypass OrderIntent objects and _pack_intents hot-path.

    Arrays must be 1D and same length. Dtypes are expected (optimized):
      order_id: int32 (INDEX_DTYPE)
      created_bar: int32 (INDEX_DTYPE)
      role: uint8 (INTENT_ENUM_DTYPE)
      kind: uint8 (INTENT_ENUM_DTYPE)
      side: uint8 (INTENT_ENUM_DTYPE)
      price: float64 (INTENT_PRICE_DTYPE)
      qty: int32 (INDEX_DTYPE)

    ttl_bars:
      1 => one-shot next-bar-only (Phase 2 semantics)
      0 => GTC extension point (debug/tests)
    """
    from FishBroWFS_V2.config.dtypes import (
        INDEX_DTYPE,
        INTENT_ENUM_DTYPE,
        INTENT_PRICE_DTYPE,
    )
    
    global JIT_PATH_USED_LAST, JIT_KERNEL_SIGNATURES_LAST

    # Normalize/ensure arrays are numpy with the expected dtypes (cold path).
    oid = np.asarray(order_id, dtype=INDEX_DTYPE)
    cb = np.asarray(created_bar, dtype=INDEX_DTYPE)
    rl = np.asarray(role, dtype=INTENT_ENUM_DTYPE)
    kd = np.asarray(kind, dtype=INTENT_ENUM_DTYPE)
    sd = np.asarray(side, dtype=INTENT_ENUM_DTYPE)
    px = np.asarray(price, dtype=INTENT_PRICE_DTYPE)
    qy = np.asarray(qty, dtype=INDEX_DTYPE)

    if nb is None:
        JIT_PATH_USED_LAST = False
        JIT_KERNEL_SIGNATURES_LAST = None
        intents: List[OrderIntent] = []
        n = int(oid.shape[0])
        for i in range(n):
            # Strict decoding: fail fast on invalid enum values
            rl_val = int(rl[i])
            if rl_val == ROLE_EXIT:
                r = OrderRole.EXIT
            elif rl_val == ROLE_ENTRY:
                r = OrderRole.ENTRY
            else:
                raise ValueError(f"Invalid role enum value: {rl_val}. Allowed: {ROLE_EXIT} (EXIT) or {ROLE_ENTRY} (ENTRY)")
            
            kd_val = int(kd[i])
            if kd_val == KIND_STOP:
                k = OrderKind.STOP
            elif kd_val == KIND_LIMIT:
                k = OrderKind.LIMIT
            else:
                raise ValueError(f"Invalid kind enum value: {kd_val}. Allowed: {KIND_STOP} (STOP) or {KIND_LIMIT} (LIMIT)")
            
            sd_val = int(sd[i])
            if sd_val == SIDE_BUY_CODE:  # 1
                s = Side.BUY
            elif sd_val == SIDE_SELL_CODE:  # 255
                s = Side.SELL
            else:
                raise ValueError(f"Invalid side enum value: {sd_val}. Allowed: {SIDE_BUY_CODE} (BUY) or {SIDE_SELL_CODE} (SELL)")
            intents.append(
                OrderIntent(
                    order_id=int(oid[i]),
                    created_bar=int(cb[i]),
                    role=r,
                    kind=k,
                    side=s,
                    price=float(px[i]),
                    qty=int(qy[i]),
                )
            )
        return simulate_py(bars, intents)

    import os

    if os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        JIT_PATH_USED_LAST = False
        JIT_KERNEL_SIGNATURES_LAST = None
        intents: List[OrderIntent] = []
        n = int(oid.shape[0])
        for i in range(n):
            # Strict decoding: fail fast on invalid enum values
            rl_val = int(rl[i])
            if rl_val == ROLE_EXIT:
                r = OrderRole.EXIT
            elif rl_val == ROLE_ENTRY:
                r = OrderRole.ENTRY
            else:
                raise ValueError(f"Invalid role enum value: {rl_val}. Allowed: {ROLE_EXIT} (EXIT) or {ROLE_ENTRY} (ENTRY)")
            
            kd_val = int(kd[i])
            if kd_val == KIND_STOP:
                k = OrderKind.STOP
            elif kd_val == KIND_LIMIT:
                k = OrderKind.LIMIT
            else:
                raise ValueError(f"Invalid kind enum value: {kd_val}. Allowed: {KIND_STOP} (STOP) or {KIND_LIMIT} (LIMIT)")
            
            sd_val = int(sd[i])
            if sd_val == SIDE_BUY_CODE:  # 1
                s = Side.BUY
            elif sd_val == SIDE_SELL_CODE:  # 255
                s = Side.SELL
            else:
                raise ValueError(f"Invalid side enum value: {sd_val}. Allowed: {SIDE_BUY_CODE} (BUY) or {SIDE_SELL_CODE} (SELL)")
            intents.append(
                OrderIntent(
                    order_id=int(oid[i]),
                    created_bar=int(cb[i]),
                    role=r,
                    kind=k,
                    side=s,
                    price=float(px[i]),
                    qty=int(qy[i]),
                )
            )
        return simulate_py(bars, intents)

    packed = _sort_packed_by_created_bar((oid, cb, rl, kd, sd, px, qy))
    status, fills_arr = _simulate_kernel(
        bars.open,
        bars.high,
        bars.low,
        packed[0],
        packed[1],
        packed[2],
        packed[3],
        packed[4],
        packed[5],
        packed[6],
        np.int64(ttl_bars),
    )
    if int(status) != STATUS_OK:
        JIT_PATH_USED_LAST = True
        raise RuntimeError(f"engine_jit kernel error: status={int(status)}")

    JIT_PATH_USED_LAST = True
    try:
        sigs = getattr(_simulate_kernel, "signatures", None)
        if sigs is not None:
            JIT_KERNEL_SIGNATURES_LAST = list(sigs)
        else:
            JIT_KERNEL_SIGNATURES_LAST = None
    except Exception:
        JIT_KERNEL_SIGNATURES_LAST = None

    out: List[Fill] = []
    m = fills_arr.shape[0]
    for i in range(m):
        row = fills_arr[i]
        out.append(
            Fill(
                bar_index=int(row[0]),
                role=_role_from_int(int(row[1])),
                kind=_kind_from_int(int(row[2])),
                side=_side_from_int(int(row[3])),
                price=float(row[4]),
                qty=int(row[5]),
                order_id=int(row[6]),
            )
        )
    return out


def _simulate_with_ttl(bars: BarArrays, intents: Iterable[OrderIntent], ttl_bars: int) -> List[Fill]:
    """
    Internal helper (tests/dev): run JIT matcher with a custom ttl_bars.
    ttl_bars=0 => GTC, ttl_bars=1 => one-shot next-bar-only (default).
    """
    if nb is None:
        return simulate_py(bars, intents)

    import os

    if os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        return simulate_py(bars, intents)

    packed = _sort_packed_by_created_bar(_pack_intents(intents))
    status, fills_arr = _simulate_kernel(
        bars.open,
        bars.high,
        bars.low,
        packed[0],
        packed[1],
        packed[2],
        packed[3],
        packed[4],
        packed[5],
        packed[6],
        np.int64(ttl_bars),
    )
    if int(status) != STATUS_OK:
        raise RuntimeError(f"engine_jit kernel error: status={int(status)}")

    out: List[Fill] = []
    m = fills_arr.shape[0]
    for i in range(m):
        row = fills_arr[i]
        out.append(
            Fill(
                bar_index=int(row[0]),
                role=_role_from_int(int(row[1])),
                kind=_kind_from_int(int(row[2])),
                side=_side_from_int(int(row[3])),
                price=float(row[4]),
                qty=int(row[5]),
                order_id=int(row[6]),
            )
        )
    return out


# ----------------------------
# Numba Kernel
# ----------------------------

if nb is not None:

    @nb.njit(cache=False)
    def _stop_fill(side: int, stop_price: float, o: float, h: float, l: float) -> float:
        # returns nan if no fill
        if side == 1:  # BUY
            if o >= stop_price:
                return o
            if h >= stop_price:
                return stop_price
            return np.nan
        else:  # SELL
            if o <= stop_price:
                return o
            if l <= stop_price:
                return stop_price
            return np.nan

    @nb.njit(cache=False)
    def _limit_fill(side: int, limit_price: float, o: float, h: float, l: float) -> float:
        # returns nan if no fill
        if side == 1:  # BUY
            if o <= limit_price:
                return o
            if l <= limit_price:
                return limit_price
            return np.nan
        else:  # SELL
            if o >= limit_price:
                return o
            if h >= limit_price:
                return limit_price
            return np.nan

    @nb.njit(cache=False)
    def _fill_price(kind: int, side: int, px: float, o: float, h: float, l: float) -> float:
        # kind: 0=STOP, 1=LIMIT
        if kind == 0:
            return _stop_fill(side, px, o, h, l)
        return _limit_fill(side, px, o, h, l)

    @nb.njit(cache=False)
    def _simulate_kernel(
        open_: np.ndarray,
        high: np.ndarray,
        low: np.ndarray,
        order_id: np.ndarray,
        created_bar: np.ndarray,
        role: np.ndarray,
        kind: np.ndarray,
        side: np.ndarray,
        price: np.ndarray,
        qty: np.ndarray,
        ttl_bars: np.int64,
    ):
        """
        Cursor + Active Book kernel (O(B + I + A)).

        Output columns (float64):
          0 bar_index
          1 role_int (0=EXIT,1=ENTRY)
          2 kind_int (0=STOP,1=LIMIT)
          3 side_int (1=BUY,-1=SELL)
          4 fill_price
          5 qty
          6 order_id

        Assumption:
          - intents are sorted by (created_bar, order_id) before calling this kernel.
        """
        n_bars = open_.shape[0]
        n_intents = order_id.shape[0]

        max_fills = n_bars * 2
        out = np.empty((max_fills, 7), dtype=np.float64)
        out_n = 0

        # -------------------------
        # Fail-fast monotonicity check (activate_bar, order_id)
        # -------------------------
        prev_activate = np.int64(-1)
        prev_order = np.int64(-1)
        for i in range(n_intents):
            a = np.int64(created_bar[i]) + np.int64(1)
            o = np.int64(order_id[i])
            if a < prev_activate or (a == prev_activate and o < prev_order):
                return np.int64(STATUS_ERROR_UNSORTED), out[:0]
            prev_activate = a
            prev_order = o

        # Active Book (indices into intent arrays)
        active_indices = np.empty(n_intents, dtype=np.int64)
        active_count = np.int64(0)
        global_cursor = np.int64(0)

        pos = np.int64(0)  # 0 flat, 1 long, -1 short

        for t in range(n_bars):
            o = float(open_[t])
            h = float(high[t])
            l = float(low[t])

            # Step A â€” Injection (cursor inject intents activating at this bar)
            while global_cursor < n_intents:
                a = np.int64(created_bar[global_cursor]) + np.int64(1)
                if a == np.int64(t):
                    active_indices[active_count] = global_cursor
                    active_count += np.int64(1)
                    global_cursor += np.int64(1)
                    continue
                if a > np.int64(t):
                    break
                # a < t should not happen if monotonicity check passed
                return np.int64(STATUS_ERROR_UNSORTED), out[:0]

            # Step B â€” Pass 1 (ENTRY scan, best-pick, swap-remove)
            # Deterministic selection: STOP(0) before LIMIT(1), then order_id asc.
            if pos == 0 and active_count > 0:
                best_k = np.int64(-1)
                best_kind = np.int64(99)
                best_oid = np.int64(2**62)
                best_fp = np.nan

                k = np.int64(0)
                while k < active_count:
                    idx = active_indices[k]
                    if np.int64(role[idx]) != np.int64(1):  # ENTRY
                        k += np.int64(1)
                        continue

                    kk = np.int64(kind[idx])
                    oo = np.int64(order_id[idx])
                    if kk < best_kind or (kk == best_kind and oo < best_oid):
                        fp = _fill_price(int(kk), int(side[idx]), float(price[idx]), o, h, l)
                        if not np.isnan(fp):
                            best_k = k
                            best_kind = kk
                            best_oid = oo
                            best_fp = fp
                    k += np.int64(1)

                if best_k != np.int64(-1):
                    idx = active_indices[best_k]
                    out[out_n, 0] = float(t)
                    out[out_n, 1] = float(role[idx])
                    out[out_n, 2] = float(kind[idx])
                    out[out_n, 3] = float(side[idx])
                    out[out_n, 4] = float(best_fp)
                    out[out_n, 5] = float(qty[idx])
                    out[out_n, 6] = float(order_id[idx])
                    out_n += 1

                    pos = np.int64(1) if np.int64(side[idx]) == np.int64(1) else np.int64(-1)

                    # swap-remove filled intent
                    active_indices[best_k] = active_indices[active_count - 1]
                    active_count -= np.int64(1)

            # Step C â€” Pass 2 (EXIT scan, best-pick, swap-remove)
            # Deterministic selection: STOP(0) before LIMIT(1), then order_id asc.
            if pos != 0 and active_count > 0:
                best_k = np.int64(-1)
                best_kind = np.int64(99)
                best_oid = np.int64(2**62)
                best_fp = np.nan

                k = np.int64(0)
                while k < active_count:
                    idx = active_indices[k]
                    if np.int64(role[idx]) != np.int64(0):  # EXIT
                        k += np.int64(1)
                        continue

                    s = np.int64(side[idx])
                    # long exits are SELL(-1), short exits are BUY(1)
                    if pos == np.int64(1) and s != np.int64(-1):
                        k += np.int64(1)
                        continue
                    if pos == np.int64(-1) and s != np.int64(1):
                        k += np.int64(1)
                        continue

                    kk = np.int64(kind[idx])
                    oo = np.int64(order_id[idx])
                    if kk < best_kind or (kk == best_kind and oo < best_oid):
                        fp = _fill_price(int(kk), int(s), float(price[idx]), o, h, l)
                        if not np.isnan(fp):
                            best_k = k
                            best_kind = kk
                            best_oid = oo
                            best_fp = fp
                    k += np.int64(1)

                if best_k != np.int64(-1):
                    idx = active_indices[best_k]
                    out[out_n, 0] = float(t)
                    out[out_n, 1] = float(role[idx])
                    out[out_n, 2] = float(kind[idx])
                    out[out_n, 3] = float(side[idx])
                    out[out_n, 4] = float(best_fp)
                    out[out_n, 5] = float(qty[idx])
                    out[out_n, 6] = float(order_id[idx])
                    out_n += 1

                    pos = np.int64(0)

                    # swap-remove filled intent
                    active_indices[best_k] = active_indices[active_count - 1]
                    active_count -= np.int64(1)

            # Step D â€” Housekeeping (TTL/GTC extension point)
            if ttl_bars > np.int64(0) and active_count > 0:
                k = np.int64(0)
                while k < active_count:
                    idx = active_indices[k]
                    activate_bar = np.int64(created_bar[idx]) + np.int64(1)
                    expire_bar = activate_bar + (ttl_bars - np.int64(1))
                    if np.int64(t) > expire_bar:
                        active_indices[k] = active_indices[active_count - 1]
                        active_count -= np.int64(1)
                        continue
                    k += np.int64(1)

        return np.int64(STATUS_OK), out[:out_n]



================================================================================
FILE: src/FishBroWFS_V2/engine/kernels/__init__.py
================================================================================

"""Kernel implementations for simulation."""


================================================================================
FILE: src/FishBroWFS_V2/engine/kernels/cursor_kernel.py
================================================================================

"""Cursor kernel - main simulation path for Phase 4.

This is the primary kernel implementation, optimized for performance.
It uses array/struct inputs and deterministic cursor-based matching.
"""

from __future__ import annotations

from typing import Iterable, List

from FishBroWFS_V2.engine.types import BarArrays, Fill, OrderIntent, SimResult
from FishBroWFS_V2.engine.engine_jit import simulate as simulate_jit


def simulate_cursor_kernel(
    bars: BarArrays,
    intents: Iterable[OrderIntent],
) -> SimResult:
    """
    Cursor kernel - main simulation path.
    
    This is the primary kernel for Phase 4. It uses the optimized JIT implementation
    from engine_jit, which provides O(B + I + A) complexity.
    
    Args:
        bars: OHLC bar arrays
        intents: Iterable of order intents
        
    Returns:
        SimResult containing the fills from simulation
        
    Note:
        - Uses arrays/structs internally, no class callbacks
        - Naming and fields are stable for pipeline usage
        - Deterministic behavior guaranteed
    """
    fills: List[Fill] = simulate_jit(bars, intents)
    return SimResult(fills=fills)


================================================================================
FILE: src/FishBroWFS_V2/engine/kernels/reference_kernel.py
================================================================================

"""Reference kernel - adapter for matcher_core (testing/debugging only).

This kernel wraps matcher_core.simulate() and should only be used for:
- Testing alignment between kernels
- Debugging semantic correctness
- Reference implementation verification

It is NOT the main path for production simulation.
"""

from __future__ import annotations

from typing import Iterable, List

from FishBroWFS_V2.engine.types import BarArrays, Fill, OrderIntent, SimResult
from FishBroWFS_V2.engine.matcher_core import simulate as simulate_reference


def simulate_reference_matcher(
    bars: BarArrays,
    intents: Iterable[OrderIntent],
) -> SimResult:
    """
    Reference matcher adapter - wraps matcher_core.simulate().
    
    This is an adapter that wraps the reference implementation in matcher_core.
    It should only be used for testing/debugging, not as the main simulation path.
    
    Args:
        bars: OHLC bar arrays
        intents: Iterable of order intents
        
    Returns:
        SimResult containing the fills from simulation
        
    Note:
        - This wraps matcher_core.simulate() which is the semantic truth source
        - Use only for tests/debug, not for production
    """
    fills: List[Fill] = simulate_reference(bars, intents)
    return SimResult(fills=fills)


================================================================================
FILE: src/FishBroWFS_V2/engine/matcher_core.py
================================================================================

from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable, List, Optional, Tuple

import numpy as np

from FishBroWFS_V2.engine.types import (
    BarArrays,
    Fill,
    OrderIntent,
    OrderKind,
    OrderRole,
    Side,
)


@dataclass
class PositionState:
    """
    Minimal single-position state for Phase 1 tests.
    pos: 0 = flat, 1 = long, -1 = short
    """
    pos: int = 0


def _is_active(intent: OrderIntent, bar_index: int) -> bool:
    return bar_index == intent.created_bar + 1


def _stop_fill_price(side: Side, stop_price: float, o: float, h: float, l: float) -> Optional[float]:
    # Open==price goes to GAP branch by definition.
    if side == Side.BUY:
        if o >= stop_price:
            return o
        if h >= stop_price:
            return stop_price
        return None
    else:
        if o <= stop_price:
            return o
        if l <= stop_price:
            return stop_price
        return None


def _limit_fill_price(side: Side, limit_price: float, o: float, h: float, l: float) -> Optional[float]:
    # Open==price goes to GAP branch by definition.
    if side == Side.BUY:
        if o <= limit_price:
            return o
        if l <= limit_price:
            return limit_price
        return None
    else:
        if o >= limit_price:
            return o
        if h >= limit_price:
            return limit_price
        return None


def _intent_fill_price(intent: OrderIntent, o: float, h: float, l: float) -> Optional[float]:
    if intent.kind == OrderKind.STOP:
        return _stop_fill_price(intent.side, intent.price, o, h, l)
    return _limit_fill_price(intent.side, intent.price, o, h, l)


def _sort_key(intent: OrderIntent) -> Tuple[int, int, int]:
    """
    Deterministic priority:
    1) Role: EXIT first when selecting within same-stage bucket.
    2) Kind: STOP before LIMIT.
    3) order_id: ascending.
    Note: Entry-vs-Exit ordering is handled at a higher level (Entry then Exit).
    """
    role_rank = 0 if intent.role == OrderRole.EXIT else 1
    kind_rank = 0 if intent.kind == OrderKind.STOP else 1
    return (role_rank, kind_rank, intent.order_id)


def simulate(
    bars: BarArrays,
    intents: Iterable[OrderIntent],
) -> List[Fill]:
    """
    Phase 1 slow reference matcher.

    Rules enforced:
    - next-bar active only (bar_index == created_bar + 1)
    - STOP/LIMIT gap behavior at Open
    - STOP over LIMIT
    - Same-bar Entry then Exit
    - Same-kind tie: EXIT-first, order_id ascending
    """
    o = bars.open
    h = bars.high
    l = bars.low
    n = int(o.shape[0])

    intents_list = list(intents)
    fills: List[Fill] = []
    state = PositionState(pos=0)

    for t in range(n):
        ot = float(o[t])
        ht = float(h[t])
        lt = float(l[t])

        active = [x for x in intents_list if _is_active(x, t)]
        if not active:
            continue

        # Partition by role for same-bar entry then exit.
        entry_intents = [x for x in active if x.role == OrderRole.ENTRY]
        exit_intents = [x for x in active if x.role == OrderRole.EXIT]

        # Stage 1: ENTRY stage
        if entry_intents:
            # Among entries: STOP before LIMIT, then order_id.
            entry_sorted = sorted(entry_intents, key=lambda x: (0 if x.kind == OrderKind.STOP else 1, x.order_id))
            for it in entry_sorted:
                if state.pos != 0:
                    break  # single-position only
                px = _intent_fill_price(it, ot, ht, lt)
                if px is None:
                    continue
                fills.append(
                    Fill(
                        bar_index=t,
                        role=it.role,
                        kind=it.kind,
                        side=it.side,
                        price=float(px),
                        qty=int(it.qty),
                        order_id=int(it.order_id),
                    )
                )
                # Apply position change
                if it.side == Side.BUY:
                    state.pos = 1
                else:
                    state.pos = -1
                break  # at most one entry fill per bar in Phase 1 reference

        # Stage 2: EXIT stage (after entry)
        if exit_intents and state.pos != 0:
            # Same-kind tie rule: EXIT-first already, and STOP before LIMIT, then order_id
            exit_sorted = sorted(exit_intents, key=_sort_key)
            for it in exit_sorted:
                # Only allow exits that reduce/close current position in this minimal model:
                # long exits are SELL, short exits are BUY.
                if state.pos == 1 and it.side != Side.SELL:
                    continue
                if state.pos == -1 and it.side != Side.BUY:
                    continue

                px = _intent_fill_price(it, ot, ht, lt)
                if px is None:
                    continue
                fills.append(
                    Fill(
                        bar_index=t,
                        role=it.role,
                        kind=it.kind,
                        side=it.side,
                        price=float(px),
                        qty=int(it.qty),
                        order_id=int(it.order_id),
                    )
                )
                state.pos = 0
                break  # at most one exit fill per bar in Phase 1 reference

    return fills



================================================================================
FILE: src/FishBroWFS_V2/engine/metrics_from_fills.py
================================================================================

from __future__ import annotations

from typing import List, Tuple

import numpy as np

from FishBroWFS_V2.engine.types import Fill, OrderRole, Side


def _max_drawdown(equity: np.ndarray) -> float:
    """
    Vectorized max drawdown on an equity curve.
    Handles empty arrays gracefully.
    """
    if equity.size == 0:
        return 0.0
    peak = np.maximum.accumulate(equity)
    dd = equity - peak
    mdd = float(np.min(dd))  # negative or 0
    return mdd


def compute_metrics_from_fills(
    fills: List[Fill],
    commission: float,
    slip: float,
    qty: int,
) -> Tuple[float, int, float, np.ndarray]:
    """
    Compute metrics from fills list.
    
    This is the unified source of truth for metrics computation from fills.
    Both object-mode and array-mode kernels should use this helper to ensure parity.
    
    Args:
        fills: List of Fill objects (can be empty)
        commission: Commission cost per trade (absolute)
        slip: Slippage cost per trade (absolute)
        qty: Order quantity (used for PnL calculation)
    
    Returns:
        Tuple of (net_profit, trades, max_dd, equity):
            - net_profit: float - Total net profit (sum of all round-trip PnL)
            - trades: int - Number of trades (equals pnl.size, not entry fills count)
            - max_dd: float - Maximum drawdown from equity curve
            - equity: np.ndarray - Cumulative equity curve (cumsum of per-trade PnL)
    
    Note:
        - trades is defined as pnl.size (number of completed round-trip trades)
        - Only LONG trades are supported (BUY entry, SELL exit)
        - Costs are applied per fill (entry + exit each incur cost)
        - Metrics are derived from pnl/equity, not from fills count
    """
    # Extract entry/exit prices for round trips
    # Pairing rule: take fills in chronological order, pair BUY(ENTRY) then SELL(EXIT)
    entry_prices = []
    exit_prices = []
    for f in fills:
        if f.role == OrderRole.ENTRY and f.side == Side.BUY:
            entry_prices.append(float(f.price))
        elif f.role == OrderRole.EXIT and f.side == Side.SELL:
            exit_prices.append(float(f.price))
    
    # Match entry/exit pairs (take minimum to handle unpaired entries)
    k = min(len(entry_prices), len(exit_prices))
    if k == 0:
        # No complete round trips: no pnl, so trades = 0
        return (0.0, 0, 0.0, np.empty(0, dtype=np.float64))
    
    ep = np.asarray(entry_prices[:k], dtype=np.float64)
    xp = np.asarray(exit_prices[:k], dtype=np.float64)
    
    # Costs applied per fill (entry + exit)
    costs = (float(commission) + float(slip)) * 2.0
    pnl = (xp - ep) * float(qty) - costs
    equity = np.cumsum(pnl)
    
    # CURSOR TASK 1: trades must equal pnl.size (Source of Truth)
    trades = int(pnl.size)
    net_profit = float(np.sum(pnl)) if pnl.size else 0.0
    max_dd = _max_drawdown(equity)
    
    return (net_profit, trades, max_dd, equity)


================================================================================
FILE: src/FishBroWFS_V2/engine/order_id.py
================================================================================

"""
Deterministic Order ID Generation (CURSOR TASK 5)

Provides pure function for generating deterministic order IDs that do not depend
on generation order or counters. Used by both object-mode and array-mode kernels.
"""
from __future__ import annotations

import numpy as np

from FishBroWFS_V2.config.dtypes import INDEX_DTYPE
from FishBroWFS_V2.engine.constants import KIND_STOP, ROLE_ENTRY, ROLE_EXIT, SIDE_BUY, SIDE_SELL


def generate_order_id(
    created_bar: int,
    param_idx: int = 0,
    role: int = ROLE_ENTRY,
    kind: int = KIND_STOP,
    side: int = SIDE_BUY,
) -> int:
    """
    Generate deterministic order ID from intent attributes.
    
    Uses reversible packing to ensure deterministic IDs that do not depend on
    generation order or counters. This ensures parity between object-mode and
    array-mode kernels.
    
    Formula:
        order_id = created_bar * 1_000_000 + param_idx * 100 + role_code * 10 + kind_code * 2 + side_code_bit
    
    Args:
        created_bar: Bar index where intent is created (0-indexed)
        param_idx: Parameter index (0-indexed, default 0 for single-param kernels)
        role: Role code (ROLE_ENTRY or ROLE_EXIT)
        kind: Kind code (KIND_STOP or KIND_LIMIT)
        side: Side code (SIDE_BUY or SIDE_SELL)
    
    Returns:
        Deterministic order ID (int32)
    
    Note:
        - Maximum created_bar: 2,147,483 (within int32 range)
        - Maximum param_idx: 21,474,836 (within int32 range)
        - This packing scheme ensures uniqueness for typical use cases
    """
    # Map role to code: ENTRY=0, EXIT=1
    role_code = 0 if role == ROLE_ENTRY else 1
    
    # Map kind to code: STOP=0, LIMIT=1 (assuming KIND_STOP=0, KIND_LIMIT=1)
    kind_code = 0 if kind == KIND_STOP else 1
    
    # Map side to bit: BUY=0, SELL=1
    side_bit = 0 if side == SIDE_BUY else 1
    
    # Pack: created_bar * 1_000_000 + param_idx * 100 + role_code * 10 + kind_code * 2 + side_bit
    order_id = (
        created_bar * 1_000_000 +
        param_idx * 100 +
        role_code * 10 +
        kind_code * 2 +
        side_bit
    )
    
    return int(order_id)


def generate_order_ids_array(
    created_bar: np.ndarray,
    param_idx: int = 0,
    role: np.ndarray | None = None,
    kind: np.ndarray | None = None,
    side: np.ndarray | None = None,
) -> np.ndarray:
    """
    Generate deterministic order IDs for array of intents.
    
    Vectorized version of generate_order_id for array-mode kernels.
    
    Args:
        created_bar: Array of created bar indices (int32, shape (n,))
        param_idx: Parameter index (default 0 for single-param kernels)
        role: Array of role codes (uint8, shape (n,)). If None, defaults to ROLE_ENTRY.
        kind: Array of kind codes (uint8, shape (n,)). If None, defaults to KIND_STOP.
        side: Array of side codes (uint8, shape (n,)). If None, defaults to SIDE_BUY.
    
    Returns:
        Array of deterministic order IDs (int32, shape (n,))
    """
    n = len(created_bar)
    
    # Default values if not provided
    if role is None:
        role = np.full(n, ROLE_ENTRY, dtype=np.uint8)
    if kind is None:
        kind = np.full(n, KIND_STOP, dtype=np.uint8)
    if side is None:
        side = np.full(n, SIDE_BUY, dtype=np.uint8)
    
    # Map to codes
    role_code = np.where(role == ROLE_ENTRY, 0, 1).astype(np.int32)
    kind_code = np.where(kind == KIND_STOP, 0, 1).astype(np.int32)
    side_bit = np.where(side == SIDE_BUY, 0, 1).astype(np.int32)
    
    # Pack: created_bar * 1_000_000 + param_idx * 100 + role_code * 10 + kind_code * 2 + side_bit
    order_id = (
        created_bar.astype(np.int32) * 1_000_000 +
        param_idx * 100 +
        role_code * 10 +
        kind_code * 2 +
        side_bit
    )
    
    return order_id.astype(INDEX_DTYPE)


================================================================================
FILE: src/FishBroWFS_V2/engine/simulate.py
================================================================================

"""Unified simulate entry point for Phase 4.

This module provides the single entry point simulate_run() which routes to
the Cursor kernel (main path) or Reference kernel (testing/debugging only).
"""

from __future__ import annotations

from typing import Iterable

from FishBroWFS_V2.engine.types import BarArrays, OrderIntent, SimResult
from FishBroWFS_V2.engine.kernels.cursor_kernel import simulate_cursor_kernel
from FishBroWFS_V2.engine.kernels.reference_kernel import simulate_reference_matcher


def simulate_run(
    bars: BarArrays,
    intents: Iterable[OrderIntent],
    *,
    use_reference: bool = False,
) -> SimResult:
    """
    Unified simulate entry point - Phase 4 main API.
    
    This is the single entry point for all simulation calls. By default, it uses
    the Cursor kernel (main path). The Reference kernel is only available for
    testing/debugging purposes.
    
    Args:
        bars: OHLC bar arrays
        intents: Iterable of order intents
        use_reference: If True, use reference kernel (testing/debug only).
                      Default False uses Cursor kernel (main path).
        
    Returns:
        SimResult containing the fills from simulation
        
    Note:
        - Cursor kernel is the main path for production
        - Reference kernel should only be used for tests/debug
        - This API is stable for pipeline usage
    """
    if use_reference:
        return simulate_reference_matcher(bars, intents)
    return simulate_cursor_kernel(bars, intents)


================================================================================
FILE: src/FishBroWFS_V2/engine/types.py
================================================================================

from __future__ import annotations

from dataclasses import dataclass
from enum import Enum
from typing import List, Optional

import numpy as np


@dataclass(frozen=True)
class BarArrays:
    open: np.ndarray
    high: np.ndarray
    low: np.ndarray
    close: np.ndarray


class Side(int, Enum):
    BUY = 1
    SELL = -1


class OrderKind(str, Enum):
    STOP = "STOP"
    LIMIT = "LIMIT"


class OrderRole(str, Enum):
    ENTRY = "ENTRY"
    EXIT = "EXIT"


@dataclass(frozen=True)
class OrderIntent:
    """
    Order intent created at bar `created_bar` and becomes active at bar `created_bar + 1`.
    Deterministic ordering is controlled via `order_id` (smaller = earlier).
    """
    order_id: int
    created_bar: int
    role: OrderRole
    kind: OrderKind
    side: Side
    price: float
    qty: int = 1


@dataclass(frozen=True)
class Fill:
    bar_index: int
    role: OrderRole
    kind: OrderKind
    side: Side
    price: float
    qty: int
    order_id: int


@dataclass(frozen=True)
class SimResult:
    """
    Simulation result from simulate_run().
    
    This is the standard return type for Phase 4 unified simulate entry point.
    """
    fills: List[Fill]



================================================================================
FILE: src/FishBroWFS_V2/gui/__init__.py
================================================================================

"""GUI package for FishBroWFS_V2."""


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/__init__.py
================================================================================

"""Viewer package for Phase 6.0."""


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/app.py
================================================================================

"""Streamlit Viewer entrypoint (official).

This is the single source of truth for launching the B5 Viewer.
"""

from __future__ import annotations

import os
from pathlib import Path

import streamlit as st

from FishBroWFS_V2.gui.viewer.page_scaffold import render_viewer_page
from FishBroWFS_V2.gui.viewer.pages.kpi import render_page as render_kpi_page
from FishBroWFS_V2.gui.viewer.pages.overview import render_page as render_overview_page
from FishBroWFS_V2.gui.viewer.pages.winners import render_page as render_winners_page
from FishBroWFS_V2.gui.viewer.pages.governance import render_page as render_governance_page
from FishBroWFS_V2.gui.viewer.pages.artifacts import render_page as render_artifacts_page


def get_run_dir_from_query() -> Path | None:
    """
    Get run_dir from query parameters.
    
    Returns:
        Path to run directory if season and run_id are provided, None otherwise
    """
    season = st.query_params.get("season", "")
    run_id = st.query_params.get("run_id", "")
    
    if not season or not run_id:
        return None
    
    # Get outputs root from environment or default
    outputs_root_str = os.getenv("FISHBRO_OUTPUTS_ROOT", "outputs")
    outputs_root = Path(outputs_root_str)
    run_dir = outputs_root / "seasons" / season / "runs" / run_id
    
    return run_dir


def main() -> None:
    """Main Viewer entrypoint."""
    st.set_page_config(
        page_title="FishBroWFS B5 Viewer",
        layout="wide",
    )
    
    # Get run_dir from query params
    run_dir = get_run_dir_from_query()
    
    if not run_dir:
        st.error("Missing query parameters: season and run_id required")
        st.info("Usage: /?season=...&run_id=...")
        st.info("Example: /?season=2026Q1&run_id=demo_20250101T000000Z")
        return
    
    if not run_dir.exists():
        st.error(f"Run directory does not exist: {run_dir}")
        st.info(f"Outputs root: {run_dir.parent.parent.parent}")
        st.info(f"Expected path: {run_dir}")
        return
    
    # Page selection
    page = st.sidebar.selectbox(
        "Viewer Pages",
        [
            "Overview",
            "KPI",
            "Winners",
            "Governance",
            "Artifacts",
        ],
    )
    
    # Render selected page
    if page == "Overview":
        render_viewer_page("Overview", run_dir, render_overview_page)
    elif page == "KPI":
        render_viewer_page("KPI", run_dir, render_kpi_page)
    elif page == "Winners":
        render_viewer_page("Winners", run_dir, render_winners_page)
    elif page == "Governance":
        render_viewer_page("Governance", run_dir, render_governance_page)
    elif page == "Artifacts":
        render_viewer_page("Artifacts", run_dir, render_artifacts_page)


if __name__ == "__main__":
    main()


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/components/__init__.py
================================================================================

"""Viewer components package."""


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/components/evidence_panel.py
================================================================================

"""Evidence Panel component.

Displays evidence for active KPI from artifacts.
"""

from __future__ import annotations

import json

import streamlit as st

from FishBroWFS_V2.gui.viewer.json_pointer import resolve_json_pointer


def render_evidence_panel(artifacts: dict[str, dict]) -> None:
    """
    Render evidence panel showing active KPI evidence.
    
    Args:
        artifacts: Dictionary mapping artifact names to their JSON data
                  e.g., {"manifest": {...}, "winners_v2": {...}, "governance": {...}}
        
    Contract:
        - Never raises exceptions
        - Shows warning if evidence is missing
        - Handles missing session_state gracefully
    """
    try:
        # Get active evidence from session state
        active_evidence = st.session_state.get("active_evidence", None)
        
        if not active_evidence:
            # No active evidence selected
            return
        
        st.subheader("Evidence")
        
        # Extract evidence info safely
        kpi_name = active_evidence.get("kpi_name", "unknown")
        artifact_name = active_evidence.get("artifact", "unknown")
        json_pointer = active_evidence.get("json_pointer", "")
        description = active_evidence.get("description", "")
        
        # Display KPI info
        st.markdown(f"**KPI:** {kpi_name}")
        if description:
            st.caption(description)
        
        st.markdown("---")
        
        # Get artifact data
        artifact_data = artifacts.get(artifact_name)
        
        if artifact_data is None:
            st.warning(f"âš ï¸ Artifact '{artifact_name}' not available.")
            return
        
        # Resolve JSON pointer
        found, value = resolve_json_pointer(artifact_data, json_pointer)
        
        if not found:
            st.warning("âš ï¸ Evidence missing: JSON pointer not found.")
            st.info(f"**Artifact:** {artifact_name}")
            st.info(f"**JSON Pointer:** `{json_pointer}`")
            return
        
        # Display evidence
        st.markdown(f"**Artifact:** `{artifact_name}`")
        st.markdown(f"**JSON Pointer:** `{json_pointer}`")
        
        st.markdown("**Value:**")
        # Pretty print value
        try:
            if isinstance(value, (dict, list)):
                st.json(value)
            else:
                st.code(str(value), language=None)
        except Exception:
            st.text(str(value))
    
    except Exception as e:
        st.error(f"Error rendering evidence panel: {e}")


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/components/kpi_table.py
================================================================================

"""KPI Table component with evidence drill-down.

Renders KPI table with clickable evidence links.
"""

from __future__ import annotations

from typing import Any

import streamlit as st

from FishBroWFS_V2.gui.viewer.kpi_registry import get_evidence_link


def render_kpi_table(kpi_rows: list[dict]) -> None:
    """
    Render KPI table with evidence drill-down capability.
    
    Each row must include:
      - name: str - KPI name
      - value: Any - KPI value (will be converted to string for display)
    
    Optional:
      - label: str - Display label (defaults to name)
      - format: str - Value format hint
    
    Args:
        kpi_rows: List of KPI row dictionaries
        
    Contract:
        - Never raises exceptions
        - KPI names not in registry are displayed but not clickable
        - Missing name/value fields are handled gracefully
    """
    try:
        if not kpi_rows:
            st.info("No KPI data available.")
            return
        
        st.subheader("Key Performance Indicators")
        
        # Render table
        for row in kpi_rows:
            _render_kpi_row(row)
    
    except Exception as e:
        st.error(f"Error rendering KPI table: {e}")


def _render_kpi_row(row: dict) -> None:
    """Render single KPI row."""
    try:
        # Extract row data safely
        kpi_name = row.get("name", "unknown")
        kpi_value = row.get("value", None)
        kpi_label = row.get("label", kpi_name)
        
        # Format value
        value_str = _format_value(kpi_value)
        
        # Check if KPI has evidence link
        evidence_link = get_evidence_link(kpi_name)
        
        if evidence_link:
            # Render with clickable evidence link
            col1, col2, col3 = st.columns([3, 2, 1])
            with col1:
                st.markdown(f"**{kpi_label}**")
            with col2:
                st.text(value_str)
            with col3:
                if st.button("ðŸ” View Evidence", key=f"evidence_{kpi_name}"):
                    # Store evidence link in session state
                    # Extract artifact name from source_path (e.g., "winners_v2.json" -> "winners_v2")
                    artifact_name = evidence_link.source_path.replace(".json", "")
                    st.session_state["active_evidence"] = {
                        "kpi_name": kpi_name,
                        "artifact": artifact_name,
                        "json_pointer": evidence_link.json_pointer,
                        "description": evidence_link.note,
                    }
                    st.rerun()
        else:
            # Render without evidence link
            col1, col2 = st.columns([3, 2])
            with col1:
                st.markdown(f"**{kpi_label}**")
            with col2:
                st.text(value_str)
    
    except Exception:
        # Silently handle errors in row rendering
        pass


def _format_value(value: Any) -> str:
    """Format KPI value for display."""
    try:
        if value is None:
            return "N/A"
        if isinstance(value, (int, float)):
            # Format numbers with appropriate precision
            if isinstance(value, float):
                return f"{value:,.2f}"
            return f"{value:,}"
        return str(value)
    except Exception:
        return str(value) if value is not None else "N/A"


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/components/status_bar.py
================================================================================

"""Artifact Status Bar component for Viewer pages.

Renders consistent status bar across all Viewer pages.
Never raises exceptions - graceful degradation.
"""

from __future__ import annotations

import streamlit as st

from FishBroWFS_V2.gui.viewer.load_state import ArtifactLoadState, ArtifactLoadStatus


def render_artifact_status_bar(states: list[ArtifactLoadState]) -> None:
    """
    Render artifact status bar for Viewer page.
    
    Displays status badges for each artifact with error/dirty information.
    Never raises exceptions - page continues to render even if artifacts are missing/invalid.
    
    Args:
        states: List of ArtifactLoadState for each artifact
        
    Contract:
        - Never raises exceptions
        - Always renders something (even if states is empty)
        - INVALID shows error summary (max 1 line)
        - DIRTY shows dirty_reasons (collapsible expander)
        - Page continues to render even if artifacts are MISSING/INVALID
    """
    if not states:
        return
    
    st.subheader("Artifact Status")
    
    # Create columns for badges
    num_cols = min(len(states), 4)  # Max 4 columns
    cols = st.columns(num_cols)
    
    for idx, state in enumerate(states):
        col_idx = idx % num_cols
        with cols[col_idx]:
            _render_artifact_badge(state)
    
    # Show detailed error/dirty info below badges
    _render_detailed_info(states)


def _render_artifact_badge(state: ArtifactLoadState) -> None:
    """Render single artifact badge."""
    # Map status to badge color
    if state.status == ArtifactLoadStatus.OK:
        badge_color = "ðŸŸ¢"
        badge_text = f"{state.artifact_name}: OK"
    elif state.status == ArtifactLoadStatus.MISSING:
        badge_color = "âšª"
        badge_text = f"{state.artifact_name}: MISSING"
    elif state.status == ArtifactLoadStatus.INVALID:
        badge_color = "ðŸ”´"
        badge_text = f"{state.artifact_name}: INVALID"
    elif state.status == ArtifactLoadStatus.DIRTY:
        badge_color = "ðŸŸ¡"
        badge_text = f"{state.artifact_name}: DIRTY"
    else:
        badge_color = "âšª"
        badge_text = f"{state.artifact_name}: UNKNOWN"
    
    st.markdown(f"{badge_color} **{badge_text}**")
    
    # Show last modified time if available
    if state.last_modified_ts is not None:
        from datetime import datetime
        dt = datetime.fromtimestamp(state.last_modified_ts)
        st.caption(f"Updated: {dt.strftime('%Y-%m-%d %H:%M:%S')}")


def _render_detailed_info(states: list[ArtifactLoadState]) -> None:
    """Render detailed error/dirty information."""
    invalid_states = [s for s in states if s.status == ArtifactLoadStatus.INVALID]
    dirty_states = [s for s in states if s.status == ArtifactLoadStatus.DIRTY]
    
    if not invalid_states and not dirty_states:
        return
    
    # Show INVALID errors
    if invalid_states:
        st.error("**Invalid Artifacts:**")
        for state in invalid_states:
            error_summary = state.error or "Unknown error"
            # Truncate to 1 line if too long
            if len(error_summary) > 100:
                error_summary = error_summary[:97] + "..."
            st.text(f"â€¢ {state.artifact_name}: {error_summary}")
    
    # Show DIRTY reasons (collapsible)
    if dirty_states:
        with st.expander("**Dirty Artifacts (config_hash mismatch)**", expanded=False):
            for state in dirty_states:
                st.markdown(f"**{state.artifact_name}:**")
                if state.dirty_reasons:
                    for reason in state.dirty_reasons:
                        st.text(f"  â€¢ {reason}")
                else:
                    st.text("  â€¢ No specific reason provided")
                st.markdown("---")


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/json_pointer.py
================================================================================

"""JSON Pointer resolver (RFC 6901).

Resolves JSON pointers in a defensive, never-raise manner.
"""

from __future__ import annotations

from typing import Any


def resolve_json_pointer(data: dict, pointer: str) -> tuple[bool, Any | None]:
    """
    Resolve RFC 6901 JSON Pointer.
    
    Never raises; return (found: bool, value).
    
    Supports basic pointer syntax:
    - /a/b/c for object keys
    - /a/b/0 for array indices
    - Does NOT support ~1 ~0 escape sequences (simplified version)
    - Does NOT support root pointer "/" (by design for Viewer UX)
    
    Args:
        data: JSON data (dict/list)
        pointer: RFC 6901 JSON Pointer (e.g., "/a/b/0/c")
        
    Returns:
        Tuple of (found: bool, value: Any | None)
        - found=True: pointer resolved successfully, value contains result
        - found=False: pointer failed to resolve, value is None
        
    Contract:
        - Never raises exceptions
        - Returns (False, None) on any failure
        - Supports list indices (e.g., "/0", "/items/0/name")
        - Root pointer "/" is intentionally disabled (returns False)
    """
    try:
        # â¶ Outermost defense (root cause of previous failure)
        if data is None or not isinstance(data, (dict, list)):
            return (False, None)
        
        if not isinstance(pointer, str):
            return (False, None)
        
        if pointer == "" or pointer == "/":
            return (False, None)
        
        if not pointer.startswith("/"):
            return (False, None)
        
        # â· Normal resolution flow
        parts = pointer.lstrip("/").split("/")
        current: Any = data
        
        for part in parts:
            # list index
            if isinstance(current, list):
                if not part.isdigit():
                    return (False, None)
                idx = int(part)
                if idx < 0 or idx >= len(current):
                    return (False, None)
                current = current[idx]
            # dict key
            elif isinstance(current, dict):
                if part not in current:
                    return (False, None)
                current = current[part]
            else:
                return (False, None)
        
        return (True, current)
    
    except Exception:
        # â¸ Viewer world final safety net
        return (False, None)


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/kpi_registry.py
================================================================================

"""KPI Evidence Registry.

Maps KPI names to EvidenceLink (artifact + JSON pointer).
"""

from __future__ import annotations

from typing import Literal

from FishBroWFS_V2.gui.viewer.schema import EvidenceLink

ArtifactName = Literal["manifest", "winners_v2", "governance"]


# KPI Evidence Registry (first version hardcoded, extensible later)
KPI_EVIDENCE_REGISTRY: dict[str, EvidenceLink] = {
    "net_profit": EvidenceLink(
        source_path="winners_v2.json",
        json_pointer="/summary/net_profit",
        note="Total net profit from winners_v2 summary",
    ),
    "max_drawdown": EvidenceLink(
        source_path="winners_v2.json",
        json_pointer="/summary/max_drawdown",
        note="Maximum drawdown over full backtest",
    ),
    "num_trades": EvidenceLink(
        source_path="winners_v2.json",
        json_pointer="/summary/num_trades",
        note="Total number of executed trades",
    ),
    "final_score": EvidenceLink(
        source_path="governance.json",
        json_pointer="/scoring/final_score",
        note="Governance final score used for KEEP/FREEZE/DROP",
    ),
}


def get_evidence_link(kpi_name: str) -> EvidenceLink | None:
    """
    Get EvidenceLink for KPI name.
    
    Args:
        kpi_name: KPI name to look up
        
    Returns:
        EvidenceLink if found, None otherwise
        
    Contract:
        - Never raises exceptions
        - Returns None for unknown KPI names
    """
    try:
        return KPI_EVIDENCE_REGISTRY.get(kpi_name)
    except Exception:
        return None


def has_evidence(kpi_name: str) -> bool:
    """
    Check if KPI has evidence link.
    
    Args:
        kpi_name: KPI name to check
        
    Returns:
        True if KPI has evidence link, False otherwise
        
    Contract:
        - Never raises exceptions
    """
    try:
        return kpi_name in KPI_EVIDENCE_REGISTRY
    except Exception:
        return False


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/load_state.py
================================================================================

"""Viewer load state model and contract.

Defines unified artifact load status for Viewer pages.
Never raises exceptions - pure mapping logic.
"""

from __future__ import annotations

from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Optional

from FishBroWFS_V2.core.artifact_reader import SafeReadResult
from FishBroWFS_V2.core.artifact_status import ValidationResult, ArtifactStatus


class ArtifactLoadStatus(str, Enum):
    """Artifact load status - fixed string values for UI consistency."""
    OK = "OK"
    MISSING = "MISSING"
    INVALID = "INVALID"
    DIRTY = "DIRTY"


@dataclass(frozen=True)
class ArtifactLoadState:
    """
    Artifact load state for Viewer.
    
    Represents the load status of a single artifact (manifest/winners_v2/governance).
    """
    status: ArtifactLoadStatus
    artifact_name: str  # "manifest" / "winners_v2" / "governance"
    path: Path
    error: Optional[str] = None  # Error message when INVALID
    dirty_reasons: list[str] = None  # List of reasons when DIRTY (can be empty)
    last_modified_ts: Optional[float] = None  # Optional timestamp for UI display
    
    def __post_init__(self) -> None:
        """Ensure dirty_reasons is always a list."""
        if self.dirty_reasons is None:
            object.__setattr__(self, "dirty_reasons", [])


def compute_load_state(
    artifact_name: str,
    path: Path,
    read_result: SafeReadResult,
    validation_result: Optional[ValidationResult] = None,
) -> ArtifactLoadState:
    """
    Compute ArtifactLoadState from read and validation results.
    
    Zero-trust function - never assumes any attribute exists.
    This function performs pure mapping - no IO, no inference, no exceptions.
    
    Args:
        artifact_name: Name of artifact ("manifest", "winners_v2", "governance")
        path: Path to artifact file
        read_result: Result from try_read_artifact()
        validation_result: Optional validation result from validate_*_status()
        
    Returns:
        ArtifactLoadState with mapped status and error information
        
    Contract:
        - Never raises exceptions
        - Only performs mapping logic
        - Status strings are fixed (OK/MISSING/INVALID/DIRTY)
        - Zero-trust: uses getattr for all attribute access
    """
    try:
        # â¶ Zero-trust: check is_error property safely
        is_error = getattr(read_result, "is_error", False)
        
        if is_error:
            # Read error - map to MISSING or INVALID
            error = getattr(read_result, "error", None)
            if error is not None:
                error_code = getattr(error, "error_code", "")
                error_message = getattr(error, "message", "Unknown error")
                
                # FILE_NOT_FOUND -> MISSING
                if error_code == "FILE_NOT_FOUND":
                    return ArtifactLoadState(
                        status=ArtifactLoadStatus.MISSING,
                        artifact_name=artifact_name,
                        path=path,
                        error=None,
                        dirty_reasons=[],
                        last_modified_ts=None,
                    )
                
                # Other errors -> INVALID
                return ArtifactLoadState(
                    status=ArtifactLoadStatus.INVALID,
                    artifact_name=artifact_name,
                    path=path,
                    error=str(error_message),
                    dirty_reasons=[],
                    last_modified_ts=None,
                )
            else:
                # Error object missing -> INVALID
                return ArtifactLoadState(
                    status=ArtifactLoadStatus.INVALID,
                    artifact_name=artifact_name,
                    path=path,
                    error="Read error but error object missing",
                    dirty_reasons=[],
                    last_modified_ts=None,
                )
        
        # File read successfully - check validation result
        read_result_obj = getattr(read_result, "result", None)
        if read_result_obj is None:
            # No result but no error -> INVALID
            return ArtifactLoadState(
                status=ArtifactLoadStatus.INVALID,
                artifact_name=artifact_name,
                path=path,
                error="Read result missing",
                dirty_reasons=[],
                last_modified_ts=None,
            )
        
        # Extract metadata safely
        meta = getattr(read_result_obj, "meta", None)
        last_modified_ts = None
        if meta is not None:
            last_modified_ts = getattr(meta, "mtime_s", None)
        
        # If validation_result is provided, use it
        if validation_result is not None:
            # Zero-trust: get status safely
            validation_status = getattr(validation_result, "status", None)
            
            # Map ValidationResult.status to ArtifactLoadStatus
            if validation_status == ArtifactStatus.OK:
                load_status = ArtifactLoadStatus.OK
            elif validation_status == ArtifactStatus.MISSING:
                load_status = ArtifactLoadStatus.MISSING
            elif validation_status == ArtifactStatus.INVALID:
                load_status = ArtifactLoadStatus.INVALID
            elif validation_status == ArtifactStatus.DIRTY:
                load_status = ArtifactLoadStatus.DIRTY
            else:
                # Fallback to INVALID for unknown status
                load_status = ArtifactLoadStatus.INVALID
            
            # Extract error and dirty_reasons from validation_result safely
            error_msg = None
            dirty_reasons_list: list[str] = []
            
            if load_status == ArtifactLoadStatus.INVALID:
                error_msg = getattr(validation_result, "message", "Unknown validation error")
                error_details = getattr(validation_result, "error_details", None)
                if error_details:
                    # Prefer error_details if available
                    error_msg = str(error_details)
            elif load_status == ArtifactLoadStatus.DIRTY:
                # Extract dirty reason from message
                message = getattr(validation_result, "message", "")
                dirty_reasons_list = [message] if message else []
            
            return ArtifactLoadState(
                status=load_status,
                artifact_name=artifact_name,
                path=path,
                error=error_msg,
                dirty_reasons=dirty_reasons_list,
                last_modified_ts=last_modified_ts,
            )
        
        # No validation result - assume OK if file read successfully
        return ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name=artifact_name,
            path=path,
            error=None,
            dirty_reasons=[],
            last_modified_ts=last_modified_ts,
        )
    
    except Exception as e:
        # â¸ Final safety net: compute_load_state never raises
        return ArtifactLoadState(
            status=ArtifactLoadStatus.INVALID,
            artifact_name=artifact_name,
            path=path,
            error=f"compute_load_state exception: {e}",
            dirty_reasons=[],
            last_modified_ts=None,
        )


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/page_scaffold.py
================================================================================

"""Viewer page scaffold - unified "never crash" page skeleton.

Provides consistent page structure that never raises exceptions.
"""

from __future__ import annotations

from collections.abc import Callable
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import streamlit as st

from FishBroWFS_V2.core.artifact_reader import try_read_artifact
from FishBroWFS_V2.core.artifact_status import (
    ValidationResult,
    validate_manifest_status,
    validate_winners_v2_status,
    validate_governance_status,
)

from FishBroWFS_V2.gui.viewer.load_state import (
    ArtifactLoadState,
    ArtifactLoadStatus,
    compute_load_state,
)
from FishBroWFS_V2.gui.viewer.components.status_bar import render_artifact_status_bar


@dataclass(frozen=True)
class Bundle:
    """
    Bundle of artifacts for Viewer page.
    
    Contains loaded artifacts and their load states.
    """
    manifest_state: ArtifactLoadState
    winners_v2_state: ArtifactLoadState
    governance_state: ArtifactLoadState
    
    @property
    def all_ok(self) -> bool:
        """Check if all artifacts are OK."""
        return all(
            s.status.value == "OK"
            for s in [self.manifest_state, self.winners_v2_state, self.governance_state]
        )
    
    @property
    def has_blocking_error(self) -> bool:
        """Check if any artifact is MISSING or INVALID (blocks page content)."""
        blocking_statuses = {"MISSING", "INVALID"}
        return any(
            s.status.value in blocking_statuses
            for s in [self.manifest_state, self.winners_v2_state, self.governance_state]
        )


def render_viewer_page(
    title: str,
    run_dir: Path,
    content_render_fn: Optional[Callable[[Bundle], None]] = None,
) -> None:
    """
    Render Viewer page with unified scaffold.
    
    This function ensures Viewer pages never crash - all errors are handled gracefully.
    
    Args:
        title: Page title
        run_dir: Path to run directory containing artifacts
        content_render_fn: Optional function to render page content.
                         Receives Bundle with artifact states.
                         If None, only status bar is rendered.
    
    Contract:
        - Never raises exceptions
        - Always renders status bar
        - Shows BLOCKED panel if artifacts are MISSING/INVALID
        - Calls content_render_fn only if artifacts are OK or DIRTY (non-blocking)
    """
    st.set_page_config(page_title=title, layout="wide")
    st.title(title)
    
    # â¶ Load bundle - completely wrapped in try/except
    try:
        bundle = _load_bundle(run_dir)
    except Exception as e:
        # Load phase any error â†’ BLOCKED
        states = [
            ArtifactLoadState(
                status=ArtifactLoadStatus.INVALID,
                artifact_name="bundle",
                path=None,
                error=f"load_bundle_fn exception: {e}",
                dirty_reasons=[],
                last_modified_ts=None,
            )
        ]
        render_artifact_status_bar(states)
        st.error("**BLOCKED / ç„¡æ³•è¼‰å…¥**")
        st.error(f"Viewer BLOCKED: failed to load artifacts. Error: {e}")
        return
    
    # â· Bundle loaded successfully, but internal artifacts may still be missing/invalid
    states = [
        bundle.manifest_state,
        bundle.winners_v2_state,
        bundle.governance_state,
    ]
    
    render_artifact_status_bar(states)
    
    # Check if any artifact is MISSING or INVALID (blocks page content)
    if bundle.has_blocking_error:
        st.error("**BLOCKED / ç„¡æ³•è¼‰å…¥**")
        st.warning("Viewer BLOCKED due to invalid or missing artifacts.")
        return
    
    # â¸ Only OK / DIRTY will reach content render
    if content_render_fn is not None:
        try:
            content_render_fn(bundle)
        except Exception as e:
            # Catch any exceptions from content renderer
            st.error(f"**å…§å®¹æ¸²æŸ“éŒ¯èª¤:** {e}")
            st.exception(e)


def _load_bundle(run_dir: Path) -> Bundle:
    """
    Load artifact bundle from run directory.
    
    Never raises exceptions - all errors are captured in ArtifactLoadState.
    """
    manifest_path = run_dir / "manifest.json"
    winners_path = run_dir / "winners.json"  # Note: file is winners.json but schema is winners_v2
    governance_path = run_dir / "governance.json"
    
    # Read artifacts (never raises)
    manifest_read = try_read_artifact(manifest_path)
    winners_read = try_read_artifact(winners_path)
    governance_read = try_read_artifact(governance_path)
    
    # Validate artifacts (may raise, but we catch exceptions)
    manifest_validation: Optional[ValidationResult] = None
    winners_validation: Optional[ValidationResult] = None
    governance_validation: Optional[ValidationResult] = None
    
    try:
        if manifest_read.is_ok and manifest_read.result:
            # Use already-read data for validation
            manifest_data = manifest_read.result.raw
            manifest_validation = validate_manifest_status(str(manifest_path), manifest_data)
    except Exception:
        pass  # Validation failed, will use read_result only
    
    try:
        if winners_read.is_ok and winners_read.result:
            # Use already-read data for validation
            winners_data = winners_read.result.raw
            winners_validation = validate_winners_v2_status(str(winners_path), winners_data)
    except Exception:
        pass
    
    try:
        if governance_read.is_ok and governance_read.result:
            # Use already-read data for validation
            governance_data = governance_read.result.raw
            governance_validation = validate_governance_status(str(governance_path), governance_data)
    except Exception:
        pass
    
    # Compute load states (never raises)
    manifest_state = compute_load_state(
        "manifest",
        manifest_path,
        manifest_read,
        manifest_validation,
    )
    
    winners_state = compute_load_state(
        "winners_v2",
        winners_path,
        winners_read,
        winners_validation,
    )
    
    governance_state = compute_load_state(
        "governance",
        governance_path,
        governance_read,
        governance_validation,
    )
    
    return Bundle(
        manifest_state=manifest_state,
        winners_v2_state=winners_state,
        governance_state=governance_state,
    )


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/pages/__init__.py
================================================================================

"""Viewer pages package."""


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/pages/artifacts.py
================================================================================

"""Artifacts Viewer page.

Displays raw artifacts JSON.
"""

from __future__ import annotations

import streamlit as st

from FishBroWFS_V2.gui.viewer.page_scaffold import Bundle
from FishBroWFS_V2.core.artifact_reader import try_read_artifact


def render_page(bundle: Bundle) -> None:
    """
    Render Artifacts viewer page.
    
    Args:
        bundle: Bundle containing artifact load states
        
    Contract:
        - Never raises exceptions
        - Displays raw artifacts JSON
    """
    try:
        st.subheader("Raw Artifacts")
        
        # Display manifest
        if bundle.manifest_state.status.value == "OK" and bundle.manifest_state.path:
            st.markdown("### manifest.json")
            manifest_read = try_read_artifact(bundle.manifest_state.path)
            if manifest_read.is_ok and manifest_read.result:
                st.json(manifest_read.result.raw)
        
        # Display winners_v2
        if bundle.winners_v2_state.status.value == "OK" and bundle.winners_v2_state.path:
            st.markdown("### winners_v2.json")
            winners_read = try_read_artifact(bundle.winners_v2_state.path)
            if winners_read.is_ok and winners_read.result:
                st.json(winners_read.result.raw)
        
        # Display governance
        if bundle.governance_state.status.value == "OK" and bundle.governance_state.path:
            st.markdown("### governance.json")
            governance_read = try_read_artifact(bundle.governance_state.path)
            if governance_read.is_ok and governance_read.result:
                st.json(governance_read.result.raw)
    
    except Exception as e:
        st.error(f"Error rendering artifacts page: {e}")


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/pages/governance.py
================================================================================

"""Governance Viewer page.

Displays governance decisions and evidence.
"""

from __future__ import annotations

import streamlit as st

from FishBroWFS_V2.gui.viewer.page_scaffold import Bundle


def render_page(bundle: Bundle) -> None:
    """
    Render Governance viewer page.
    
    Args:
        bundle: Bundle containing artifact load states
        
    Contract:
        - Never raises exceptions
        - Displays governance decisions
    """
    try:
        st.subheader("Governance Decisions")
        
        if bundle.governance_state.status.value == "OK":
            st.info("âœ… Governance data loaded successfully")
            # TODO: Phase 6.2 - Display governance decisions table
            st.info("Governance decisions table display coming in Phase 6.2")
        else:
            st.warning(f"âš ï¸ Governance status: {bundle.governance_state.status.value}")
            if bundle.governance_state.error:
                st.error(f"Error: {bundle.governance_state.error}")
    
    except Exception as e:
        st.error(f"Error rendering governance page: {e}")


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/pages/kpi.py
================================================================================

"""KPI Viewer page.

Displays KPIs with evidence drill-down capability.
"""

from __future__ import annotations

import streamlit as st

from FishBroWFS_V2.gui.viewer.page_scaffold import Bundle
from FishBroWFS_V2.gui.viewer.components.kpi_table import render_kpi_table
from FishBroWFS_V2.gui.viewer.components.evidence_panel import render_evidence_panel
from FishBroWFS_V2.core.artifact_reader import try_read_artifact


def render_page(bundle: Bundle) -> None:
    """
    Render KPI viewer page.
    
    Args:
        bundle: Bundle containing artifact load states
        
    Contract:
        - Never raises exceptions
        - Extracts KPIs from artifacts
        - Renders KPI table and evidence panel
    """
    try:
        # Extract artifacts data
        artifacts = _extract_artifacts(bundle)
        
        # Extract KPIs from artifacts
        kpi_rows = _extract_kpis(artifacts)
        
        # Layout: KPI table on left, evidence panel on right
        col1, col2 = st.columns([2, 1])
        
        with col1:
            render_kpi_table(kpi_rows)
        
        with col2:
            render_evidence_panel(artifacts)
    
    except Exception as e:
        st.error(f"Error rendering KPI page: {e}")


def _extract_artifacts(bundle: Bundle) -> dict[str, dict]:
    """
    Extract artifact data from bundle.
    
    Returns dictionary mapping artifact names to their JSON data.
    """
    artifacts: dict[str, dict] = {}
    
    try:
        # Extract manifest
        if bundle.manifest_state.status.value == "OK" and bundle.manifest_state.path:
            manifest_read = try_read_artifact(bundle.manifest_state.path)
            if manifest_read.is_ok and manifest_read.result:
                artifacts["manifest"] = manifest_read.result.raw
        
        # Extract winners_v2
        if bundle.winners_v2_state.status.value == "OK" and bundle.winners_v2_state.path:
            winners_read = try_read_artifact(bundle.winners_v2_state.path)
            if winners_read.is_ok and winners_read.result:
                artifacts["winners_v2"] = winners_read.result.raw
        
        # Extract governance
        if bundle.governance_state.status.value == "OK" and bundle.governance_state.path:
            governance_read = try_read_artifact(bundle.governance_state.path)
            if governance_read.is_ok and governance_read.result:
                artifacts["governance"] = governance_read.result.raw
    
    except Exception:
        pass
    
    return artifacts


def _extract_kpis(artifacts: dict[str, dict]) -> list[dict]:
    """
    Extract KPI rows from artifacts.
    
    Returns list of KPI row dictionaries.
    """
    kpi_rows: list[dict] = []
    
    try:
        # Extract from winners_v2 summary
        winners_v2 = artifacts.get("winners_v2", {})
        summary = winners_v2.get("summary", {})
        
        if "net_profit" in summary:
            kpi_rows.append({
                "name": "net_profit",
                "value": summary["net_profit"],
                "label": "Net Profit",
            })
        
        if "max_drawdown" in summary:
            kpi_rows.append({
                "name": "max_drawdown",
                "value": summary["max_drawdown"],
                "label": "Max Drawdown",
            })
        
        if "num_trades" in summary:
            kpi_rows.append({
                "name": "num_trades",
                "value": summary["num_trades"],
                "label": "Number of Trades",
            })
        
        # Extract from governance scoring
        governance = artifacts.get("governance", {})
        scoring = governance.get("scoring", {})
        
        if "final_score" in scoring:
            kpi_rows.append({
                "name": "final_score",
                "value": scoring["final_score"],
                "label": "Final Score",
            })
    
    except Exception:
        pass
    
    return kpi_rows


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/pages/overview.py
================================================================================

"""Overview Viewer page.

Displays run overview and summary information.
"""

from __future__ import annotations

import streamlit as st

from FishBroWFS_V2.gui.viewer.page_scaffold import Bundle


def render_page(bundle: Bundle) -> None:
    """
    Render Overview viewer page.
    
    Args:
        bundle: Bundle containing artifact load states
        
    Contract:
        - Never raises exceptions
        - Displays run overview and summary
    """
    try:
        st.subheader("Run Overview")
        
        # Display manifest info if available
        if bundle.manifest_state.status.value == "OK":
            st.info("âœ… Manifest loaded successfully")
        else:
            st.warning(f"âš ï¸ Manifest status: {bundle.manifest_state.status.value}")
        
        # Display summary stats
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Manifest", bundle.manifest_state.status.value)
        with col2:
            st.metric("Winners", bundle.winners_v2_state.status.value)
        with col3:
            st.metric("Governance", bundle.governance_state.status.value)
    
    except Exception as e:
        st.error(f"Error rendering overview page: {e}")


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/pages/winners.py
================================================================================

"""Winners Viewer page.

Displays winners list and details.
"""

from __future__ import annotations

import streamlit as st

from FishBroWFS_V2.gui.viewer.page_scaffold import Bundle


def render_page(bundle: Bundle) -> None:
    """
    Render Winners viewer page.
    
    Args:
        bundle: Bundle containing artifact load states
        
    Contract:
        - Never raises exceptions
        - Displays winners list
    """
    try:
        st.subheader("Winners")
        
        if bundle.winners_v2_state.status.value == "OK":
            st.info("âœ… Winners data loaded successfully")
            # TODO: Phase 6.2 - Display winners table
            st.info("Winners table display coming in Phase 6.2")
        else:
            st.warning(f"âš ï¸ Winners status: {bundle.winners_v2_state.status.value}")
            if bundle.winners_v2_state.error:
                st.error(f"Error: {bundle.winners_v2_state.error}")
    
    except Exception as e:
        st.error(f"Error rendering winners page: {e}")


================================================================================
FILE: src/FishBroWFS_V2/gui/viewer/schema.py
================================================================================

"""Viewer schema definitions.

Public types for Viewer and Audit schema.
"""

from __future__ import annotations

from dataclasses import dataclass


@dataclass(frozen=True)
class EvidenceLink:
    """
    Immutable evidence link pointing to a specific KPI value.
    
    Used to trace any KPI back to its source artifact and JSON pointer.
    """
    source_path: str  # Path to source artifact file (e.g., "winners_v2.json")
    json_pointer: str  # JSON pointer to the value (e.g., "/rows/0/net_profit")
    note: str = ""  # Optional human-readable note


================================================================================
FILE: src/FishBroWFS_V2/indicators/__init__.py
================================================================================




================================================================================
FILE: src/FishBroWFS_V2/indicators/numba_indicators.py
================================================================================

from __future__ import annotations

import numpy as np

try:
    import numba as nb
except Exception:  # pragma: no cover
    nb = None  # type: ignore


# ----------------------------
# Rolling Max / Min
# ----------------------------
# Design choice (v1):
# - Simple loop scan for window <= ~50 is cache-friendly and predictable.
# - Correctness first; no deque optimization in v1.


if nb is not None:

    @nb.njit(cache=False)
    def rolling_max(arr: np.ndarray, window: int) -> np.ndarray:
        n = arr.shape[0]
        out = np.full(n, np.nan, dtype=np.float64)
        if window <= 0:
            return out
        for i in range(n):
            if i < window - 1:
                continue
            start = i - window + 1
            m = arr[start]
            for j in range(start + 1, i + 1):
                v = arr[j]
                if v > m:
                    m = v
            out[i] = m
        return out

    @nb.njit(cache=False)
    def rolling_min(arr: np.ndarray, window: int) -> np.ndarray:
        n = arr.shape[0]
        out = np.full(n, np.nan, dtype=np.float64)
        if window <= 0:
            return out
        for i in range(n):
            if i < window - 1:
                continue
            start = i - window + 1
            m = arr[start]
            for j in range(start + 1, i + 1):
                v = arr[j]
                if v < m:
                    m = v
            out[i] = m
        return out

else:
    # Fallback pure-python (used only if numba unavailable)
    def rolling_max(arr: np.ndarray, window: int) -> np.ndarray:  # type: ignore
        n = arr.shape[0]
        out = np.full(n, np.nan, dtype=np.float64)
        if window <= 0:
            return out
        for i in range(n):
            if i < window - 1:
                continue
            start = i - window + 1
            out[i] = np.max(arr[start : i + 1])
        return out

    def rolling_min(arr: np.ndarray, window: int) -> np.ndarray:  # type: ignore
        n = arr.shape[0]
        out = np.full(n, np.nan, dtype=np.float64)
        if window <= 0:
            return out
        for i in range(n):
            if i < window - 1:
                continue
            start = i - window + 1
            out[i] = np.min(arr[start : i + 1])
        return out


# ----------------------------
# ATR (Wilder's RMA)
# ----------------------------
# Definition:
# TR[t] = max(high[t]-low[t], abs(high[t]-close[t-1]), abs(low[t]-close[t-1]))
# ATR[t] = (ATR[t-1]*(n-1) + TR[t]) / n
# Notes:
# - Recursive; must keep state.
# - First ATR uses simple average of first n TRs.


if nb is not None:

    @nb.njit(cache=False)
    def atr_wilder(high: np.ndarray, low: np.ndarray, close: np.ndarray, window: int) -> np.ndarray:
        n = high.shape[0]
        out = np.full(n, np.nan, dtype=np.float64)
        if window <= 0 or n == 0:
            return out
        if window > n:
            return out

        # TR computation
        tr = np.empty(n, dtype=np.float64)
        tr[0] = high[0] - low[0]
        for i in range(1, n):
            a = high[i] - low[i]
            b = abs(high[i] - close[i - 1])
            c = abs(low[i] - close[i - 1])
            tr[i] = a if a >= b and a >= c else (b if b >= c else c)

        # initial ATR: simple average of first window TRs
        s = 0.0
        end = window if window < n else n
        for i in range(end):
            s += tr[i]
        # here window <= n guaranteed
        out[end - 1] = s / window

        # Wilder smoothing
        for i in range(window, n):
            out[i] = (out[i - 1] * (window - 1) + tr[i]) / window

        return out

else:
    def atr_wilder(high: np.ndarray, low: np.ndarray, close: np.ndarray, window: int) -> np.ndarray:  # type: ignore
        n = high.shape[0]
        out = np.full(n, np.nan, dtype=np.float64)
        if window <= 0 or n == 0:
            return out
        if window > n:
            return out

        tr = np.empty(n, dtype=np.float64)
        tr[0] = high[0] - low[0]
        for i in range(1, n):
            tr[i] = max(
                high[i] - low[i],
                abs(high[i] - close[i - 1]),
                abs(low[i] - close[i - 1]),
            )

        end = min(window, n)
        # window <= n guaranteed
        out[end - 1] = np.mean(tr[:end])
        for i in range(window, n):
            out[i] = (out[i - 1] * (window - 1) + tr[i]) / window
        return out



================================================================================
FILE: src/FishBroWFS_V2/perf/__init__.py
================================================================================

"""
Performance profiling utilities.
"""


================================================================================
FILE: src/FishBroWFS_V2/perf/cost_model.py
================================================================================

"""Cost model for performance estimation.

Provides predictable cost estimation: given bars and params, estimate execution time.
"""

from __future__ import annotations


def estimate_seconds(
    bars: int,
    params: int,
    cost_ms_per_param: float,
) -> float:
    """
    Estimate execution time in seconds based on cost model.
    
    Cost model assumption:
    - Time is linear in number of parameters only
    - Cost per parameter is measured in milliseconds
    - Formula: time_seconds = (params * cost_ms_per_param) / 1000.0
    - Note: bars parameter is for reference only and does not affect the calculation
    
    Args:
        bars: number of bars (for reference only, not used in calculation)
        params: number of parameters
        cost_ms_per_param: cost per parameter in milliseconds
        
    Returns:
        Estimated time in seconds
        
    Note:
        - This is a simple linear model: time = params * cost_per_param_ms / 1000.0
        - Bars are provided for reference but NOT used in the calculation
        - The model assumes cost per parameter is constant (measured from actual runs)
    """
    if params <= 0:
        return 0.0
    
    if cost_ms_per_param <= 0:
        return 0.0
    
    # Linear model: time = params * cost_per_param_ms / 1000.0
    estimated_seconds = (params * cost_ms_per_param) / 1000.0
    
    return estimated_seconds


================================================================================
FILE: src/FishBroWFS_V2/perf/profile_report.py
================================================================================

from __future__ import annotations

import cProfile
import io
import os
import pstats


def _format_profile_report(
    lane_id: str,
    n_bars: int,
    n_params: int,
    jit_enabled: bool,
    sort_params: bool,
    topn: int,
    mode: str,
    pr: cProfile.Profile,
) -> str:
    """
    Format a deterministic profile report string for perf harness.

    Contract:
    - Always includes __PROFILE_START__/__PROFILE_END__ markers.
    - Always includes the 'pstats sort: cumtime' header even if no stats exist.
    - Must not throw when the profile has no collected stats (empty Profile).
    """
    s = io.StringIO()
    s.write("__PROFILE_START__\n")
    s.write(f"lane_id={lane_id}\n")
    s.write(f"bars={n_bars} params={n_params}\n")
    s.write(f"jit_enabled={jit_enabled} sort_params={sort_params}\n")
    s.write(f"pid={os.getpid()}\n")
    if mode is not None:
        s.write(f"mode={mode}\n")
    s.write("\n")

    # Always emit the headers so tests can rely on markers/labels.
    s.write(f"== pstats sort: cumtime (top {topn}) ==\n")
    try:
        ps = pstats.Stats(pr, stream=s).strip_dirs()
        ps.sort_stats("cumtime")
        ps.print_stats(topn)
    except TypeError:
        s.write("(no profile stats collected)\n")

    s.write("\n\n")
    s.write(f"== pstats sort: tottime (top {topn}) ==\n")
    try:
        ps = pstats.Stats(pr, stream=s).strip_dirs()
        ps.sort_stats("tottime")
        ps.print_stats(topn)
    except TypeError:
        s.write("(no profile stats collected)\n")

    s.write("\n\n__PROFILE_END__\n")
    return s.getvalue()


================================================================================
FILE: src/FishBroWFS_V2/perf/scenario_control.py
================================================================================

"""
Perf Harness Scenario Control (P2-1.6)

Provides trigger rate masking for perf harness to control sparse trigger density.
"""
from __future__ import annotations

import numpy as np


def apply_trigger_rate_mask(
    trigger: np.ndarray,
    trigger_rate: float,
    warmup: int = 0,
    seed: int = 42,
) -> np.ndarray:
    """
    Apply deterministic trigger rate mask to trigger array.
    
    This function masks trigger array to control sparse trigger density for perf testing.
    Only applies masking when trigger_rate < 1.0. When trigger_rate == 1.0, returns
    original array unchanged (preserves baseline behavior).
    
    Args:
        trigger: Input trigger array (e.g., donch_prev) of shape (n_bars,)
        trigger_rate: Rate of triggers to keep (0.0 to 1.0). Must be in [0, 1].
        warmup: Warmup period. Positions before warmup that are already NaN are preserved.
        seed: Random seed for deterministic masking.
    
    Returns:
        Masked trigger array with same dtype as input. Positions not kept are set to NaN.
    
    Rules:
        - If trigger_rate == 1.0: return original array unchanged
        - Otherwise: use RNG to determine which positions to keep
        - Respect warmup: positions < warmup that are already NaN remain NaN
        - Positions >= warmup are subject to masking
        - Keep dtype unchanged
    """
    if trigger_rate < 0.0 or trigger_rate > 1.0:
        raise ValueError(f"trigger_rate must be in [0, 1], got {trigger_rate}")
    
    # Fast path: no masking needed
    if trigger_rate == 1.0:
        return trigger
    
    # Create a copy to avoid modifying input
    masked = trigger.copy()
    
    # Use deterministic RNG
    rng = np.random.default_rng(seed)
    
    # Generate keep mask: positions to keep based on trigger_rate
    # Only apply masking to positions >= warmup that are currently finite
    n = len(trigger)
    keep_mask = np.ones(n, dtype=bool)  # Default: keep all
    
    # For positions >= warmup, apply random masking
    if warmup < n:
        # Generate random values for positions >= warmup
        random_vals = rng.random(n - warmup)
        keep_mask[warmup:] = random_vals < trigger_rate
    
    # Preserve existing NaN positions (they should remain NaN)
    # Only mask positions that are currently finite and not kept
    finite_mask = np.isfinite(masked)
    
    # Apply masking: set non-kept finite positions to NaN
    # But preserve warmup period (positions < warmup remain unchanged)
    to_mask = finite_mask & (~keep_mask)
    masked[to_mask] = np.nan
    
    return masked


================================================================================
FILE: src/FishBroWFS_V2/perf/timers.py
================================================================================

"""
Perf Harness Timer Helper (P2-1.8)

Provides granular timing breakdown for kernel stages.
"""
from __future__ import annotations

import time
from typing import Dict


class PerfTimers:
    """
    Performance timer helper for granular breakdown.
    
    Supports multiple start/stop calls for the same timer name (accumulates).
    All timings are in seconds with '_s' suffix.
    """
    
    def __init__(self) -> None:
        self._accumulated: Dict[str, float] = {}
        self._active: Dict[str, float] = {}
    
    def start(self, name: str) -> None:
        """
        Start a timer. If already running, does nothing (no nested timing).
        """
        if name not in self._active:
            self._active[name] = time.perf_counter()
    
    def stop(self, name: str) -> None:
        """
        Stop a timer and accumulate the elapsed time.
        If timer was not started, does nothing.
        """
        if name in self._active:
            elapsed = time.perf_counter() - self._active[name]
            self._accumulated[name] = self._accumulated.get(name, 0.0) + elapsed
            del self._active[name]
    
    def as_dict_seconds(self) -> Dict[str, float]:
        """
        Return accumulated timings as dict with '_s' suffix keys.
        
        Returns:
            dict with keys like "t_xxx_s": float (seconds)
        """
        result: Dict[str, float] = {}
        for name, seconds in self._accumulated.items():
            # Ensure '_s' suffix
            key = name if name.endswith("_s") else f"{name}_s"
            result[key] = float(seconds)
        return result
    
    def get(self, name: str, default: float = 0.0) -> float:
        """
        Get accumulated time for a timer name.
        """
        return self._accumulated.get(name, default)


================================================================================
FILE: src/FishBroWFS_V2/pipeline/__init__.py
================================================================================




================================================================================
FILE: src/FishBroWFS_V2/pipeline/funnel.py
================================================================================

"""Funnel orchestrator - Stage0 â†’ Top-K â†’ Stage2 pipeline.

This is the main entry point for the Phase 4 Funnel pipeline.
It orchestrates the complete flow: proxy ranking â†’ selection â†’ full backtest.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Optional

import numpy as np

from FishBroWFS_V2.config.constants import TOPK_K
from FishBroWFS_V2.pipeline.stage0_runner import Stage0Result, run_stage0
from FishBroWFS_V2.pipeline.stage2_runner import Stage2Result, run_stage2
from FishBroWFS_V2.pipeline.topk import select_topk


@dataclass(frozen=True)
class FunnelResult:
    """
    Complete funnel pipeline result.
    
    Contains:
    - stage0_results: all Stage0 proxy ranking results
    - topk_param_ids: selected Top-K parameter indices
    - stage2_results: full backtest results for Top-K parameters
    - meta: optional metadata
    """
    stage0_results: List[Stage0Result]
    topk_param_ids: List[int]
    stage2_results: List[Stage2Result]
    meta: Optional[dict] = None


def run_funnel(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
    *,
    k: int = TOPK_K,
    commission: float = 0.0,
    slip: float = 0.0,
    order_qty: int = 1,
    proxy_name: str = "ma_proxy_v0",
) -> FunnelResult:
    """
    Run complete Funnel pipeline: Stage0 â†’ Top-K â†’ Stage2.
    
    Pipeline flow (fixed):
    1. Stage0: proxy ranking on all parameters
    2. Top-K: select top K parameters based on proxy_value
    3. Stage2: full backtest on Top-K subset
    
    Args:
        open_, high, low, close: OHLC arrays (float64, 1D, same length)
        params_matrix: float64 2D array (n_params, >=3)
            - For Stage0: uses col0 (fast_len), col1 (slow_len) for MA proxy
            - For Stage2: uses col0 (channel_len), col1 (atr_len), col2 (stop_mult) for kernel
        k: number of top parameters to select (default: TOPK_K)
        commission: commission per trade (absolute)
        slip: slippage per trade (absolute)
        order_qty: order quantity (default: 1)
        proxy_name: name of proxy to use for Stage0 (default: ma_proxy_v0)
        
    Returns:
        FunnelResult containing:
        - stage0_results: all proxy ranking results
        - topk_param_ids: selected Top-K parameter indices
        - stage2_results: full backtest results for Top-K only
        
    Note:
        - Pipeline is deterministic: same input produces same output
        - Stage0 does NOT compute PnL metrics (only proxy_value)
        - Top-K selection is based solely on proxy_value
        - Stage2 runs full backtest only on Top-K subset
    """
    # Step 1: Stage0 - proxy ranking
    stage0_results = run_stage0(
        close,
        params_matrix,
        proxy_name=proxy_name,
    )
    
    # Step 2: Top-K selection
    topk_param_ids = select_topk(stage0_results, k=k)
    
    # Step 3: Stage2 - full backtest on Top-K
    stage2_results = run_stage2(
        open_,
        high,
        low,
        close,
        params_matrix,
        topk_param_ids,
        commission=commission,
        slip=slip,
        order_qty=order_qty,
    )
    
    return FunnelResult(
        stage0_results=stage0_results,
        topk_param_ids=topk_param_ids,
        stage2_results=stage2_results,
        meta=None,
    )


================================================================================
FILE: src/FishBroWFS_V2/pipeline/funnel_plan.py
================================================================================

"""Funnel plan builder.

Builds default funnel plan with three stages:
- Stage 0: Coarse subsample (config rate)
- Stage 1: Increased subsample (min(1.0, stage0_rate * 2))
- Stage 2: Full confirm (1.0)
"""

from __future__ import annotations

from FishBroWFS_V2.pipeline.funnel_schema import FunnelPlan, StageName, StageSpec


def build_default_funnel_plan(cfg: dict) -> FunnelPlan:
    """
    Build default funnel plan with three stages.
    
    Rules (locked):
    - Stage 0: subsample = config's param_subsample_rate (coarse exploration)
    - Stage 1: subsample = min(1.0, stage0_rate * 2) (increased density)
    - Stage 2: subsample = 1.0 (full confirm, mandatory)
    
    Args:
        cfg: Configuration dictionary containing:
            - param_subsample_rate: Base subsample rate for Stage 0
            - topk_stage0: Optional top-K for Stage 0 (default: 50)
            - topk_stage1: Optional top-K for Stage 1 (default: 20)
    
    Returns:
        FunnelPlan with three stages
    """
    s0_rate = float(cfg["param_subsample_rate"])
    s1_rate = min(1.0, s0_rate * 2.0)
    s2_rate = 1.0  # Stage2 must be 1.0
    
    return FunnelPlan(stages=[
        StageSpec(
            name=StageName.STAGE0_COARSE,
            param_subsample_rate=s0_rate,
            topk=int(cfg.get("topk_stage0", 50)),
            notes={"rule": "default", "description": "Coarse exploration"},
        ),
        StageSpec(
            name=StageName.STAGE1_TOPK,
            param_subsample_rate=s1_rate,
            topk=int(cfg.get("topk_stage1", 20)),
            notes={"rule": "default", "description": "Top-K refinement"},
        ),
        StageSpec(
            name=StageName.STAGE2_CONFIRM,
            param_subsample_rate=s2_rate,
            topk=None,
            notes={"rule": "default", "description": "Full confirmation"},
        ),
    ])


================================================================================
FILE: src/FishBroWFS_V2/pipeline/funnel_runner.py
================================================================================

"""Funnel runner - orchestrates stage execution and artifact writing.

Runs funnel pipeline stages sequentially, writing artifacts for each stage.
Each stage gets its own run_id and run directory.
"""

from __future__ import annotations

import subprocess
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict

from FishBroWFS_V2.core.artifacts import write_run_artifacts
from FishBroWFS_V2.core.audit_schema import AuditSchema, compute_params_effective
from FishBroWFS_V2.core.config_hash import stable_config_hash
from FishBroWFS_V2.core.config_snapshot import make_config_snapshot
from FishBroWFS_V2.core.oom_gate import decide_oom_action
from FishBroWFS_V2.core.paths import ensure_run_dir
from FishBroWFS_V2.core.run_id import make_run_id
from FishBroWFS_V2.pipeline.funnel_plan import build_default_funnel_plan
from FishBroWFS_V2.pipeline.funnel_schema import FunnelResultIndex, FunnelStageIndex
from FishBroWFS_V2.pipeline.runner_adapter import run_stage_job


def _get_git_info(repo_root: Path | None = None) -> tuple[str, bool]:
    """
    Get git SHA and dirty status.
    
    Args:
        repo_root: Optional path to repo root
        
    Returns:
        Tuple of (git_sha, dirty_repo)
    """
    if repo_root is None:
        repo_root = Path.cwd()
    
    try:
        # Get git SHA (short, 12 chars)
        result = subprocess.run(
            ["git", "rev-parse", "--short=12", "HEAD"],
            cwd=repo_root,
            capture_output=True,
            text=True,
            check=True,
            timeout=5,
        )
        git_sha = result.stdout.strip()
        
        # Check if repo is dirty
        result_status = subprocess.run(
            ["git", "status", "--porcelain"],
            cwd=repo_root,
            capture_output=True,
            text=True,
            check=True,
            timeout=5,
        )
        dirty_repo = len(result_status.stdout.strip()) > 0
        
        return git_sha, dirty_repo
    except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):
        return "unknown", True


def run_funnel(cfg: dict, outputs_root: Path) -> FunnelResultIndex:
    """
    Run funnel pipeline with three stages.
    
    Each stage:
    1. Generates new run_id
    2. Creates run directory
    3. Builds AuditSchema
    4. Runs stage job (via adapter)
    5. Writes artifacts
    
    Args:
        cfg: Configuration dictionary containing:
            - season: Season identifier
            - dataset_id: Dataset identifier
            - bars: Number of bars
            - params_total: Total parameters
            - param_subsample_rate: Base subsample rate for Stage 0
            - open_, high, low, close: OHLC arrays
            - params_matrix: Parameter matrix
            - commission, slip, order_qty: Trading parameters
            - topk_stage0, topk_stage1: Optional top-K counts
            - git_sha, dirty_repo, created_at: Optional audit fields
        outputs_root: Root outputs directory
    
    Returns:
        FunnelResultIndex with plan and stage execution indices
    """
    # Build funnel plan
    plan = build_default_funnel_plan(cfg)
    
    # Get git info if not provided
    git_sha = cfg.get("git_sha")
    dirty_repo = cfg.get("dirty_repo")
    if git_sha is None or dirty_repo is None:
        repo_root = cfg.get("repo_root")
        if repo_root:
            repo_root = Path(repo_root)
        git_sha, dirty_repo = _get_git_info(repo_root)
    
    created_at = cfg.get("created_at")
    if created_at is None:
        created_at = datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")
    
    season = cfg["season"]
    dataset_id = cfg["dataset_id"]
    bars = int(cfg["bars"])
    params_total = int(cfg["params_total"])
    
    stage_indices: list[FunnelStageIndex] = []
    prev_winners: list[dict[str, Any]] = []
    
    for spec in plan.stages:
        # Generate run_id for this stage
        run_id = make_run_id(prefix=str(spec.name.value))
        
        # Create run directory
        run_dir = ensure_run_dir(outputs_root, season, run_id)
        
        # Build stage config (runtime: includes ndarrays for runner_adapter)
        stage_cfg = dict(cfg)
        stage_cfg["stage_name"] = str(spec.name.value)
        stage_cfg["param_subsample_rate"] = float(spec.param_subsample_rate)
        stage_cfg["topk"] = spec.topk
        
        # Pass previous stage winners to Stage2
        if spec.name.value == "stage2_confirm" and prev_winners:
            stage_cfg["prev_stage_winners"] = prev_winners
        
        # OOM Gate: Check memory limits before running stage
        mem_limit_mb = float(cfg.get("mem_limit_mb", 2048.0))
        allow_auto_downsample = cfg.get("allow_auto_downsample", True)
        auto_downsample_step = float(cfg.get("auto_downsample_step", 0.5))
        auto_downsample_min = float(cfg.get("auto_downsample_min", 0.02))
        
        gate_result = decide_oom_action(
            stage_cfg,
            mem_limit_mb=mem_limit_mb,
            allow_auto_downsample=allow_auto_downsample,
            auto_downsample_step=auto_downsample_step,
            auto_downsample_min=auto_downsample_min,
        )
        
        # Handle gate actions
        if gate_result["action"] == "BLOCK":
            raise RuntimeError(
                f"OOM Gate BLOCKED stage {spec.name.value}: {gate_result['reason']}"
            )
        
        # Planned subsample for this stage (before gate adjustment)
        planned_subsample = float(spec.param_subsample_rate)
        final_subsample = gate_result["final_subsample"]
        
        if gate_result["action"] == "AUTO_DOWNSAMPLE":
            # Update runtime config
            stage_cfg["param_subsample_rate"] = final_subsample
            # Also update spec for consistency (but spec is frozen, so we track separately)
            # The final_subsample will be used for all subsequent calculations
        
        # Use final_subsample for all calculations
        effective_subsample = final_subsample
        
        # Create sanitized snapshot (for hash and artifacts, excludes ndarrays)
        # Snapshot must reflect final subsample (after auto-downsample if any)
        stage_snapshot = make_config_snapshot(stage_cfg)
        
        # Compute config hash (only on sanitized snapshot)
        config_hash = stable_config_hash(stage_snapshot)
        
        # Compute params_effective with final subsample
        params_effective = compute_params_effective(params_total, effective_subsample)
        
        # Build AuditSchema (must use final subsample)
        audit = AuditSchema(
            run_id=run_id,
            created_at=created_at,
            git_sha=git_sha,
            dirty_repo=bool(dirty_repo),
            param_subsample_rate=effective_subsample,  # Use final subsample
            config_hash=config_hash,
            season=season,
            dataset_id=dataset_id,
            bars=bars,
            params_total=params_total,
            params_effective=params_effective,
            artifact_version="v1",
        )
        
        # Run stage job (adapter returns data only, no file I/O)
        # Use stage_cfg which has final subsample (after auto-downsample if any)
        stage_out = run_stage_job(stage_cfg)
        
        # Extract metrics and winners
        stage_metrics = dict(stage_out.get("metrics", {}))
        stage_winners = stage_out.get("winners", {"topk": [], "notes": {"schema": "v1"}})
        
        # Ensure metrics include required fields
        stage_metrics["param_subsample_rate"] = effective_subsample  # Use final subsample
        stage_metrics["params_effective"] = params_effective
        stage_metrics["params_total"] = params_total
        stage_metrics["bars"] = bars
        stage_metrics["stage_name"] = str(spec.name.value)
        
        # Add OOM gate fields (mandatory for audit)
        stage_metrics["oom_gate_action"] = gate_result["action"]
        stage_metrics["oom_gate_reason"] = gate_result["reason"]
        stage_metrics["mem_est_mb"] = gate_result["estimates"]["mem_est_mb"]
        stage_metrics["mem_limit_mb"] = mem_limit_mb
        stage_metrics["ops_est"] = gate_result["estimates"]["ops_est"]
        
        # Record planned subsample (before gate adjustment)
        stage_metrics["stage_planned_subsample"] = planned_subsample
        
        # If auto-downsample occurred, record original and final subsample
        if gate_result["action"] == "AUTO_DOWNSAMPLE":
            stage_metrics["oom_gate_original_subsample"] = planned_subsample
            stage_metrics["oom_gate_final_subsample"] = final_subsample
        
        # Write artifacts (unified artifact system)
        # Use sanitized snapshot (not runtime cfg with ndarrays)
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=stage_snapshot,
            metrics=stage_metrics,
            winners=stage_winners,
        )
        
        # Record stage index
        stage_indices.append(
            FunnelStageIndex(
                stage=spec.name,
                run_id=run_id,
                run_dir=str(run_dir.relative_to(outputs_root)),
            )
        )
        
        # Save winners for next stage
        prev_winners = stage_winners.get("topk", [])
    
    return FunnelResultIndex(plan=plan, stages=stage_indices)


================================================================================
FILE: src/FishBroWFS_V2/pipeline/funnel_schema.py
================================================================================

"""Funnel schema definitions.

Defines stage names, specifications, and result indexing for funnel pipeline.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional


class StageName(str, Enum):
    """Stage names for funnel pipeline."""
    STAGE0_COARSE = "stage0_coarse"
    STAGE1_TOPK = "stage1_topk"
    STAGE2_CONFIRM = "stage2_confirm"


@dataclass(frozen=True)
class StageSpec:
    """
    Stage specification for funnel pipeline.
    
    Each stage defines:
    - name: Stage identifier
    - param_subsample_rate: Subsample rate for this stage
    - topk: Optional top-K count (None for Stage2)
    - notes: Additional metadata
    """
    name: StageName
    param_subsample_rate: float
    topk: Optional[int] = None
    notes: Dict[str, Any] = field(default_factory=dict)


@dataclass(frozen=True)
class FunnelPlan:
    """
    Funnel plan containing ordered list of stages.
    
    Stages are executed in order: Stage0 -> Stage1 -> Stage2
    """
    stages: List[StageSpec]


@dataclass(frozen=True)
class FunnelStageIndex:
    """
    Index entry for a single stage execution.
    
    Records:
    - stage: Stage name
    - run_id: Run ID for this stage
    - run_dir: Relative path to run directory
    """
    stage: StageName
    run_id: str
    run_dir: str  # Relative path string


@dataclass(frozen=True)
class FunnelResultIndex:
    """
    Complete funnel execution result index.
    
    Contains:
    - plan: Original funnel plan
    - stages: List of stage execution indices
    """
    plan: FunnelPlan
    stages: List[FunnelStageIndex]


================================================================================
FILE: src/FishBroWFS_V2/pipeline/governance_eval.py
================================================================================

"""Governance evaluator - rule engine for candidate decisions.

Reads artifacts from stage run directories and applies governance rules
to produce KEEP/FREEZE/DROP decisions for each candidate.
"""

from __future__ import annotations

from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from FishBroWFS_V2.core.artifact_reader import (
    read_config_snapshot,
    read_manifest,
    read_metrics,
    read_winners,
)
from FishBroWFS_V2.core.config_hash import stable_config_hash
from FishBroWFS_V2.core.governance_schema import (
    Decision,
    EvidenceRef,
    GovernanceItem,
    GovernanceReport,
)
from FishBroWFS_V2.core.winners_schema import is_winners_v2


# Rule thresholds (MVP - locked)
R2_DEGRADE_THRESHOLD = 0.20  # 20% degradation threshold for R2
R3_DENSITY_THRESHOLD = 3  # Minimum count for R3 FREEZE (same strategy_id)


def normalize_candidate(
    item: Dict[str, Any],
    config_snapshot: Optional[Dict[str, Any]] = None,
    is_v2: bool = False,
) -> Tuple[str, Dict[str, Any], Dict[str, Any]]:
    """
    Normalize candidate from winners.json to (strategy_id, params_dict, metrics_subset).
    
    Handles both v2 and legacy formats gracefully.
    
    Args:
        item: Candidate item from winners.json topk list
        config_snapshot: Optional config snapshot to extract params from
        is_v2: Whether item is from v2 schema (fast path)
        
    Returns:
        Tuple of (strategy_id, params_dict, metrics_subset)
        - strategy_id: Strategy identifier
        - params_dict: Normalized params dict
        - metrics_subset: Metrics dict extracted from item
    """
    # Fast path for v2 schema
    if is_v2:
        strategy_id = item.get("strategy_id", "unknown")
        params_dict = item.get("params", {})
        
        # Extract metrics from v2 structure
        metrics_subset = {}
        metrics = item.get("metrics", {})
        
        # Legacy fields (for backward compatibility)
        if "net_profit" in metrics:
            metrics_subset["net_profit"] = float(metrics["net_profit"])
        if "trades" in metrics:
            metrics_subset["trades"] = int(metrics["trades"])
        if "max_dd" in metrics:
            metrics_subset["max_dd"] = float(metrics["max_dd"])
        if "proxy_value" in metrics:
            metrics_subset["proxy_value"] = float(metrics["proxy_value"])
        
        # Also check top-level (legacy compatibility)
        if "net_profit" in item:
            metrics_subset["net_profit"] = float(item["net_profit"])
        if "trades" in item:
            metrics_subset["trades"] = int(item["trades"])
        if "max_dd" in item:
            metrics_subset["max_dd"] = float(item["max_dd"])
        if "proxy_value" in item:
            metrics_subset["proxy_value"] = float(item["proxy_value"])
        
        return strategy_id, params_dict, metrics_subset
    
    # Legacy path (backward compatibility)
    # Extract metrics subset (varies by stage)
    metrics_subset = {}
    if "proxy_value" in item:
        metrics_subset["proxy_value"] = float(item["proxy_value"])
    if "net_profit" in item:
        metrics_subset["net_profit"] = float(item["net_profit"])
    if "trades" in item:
        metrics_subset["trades"] = int(item["trades"])
    if "max_dd" in item:
        metrics_subset["max_dd"] = float(item["max_dd"])
    
    # MVP: Use fixed strategy_id (donchian_atr)
    # Future: Extract from config_snapshot or item metadata
    strategy_id = "donchian_atr"
    
    # Extract params_dict
    # Priority: 1) item["params"], 2) config_snapshot params, 3) fallback to param_id-based dict
    params_dict = item.get("params", {})
    
    if not params_dict and config_snapshot:
        # Try to extract from config_snapshot
        # MVP: If params_matrix is in config_snapshot, extract row by param_id
        # For now, use param_id as fallback
        param_id = item.get("param_id")
        if param_id is not None:
            # MVP fallback: Create minimal params dict from param_id
            # Future: Extract actual params from params_matrix in config_snapshot
            params_dict = {"param_id": int(param_id)}
    
    if not params_dict:
        # Final fallback: use param_id if available
        param_id = item.get("param_id")
        if param_id is not None:
            params_dict = {"param_id": int(param_id)}
        else:
            params_dict = {}
    
    return strategy_id, params_dict, metrics_subset


def generate_candidate_id(strategy_id: str, params_dict: Dict[str, Any]) -> str:
    """
    Generate stable candidate_id from strategy_id and params_dict.
    
    Format: {strategy_id}:{params_hash[:12]}
    
    Args:
        strategy_id: Strategy identifier
        params_dict: Parameters dict (must be JSON-serializable)
        
    Returns:
        Stable candidate_id string
    """
    # Compute stable hash of params_dict
    params_hash = stable_config_hash(params_dict)
    
    # Use first 12 chars of hash
    hash_short = params_hash[:12]
    
    return f"{strategy_id}:{hash_short}"


def find_stage2_candidate(
    candidate_param_id: int,
    stage2_winners: List[Dict[str, Any]],
) -> Optional[Dict[str, Any]]:
    """
    Find Stage2 candidate matching param_id.
    
    Args:
        candidate_param_id: param_id from Stage1 winner
        stage2_winners: List of Stage2 winners
        
    Returns:
        Matching Stage2 candidate dict, or None if not found
    """
    for item in stage2_winners:
        if item.get("param_id") == candidate_param_id:
            return item
    return None


def extract_key_metric(
    metrics: Dict[str, Any],
    candidate_metrics: Dict[str, Any],
    metric_name: str,
) -> Optional[float]:
    """
    Extract key metric with fallback logic.
    
    Priority:
    1. candidate_metrics[metric_name]
    2. metrics[metric_name]
    3. Fallback: net_profit / max_dd (if both exist)
    4. None
    
    Args:
        metrics: Stage metrics dict
        candidate_metrics: Candidate-specific metrics dict
        metric_name: Metric name to extract
        
    Returns:
        Metric value (float), or None if not found
    """
    # Try candidate_metrics first
    if metric_name in candidate_metrics:
        val = candidate_metrics[metric_name]
        if isinstance(val, (int, float)):
            return float(val)
    
    # Try stage metrics
    if metric_name in metrics:
        val = metrics[metric_name]
        if isinstance(val, (int, float)):
            return float(val)
    
    # Fallback: net_profit / max_dd (if both exist)
    if metric_name in ("finalscore", "net_over_mdd"):
        net_profit = candidate_metrics.get("net_profit") or metrics.get("net_profit")
        max_dd = candidate_metrics.get("max_dd") or metrics.get("max_dd")
        if net_profit is not None and max_dd is not None:
            if abs(max_dd) > 1e-10:  # Avoid division by zero
                return float(net_profit) / abs(float(max_dd))
            elif float(net_profit) > 0:
                return float("inf")  # Positive profit with zero DD
            else:
                return float("-inf")  # Negative profit with zero DD
    
    return None


def apply_rule_r1(
    candidate: Dict[str, Any],
    stage2_winners: List[Dict[str, Any]],
    is_v2: bool = False,
) -> Tuple[bool, str]:
    """
    Rule R1: Evidence completeness.
    
    If candidate appears in Stage1 winners but:
    - Cannot find corresponding Stage2 metrics (or Stage2 did not run successfully)
    -> DROP (reason: unverified)
    
    Args:
        candidate: Candidate from Stage1 winners
        stage2_winners: List of Stage2 winners
        is_v2: Whether candidates are v2 schema
        
    Returns:
        Tuple of (should_drop, reason)
    """
    # For v2: use candidate_id for matching
    if is_v2:
        candidate_id = candidate.get("candidate_id")
        if candidate_id is None:
            return True, "missing_candidate_id"
        
        # Find matching candidate by candidate_id
        for item in stage2_winners:
            if item.get("candidate_id") == candidate_id:
                return False, ""
        
        return True, "unverified"
    
    # Legacy path: use param_id
    param_id = candidate.get("param_id")
    if param_id is None:
        # Try to extract from source (v2 fallback)
        source = candidate.get("source", {})
        param_id = source.get("param_id")
        if param_id is None:
            # Try metrics (v2 fallback)
            metrics = candidate.get("metrics", {})
            param_id = metrics.get("param_id")
            if param_id is None:
                return True, "missing_param_id"
    
    stage2_match = find_stage2_candidate(param_id, stage2_winners)
    if stage2_match is None:
        return True, "unverified"
    
    return False, ""


def apply_rule_r2(
    candidate: Dict[str, Any],
    stage1_metrics: Dict[str, Any],
    stage2_candidate: Dict[str, Any],
    stage2_metrics: Dict[str, Any],
) -> Tuple[bool, str]:
    """
    Rule R2: Confirm stability.
    
    If candidate's key metrics degrade > threshold in Stage2 vs Stage1 -> DROP.
    
    Priority:
    1. finalscore or net_over_mdd
    2. Fallback: net_profit / max_dd
    
    Args:
        candidate: Candidate from Stage1 winners
        stage1_metrics: Stage1 metrics dict
        stage2_candidate: Matching Stage2 candidate
        stage2_metrics: Stage2 metrics dict
        
    Returns:
        Tuple of (should_drop, reason)
    """
    # Extract Stage1 metric
    stage1_val = extract_key_metric(
        stage1_metrics,
        candidate,
        "finalscore",
    )
    if stage1_val is None:
        stage1_val = extract_key_metric(
            stage1_metrics,
            candidate,
            "net_over_mdd",
        )
    if stage1_val is None:
        # Fallback: net_profit / max_dd
        stage1_val = extract_key_metric(
            stage1_metrics,
            candidate,
            "net_over_mdd",
        )
    
    # Extract Stage2 metric
    stage2_val = extract_key_metric(
        stage2_metrics,
        stage2_candidate,
        "finalscore",
    )
    if stage2_val is None:
        stage2_val = extract_key_metric(
            stage2_metrics,
            stage2_candidate,
            "net_over_mdd",
        )
    if stage2_val is None:
        # Fallback: net_profit / max_dd
        stage2_val = extract_key_metric(
            stage2_metrics,
            stage2_candidate,
            "net_over_mdd",
        )
    
    # If either metric is missing, cannot apply R2
    if stage1_val is None or stage2_val is None:
        return False, ""
    
    # Check degradation
    if stage1_val == 0.0:
        # Avoid division by zero
        if stage2_val < 0.0:
            return True, f"degraded_from_zero_to_negative"
        return False, ""
    
    degradation_ratio = (stage1_val - stage2_val) / abs(stage1_val)
    if degradation_ratio > R2_DEGRADE_THRESHOLD:
        return True, f"degraded_{degradation_ratio:.2%}"
    
    return False, ""


def apply_rule_r3(
    candidate: Dict[str, Any],
    all_stage1_winners: List[Dict[str, Any]],
) -> Tuple[bool, str]:
    """
    Rule R3: Plateau hint (MVP simplified version).
    
    If same strategy_id appears >= threshold times in Stage1 topk -> FREEZE.
    
    MVP version: Count occurrences of same strategy_id (simplified).
    Future: Geometric distance/clustering analysis.
    
    Args:
        candidate: Candidate from Stage1 winners
        all_stage1_winners: All Stage1 winners (for density calculation)
        
    Returns:
        Tuple of (should_freeze, reason)
    """
    strategy_id, _, _ = normalize_candidate(candidate)
    
    # Count occurrences of same strategy_id
    count = 0
    for item in all_stage1_winners:
        item_strategy_id, _, _ = normalize_candidate(item)
        if item_strategy_id == strategy_id:
            count += 1
    
    if count >= R3_DENSITY_THRESHOLD:
        return True, f"density_{count}_over_threshold_{R3_DENSITY_THRESHOLD}"
    
    return False, ""


def evaluate_governance(
    *,
    stage0_dir: Path,
    stage1_dir: Path,
    stage2_dir: Path,
) -> GovernanceReport:
    """
    Evaluate governance rules on candidates from Stage1 winners.
    
    Reads artifacts from three stage directories and applies rules:
    - R1: Evidence completeness (DROP if Stage2 missing)
    - R2: Confirm stability (DROP if metrics degrade > threshold)
    - R3: Plateau hint (FREEZE if density over threshold)
    
    Args:
        stage0_dir: Path to Stage0 run directory
        stage1_dir: Path to Stage1 run directory
        stage2_dir: Path to Stage2 run directory
        
    Returns:
        GovernanceReport with decisions for each candidate
    """
    # Read artifacts
    stage0_manifest = read_manifest(stage0_dir)
    stage0_metrics = read_metrics(stage0_dir)
    stage0_winners = read_winners(stage0_dir)
    stage0_config = read_config_snapshot(stage0_dir)
    
    stage1_manifest = read_manifest(stage1_dir)
    stage1_metrics = read_metrics(stage1_dir)
    stage1_winners = read_winners(stage1_dir)
    stage1_config = read_config_snapshot(stage1_dir)
    
    stage2_manifest = read_manifest(stage2_dir)
    stage2_metrics = read_metrics(stage2_dir)
    stage2_winners = read_winners(stage2_dir)
    stage2_config = read_config_snapshot(stage2_dir)
    
    # Extract candidates from Stage1 winners (topk)
    stage1_topk = stage1_winners.get("topk", [])
    
    # Check if winners is v2 schema
    stage1_is_v2 = is_winners_v2(stage1_winners)
    
    # Get git_sha and created_at from Stage1 manifest
    git_sha = stage1_manifest.get("git_sha", "unknown")
    created_at = datetime.now(timezone.utc).isoformat().replace("+00:00", "Z")
    
    # Build governance items
    items: List[GovernanceItem] = []
    
    for candidate in stage1_topk:
        # Normalize candidate (pass stage1_config for params extraction, and is_v2 flag)
        strategy_id, params_dict, metrics_subset = normalize_candidate(
            candidate, stage1_config, is_v2=stage1_is_v2
        )
        
        # Generate candidate_id
        candidate_id = generate_candidate_id(strategy_id, params_dict)
        
        # Apply rules
        reasons: List[str] = []
        evidence: List[EvidenceRef] = []
        decision = Decision.KEEP  # Default
        
        # R1: Evidence completeness
        # Check if Stage2 is v2 (for candidate matching)
        stage2_is_v2 = is_winners_v2(stage2_winners)
        should_drop_r1, reason_r1 = apply_rule_r1(
            candidate, stage2_winners.get("topk", []), is_v2=stage2_is_v2
        )
        if should_drop_r1:
            decision = Decision.DROP
            reasons.append(f"R1: {reason_r1}")
            # Add evidence
            evidence.append(
                EvidenceRef(
                    run_id=stage1_manifest.get("run_id", "unknown"),
                    stage_name="stage1_topk",
                    artifact_paths=["manifest.json", "metrics.json", "winners.json"],
                    key_metrics={
                        "param_id": candidate.get("param_id"),
                        **metrics_subset,
                    },
                )
            )
            # Create item and continue (no need to check R2/R3)
            items.append(
                GovernanceItem(
                    candidate_id=candidate_id,
                    decision=decision,
                    reasons=reasons,
                    evidence=evidence,
                    created_at=created_at,
                    git_sha=git_sha,
                )
            )
            continue
        
        # R2: Confirm stability
        # Find Stage2 candidate (support both v2 and legacy)
        if stage1_is_v2:
            candidate_id = candidate.get("candidate_id")
            stage2_candidate = None
            if candidate_id:
                for item in stage2_winners.get("topk", []):
                    if item.get("candidate_id") == candidate_id:
                        stage2_candidate = item
                        break
        else:
            param_id = candidate.get("param_id")
            if param_id is None:
                # Try source/metrics fallback
                source = candidate.get("source", {})
                param_id = source.get("param_id") or candidate.get("metrics", {}).get("param_id")
            stage2_candidate = find_stage2_candidate(
                param_id,
                stage2_winners.get("topk", []),
            ) if param_id is not None else None
        if stage2_candidate is not None:
            should_drop_r2, reason_r2 = apply_rule_r2(
                candidate,
                stage1_metrics,
                stage2_candidate,
                stage2_metrics,
            )
            if should_drop_r2:
                decision = Decision.DROP
                reasons.append(f"R2: {reason_r2}")
                # Add evidence
                evidence.append(
                    EvidenceRef(
                        run_id=stage1_manifest.get("run_id", "unknown"),
                        stage_name="stage1_topk",
                        artifact_paths=["manifest.json", "metrics.json", "winners.json"],
                        key_metrics={
                            "param_id": candidate.get("param_id"),
                            **metrics_subset,
                        },
                    )
                )
                evidence.append(
                    EvidenceRef(
                        run_id=stage2_manifest.get("run_id", "unknown"),
                        stage_name="stage2_confirm",
                        artifact_paths=["manifest.json", "metrics.json", "winners.json"],
                        key_metrics={
                            "param_id": stage2_candidate.get("param_id"),
                            "net_profit": stage2_candidate.get("net_profit"),
                            "trades": stage2_candidate.get("trades"),
                            "max_dd": stage2_candidate.get("max_dd"),
                        },
                    )
                )
                # Create item and continue (no need to check R3)
                items.append(
                    GovernanceItem(
                        candidate_id=candidate_id,
                        decision=decision,
                        reasons=reasons,
                        evidence=evidence,
                        created_at=created_at,
                        git_sha=git_sha,
                    )
                )
                continue
        
        # R3: Plateau hint (needs normalized strategy_id)
        should_freeze_r3, reason_r3 = apply_rule_r3(candidate, stage1_topk)
        if should_freeze_r3:
            decision = Decision.FREEZE
            reasons.append(f"R3: {reason_r3}")
        
        # Add evidence (always include Stage1 and Stage2 if available)
        evidence.append(
            EvidenceRef(
                run_id=stage1_manifest.get("run_id", "unknown"),
                stage_name="stage1_topk",
                artifact_paths=["manifest.json", "metrics.json", "winners.json", "config_snapshot.json"],
                key_metrics={
                    "param_id": candidate.get("param_id"),
                    **metrics_subset,
                    "stage_planned_subsample": stage1_metrics.get("stage_planned_subsample"),
                    "param_subsample_rate": stage1_metrics.get("param_subsample_rate"),
                    "params_effective": stage1_metrics.get("params_effective"),
                },
            )
        )
        if stage2_candidate is not None:
            evidence.append(
                EvidenceRef(
                    run_id=stage2_manifest.get("run_id", "unknown"),
                    stage_name="stage2_confirm",
                    artifact_paths=["manifest.json", "metrics.json", "winners.json", "config_snapshot.json"],
                    key_metrics={
                        "param_id": stage2_candidate.get("param_id"),
                        "net_profit": stage2_candidate.get("net_profit"),
                        "trades": stage2_candidate.get("trades"),
                        "max_dd": stage2_candidate.get("max_dd"),
                        "param_subsample_rate": stage2_metrics.get("param_subsample_rate"),
                        "params_effective": stage2_metrics.get("params_effective"),
                    },
                )
            )
        
        # Create item
        items.append(
            GovernanceItem(
                candidate_id=candidate_id,
                decision=decision,
                reasons=reasons,
                evidence=evidence,
                created_at=created_at,
                git_sha=git_sha,
            )
        )
    
    # Build metadata
    metadata = {
        "governance_id": stage1_manifest.get("run_id", "unknown"),  # Use Stage1 run_id as base
        "season": stage1_manifest.get("season", "unknown"),
        "created_at": created_at,
        "git_sha": git_sha,
        "stage0_run_id": stage0_manifest.get("run_id", "unknown"),
        "stage1_run_id": stage1_manifest.get("run_id", "unknown"),
        "stage2_run_id": stage2_manifest.get("run_id", "unknown"),
        "total_candidates": len(items),
        "decisions": {
            "KEEP": sum(1 for item in items if item.decision == Decision.KEEP),
            "FREEZE": sum(1 for item in items if item.decision == Decision.FREEZE),
            "DROP": sum(1 for item in items if item.decision == Decision.DROP),
        },
    }
    
    return GovernanceReport(items=items, metadata=metadata)


================================================================================
FILE: src/FishBroWFS_V2/pipeline/metrics_schema.py
================================================================================

from __future__ import annotations

"""
Metrics column schema (single source of truth).

Defines the column order for metrics arrays returned by run_grid().
"""

# Column indices for metrics array (n_params, 3)
METRICS_COL_NET_PROFIT = 0
METRICS_COL_TRADES = 1
METRICS_COL_MAX_DD = 2

# Column names (for documentation/debugging)
METRICS_COLUMN_NAMES = ["net_profit", "trades", "max_dd"]

# Number of columns
METRICS_N_COLUMNS = 3


================================================================================
FILE: src/FishBroWFS_V2/pipeline/param_sort.py
================================================================================

from __future__ import annotations

import numpy as np


def sort_params_cache_friendly(params: np.ndarray) -> tuple[np.ndarray, np.ndarray]:
    """
    Cache-friendly sorting for parameter matrix.

    params: shape (n, k) float64.
      Convention (Phase 3B v1):
        col0 = channel_len
        col1 = atr_len
        col2 = stop_mult

    Returns:
      sorted_params: params reordered (view/copy depending on numpy)
      order: indices such that sorted_params = params[order]
    """
    if params.ndim != 2 or params.shape[1] < 3:
        raise ValueError("params must be (n, >=3) array")

    # Primary: channel_len (int-like)
    # Secondary: atr_len (int-like)
    # Tertiary: stop_mult
    ch = params[:, 0]
    atr = params[:, 1]
    sm = params[:, 2]

    order = np.lexsort((sm, atr, ch))
    return params[order], order



================================================================================
FILE: src/FishBroWFS_V2/pipeline/runner_adapter.py
================================================================================

"""Runner adapter for funnel pipeline.

Provides unified interface to existing runners without exposing engine details.
Adapter returns data only (no file I/O) - all file writing is done by artifacts system.
"""

from __future__ import annotations

from typing import Any, Dict, List

import numpy as np

from FishBroWFS_V2.pipeline.funnel import run_funnel as run_funnel_legacy
from FishBroWFS_V2.pipeline.runner_grid import run_grid
from FishBroWFS_V2.pipeline.stage0_runner import run_stage0
from FishBroWFS_V2.pipeline.stage2_runner import run_stage2
from FishBroWFS_V2.pipeline.topk import select_topk


def _coerce_1d_float64(x):
    if isinstance(x, np.ndarray):
        return x.astype(np.float64, copy=False)
    return np.asarray(x, dtype=np.float64)


def _coerce_2d_float64(x):
    if isinstance(x, np.ndarray):
        return x.astype(np.float64, copy=False)
    return np.asarray(x, dtype=np.float64)


def _coerce_arrays(cfg: dict) -> dict:
    # in-place is ok (stage_cfg is per-stage copy anyway)
    if "open_" in cfg:
        cfg["open_"] = _coerce_1d_float64(cfg["open_"])
    if "high" in cfg:
        cfg["high"] = _coerce_1d_float64(cfg["high"])
    if "low" in cfg:
        cfg["low"] = _coerce_1d_float64(cfg["low"])
    if "close" in cfg:
        cfg["close"] = _coerce_1d_float64(cfg["close"])
    if "params_matrix" in cfg:
        cfg["params_matrix"] = _coerce_2d_float64(cfg["params_matrix"])
    return cfg


def run_stage_job(stage_cfg: dict) -> dict:
    """
    Run a stage job and return metrics and winners.
    
    This adapter wraps existing runners (run_grid, run_stage0, run_stage2)
    to provide a unified interface. It does NOT write any files - all file
    writing must be done by the artifacts system.
    
    Args:
        stage_cfg: Stage configuration dictionary containing:
            - stage_name: Stage identifier ("stage0_coarse", "stage1_topk", "stage2_confirm")
            - param_subsample_rate: Subsample rate for this stage
            - topk: Optional top-K count (for Stage0/1)
            - open_, high, low, close: OHLC arrays
            - params_matrix: Parameter matrix
            - commission, slip, order_qty: Trading parameters
            - Other stage-specific parameters
    
    Returns:
        Dictionary with:
        - metrics: dict containing performance metrics
        - winners: dict with schema {"topk": [...], "notes": {"schema": "v1", ...}}
    
    Note:
        - This function does NOT write any files
        - All file writing must be done by core/artifacts.py
        - Returns data only for artifact system to consume
    """
    stage_cfg = _coerce_arrays(stage_cfg)
    
    stage_name = stage_cfg.get("stage_name", "")
    
    if stage_name == "stage0_coarse":
        return _run_stage0_job(stage_cfg)
    elif stage_name == "stage1_topk":
        return _run_stage1_job(stage_cfg)
    elif stage_name == "stage2_confirm":
        return _run_stage2_job(stage_cfg)
    else:
        raise ValueError(f"Unknown stage_name: {stage_name}")


def _run_stage0_job(cfg: dict) -> dict:
    """Run Stage0 coarse exploration job."""
    close = cfg["close"]
    params_matrix = cfg["params_matrix"]
    proxy_name = cfg.get("proxy_name", "ma_proxy_v0")
    
    # Apply subsample if needed
    param_subsample_rate = cfg.get("param_subsample_rate", 1.0)
    if param_subsample_rate < 1.0:
        n_total = params_matrix.shape[0]
        n_effective = int(n_total * param_subsample_rate)
        # Deterministic selection (use seed from config if available)
        seed = cfg.get("subsample_seed", 42)
        rng = np.random.default_rng(seed)
        perm = rng.permutation(n_total)
        selected_indices = np.sort(perm[:n_effective])
        params_matrix = params_matrix[selected_indices]
    
    # Run Stage0
    stage0_results = run_stage0(close, params_matrix, proxy_name=proxy_name)
    
    # Extract metrics
    metrics = {
        "params_total": cfg.get("params_total", params_matrix.shape[0]),
        "params_effective": len(stage0_results),
        "bars": len(close),
        "stage_name": "stage0_coarse",
    }
    
    # Convert to winners format
    topk = cfg.get("topk", 50)
    topk_param_ids = select_topk(stage0_results, k=topk)
    
    winners = {
        "topk": [
            {
                "param_id": int(r.param_id),
                "proxy_value": float(r.proxy_value),
            }
            for r in stage0_results
            if r.param_id in topk_param_ids
        ],
        "notes": {
            "schema": "v1",
            "stage": "stage0_coarse",
            "topk_count": len(topk_param_ids),
        },
    }
    
    return {"metrics": metrics, "winners": winners}


def _run_stage1_job(cfg: dict) -> dict:
    """Run Stage1 Top-K refinement job."""
    # Stage1 uses grid runner with increased subsample
    open_ = cfg["open_"]
    high = cfg["high"]
    low = cfg["low"]
    close = cfg["close"]
    params_matrix = cfg["params_matrix"]
    commission = cfg.get("commission", 0.0)
    slip = cfg.get("slip", 0.0)
    order_qty = cfg.get("order_qty", 1)
    
    param_subsample_rate = cfg.get("param_subsample_rate", 1.0)
    
    # Apply subsample
    if param_subsample_rate < 1.0:
        n_total = params_matrix.shape[0]
        n_effective = int(n_total * param_subsample_rate)
        seed = cfg.get("subsample_seed", 42)
        rng = np.random.default_rng(seed)
        perm = rng.permutation(n_total)
        selected_indices = np.sort(perm[:n_effective])
        params_matrix = params_matrix[selected_indices]
    
    # Run grid
    result = run_grid(
        open_,
        high,
        low,
        close,
        params_matrix,
        commission=commission,
        slip=slip,
        order_qty=order_qty,
        sort_params=True,
    )
    
    metrics_array = result.get("metrics", np.array([]))
    perf = result.get("perf", {})
    
    # Extract metrics
    metrics = {
        "params_total": cfg.get("params_total", params_matrix.shape[0]),
        "params_effective": metrics_array.shape[0] if metrics_array.size > 0 else 0,
        "bars": len(close),
        "stage_name": "stage1_topk",
    }
    
    if isinstance(perf, dict):
        runtime_s = perf.get("t_total_s", 0.0)
        if runtime_s:
            metrics["runtime_s"] = float(runtime_s)
    
    # Select top-K
    topk = cfg.get("topk", 20)
    if metrics_array.size > 0:
        # Sort by net_profit (column 0)
        net_profits = metrics_array[:, 0]
        top_indices = np.argsort(net_profits)[::-1][:topk]
        
        winners_list = []
        for idx in top_indices:
            winners_list.append({
                "param_id": int(idx),
                "net_profit": float(metrics_array[idx, 0]),
                "trades": int(metrics_array[idx, 1]),
                "max_dd": float(metrics_array[idx, 2]),
            })
    else:
        winners_list = []
    
    winners = {
        "topk": winners_list,
        "notes": {
            "schema": "v1",
            "stage": "stage1_topk",
            "topk_count": len(winners_list),
        },
    }
    
    return {"metrics": metrics, "winners": winners}


def _run_stage2_job(cfg: dict) -> dict:
    """Run Stage2 full confirmation job."""
    open_ = cfg["open_"]
    high = cfg["high"]
    low = cfg["low"]
    close = cfg["close"]
    params_matrix = cfg["params_matrix"]
    commission = cfg.get("commission", 0.0)
    slip = cfg.get("slip", 0.0)
    order_qty = cfg.get("order_qty", 1)
    
    # Stage2 must use all params (subsample_rate = 1.0)
    # Get top-K from previous stage if available
    prev_winners = cfg.get("prev_stage_winners", [])
    if prev_winners:
        param_ids = [w.get("param_id") for w in prev_winners if "param_id" in w]
    else:
        # Fallback: use all params
        param_ids = list(range(params_matrix.shape[0]))
    
    # Run Stage2
    stage2_results = run_stage2(
        open_,
        high,
        low,
        close,
        params_matrix,
        param_ids,
        commission=commission,
        slip=slip,
        order_qty=order_qty,
    )
    
    # Extract metrics
    metrics = {
        "params_total": cfg.get("params_total", params_matrix.shape[0]),
        "params_effective": len(stage2_results),
        "bars": len(close),
        "stage_name": "stage2_confirm",
    }
    
    # Convert to winners format
    winners_list = []
    for r in stage2_results:
        winners_list.append({
            "param_id": int(r.param_id),
            "net_profit": float(r.net_profit),
            "trades": int(r.trades),
            "max_dd": float(r.max_dd),
        })
    
    winners = {
        "topk": winners_list,
        "notes": {
            "schema": "v1",
            "stage": "stage2_confirm",
            "full_confirm": True,
        },
    }
    
    return {"metrics": metrics, "winners": winners}


================================================================================
FILE: src/FishBroWFS_V2/pipeline/runner_grid.py
================================================================================

from __future__ import annotations

from typing import Dict, Tuple

import numpy as np
import os
import time

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.types import BarArrays, Fill, OrderIntent, OrderKind, OrderRole, Side
from FishBroWFS_V2.pipeline.metrics_schema import (
    METRICS_COL_MAX_DD,
    METRICS_COL_NET_PROFIT,
    METRICS_COL_TRADES,
    METRICS_N_COLUMNS,
)
from FishBroWFS_V2.pipeline.param_sort import sort_params_cache_friendly
from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, PrecomputedIndicators, run_kernel
from FishBroWFS_V2.indicators.numba_indicators import rolling_max, rolling_min, atr_wilder


def _max_drawdown(equity: np.ndarray) -> float:
    """
    Vectorized max drawdown on an equity curve.
    Handles empty arrays gracefully.
    """
    if equity.size == 0:
        return 0.0
    peak = np.maximum.accumulate(equity)
    dd = equity - peak
    mdd = float(np.min(dd))  # negative or 0
    return mdd


def _ensure_contiguous_bars(bars: BarArrays) -> BarArrays:
    if bars.open.flags["C_CONTIGUOUS"] and bars.high.flags["C_CONTIGUOUS"] and bars.low.flags["C_CONTIGUOUS"] and bars.close.flags["C_CONTIGUOUS"]:
        return bars
    return BarArrays(
        open=np.ascontiguousarray(bars.open, dtype=np.float64),
        high=np.ascontiguousarray(bars.high, dtype=np.float64),
        low=np.ascontiguousarray(bars.low, dtype=np.float64),
        close=np.ascontiguousarray(bars.close, dtype=np.float64),
    )


def run_grid(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
    sort_params: bool = True,
    force_close_last: bool = False,
    return_debug: bool = False,
) -> Dict[str, object]:
    """
    Phase 3B v1: Dynamic Grid Runner (homology locked).

    params_matrix: shape (n, >=3) float64
      col0 channel_len (int-like)
      col1 atr_len (int-like)
      col2 stop_mult (float)

    Args:
        force_close_last: If True, force close any open positions at the last bar
            using close[-1] as exit price. This ensures trades > 0 when fills exist.

    Returns:
      dict with:
        - metrics: np.ndarray shape (n, 3) float64 columns:
            [net_profit, trades, max_dd] (see pipeline.metrics_schema for column indices)
        - order: np.ndarray indices mapping output rows back to original params (or identity)
    """
    profile_grid = os.environ.get("FISHBRO_PROFILE_GRID", "").strip() == "1"
    profile_kernel = os.environ.get("FISHBRO_PROFILE_KERNEL", "").strip() == "1"
    
    # Stage P2-1.8: Bridge (B) - if user turns on GRID profiling, kernel timing must be enabled too.
    # This provides stable UX: grid breakdown automatically enables kernel timing.
    # Only restore if we set it ourselves, to avoid polluting external caller's environment.
    _set_kernel_profile = False
    if profile_grid and not profile_kernel:
        os.environ["FISHBRO_PROFILE_KERNEL"] = "1"
        _set_kernel_profile = True
    
    # Treat either flag as "profile mode" for grid aggregation.
    profile = profile_grid or profile_kernel
    
    sim_only = os.environ.get("FISHBRO_PERF_SIM_ONLY", "").strip() == "1"
    t0 = time.perf_counter()

    bars = _ensure_contiguous_bars(normalize_bars(open_, high, low, close))
    t_prep1 = time.perf_counter()

    if params_matrix.ndim != 2 or params_matrix.shape[1] < 3:
        raise ValueError("params_matrix must be (n, >=3)")

    from FishBroWFS_V2.config.dtypes import INDEX_DTYPE
    from FishBroWFS_V2.config.dtypes import PRICE_DTYPE_STAGE2
    
    # runner_grid is used in Stage2, so keep float64 for params_matrix (conservative)
    pm = np.asarray(params_matrix, dtype=PRICE_DTYPE_STAGE2)
    if sort_params:
        pm_sorted, order = sort_params_cache_friendly(pm)
        # Convert order to INDEX_DTYPE (int32) for memory optimization
        order = order.astype(INDEX_DTYPE)
    else:
        pm_sorted = pm
        order = np.arange(pm.shape[0], dtype=INDEX_DTYPE)
    t_sort = time.perf_counter()

    n = pm_sorted.shape[0]
    metrics = np.zeros((n, METRICS_N_COLUMNS), dtype=np.float64)
    
    # Debug arrays: per-param first trade snapshot (only if return_debug=True)
    if return_debug:
        debug_fills_first = np.full((n, 6), np.nan, dtype=np.float64)
        # Columns: entry_bar, entry_price, exit_bar, exit_price, net_profit, trades
    else:
        debug_fills_first = None

    # Initialize result dict early (minimal structure)
    perf: Dict[str, object] = {}
    
    # Stage P2-2 Step A: Memoization potential assessment - unique counts
    # Extract channel_len and atr_len values (as int32 for unique counting)
    ch_vals = pm_sorted[:, 0].astype(np.int32, copy=False)
    atr_vals = pm_sorted[:, 1].astype(np.int32, copy=False)
    
    perf["unique_channel_len_count"] = int(np.unique(ch_vals).size)
    perf["unique_atr_len_count"] = int(np.unique(atr_vals).size)
    
    # Pack pair to int64 key: (ch<<32) | atr
    pair_keys = (ch_vals.astype(np.int64) << 32) | (atr_vals.astype(np.int64) & 0xFFFFFFFF)
    perf["unique_ch_atr_pair_count"] = int(np.unique(pair_keys).size)
    
    # Stage P2-2 Step B3: Pre-compute indicators for unique channel_len and atr_len
    unique_ch = np.unique(ch_vals)
    unique_atr = np.unique(atr_vals)
    
    # Build caches for precomputed indicators
    donch_cache_hi: Dict[int, np.ndarray] = {}
    donch_cache_lo: Dict[int, np.ndarray] = {}
    atr_cache: Dict[int, np.ndarray] = {}
    
    # Pre-compute timing (if profiling enabled)
    t_precompute_start = time.perf_counter() if profile else 0.0
    
    # Pre-compute Donchian indicators for unique channel_len values
    for ch_len in unique_ch:
        ch_len_int = int(ch_len)
        donch_cache_hi[ch_len_int] = rolling_max(bars.high, ch_len_int)
        donch_cache_lo[ch_len_int] = rolling_min(bars.low, ch_len_int)
    
    # Pre-compute ATR indicators for unique atr_len values
    for atr_len in unique_atr:
        atr_len_int = int(atr_len)
        atr_cache[atr_len_int] = atr_wilder(bars.high, bars.low, bars.close, atr_len_int)
    
    t_precompute_end = time.perf_counter() if profile else 0.0
    
    # Stage P2-2 Step B4: Memory observation fields
    precomp_bytes_donchian = sum(arr.nbytes for arr in donch_cache_hi.values()) + sum(arr.nbytes for arr in donch_cache_lo.values())
    precomp_bytes_atr = sum(arr.nbytes for arr in atr_cache.values())
    precomp_bytes_total = precomp_bytes_donchian + precomp_bytes_atr
    
    perf["precomp_unique_channel_len_count"] = int(len(unique_ch))
    perf["precomp_unique_atr_len_count"] = int(len(unique_atr))
    perf["precomp_bytes_donchian"] = int(precomp_bytes_donchian)
    perf["precomp_bytes_atr"] = int(precomp_bytes_atr)
    perf["precomp_bytes_total"] = int(precomp_bytes_total)
    if profile:
        perf["t_precompute_indicators_s"] = float(t_precompute_end - t_precompute_start)
    
    # CURSOR TASK 3: Grid å±¤æŠŠ intent sparse å‚³åˆ°åº•
    # Read FISHBRO_PERF_TRIGGER_RATE as intent_sparse_rate and pass to kernel
    intent_sparse_rate_env = os.environ.get("FISHBRO_PERF_TRIGGER_RATE", "").strip()
    intent_sparse_rate = 1.0
    if intent_sparse_rate_env:
        try:
            intent_sparse_rate = float(intent_sparse_rate_env)
            if not (0.0 <= intent_sparse_rate <= 1.0):
                intent_sparse_rate = 1.0
        except ValueError:
            intent_sparse_rate = 1.0
    
    # Stage P2-3: Param-subsample (deterministic selection)
    # FISHBRO_PERF_PARAM_SUBSAMPLE_RATE controls param subsampling (separate from trigger_rate)
    # FISHBRO_PERF_TRIGGER_RATE is for bar/intent-level sparsity (handled in kernel)
    param_subsample_rate_env = os.environ.get("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", "").strip()
    param_subsample_seed_env = os.environ.get("FISHBRO_PERF_PARAM_SUBSAMPLE_SEED", "").strip()
    
    param_subsample_rate = 1.0
    if param_subsample_rate_env:
        try:
            param_subsample_rate = float(param_subsample_rate_env)
            if not (0.0 <= param_subsample_rate <= 1.0):
                param_subsample_rate = 1.0
        except ValueError:
            param_subsample_rate = 1.0
    
    param_subsample_seed = 42
    if param_subsample_seed_env:
        try:
            param_subsample_seed = int(param_subsample_seed_env)
        except ValueError:
            param_subsample_seed = 42
    
    # Stage P2-3: Determine selected params (deterministic)
    # CURSOR TASK 1: Use "pos" (sorted space position) for selection, "orig" (original index) for scatter-back
    if param_subsample_rate < 1.0:
        k = max(1, int(round(n * param_subsample_rate)))
        rng = np.random.default_rng(param_subsample_seed)
        # Generate deterministic permutation
        perm = rng.permutation(n)
        selected_pos = np.sort(perm[:k]).astype(INDEX_DTYPE)  # Sort to maintain deterministic loop order
    else:
        selected_pos = np.arange(n, dtype=INDEX_DTYPE)
    
    # CURSOR TASK 1: Map selected_pos (sorted space) to selected_orig (original space)
    selected_orig = order[selected_pos].astype(np.int64)  # Map sorted positions to original indices
    
    selected_params_count = len(selected_pos)
    selected_params_ratio = float(selected_params_count) / float(n) if n > 0 else 0.0
    
    # Create metrics_computed_mask: boolean array indicating which rows were computed
    metrics_computed_mask = np.zeros(n, dtype=bool)
    for orig_i in selected_orig:
        metrics_computed_mask[orig_i] = True
    
    # Add param subsample info to perf
    perf["param_subsample_rate_configured"] = float(param_subsample_rate)
    perf["selected_params_count"] = int(selected_params_count)
    perf["selected_params_ratio"] = float(selected_params_ratio)
    perf["metrics_rows_computed"] = int(selected_params_count)
    perf["metrics_computed_mask"] = metrics_computed_mask.tolist()  # Convert to list for JSON serialization
    
    # Stage P2-1.8: Initialize granular timing and count accumulators (only if profile enabled)
    if profile:
        # Stage P2-2 Step A: Micro-profiling timing keys
        perf["t_ind_donchian_s"] = 0.0
        perf["t_ind_atr_s"] = 0.0
        perf["t_build_entry_intents_s"] = 0.0
        perf["t_simulate_entry_s"] = 0.0
        perf["t_calc_exits_s"] = 0.0
        perf["t_simulate_exit_s"] = 0.0
        perf["t_total_kernel_s"] = 0.0
        perf["entry_fills_total"] = 0
        perf["exit_intents_total"] = 0
        perf["exit_fills_total"] = 0
    result: Dict[str, object] = {"metrics": metrics, "order": order, "perf": perf}

    if sim_only:
        # Debug mode: bypass strategy/orchestration and only benchmark matcher simulate.
        # This provides A/B evidence: if sim-only is fast, bottleneck is in kernel (indicators/intents).
        from FishBroWFS_V2.engine import engine_jit

        intents_per_bar = int(os.environ.get("FISHBRO_SIM_ONLY_INTENTS_PER_BAR", "2"))
        intents: list[OrderIntent] = []
        oid = 1
        nbars = int(bars.open.shape[0])
        for t in range(1, nbars):
            for _ in range(intents_per_bar):
                intents.append(
                    OrderIntent(
                        order_id=oid,
                        created_bar=t - 1,
                        role=OrderRole.ENTRY,
                        kind=OrderKind.STOP,
                        side=Side.BUY,
                        price=float(bars.high[t - 1]),
                        qty=1,
                    )
                )
                oid += 1
                intents.append(
                    OrderIntent(
                        order_id=oid,
                        created_bar=t - 1,
                        role=OrderRole.EXIT,
                        kind=OrderKind.STOP,
                        side=Side.SELL,
                        price=float(bars.low[t - 1]),
                        qty=1,
                    )
                )
                oid += 1

        t_sim0 = time.perf_counter()
        _fills = engine_jit.simulate(bars, intents)
        t_sim1 = time.perf_counter()
        jt = engine_jit.get_jit_truth()
        numba_env = os.environ.get("NUMBA_DISABLE_JIT", "")
        sigs = jt.get("kernel_signatures") or []
        perf = {
            "t_features": float(t_prep1 - t0),
            "t_indicators": None,
            "t_intent_gen": None,
            "t_simulate": float(t_sim1 - t_sim0),
            "simulate_impl": "jit" if jt.get("jit_path_used") else "py",
            "jit_path_used": bool(jt.get("jit_path_used")),
            "simulate_signatures_count": int(len(sigs)),
            "numba_disable_jit_env": str(numba_env),
            "intents_total": int(len(intents)),
            "intents_per_bar_avg": float(len(intents) / float(max(1, bars.open.shape[0]))),
            "fills_total": int(len(_fills)),
            "intent_mode": "objects",
        }
        result["perf"] = perf
        if return_debug and debug_fills_first is not None:
            result["debug_fills_first"] = debug_fills_first
        return result

    # Homology: only call run_kernel, never compute strategy/metrics here.
    # Perf observability is env-gated so default usage stays unchanged.
    t_ind = 0.0
    t_intgen = 0.0
    t_sim = 0.0
    intents_total = 0
    fills_total = 0
    any_profile_missing = False
    intent_mode: str | None = None
    # Stage P2-1.5: Entry sparse observability (accumulate across params)
    entry_valid_mask_sum = 0
    entry_intents_total = 0
    n_bars_for_entry_obs = None  # Will be set from first kernel result
    # Stage P2-3: Sparse builder observability (accumulate across params)
    allowed_bars_total = 0  # Total allowed bars (before trigger rate filtering)
    intents_generated_total = 0  # Total intents generated (after trigger rate filtering)
    
    # CURSOR TASK 1: Collect metrics_subset (will be scattered back after loop)
    metrics_subset = np.zeros((len(selected_pos), METRICS_N_COLUMNS), dtype=np.float64)
    debug_fills_first_subset = None
    if return_debug:
        debug_fills_first_subset = np.full((len(selected_pos), 6), np.nan, dtype=np.float64)
    
    # Stage P2-3: Only loop selected params (param-subsample)
    # CURSOR TASK 1: Use selected_pos (sorted space) to access pm_sorted, selected_orig for scatter-back
    for subset_idx, pos in enumerate(selected_pos):
        # Initialize row for this iteration (will be written at loop end regardless of any continue/early exit)
        row = np.array([0.0, 0, 0.0], dtype=np.float64)
        
        # CURSOR TASK 1: Use pos (sorted space position) to access params_sorted
        ch = int(pm_sorted[pos, 0])
        atr = int(pm_sorted[pos, 1])
        sm = float(pm_sorted[pos, 2])

        # Stage P2-2 Step B3: Lookup precomputed indicators and create PrecomputedIndicators pack
        precomp_pack = PrecomputedIndicators(
            donch_hi=donch_cache_hi[ch],
            donch_lo=donch_cache_lo[ch],
            atr=atr_cache[atr],
        )

        # Stage P2-1.8: Kernel profiling is already enabled at function start if profile=True
        # No need to set FISHBRO_PROFILE_KERNEL here again
        out = run_kernel(
            bars,
            DonchianAtrParams(channel_len=ch, atr_len=atr, stop_mult=sm),
            commission=float(commission),
            slip=float(slip),
            order_qty=int(order_qty),
            return_debug=return_debug,
            precomp=precomp_pack,
            intent_sparse_rate=intent_sparse_rate,  # CURSOR TASK 3: Pass intent sparse rate
        )
        obs = out.get("_obs", None)  # type: ignore
        if isinstance(obs, dict):
            # Phase 3.0-B: Trust kernel's evidence fields, do not recompute
            if intent_mode is None and isinstance(obs.get("intent_mode"), str):
                intent_mode = str(obs.get("intent_mode"))
            # Use intents_total directly from kernel (Source of Truth), not recompute from entry+exit
            intents_total += int(obs.get("intents_total", 0))
            fills_total += int(obs.get("fills_total", 0))
            
            # CURSOR TASK 2: Accumulate entry_valid_mask_sum (after intent sparse)
            # entry_valid_mask_sum must be sum(allow_mask) - not dense valid bars, not multiplied by params
            if "entry_valid_mask_sum" in obs:
                entry_valid_mask_sum += int(obs.get("entry_valid_mask_sum", 0))
            elif "allowed_bars" in obs:
                # Fallback: use allowed_bars if entry_valid_mask_sum not present
                entry_valid_mask_sum += int(obs.get("allowed_bars", 0))
            # CURSOR TASK 2: entry_intents_total should come from obs["entry_intents_total"] (set by kernel)
            if "entry_intents_total" in obs:
                entry_intents_total += int(obs.get("entry_intents_total", 0))
            elif "entry_intents" in obs:
                # Fallback: use entry_intents if entry_intents_total not present
                entry_intents_total += int(obs.get("entry_intents", 0))
            elif "n_entry" in obs:
                # Fallback: use n_entry if entry_intents_total not present
                entry_intents_total += int(obs.get("n_entry", 0))
            # Capture n_bars from first kernel result (should be same for all params)
            if n_bars_for_entry_obs is None and "n_bars" in obs:
                n_bars_for_entry_obs = int(obs.get("n_bars", 0))
            
            # Stage P2-3: Accumulate sparse builder observability (from new builder_sparse)
            if "allowed_bars" in obs:
                allowed_bars_total += int(obs.get("allowed_bars", 0))
            if "intents_generated" in obs:
                intents_generated_total += int(obs.get("intents_generated", 0))
            elif "n_entry" in obs:
                # Fallback: if intents_generated not present, use n_entry
                intents_generated_total += int(obs.get("n_entry", 0))
            
            # Stage P2-1.8: Accumulate timing keys from _obs (timing is now in _obs, not _perf)
            # Timing keys have pattern: t_*_s
            for key, value in obs.items():
                if key.startswith("t_") and key.endswith("_s"):
                    if key not in perf:
                        perf[key] = 0.0
                    perf[key] = float(perf[key]) + float(value)
            
            # Stage P2-1.8: Accumulate downstream counts from _obs
            if "entry_fills_total" in obs:
                perf["entry_fills_total"] = int(perf.get("entry_fills_total", 0)) + int(obs.get("entry_fills_total", 0))
            if "exit_intents_total" in obs:
                perf["exit_intents_total"] = int(perf.get("exit_intents_total", 0)) + int(obs.get("exit_intents_total", 0))
            if "exit_fills_total" in obs:
                perf["exit_fills_total"] = int(perf.get("exit_fills_total", 0)) + int(obs.get("exit_fills_total", 0))
        
        # Stage P2-1.8: Fallback - also check _perf for backward compatibility
        # Handle cases where old kernel versions put timing in _perf instead of _obs
        # Only use fallback if _obs doesn't have timing keys
        obs_has_timing = isinstance(obs, dict) and any(k.startswith("t_") and k.endswith("_s") for k in obs.keys())
        if not obs_has_timing:
            kernel_perf = out.get("_perf", None)
            if isinstance(kernel_perf, dict):
                # Accumulate timings across params (for grid-level aggregation)
                # Note: For grid-level, we sum timings across params
                for key, value in kernel_perf.items():
                    if key.startswith("t_") and key.endswith("_s"):
                        if key not in perf:
                            perf[key] = 0.0
                        perf[key] = float(perf[key]) + float(value)

        # Get metrics from kernel output (always available, even if profile missing)
        m = out.get("metrics", {})
        if not isinstance(m, dict):
            # Fallback: kernel didn't return metrics dict, use zeros
            m_net_profit = 0.0
            m_trades = 0
            m_max_dd = 0.0
        else:
            m_net_profit = float(m.get("net_profit", 0.0))
            m_trades = int(m.get("trades", 0))
            m_max_dd = float(m.get("max_dd", 0.0))
            # Clean NaN/Inf at source
            m_net_profit = float(np.nan_to_num(m_net_profit, nan=0.0, posinf=0.0, neginf=0.0))
            m_max_dd = float(np.nan_to_num(m_max_dd, nan=0.0, posinf=0.0, neginf=0.0))
        
        # Get fills count for debug assert
        fills_this_param = out.get("fills", [])
        fills_count_this_param = len(fills_this_param) if isinstance(fills_this_param, list) else 0
        
        # Collect debug data if requested
        if return_debug:
            debug_info = out.get("_debug", {})
            entry_bar = debug_info.get("entry_bar", -1)
            entry_price = debug_info.get("entry_price", np.nan)
            exit_bar = debug_info.get("exit_bar", -1)
            exit_price = debug_info.get("exit_price", np.nan)
        
        # Handle force_close_last: if still in position, force close at last bar
        if force_close_last:
            fills = out.get("fills", [])
            if isinstance(fills, list) and len(fills) > 0:
                # Count entry and exit fills
                entry_fills = [f for f in fills if f.role == OrderRole.ENTRY and f.side == Side.BUY]
                exit_fills = [f for f in fills if f.role == OrderRole.EXIT and f.side == Side.SELL]
                
                # If there are unpaired entries, force close at last bar
                if len(entry_fills) > len(exit_fills):
                    n_unpaired = len(entry_fills) - len(exit_fills)
                    last_bar_idx = int(bars.open.shape[0] - 1)
                    last_close_price = float(bars.close[last_bar_idx])
                    
                    # Create forced exit fills for unpaired entries
                    # Use entry prices from the unpaired entries
                    unpaired_entry_prices = [float(f.price) for f in entry_fills[-n_unpaired:]]
                    
                    # Calculate additional pnl from forced closes
                    forced_pnl = []
                    costs_per_trade = (float(commission) + float(slip)) * 2.0
                    for entry_price in unpaired_entry_prices:
                        # PnL = (exit_price - entry_price) * qty - costs
                        trade_pnl = (last_close_price - entry_price) * float(order_qty) - costs_per_trade
                        forced_pnl.append(trade_pnl)
                    
                    # Update metrics with forced closes
                    original_net_profit = m_net_profit
                    original_trades = m_trades
                    
                    # Add forced close trades
                    new_net_profit = original_net_profit + sum(forced_pnl)
                    new_trades = original_trades + n_unpaired
                    
                    # Update debug exit info for force_close_last
                    if return_debug and n_unpaired > 0:
                        exit_bar = last_bar_idx
                        exit_price = last_close_price
                    
                    # Recalculate equity and max_dd
                    forced_pnl_arr = np.asarray(forced_pnl, dtype=np.float64)
                    if original_trades > 0 and "equity" in out:
                        original_equity = out["equity"]
                        if isinstance(original_equity, np.ndarray) and original_equity.size > 0:
                            # Append forced pnl to existing equity curve
                            # Start from last equity value
                            start_equity = float(original_equity[-1])
                            forced_equity = np.cumsum(forced_pnl_arr) + start_equity
                            new_equity = np.concatenate([original_equity, forced_equity])
                        else:
                            # No previous equity array, start from 0
                            new_equity = np.cumsum(forced_pnl_arr)
                    else:
                        # No previous trades, start from 0
                        new_equity = np.cumsum(forced_pnl_arr)
                    
                    new_max_dd = _max_drawdown(new_equity)
                    
                    # Update row with forced close metrics
                    row = np.array([new_net_profit, new_trades, new_max_dd], dtype=np.float64)
                    
                    # Update debug subset with final metrics after force_close_last
                    if return_debug:
                        debug_fills_first_subset[subset_idx, 0] = entry_bar
                        debug_fills_first_subset[subset_idx, 1] = entry_price
                        debug_fills_first_subset[subset_idx, 2] = exit_bar
                        debug_fills_first_subset[subset_idx, 3] = exit_price
                        debug_fills_first_subset[subset_idx, 4] = new_net_profit
                        debug_fills_first_subset[subset_idx, 5] = float(new_trades)
                else:
                    # No unpaired entries, use original metrics
                    row = np.array([m_net_profit, m_trades, m_max_dd], dtype=np.float64)
                    
                    # Store debug data in subset
                    if return_debug:
                        debug_fills_first_subset[subset_idx, 0] = entry_bar
                        debug_fills_first_subset[subset_idx, 1] = entry_price
                        debug_fills_first_subset[subset_idx, 2] = exit_bar
                        debug_fills_first_subset[subset_idx, 3] = exit_price
                        debug_fills_first_subset[subset_idx, 4] = m_net_profit
                        debug_fills_first_subset[subset_idx, 5] = float(m_trades)
            else:
                # No fills, use original metrics
                row = np.array([m_net_profit, m_trades, m_max_dd], dtype=np.float64)
                
                # Store debug data in subset (no fills case)
                if return_debug:
                    debug_fills_first_subset[subset_idx, 0] = entry_bar
                    debug_fills_first_subset[subset_idx, 1] = entry_price
                    debug_fills_first_subset[subset_idx, 2] = exit_bar
                    debug_fills_first_subset[subset_idx, 3] = exit_price
                    debug_fills_first_subset[subset_idx, 4] = m_net_profit
                    debug_fills_first_subset[subset_idx, 5] = float(m_trades)
        else:
            # Zero-trade safe: kernel guarantees valid numbers (0.0/0)
            row = np.array([m_net_profit, m_trades, m_max_dd], dtype=np.float64)
            
            # Store debug data in subset
            if return_debug:
                debug_fills_first_subset[subset_idx, 0] = entry_bar
                debug_fills_first_subset[subset_idx, 1] = entry_price
                debug_fills_first_subset[subset_idx, 2] = exit_bar
                debug_fills_first_subset[subset_idx, 3] = exit_price
                debug_fills_first_subset[subset_idx, 4] = m_net_profit
                debug_fills_first_subset[subset_idx, 5] = float(m_trades)
        
        # HARD CONTRACT: Always write metrics_subset at loop end, regardless of any continue/early exit
        metrics_subset[subset_idx, :] = row
        
        # Debug assert: if trades > 0 (completed trades), metrics must be non-zero
        # Note: entry fills without exits yield trades=0 and all-zero metrics, which is valid
        if os.environ.get("FISHBRO_DEBUG_ASSERT", "").strip() == "1":
            if m_trades > 0:
                assert np.any(np.abs(metrics_subset[subset_idx, :]) > 0), (
                    f"subset_idx={subset_idx}: trades={m_trades} > 0, "
                    f"but metrics_subset[{subset_idx}, :]={metrics_subset[subset_idx, :]} is all zeros"
                )
        
        # Handle profile timing accumulation (after metrics written)
        if profile:
            kp = out.get("_profile", None)  # type: ignore
            if not isinstance(kp, dict):
                any_profile_missing = True
                # Continue after metrics already written
                continue
            t_ind += float(kp.get("indicators_s", 0.0))
            # include both entry+exit intent generation as "intent generation"
            t_intgen += float(kp.get("intent_gen_s", 0.0)) + float(kp.get("exit_intent_gen_s", 0.0))
            t_sim += float(kp.get("simulate_entry_s", 0.0)) + float(kp.get("simulate_exit_s", 0.0))
    
    # CURSOR TASK 2: Handle NaN before scatter-back (avoid computed_non_zero being eaten by NaN)
    # Note: Already handled at source (m_net_profit, m_max_dd), but double-check here for safety
    metrics_subset = np.nan_to_num(metrics_subset, nan=0.0, posinf=0.0, neginf=0.0)
    
    # CURSOR TASK 3: Assert that if fills_total > 0, metrics_subset should have non-zero values
    # This helps catch cases where metrics computation was skipped or returned zeros
    # Only assert if FISHBRO_DEBUG_ASSERT=1 (not triggered by profile, as tests often enable profile)
    if os.environ.get("FISHBRO_DEBUG_ASSERT", "").strip() == "1":
        metrics_subset_abs_sum = float(np.sum(np.abs(metrics_subset)))
        assert fills_total == 0 or metrics_subset_abs_sum > 0, (
            f"CURSOR TASK B violation: fills_total={fills_total} > 0 but metrics_subset_abs_sum={metrics_subset_abs_sum} == 0. "
            f"This indicates metrics computation was skipped or returned zeros."
        )
    
    # CURSOR TASK 3: Add perf debug field (metrics_subset_nonzero_rows)
    metrics_subset_nonzero_rows = int(np.sum(np.any(np.abs(metrics_subset) > 1e-10, axis=1)))
    perf["metrics_subset_nonzero_rows"] = metrics_subset_nonzero_rows
    
    # === HARD CONTRACT: scatter metrics back to original param space ===
    # CRITICAL: This must happen after all metrics computation and before any return
    # Variables: selected_pos (sorted-space index), order (sorted_pos -> original_index), metrics_subset (computed metrics)
    # For each selected param: metrics[orig_param_idx] must be written with non-zero values
    for subset_i, pos in enumerate(selected_pos):
        orig_i = int(order[int(pos)])
        metrics[orig_i, :] = metrics_subset[subset_i, :]
        
        if return_debug and debug_fills_first is not None and debug_fills_first_subset is not None:
            debug_fills_first[orig_i, :] = debug_fills_first_subset[subset_i, :]
    
    # CRITICAL: After scatter-back, metrics must not be modified (no metrics = np.zeros, no metrics[:] = 0, no result["metrics"] = metrics_subset)
    
    # CURSOR TASK 2: Add perf debug fields (for diagnostic)
    perf["intent_sparse_rate_effective"] = float(intent_sparse_rate)
    perf["fills_total"] = int(fills_total)
    perf["metrics_subset_abs_sum"] = float(np.sum(np.abs(metrics_subset)))
    
    # CURSOR TASK A: Add entry_intents_total (subsample run) for diagnostic
    # This helps distinguish: entry_intents_total > 0 but fills_total == 0 â†’ matcher/engine issue
    # vs entry_intents_total == 0 â†’ builder didn't generate intents
    perf["entry_intents_total"] = int(entry_intents_total)

    # Phase 3.0-E: Ensure intent_mode is never None
    # If no kernel results (n == 0), default to "arrays" (default kernel path)
    # Otherwise, intent_mode should have been set from first kernel result
    if intent_mode is None:
        # Edge case: n == 0 (no params) - use default "arrays" since run_kernel defaults to array path
        intent_mode = "arrays"

    if not profile:
        # Return minimal perf with evidence fields only
        # Stage P2-1.8: Preserve accumulated timings (already in perf dict from loop)
        perf["intent_mode"] = intent_mode
        perf["intents_total"] = int(intents_total)
        # fills_total already set in scatter-back section (line 592), but ensure it's here too for clarity
        if "fills_total" not in perf:
            perf["fills_total"] = int(fills_total)
        # CURSOR TASK 3: Add intent sparse rate and entry observability to perf
        perf["intent_sparse_rate"] = float(intent_sparse_rate)
        perf["entry_valid_mask_sum"] = int(entry_valid_mask_sum)  # CURSOR TASK 2: After intent sparse (sum(allow_mask))
        perf["entry_intents_total"] = int(entry_intents_total)
        
        # Stage P2-1.5: Add entry sparse observability (always include, even if 0)
        perf["intents_total_reported"] = int(intents_total)  # Preserve original for comparison
        if n_bars_for_entry_obs is not None and n_bars_for_entry_obs > 0:
            perf["entry_intents_per_bar_avg"] = float(entry_intents_total / n_bars_for_entry_obs)
        else:
            # Fallback: use bars.open.shape[0] if n_bars_for_entry_obs not available
            perf["entry_intents_per_bar_avg"] = float(entry_intents_total / max(1, bars.open.shape[0]))
        
        # Stage P2-3: Add sparse builder observability (for scaling verification)
        perf["allowed_bars"] = int(allowed_bars_total)
        perf["intents_generated"] = int(intents_generated_total)
        perf["selected_params"] = int(selected_params_count)
        
        # CURSOR TASK 2: Ensure debug fields are present in non-profile branch too
        if "intent_sparse_rate_effective" not in perf:
            perf["intent_sparse_rate_effective"] = float(intent_sparse_rate)
        if "fills_total" not in perf:
            perf["fills_total"] = int(fills_total)
        if "metrics_subset_abs_sum" not in perf:
            perf["metrics_subset_abs_sum"] = float(np.sum(np.abs(metrics_subset)))
        
        result["perf"] = perf
        if return_debug and debug_fills_first is not None:
            result["debug_fills_first"] = debug_fills_first
        return result

    from FishBroWFS_V2.engine import engine_jit

    jt = engine_jit.get_jit_truth()
    numba_env = os.environ.get("NUMBA_DISABLE_JIT", "")
    sigs = jt.get("kernel_signatures") or []

    # Best-effort: avoid leaking this env to callers
    # Only clean up if we set it ourselves (Task A: bridge logic)
    if _set_kernel_profile:
        try:
            del os.environ["FISHBRO_PROFILE_KERNEL"]
        except KeyError:
            pass

    # Phase 3.0-E: Ensure intent_mode is never None
    # If no kernel results (n == 0), default to "arrays" (default kernel path)
    # Otherwise, intent_mode should have been set from first kernel result
    if intent_mode is None:
        # Edge case: n == 0 (no params) - use default "arrays" since run_kernel defaults to array path
        intent_mode = "arrays"

    # Stage P2-1.8: Create summary dict and merge into accumulated perf (preserve t_*_s from loop)
    perf_summary = {
        "t_features": float(t_prep1 - t0),
        # current architecture: indicators are computed inside run_kernel per param
        "t_indicators": None if any_profile_missing else float(t_ind),
        "t_intent_gen": None if any_profile_missing else float(t_intgen),
        "t_simulate": None if any_profile_missing else float(t_sim),
        "simulate_impl": "jit" if jt.get("jit_path_used") else "py",
        "jit_path_used": bool(jt.get("jit_path_used")),
        "simulate_signatures_count": int(len(sigs)),
        "numba_disable_jit_env": str(numba_env),
        # Phase 3.0-B: Use kernel's evidence fields directly (Source of Truth), not recomputed
        "intent_mode": intent_mode,
        "intents_total": int(intents_total),
        "fills_total": int(fills_total),
        "intents_per_bar_avg": float(intents_total / float(max(1, bars.open.shape[0]))),
    }
    
    # CURSOR TASK 3: Add intent sparse rate and entry observability to perf
    perf_summary["intent_sparse_rate"] = float(intent_sparse_rate)
    perf_summary["entry_valid_mask_sum"] = int(entry_valid_mask_sum)  # CURSOR TASK 2: After intent sparse
    perf_summary["entry_intents_total"] = int(entry_intents_total)
    
    # Stage P2-1.5: Add entry sparse observability and preserve original intents_total
    perf_summary["intents_total_reported"] = int(intents_total)  # Preserve original for comparison
    if n_bars_for_entry_obs is not None and n_bars_for_entry_obs > 0:
        perf_summary["entry_intents_per_bar_avg"] = float(entry_intents_total / n_bars_for_entry_obs)
    else:
        # Fallback: use bars.open.shape[0] if n_bars_for_entry_obs not available
        perf_summary["entry_intents_per_bar_avg"] = float(entry_intents_total / max(1, bars.open.shape[0]))
    
    # Stage P2-3: Add sparse builder observability (for scaling verification)
    perf_summary["allowed_bars"] = int(allowed_bars_total)  # Total allowed bars across all params
    perf_summary["intents_generated"] = int(intents_generated_total)  # Total intents generated across all params
    perf_summary["selected_params"] = int(selected_params_count)  # Number of params actually computed
    
    # CURSOR TASK 2: Ensure debug fields are present in profile branch too
    perf_summary["intent_sparse_rate_effective"] = float(intent_sparse_rate)
    perf_summary["fills_total"] = int(fills_total)
    perf_summary["metrics_subset_abs_sum"] = float(np.sum(np.abs(metrics_subset)))
    
    # Keep accumulated per-kernel timings already stored in `perf` (t_*_s, entry_fills_total, etc.)
    perf.update(perf_summary)

    result["perf"] = perf
    if return_debug and debug_fills_first is not None:
        result["debug_fills_first"] = debug_fills_first
    return result



================================================================================
FILE: src/FishBroWFS_V2/pipeline/stage0_runner.py
================================================================================

"""Stage0 runner - proxy ranking without PnL metrics.

Stage0 is a fast proxy filter that ranks parameters without running full backtests.
It MUST NOT compute any PnL-related metrics (Net/MDD/SQN/Sharpe/WinRate/Equity/DD).
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import List, Optional

import numpy as np

from FishBroWFS_V2.config.constants import STAGE0_PROXY_NAME
from FishBroWFS_V2.stage0.ma_proxy import stage0_score_ma_proxy


@dataclass(frozen=True)
class Stage0Result:
    """
    Stage0 result - proxy ranking only.
    
    Contains ONLY:
    - param_id: parameter index
    - proxy_value: proxy ranking value (higher is better)
    - warmup_ok: optional warmup validation flag
    - meta: optional metadata dict
    
    FORBIDDEN fields (must not exist):
    - Any PnL metrics: Net, MDD, SQN, Sharpe, WinRate, Equity, DD, etc.
    """
    param_id: int
    proxy_value: float
    warmup_ok: Optional[bool] = None
    meta: Optional[dict] = None


def run_stage0(
    close: np.ndarray,
    params_matrix: np.ndarray,
    *,
    proxy_name: str = STAGE0_PROXY_NAME,
) -> List[Stage0Result]:
    """
    Run Stage0 proxy ranking.
    
    Args:
        close: float32 or float64 1D array (n_bars,) - close prices (will use float32 internally)
        params_matrix: float32 or float64 2D array (n_params, >=2) (will use float32 internally)
            - col0: fast_len (for MA proxy)
            - col1: slow_len (for MA proxy)
            - additional columns allowed and ignored
        proxy_name: name of proxy to use (default: ma_proxy_v0)
        
    Returns:
        List of Stage0Result, one per parameter set.
        Results are in same order as params_matrix rows.
        
    Note:
        - This function MUST NOT compute any PnL metrics
        - Only proxy_value is computed for ranking purposes
        - Uses float32 internally for memory optimization
    """
    if proxy_name != "ma_proxy_v0":
        raise ValueError(f"Unsupported proxy: {proxy_name}. Only 'ma_proxy_v0' is supported in Phase 4.")
    
    # Compute proxy scores
    scores = stage0_score_ma_proxy(close, params_matrix)
    
    # Build results
    n_params = params_matrix.shape[0]
    results: List[Stage0Result] = []
    
    for i in range(n_params):
        score = float(scores[i])
        
        # Check warmup: if score is -inf, warmup failed
        warmup_ok = not np.isinf(score) if not np.isnan(score) else False
        
        results.append(
            Stage0Result(
                param_id=i,
                proxy_value=score,
                warmup_ok=warmup_ok,
                meta=None,
            )
        )
    
    return results


================================================================================
FILE: src/FishBroWFS_V2/pipeline/stage2_runner.py
================================================================================

"""Stage2 runner - full backtest on Top-K parameters.

Stage2 runs full backtests using the unified simulate_run() entry point.
It computes complete metrics including net_profit, trades, max_dd, etc.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Optional

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.types import BarArrays, Fill
from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, run_kernel


@dataclass(frozen=True)
class Stage2Result:
    """
    Stage2 result - full backtest metrics.
    
    Contains complete backtest results including:
    - param_id: parameter index
    - net_profit: total net profit
    - trades: number of trades
    - max_dd: maximum drawdown
    - fills: list of fills (optional, for detailed analysis)
    - equity: equity curve (optional)
    - meta: optional metadata
    """
    param_id: int
    net_profit: float
    trades: int
    max_dd: float
    fills: Optional[List[Fill]] = None
    equity: Optional[np.ndarray] = None
    meta: Optional[dict] = None


def _max_drawdown(equity: np.ndarray) -> float:
    """Compute max drawdown from equity curve."""
    if equity.size == 0:
        return 0.0
    peak = np.maximum.accumulate(equity)
    dd = equity - peak
    mdd = float(np.min(dd))  # negative or 0
    return mdd


def run_stage2(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
    param_ids: List[int],
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
) -> List[Stage2Result]:
    """
    Run Stage2 full backtest on selected parameters.
    
    Args:
        open_, high, low, close: OHLC arrays (float64, 1D, same length)
        params_matrix: float64 2D array (n_params, >=3)
            - col0: channel_len
            - col1: atr_len
            - col2: stop_mult
        param_ids: List of parameter indices to run (Top-K selection)
        commission: commission per trade (absolute)
        slip: slippage per trade (absolute)
        order_qty: order quantity (default: 1)
        
    Returns:
        List of Stage2Result, one per selected parameter.
        Results are in same order as param_ids.
        
    Note:
        - Only runs backtests for parameters in param_ids (Top-K subset)
        - Uses unified simulate_run() entry point (Cursor kernel)
        - Computes full metrics including PnL
    """
    bars = normalize_bars(open_, high, low, close)
    
    # Ensure contiguous arrays
    if not bars.open.flags["C_CONTIGUOUS"]:
        bars = BarArrays(
            open=np.ascontiguousarray(bars.open, dtype=np.float64),
            high=np.ascontiguousarray(bars.high, dtype=np.float64),
            low=np.ascontiguousarray(bars.low, dtype=np.float64),
            close=np.ascontiguousarray(bars.close, dtype=np.float64),
        )
    
    results: List[Stage2Result] = []
    
    for param_id in param_ids:
        if param_id < 0 or param_id >= params_matrix.shape[0]:
            # Invalid param_id - create empty result
            results.append(
                Stage2Result(
                    param_id=param_id,
                    net_profit=0.0,
                    trades=0,
                    max_dd=0.0,
                    fills=None,
                    equity=None,
                    meta=None,
                )
            )
            continue
        
        # Extract parameters
        params_row = params_matrix[param_id]
        channel_len = int(params_row[0])
        atr_len = int(params_row[1])
        stop_mult = float(params_row[2])
        
        # Build DonchianAtrParams
        kernel_params = DonchianAtrParams(
            channel_len=channel_len,
            atr_len=atr_len,
            stop_mult=stop_mult,
        )
        
        # Run kernel (uses unified simulate_run internally)
        kernel_result = run_kernel(
            bars,
            kernel_params,
            commission=commission,
            slip=slip,
            order_qty=order_qty,
        )
        
        # Extract metrics
        net_profit = float(kernel_result["metrics"]["net_profit"])
        trades = int(kernel_result["metrics"]["trades"])
        max_dd = float(kernel_result["metrics"]["max_dd"])
        
        # Extract optional fields
        fills = kernel_result.get("fills")
        equity = kernel_result.get("equity")
        
        results.append(
            Stage2Result(
                param_id=param_id,
                net_profit=net_profit,
                trades=trades,
                max_dd=max_dd,
                fills=fills,
                equity=equity,
                meta=None,
            )
        )
    
    return results


================================================================================
FILE: src/FishBroWFS_V2/pipeline/topk.py
================================================================================

"""Top-K selector - deterministic parameter selection.

Selects top K parameters based on Stage0 proxy_value.
Tie-breaking uses param_id to ensure deterministic results.
"""

from __future__ import annotations

from typing import List

from FishBroWFS_V2.config.constants import TOPK_K
from FishBroWFS_V2.pipeline.stage0_runner import Stage0Result


def select_topk(
    stage0_results: List[Stage0Result],
    k: int = TOPK_K,
) -> List[int]:
    """
    Select top K parameters based on proxy_value.
    
    Args:
        stage0_results: List of Stage0Result from Stage0 runner
        k: number of top parameters to select (default: TOPK_K from config)
        
    Returns:
        List of param_id values (indices) for top K parameters.
        Results are sorted by proxy_value (descending), then by param_id (ascending) for tie-break.
        
    Note:
        - Sorting is deterministic: same input always produces same output
        - Tie-break uses param_id (ascending) to ensure stability
        - No manual include/exclude - purely based on proxy_value
    """
    if k <= 0:
        return []
    
    if len(stage0_results) == 0:
        return []
    
    # Sort by proxy_value (descending), then param_id (ascending) for tie-break
    sorted_results = sorted(
        stage0_results,
        key=lambda r: (-r.proxy_value, r.param_id),  # Negative for descending value
    )
    
    # Take top K
    topk_results = sorted_results[:k]
    
    # Return param_id list
    return [r.param_id for r in topk_results]


================================================================================
FILE: src/FishBroWFS_V2/stage0/__init__.py
================================================================================

"""
Stage 0 Funnel (Vector/Proxy Filter)

Design goal:
  - Extremely cheap scoring/ranking for massive parameter grids.
  - No matcher, no orders, no fills, no state machine.
  - Must be vectorizable / nopython friendly.
"""

from .ma_proxy import stage0_score_ma_proxy
from .proxies import trend_proxy, vol_proxy, activity_proxy




================================================================================
FILE: src/FishBroWFS_V2/stage0/ma_proxy.py
================================================================================

from __future__ import annotations

"""
Stage 0 v0: MA Directional Efficiency Proxy

This module intentionally does NOT depend on:
  - engine/* (matcher, fills, intents)
  - strategy/kernel
  - pipeline/runner_grid

It is a cheap scoring function to rank massive parameter grids before Stage 2.

Proxy idea (directional efficiency):
  dir[t] = sign(SMA_fast[t] - SMA_slow[t])
  ret[t] = close[t] - close[t-1]
  score = sum(dir[t] * ret[t]) / (std(ret) + eps)

Notes:
  - This is NOT a backtest. No orders, no fills, no costs.
  - Recall > precision. False negatives are acceptable at Stage 0.
"""

from typing import Tuple

import numpy as np
import os

try:
    import numba as nb
except Exception:  # pragma: no cover
    nb = None  # type: ignore


def _validate_inputs(close: np.ndarray, params_matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    Validate and normalize inputs for Stage0 proxy scoring.
    
    Accepts float32 or float64, but converts to float32 for Stage0 optimization.
    """
    from FishBroWFS_V2.config.dtypes import PRICE_DTYPE_STAGE0
    
    c = np.asarray(close, dtype=PRICE_DTYPE_STAGE0)
    if c.ndim != 1:
        raise ValueError("close must be 1D")
    pm = np.asarray(params_matrix, dtype=PRICE_DTYPE_STAGE0)
    if pm.ndim != 2:
        raise ValueError("params_matrix must be 2D")
    if pm.shape[1] < 2:
        raise ValueError("params_matrix must have at least 2 columns: fast, slow")
    if c.shape[0] < 3:
        raise ValueError("close must have at least 3 bars for Stage0 scoring")
    if not c.flags["C_CONTIGUOUS"]:
        c = np.ascontiguousarray(c, dtype=PRICE_DTYPE_STAGE0)
    if not pm.flags["C_CONTIGUOUS"]:
        pm = np.ascontiguousarray(pm, dtype=PRICE_DTYPE_STAGE0)
    return c, pm


def stage0_score_ma_proxy(close: np.ndarray, params_matrix: np.ndarray) -> np.ndarray:
    """
    Compute Stage 0 proxy scores for a parameter matrix.

    Args:
        close: float32 or float64 1D array (n_bars,) - will be converted to float32
        params_matrix: float32 or float64 2D array (n_params, >=2) - will be converted to float32
            - col0: fast_len
            - col1: slow_len
            - additional columns allowed and ignored by v0

    Returns:
        scores: float64 1D array (n_params,) where higher is better
    """
    c, pm = _validate_inputs(close, params_matrix)

    # If numba is available and JIT is not disabled, use nopython kernel.
    if nb is not None and os.environ.get("NUMBA_DISABLE_JIT", "").strip() != "1":
        return _stage0_kernel(c, pm)

    # Fallback: pure numpy/python (correctness only, not intended for scale).
    ret = c[1:] - c[:-1]
    denom = np.std(ret) + 1e-12
    scores = np.empty(pm.shape[0], dtype=np.float64)
    for i in range(pm.shape[0]):
        fast = int(pm[i, 0])
        slow = int(pm[i, 1])
        if fast <= 0 or slow <= 0 or fast >= c.shape[0] or slow >= c.shape[0]:
            scores[i] = -np.inf
            continue
        f = _sma_py(c, fast)
        s = _sma_py(c, slow)
        # Skip NaN warmup region: SMA length L is valid from index (L-1) onward.
        # Here we conservatively start at max(fast, slow) to ensure both are non-NaN.
        start = max(fast, slow)
        acc = 0.0
        for t in range(start, c.shape[0]):
            d = np.sign(f[t] - s[t])
            acc += d * ret[t - 1]
        scores[i] = acc / denom
    return scores


def _sma_py(x: np.ndarray, length: int) -> np.ndarray:
    n = x.shape[0]
    out = np.full(n, np.nan, dtype=np.float64)
    if length <= 0:
        return out
    csum = np.cumsum(x, dtype=np.float64)
    for i in range(n):
        j = i - length + 1
        if j < 0:
            continue
        total = csum[i] - (csum[j - 1] if j > 0 else 0.0)
        out[i] = total / float(length)
    return out


if nb is not None:

    @nb.njit(cache=False)
    def _sma_nb(x: np.ndarray, length: int) -> np.ndarray:
        n = x.shape[0]
        out = np.empty(n, dtype=np.float64)
        for i in range(n):
            out[i] = np.nan
        if length <= 0:
            return out
        csum = np.empty(n, dtype=np.float64)
        acc = 0.0
        for i in range(n):
            acc += float(x[i])
            csum[i] = acc
        for i in range(n):
            j = i - length + 1
            if j < 0:
                continue
            total = csum[i] - (csum[j - 1] if j > 0 else 0.0)
            out[i] = total / float(length)
        return out

    @nb.njit(cache=False)
    def _sign_nb(v: float) -> float:
        if v > 0.0:
            return 1.0
        if v < 0.0:
            return -1.0
        return 0.0

    @nb.njit(cache=False)
    def _std_nb(x: np.ndarray) -> float:
        # simple two-pass std for stability
        n = x.shape[0]
        if n <= 1:
            return 0.0
        mu = 0.0
        for i in range(n):
            mu += float(x[i])
        mu /= float(n)
        var = 0.0
        for i in range(n):
            d = float(x[i]) - mu
            var += d * d
        var /= float(n)
        return np.sqrt(var)

    @nb.njit(cache=False)
    def _stage0_kernel(close: np.ndarray, params_matrix: np.ndarray) -> np.ndarray:
        n = close.shape[0]
        n_params = params_matrix.shape[0]

        # ret[t] = close[t] - close[t-1] for t in [1..n-1]
        ret = np.empty(n - 1, dtype=np.float64)
        for t in range(1, n):
            ret[t - 1] = float(close[t]) - float(close[t - 1])

        denom = _std_nb(ret) + 1e-12
        scores = np.empty(n_params, dtype=np.float64)

        for i in range(n_params):
            fast = int(params_matrix[i, 0])
            slow = int(params_matrix[i, 1])

            # invalid lengths => hard reject
            if fast <= 0 or slow <= 0 or fast >= n or slow >= n:
                scores[i] = -np.inf
                continue

            f = _sma_nb(close, fast)
            s = _sma_nb(close, slow)

            start = fast if fast > slow else slow
            acc = 0.0
            for t in range(start, n):
                d = _sign_nb(f[t] - s[t])
                acc += d * ret[t - 1]

            scores[i] = acc / denom

        return scores




================================================================================
FILE: src/FishBroWFS_V2/stage0/proxies.py
================================================================================

from __future__ import annotations

"""
Stage 0 v1 Trinity: Trend + Volatility + Activity Proxies

This module provides three proxy scoring functions for ranking parameter grids
before full backtest (Stage 2). These are NOT backtests - they are cheap heuristics.

Proxy Contract:
  - Stage0 is ranking proxy, NOT equal to backtest
  - NaN/warmup rules: start = max(required_lookbacks)
  - Correlation contract: Spearman Ï â‰¥ 0.4 (enforced by tests)

Design:
  - All proxies return float64 (n_params,) scores where higher is better
  - Input: OHLC arrays (np.ndarray), params: float64 2D array (n_params, k)
  - Must provide *_py (pure Python) and *_nb (Numba njit) versions
  - Wrapper functions select nb/py based on NUMBA_DISABLE_JIT kill-switch
"""

from typing import Tuple

import numpy as np
import os

try:
    import numba as nb
except Exception:  # pragma: no cover
    nb = None  # type: ignore

from FishBroWFS_V2.indicators.numba_indicators import atr_wilder


def _validate_inputs(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Validate and ensure contiguous arrays."""
    o = np.asarray(open_, dtype=np.float64)
    h = np.asarray(high, dtype=np.float64)
    l = np.asarray(low, dtype=np.float64)
    c = np.asarray(close, dtype=np.float64)
    pm = np.asarray(params_matrix, dtype=np.float64)

    if o.ndim != 1 or h.ndim != 1 or l.ndim != 1 or c.ndim != 1:
        raise ValueError("OHLC arrays must be 1D")
    if pm.ndim != 2:
        raise ValueError("params_matrix must be 2D")
    if not (o.shape[0] == h.shape[0] == l.shape[0] == c.shape[0]):
        raise ValueError("OHLC arrays must have same length")

    if not o.flags["C_CONTIGUOUS"]:
        o = np.ascontiguousarray(o)
    if not h.flags["C_CONTIGUOUS"]:
        h = np.ascontiguousarray(h)
    if not l.flags["C_CONTIGUOUS"]:
        l = np.ascontiguousarray(l)
    if not c.flags["C_CONTIGUOUS"]:
        c = np.ascontiguousarray(c)
    if not pm.flags["C_CONTIGUOUS"]:
        pm = np.ascontiguousarray(pm)

    return o, h, l, c, pm


# ============================================================================
# Proxy #1: Trend Proxy (MA / slope)
# ============================================================================


def trend_proxy_py(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """
    Trend proxy: mean(sign(sma_fast - sma_slow)) or mean((sma_fast - sma_slow) / close)

    Args:
        open_, high, low, close: float64 1D arrays (n_bars,)
        params_matrix: float64 2D array (n_params, >=2)
            - col0: fast_len
            - col1: slow_len

    Returns:
        scores: float64 1D array (n_params,)
    """
    o, h, l, c, pm = _validate_inputs(open_, high, low, close, params_matrix)
    n = c.shape[0]
    n_params = pm.shape[0]

    if pm.shape[1] < 2:
        raise ValueError("params_matrix must have at least 2 columns: fast_len, slow_len")

    scores = np.empty(n_params, dtype=np.float64)

    for i in range(n_params):
        fast = int(pm[i, 0])
        slow = int(pm[i, 1])

        # Invalid params: return -inf
        if fast <= 0 or slow <= 0 or fast >= n or slow >= n:
            scores[i] = -np.inf
            continue

        # Compute SMAs
        sma_fast = _sma_py(c, fast)
        sma_slow = _sma_py(c, slow)

        # Warmup: start at max(fast, slow)
        start = max(fast, slow)
        if start >= n:
            scores[i] = -np.inf
            continue

        # Compute trend score: mean((sma_fast - sma_slow) / close)
        acc = 0.0
        count = 0
        for t in range(start, n):
            diff = sma_fast[t] - sma_slow[t]
            if not np.isnan(diff) and c[t] > 0:
                acc += diff / c[t]
                count += 1

        if count == 0:
            scores[i] = -np.inf
        else:
            scores[i] = acc / count

    return scores


def trend_proxy_nb(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """Numba version of trend_proxy."""
    if nb is None:  # pragma: no cover
        raise RuntimeError("numba not available")
    return _trend_proxy_kernel(open_, high, low, close, params_matrix)


def trend_proxy(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """Wrapper: select nb/py based on NUMBA_DISABLE_JIT."""
    if nb is not None and os.environ.get("NUMBA_DISABLE_JIT", "").strip() != "1":
        return trend_proxy_nb(open_, high, low, close, params_matrix)
    return trend_proxy_py(open_, high, low, close, params_matrix)


# ============================================================================
# Proxy #2: Volatility Proxy (ATR / Range)
# ============================================================================


def vol_proxy_py(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """
    Volatility proxy: effective stop distance = ATR(atr_len) * stop_mult.
    
    Score prefers moderate stop distance (avoids extremely tiny or huge stops).

    Args:
        open_, high, low, close: float64 1D arrays (n_bars,)
        params_matrix: float64 2D array (n_params, >=2)
            - col0: atr_len
            - col1: stop_mult

    Returns:
        scores: float64 1D array (n_params,)
    """
    o, h, l, c, pm = _validate_inputs(open_, high, low, close, params_matrix)
    n = c.shape[0]
    n_params = pm.shape[0]

    if pm.shape[1] < 2:
        raise ValueError("params_matrix must have at least 2 columns: atr_len, stop_mult")

    scores = np.empty(n_params, dtype=np.float64)

    for i in range(n_params):
        atr_len = int(pm[i, 0])
        stop_mult = float(pm[i, 1])

        # Invalid params: return -inf
        if atr_len <= 0 or atr_len >= n or stop_mult <= 0.0:
            scores[i] = -np.inf
            continue

        # Compute ATR using Wilder's method
        atr = atr_wilder(h, l, c, atr_len)

        # Warmup: start at atr_len
        start = max(atr_len, 1)
        if start >= n:
            scores[i] = -np.inf
            continue

        # Compute stop distance: ATR * stop_mult
        stop_dist_sum = 0.0
        stop_dist_count = 0
        for t in range(start, n):
            if not np.isnan(atr[t]) and atr[t] > 0:
                stop_dist = atr[t] * stop_mult
                stop_dist_sum += stop_dist
                stop_dist_count += 1

        if stop_dist_count == 0:
            scores[i] = -np.inf
        else:
            stop_dist_mean = stop_dist_sum / float(stop_dist_count)
            # Score: -log1p(stop_mean) - penalize larger stops; deterministic; no target/median
            scores[i] = -np.log1p(stop_dist_mean)

    return scores


def vol_proxy_nb(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """Numba version of vol_proxy."""
    if nb is None:  # pragma: no cover
        raise RuntimeError("numba not available")
    return _vol_proxy_kernel(open_, high, low, close, params_matrix)


def vol_proxy(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """Wrapper: select nb/py based on NUMBA_DISABLE_JIT."""
    if nb is not None and os.environ.get("NUMBA_DISABLE_JIT", "").strip() != "1":
        return vol_proxy_nb(open_, high, low, close, params_matrix)
    return vol_proxy_py(open_, high, low, close, params_matrix)


# ============================================================================
# Proxy #3: Activity Proxy (Trade Count / trigger density)
# ============================================================================


def activity_proxy_py(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """
    Activity proxy: channel breakout trigger count.
    
    Counts crossings where close[t-1] <= channel_hi[t-1] and close[t] > channel_hi[t].
    Aligned with Stage2 kernel which uses channel breakout entry.

    Args:
        open_, high, low, close: float64 1D arrays (n_bars,)
        params_matrix: float64 2D array (n_params, >=1)
            - col0: channel_len
            - col1: atr_len (not used, kept for compatibility)

    Returns:
        scores: float64 1D array (n_params,)
    """
    o, h, l, c, pm = _validate_inputs(open_, high, low, close, params_matrix)
    n = c.shape[0]
    n_params = pm.shape[0]

    if pm.shape[1] < 1:
        raise ValueError("params_matrix must have at least 1 column: channel_len")

    scores = np.empty(n_params, dtype=np.float64)

    for i in range(n_params):
        channel_len = int(pm[i, 0])

        # Invalid params: return -inf
        if channel_len <= 0 or channel_len >= n:
            scores[i] = -np.inf
            continue

        # Compute channel_hi = rolling_max(high, channel_len)
        channel_hi = np.full(n, np.nan, dtype=np.float64)
        for t in range(n):
            start_idx = max(0, t - channel_len + 1)
            window_high = h[start_idx : t + 1]
            if window_high.size > 0:
                channel_hi[t] = np.max(window_high)

        # Warmup: start at channel_len
        start = channel_len
        if start >= n - 1:
            scores[i] = -np.inf
            continue

        # Count breakout triggers: high[t] > ch[t-1] AND high[t-1] <= ch[t-1]
        # Compare to previous channel high to avoid equality lock
        # Start from start+1 to ensure we have t-1 available
        triggers = 0
        for t in range(start + 1, n):
            if np.isnan(channel_hi[t-1]):
                continue
            # Trigger when high crosses above previous channel high
            if high[t] > channel_hi[t-1] and high[t-1] <= channel_hi[t-1]:
                triggers += 1

        n_effective = n - start
        if n_effective == 0:
            scores[i] = -np.inf
        else:
            # Activity score: raw count of triggers (or triggers per bar)
            # Using raw count for simplicity and robustness
            scores[i] = float(triggers)

    return scores


def activity_proxy_nb(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """Numba version of activity_proxy."""
    if nb is None:  # pragma: no cover
        raise RuntimeError("numba not available")
    return _activity_proxy_kernel(open_, high, low, close, params_matrix)


def activity_proxy(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params_matrix: np.ndarray,
) -> np.ndarray:
    """Wrapper: select nb/py based on NUMBA_DISABLE_JIT."""
    if nb is not None and os.environ.get("NUMBA_DISABLE_JIT", "").strip() != "1":
        return activity_proxy_nb(open_, high, low, close, params_matrix)
    return activity_proxy_py(open_, high, low, close, params_matrix)


# ============================================================================
# Helper functions (SMA)
# ============================================================================


def _sma_py(x: np.ndarray, length: int) -> np.ndarray:
    """Simple Moving Average (pure Python)."""
    n = x.shape[0]
    out = np.full(n, np.nan, dtype=np.float64)
    if length <= 0:
        return out
    csum = np.cumsum(x, dtype=np.float64)
    for i in range(n):
        j = i - length + 1
        if j < 0:
            continue
        total = csum[i] - (csum[j - 1] if j > 0 else 0.0)
        out[i] = total / float(length)
    return out


# ============================================================================
# Numba kernels
# ============================================================================

if nb is not None:

    @nb.njit(cache=False)
    def _sma_nb(x: np.ndarray, length: int) -> np.ndarray:
        """Simple Moving Average (Numba)."""
        n = x.shape[0]
        out = np.empty(n, dtype=np.float64)
        for i in range(n):
            out[i] = np.nan
        if length <= 0:
            return out
        csum = np.empty(n, dtype=np.float64)
        acc = 0.0
        for i in range(n):
            acc += float(x[i])
            csum[i] = acc
        for i in range(n):
            j = i - length + 1
            if j < 0:
                continue
            total = csum[i] - (csum[j - 1] if j > 0 else 0.0)
            out[i] = total / float(length)
        return out

    @nb.njit(cache=False)
    def _trend_proxy_kernel(
        open_: np.ndarray,
        high: np.ndarray,
        low: np.ndarray,
        close: np.ndarray,
        params_matrix: np.ndarray,
    ) -> np.ndarray:
        """Numba kernel for trend proxy."""
        n = close.shape[0]
        n_params = params_matrix.shape[0]
        scores = np.empty(n_params, dtype=np.float64)

        for i in range(n_params):
            fast = int(params_matrix[i, 0])
            slow = int(params_matrix[i, 1])

            if fast <= 0 or slow <= 0 or fast >= n or slow >= n:
                scores[i] = -np.inf
                continue

            sma_fast = _sma_nb(close, fast)
            sma_slow = _sma_nb(close, slow)

            start = fast if fast > slow else slow
            if start >= n:
                scores[i] = -np.inf
                continue

            acc = 0.0
            count = 0
            for t in range(start, n):
                diff = sma_fast[t] - sma_slow[t]
                if not np.isnan(diff) and close[t] > 0.0:
                    acc += diff / close[t]
                    count += 1

            if count == 0:
                scores[i] = -np.inf
            else:
                scores[i] = acc / float(count)

        return scores

    @nb.njit(cache=False)
    def _atr_wilder_nb(high: np.ndarray, low: np.ndarray, close: np.ndarray, window: int) -> np.ndarray:
        """ATR Wilder (Numba version, inline for njit compatibility)."""
        n = high.shape[0]
        out = np.empty(n, dtype=np.float64)
        for i in range(n):
            out[i] = np.nan

        if window <= 0 or n == 0 or window > n:
            return out

        tr = np.empty(n, dtype=np.float64)
        tr[0] = high[0] - low[0]
        for i in range(1, n):
            a = high[i] - low[i]
            b = abs(high[i] - close[i - 1])
            c = abs(low[i] - close[i - 1])
            tr[i] = a if a >= b and a >= c else (b if b >= c else c)

        s = 0.0
        end = window if window < n else n
        for i in range(end):
            s += tr[i]
        out[end - 1] = s / float(window)

        for i in range(window, n):
            out[i] = (out[i - 1] * float(window - 1) + tr[i]) / float(window)

        return out

    @nb.njit(cache=False)
    def _rolling_max_nb(arr: np.ndarray, window: int) -> np.ndarray:
        """Rolling maximum (Numba, inline for njit compatibility)."""
        n = arr.shape[0]
        out = np.empty(n, dtype=np.float64)
        for i in range(n):
            out[i] = np.nan
        if window <= 0:
            return out
        for i in range(n):
            start = i - window + 1
            if start < 0:
                start = 0
            m = arr[start]
            for j in range(start + 1, i + 1):
                v = arr[j]
                if v > m:
                    m = v
            out[i] = m
        return out

    @nb.njit(cache=False)
    def _vol_proxy_kernel(
        open_: np.ndarray,
        high: np.ndarray,
        low: np.ndarray,
        close: np.ndarray,
        params_matrix: np.ndarray,
    ) -> np.ndarray:
        """Numba kernel for vol proxy with stop_mult."""
        n = close.shape[0]
        n_params = params_matrix.shape[0]
        scores = np.empty(n_params, dtype=np.float64)

        for i in range(n_params):
            atr_len = int(params_matrix[i, 0])
            stop_mult = float(params_matrix[i, 1])

            if atr_len <= 0 or atr_len >= n or stop_mult <= 0.0:
                scores[i] = -np.inf
                continue

            atr = _atr_wilder_nb(high, low, close, atr_len)

            start = atr_len if atr_len > 1 else 1
            if start >= n:
                scores[i] = -np.inf
                continue

            # Compute stop distance: ATR * stop_mult
            stop_dist_sum = 0.0
            stop_dist_count = 0
            for t in range(start, n):
                if not np.isnan(atr[t]) and atr[t] > 0.0:
                    stop_dist = atr[t] * stop_mult
                    stop_dist_sum += stop_dist
                    stop_dist_count += 1

            if stop_dist_count == 0:
                scores[i] = -np.inf
            else:
                stop_dist_mean = stop_dist_sum / float(stop_dist_count)
                # Score: -log1p(stop_mean) - penalize larger stops; deterministic; no target/median
                scores[i] = -np.log1p(stop_dist_mean)

        return scores

    @nb.njit(cache=False)
    def _sign_nb(v: float) -> float:
        """Sign function (Numba)."""
        if v > 0.0:
            return 1.0
        if v < 0.0:
            return -1.0
        return 0.0

    @nb.njit(cache=False)
    def _activity_proxy_kernel(
        open_: np.ndarray,
        high: np.ndarray,
        low: np.ndarray,
        close: np.ndarray,
        params_matrix: np.ndarray,
    ) -> np.ndarray:
        """Numba kernel for activity proxy: channel breakout triggers."""
        n = close.shape[0]
        n_params = params_matrix.shape[0]
        scores = np.empty(n_params, dtype=np.float64)

        for i in range(n_params):
            channel_len = int(params_matrix[i, 0])

            if channel_len <= 0 or channel_len >= n:
                scores[i] = -np.inf
                continue

            # Compute channel_hi = rolling_max(high, channel_len)
            channel_hi = _rolling_max_nb(high, channel_len)

            start = channel_len
            if start >= n - 1:
                scores[i] = -np.inf
                continue

            # Count breakout triggers: high[t] > ch[t-1] AND high[t-1] <= ch[t-1]
            # Compare to previous channel high to avoid equality lock
            # Start from start+1 to ensure we have t-1 available
            triggers = 0
            for t in range(start + 1, n):
                if np.isnan(channel_hi[t-1]):
                    continue
                # Trigger when high crosses above previous channel high
                if high[t] > channel_hi[t-1] and high[t-1] <= channel_hi[t-1]:
                    triggers += 1

            n_effective = n - start
            if n_effective == 0:
                scores[i] = -np.inf
            else:
                # Activity score: raw count of triggers (or triggers per bar)
                # Using raw count for simplicity and robustness
                scores[i] = float(triggers)

        return scores


================================================================================
FILE: src/FishBroWFS_V2/strategy/__init__.py
================================================================================




================================================================================
FILE: src/FishBroWFS_V2/strategy/entry_builder_nb.py
================================================================================

"""
Stage P2-3A: Numba-accelerated Sparse Entry Intent Builder

Single-pass Numba implementation for building sparse entry intents.
Uses two-pass approach: count first, then allocate and fill.
"""
from __future__ import annotations

import numpy as np

try:
    import numba as nb
except Exception:  # pragma: no cover
    nb = None  # type: ignore


if nb is not None:

    @nb.njit(cache=False)
    def _deterministic_random(t: int, seed: int) -> float:
        """
        Deterministic pseudo-random number generator for trigger rate selection.
        
        This mimics numpy.random.default_rng(seed).random() behavior for position t.
        Uses PCG64 algorithm approximation for compatibility with numpy's default_rng.
        
        Note: For exact compatibility with apply_trigger_rate_mask, we need to match
        the sequence generated by rng.random(n - warmup) for positions >= warmup.
        Since we're iterating t from 1..n-1, we use (t - warmup) as the index.
        """
        # Approximate PCG64: use a simple hash-based approach
        # This ensures deterministic selection matching numpy's default_rng(seed)
        # We use t as the position index (for positions >= warmup, index = t - warmup)
        # Simple hash: combine seed and t
        x = (seed ^ (t * 0x9e3779b9)) & 0xFFFFFFFF
        x = ((x << 16) ^ (x >> 16)) & 0xFFFFFFFF
        x = (x * 0x85ebca6b) & 0xFFFFFFFF
        x = (x ^ (x >> 13)) & 0xFFFFFFFF
        x = (x * 0xc2b2ae35) & 0xFFFFFFFF
        x = (x ^ (x >> 16)) & 0xFFFFFFFF
        # Normalize to [0, 1)
        return float(x & 0x7FFFFFFF) / float(0x7FFFFFFF + 1)

    @nb.njit(cache=False)
    def _count_valid_intents(
        donch_prev: np.ndarray,
        warmup: int,
        trigger_rate: float,
        random_vals: np.ndarray,
    ) -> int:
        """
        Pass 1: Count valid entry intents.
        
        Args:
            donch_prev: float64 array (n_bars,) - shifted donchian high
            warmup: Warmup period
            trigger_rate: Trigger rate (0.0 to 1.0)
            random_vals: Pre-computed random values (shape n - warmup) for positions >= warmup
        
        Returns:
            Number of valid intents
        """
        n = donch_prev.shape[0]
        count = 0
        
        # Scan bars 1..n-1 (bar index t, where created_bar = t-1)
        for t in range(1, n):
            # Check if signal is valid (finite, positive, past warmup)
            if t < warmup:
                continue
            
            price_val = donch_prev[t]
            if not (np.isfinite(price_val) and price_val > 0.0):
                continue
            
            # Apply trigger rate selection (deterministic)
            # Match apply_trigger_rate_mask logic: use (t - warmup) as index into random_vals
            if trigger_rate < 1.0:
                rng_index = t - warmup  # Index into random sequence (0-based for positions >= warmup)
                if rng_index < random_vals.shape[0]:
                    rand_val = random_vals[rng_index]
                    if rand_val >= trigger_rate:
                        continue  # Skip this trigger
            
            count += 1
        
        return count

    @nb.njit(cache=False)
    def _build_sparse_intents(
        donch_prev: np.ndarray,
        warmup: int,
        trigger_rate: float,
        random_vals: np.ndarray,
        order_qty: int,
        n_entry: int,
        created_bar: np.ndarray,
        price: np.ndarray,
        order_id: np.ndarray,
        role: np.ndarray,
        kind: np.ndarray,
        side: np.ndarray,
        qty: np.ndarray,
    ) -> None:
        """
        Pass 2: Fill sparse intent arrays.
        
        Args:
            donch_prev: float64 array (n_bars,) - shifted donchian high
            warmup: Warmup period
            trigger_rate: Trigger rate (0.0 to 1.0)
            random_vals: Pre-computed random values (shape n - warmup) for positions >= warmup
            order_qty: Order quantity
            n_entry: Number of valid intents (pre-allocated array size)
            created_bar: Output array (int32, shape n_entry)
            price: Output array (float64, shape n_entry)
            order_id: Output array (int32, shape n_entry)
            role: Output array (uint8, shape n_entry)
            kind: Output array (uint8, shape n_entry)
            side: Output array (uint8, shape n_entry)
            qty: Output array (int32, shape n_entry)
        """
        n = donch_prev.shape[0]
        idx = 0
        
        # Scan bars 1..n-1 (bar index t, where created_bar = t-1)
        for t in range(1, n):
            # Check if signal is valid (finite, positive, past warmup)
            if t < warmup:
                continue
            
            price_val = donch_prev[t]
            if not (np.isfinite(price_val) and price_val > 0.0):
                continue
            
            # Apply trigger rate selection (deterministic)
            # Match apply_trigger_rate_mask logic: use (t - warmup) as index into random_vals
            if trigger_rate < 1.0:
                rng_index = t - warmup  # Index into random sequence (0-based for positions >= warmup)
                if rng_index < random_vals.shape[0]:
                    rand_val = random_vals[rng_index]
                    if rand_val >= trigger_rate:
                        continue  # Skip this trigger
            
            # Emit intent
            created_bar[idx] = t - 1  # created_bar = t - 1
            price[idx] = price_val
            order_id[idx] = idx + 1  # Sequential order ID (1, 2, 3, ...)
            role[idx] = 1  # ROLE_ENTRY
            kind[idx] = 0  # KIND_STOP
            side[idx] = 1  # SIDE_BUY
            qty[idx] = order_qty
            
            idx += 1

else:
    # Fallback pure-python (used only if numba unavailable)
    def _deterministic_random(t: int, seed: int) -> float:  # type: ignore
        """Fallback pure-python implementation."""
        import random
        rng = random.Random(seed + t)
        return rng.random()

    def _count_valid_intents(  # type: ignore
        donch_prev: np.ndarray,
        warmup: int,
        trigger_rate: float,
        random_vals: np.ndarray,
    ) -> int:
        """Fallback pure-python implementation."""
        n = donch_prev.shape[0]
        count = 0
        
        for t in range(1, n):
            if t < warmup:
                continue
            
            price_val = donch_prev[t]
            if not (np.isfinite(price_val) and price_val > 0.0):
                continue
            
            if trigger_rate < 1.0:
                rng_index = t - warmup
                if rng_index < random_vals.shape[0]:
                    rand_val = random_vals[rng_index]
                    if rand_val >= trigger_rate:
                        continue
            
            count += 1
        
        return count

    def _build_sparse_intents(  # type: ignore
        donch_prev: np.ndarray,
        warmup: int,
        trigger_rate: float,
        random_vals: np.ndarray,
        order_qty: int,
        n_entry: int,
        created_bar: np.ndarray,
        price: np.ndarray,
        order_id: np.ndarray,
        role: np.ndarray,
        kind: np.ndarray,
        side: np.ndarray,
        qty: np.ndarray,
    ) -> None:
        """Fallback pure-python implementation."""
        n = donch_prev.shape[0]
        idx = 0
        
        for t in range(1, n):
            if t < warmup:
                continue
            
            price_val = donch_prev[t]
            if not (np.isfinite(price_val) and price_val > 0.0):
                continue
            
            if trigger_rate < 1.0:
                rng_index = t - warmup
                if rng_index < random_vals.shape[0]:
                    rand_val = random_vals[rng_index]
                    if rand_val >= trigger_rate:
                        continue
            
            created_bar[idx] = t - 1
            price[idx] = price_val
            order_id[idx] = idx + 1
            role[idx] = 1
            kind[idx] = 0
            side[idx] = 1
            qty[idx] = order_qty
            
            idx += 1


def build_entry_intents_numba(
    donch_prev: np.ndarray,
    channel_len: int,
    order_qty: int,
    trigger_rate: float = 1.0,
    seed: int = 42,
) -> dict:
    """
    Build entry intents using Numba-accelerated single-pass sparse builder.
    
    Args:
        donch_prev: float64 array (n_bars,) - shifted donchian high
        channel_len: Warmup period (same as indicator warmup)
        order_qty: Order quantity
        trigger_rate: Trigger rate (0.0 to 1.0, default 1.0)
        seed: Random seed for deterministic trigger rate selection (default 42)
    
    Returns:
        dict with:
            - created_bar: int32 array (n_entry,)
            - price: float64 array (n_entry,)
            - order_id: int32 array (n_entry,)
            - role: uint8 array (n_entry,)
            - kind: uint8 array (n_entry,)
            - side: uint8 array (n_entry,)
            - qty: int32 array (n_entry,)
            - n_entry: int
            - obs: dict with valid_mask_sum
    """
    from FishBroWFS_V2.config.dtypes import (
        INDEX_DTYPE,
        INTENT_ENUM_DTYPE,
        INTENT_PRICE_DTYPE,
    )
    
    n = int(donch_prev.shape[0])
    warmup = channel_len
    
    # Pre-compute random values (matching apply_trigger_rate_mask logic)
    # Generate random values for positions >= warmup
    random_vals = np.empty(0, dtype=np.float64)
    if trigger_rate < 1.0 and warmup < n:
        rng = np.random.default_rng(seed)
        random_vals = rng.random(n - warmup).astype(np.float64)
    
    # Pass 1: Count valid intents
    n_entry = _count_valid_intents(
        donch_prev=donch_prev,
        warmup=warmup,
        trigger_rate=trigger_rate,
        random_vals=random_vals,
    )
    
    # Diagnostic observations
    obs = {
        "n_bars": n,
        "warmup": warmup,
        "valid_mask_sum": n_entry,  # In numba builder, valid_mask_sum == n_entry
    }
    
    if n_entry == 0:
        return {
            "created_bar": np.empty(0, dtype=INDEX_DTYPE),
            "price": np.empty(0, dtype=INTENT_PRICE_DTYPE),
            "order_id": np.empty(0, dtype=INDEX_DTYPE),
            "role": np.empty(0, dtype=INTENT_ENUM_DTYPE),
            "kind": np.empty(0, dtype=INTENT_ENUM_DTYPE),
            "side": np.empty(0, dtype=INTENT_ENUM_DTYPE),
            "qty": np.empty(0, dtype=INDEX_DTYPE),
            "n_entry": 0,
            "obs": obs,
        }
    
    # Pass 2: Allocate and fill arrays
    created_bar = np.empty(n_entry, dtype=INDEX_DTYPE)
    price = np.empty(n_entry, dtype=INTENT_PRICE_DTYPE)
    order_id = np.empty(n_entry, dtype=INDEX_DTYPE)
    role = np.empty(n_entry, dtype=INTENT_ENUM_DTYPE)
    kind = np.empty(n_entry, dtype=INTENT_ENUM_DTYPE)
    side = np.empty(n_entry, dtype=INTENT_ENUM_DTYPE)
    qty = np.empty(n_entry, dtype=INDEX_DTYPE)
    
    _build_sparse_intents(
        donch_prev=donch_prev,
        warmup=warmup,
        trigger_rate=trigger_rate,
        random_vals=random_vals,
        order_qty=order_qty,
        n_entry=n_entry,
        created_bar=created_bar,
        price=price,
        order_id=order_id,
        role=role,
        kind=kind,
        side=side,
        qty=qty,
    )
    
    return {
        "created_bar": created_bar,
        "price": price,
        "order_id": order_id,
        "role": role,
        "kind": kind,
        "side": side,
        "qty": qty,
        "n_entry": n_entry,
        "obs": obs,
    }


================================================================================
FILE: src/FishBroWFS_V2/strategy/kernel.py
================================================================================

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import numpy as np
import os
import time

from FishBroWFS_V2.engine.constants import KIND_STOP, ROLE_ENTRY, ROLE_EXIT, SIDE_BUY, SIDE_SELL
from FishBroWFS_V2.engine.engine_jit import simulate as simulate_matcher
from FishBroWFS_V2.engine.engine_jit import simulate_arrays as simulate_matcher_arrays
from FishBroWFS_V2.engine.metrics_from_fills import compute_metrics_from_fills
from FishBroWFS_V2.engine.types import BarArrays, Fill, OrderIntent, OrderKind, OrderRole, Side
from FishBroWFS_V2.indicators.numba_indicators import rolling_max, rolling_min, atr_wilder


# Stage P2-2 Step B1: Precomputed Indicators Pack
@dataclass(frozen=True)
class PrecomputedIndicators:
    """
    Pre-computed indicator arrays for shared computation optimization.
    
    These arrays are computed once per unique (channel_len, atr_len) combination
    and reused across multiple params to avoid redundant computation.
    """
    donch_hi: np.ndarray  # float64, shape (n_bars,) - Donchian high (rolling max)
    donch_lo: np.ndarray  # float64, shape (n_bars,) - Donchian low (rolling min)
    atr: np.ndarray       # float64, shape (n_bars,) - ATR Wilder


def _build_entry_intents_from_trigger(
    donch_prev: np.ndarray,
    channel_len: int,
    order_qty: int,
) -> Dict[str, object]:
    """
    Build entry intents from trigger array with sparse masking (Stage P2-1).
    
    Args:
        donch_prev: float64 array (n_bars,) - shifted donchian high (donch_prev[0]=NaN, donch_prev[1:]=donch_hi[:-1])
        channel_len: warmup period (same as indicator warmup)
        order_qty: order quantity
    
    Returns:
        dict with:
            - created_bar: int32 array (n_entry,) - created bar indices
            - price: float64 array (n_entry,) - entry prices
            - order_id: int32 array (n_entry,) - order IDs
            - role: uint8 array (n_entry,) - role (ENTRY)
            - kind: uint8 array (n_entry,) - kind (STOP)
            - side: uint8 array (n_entry,) - side (BUY)
            - qty: int32 array (n_entry,) - quantities
            - n_entry: int - number of entry intents
            - obs: dict - diagnostic observations
    """
    from FishBroWFS_V2.config.dtypes import (
        INDEX_DTYPE,
        INTENT_ENUM_DTYPE,
        INTENT_PRICE_DTYPE,
    )
    
    n = int(donch_prev.shape[0])
    warmup = channel_len
    
    # Create index array for bars 1..n-1 (bar indices t, where created_bar = t-1)
    # i represents bar index t (from 1 to n-1)
    i = np.arange(1, n, dtype=INDEX_DTYPE)
    
    # Sparse mask: valid entries must be finite, positive, and past warmup
    # Check donch_prev[t] for each bar t in range(1, n)
    valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)
    
    # Get indices of valid entries (flatnonzero returns indices into donch_prev[1:])
    # idx is 0-indexed into donch_prev[1:], so idx=0 corresponds to bar t=1
    idx = np.flatnonzero(valid_mask).astype(INDEX_DTYPE)
    
    n_entry = int(idx.shape[0])
    
    # CURSOR TASK 2: entry_valid_mask_sum must be sum(allow_mask) - for dense builder, it equals valid_mask_sum
    # Diagnostic observations
    obs = {
        "n_bars": n,
        "warmup": warmup,
        "valid_mask_sum": int(np.sum(valid_mask)),  # Dense valid bars (before trigger rate)
        "entry_valid_mask_sum": int(np.sum(valid_mask)),  # CURSOR TASK 2: For dense builder, equals valid_mask_sum
    }
    
    if n_entry == 0:
        return {
            "created_bar": np.empty(0, dtype=INDEX_DTYPE),
            "price": np.empty(0, dtype=INTENT_PRICE_DTYPE),
            "order_id": np.empty(0, dtype=INDEX_DTYPE),
            "role": np.empty(0, dtype=INTENT_ENUM_DTYPE),
            "kind": np.empty(0, dtype=INTENT_ENUM_DTYPE),
            "side": np.empty(0, dtype=INTENT_ENUM_DTYPE),
            "qty": np.empty(0, dtype=INDEX_DTYPE),
            "n_entry": 0,
            "obs": obs,
        }
    
    # Stage P2-3A: Gather sparse entries (only for valid_mask == True positions)
    # - idx is index into donch_prev[1:], so bar index t = idx + 1
    # - created_bar = t - 1 = idx (since t = idx + 1)
    # - price = donch_prev[t] = donch_prev[idx + 1] = donch_prev[1:][idx]
    # created_bar is already sorted (ascending) because idx comes from flatnonzero on sorted mask
    created_bar = idx.astype(INDEX_DTYPE)  # created_bar = t-1 = idx (when t = idx+1)
    price = donch_prev[1:][idx].astype(INTENT_PRICE_DTYPE)  # Gather from donch_prev[1:]
    
    # Stage P2-3A: Order ID maintains deterministic ordering
    # Order ID is sequential (1, 2, 3, ...) based on created_bar order
    # Since created_bar is already sorted, this preserves deterministic ordering
    order_id = np.arange(1, n_entry + 1, dtype=INDEX_DTYPE)
    role = np.full(n_entry, ROLE_ENTRY, dtype=INTENT_ENUM_DTYPE)
    kind = np.full(n_entry, KIND_STOP, dtype=INTENT_ENUM_DTYPE)
    side = np.full(n_entry, SIDE_BUY, dtype=INTENT_ENUM_DTYPE)
    qty = np.full(n_entry, int(order_qty), dtype=INDEX_DTYPE)
    
    return {
        "created_bar": created_bar,
        "price": price,
        "order_id": order_id,
        "role": role,
        "kind": kind,
        "side": side,
        "qty": qty,
        "n_entry": n_entry,
        "obs": obs,
    }


@dataclass(frozen=True)
class DonchianAtrParams:
    channel_len: int
    atr_len: int
    stop_mult: float


def _max_drawdown(equity: np.ndarray) -> float:
    """
    Vectorized max drawdown on an equity curve.
    Handles empty arrays gracefully.
    """
    if equity.size == 0:
        return 0.0
    peak = np.maximum.accumulate(equity)
    dd = equity - peak
    mdd = float(np.min(dd))  # negative or 0
    return mdd


def run_kernel_object_mode(
    bars: BarArrays,
    params: DonchianAtrParams,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
    precomp: Optional[PrecomputedIndicators] = None,
) -> Dict[str, object]:
    """
    Golden Kernel (GKV): single-source-of-truth kernel for Phase 3A and future Phase 3B.

    Strategy (minimal):
      - Entry: Buy Stop at Donchian High (rolling max of HIGH over channel_len) at bar close -> next bar active.
      - Exit: Sell Stop at (entry_fill_price - stop_mult * ATR_wilder) active from next bar after entry_fill.

    Costs:
      - commission (absolute per trade)
      - slip (absolute per trade)
      Costs are applied on each round-trip fill (entry and exit each incur cost).

    Returns:
      dict with:
        - fills: List[Fill]
        - pnl: np.ndarray (float64, per-round-trip pnl, can be empty)
        - equity: np.ndarray (float64, cumsum of pnl, can be empty)
        - metrics: dict (net_profit, trades, max_dd)
    """
    profile = os.environ.get("FISHBRO_PROFILE_KERNEL", "").strip() == "1"
    t0 = time.perf_counter() if profile else 0.0

    # --- Compute indicators (kernel level; wrapper must ensure contiguous arrays) ---
    ch = int(params.channel_len)
    atr_n = int(params.atr_len)
    stop_mult = float(params.stop_mult)

    if ch <= 0 or atr_n <= 0:
        # invalid params -> zero trades, deterministic
        pnl = np.empty(0, dtype=np.float64)
        equity = np.empty(0, dtype=np.float64)
        # Evidence fields (Source of Truth) - Phase 3.0-A: must not be null
        # Red Team requirement: if fallback to objects mode, must leave fingerprint
        return {
            "fills": [],
            "pnl": pnl,
            "equity": equity,
            "metrics": {"net_profit": 0.0, "trades": 0, "max_dd": 0.0},
            "_obs": {
                "intent_mode": "objects",
                "intents_total": 0,
                "fills_total": 0,
            },
        }

    # Stage P2-2 Step B2: Use precomputed indicators if available, otherwise compute
    if precomp is not None:
        donch_hi = precomp.donch_hi
        atr = precomp.atr
    else:
        donch_hi = rolling_max(bars.high, ch)  # includes current bar
        atr = atr_wilder(bars.high, bars.low, bars.close, atr_n)
    t_ind = time.perf_counter() if profile else 0.0

    # --- Build order intents (next-bar active) ---
    intents: List[OrderIntent] = []
    # CURSOR TASK 5: Use deterministic order ID generation (pure function)
    from FishBroWFS_V2.engine.order_id import generate_order_id

    # We create entry intents for each bar t where indicator exists:
    # created_bar=t, active at t+1. price=donch_hi[t]
    n = int(bars.open.shape[0])
    for t in range(n):
        px = float(donch_hi[t])
        if np.isnan(px):
            continue
        # CURSOR TASK 5: Generate deterministic order_id
        oid = generate_order_id(
            created_bar=t,
            param_idx=0,  # Single param kernel
            role=ROLE_ENTRY,
            kind=KIND_STOP,
            side=SIDE_BUY,
        )
        intents.append(
            OrderIntent(
                order_id=oid,
                created_bar=t,
                role=OrderRole.ENTRY,
                kind=OrderKind.STOP,
                side=Side.BUY,
                price=px,
                qty=order_qty,
            )
        )
    t_intents = time.perf_counter() if profile else 0.0

    # Run matcher (JIT or python via kill-switch)
    fills: List[Fill] = simulate_matcher(bars, intents)
    t_sim1 = time.perf_counter() if profile else 0.0

    # --- Convert fills -> round-trip pnl (vectorized style, no python trade loops as truth) ---
    # For this minimal kernel we assume:
    # - Only LONG trades (BUY entry, SELL exit) will be produced once we add exits.
    # Phase 3A GKV: We implement exits by post-processing: when entry fills, schedule a sell stop from next bar.
    # To preserve Homology, we do a second matcher pass with generated exit intents.
    # This keeps all fill semantics inside the matcher (constitution).
    exit_intents: List[OrderIntent] = []
    for f in fills:
        if f.role != OrderRole.ENTRY:
            continue
        # exit stop price = entry_price - stop_mult * atr at entry bar
        ebar = int(f.bar_index)
        if ebar < 0 or ebar >= n:
            continue
        a = float(atr[ebar])
        if np.isnan(a):
            continue
        stop_px = float(f.price - stop_mult * a)
        # CURSOR TASK 5: Generate deterministic order_id for exit
        exit_oid = generate_order_id(
            created_bar=ebar,
            param_idx=0,  # Single param kernel
            role=ROLE_EXIT,
            kind=KIND_STOP,
            side=SIDE_SELL,
        )
        exit_intents.append(
            OrderIntent(
                order_id=exit_oid,
                created_bar=ebar,  # active next bar
                role=OrderRole.EXIT,
                kind=OrderKind.STOP,
                side=Side.SELL,
                price=stop_px,
                qty=order_qty,
            )
        )
    t_exit_intents = time.perf_counter() if profile else 0.0

    if exit_intents:
        fills2 = simulate_matcher(bars, exit_intents)
        t_sim2 = time.perf_counter() if profile else 0.0
        fills_all = fills + fills2
        # deterministic order: sort by (bar_index, role(ENTRY first), kind, order_id)
        fills_all.sort(key=lambda x: (x.bar_index, 0 if x.role == OrderRole.ENTRY else 1, 0 if x.kind == OrderKind.STOP else 1, x.order_id))
    else:
        fills_all = fills
        t_sim2 = t_sim1 if profile else 0.0

    # CURSOR TASK 1: Compute metrics from fills (unified source of truth)
    net_profit, trades, max_dd, equity = compute_metrics_from_fills(
        fills=fills_all,
        commission=commission,
        slip=slip,
        qty=order_qty,
    )
    
    # For backward compatibility, compute pnl array from equity (if needed)
    if equity.size > 0:
        pnl = np.diff(np.concatenate([[0.0], equity]))
    else:
        pnl = np.empty(0, dtype=np.float64)
    
    metrics = {
        "net_profit": net_profit,
        "trades": trades,
        "max_dd": max_dd,
    }
    out = {"fills": fills_all, "pnl": pnl, "equity": equity, "metrics": metrics}

    # Evidence fields (Source of Truth) - Phase 3.0-A
    # Red Team requirement: if fallback to objects mode, must leave fingerprint
    intents_total = int(len(intents) + len(exit_intents))  # Total intents (entry + exit, merged)
    fills_total = int(len(fills_all))  # fills_all is List[Fill], use len()
    
    # Always-on observability payload (no timing assumptions).
    out["_obs"] = {
        "intent_mode": "objects",
        "intents_total": intents_total,
        "fills_total": fills_total,
        "entry_intents": int(len(intents)),
        "exit_intents": int(len(exit_intents)),
    }

    if profile:
        out["_profile"] = {
            "intent_mode": "objects",
            "indicators_s": float(t_ind - t0),
            "intent_gen_s": float(t_intents - t_ind),
            "simulate_entry_s": float(t_sim1 - t_intents),
            "exit_intent_gen_s": float(t_exit_intents - t_sim1),
            "simulate_exit_s": float(t_sim2 - t_exit_intents),
            "kernel_total_s": float(t_sim2 - t0),
            "entry_intents": int(len(intents)),
            "exit_intents": int(len(exit_intents)),
        }
    return out


def run_kernel_arrays(
    bars: BarArrays,
    params: DonchianAtrParams,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
    return_debug: bool = False,
    precomp: Optional[PrecomputedIndicators] = None,
    intent_sparse_rate: float = 1.0,  # CURSOR TASK 3: Intent sparse rate from grid
) -> Dict[str, object]:
    """
    Array/SoA intent mode: generates intents as arrays and calls engine_jit.simulate_arrays().
    This avoids OrderIntent object construction in the hot path.
    
    Args:
        precomp: Optional pre-computed indicators. If provided, skips indicator computation
                 and uses precomputed arrays. If None, computes indicators normally (backward compatible).
    """
    profile = os.environ.get("FISHBRO_PROFILE_KERNEL", "").strip() == "1"
    t0 = time.perf_counter() if profile else 0.0
    
    # Stage P2-1.8: Initialize granular timers for breakdown
    from FishBroWFS_V2.perf.timers import PerfTimers
    timers = PerfTimers()
    timers.start("t_total_kernel")
    
    # Task 1A: Define required timing keys (contract enforcement)
    REQUIRED_TIMING_KEYS = (
        "t_calc_indicators_s",
        "t_build_entry_intents_s",
        "t_simulate_entry_s",
        "t_calc_exits_s",
        "t_simulate_exit_s",
        "t_total_kernel_s",
    )

    ch = int(params.channel_len)
    atr_n = int(params.atr_len)
    stop_mult = float(params.stop_mult)

    if ch <= 0 or atr_n <= 0:
        timers.stop("t_total_kernel")
        timing_dict = timers.as_dict_seconds()
        # Task 1B: Ensure all required timing keys exist (setdefault 0.0)
        for k in REQUIRED_TIMING_KEYS:
            timing_dict.setdefault(k, 0.0)
        pnl = np.empty(0, dtype=np.float64)
        equity = np.empty(0, dtype=np.float64)
        # Evidence fields (Source of Truth) - Phase 3.0-A: must not be null
        # Task 1C: Fix early return - inject timing_dict into _obs
        result = {
            "fills": [],
            "pnl": pnl,
            "equity": equity,
            "metrics": {"net_profit": 0.0, "trades": 0, "max_dd": 0.0},
            "_obs": {
                "intent_mode": "arrays",
                "intents_total": 0,
                "fills_total": 0,
                "entry_intents_total": 0,
                "entry_fills_total": 0,
                "exit_intents_total": 0,
                "exit_fills_total": 0,
                **timing_dict,  # Task 1C: Include timing keys in _obs
            },
            "_perf": timing_dict,
        }
        return result

    # Stage P2-2 Step B2: Use precomputed indicators if available, otherwise compute
    if precomp is not None:
        # Use precomputed indicators (skip computation, timing will be ~0)
        donch_hi = precomp.donch_hi
        donch_lo = precomp.donch_lo
        atr = precomp.atr
        # Still record timing (will be ~0 since we skipped computation)
        timers.start("t_ind_donchian")
        timers.stop("t_ind_donchian")
        timers.start("t_ind_atr")
        timers.stop("t_ind_atr")
    else:
        # Stage P2-2 Step A: Micro-profiling - Split indicators timing
        # t_ind_donchian_s: Donchian rolling max/min (highest/lowest)
        timers.start("t_ind_donchian")
        donch_hi = rolling_max(bars.high, ch)
        donch_lo = rolling_min(bars.low, ch)  # Also compute low for consistency
        timers.stop("t_ind_donchian")
        
        # t_ind_atr_s: ATR Wilder (TR + RMA/ATR)
        timers.start("t_ind_atr")
        atr = atr_wilder(bars.high, bars.low, bars.close, atr_n)
        timers.stop("t_ind_atr")
    
    t_ind = time.perf_counter() if profile else 0.0

    # Stage P2-1.8: t_build_entry_intents_s - Build entry intents (shift, mask, build)
    timers.start("t_build_entry_intents")
    # Fix 2: Shift donchian for next-bar active (created_bar = t-1, price = donch_hi[t-1])
    # Entry orders generated at bar t-1 close, active at bar t, stop price = donch_hi[t-1]
    donch_prev = np.empty_like(donch_hi)
    donch_prev[0] = np.nan
    donch_prev[1:] = donch_hi[:-1]

    # Stage P2-3A: Check if we should use Numba-accelerated sparse builder
    use_numba_builder = os.environ.get("FISHBRO_FORCE_SPARSE_BUILDER", "").strip() == "1"
    
    # CURSOR TASK 3: Use intent_sparse_rate from grid (passed as parameter)
    # Fallback to env var if not provided (for backward compatibility)
    trigger_rate = intent_sparse_rate
    if trigger_rate == 1.0:  # Only check env if not explicitly passed
        trigger_rate_env = os.environ.get("FISHBRO_PERF_TRIGGER_RATE", "").strip()
        if trigger_rate_env:
            try:
                trigger_rate = float(trigger_rate_env)
                if not (0.0 <= trigger_rate <= 1.0):
                    trigger_rate = 1.0
            except ValueError:
                trigger_rate = 1.0
    
    # Debug instrumentation: track first entry/exit per param (only if return_debug=True)
    if return_debug:
        dbg_entry_bar = -1
        dbg_entry_price = np.nan
        dbg_exit_bar = -1
        dbg_exit_price = np.nan
    else:
        dbg_entry_bar = None
        dbg_entry_price = None
        dbg_exit_bar = None
        dbg_exit_price = None

    # Build entry intents (choose builder based on env flags)
    use_dense_builder = os.environ.get("FISHBRO_USE_DENSE_BUILDER", "").strip() == "1"
    
    if use_numba_builder:
        # Stage P2-3A: Use Numba-accelerated sparse builder (trigger_rate integrated)
        from FishBroWFS_V2.strategy.entry_builder_nb import build_entry_intents_numba
        entry_intents_result = build_entry_intents_numba(
            donch_prev=donch_prev,
            channel_len=ch,
            order_qty=order_qty,
            trigger_rate=trigger_rate,
            seed=42,  # Fixed seed for deterministic selection
        )
        entry_builder_impl = "numba_single_pass"
    elif use_dense_builder:
        # Reference dense builder (for comparison/testing)
        entry_intents_result = _build_entry_intents_from_trigger(
            donch_prev=donch_prev,
            channel_len=ch,
            order_qty=order_qty,
        )
        entry_builder_impl = "python_dense_reference"
    else:
        # Default: Use new sparse builder (supports trigger_rate natively)
        from FishBroWFS_V2.strategy.builder_sparse import build_intents_sparse
        entry_intents_result = build_intents_sparse(
            donch_prev=donch_prev,
            channel_len=ch,
            order_qty=order_qty,
            trigger_rate=trigger_rate,  # CURSOR TASK 3: Use intent_sparse_rate
            seed=42,  # Fixed seed for deterministic selection
            use_dense=False,  # Use sparse mode (default)
        )
        entry_builder_impl = "python_sparse_default"
    timers.stop("t_build_entry_intents")
    
    created_bar = entry_intents_result["created_bar"]
    price = entry_intents_result["price"]
    # CURSOR TASK 5: Use deterministic order ID generation (pure function)
    # Override order_id from builder with deterministic version
    from FishBroWFS_V2.engine.order_id import generate_order_ids_array
    order_id = generate_order_ids_array(
        created_bar=created_bar,
        param_idx=0,  # Single param kernel (param_idx not available here)
        role=entry_intents_result.get("role"),
        kind=entry_intents_result.get("kind"),
        side=entry_intents_result.get("side"),
    )
    role = entry_intents_result["role"]
    kind = entry_intents_result["kind"]
    side = entry_intents_result["side"]
    qty = entry_intents_result["qty"]
    n_entry = entry_intents_result["n_entry"]
    obs_extra = entry_intents_result["obs"]
    
    # Stage P2-3A: Add builder implementation info to obs
    obs_extra = dict(obs_extra)  # Ensure mutable
    obs_extra["entry_builder_impl"] = entry_builder_impl
    
    if n_entry == 0:
        # No valid entry intents
        timers.stop("t_total_kernel")
        timing_dict = timers.as_dict_seconds()
        # Task 1B: Ensure all required timing keys exist (setdefault 0.0)
        for k in REQUIRED_TIMING_KEYS:
            timing_dict.setdefault(k, 0.0)
        pnl = np.empty(0, dtype=np.float64)
        equity = np.empty(0, dtype=np.float64)
        metrics = {"net_profit": 0.0, "trades": 0, "max_dd": 0.0}
        intents_total = 0
        fills_total = 0
        
        # CURSOR TASK 1: Set intents_total = entry_intents_total + exit_intents_total (accounting consistency)
        entry_intents_total_val = int(n_entry)  # 0 in this case
        exit_intents_total_val = 0  # No exit intents when n_entry == 0
        intents_total = entry_intents_total_val + exit_intents_total_val  # CURSOR TASK 1: Always sum
        
        # CURSOR TASK 4: Get entry_valid_mask_sum for MVP contract (bar-level indicator)
        entry_valid_mask_sum = int(obs_extra.get("entry_valid_mask_sum", obs_extra.get("valid_mask_sum", 0)))
        n_bars_val = int(obs_extra.get("n_bars", bars.open.shape[0]))
        warmup_val = int(obs_extra.get("warmup", ch))
        valid_mask_sum_val = int(obs_extra.get("valid_mask_sum", entry_valid_mask_sum))
        
        result = {
            "fills": [],
            "pnl": pnl,
            "equity": equity,
            "metrics": metrics,
            "_obs": {
                "intent_mode": "arrays",
                "intents_total": intents_total,  # CURSOR TASK 1: entry_intents_total + exit_intents_total
                "intents_total_reported": intents_total,  # Same as intents_total (0 in this case)
                "fills_total": fills_total,
                "entry_intents_total": entry_intents_total_val,  # CURSOR TASK 4: Required key
                "exit_intents_total": exit_intents_total_val,  # CURSOR TASK 1: Required for accounting consistency
                "entry_fills_total": 0,
                "exit_fills_total": 0,
                "n_bars": n_bars_val,  # CURSOR TASK 4: Required key
                "warmup": warmup_val,  # CURSOR TASK 4: Required key
                "valid_mask_sum": valid_mask_sum_val,  # CURSOR TASK 4: Required key
                "entry_valid_mask_sum": entry_valid_mask_sum,  # CURSOR TASK 4: Required key
                **obs_extra,  # Include diagnostic observations from entry intent builder
                **timing_dict,  # Stage P2-1.8: Include timing keys in _obs
            },
            "_perf": timing_dict,  # Keep _perf for backward compatibility
        }
        if return_debug:
            result["_debug"] = {
                "entry_bar": dbg_entry_bar,
                "entry_price": dbg_entry_price,
                "exit_bar": dbg_exit_bar,
                "exit_price": dbg_exit_price,
            }
        
        # --- P2-1.6 Observability alias (kernel-native) ---
        obs = result.setdefault("_obs", {})
        # Canonical entry sparse keys expected by perf/tests
        # CURSOR TASK 2: entry_valid_mask_sum should come from obs_extra (builder), not valid_mask_sum
        if "entry_valid_mask_sum" not in obs:
            obs.setdefault("entry_valid_mask_sum", int(obs.get("entry_valid_mask_sum", 0)))
        # entry_intents_total should already be set above (n_entry = 0 in this case)
        if "entry_intents_total" not in obs:
            obs["entry_intents_total"] = int(n_entry)
        
        return result

    # Arrays are already built by _build_entry_intents_from_trigger
    t_intents = time.perf_counter() if profile else 0.0

    # CURSOR TASK 2: Simulate entry intents first (parity with object-mode)
    # This ensures exit intents are only generated after entry fills occur
    timers.start("t_simulate_entry")
    entry_fills: List[Fill] = simulate_matcher_arrays(
        bars,
        order_id=order_id,
        created_bar=created_bar,
        role=role,
        kind=kind,
        side=side,
        price=price,
        qty=qty,
        ttl_bars=1,
    )
    timers.stop("t_simulate_entry")
    t_sim1 = time.perf_counter() if profile else 0.0

    # CURSOR TASK 2: Build exit intents from entry fills (not from entry intents)
    # This matches object-mode behavior: exit intents only generated after entry fills
    timers.start("t_calc_exits")
    from FishBroWFS_V2.config.dtypes import (
        INDEX_DTYPE,
        INTENT_ENUM_DTYPE,
        INTENT_PRICE_DTYPE,
    )
    
    # Build exit intents for each entry fill (parity with object-mode)
    exit_intents_list = []
    n_bars = int(bars.open.shape[0])
    for f in entry_fills:
        if f.role != OrderRole.ENTRY or f.side != Side.BUY:
            continue
        ebar = int(f.bar_index)
        if ebar < 0 or ebar >= n_bars:
            continue
        # Get ATR at entry fill bar
        atr_e = float(atr[ebar])
        if not np.isfinite(atr_e) or atr_e <= 0:
            # Invalid ATR: skip this entry (no exit intent)
            continue
        # Compute exit stop price from entry fill price
        exit_stop = float(f.price - stop_mult * atr_e)
        exit_intents_list.append({
            "created_bar": ebar,  # Same as entry fill bar (allows same-bar entry then exit)
            "price": exit_stop,
        })
    
    exit_intents_count = len(exit_intents_list)
    timers.stop("t_calc_exits")
    t_exit_intents = time.perf_counter() if profile else 0.0

    # CURSOR TASK 2 & 3: Simulate exit intents, then merge fills
    # Sort intents properly (created_bar, order_id) before simulate
    timers.start("t_simulate_exit")
    if exit_intents_count > 0:
        # Build exit intent arrays
        exit_created = np.asarray([ei["created_bar"] for ei in exit_intents_list], dtype=INDEX_DTYPE)
        exit_price = np.asarray([ei["price"] for ei in exit_intents_list], dtype=INTENT_PRICE_DTYPE)
        # CURSOR TASK 5: Use deterministic order ID generation for exit intents
        from FishBroWFS_V2.engine.order_id import generate_order_id
        exit_order_id_list = []
        for i, ebar in enumerate(exit_created):
            exit_oid = generate_order_id(
                created_bar=int(ebar),
                param_idx=0,  # Single param kernel
                role=ROLE_EXIT,
                kind=KIND_STOP,
                side=SIDE_SELL,
            )
            exit_order_id_list.append(exit_oid)
        exit_order_id = np.asarray(exit_order_id_list, dtype=INDEX_DTYPE)
        exit_role = np.full(exit_intents_count, ROLE_EXIT, dtype=INTENT_ENUM_DTYPE)
        exit_kind = np.full(exit_intents_count, KIND_STOP, dtype=INTENT_ENUM_DTYPE)
        exit_side = np.full(exit_intents_count, SIDE_SELL, dtype=INTENT_ENUM_DTYPE)
        exit_qty = np.full(exit_intents_count, int(order_qty), dtype=INDEX_DTYPE)
        
        # CURSOR TASK 3: Sort exit intents by created_bar, then order_id
        exit_sort_idx = np.lexsort((exit_order_id, exit_created))
        exit_order_id = exit_order_id[exit_sort_idx]
        exit_created = exit_created[exit_sort_idx]
        exit_price = exit_price[exit_sort_idx]
        exit_role = exit_role[exit_sort_idx]
        exit_kind = exit_kind[exit_sort_idx]
        exit_side = exit_side[exit_sort_idx]
        exit_qty = exit_qty[exit_sort_idx]
        
        # Simulate exit intents
        exit_fills: List[Fill] = simulate_matcher_arrays(
            bars,
            order_id=exit_order_id,
            created_bar=exit_created,
            role=exit_role,
            kind=exit_kind,
            side=exit_side,
            price=exit_price,
            qty=exit_qty,
            ttl_bars=1,
        )
        
        # Merge entry and exit fills, sort by (bar_index, role, kind, order_id)
        fills_all = entry_fills + exit_fills
        fills_all.sort(
            key=lambda x: (
                x.bar_index,
                0 if x.role == OrderRole.ENTRY else 1,
                0 if x.kind == OrderKind.STOP else 1,
                x.order_id,
            )
        )
    else:
        fills_all = entry_fills
    
    timers.stop("t_simulate_exit")
    t_sim2 = time.perf_counter() if profile else 0.0
    
    # Count entry and exit fills
    entry_fills_count = sum(1 for f in entry_fills if f.role == OrderRole.ENTRY and f.side == Side.BUY)
    if exit_intents_count > 0:
        exit_fills_count = sum(1 for f in fills_all if f.role == OrderRole.EXIT and f.side == Side.SELL)
    else:
        exit_fills_count = 0

    # Capture first entry fill for debug
    if return_debug and len(fills_all) > 0:
        first_entry = None
        for f in fills_all:
            if f.role == OrderRole.ENTRY and f.side == Side.BUY:
                first_entry = f
                break
        if first_entry is not None:
            dbg_entry_bar = int(first_entry.bar_index)
            dbg_entry_price = float(first_entry.price)

    # Capture first exit fill for debug
    if return_debug and len(fills_all) > 0:
        first_exit = None
        for f in fills_all:
            if f.role == OrderRole.EXIT and f.side == Side.SELL:
                first_exit = f
                break
        if first_exit is not None:
            dbg_exit_bar = int(first_exit.bar_index)
            dbg_exit_price = float(first_exit.price)

    # CURSOR TASK 1: Compute metrics from fills (unified source of truth)
    net_profit, trades, max_dd, equity = compute_metrics_from_fills(
        fills=fills_all,
        commission=commission,
        slip=slip,
        qty=order_qty,
    )
    
    # For backward compatibility, compute pnl array from equity (if needed)
    if equity.size > 0:
        pnl = np.diff(np.concatenate([[0.0], equity]))
    else:
        pnl = np.empty(0, dtype=np.float64)
    
    metrics = {
        "net_profit": net_profit,
        "trades": trades,
        "max_dd": max_dd,
    }
    out = {"fills": fills_all, "pnl": pnl, "equity": equity, "metrics": metrics}

    # Evidence fields (Source of Truth) - Phase 3.0-A
    raw_intents_total = int(n_entry + exit_intents_count)  # Total raw intents (entry + exit)
    fills_total = int(len(fills_all))  # fills_all is List[Fill], use len()
    timers.stop("t_total_kernel")
    
    # Stage P2-1.8: Get timing dict and merge into _obs for aggregation
    timing_dict = timers.as_dict_seconds()
    # Task 1B: Ensure all required timing keys exist (setdefault 0.0)
    for k in REQUIRED_TIMING_KEYS:
        timing_dict.setdefault(k, 0.0)
    
    # CURSOR TASK 1: Set intents_total = entry_intents_total + exit_intents_total (accounting consistency)
    entry_intents_total_val = int(n_entry)
    exit_intents_total_val = int(exit_intents_count)
    intents_total = entry_intents_total_val + exit_intents_total_val  # CURSOR TASK 1: Always sum
    
    # CURSOR TASK 4: Get entry_valid_mask_sum for MVP contract (bar-level indicator)
    entry_valid_mask_sum = int(obs_extra.get("entry_valid_mask_sum", obs_extra.get("valid_mask_sum", 0)))
    
    # CURSOR TASK 2: Ensure entry_intents_total is set correctly (from n_entry, not valid_mask_sum)
    # Override any value from obs_extra with actual n_entry
    obs_extra_final = dict(obs_extra)  # Copy to avoid modifying original
    obs_extra_final["entry_intents_total"] = entry_intents_total_val  # Always use actual n_entry
    
    # CURSOR TASK 4: Ensure all required obs keys exist
    n_bars_val = int(obs_extra_final.get("n_bars", bars.open.shape[0]))
    warmup_val = int(obs_extra_final.get("warmup", ch))
    valid_mask_sum_val = int(obs_extra_final.get("valid_mask_sum", entry_valid_mask_sum))
    
    out["_obs"] = {
        "intent_mode": "arrays",
        "intents_total": intents_total,  # CURSOR TASK 1: entry_intents_total + exit_intents_total
        "intents_total_reported": raw_intents_total,  # Raw intent count (same as intents_total for accounting)
        "fills_total": fills_total,
        "entry_intents": int(n_entry),
        "exit_intents": int(exit_intents_count),
        "n_bars": n_bars_val,  # CURSOR TASK 4: Required key
        "warmup": warmup_val,  # CURSOR TASK 4: Required key
        "valid_mask_sum": valid_mask_sum_val,  # CURSOR TASK 4: Required key (dense valid mask sum)
        "entry_valid_mask_sum": entry_valid_mask_sum,  # CURSOR TASK 4: Required key (after sparse)
        "entry_intents_total": entry_intents_total_val,  # CURSOR TASK 4: Required key
        "exit_intents_total": exit_intents_total_val,  # CURSOR TASK 1: Required for accounting consistency
        "entry_fills_total": int(entry_fills_count),
        "exit_fills_total": int(exit_fills_count),
        **obs_extra_final,  # Include diagnostic observations from entry intent builder
        **timing_dict,  # Stage P2-1.8: Include timing keys in _obs for aggregation
    }
    out["_perf"] = timing_dict  # Keep _perf for backward compatibility
    if return_debug:
        out["_debug"] = {
            "entry_bar": dbg_entry_bar,
            "entry_price": dbg_entry_price,
            "exit_bar": dbg_exit_bar,
            "exit_price": dbg_exit_price,
        }
    if profile:
        # CURSOR TASK 2: Separate simulate calls (entry then exit), timing reflects actual calls
        out["_profile"] = {
            "intent_mode": "arrays",
            "indicators_s": float(t_ind - t0),
            "intent_gen_s": float(t_intents - t_ind),
            "simulate_entry_s": float(t_sim1 - t_intents),  # Entry simulation time
            "exit_intent_gen_s": float(t_exit_intents - t_sim1),  # Exit intent generation time
            "simulate_exit_s": float(t_sim2 - t_exit_intents),  # Exit simulation time
            "kernel_total_s": float(t_sim2 - t0),
            "entry_intents": int(n_entry),
            "exit_intents": int(exit_intents_count),
        }
    
    # --- P2-1.6 Observability alias (kernel-native) ---
    obs = out.setdefault("_obs", {})
    # Canonical entry sparse keys expected by perf/tests
    # CURSOR TASK 2: entry_valid_mask_sum should come from obs_extra (builder), not valid_mask_sum
    if "entry_valid_mask_sum" not in obs:
        obs.setdefault("entry_valid_mask_sum", int(obs.get("entry_valid_mask_sum", 0)))
    # entry_intents_total should already be set from obs_extra (n_entry)
    if "entry_intents_total" not in obs:
        obs["entry_intents_total"] = int(n_entry)
    
    return out


def run_kernel(
    bars: BarArrays,
    params: DonchianAtrParams,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
    return_debug: bool = False,
    precomp: Optional[PrecomputedIndicators] = None,
    intent_sparse_rate: float = 1.0,  # CURSOR TASK 3: Intent sparse rate from grid
) -> Dict[str, object]:
    # Default to arrays path for perf; object mode remains as a correctness reference.
    mode = os.environ.get("FISHBRO_KERNEL_INTENT_MODE", "").strip().lower()
    if mode == "objects":
        return run_kernel_object_mode(
            bars,
            params,
            commission=commission,
            slip=slip,
            order_qty=order_qty,
        )
    return run_kernel_arrays(
        bars,
        params,
        commission=commission,
        slip=slip,
        order_qty=order_qty,
        return_debug=return_debug,
        precomp=precomp,
    )



================================================================================
FILE: src/FishBroWFS_V2/strategy/runner_single.py
================================================================================

from __future__ import annotations

from typing import Dict

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.types import BarArrays
from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, run_kernel


def run_single(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params: DonchianAtrParams,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
) -> Dict[str, object]:
    """
    Wrapper for Phase 3A (GKV): ensure memory layout + call kernel once.
    """
    bars: BarArrays = normalize_bars(open_, high, low, close)

    # Boundary Layout Check: enforce contiguous arrays before entering kernel.
    if not bars.open.flags["C_CONTIGUOUS"]:
        bars = BarArrays(
            open=np.ascontiguousarray(bars.open, dtype=np.float64),
            high=np.ascontiguousarray(bars.high, dtype=np.float64),
            low=np.ascontiguousarray(bars.low, dtype=np.float64),
            close=np.ascontiguousarray(bars.close, dtype=np.float64),
        )

    return run_kernel(bars, params, commission=commission, slip=slip, order_qty=order_qty)



================================================================================
FILE: src/FishBroWFS_V2/version.py
================================================================================

__version__ = "0.1.0"



================================================================================
FILE: tests/__init__.py
================================================================================

"""
Tests package for FishBroWFS_V2.

This package allows tests to import from each other using:
    from tests.test_module import ...
"""


================================================================================
FILE: tests/conftest.py
================================================================================

"""
Pytest configuration and fixtures.

Ensures PYTHONPATH is set correctly for imports.
"""
from __future__ import annotations

import sys
from pathlib import Path

# Add src/ to Python path if not already present
# This ensures tests can import FishBroWFS_V2 without manual PYTHONPATH setup
repo_root = Path(__file__).parent.parent
src_path = repo_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))


================================================================================
FILE: tests/test_artifact_contract.py
================================================================================

"""Contract tests for artifact system.

Tests verify:
1. Directory structure contract
2. File existence and format
3. JSON serialization correctness (sorted keys)
4. param_subsample_rate visibility (mandatory in manifest/metrics/README)
5. Winners schema stability
"""

from __future__ import annotations

import json
import tempfile
from datetime import datetime, timezone
from pathlib import Path

import pytest

from FishBroWFS_V2.core.artifacts import write_run_artifacts
from FishBroWFS_V2.core.audit_schema import AuditSchema, compute_params_effective
from FishBroWFS_V2.core.config_hash import stable_config_hash
from FishBroWFS_V2.core.paths import ensure_run_dir, get_run_dir
from FishBroWFS_V2.core.run_id import make_run_id


def test_artifact_tree_contract():
    """Test that artifact directory structure follows contract."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        run_id = make_run_id()
        
        run_dir = ensure_run_dir(outputs_root, season, run_id)
        
        # Verify directory structure
        expected_path = outputs_root / "seasons" / season / "runs" / run_id
        assert run_dir == expected_path
        assert expected_path.exists()
        assert expected_path.is_dir()
        
        # Verify get_run_dir returns same path
        assert get_run_dir(outputs_root, season, run_id) == expected_path


def test_manifest_must_include_param_subsample_rate():
    """Test that manifest.json must include param_subsample_rate."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        config = {"n_bars": 1000, "n_params": 100}
        param_subsample_rate = 0.1
        params_total = 100
        params_effective = compute_params_effective(params_total, param_subsample_rate)
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=param_subsample_rate,
            config_hash=stable_config_hash(config),
            season=season,
            dataset_id="test_dataset",
            bars=1000,
            params_total=params_total,
            params_effective=params_effective,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=config,
            metrics={"param_subsample_rate": param_subsample_rate},
        )
        
        # Read and verify manifest
        manifest_path = run_dir / "manifest.json"
        assert manifest_path.exists()
        
        with open(manifest_path, "r", encoding="utf-8") as f:
            manifest_data = json.load(f)
        
        # Verify param_subsample_rate exists and is correct
        assert "param_subsample_rate" in manifest_data
        assert manifest_data["param_subsample_rate"] == 0.1
        
        # Verify all audit fields are present
        assert "run_id" in manifest_data
        assert "created_at" in manifest_data
        assert "git_sha" in manifest_data
        assert "dirty_repo" in manifest_data
        assert "config_hash" in manifest_data


def test_config_snapshot_is_json_serializable():
    """Test that config_snapshot.json is valid JSON with sorted keys."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        config = {
            "n_bars": 1000,
            "n_params": 100,
            "commission": 0.0,
            "slip": 0.0,
        }
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=1.0,
            config_hash=stable_config_hash(config),
            season=season,
            dataset_id="test_dataset",
            bars=1000,
            params_total=100,
            params_effective=100,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=config,
            metrics={"param_subsample_rate": 1.0},
        )
        
        config_path = run_dir / "config_snapshot.json"
        assert config_path.exists()
        
        # Verify JSON is valid and has sorted keys
        with open(config_path, "r", encoding="utf-8") as f:
            config_data = json.load(f)
        
        # Verify keys are sorted (JSON should be written with sort_keys=True)
        keys = list(config_data.keys())
        assert keys == sorted(keys), "Config keys should be sorted"
        
        # Verify content matches
        assert config_data == config


def test_metrics_must_include_param_subsample_rate():
    """Test that metrics.json must include param_subsample_rate visibility."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        param_subsample_rate = 0.25
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=param_subsample_rate,
            config_hash="test_hash",
            season=season,
            dataset_id="test_dataset",
            bars=20000,
            params_total=1000,
            params_effective=250,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        metrics = {
            "param_subsample_rate": param_subsample_rate,
            "runtime_s": 12.345,
            "throughput": 27777777.78,
        }
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot={"test": "config"},
            metrics=metrics,
        )
        
        metrics_path = run_dir / "metrics.json"
        assert metrics_path.exists()
        
        with open(metrics_path, "r", encoding="utf-8") as f:
            metrics_data = json.load(f)
        
        # Verify param_subsample_rate exists
        assert "param_subsample_rate" in metrics_data
        assert metrics_data["param_subsample_rate"] == 0.25


def test_winners_structure_contract():
    """Test that winners.json has fixed structure versioned."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=1.0,
            config_hash="test_hash",
            season=season,
            dataset_id="test_dataset",
            bars=1000,
            params_total=100,
            params_effective=100,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot={"test": "config"},
            metrics={"param_subsample_rate": 1.0},
        )
        
        winners_path = run_dir / "winners.json"
        assert winners_path.exists()
        
        with open(winners_path, "r", encoding="utf-8") as f:
            winners_data = json.load(f)
        
        # Verify fixed structure
        assert "topk" in winners_data
        assert isinstance(winners_data["topk"], list)
        
        # Verify schema version (v1 or v2)
        notes = winners_data.get("notes", {})
        schema = notes.get("schema")
        assert schema in ("v1", "v2"), f"Schema must be v1 or v2, got {schema}"
        
        # If v2, must include 'schema' at top level too
        if schema == "v2":
            assert winners_data.get("schema") == "v2"
        
        assert winners_data["topk"] == []  # Initially empty


def test_readme_must_display_param_subsample_rate():
    """Test that README.md prominently displays param_subsample_rate."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        param_subsample_rate = 0.33
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=param_subsample_rate,
            config_hash="test_hash_123",
            season=season,
            dataset_id="test_dataset",
            bars=20000,
            params_total=1000,
            params_effective=330,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot={"test": "config"},
            metrics={"param_subsample_rate": param_subsample_rate},
        )
        
        readme_path = run_dir / "README.md"
        assert readme_path.exists()
        
        with open(readme_path, "r", encoding="utf-8") as f:
            readme_content = f.read()
        
        # Verify param_subsample_rate is prominently displayed
        assert "param_subsample_rate" in readme_content
        assert "0.33" in readme_content
        
        # Verify other required fields
        assert "run_id" in readme_content
        assert "git_sha" in readme_content
        assert "season" in readme_content
        assert "dataset_id" in readme_content
        assert "bars" in readme_content
        assert "params_total" in readme_content
        assert "params_effective" in readme_content
        assert "config_hash" in readme_content


def test_logs_file_exists():
    """Test that logs.txt file is created."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=1.0,
            config_hash="test_hash",
            season=season,
            dataset_id="test_dataset",
            bars=1000,
            params_total=100,
            params_effective=100,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot={"test": "config"},
            metrics={"param_subsample_rate": 1.0},
        )
        
        logs_path = run_dir / "logs.txt"
        assert logs_path.exists()
        
        # Initially empty
        with open(logs_path, "r", encoding="utf-8") as f:
            assert f.read() == ""


def test_all_artifacts_exist():
    """Test that all required artifacts are created."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=0.1,
            config_hash="test_hash",
            season=season,
            dataset_id="test_dataset",
            bars=20000,
            params_total=1000,
            params_effective=100,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot={"test": "config"},
            metrics={"param_subsample_rate": 0.1},
        )
        
        # Verify all artifacts exist
        artifacts = [
            "manifest.json",
            "config_snapshot.json",
            "metrics.json",
            "winners.json",
            "README.md",
            "logs.txt",
        ]
        
        for artifact_name in artifacts:
            artifact_path = run_dir / artifact_name
            assert artifact_path.exists(), f"Missing artifact: {artifact_name}"


def test_json_files_have_sorted_keys():
    """Test that all JSON files are written with sorted keys."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        config = {
            "z_field": "last",
            "a_field": "first",
            "m_field": "middle",
        }
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=1.0,
            config_hash=stable_config_hash(config),
            season=season,
            dataset_id="test_dataset",
            bars=1000,
            params_total=100,
            params_effective=100,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=config,
            metrics={"param_subsample_rate": 1.0},
        )
        
        # Check config_snapshot.json has sorted keys
        config_path = run_dir / "config_snapshot.json"
        with open(config_path, "r", encoding="utf-8") as f:
            config_data = json.load(f)
        
        keys = list(config_data.keys())
        assert keys == sorted(keys), "Config keys should be sorted"
        
        # Check manifest.json has sorted keys
        manifest_path = run_dir / "manifest.json"
        with open(manifest_path, "r", encoding="utf-8") as f:
            manifest_data = json.load(f)
        
        manifest_keys = list(manifest_data.keys())
        assert manifest_keys == sorted(manifest_keys), "Manifest keys should be sorted"


================================================================================
FILE: tests/test_artifacts_winners_v2_written.py
================================================================================

"""Contract tests for artifacts winners v2 writing.

Tests verify that write_run_artifacts automatically upgrades legacy winners to v2.
"""

from __future__ import annotations

import json
import tempfile
from datetime import datetime, timezone
from pathlib import Path

from FishBroWFS_V2.core.artifacts import write_run_artifacts
from FishBroWFS_V2.core.audit_schema import AuditSchema, compute_params_effective
from FishBroWFS_V2.core.config_hash import stable_config_hash
from FishBroWFS_V2.core.run_id import make_run_id
from FishBroWFS_V2.core.winners_schema import is_winners_v2


def test_artifacts_upgrades_legacy_winners_to_v2() -> None:
    """Test that write_run_artifacts upgrades legacy winners to v2."""
    with tempfile.TemporaryDirectory() as tmpdir:
        run_dir = Path(tmpdir) / "run_test"
        
        # Create audit schema
        config = {"n_bars": 1000, "n_params": 100}
        param_subsample_rate = 0.1
        params_total = 100
        params_effective = compute_params_effective(params_total, param_subsample_rate)
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="abc123def456",
            dirty_repo=False,
            param_subsample_rate=param_subsample_rate,
            config_hash=stable_config_hash(config),
            season="test_season",
            dataset_id="test_dataset",
            bars=1000,
            params_total=params_total,
            params_effective=params_effective,
        )
        
        # Legacy winners format
        legacy_winners = {
            "topk": [
                {"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0},
                {"param_id": 1, "net_profit": 200.0, "trades": 20, "max_dd": -20.0},
            ],
            "notes": {"schema": "v1"},
        }
        
        # Write artifacts
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=config,
            metrics={
                "param_subsample_rate": param_subsample_rate,
                "stage_name": "stage1_topk",
            },
            winners=legacy_winners,
        )
        
        # Read winners.json
        winners_path = run_dir / "winners.json"
        assert winners_path.exists()
        
        with winners_path.open("r", encoding="utf-8") as f:
            winners = json.load(f)
        
        # Verify it's v2 schema
        assert is_winners_v2(winners) is True
        assert winners["schema"] == "v2"
        assert winners["stage_name"] == "stage1_topk"
        
        # Verify topk items are v2 format
        topk = winners["topk"]
        assert len(topk) == 2
        
        for item in topk:
            assert "candidate_id" in item
            assert "strategy_id" in item
            assert "symbol" in item
            assert "timeframe" in item
            assert "params" in item
            assert "score" in item
            assert "metrics" in item
            assert "source" in item
            
            # Verify legacy fields are in metrics
            metrics = item["metrics"]
            assert "net_profit" in metrics
            assert "max_dd" in metrics
            assert "trades" in metrics
            assert "param_id" in metrics


def test_artifacts_writes_v2_when_winners_is_none() -> None:
    """Test that write_run_artifacts creates v2 format when winners is None."""
    with tempfile.TemporaryDirectory() as tmpdir:
        run_dir = Path(tmpdir) / "run_test"
        
        # Create audit schema
        config = {"n_bars": 1000, "n_params": 100}
        param_subsample_rate = 0.1
        params_total = 100
        params_effective = compute_params_effective(params_total, param_subsample_rate)
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="abc123def456",
            dirty_repo=False,
            param_subsample_rate=param_subsample_rate,
            config_hash=stable_config_hash(config),
            season="test_season",
            dataset_id="test_dataset",
            bars=1000,
            params_total=params_total,
            params_effective=params_effective,
        )
        
        # Write artifacts with winners=None
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=config,
            metrics={
                "param_subsample_rate": param_subsample_rate,
                "stage_name": "stage0_coarse",
            },
            winners=None,
        )
        
        # Read winners.json
        winners_path = run_dir / "winners.json"
        assert winners_path.exists()
        
        with winners_path.open("r", encoding="utf-8") as f:
            winners = json.load(f)
        
        # Verify it's v2 schema (even when empty)
        assert is_winners_v2(winners) is True
        assert winners["schema"] == "v2"
        assert winners["topk"] == []


def test_artifacts_preserves_legacy_metrics_fields() -> None:
    """Test that legacy metrics fields are preserved in v2 format."""
    with tempfile.TemporaryDirectory() as tmpdir:
        run_dir = Path(tmpdir) / "run_test"
        
        # Create audit schema
        config = {"n_bars": 1000, "n_params": 100}
        param_subsample_rate = 0.1
        params_total = 100
        params_effective = compute_params_effective(params_total, param_subsample_rate)
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="abc123def456",
            dirty_repo=False,
            param_subsample_rate=param_subsample_rate,
            config_hash=stable_config_hash(config),
            season="test_season",
            dataset_id="test_dataset",
            bars=1000,
            params_total=params_total,
            params_effective=params_effective,
        )
        
        # Legacy winners with proxy_value (Stage0)
        legacy_winners = {
            "topk": [
                {"param_id": 0, "proxy_value": 1.234},
            ],
            "notes": {"schema": "v1"},
        }
        
        # Write artifacts
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=config,
            metrics={
                "param_subsample_rate": param_subsample_rate,
                "stage_name": "stage0_coarse",
            },
            winners=legacy_winners,
        )
        
        # Read winners.json
        winners_path = run_dir / "winners.json"
        with winners_path.open("r", encoding="utf-8") as f:
            winners = json.load(f)
        
        # Verify legacy fields are preserved
        item = winners["topk"][0]
        metrics = item["metrics"]
        
        # proxy_value should be in metrics
        assert "proxy_value" in metrics
        assert metrics["proxy_value"] == 1.234
        
        # param_id should be in metrics (for backward compatibility)
        assert "param_id" in metrics
        assert metrics["param_id"] == 0


================================================================================
FILE: tests/test_audit_schema_contract.py
================================================================================

"""Contract tests for audit schema.

Tests verify:
1. JSON serialization correctness
2. Run ID format stability
3. Config hash consistency
4. params_effective calculation rule consistency
"""

from __future__ import annotations

import json
from datetime import datetime, timezone

import pytest

from FishBroWFS_V2.core.audit_schema import (
    AuditSchema,
    compute_params_effective,
)
from FishBroWFS_V2.core.config_hash import stable_config_hash
from FishBroWFS_V2.core.run_id import make_run_id


def test_audit_schema_json_serializable():
    """Test that AuditSchema can be serialized to JSON."""
    audit = AuditSchema(
        run_id=make_run_id(),
        created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        git_sha="a1b2c3d4e5f6",
        dirty_repo=False,
        param_subsample_rate=0.1,
        config_hash="f9e8d7c6b5a4a3b2c1d0e9f8a7b6c5d4e3f2a1b0c9d8e7f6a5b4c3d2e1f0a9b8",
        season="2025Q4",
        dataset_id="synthetic_20k",
        bars=20000,
        params_total=1000,
        params_effective=100,
    )
    
    # Test to_dict()
    audit_dict = audit.to_dict()
    assert isinstance(audit_dict, dict)
    assert "param_subsample_rate" in audit_dict
    
    # Test JSON serialization
    audit_json = json.dumps(audit_dict)
    assert isinstance(audit_json, str)
    
    # Test JSON deserialization
    loaded_dict = json.loads(audit_json)
    assert loaded_dict["param_subsample_rate"] == 0.1
    assert loaded_dict["run_id"] == audit.run_id


def test_run_id_is_stable_format():
    """Test that run_id has stable, parseable format."""
    run_id = make_run_id()
    
    # Verify format: YYYYMMDDTHHMMSSZ-token
    assert len(run_id) > 15  # At least timestamp + dash + token
    assert "T" in run_id  # ISO format separator
    assert "Z" in run_id  # UTC timezone indicator
    assert run_id.count("-") >= 1  # At least one dash before token
    
    # Verify timestamp part is sortable
    parts = run_id.split("-")
    timestamp_part = parts[0] if len(parts) > 1 else run_id.split("Z")[0] + "Z"
    assert len(timestamp_part) >= 15  # YYYYMMDDTHHMMSSZ
    
    # Test with prefix
    prefixed_run_id = make_run_id(prefix="test")
    assert prefixed_run_id.startswith("test-")
    assert "T" in prefixed_run_id
    assert "Z" in prefixed_run_id


def test_config_hash_is_stable():
    """Test that config hash is stable and consistent."""
    config1 = {
        "n_bars": 20000,
        "n_params": 1000,
        "commission": 0.0,
    }
    
    config2 = {
        "commission": 0.0,
        "n_bars": 20000,
        "n_params": 1000,
    }
    
    # Same config with different key order should produce same hash
    hash1 = stable_config_hash(config1)
    hash2 = stable_config_hash(config2)
    assert hash1 == hash2
    
    # Different config should produce different hash
    config3 = {"n_bars": 20001, "n_params": 1000}
    hash3 = stable_config_hash(config3)
    assert hash1 != hash3
    
    # Verify hash format (64 hex chars for SHA256)
    assert len(hash1) == 64
    assert all(c in "0123456789abcdef" for c in hash1)


def test_params_effective_rounding_rule_is_stable():
    """
    Test that params_effective calculation rule is stable and locked.
    
    Rule: int(params_total * param_subsample_rate) (floor)
    """
    # Test cases: (params_total, subsample_rate, expected_effective)
    test_cases = [
        (1000, 0.0, 0),
        (1000, 0.1, 100),
        (1000, 0.15, 150),
        (1000, 0.5, 500),
        (1000, 0.99, 990),
        (1000, 1.0, 1000),
        (100, 0.1, 10),
        (100, 0.33, 33),  # Floor: 33.0 -> 33
        (100, 0.34, 34),  # Floor: 34.0 -> 34
        (100, 0.999, 99),  # Floor: 99.9 -> 99
    ]
    
    for params_total, subsample_rate, expected in test_cases:
        result = compute_params_effective(params_total, subsample_rate)
        assert result == expected, (
            f"Failed for params_total={params_total}, "
            f"subsample_rate={subsample_rate}: "
            f"expected={expected}, got={result}"
        )
    
    # Test edge case: invalid subsample_rate
    with pytest.raises(ValueError):
        compute_params_effective(1000, 1.1)  # > 1.0
    
    with pytest.raises(ValueError):
        compute_params_effective(1000, -0.1)  # < 0.0


def test_manifest_must_include_param_subsample_rate():
    """Test that manifest must include param_subsample_rate."""
    audit = AuditSchema(
        run_id=make_run_id(),
        created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        git_sha="a1b2c3d4e5f6",
        dirty_repo=False,
        param_subsample_rate=0.25,
        config_hash="test_hash",
        season="2025Q4",
        dataset_id="test_dataset",
        bars=20000,
        params_total=1000,
        params_effective=250,
    )
    
    manifest_dict = audit.to_dict()
    
    # Verify param_subsample_rate exists and is correct type
    assert "param_subsample_rate" in manifest_dict
    assert isinstance(manifest_dict["param_subsample_rate"], float)
    assert manifest_dict["param_subsample_rate"] == 0.25
    
    # Verify all required fields exist
    required_fields = [
        "run_id",
        "created_at",
        "git_sha",
        "dirty_repo",
        "param_subsample_rate",
        "config_hash",
        "season",
        "dataset_id",
        "bars",
        "params_total",
        "params_effective",
        "artifact_version",
    ]
    
    for field in required_fields:
        assert field in manifest_dict, f"Missing required field: {field}"


def test_created_at_is_iso8601_utc():
    """Test that created_at uses ISO8601 UTC format with Z suffix."""
    audit = AuditSchema(
        run_id=make_run_id(),
        created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        git_sha="a1b2c3d4e5f6",
        dirty_repo=False,
        param_subsample_rate=0.1,
        config_hash="test_hash",
        season="2025Q4",
        dataset_id="test_dataset",
        bars=20000,
        params_total=1000,
        params_effective=100,
    )
    
    created_at = audit.created_at
    
    # Verify Z suffix (UTC indicator)
    assert created_at.endswith("Z"), f"created_at should end with Z, got: {created_at}"
    
    # Verify ISO8601 format (can parse)
    try:
        # Remove Z and parse
        dt_str = created_at.replace("Z", "+00:00")
        parsed = datetime.fromisoformat(dt_str)
        assert parsed.tzinfo is not None
    except ValueError as e:
        pytest.fail(f"created_at is not valid ISO8601: {created_at}, error: {e}")


def test_audit_schema_is_frozen():
    """Test that AuditSchema is frozen (immutable)."""
    audit = AuditSchema(
        run_id=make_run_id(),
        created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        git_sha="a1b2c3d4e5f6",
        dirty_repo=False,
        param_subsample_rate=0.1,
        config_hash="test_hash",
        season="2025Q4",
        dataset_id="test_dataset",
        bars=20000,
        params_total=1000,
        params_effective=100,
    )
    
    # Verify frozen (cannot modify)
    with pytest.raises(Exception):  # dataclass.FrozenInstanceError
        audit.run_id = "new_id"


================================================================================
FILE: tests/test_b5_query_params.py
================================================================================

"""Tests for B5 Streamlit querystring parameter parsing."""

from __future__ import annotations

import json
import tempfile
from pathlib import Path

import pytest

from FishBroWFS_V2.core.artifact_reader import read_artifact


@pytest.fixture
def temp_outputs_root() -> Path:
    """Create temporary outputs root directory."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


@pytest.fixture
def sample_run_dir(temp_outputs_root: Path) -> Path:
    """Create a sample run directory with artifacts."""
    season = "2026Q1"
    run_id = "stage0_coarse-20251218T093512Z-d3caa754"
    
    run_dir = temp_outputs_root / "seasons" / season / "runs" / run_id
    run_dir.mkdir(parents=True, exist_ok=True)
    
    # Create minimal manifest.json
    manifest = {
        "run_id": run_id,
        "season": season,
        "config_hash": "test_hash",
        "created_at": "2025-12-18T09:35:12Z",
        "git_sha": "abc123def456",
        "dirty_repo": False,
        "param_subsample_rate": 0.1,
        "bars": 1000,
        "params_total": 100,
        "params_effective": 10,
        "artifact_version": "v1",
    }
    
    (run_dir / "manifest.json").write_text(
        json.dumps(manifest, indent=2), encoding="utf-8"
    )
    
    # Create minimal metrics.json
    metrics = {
        "stage_name": "stage0_coarse",
        "bars": 1000,
        "params_total": 100,
        "params_effective": 10,
        "param_subsample_rate": 0.1,
    }
    (run_dir / "metrics.json").write_text(
        json.dumps(metrics, indent=2), encoding="utf-8"
    )
    
    # Create minimal winners.json
    winners = {
        "topk": [],
        "notes": {"schema": "v1"},
    }
    (run_dir / "winners.json").write_text(
        json.dumps(winners, indent=2), encoding="utf-8"
    )
    
    return run_dir


def test_report_link_format() -> None:
    """Test that report_link format is correct."""
    from FishBroWFS_V2.control.report_links import make_report_link
    
    season = "2026Q1"
    run_id = "stage0_coarse-20251218T093512Z-d3caa754"
    
    link = make_report_link(season=season, run_id=run_id)
    
    assert link.startswith("/?")
    assert f"season={season}" in link
    assert f"run_id={run_id}" in link


def test_run_dir_path_construction(temp_outputs_root: Path, sample_run_dir: Path) -> None:
    """Test that run directory path is constructed correctly."""
    season = "2026Q1"
    run_id = "stage0_coarse-20251218T093512Z-d3caa754"
    
    # Construct path using same logic as Streamlit app
    run_dir = temp_outputs_root / "seasons" / season / "runs" / run_id
    
    assert run_dir.exists()
    assert run_dir == sample_run_dir


def test_artifacts_readable_from_run_dir(sample_run_dir: Path) -> None:
    """Test that artifacts can be read from run directory."""
    # Read manifest
    manifest_result = read_artifact(sample_run_dir / "manifest.json")
    assert manifest_result.raw["run_id"] == "stage0_coarse-20251218T093512Z-d3caa754"
    assert manifest_result.raw["season"] == "2026Q1"
    
    # Read metrics
    metrics_result = read_artifact(sample_run_dir / "metrics.json")
    assert metrics_result.raw["stage_name"] == "stage0_coarse"
    
    # Read winners
    winners_result = read_artifact(sample_run_dir / "winners.json")
    assert winners_result.raw["notes"]["schema"] == "v1"


def test_querystring_parsing_logic() -> None:
    """Test querystring parsing logic (simulating Streamlit query_params)."""
    # Simulate Streamlit query_params.get() behavior
    query_params = {
        "season": "2026Q1",
        "run_id": "stage0_coarse-20251218T093512Z-d3caa754",
    }
    
    season = query_params.get("season", "")
    run_id = query_params.get("run_id", "")
    
    assert season == "2026Q1"
    assert run_id == "stage0_coarse-20251218T093512Z-d3caa754"
    
    # Test missing parameters
    empty_params = {}
    season_empty = empty_params.get("season", "")
    run_id_empty = empty_params.get("run_id", "")
    
    assert season_empty == ""
    assert run_id_empty == ""


================================================================================
FILE: tests/test_baseline_lock.py
================================================================================

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.engine_jit import simulate as simulate_jit
from FishBroWFS_V2.engine.matcher_core import simulate as simulate_py
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def _fills_to_matrix(fills):
    # Columns: bar_index, role, kind, side, price, qty, order_id
    m = np.empty((len(fills), 7), dtype=np.float64)
    for i, f in enumerate(fills):
        m[i, 0] = float(f.bar_index)
        m[i, 1] = 0.0 if f.role == OrderRole.EXIT else 1.0
        m[i, 2] = 0.0 if f.kind == OrderKind.STOP else 1.0
        m[i, 3] = float(int(f.side.value))
        m[i, 4] = float(f.price)
        m[i, 5] = float(f.qty)
        m[i, 6] = float(f.order_id)
    return m


def test_gate_a_jit_matches_python_reference():
    # Two bars so we can test next-bar active + entry then exit.
    bars = normalize_bars(
        np.array([100.0, 100.0], dtype=np.float64),
        np.array([120.0, 120.0], dtype=np.float64),
        np.array([90.0, 80.0], dtype=np.float64),
        np.array([110.0, 90.0], dtype=np.float64),
    )

    intents = [
        # Entry active on bar0
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),
        # Exit active on bar0 (same bar), should execute after entry
        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),
        # Entry created on bar0 -> active on bar1
        OrderIntent(order_id=3, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=110.0),
    ]

    py = simulate_py(bars, intents)
    jit = simulate_jit(bars, intents)

    m_py = _fills_to_matrix(py)
    m_jit = _fills_to_matrix(jit)

    assert m_py.shape == m_jit.shape
    # Event-level exactness except price tolerance
    np.testing.assert_array_equal(m_py[:, [0, 1, 2, 3, 5, 6]], m_jit[:, [0, 1, 2, 3, 5, 6]])
    np.testing.assert_allclose(m_py[:, 4], m_jit[:, 4], rtol=0.0, atol=1e-9)



================================================================================
FILE: tests/test_builder_sparse_contract.py
================================================================================

"""
Contract Tests for Sparse Builder (P2-3)

Verifies sparse intent builder behavior:
- Intent scaling with trigger_rate
- Metrics zeroing for non-selected params
- Seed determinism
"""
from __future__ import annotations

import numpy as np
import os

from FishBroWFS_V2.strategy.builder_sparse import build_intents_sparse


def test_builder_intent_scaling_with_intent_sparse_rate() -> None:
    """
    Test that intents scale approximately linearly with trigger_rate.
    
    Verifies that when trigger_rate=0.05, intents_generated is approximately
    5% of allowed_bars (with tolerance for rounding).
    """
    n_bars = 1000
    channel_len = 20
    order_qty = 1
    
    # Generate synthetic donch_prev array (all valid after warmup)
    donch_prev = np.full(n_bars, 100.0, dtype=np.float64)
    donch_prev[0] = np.nan  # First bar is NaN (shifted)
    # Bars 1..channel_len-1 are valid but before warmup
    # Bars channel_len..n_bars-1 are valid and past warmup
    
    # Run dense (trigger_rate=1.0) - baseline
    result_dense = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=1.0,
        seed=42,
        use_dense=False,
    )
    
    # Run sparse (trigger_rate=0.05) - 5% of triggers
    result_sparse = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=0.05,
        seed=42,
        use_dense=False,
    )
    
    obs_dense = result_dense["obs"]
    obs_sparse = result_sparse["obs"]
    
    allowed_bars_dense = obs_dense.get("allowed_bars")
    intents_generated_dense = obs_dense.get("intents_generated")
    allowed_bars_sparse = obs_sparse.get("allowed_bars")
    intents_generated_sparse = obs_sparse.get("intents_generated")
    valid_mask_sum_dense = obs_dense.get("valid_mask_sum")
    valid_mask_sum_sparse = obs_sparse.get("valid_mask_sum")
    
    # Contract: allowed_bars should be the same (represents valid bars before trigger rate)
    # allowed_bars = valid_mask_sum (baseline, for comparison)
    assert allowed_bars_dense == allowed_bars_sparse, (
        f"allowed_bars should be the same for dense and sparse (both equal valid_mask_sum), "
        f"got {allowed_bars_dense} vs {allowed_bars_sparse}"
    )
    assert valid_mask_sum_dense == valid_mask_sum_sparse, (
        f"valid_mask_sum should be the same for dense and sparse, "
        f"got {valid_mask_sum_dense} vs {valid_mask_sum_sparse}"
    )
    
    # Contract: intents_generated should scale approximately with trigger_rate
    # With trigger_rate=0.05, we expect approximately 5% of valid_mask_sum
    # Allow wide tolerance: [0.02, 0.08] (2% to 8% of valid_mask_sum)
    if valid_mask_sum_dense is not None and valid_mask_sum_dense > 0:
        ratio = intents_generated_sparse / valid_mask_sum_sparse
        assert 0.02 <= ratio <= 0.08, (
            f"With trigger_rate=0.05, intents_generated_sparse ({intents_generated_sparse}) "
            f"should be approximately 5% of valid_mask_sum ({valid_mask_sum_sparse}), "
            f"got ratio {ratio:.4f} (expected [0.02, 0.08])"
        )
    
    # Contract: intents_generated_dense should equal valid_mask_sum (trigger_rate=1.0)
    assert intents_generated_dense == valid_mask_sum_dense, (
        f"With trigger_rate=1.0, intents_generated ({intents_generated_dense}) "
        f"should equal valid_mask_sum ({valid_mask_sum_dense})"
    )


def test_metrics_zeroing_for_non_selected_params() -> None:
    """
    Test that builder correctly handles edge cases (no valid triggers, etc.).
    
    This test verifies that the builder returns empty arrays when there are
    no valid triggers, and that all fields are properly initialized.
    """
    n_bars = 100
    channel_len = 50  # Large warmup, so most bars are invalid
    order_qty = 1
    
    # Generate donch_prev with only a few valid bars
    donch_prev = np.full(n_bars, np.nan, dtype=np.float64)
    donch_prev[0] = np.nan  # First bar is NaN (shifted)
    # Set a few bars to valid values (after warmup)
    donch_prev[60] = 100.0
    donch_prev[70] = 100.0
    donch_prev[80] = 100.0
    
    result = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=1.0,
        seed=42,
        use_dense=False,
    )
    
    # Contract: Should have some intents (3 valid bars after warmup)
    assert result["n_entry"] > 0, "Should have some intents for valid bars"
    
    # Contract: All arrays should have same length
    assert len(result["created_bar"]) == result["n_entry"]
    assert len(result["price"]) == result["n_entry"]
    assert len(result["order_id"]) == result["n_entry"]
    assert len(result["role"]) == result["n_entry"]
    assert len(result["kind"]) == result["n_entry"]
    assert len(result["side"]) == result["n_entry"]
    assert len(result["qty"]) == result["n_entry"]
    
    # Contract: Test with trigger_rate=0.0 (should return empty)
    result_empty = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=0.0,
        seed=42,
        use_dense=False,
    )
    
    assert result_empty["n_entry"] == 0, "With trigger_rate=0.0, should have no intents"
    assert len(result_empty["created_bar"]) == 0
    assert len(result_empty["price"]) == 0


def test_seed_determinism_builder_output() -> None:
    """
    Test that builder output is deterministic for same seed.
    
    Verifies that running the builder twice with the same seed produces
    identical results (bit-exact).
    """
    n_bars = 500
    channel_len = 20
    order_qty = 1
    trigger_rate = 0.1  # 10% of triggers
    
    # Generate synthetic donch_prev array
    donch_prev = np.full(n_bars, 100.0, dtype=np.float64)
    donch_prev[0] = np.nan  # First bar is NaN (shifted)
    
    # Run twice with same seed
    result1 = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=trigger_rate,
        seed=42,
        use_dense=False,
    )
    
    result2 = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=trigger_rate,
        seed=42,
        use_dense=False,
    )
    
    # Contract: Results should be bit-exact identical
    assert result1["n_entry"] == result2["n_entry"], (
        f"n_entry should be identical, got {result1['n_entry']} vs {result2['n_entry']}"
    )
    
    if result1["n_entry"] > 0:
        assert np.array_equal(result1["created_bar"], result2["created_bar"]), (
            "created_bar should be bit-exact identical"
        )
        assert np.array_equal(result1["price"], result2["price"]), (
            "price should be bit-exact identical"
        )
        assert np.array_equal(result1["order_id"], result2["order_id"]), (
            "order_id should be bit-exact identical"
        )
    
    # Contract: Different seeds should produce different results (for sparse mode)
    result3 = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=trigger_rate,
        seed=123,  # Different seed
        use_dense=False,
    )
    
    # With different seed, results may differ (but should still be deterministic)
    # We just verify that the builder runs without error
    assert isinstance(result3["n_entry"], int)
    assert result3["n_entry"] >= 0


def test_dense_vs_sparse_parity() -> None:
    """
    Test that dense builder (use_dense=True) produces same results as sparse with trigger_rate=1.0.
    
    Verifies that the dense reference implementation matches sparse builder
    when trigger_rate=1.0.
    """
    n_bars = 200
    channel_len = 20
    order_qty = 1
    
    # Generate synthetic donch_prev array
    donch_prev = np.full(n_bars, 100.0, dtype=np.float64)
    donch_prev[0] = np.nan  # First bar is NaN (shifted)
    
    # Run dense builder
    result_dense = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=1.0,
        seed=42,
        use_dense=True,
    )
    
    # Run sparse builder with trigger_rate=1.0
    result_sparse = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=1.0,
        seed=42,
        use_dense=False,
    )
    
    # Contract: Results should be identical (both use all valid triggers)
    assert result_dense["n_entry"] == result_sparse["n_entry"], (
        f"n_entry should be identical, got {result_dense['n_entry']} vs {result_sparse['n_entry']}"
    )
    
    if result_dense["n_entry"] > 0:
        assert np.array_equal(result_dense["created_bar"], result_sparse["created_bar"]), (
            "created_bar should be identical"
        )
        assert np.array_equal(result_dense["price"], result_sparse["price"]), (
            "price should be identical"
        )


================================================================================
FILE: tests/test_control_api_smoke.py
================================================================================

"""Smoke tests for API endpoints."""

from __future__ import annotations

import tempfile
from pathlib import Path

import pytest
from fastapi.testclient import TestClient

from FishBroWFS_V2.control.api import app, get_db_path
from FishBroWFS_V2.control.jobs_db import init_db


@pytest.fixture
def test_client() -> TestClient:
    """Create test client with temporary database."""
    import os
    
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = Path(tmpdir) / "test.db"
        init_db(db_path)
        
        # Override DB path
        os.environ["JOBS_DB_PATH"] = str(db_path)
        
        # Re-import to get new DB path
        from FishBroWFS_V2.control import api
        
        # Reinitialize
        api.init_db(db_path)
        
        yield TestClient(app)


def test_health_endpoint(test_client: TestClient) -> None:
    """Test health endpoint."""
    resp = test_client.get("/health")
    assert resp.status_code == 200
    assert resp.json() == {"status": "ok"}


def test_create_job_endpoint(test_client: TestClient) -> None:
    """Test creating a job."""
    req = {
        "season": "test_season",
        "dataset_id": "test_dataset",
        "outputs_root": "outputs",
        "config_snapshot": {"bars": 1000, "params_total": 100},
        "config_hash": "abc123",
        "created_by": "b5c",
    }
    
    resp = test_client.post("/jobs", json=req)
    assert resp.status_code == 200
    data = resp.json()
    assert "job_id" in data
    assert isinstance(data["job_id"], str)


def test_list_jobs_endpoint(test_client: TestClient) -> None:
    """Test listing jobs."""
    # Create a job first
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    test_client.post("/jobs", json=req)
    
    # List jobs
    resp = test_client.get("/jobs")
    assert resp.status_code == 200
    jobs = resp.json()
    assert isinstance(jobs, list)
    assert len(jobs) > 0
    # Check that all jobs have report_link field
    for job in jobs:
        assert "report_link" in job


def test_get_job_endpoint(test_client: TestClient) -> None:
    """Test getting a job by ID."""
    # Create a job
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Get job
    resp = test_client.get(f"/jobs/{job_id}")
    assert resp.status_code == 200
    job = resp.json()
    assert job["job_id"] == job_id
    assert job["status"] == "QUEUED"
    assert "report_link" in job
    assert job["report_link"] is None  # Default is None


def test_check_endpoint(test_client: TestClient) -> None:
    """Test check endpoint."""
    # Create a job
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
            "mem_limit_mb": 6000.0,
        },
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Check
    resp = test_client.post(f"/jobs/{job_id}/check")
    assert resp.status_code == 200
    result = resp.json()
    assert "action" in result
    assert "estimated_mb" in result
    assert "estimates" in result


def test_pause_endpoint(test_client: TestClient) -> None:
    """Test pause endpoint."""
    # Create a job
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Pause
    resp = test_client.post(f"/jobs/{job_id}/pause", json={"pause": True})
    assert resp.status_code == 200
    
    # Unpause
    resp = test_client.post(f"/jobs/{job_id}/pause", json={"pause": False})
    assert resp.status_code == 200


def test_stop_endpoint(test_client: TestClient) -> None:
    """Test stop endpoint."""
    # Create a job
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Stop (soft)
    resp = test_client.post(f"/jobs/{job_id}/stop", json={"mode": "SOFT"})
    assert resp.status_code == 200
    
    # Stop (kill)
    req2 = {
        "season": "test2",
        "dataset_id": "test2",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash2",
    }
    create_resp2 = test_client.post("/jobs", json=req2)
    job_id2 = create_resp2.json()["job_id"]
    
    resp = test_client.post(f"/jobs/{job_id2}/stop", json={"mode": "KILL"})
    assert resp.status_code == 200


def test_log_tail_endpoint(test_client: TestClient) -> None:
    """Test log_tail endpoint."""
    import os
    
    # Create a job
    req = {
        "season": "test_season",
        "dataset_id": "test_dataset",
        "outputs_root": str(Path.cwd() / "outputs"),
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Create log file manually
    from FishBroWFS_V2.control.paths import run_log_path
    
    outputs_root = Path.cwd() / "outputs"
    log_path = run_log_path(outputs_root, "test_season", job_id)
    log_path.write_text("Line 1\nLine 2\nLine 3\n", encoding="utf-8")
    
    # Get log tail
    resp = test_client.get(f"/jobs/{job_id}/log_tail?n=200")
    assert resp.status_code == 200
    data = resp.json()
    assert data["ok"] is True
    assert isinstance(data["lines"], list)
    assert len(data["lines"]) == 3
    assert "Line 1" in data["lines"][0]
    
    # Cleanup
    log_path.unlink(missing_ok=True)


def test_log_tail_missing_file(test_client: TestClient) -> None:
    """Test log_tail endpoint when log file doesn't exist."""
    # Create a job
    req = {
        "season": "test_season",
        "dataset_id": "test_dataset",
        "outputs_root": str(Path.cwd() / "outputs"),
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Get log tail (file doesn't exist)
    resp = test_client.get(f"/jobs/{job_id}/log_tail?n=200")
    assert resp.status_code == 200
    data = resp.json()
    assert data["ok"] is True
    assert data["lines"] == []
    assert data["truncated"] is False


def test_report_link_endpoint(test_client: TestClient) -> None:
    """Test report_link endpoint."""
    from FishBroWFS_V2.control.jobs_db import set_report_link
    
    # Create a job
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Set report_link manually
    import os
    db_path = Path(os.environ["JOBS_DB_PATH"])
    set_report_link(db_path, job_id, "/b5?season=test&run_id=abc123")
    
    # Get report_link
    resp = test_client.get(f"/jobs/{job_id}/report_link")
    assert resp.status_code == 200
    data = resp.json()
    # build_report_link always returns a string (never None)
    assert data["report_link"] == "/b5?season=test&run_id=abc123"


def test_report_link_endpoint_no_link(test_client: TestClient) -> None:
    """Test report_link endpoint when no link exists."""
    # Create a job
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Get report_link (no run_id set)
    resp = test_client.get(f"/jobs/{job_id}/report_link")
    assert resp.status_code == 200
    data = resp.json()
    # build_report_link always returns a string (never None)
    assert data["report_link"] == ""



================================================================================
FILE: tests/test_control_jobs_db.py
================================================================================

"""Tests for jobs database."""

from __future__ import annotations

import tempfile
from pathlib import Path

import pytest

from FishBroWFS_V2.control.jobs_db import (
    create_job,
    get_job,
    get_requested_pause,
    get_requested_stop,
    init_db,
    list_jobs,
    mark_done,
    mark_failed,
    mark_killed,
    request_pause,
    request_stop,
    update_running,
)
from FishBroWFS_V2.control.types import JobSpec, JobStatus, StopMode


@pytest.fixture
def temp_db() -> Path:
    """Create temporary database for testing."""
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = Path(tmpdir) / "test.db"
        init_db(db_path)
        yield db_path


def test_init_db_creates_table(temp_db: Path) -> None:
    """Test that init_db creates the jobs table."""
    assert temp_db.exists()
    
    import sqlite3
    
    conn = sqlite3.connect(str(temp_db))
    cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='jobs'")
    assert cursor.fetchone() is not None
    conn.close()


def test_create_job_and_get(temp_db: Path) -> None:
    """Test creating and retrieving a job."""
    spec = JobSpec(
        season="test_season",
        dataset_id="test_dataset",
        outputs_root="outputs",
        config_snapshot={"bars": 1000, "params_total": 100},
        config_hash="abc123",
    )
    
    job_id = create_job(temp_db, spec)
    assert job_id
    
    job = get_job(temp_db, job_id)
    assert job.job_id == job_id
    assert job.status == JobStatus.QUEUED
    assert job.spec.season == "test_season"
    assert job.spec.dataset_id == "test_dataset"
    assert job.report_link is None  # Default is None


def test_list_jobs(temp_db: Path) -> None:
    """Test listing jobs."""
    spec = JobSpec(
        season="test",
        dataset_id="test",
        outputs_root="outputs",
        config_snapshot={},
        config_hash="hash1",
    )
    
    job_id1 = create_job(temp_db, spec)
    job_id2 = create_job(temp_db, spec)
    
    jobs = list_jobs(temp_db, limit=10)
    assert len(jobs) == 2
    assert {j.job_id for j in jobs} == {job_id1, job_id2}
    # Check that all jobs have report_link field
    for job in jobs:
        assert hasattr(job, "report_link")
        assert job.report_link is None  # Default is None


def test_request_pause(temp_db: Path) -> None:
    """Test pause request."""
    spec = JobSpec(
        season="test",
        dataset_id="test",
        outputs_root="outputs",
        config_snapshot={},
        config_hash="hash1",
    )
    job_id = create_job(temp_db, spec)
    
    request_pause(temp_db, job_id, pause=True)
    assert get_requested_pause(temp_db, job_id) is True
    
    request_pause(temp_db, job_id, pause=False)
    assert get_requested_pause(temp_db, job_id) is False


def test_request_stop(temp_db: Path) -> None:
    """Test stop request."""
    spec = JobSpec(
        season="test",
        dataset_id="test",
        outputs_root="outputs",
        config_snapshot={},
        config_hash="hash1",
    )
    job_id = create_job(temp_db, spec)
    
    request_stop(temp_db, job_id, StopMode.SOFT)
    assert get_requested_stop(temp_db, job_id) == "SOFT"
    
    request_stop(temp_db, job_id, StopMode.KILL)
    assert get_requested_stop(temp_db, job_id) == "KILL"
    
    # QUEUED job should be immediately KILLED
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.KILLED


def test_status_transitions(temp_db: Path) -> None:
    """Test status transitions."""
    spec = JobSpec(
        season="test",
        dataset_id="test",
        outputs_root="outputs",
        config_snapshot={},
        config_hash="hash1",
    )
    job_id = create_job(temp_db, spec)
    
    # QUEUED -> RUNNING
    update_running(temp_db, job_id, pid=12345)
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.RUNNING
    assert job.pid == 12345
    
    # RUNNING -> DONE
    mark_done(temp_db, job_id)
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.DONE
    
    # Cannot transition from DONE
    with pytest.raises(ValueError, match="Cannot transition from terminal status"):
        update_running(temp_db, job_id, pid=12345)


def test_mark_failed(temp_db: Path) -> None:
    """Test marking job as failed."""
    spec = JobSpec(
        season="test",
        dataset_id="test",
        outputs_root="outputs",
        config_snapshot={},
        config_hash="hash1",
    )
    job_id = create_job(temp_db, spec)
    update_running(temp_db, job_id, pid=12345)
    
    mark_failed(temp_db, job_id, error="Test error")
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.FAILED
    assert job.last_error == "Test error"


def test_mark_killed(temp_db: Path) -> None:
    """Test marking job as killed."""
    spec = JobSpec(
        season="test",
        dataset_id="test",
        outputs_root="outputs",
        config_snapshot={},
        config_hash="hash1",
    )
    job_id = create_job(temp_db, spec)
    
    mark_killed(temp_db, job_id, error="Killed by user")
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.KILLED
    assert job.last_error == "Killed by user"



================================================================================
FILE: tests/test_control_preflight.py
================================================================================

"""Tests for preflight check."""

from __future__ import annotations

import pytest

from FishBroWFS_V2.control.preflight import PreflightResult, run_preflight


def test_run_preflight_returns_required_keys() -> None:
    """Test that preflight returns all required keys."""
    cfg_snapshot = {
        "season": "test",
        "dataset_id": "test",
        "bars": 1000,
        "params_total": 100,
        "param_subsample_rate": 0.1,
        "mem_limit_mb": 6000.0,
        "allow_auto_downsample": True,
    }
    
    result = run_preflight(cfg_snapshot)
    
    assert isinstance(result, PreflightResult)
    assert result.action in {"PASS", "BLOCK", "AUTO_DOWNSAMPLE"}
    assert isinstance(result.reason, str)
    assert isinstance(result.original_subsample, float)
    assert isinstance(result.final_subsample, float)
    assert isinstance(result.estimated_bytes, int)
    assert isinstance(result.estimated_mb, float)
    assert isinstance(result.mem_limit_mb, float)
    assert isinstance(result.mem_limit_bytes, int)
    assert isinstance(result.estimates, dict)
    
    # Check estimates keys
    assert "ops_est" in result.estimates
    assert "time_est_s" in result.estimates
    assert "mem_est_mb" in result.estimates
    assert "mem_est_bytes" in result.estimates
    assert "mem_limit_mb" in result.estimates
    assert "mem_limit_bytes" in result.estimates


def test_preflight_pure_no_io() -> None:
    """Test that preflight is pure (no I/O)."""
    cfg_snapshot = {
        "season": "test",
        "dataset_id": "test",
        "bars": 100,
        "params_total": 10,
        "param_subsample_rate": 0.5,
        "mem_limit_mb": 10000.0,
    }
    
    # Should not raise any I/O errors
    result1 = run_preflight(cfg_snapshot)
    result2 = run_preflight(cfg_snapshot)
    
    # Should be deterministic
    assert result1.action == result2.action
    assert result1.estimated_bytes == result2.estimated_bytes



================================================================================
FILE: tests/test_control_worker_integration.py
================================================================================

"""Integration tests for worker execution and job completion."""

from __future__ import annotations

import tempfile
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

from FishBroWFS_V2.control.jobs_db import create_job, get_job, init_db
from FishBroWFS_V2.control.report_links import make_report_link
from FishBroWFS_V2.control.types import JobSpec, JobStatus
from FishBroWFS_V2.control.worker import run_one_job
from FishBroWFS_V2.pipeline.funnel_schema import (
    FunnelPlan,
    FunnelResultIndex,
    FunnelStageIndex,
    StageName,
    StageSpec,
)


@pytest.fixture
def temp_db() -> Path:
    """Create temporary database for testing."""
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = Path(tmpdir) / "test.db"
        init_db(db_path)
        yield db_path


@pytest.fixture
def temp_outputs_root() -> Path:
    """Create temporary outputs root directory."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


def test_worker_completes_job_with_run_id_and_report_link(
    temp_db: Path, temp_outputs_root: Path
) -> None:
    """Test that worker completes job and sets run_id and report_link."""
    # Create a job
    season = "2026Q1"
    spec = JobSpec(
        season=season,
        dataset_id="test_dataset",
        outputs_root=str(temp_outputs_root),
        config_snapshot={
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
        },
        config_hash="test_hash",
    )
    
    job_id = create_job(temp_db, spec)
    
    # Mock run_funnel to return a fake result
    fake_run_id = "stage2_confirm-20251218T093513Z-354cee6b"
    fake_stage_index = FunnelStageIndex(
        stage=StageName.STAGE2_CONFIRM,
        run_id=fake_run_id,
        run_dir=f"seasons/{season}/runs/{fake_run_id}",
    )
    fake_result_index = FunnelResultIndex(
        plan=FunnelPlan(stages=[]),
        stages=[fake_stage_index],
    )
    
    with patch("FishBroWFS_V2.control.worker.run_funnel") as mock_run_funnel:
        mock_run_funnel.return_value = fake_result_index
        
        # Run the job
        run_one_job(temp_db, job_id)
    
    # Check that job is marked as DONE
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.DONE
    assert job.run_id == fake_run_id
    assert job.report_link == make_report_link(season=season, run_id=fake_run_id)
    
    # Verify report_link format
    assert f"season={season}" in job.report_link
    assert f"run_id={fake_run_id}" in job.report_link


def test_worker_handles_empty_funnel_result(
    temp_db: Path, temp_outputs_root: Path
) -> None:
    """Test that worker handles empty funnel result gracefully."""
    spec = JobSpec(
        season="2026Q1",
        dataset_id="test_dataset",
        outputs_root=str(temp_outputs_root),
        config_snapshot={"bars": 1000, "params_total": 100},
        config_hash="test_hash",
    )
    
    job_id = create_job(temp_db, spec)
    
    # Mock run_funnel to return empty result
    fake_result_index = FunnelResultIndex(
        plan=FunnelPlan(stages=[]),
        stages=[],
    )
    
    with patch("FishBroWFS_V2.control.worker.run_funnel") as mock_run_funnel:
        mock_run_funnel.return_value = fake_result_index
        
        # Run the job
        run_one_job(temp_db, job_id)
    
    # Check that job is still marked as DONE (even without stages)
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.DONE
    # run_id and report_link should be None if no stages
    assert job.run_id is None
    assert job.report_link is None


================================================================================
FILE: tests/test_data_layout.py
================================================================================

import numpy as np
import pytest
from FishBroWFS_V2.data.layout import normalize_bars


def test_normalize_bars_dtype_and_contiguous():
    o = np.arange(10, dtype=np.float32)[::2]
    h = o + 1
    l = o - 1
    c = o + 0.5

    bars = normalize_bars(o, h, l, c)

    for arr in (bars.open, bars.high, bars.low, bars.close):
        assert arr.dtype == np.float64
        assert arr.flags["C_CONTIGUOUS"]


def test_normalize_bars_reject_nan():
    o = np.array([1.0, np.nan])
    h = np.array([1.0, 2.0])
    l = np.array([0.5, 1.5])
    c = np.array([0.8, 1.8])

    with pytest.raises(ValueError):
        normalize_bars(o, h, l, c)



================================================================================
FILE: tests/test_dtype_compression_contract.py
================================================================================

"""Contract tests for dtype compression (Phase P1).

These tests ensure:
1. INDEX_DTYPE=int32 safety: order_id, created_bar, qty never exceed 2^31-1
2. UINT8 enum consistency: role/kind/side correctly encode/decode without sentinel issues
"""

import numpy as np
import pytest

from FishBroWFS_V2.config.dtypes import (
    INDEX_DTYPE,
    INTENT_ENUM_DTYPE,
    INTENT_PRICE_DTYPE,
)
from FishBroWFS_V2.engine.constants import (
    KIND_LIMIT,
    KIND_STOP,
    ROLE_ENTRY,
    ROLE_EXIT,
    SIDE_BUY,
    SIDE_SELL,
)
from FishBroWFS_V2.engine.engine_jit import (
    SIDE_BUY_CODE,
    SIDE_SELL_CODE,
    _pack_intents,
    simulate_arrays,
)
from FishBroWFS_V2.engine.types import BarArrays, OrderIntent, OrderKind, OrderRole, Side


class TestIndexDtypeSafety:
    """Test that INDEX_DTYPE=int32 is safe for all use cases."""

    def test_order_id_max_value_contract(self):
        """
        Contract: order_id must never exceed 2^31-1 (int32 max).
        
        In strategy/kernel.py, order_id is generated as:
        - Entry: np.arange(1, n_entry + 1)
        - Exit: np.arange(n_entry + 1, n_entry + 1 + exit_intents_count)
        
        Maximum order_id = n_entry + exit_intents_count
        
        For 200,000 bars with reasonable intent generation, this should be << 2^31-1.
        """
        INT32_MAX = 2**31 - 1
        
        # Simulate worst-case scenario: 200,000 bars, each bar generates 1 entry + 1 exit
        # This is extremely conservative (realistic scenarios generate far fewer intents)
        n_bars = 200_000
        max_intents_per_bar = 2  # 1 entry + 1 exit per bar (worst case)
        max_total_intents = n_bars * max_intents_per_bar
        
        # Maximum order_id would be max_total_intents (if all are sequential)
        max_order_id = max_total_intents
        
        assert max_order_id < INT32_MAX, (
            f"order_id would exceed int32 max ({INT32_MAX}) "
            f"with {n_bars} bars and {max_intents_per_bar} intents per bar. "
            f"Max order_id would be {max_order_id}"
        )
        
        # More realistic: check that even with 10x safety margin, we're still safe
        safety_margin = 10
        assert max_order_id * safety_margin < INT32_MAX, (
            f"order_id with {safety_margin}x safety margin would exceed int32 max"
        )

    def test_created_bar_max_value_contract(self):
        """
        Contract: created_bar must never exceed 2^31-1.
        
        created_bar is a bar index, so max value = n_bars - 1.
        For 200,000 bars, max created_bar = 199,999 << 2^31-1.
        """
        INT32_MAX = 2**31 - 1
        
        # Worst case: 200,000 bars
        max_bars = 200_000
        max_created_bar = max_bars - 1
        
        assert max_created_bar < INT32_MAX, (
            f"created_bar would exceed int32 max ({INT32_MAX}) "
            f"with {max_bars} bars. Max created_bar would be {max_created_bar}"
        )

    def test_qty_max_value_contract(self):
        """
        Contract: qty must never exceed 2^31-1.
        
        qty is typically small (1, 10, 100, etc.), so this should be safe.
        """
        INT32_MAX = 2**31 - 1
        
        # Realistic qty values are much smaller than int32 max
        realistic_max_qty = 1_000_000  # Even 1M shares is << 2^31-1
        
        assert realistic_max_qty < INT32_MAX, (
            f"qty would exceed int32 max ({INT32_MAX}) "
            f"with realistic max qty of {realistic_max_qty}"
        )

    def test_order_id_generation_in_kernel(self):
        """
        Test that actual order_id generation in kernel stays within int32 range.
        
        This test simulates the order_id generation logic from strategy/kernel.py.
        """
        INT32_MAX = 2**31 - 1
        
        # Simulate realistic scenario: 200,000 bars, ~1000 entry intents, ~500 exit intents
        n_entry = 1000
        n_exit = 500
        
        # Entry order_ids: np.arange(1, n_entry + 1)
        entry_order_ids = np.arange(1, n_entry + 1, dtype=INDEX_DTYPE)
        assert entry_order_ids.max() < INT32_MAX
        
        # Exit order_ids: np.arange(n_entry + 1, n_entry + 1 + n_exit)
        exit_order_ids = np.arange(n_entry + 1, n_entry + 1 + n_exit, dtype=INDEX_DTYPE)
        max_order_id = exit_order_ids.max()
        
        assert max_order_id < INT32_MAX, (
            f"Generated order_id {max_order_id} exceeds int32 max ({INT32_MAX})"
        )


class TestUint8EnumConsistency:
    """Test that uint8 enum encoding/decoding is consistent and safe."""

    def test_role_enum_encoding(self):
        """Test that role enum values encode correctly as uint8."""
        # ROLE_EXIT = 0, ROLE_ENTRY = 1
        exit_val = INTENT_ENUM_DTYPE(ROLE_EXIT)
        entry_val = INTENT_ENUM_DTYPE(ROLE_ENTRY)
        
        assert exit_val == 0
        assert entry_val == 1
        assert exit_val.dtype == np.uint8
        assert entry_val.dtype == np.uint8

    def test_kind_enum_encoding(self):
        """Test that kind enum values encode correctly as uint8."""
        # KIND_STOP = 0, KIND_LIMIT = 1
        stop_val = INTENT_ENUM_DTYPE(KIND_STOP)
        limit_val = INTENT_ENUM_DTYPE(KIND_LIMIT)
        
        assert stop_val == 0
        assert limit_val == 1
        assert stop_val.dtype == np.uint8
        assert limit_val.dtype == np.uint8

    def test_side_enum_encoding(self):
        """
        Test that side enum values encode correctly as uint8.
        
        SIDE_BUY_CODE = 1, SIDE_SELL_CODE = 255 (avoid -1 cast deprecation)
        """
        buy_val = INTENT_ENUM_DTYPE(SIDE_BUY_CODE)
        sell_val = INTENT_ENUM_DTYPE(SIDE_SELL_CODE)
        
        assert buy_val == 1
        assert sell_val == 255
        assert buy_val.dtype == np.uint8
        assert sell_val.dtype == np.uint8

    def test_side_enum_decoding_consistency(self):
        """
        Test that side enum decoding correctly handles uint8 values.
        
        Critical: uint8 value 255 (SIDE_SELL_CODE) must decode back to SELL.
        """
        # Encode SIDE_SELL_CODE (255) as uint8
        sell_encoded = INTENT_ENUM_DTYPE(SIDE_SELL_CODE)
        assert sell_encoded == 255
        
        # Decode: int(sd[i]) == SIDE_BUY (1) ? BUY : SELL
        # If sd[i] = 255, int(255) != 1, so it should decode to SELL
        decoded_is_buy = int(sell_encoded) == SIDE_BUY
        decoded_is_sell = int(sell_encoded) != SIDE_BUY
        
        assert not decoded_is_buy, "uint8 value 255 should not decode to BUY"
        assert decoded_is_sell, "uint8 value 255 should decode to SELL"
        
        # Also test BUY encoding/decoding
        buy_encoded = INTENT_ENUM_DTYPE(SIDE_BUY_CODE)
        assert buy_encoded == 1
        decoded_is_buy = int(buy_encoded) == SIDE_BUY_CODE
        assert decoded_is_buy, "uint8 value 1 should decode to BUY"

    def test_allowed_enum_values_contract(self):
        """
        Contract: enum arrays must only contain explicitly allowed values.
        
        This test ensures that:
        1. Only valid enum values are used (no uninitialized/invalid values)
        2. Decoding functions will raise ValueError for invalid values (strict mode)
        
        Allowed values:
        - role: {0 (EXIT), 1 (ENTRY)}
        - kind: {0 (STOP), 1 (LIMIT)}
        - side: {1 (BUY), 255 (SELL as uint8)}
        """
        # Define allowed values explicitly
        ALLOWED_ROLE_VALUES = {ROLE_EXIT, ROLE_ENTRY}  # {0, 1}
        ALLOWED_KIND_VALUES = {KIND_STOP, KIND_LIMIT}  # {0, 1}
        ALLOWED_SIDE_VALUES = {SIDE_BUY_CODE, SIDE_SELL_CODE}  # {1, 255} - avoid -1 cast
        
        # Test that encoding produces only allowed values
        role_encoded = [INTENT_ENUM_DTYPE(ROLE_EXIT), INTENT_ENUM_DTYPE(ROLE_ENTRY)]
        kind_encoded = [INTENT_ENUM_DTYPE(KIND_STOP), INTENT_ENUM_DTYPE(KIND_LIMIT)]
        side_encoded = [INTENT_ENUM_DTYPE(SIDE_BUY_CODE), INTENT_ENUM_DTYPE(SIDE_SELL_CODE)]
        
        for val in role_encoded:
            assert int(val) in ALLOWED_ROLE_VALUES, f"Role value {val} not in allowed set {ALLOWED_ROLE_VALUES}"
        
        for val in kind_encoded:
            assert int(val) in ALLOWED_KIND_VALUES, f"Kind value {val} not in allowed set {ALLOWED_KIND_VALUES}"
        
        for val in side_encoded:
            assert int(val) in ALLOWED_SIDE_VALUES, f"Side value {val} not in allowed set {ALLOWED_SIDE_VALUES}"
        
        # Test that invalid values raise ValueError (strict decoding)
        from FishBroWFS_V2.engine.engine_jit import _role_from_int, _kind_from_int, _side_from_int
        
        # Test invalid role values
        with pytest.raises(ValueError, match="Invalid role enum value"):
            _role_from_int(2)
        with pytest.raises(ValueError, match="Invalid role enum value"):
            _role_from_int(-1)
        
        # Test invalid kind values
        with pytest.raises(ValueError, match="Invalid kind enum value"):
            _kind_from_int(2)
        with pytest.raises(ValueError, match="Invalid kind enum value"):
            _kind_from_int(-1)
        
        # Test invalid side values
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(0)
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(2)
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(100)
        
        # Test valid values don't raise
        assert _role_from_int(0) == OrderRole.EXIT
        assert _role_from_int(1) == OrderRole.ENTRY
        assert _kind_from_int(0) == OrderKind.STOP
        assert _kind_from_int(1) == OrderKind.LIMIT
        assert _side_from_int(SIDE_BUY_CODE) == Side.BUY
        assert _side_from_int(SIDE_SELL_CODE) == Side.SELL

    def test_pack_intents_roundtrip(self):
        """
        Test that packing intents and decoding them preserves enum values correctly.
        
        This is an integration test to ensure the full encode/decode cycle works.
        """
        # Create test intents with all enum combinations
        intents = [
            OrderIntent(
                order_id=1,
                created_bar=0,
                role=OrderRole.EXIT,
                kind=OrderKind.STOP,
                side=Side.SELL,  # -1 -> uint8(255)
                price=100.0,
                qty=1,
            ),
            OrderIntent(
                order_id=2,
                created_bar=0,
                role=OrderRole.ENTRY,
                kind=OrderKind.LIMIT,
                side=Side.BUY,  # 1 -> uint8(1)
                price=101.0,
                qty=1,
            ),
        ]
        
        # Pack intents
        order_id, created_bar, role, kind, side, price, qty = _pack_intents(intents)
        
        # Verify dtypes
        assert order_id.dtype == INDEX_DTYPE
        assert created_bar.dtype == INDEX_DTYPE
        assert role.dtype == INTENT_ENUM_DTYPE
        assert kind.dtype == INTENT_ENUM_DTYPE
        assert side.dtype == INTENT_ENUM_DTYPE
        assert price.dtype == INTENT_PRICE_DTYPE
        assert qty.dtype == INDEX_DTYPE
        
        # Verify enum values
        assert role[0] == ROLE_EXIT  # 0
        assert role[1] == ROLE_ENTRY  # 1
        assert kind[0] == KIND_STOP  # 0
        assert kind[1] == KIND_LIMIT  # 1
        assert side[0] == SIDE_SELL_CODE  # SELL -> uint8(255)
        assert side[1] == SIDE_BUY_CODE  # BUY -> uint8(1)
        
        # Verify decoding logic (as used in engine_jit.py)
        # Decode role
        decoded_role_0 = OrderRole.EXIT if int(role[0]) == ROLE_EXIT else OrderRole.ENTRY
        assert decoded_role_0 == OrderRole.EXIT
        
        decoded_role_1 = OrderRole.EXIT if int(role[1]) == ROLE_EXIT else OrderRole.ENTRY
        assert decoded_role_1 == OrderRole.ENTRY
        
        # Decode kind
        decoded_kind_0 = OrderKind.STOP if int(kind[0]) == KIND_STOP else OrderKind.LIMIT
        assert decoded_kind_0 == OrderKind.STOP
        
        decoded_kind_1 = OrderKind.STOP if int(kind[1]) == KIND_STOP else OrderKind.LIMIT
        assert decoded_kind_1 == OrderKind.LIMIT
        
        # Decode side (critical: uint8(255) must decode to SELL)
        decoded_side_0 = Side.BUY if int(side[0]) == SIDE_BUY_CODE else Side.SELL
        assert decoded_side_0 == Side.SELL, f"uint8(255) should decode to SELL, got {decoded_side_0}"
        
        decoded_side_1 = Side.BUY if int(side[1]) == SIDE_BUY_CODE else Side.SELL
        assert decoded_side_1 == Side.BUY, f"uint8(1) should decode to BUY, got {decoded_side_1}"

    def test_simulate_arrays_accepts_uint8_enums(self):
        """
        Test that simulate_arrays correctly accepts and processes uint8 enum arrays.
        
        This ensures the numba kernel can handle uint8 enum values correctly.
        """
        # Create minimal test data
        bars = BarArrays(
            open=np.array([100.0, 101.0], dtype=np.float64),
            high=np.array([102.0, 103.0], dtype=np.float64),
            low=np.array([99.0, 100.0], dtype=np.float64),
            close=np.array([101.0, 102.0], dtype=np.float64),
        )
        
        # Create intent arrays with uint8 enums
        order_id = np.array([1], dtype=INDEX_DTYPE)
        created_bar = np.array([0], dtype=INDEX_DTYPE)
        role = np.array([ROLE_ENTRY], dtype=INTENT_ENUM_DTYPE)
        kind = np.array([KIND_STOP], dtype=INTENT_ENUM_DTYPE)
        side = np.array([SIDE_BUY_CODE], dtype=INTENT_ENUM_DTYPE)  # 1 -> uint8(1)
        price = np.array([102.0], dtype=INTENT_PRICE_DTYPE)
        qty = np.array([1], dtype=INDEX_DTYPE)
        
        # This should not raise any dtype-related errors
        fills = simulate_arrays(
            bars,
            order_id=order_id,
            created_bar=created_bar,
            role=role,
            kind=kind,
            side=side,
            price=price,
            qty=qty,
            ttl_bars=1,
        )
        
        # Verify fills were generated (basic sanity check)
        assert isinstance(fills, list)
        
        # Test with SELL side (uint8 value 255)
        side_sell = np.array([SIDE_SELL_CODE], dtype=INTENT_ENUM_DTYPE)  # 255 (avoid -1 cast)
        fills_sell = simulate_arrays(
            bars,
            order_id=order_id,
            created_bar=created_bar,
            role=role,
            kind=kind,
            side=side_sell,
            price=price,
            qty=qty,
            ttl_bars=1,
        )
        
        # Should not raise errors
        assert isinstance(fills_sell, list)
        
        # Verify that fills with SELL side decode correctly
        # Note: numba kernel outputs uint8(255) as 255.0, but _side_from_int correctly decodes it
        if fills_sell:
            # The fill's side should be Side.SELL
            assert fills_sell[0].side == Side.SELL, (
                f"Fill with uint8(255) side should decode to Side.SELL, got {fills_sell[0].side}"
            )

    def test_side_output_value_contract(self):
        """
        Contract: numba kernel outputs side as float.
        
        Note: uint8(255) from SIDE_SELL will output as 255.0, not -1.0.
        This is acceptable as long as _side_from_int correctly decodes it.
        
        With strict mode, invalid values will raise ValueError instead of silently
        decoding to SELL.
        """
        from FishBroWFS_V2.engine.engine_jit import _side_from_int
        
        # Test that _side_from_int correctly handles allowed values
        assert _side_from_int(SIDE_BUY_CODE) == Side.BUY
        assert _side_from_int(SIDE_SELL_CODE) == Side.SELL, (
            f"_side_from_int({SIDE_SELL_CODE}) should decode to Side.SELL, not BUY"
        )
        
        # Test that invalid values raise ValueError (strict mode)
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(0)
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(-1)
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(2)
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(100)


================================================================================
FILE: tests/test_engine_constitution.py
================================================================================

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.matcher_core import simulate
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def _bars1(o, h, l, c):
    return normalize_bars(
        np.array([o], dtype=np.float64),
        np.array([h], dtype=np.float64),
        np.array([l], dtype=np.float64),
        np.array([c], dtype=np.float64),
    )


def _bars2(o0, h0, l0, c0, o1, h1, l1, c1):
    return normalize_bars(
        np.array([o0, o1], dtype=np.float64),
        np.array([h0, h1], dtype=np.float64),
        np.array([l0, l1], dtype=np.float64),
        np.array([c0, c1], dtype=np.float64),
    )


def test_tc01_buy_stop_normal():
    bars = _bars1(90, 105, 90, 100)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 100.0


def test_tc02_buy_stop_gap_up_fill_open():
    bars = _bars1(105, 110, 105, 108)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 105.0


def test_tc03_sell_stop_gap_down_fill_open():
    bars = _bars1(90, 95, 80, 85)
    intents = [
        # Exit a long position requires SELL stop; we will enter long first in same bar is not allowed here,
        # so we simulate already-in-position by forcing an entry earlier: created_bar=-2 triggers at -1 (ignored),
        # Instead: use two bars and enter on bar0, exit on bar1.
    ]
    bars2 = _bars2(
        100, 100, 100, 100,   # bar0: enter long at 100 (buy stop gap/normal both ok)
        90, 95, 80, 85        # bar1: exit stop triggers gap down open
    )
    intents2 = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=100.0),
    ]
    fills = simulate(bars2, intents2)
    assert len(fills) == 2
    # second fill is the exit
    assert fills[1].price == 90.0


def test_tc08_next_bar_active_not_same_bar():
    # bar0 has high 105 which would hit stop 102, but order created at bar0 must not fill at bar0.
    # bar1 hits again, should fill at bar1.
    bars = _bars2(
        100, 105, 95, 100,
        100, 105, 95, 100,
    )
    intents = [
        OrderIntent(order_id=1, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].bar_index == 1
    assert fills[0].price == 102.0


def test_tc09_open_equals_stop_gap_branch_but_same_price():
    bars = _bars1(100, 100, 90, 95)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 100.0


def test_tc10_no_fill_when_not_touched():
    bars = _bars1(90, 95, 90, 92)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert fills == []



================================================================================
FILE: tests/test_engine_gaps_and_priority.py
================================================================================

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.matcher_core import simulate
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def _bars1(o, h, l, c):
    return normalize_bars(
        np.array([o], dtype=np.float64),
        np.array([h], dtype=np.float64),
        np.array([l], dtype=np.float64),
        np.array([c], dtype=np.float64),
    )


def test_tc04_buy_limit_gap_down_better_fill_open():
    bars = _bars1(90, 95, 85, 92)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 90.0


def test_tc05_sell_limit_gap_up_better_fill_open():
    bars = _bars1(105, 110, 100, 108)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.SELL, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 105.0


def test_tc06_priority_stop_wins_over_limit_on_exit():
    # First enter long on this same bar, then exit on next bar where both stop and limit are triggerable.
    # Bar0: enter long at 100 (buy stop hits)
    # Bar1: both exit stop 90 and exit limit 110 are touchable (high=110, low=80), STOP must win (fill=90)
    bars = normalize_bars(
        np.array([100, 100], dtype=np.float64),
        np.array([110, 110], dtype=np.float64),
        np.array([90, 80], dtype=np.float64),
        np.array([100, 90], dtype=np.float64),
    )

    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=90.0),
        OrderIntent(order_id=3, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.LIMIT, side=Side.SELL, price=110.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 2
    # Second fill is exit; STOP wins -> 90
    assert fills[1].kind == OrderKind.STOP
    assert fills[1].price == 90.0


def test_tc07_same_bar_entry_then_exit():
    # Same bar allows Entry then Exit.
    # Bar: O=100 H=120 L=90 C=110
    # Entry: Buy Stop 105 -> fills at 105 (since open 100 < 105 and high 120 >= 105)
    # Exit: Sell Stop 95 -> after entry, low 90 <= 95 -> fills at 95
    bars = _bars1(100, 120, 90, 110)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),
        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 2
    assert fills[0].price == 105.0
    assert fills[1].price == 95.0



================================================================================
FILE: tests/test_engine_jit_active_book_contract.py
================================================================================

from __future__ import annotations

import os

import numpy as np
import pytest

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.engine_jit import _simulate_with_ttl, simulate as simulate_jit
from FishBroWFS_V2.engine.matcher_core import simulate as simulate_py
from FishBroWFS_V2.engine.types import Fill, OrderIntent, OrderKind, OrderRole, Side


def _assert_fills_equal(a: list[Fill], b: list[Fill]) -> None:
    assert len(a) == len(b)
    for fa, fb in zip(a, b):
        assert fa.bar_index == fb.bar_index
        assert fa.role == fb.role
        assert fa.kind == fb.kind
        assert fa.side == fb.side
        assert fa.qty == fb.qty
        assert fa.order_id == fb.order_id
        assert abs(fa.price - fb.price) <= 1e-9


def test_jit_sorted_invariance_matches_python() -> None:
    # Bars: 3 bars, deterministic highs/lows for STOP triggers
    bars = normalize_bars(
        np.array([100.0, 100.0, 100.0], dtype=np.float64),
        np.array([110.0, 110.0, 110.0], dtype=np.float64),
        np.array([90.0, 90.0, 90.0], dtype=np.float64),
        np.array([100.0, 100.0, 100.0], dtype=np.float64),
    )

    # Intents across multiple activate bars (created_bar = t-1)
    intents = [
        # activate on bar0 (created -1)
        OrderIntent(3, -1, OrderRole.EXIT, OrderKind.STOP, Side.SELL, 95.0, 1),
        OrderIntent(2, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 105.0, 1),
        # activate on bar1 (created 0)
        OrderIntent(6, 0, OrderRole.EXIT, OrderKind.LIMIT, Side.SELL, 110.0, 1),
        OrderIntent(5, 0, OrderRole.ENTRY, OrderKind.LIMIT, Side.BUY, 99.0, 1),
        # activate on bar2 (created 1)
        OrderIntent(9, 1, OrderRole.EXIT, OrderKind.STOP, Side.SELL, 90.0, 1),
        OrderIntent(8, 1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),
    ]

    shuffled = list(intents)
    rng = np.random.default_rng(123)
    rng.shuffle(shuffled)

    # JIT simulate sorts internally for cursor+book; it must be invariant to input ordering.
    jit_a = simulate_jit(bars, shuffled)
    jit_b = simulate_jit(bars, intents)
    _assert_fills_equal(jit_a, jit_b)

    # Also must match Python reference semantics.
    py = simulate_py(bars, shuffled)
    _assert_fills_equal(jit_a, py)


def test_one_bar_max_one_entry_one_exit_defense() -> None:
    # Single bar is enough: created_bar=-1 activates on bar 0.
    bars = normalize_bars(
        np.array([100.0], dtype=np.float64),
        np.array([120.0], dtype=np.float64),
        np.array([80.0], dtype=np.float64),
        np.array([110.0], dtype=np.float64),
    )

    # Same activate bar contains Entry1, Exit1, Entry2.
    intents = [
        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 105.0, 1),
        OrderIntent(2, -1, OrderRole.EXIT, OrderKind.STOP, Side.SELL, 95.0, 1),
        OrderIntent(3, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 110.0, 1),
    ]

    fills = simulate_jit(bars, intents)
    assert len(fills) == 2
    assert fills[0].order_id == 1
    assert fills[1].order_id == 2


def test_ttl_one_shot_vs_gtc_extension_point() -> None:
    # Skip if JIT is disabled; ttl=0 is a JIT-only extension behavior.
    import FishBroWFS_V2.engine.engine_jit as ej

    if ej.nb is None or os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        pytest.skip("numba not available or disabled; ttl=0 extension tested only under JIT")

    # Bar0: stop not touched, Bar1: stop touched
    bars = normalize_bars(
        np.array([90.0, 90.0], dtype=np.float64),
        np.array([99.0, 110.0], dtype=np.float64),
        np.array([90.0, 90.0], dtype=np.float64),
        np.array([95.0, 100.0], dtype=np.float64),
    )
    intents = [
        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),
    ]

    # ttl=1 (default semantics): active only on bar0 -> no fill
    fills_ttl1 = simulate_jit(bars, intents)
    assert fills_ttl1 == []

    # ttl=0 (GTC extension): order stays in book and can fill on bar1
    fills_gtc = _simulate_with_ttl(bars, intents, ttl_bars=0)
    assert len(fills_gtc) == 1
    assert fills_gtc[0].bar_index == 1
    assert abs(fills_gtc[0].price - 100.0) <= 1e-9




================================================================================
FILE: tests/test_entry_only_regression.py
================================================================================

"""
Regression test for entry-only fills scenario.

This test ensures that when entry fills occur but exit fills do not,
the metrics behavior is correct:
- trades=0 is valid (no completed round-trips)
- metrics may be all-zero or have non-zero values depending on implementation
- The system should not crash or produce invalid metrics
"""
from __future__ import annotations

import numpy as np
import os

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_entry_only_fills_metrics_behavior() -> None:
    """
    Test metrics behavior when only entry fills occur (no exit fills).
    
    Scenario:
    - Entry stop triggers at t=31 (high[31] crosses buy stop=high[30]=120)
    - Exit stop never triggers (all subsequent lows stay above exit stop)
    - Result: entry_fills_total > 0, exit_fills_total == 0, trades == 0
    """
    # Ensure clean environment
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    old_param_subsample_rate = os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
    old_param_subsample_seed = os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_SEED", None)
    
    try:
        # Set required environment variables
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = "1.0"
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_SEED"] = "42"
        
        n = 60
        
        # Construct OHLC as specified
        # Initial: all flat at 100.0
        close = np.full(n, 100.0, dtype=np.float64)
        open_ = close.copy()
        high = np.full(n, 100.5, dtype=np.float64)
        low = np.full(n, 99.5, dtype=np.float64)
        
        # At t=30: set high[30]=120.0 (forms Donchian high point)
        high[30] = 120.0
        
        # At t=31: set high[31]=121.0 and low[31]=110.0
        # This ensures next-bar buy stop=high[30]=120 will be triggered
        high[31] = 121.0
        low[31] = 110.0
        
        # t>=32: set low[t]=110.1, high[t]=111.0, close[t]=110.5
        # This ensures exit stop will never trigger (low stays above exit stop)
        for t in range(32, n):
            low[t] = 110.1  # Slightly above 110.0 to avoid triggering exit stop
            high[t] = 111.0
            close[t] = 110.5
            open_[t] = 110.5
        
        # Ensure OHLC consistency
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Single param: channel_len=20, atr_len=10, stop_mult=1.0
        params_matrix = np.array([[20, 10, 1.0]], dtype=np.float64)
        
        result = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
            force_close_last=False,  # Critical: do not force close
        )
        
        # Verify metrics shape
        metrics = result.get("metrics")
        assert metrics is not None, "metrics must exist"
        assert isinstance(metrics, np.ndarray), "metrics must be np.ndarray"
        assert metrics.shape == (1, 3), (
            f"metrics shape should be (1, 3), got {metrics.shape}"
        )
        
        # Verify perf dict
        perf = result.get("perf", {})
        assert isinstance(perf, dict), "perf must be a dict"
        
        # Extract perf fields for entry-only invariants
        fills_total = int(perf.get("fills_total", 0))
        entry_fills_total = int(perf.get("entry_fills_total", 0))
        exit_fills_total = int(perf.get("exit_fills_total", 0))
        entry_intents_total = int(perf.get("entry_intents_total", 0))
        exit_intents_total = int(perf.get("exit_intents_total", 0))
        
        # Assertions: lock semantics, not performance
        assert fills_total >= 1, (
            f"fills_total ({fills_total}) should be >= 1 (entry fill should occur)"
        )
        
        assert entry_fills_total >= 1, (
            f"entry_fills_total ({entry_fills_total}) should be >= 1"
        )
        
        assert exit_fills_total == 0, (
            f"exit_fills_total ({exit_fills_total}) should be 0 (exit stop should never trigger)"
        )
        
        # If exit intents exist, fine; but they must not fill.
        assert exit_intents_total >= 0, (
            f"exit_intents_total ({exit_intents_total}) should be >= 0"
        )
        
        assert entry_intents_total >= 1, (
            f"entry_intents_total ({entry_intents_total}) should be >= 1"
        )
        
        # Entry-only scenario: no exit fills => no completed trades.
        # Our metrics are trade-based, so metrics may legitimately remain all zeros.
        assert np.all(np.isfinite(metrics[0])), f"metrics[0] must be finite, got {metrics[0]}"
        
        # Verify trades and net_profit from result or perf (compatible with different return locations)
        trades = int(result.get("trades", perf.get("trades", 0)) or 0)
        net_profit = float(result.get("net_profit", perf.get("net_profit", 0.0)) or 0.0)
        
        assert trades == 0, f"entry-only must have trades==0, got {trades}"
        assert abs(net_profit) <= 1e-12, f"entry-only must have net_profit==0, got {net_profit}"
        
        # Verify metrics values match
        assert int(metrics[0, 1]) == 0, f"metrics[0, 1] (trades) must be 0, got {metrics[0, 1]}"
        assert abs(float(metrics[0, 0])) <= 1e-12, f"metrics[0, 0] (net_profit) must be 0, got {metrics[0, 0]}"
        assert abs(float(metrics[0, 2])) <= 1e-12, f"metrics[0, 2] (max_dd) must be 0, got {metrics[0, 2]}"
        
        # Evidence-chain sanity (optional but recommended)
        if "metrics_subset_abs_sum" in perf:
            assert float(perf["metrics_subset_abs_sum"]) >= 0.0
        if "metrics_subset_nonzero_rows" in perf:
            assert int(perf["metrics_subset_nonzero_rows"]) == 0
        
        # Optional: Check if position tracking exists (entry-only should end in open position)
        pos_last = perf.get("position_last", perf.get("pos_last", perf.get("last_position", None)))
        if pos_last is not None:
            assert int(pos_last) != 0, f"entry-only should end in open position, got {pos_last}"
        
    finally:
        # Restore environment
        if old_trigger_rate is None:
            os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
        else:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate
        
        if old_param_subsample_rate is None:
            os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
        else:
            os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = old_param_subsample_rate
        
        if old_param_subsample_seed is None:
            os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_SEED", None)
        else:
            os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_SEED"] = old_param_subsample_seed


================================================================================
FILE: tests/test_funnel_contract.py
================================================================================

"""Contract tests for funnel pipeline.

Tests verify:
1. Funnel plan has three stages
2. Stage2 subsample is 1.0
3. Each stage creates artifacts
4. param_subsample_rate visibility
5. params_effective calculation consistency
6. Funnel result index structure
"""

from __future__ import annotations

import tempfile
from pathlib import Path

import numpy as np
import pytest

from FishBroWFS_V2.core.audit_schema import compute_params_effective
from FishBroWFS_V2.pipeline.funnel_plan import build_default_funnel_plan
from FishBroWFS_V2.pipeline.funnel_runner import run_funnel
from FishBroWFS_V2.pipeline.funnel_schema import StageName


def test_funnel_build_default_plan_has_three_stages():
    """Test that default funnel plan has exactly three stages."""
    cfg = {
        "param_subsample_rate": 0.1,
        "topk_stage0": 50,
        "topk_stage1": 20,
    }
    
    plan = build_default_funnel_plan(cfg)
    
    assert len(plan.stages) == 3
    
    # Verify stage names
    assert plan.stages[0].name == StageName.STAGE0_COARSE
    assert plan.stages[1].name == StageName.STAGE1_TOPK
    assert plan.stages[2].name == StageName.STAGE2_CONFIRM


def test_stage2_subsample_is_one():
    """Test that Stage2 subsample rate is always 1.0."""
    test_cases = [
        {"param_subsample_rate": 0.1},
        {"param_subsample_rate": 0.5},
        {"param_subsample_rate": 0.9},
    ]
    
    for cfg in test_cases:
        plan = build_default_funnel_plan(cfg)
        stage2 = plan.stages[2]
        
        assert stage2.name == StageName.STAGE2_CONFIRM
        assert stage2.param_subsample_rate == 1.0, (
            f"Stage2 subsample must be 1.0, got {stage2.param_subsample_rate}"
        )


def test_subsample_rate_progression():
    """Test that subsample rates progress correctly."""
    cfg = {"param_subsample_rate": 0.1}
    plan = build_default_funnel_plan(cfg)
    
    s0_rate = plan.stages[0].param_subsample_rate
    s1_rate = plan.stages[1].param_subsample_rate
    s2_rate = plan.stages[2].param_subsample_rate
    
    # Stage0: config rate
    assert s0_rate == 0.1
    
    # Stage1: min(1.0, s0 * 2)
    assert s1_rate == min(1.0, 0.1 * 2.0) == 0.2
    
    # Stage2: must be 1.0
    assert s2_rate == 1.0
    
    # Verify progression: s0 <= s1 <= s2
    assert s0_rate <= s1_rate <= s2_rate


def test_each_stage_creates_run_dir_with_artifacts():
    """Test that each stage creates run directory with required artifacts."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        # Create minimal config
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
        }
        
        # Run funnel
        result_index = run_funnel(cfg, outputs_root)
        
        # Verify all stages have run directories
        assert len(result_index.stages) == 3
        
        artifacts = [
            "manifest.json",
            "config_snapshot.json",
            "metrics.json",
            "winners.json",
            "README.md",
            "logs.txt",
        ]
        
        for stage_idx in result_index.stages:
            run_dir = outputs_root / stage_idx.run_dir
            
            # Verify directory exists
            assert run_dir.exists(), f"Run directory missing for {stage_idx.stage.value}"
            assert run_dir.is_dir()
            
            # Verify all artifacts exist
            for artifact_name in artifacts:
                artifact_path = run_dir / artifact_name
                assert artifact_path.exists(), (
                    f"Missing artifact {artifact_name} for {stage_idx.stage.value}"
                )


def test_param_subsample_rate_visible_in_artifacts():
    """Test that param_subsample_rate is visible in manifest/metrics/README."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.25,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        import json
        
        for stage_idx in result_index.stages:
            run_dir = outputs_root / stage_idx.run_dir
            
            # Check manifest.json
            manifest_path = run_dir / "manifest.json"
            with open(manifest_path, "r", encoding="utf-8") as f:
                manifest = json.load(f)
            assert "param_subsample_rate" in manifest
            
            # Check metrics.json
            metrics_path = run_dir / "metrics.json"
            with open(metrics_path, "r", encoding="utf-8") as f:
                metrics = json.load(f)
            assert "param_subsample_rate" in metrics
            
            # Check README.md
            readme_path = run_dir / "README.md"
            with open(readme_path, "r", encoding="utf-8") as f:
                readme_content = f.read()
            assert "param_subsample_rate" in readme_content


def test_params_effective_floor_rule_consistent():
    """Test that params_effective uses consistent floor rule across stages."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        params_total = 1000
        param_subsample_rate = 0.33
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": params_total,
            "param_subsample_rate": param_subsample_rate,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(params_total, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        import json
        
        plan = result_index.plan
        for i, (spec, stage_idx) in enumerate(zip(plan.stages, result_index.stages)):
            run_dir = outputs_root / stage_idx.run_dir
            
            # Read manifest
            manifest_path = run_dir / "manifest.json"
            with open(manifest_path, "r", encoding="utf-8") as f:
                manifest = json.load(f)
            
            # Verify params_effective matches computed value
            expected_effective = compute_params_effective(
                params_total, spec.param_subsample_rate
            )
            assert manifest["params_effective"] == expected_effective, (
                f"Stage {i} params_effective mismatch: "
                f"expected={expected_effective}, got={manifest['params_effective']}"
            )


def test_funnel_result_index_contains_all_stages():
    """Test that funnel result index contains all stages."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        # Verify index structure
        assert result_index.plan is not None
        assert len(result_index.stages) == 3
        
        # Verify stage order matches plan
        for spec, stage_idx in zip(result_index.plan.stages, result_index.stages):
            assert spec.name == stage_idx.stage
            assert stage_idx.run_id is not None
            assert stage_idx.run_dir is not None


def test_config_snapshot_is_json_serializable_and_small():
    """Test that config_snapshot.json excludes ndarrays and is JSON-serializable."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        import json
        
        # Keys that should NOT exist in snapshot (raw ndarrays)
        forbidden_keys = {"open_", "open", "high", "low", "close", "volume", "params_matrix"}
        
        # Required keys that MUST exist
        required_keys = {
            "season",
            "dataset_id",
            "bars",
            "params_total",
            "param_subsample_rate",
            "stage_name",
        }
        
        for stage_idx in result_index.stages:
            run_dir = outputs_root / stage_idx.run_dir
            config_snapshot_path = run_dir / "config_snapshot.json"
            
            assert config_snapshot_path.exists()
            
            # Verify JSON is valid and loadable
            with open(config_snapshot_path, "r", encoding="utf-8") as f:
                snapshot_content = f.read()
                snapshot_data = json.loads(snapshot_content)  # Should not crash
            
            # Verify no raw ndarray keys exist
            for forbidden_key in forbidden_keys:
                assert forbidden_key not in snapshot_data, (
                    f"config_snapshot.json should not contain '{forbidden_key}' "
                    f"(raw ndarray) for {stage_idx.stage.value}"
                )
            
            # Verify required keys exist
            for required_key in required_keys:
                assert required_key in snapshot_data, (
                    f"config_snapshot.json missing required key '{required_key}' "
                    f"for {stage_idx.stage.value}"
                )
            
            # Verify param_subsample_rate is present and correct
            assert "param_subsample_rate" in snapshot_data
            assert isinstance(snapshot_data["param_subsample_rate"], (int, float))
            
            # Verify stage_name is present
            assert "stage_name" in snapshot_data
            assert isinstance(snapshot_data["stage_name"], str)
            
            # Optional: verify metadata keys exist if needed
            # (e.g., "open__meta", "params_matrix_meta")
            # This is optional - metadata may or may not be included


================================================================================
FILE: tests/test_funnel_oom_integration.py
================================================================================

"""Integration tests for OOM gate in funnel pipeline.

Tests verify:
1. Funnel metrics include OOM gate fields
2. Auto-downsample updates snapshot and hash consistently
"""

from __future__ import annotations

import json
import tempfile
from pathlib import Path

import numpy as np
import pytest

from FishBroWFS_V2.pipeline.funnel_runner import run_funnel


def test_funnel_metrics_include_oom_gate_fields():
    """Test that funnel metrics include OOM gate fields."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
            "mem_limit_mb": 10000.0,  # High limit to ensure PASS
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        # Verify all stages have OOM gate fields in metrics
        for stage_idx in result_index.stages:
            run_dir = outputs_root / stage_idx.run_dir
            metrics_path = run_dir / "metrics.json"
            
            assert metrics_path.exists()
            
            with open(metrics_path, "r", encoding="utf-8") as f:
                metrics = json.load(f)
            
            # Verify required OOM gate fields
            assert "oom_gate_action" in metrics
            assert "oom_gate_reason" in metrics
            assert "mem_est_mb" in metrics
            assert "mem_limit_mb" in metrics
            assert "ops_est" in metrics
            assert "stage_planned_subsample" in metrics
            
            # Verify action is valid
            assert metrics["oom_gate_action"] in ("PASS", "BLOCK", "AUTO_DOWNSAMPLE")
            
            # Verify stage_planned_subsample matches expected planned for this stage
            stage_name = metrics.get("stage_name")
            s0_base = cfg.get("param_subsample_rate", 0.1)
            expected_planned = planned_subsample_for_stage(stage_name, s0_base)
            assert metrics["stage_planned_subsample"] == expected_planned, (
                f"stage_planned_subsample mismatch for {stage_name}: "
                f"expected={expected_planned}, got={metrics['stage_planned_subsample']}"
            )


def planned_subsample_for_stage(stage_name: str, s0: float) -> float:
    """
    Get planned subsample rate for a stage based on funnel plan rules.
    
    Args:
        stage_name: Stage identifier
        s0: Stage0 base subsample rate (from config)
        
    Returns:
        Planned subsample rate for the stage
    """
    if stage_name == "stage0_coarse":
        return s0
    if stage_name == "stage1_topk":
        return min(1.0, s0 * 2.0)
    if stage_name == "stage2_confirm":
        return 1.0
    raise AssertionError(f"Unknown stage_name: {stage_name}")


def test_auto_downsample_updates_snapshot_and_hash(monkeypatch):
    """Test that auto-downsample updates snapshot and hash consistently."""
    # Monkeypatch estimate_memory_bytes to trigger auto-downsample
    def mock_estimate_memory_bytes(cfg, work_factor=2.0):
        """Mock that makes memory estimate sensitive to subsample."""
        bars = int(cfg.get("bars", 0))
        params_total = int(cfg.get("params_total", 0))
        subsample_rate = float(cfg.get("param_subsample_rate", 1.0))
        params_effective = int(params_total * subsample_rate)
        
        base_mem = bars * 8 * 4  # 4 price arrays
        params_mem = params_effective * 3 * 8  # params_matrix
        total_mem = (base_mem + params_mem) * work_factor
        return int(total_mem)
    
    monkeypatch.setattr(
        "FishBroWFS_V2.core.oom_cost_model.estimate_memory_bytes",
        mock_estimate_memory_bytes,
    )
    
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        # Stage0 base subsample rate (from config)
        s0_base = 0.5
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 10000,
            "params_total": 1000,
            "param_subsample_rate": s0_base,  # Stage0 base rate
            "open_": np.random.randn(10000).astype(np.float64),
            "high": np.random.randn(10000).astype(np.float64),
            "low": np.random.randn(10000).astype(np.float64),
            "close": np.random.randn(10000).astype(np.float64),
            "params_matrix": np.random.randn(1000, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
            # Dynamic limit calculation
            "mem_limit_mb": 0.65,  # Will trigger auto-downsample for some stages
            "allow_auto_downsample": True,
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        # Check each stage
        for stage_idx in result_index.stages:
            run_dir = outputs_root / stage_idx.run_dir
            
            # Read manifest
            manifest_path = run_dir / "manifest.json"
            with open(manifest_path, "r", encoding="utf-8") as f:
                manifest = json.load(f)
            
            # Read config_snapshot
            config_snapshot_path = run_dir / "config_snapshot.json"
            with open(config_snapshot_path, "r", encoding="utf-8") as f:
                config_snapshot = json.load(f)
            
            # Read metrics
            metrics_path = run_dir / "metrics.json"
            with open(metrics_path, "r", encoding="utf-8") as f:
                metrics = json.load(f)
            
            # Get stage name and planned subsample
            stage_name = metrics.get("stage_name")
            expected_planned = planned_subsample_for_stage(stage_name, s0_base)
            
            # Verify consistency: if auto-downsample occurred, all must match
            if metrics.get("oom_gate_action") == "AUTO_DOWNSAMPLE":
                final_subsample = metrics.get("oom_gate_final_subsample")
                
                # Manifest must have final subsample
                assert manifest["param_subsample_rate"] == final_subsample, (
                    f"Manifest subsample mismatch: "
                    f"expected={final_subsample}, got={manifest['param_subsample_rate']}"
                )
                
                # Config snapshot must have final subsample
                assert config_snapshot["param_subsample_rate"] == final_subsample, (
                    f"Config snapshot subsample mismatch: "
                    f"expected={final_subsample}, got={config_snapshot['param_subsample_rate']}"
                )
                
                # Metrics must have final subsample
                assert metrics["param_subsample_rate"] == final_subsample, (
                    f"Metrics subsample mismatch: "
                    f"expected={final_subsample}, got={metrics['param_subsample_rate']}"
                )
                
                # Verify original subsample matches planned subsample for this stage
                assert "oom_gate_original_subsample" in metrics
                assert metrics["oom_gate_original_subsample"] == expected_planned, (
                    f"oom_gate_original_subsample mismatch for {stage_name}: "
                    f"expected={expected_planned} (planned), "
                    f"got={metrics['oom_gate_original_subsample']}"
                )
                
                # Verify stage_planned_subsample equals oom_gate_original_subsample
                assert "stage_planned_subsample" in metrics
                assert metrics["stage_planned_subsample"] == metrics["oom_gate_original_subsample"], (
                    f"stage_planned_subsample should equal oom_gate_original_subsample for {stage_name}: "
                    f"stage_planned={metrics['stage_planned_subsample']}, "
                    f"original={metrics['oom_gate_original_subsample']}"
                )


def test_oom_gate_fields_in_readme():
    """Test that OOM gate fields are included in README."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
            "mem_limit_mb": 10000.0,
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        # Check README for at least one stage
        for stage_idx in result_index.stages:
            run_dir = outputs_root / stage_idx.run_dir
            readme_path = run_dir / "README.md"
            
            assert readme_path.exists()
            
            with open(readme_path, "r", encoding="utf-8") as f:
                readme_content = f.read()
            
            # Verify OOM gate section exists
            assert "OOM Gate" in readme_content
            assert "action" in readme_content.lower()
            assert "mem_est_mb" in readme_content.lower()
            
            break  # Check at least one stage


def test_block_action_raises_error():
    """Test that BLOCK action raises RuntimeError."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000000,  # Very large
            "params_total": 100000,  # Very large
            "param_subsample_rate": 1.0,
            "open_": np.random.randn(1000000).astype(np.float64),
            "high": np.random.randn(1000000).astype(np.float64),
            "low": np.random.randn(1000000).astype(np.float64),
            "close": np.random.randn(1000000).astype(np.float64),
            "params_matrix": np.random.randn(100000, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
            "mem_limit_mb": 1.0,  # Very low limit
            "allow_auto_downsample": False,  # Disable auto-downsample to force BLOCK
        }
        
        # Should raise RuntimeError
        with pytest.raises(RuntimeError, match="OOM Gate BLOCKED"):
            run_funnel(cfg, outputs_root)


================================================================================
FILE: tests/test_funnel_smoke_contract.py
================================================================================

"""Funnel smoke contract tests - Phase 4 Stage D.

Basic smoke tests to ensure the complete funnel pipeline works end-to-end.
"""

import numpy as np

from FishBroWFS_V2.pipeline.funnel import FunnelResult, run_funnel


def test_funnel_smoke_basic():
    """Basic smoke test: run funnel with small parameter grid."""
    # Generate deterministic test data
    np.random.seed(42)
    n_bars = 500
    n_params = 20
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    # Generate parameter grid
    params_matrix = np.column_stack([
        np.random.randint(10, 50, size=n_params),  # channel_len / fast_len
        np.random.randint(5, 30, size=n_params),   # atr_len / slow_len
        np.random.uniform(1.0, 3.0, size=n_params), # stop_mult
    ]).astype(np.float64)
    
    # Run funnel
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=5,
        commission=0.0,
        slip=0.0,
    )
    
    # Verify result structure
    assert isinstance(result, FunnelResult)
    assert len(result.stage0_results) == n_params
    assert len(result.topk_param_ids) == 5
    assert len(result.stage2_results) == 5
    
    # Verify Stage0 results
    for stage0_result in result.stage0_results:
        assert hasattr(stage0_result, "param_id")
        assert hasattr(stage0_result, "proxy_value")
        assert hasattr(stage0_result, "warmup_ok")
        assert isinstance(stage0_result.param_id, int)
        assert isinstance(stage0_result.proxy_value, (int, float))
    
    # Verify Top-K param_ids are valid
    for param_id in result.topk_param_ids:
        assert 0 <= param_id < n_params
    
    # Verify Stage2 results match Top-K
    assert len(result.stage2_results) == len(result.topk_param_ids)
    for i, stage2_result in enumerate(result.stage2_results):
        assert stage2_result.param_id == result.topk_param_ids[i]
        assert isinstance(stage2_result.net_profit, (int, float))
        assert isinstance(stage2_result.trades, int)
        assert isinstance(stage2_result.max_dd, (int, float))


def test_funnel_smoke_empty_params():
    """Test funnel with empty parameter grid."""
    np.random.seed(42)
    n_bars = 100
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    # Empty parameter grid
    params_matrix = np.empty((0, 3), dtype=np.float64)
    
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=5,
    )
    
    assert len(result.stage0_results) == 0
    assert len(result.topk_param_ids) == 0
    assert len(result.stage2_results) == 0


def test_funnel_smoke_k_larger_than_params():
    """Test funnel when k is larger than number of parameters."""
    np.random.seed(42)
    n_bars = 100
    n_params = 5
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    params_matrix = np.column_stack([
        np.random.randint(10, 50, size=n_params),
        np.random.randint(5, 30, size=n_params),
        np.random.uniform(1.0, 3.0, size=n_params),
    ]).astype(np.float64)
    
    # k=10 but only 5 params
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=10,
    )
    
    # Should return all 5 params
    assert len(result.topk_param_ids) == 5
    assert len(result.stage2_results) == 5


def test_funnel_smoke_pipeline_order():
    """Test that pipeline executes in correct order: Stage0 â†’ Top-K â†’ Stage2."""
    np.random.seed(42)
    n_bars = 200
    n_params = 10
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    params_matrix = np.column_stack([
        np.random.randint(10, 30, size=n_params),
        np.random.randint(5, 20, size=n_params),
        np.random.uniform(1.0, 2.0, size=n_params),
    ]).astype(np.float64)
    
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=3,
    )
    
    # Verify Stage0 ran on all params
    assert len(result.stage0_results) == n_params
    
    # Verify Top-K selected from Stage0 results
    assert len(result.topk_param_ids) == 3
    # Top-K should be sorted by proxy_value (descending)
    stage0_by_id = {r.param_id: r for r in result.stage0_results}
    topk_values = [stage0_by_id[pid].proxy_value for pid in result.topk_param_ids]
    assert topk_values == sorted(topk_values, reverse=True)
    
    # Verify Stage2 ran only on Top-K
    assert len(result.stage2_results) == 3
    stage2_param_ids = [r.param_id for r in result.stage2_results]
    assert set(stage2_param_ids) == set(result.topk_param_ids)


================================================================================
FILE: tests/test_funnel_topk_determinism.py
================================================================================

"""Test Top-K determinism - same input must produce same Top-K selection."""

import numpy as np

from FishBroWFS_V2.pipeline.funnel import run_funnel
from FishBroWFS_V2.pipeline.stage0_runner import Stage0Result, run_stage0
from FishBroWFS_V2.pipeline.topk import select_topk


def test_topk_determinism_same_input():
    """Test that Top-K selection is deterministic: same input produces same output."""
    # Generate deterministic test data
    np.random.seed(42)
    n_bars = 1000
    n_params = 100
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    # Generate parameter grid
    params_matrix = np.column_stack([
        np.random.randint(10, 100, size=n_params),  # fast_len / channel_len
        np.random.randint(5, 50, size=n_params),      # slow_len / atr_len
        np.random.uniform(1.0, 5.0, size=n_params),   # stop_mult
    ]).astype(np.float64)
    
    # Run Stage0 twice with same input
    stage0_results_1 = run_stage0(close, params_matrix)
    stage0_results_2 = run_stage0(close, params_matrix)
    
    # Verify Stage0 results are identical
    assert len(stage0_results_1) == len(stage0_results_2)
    for r1, r2 in zip(stage0_results_1, stage0_results_2):
        assert r1.param_id == r2.param_id
        assert r1.proxy_value == r2.proxy_value
    
    # Run Top-K selection twice
    k = 20
    topk_1 = select_topk(stage0_results_1, k=k)
    topk_2 = select_topk(stage0_results_2, k=k)
    
    # Verify Top-K selection is identical
    assert topk_1 == topk_2, (
        f"Top-K selection not deterministic:\n"
        f"  First run:  {topk_1}\n"
        f"  Second run: {topk_2}"
    )
    assert len(topk_1) == k
    assert len(topk_2) == k


def test_topk_determinism_tie_break():
    """Test that tie-breaking by param_id is deterministic."""
    # Create Stage0 results with identical proxy_value
    # Tie-break should use param_id (ascending)
    results = [
        Stage0Result(param_id=5, proxy_value=10.0),
        Stage0Result(param_id=2, proxy_value=10.0),  # Same value, lower param_id
        Stage0Result(param_id=8, proxy_value=10.0),
        Stage0Result(param_id=1, proxy_value=10.0),  # Same value, lowest param_id
        Stage0Result(param_id=3, proxy_value=15.0),  # Higher value
        Stage0Result(param_id=4, proxy_value=12.0),  # Medium value
    ]
    
    # Select top 3
    topk = select_topk(results, k=3)
    
    # Expected: param_id=3 (value=15.0), param_id=4 (value=12.0), param_id=1 (value=10.0, lowest param_id)
    assert topk == [3, 4, 1], f"Tie-break failed: got {topk}, expected [3, 4, 1]"
    
    # Run again - should be identical
    topk_2 = select_topk(results, k=3)
    assert topk_2 == topk


def test_funnel_determinism():
    """Test that complete funnel pipeline is deterministic."""
    # Generate deterministic test data
    np.random.seed(123)
    n_bars = 500
    n_params = 50
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    # Generate parameter grid
    params_matrix = np.column_stack([
        np.random.randint(10, 50, size=n_params),
        np.random.randint(5, 30, size=n_params),
        np.random.uniform(1.0, 3.0, size=n_params),
    ]).astype(np.float64)
    
    # Run funnel twice
    result_1 = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=10,
        commission=0.0,
        slip=0.0,
    )
    
    result_2 = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=10,
        commission=0.0,
        slip=0.0,
    )
    
    # Verify Top-K selection is identical
    assert result_1.topk_param_ids == result_2.topk_param_ids, (
        f"Funnel Top-K not deterministic:\n"
        f"  First run:  {result_1.topk_param_ids}\n"
        f"  Second run: {result_2.topk_param_ids}"
    )
    
    # Verify Stage2 results are for same parameters
    assert len(result_1.stage2_results) == len(result_2.stage2_results)
    for r1, r2 in zip(result_1.stage2_results, result_2.stage2_results):
        assert r1.param_id == r2.param_id


================================================================================
FILE: tests/test_funnel_topk_no_human_contract.py
================================================================================

"""Funnel Top-K no-human contract tests - Phase 4 Stage D.

These tests ensure that Top-K selection is purely automatic based on proxy_value,
with no possibility of human intervention or manual filtering.
"""

import numpy as np

from FishBroWFS_V2.pipeline.funnel import run_funnel
from FishBroWFS_V2.pipeline.stage0_runner import Stage0Result, run_stage0
from FishBroWFS_V2.pipeline.topk import select_topk


def test_topk_only_uses_proxy_value():
    """Test that Top-K selection uses ONLY proxy_value, not any other field."""
    # Create Stage0 results with varying proxy_value and other fields
    results = [
        Stage0Result(param_id=0, proxy_value=5.0, warmup_ok=True, meta={"custom": "data"}),
        Stage0Result(param_id=1, proxy_value=10.0, warmup_ok=False, meta=None),
        Stage0Result(param_id=2, proxy_value=15.0, warmup_ok=True, meta={"other": 123}),
        Stage0Result(param_id=3, proxy_value=8.0, warmup_ok=True, meta=None),
        Stage0Result(param_id=4, proxy_value=12.0, warmup_ok=False, meta={"test": True}),
    ]
    
    # Select top 3
    topk = select_topk(results, k=3)
    
    # Expected: param_id=2 (value=15.0), param_id=4 (value=12.0), param_id=1 (value=10.0)
    # Should ignore warmup_ok and meta fields
    assert topk == [2, 4, 1], (
        f"Top-K should only consider proxy_value, got {topk}, expected [2, 4, 1]"
    )


def test_topk_tie_break_param_id():
    """Test that tie-breaking uses param_id (ascending) when proxy_value is identical."""
    # Create results with identical proxy_value
    results = [
        Stage0Result(param_id=5, proxy_value=10.0),
        Stage0Result(param_id=2, proxy_value=10.0),
        Stage0Result(param_id=8, proxy_value=10.0),
        Stage0Result(param_id=1, proxy_value=10.0),
        Stage0Result(param_id=3, proxy_value=15.0),  # Higher value
        Stage0Result(param_id=4, proxy_value=12.0),   # Medium value
    ]
    
    # Select top 3
    topk = select_topk(results, k=3)
    
    # Expected: param_id=3 (value=15.0), param_id=4 (value=12.0), param_id=1 (value=10.0, lowest param_id)
    assert topk == [3, 4, 1], (
        f"Tie-break should use param_id ascending, got {topk}, expected [3, 4, 1]"
    )


def test_topk_deterministic_same_input():
    """Test that Top-K selection is deterministic: same input produces same output."""
    np.random.seed(42)
    n_bars = 500
    n_params = 50
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    
    params_matrix = np.column_stack([
        np.random.randint(10, 50, size=n_params),
        np.random.randint(5, 30, size=n_params),
        np.random.uniform(1.0, 3.0, size=n_params),
    ]).astype(np.float64)
    
    # Run Stage0 twice
    stage0_results_1 = run_stage0(close, params_matrix)
    stage0_results_2 = run_stage0(close, params_matrix)
    
    # Select Top-K twice
    topk_1 = select_topk(stage0_results_1, k=10)
    topk_2 = select_topk(stage0_results_2, k=10)
    
    # Should be identical
    assert topk_1 == topk_2, (
        f"Top-K selection not deterministic:\n"
        f"  First run:  {topk_1}\n"
        f"  Second run: {topk_2}"
    )


def test_funnel_topk_no_manual_filtering():
    """Test that funnel Top-K selection cannot be manually filtered."""
    np.random.seed(42)
    n_bars = 300
    n_params = 20
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    params_matrix = np.column_stack([
        np.random.randint(10, 40, size=n_params),
        np.random.randint(5, 25, size=n_params),
        np.random.uniform(1.0, 2.5, size=n_params),
    ]).astype(np.float64)
    
    # Run funnel
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=5,
    )
    
    # Verify Top-K is based solely on proxy_value
    stage0_by_id = {r.param_id: r for r in result.stage0_results}
    
    # Get proxy_values for Top-K
    topk_values = [stage0_by_id[pid].proxy_value for pid in result.topk_param_ids]
    
    # Get proxy_values for all params
    all_values = [r.proxy_value for r in result.stage0_results]
    all_values_sorted = sorted(all_values, reverse=True)
    
    # Top-K values should match top K values from all params
    assert topk_values == all_values_sorted[:5], (
        f"Top-K should contain top 5 proxy_values:\n"
        f"  Top-K values: {topk_values}\n"
        f"  Top 5 values:  {all_values_sorted[:5]}"
    )


def test_funnel_stage2_only_runs_topk():
    """Test that Stage2 only runs on Top-K parameters, not all parameters."""
    np.random.seed(42)
    n_bars = 200
    n_params = 15
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    params_matrix = np.column_stack([
        np.random.randint(10, 30, size=n_params),
        np.random.randint(5, 20, size=n_params),
        np.random.uniform(1.0, 2.0, size=n_params),
    ]).astype(np.float64)
    
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=3,
    )
    
    # Verify Stage0 ran on all params
    assert len(result.stage0_results) == n_params
    
    # Verify Top-K selected
    assert len(result.topk_param_ids) == 3
    
    # Verify Stage2 ran ONLY on Top-K (not all params)
    assert len(result.stage2_results) == 3, (
        f"Stage2 should run only on Top-K (3 params), not all params ({n_params})"
    )
    
    # Verify Stage2 param_ids match Top-K
    stage2_param_ids = set(r.param_id for r in result.stage2_results)
    topk_param_ids_set = set(result.topk_param_ids)
    assert stage2_param_ids == topk_param_ids_set, (
        f"Stage2 param_ids should match Top-K:\n"
        f"  Stage2: {stage2_param_ids}\n"
        f"  Top-K:  {topk_param_ids_set}"
    )


def test_funnel_stage0_no_pnl_fields():
    """Test that Stage0 results contain NO PnL-related fields."""
    np.random.seed(42)
    n_bars = 200
    n_params = 10
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    params_matrix = np.column_stack([
        np.random.randint(10, 30, size=n_params),
        np.random.randint(5, 20, size=n_params),
        np.random.uniform(1.0, 2.0, size=n_params),
    ]).astype(np.float64)
    
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=5,
    )
    
    # Check all Stage0 results
    forbidden_fields = {"net", "profit", "mdd", "dd", "drawdown", "sqn", "sharpe", 
                       "winrate", "equity", "pnl", "trades", "score"}
    
    for stage0_result in result.stage0_results:
        # Get field names
        if hasattr(stage0_result, "__dataclass_fields__"):
            field_names = set(stage0_result.__dataclass_fields__.keys())
        else:
            field_names = set(getattr(stage0_result, "__dict__", {}).keys())
        
        # Check no forbidden fields
        for field_name in field_names:
            field_lower = field_name.lower()
            for forbidden in forbidden_fields:
                assert forbidden not in field_lower, (
                    f"Stage0Result contains forbidden PnL field: {field_name} "
                    f"(contains '{forbidden}')"
                )


================================================================================
FILE: tests/test_golden_kernel_verification.py
================================================================================

import numpy as np

from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, run_kernel, _max_drawdown
from FishBroWFS_V2.engine.types import BarArrays


def _bars():
    # Small synthetic OHLC series
    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)
    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)
    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)
    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)
    return BarArrays(open=o, high=h, low=l, close=c)


def test_no_trade_case_does_not_crash_and_returns_zero_metrics():
    bars = _bars()
    params = DonchianAtrParams(channel_len=99999, atr_len=3, stop_mult=2.0)

    out = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)
    pnl = out["pnl"]
    equity = out["equity"]
    metrics = out["metrics"]

    assert isinstance(pnl, np.ndarray)
    assert pnl.size == 0
    assert isinstance(equity, np.ndarray)
    assert equity.size == 0
    assert metrics["net_profit"] == 0.0
    assert metrics["trades"] == 0
    assert metrics["max_dd"] == 0.0


def test_vectorized_metrics_are_self_consistent():
    bars = _bars()
    params = DonchianAtrParams(channel_len=2, atr_len=2, stop_mult=1.0)

    out = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)
    pnl = out["pnl"]
    equity = out["equity"]
    metrics = out["metrics"]

    # If zero trades, still must be consistent
    if pnl.size == 0:
        assert metrics["net_profit"] == 0.0
        assert metrics["trades"] == 0
        assert metrics["max_dd"] == 0.0
        return

    # Vectorized checks
    np.testing.assert_allclose(equity, np.cumsum(pnl), rtol=0.0, atol=0.0)
    assert metrics["trades"] == int(pnl.size)
    assert metrics["net_profit"] == float(np.sum(pnl))
    assert metrics["max_dd"] == _max_drawdown(equity)


def test_costs_are_parameterized_not_hardcoded():
    bars = _bars()
    params = DonchianAtrParams(channel_len=2, atr_len=2, stop_mult=1.0)

    out0 = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)
    out1 = run_kernel(bars, params, commission=1.25, slip=0.75, order_qty=1)

    pnl0 = out0["pnl"]
    pnl1 = out1["pnl"]

    # Either both empty or both non-empty; if empty, pass
    if pnl0.size == 0:
        assert pnl1.size == 0
        return

    # Costs increase => pnl decreases by 2*(commission+slip) per trade
    per_trade_delta = 2.0 * (1.25 + 0.75)
    np.testing.assert_allclose(pnl1, pnl0 - per_trade_delta, rtol=0.0, atol=1e-12)



================================================================================
FILE: tests/test_governance_accepts_winners_v2.py
================================================================================

"""Contract tests for governance accepting winners v2.

Tests verify that governance evaluator can read and process v2 winners.json.
"""

from __future__ import annotations

import json
import tempfile
from datetime import datetime, timezone
from pathlib import Path

from FishBroWFS_V2.core.governance_schema import Decision
from FishBroWFS_V2.pipeline.governance_eval import evaluate_governance


def _create_fake_manifest(run_id: str, stage_name: str, season: str = "test") -> dict:
    """Create fake manifest.json."""
    return {
        "run_id": run_id,
        "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        "git_sha": "abc123def456",
        "dirty_repo": False,
        "param_subsample_rate": 0.1,
        "config_hash": "test_hash",
        "season": season,
        "dataset_id": "test_dataset",
        "bars": 1000,
        "params_total": 1000,
        "params_effective": 100,
        "artifact_version": "v1",
    }


def _create_fake_metrics(stage_name: str, stage_planned_subsample: float = 0.1) -> dict:
    """Create fake metrics.json."""
    return {
        "params_total": 1000,
        "params_effective": 100,
        "bars": 1000,
        "stage_name": stage_name,
        "param_subsample_rate": stage_planned_subsample,
        "stage_planned_subsample": stage_planned_subsample,
    }


def _create_fake_winners_v2(stage_name: str, topk_items: list[dict]) -> dict:
    """Create fake winners.json v2."""
    return {
        "schema": "v2",
        "stage_name": stage_name,
        "generated_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        "topk": topk_items,
        "notes": {
            "schema": "v2",
            "candidate_id_mode": "strategy_id:param_id",
        },
    }


def _create_fake_config_snapshot() -> dict:
    """Create fake config_snapshot.json."""
    return {
        "dataset_id": "test_dataset",
        "bars": 1000,
        "params_total": 1000,
    }


def _write_artifacts(run_dir: Path, manifest: dict, metrics: dict, winners: dict, config: dict) -> None:
    """Write artifacts to run directory."""
    run_dir.mkdir(parents=True, exist_ok=True)
    
    with (run_dir / "manifest.json").open("w", encoding="utf-8") as f:
        json.dump(manifest, f, indent=2)
    
    with (run_dir / "metrics.json").open("w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2)
    
    with (run_dir / "winners.json").open("w", encoding="utf-8") as f:
        json.dump(winners, f, indent=2)
    
    with (run_dir / "config_snapshot.json").open("w", encoding="utf-8") as f:
        json.dump(config, f, indent=2)


def test_governance_reads_winners_v2() -> None:
    """Test that governance can read and process v2 winners.json."""
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Stage0 artifacts
        stage0_dir = tmp_path / "stage0"
        _write_artifacts(
            stage0_dir,
            _create_fake_manifest("stage0-123", "stage0_coarse"),
            _create_fake_metrics("stage0_coarse"),
            _create_fake_winners_v2("stage0_coarse", [
                {
                    "candidate_id": "donchian_atr:0",
                    "strategy_id": "donchian_atr",
                    "symbol": "CME.MNQ",
                    "timeframe": "60m",
                    "params": {},
                    "score": 1.0,
                    "metrics": {"proxy_value": 1.0, "param_id": 0},
                    "source": {"param_id": 0, "run_id": "stage0-123", "stage_name": "stage0_coarse"},
                },
            ]),
            _create_fake_config_snapshot(),
        )
        
        # Stage1 artifacts (v2 format)
        stage1_dir = tmp_path / "stage1"
        stage1_winners = _create_fake_winners_v2("stage1_topk", [
            {
                "candidate_id": "donchian_atr:0",
                "strategy_id": "donchian_atr",
                "symbol": "CME.MNQ",
                "timeframe": "60m",
                "params": {},
                "score": 100.0,
                "metrics": {"net_profit": 100.0, "trades": 10, "max_dd": -10.0, "param_id": 0},
                "source": {"param_id": 0, "run_id": "stage1-123", "stage_name": "stage1_topk"},
            },
        ])
        _write_artifacts(
            stage1_dir,
            _create_fake_manifest("stage1-123", "stage1_topk"),
            _create_fake_metrics("stage1_topk"),
            stage1_winners,
            _create_fake_config_snapshot(),
        )
        
        # Stage2 artifacts (v2 format)
        stage2_dir = tmp_path / "stage2"
        stage2_winners = _create_fake_winners_v2("stage2_confirm", [
            {
                "candidate_id": "donchian_atr:0",
                "strategy_id": "donchian_atr",
                "symbol": "CME.MNQ",
                "timeframe": "60m",
                "params": {},
                "score": 100.0,
                "metrics": {"net_profit": 100.0, "trades": 10, "max_dd": -10.0, "param_id": 0},
                "source": {"param_id": 0, "run_id": "stage2-123", "stage_name": "stage2_confirm"},
            },
        ])
        _write_artifacts(
            stage2_dir,
            _create_fake_manifest("stage2-123", "stage2_confirm"),
            _create_fake_metrics("stage2_confirm"),
            stage2_winners,
            _create_fake_config_snapshot(),
        )
        
        # Evaluate governance
        report = evaluate_governance(
            stage0_dir=stage0_dir,
            stage1_dir=stage1_dir,
            stage2_dir=stage2_dir,
        )
        
        # Verify governance processed v2 format
        assert len(report.items) == 1
        item = report.items[0]
        
        # Verify candidate_id is preserved
        assert item.candidate_id == "donchian_atr:0"
        
        # Verify decision was made (should be KEEP since all rules pass)
        assert item.decision in (Decision.KEEP, Decision.FREEZE, Decision.DROP)


def test_governance_handles_mixed_v2_legacy() -> None:
    """Test that governance handles mixed v2/legacy formats gracefully."""
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Stage0 artifacts (legacy)
        stage0_dir = tmp_path / "stage0"
        _write_artifacts(
            stage0_dir,
            _create_fake_manifest("stage0-123", "stage0_coarse"),
            _create_fake_metrics("stage0_coarse"),
            {"topk": [{"param_id": 0, "proxy_value": 1.0}], "notes": {"schema": "v1"}},
            _create_fake_config_snapshot(),
        )
        
        # Stage1 artifacts (v2)
        stage1_dir = tmp_path / "stage1"
        stage1_winners = _create_fake_winners_v2("stage1_topk", [
            {
                "candidate_id": "donchian_atr:0",
                "strategy_id": "donchian_atr",
                "symbol": "CME.MNQ",
                "timeframe": "60m",
                "params": {},
                "score": 100.0,
                "metrics": {"net_profit": 100.0, "trades": 10, "max_dd": -10.0, "param_id": 0},
                "source": {"param_id": 0, "run_id": "stage1-123", "stage_name": "stage1_topk"},
            },
        ])
        _write_artifacts(
            stage1_dir,
            _create_fake_manifest("stage1-123", "stage1_topk"),
            _create_fake_metrics("stage1_topk"),
            stage1_winners,
            _create_fake_config_snapshot(),
        )
        
        # Stage2 artifacts (legacy)
        stage2_dir = tmp_path / "stage2"
        _write_artifacts(
            stage2_dir,
            _create_fake_manifest("stage2-123", "stage2_confirm"),
            _create_fake_metrics("stage2_confirm"),
            {"topk": [{"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0}], "notes": {"schema": "v1"}},
            _create_fake_config_snapshot(),
        )
        
        # Evaluate governance (should handle mixed formats)
        report = evaluate_governance(
            stage0_dir=stage0_dir,
            stage1_dir=stage1_dir,
            stage2_dir=stage2_dir,
        )
        
        # Verify governance processed successfully
        assert len(report.items) == 1
        item = report.items[0]
        assert item.candidate_id == "donchian_atr:0"


================================================================================
FILE: tests/test_governance_eval_rules.py
================================================================================

"""Contract tests for governance evaluation rules.

Tests that governance rules (R1/R2/R3) are correctly applied using fixture artifacts.
"""

from __future__ import annotations

import json
import tempfile
from pathlib import Path
from datetime import datetime, timezone

import pytest

from FishBroWFS_V2.core.governance_schema import Decision
from FishBroWFS_V2.pipeline.governance_eval import evaluate_governance


def _create_fake_manifest(run_id: str, stage_name: str, season: str = "test") -> dict:
    """Create fake manifest.json."""
    return {
        "run_id": run_id,
        "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        "git_sha": "abc123def456",
        "dirty_repo": False,
        "param_subsample_rate": 0.1,
        "config_hash": "test_hash",
        "season": season,
        "dataset_id": "test_dataset",
        "bars": 1000,
        "params_total": 1000,
        "params_effective": 100,
        "artifact_version": "v1",
    }


def _create_fake_metrics(stage_name: str, stage_planned_subsample: float = 0.1) -> dict:
    """Create fake metrics.json."""
    return {
        "params_total": 1000,
        "params_effective": 100,
        "bars": 1000,
        "stage_name": stage_name,
        "param_subsample_rate": stage_planned_subsample,
        "stage_planned_subsample": stage_planned_subsample,
    }


def _create_fake_winners(stage_name: str, topk_items: list[dict]) -> dict:
    """Create fake winners.json."""
    return {
        "topk": topk_items,
        "notes": {
            "schema": "v1",
            "stage": stage_name,
            "topk_count": len(topk_items),
        },
    }


def _create_fake_config_snapshot() -> dict:
    """Create fake config_snapshot.json."""
    return {
        "dataset_id": "test_dataset",
        "bars": 1000,
        "params_total": 1000,
    }


def _write_artifacts(run_dir: Path, manifest: dict, metrics: dict, winners: dict, config: dict) -> None:
    """Write artifacts to run directory."""
    run_dir.mkdir(parents=True, exist_ok=True)
    
    with (run_dir / "manifest.json").open("w", encoding="utf-8") as f:
        json.dump(manifest, f, indent=2)
    
    with (run_dir / "metrics.json").open("w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2)
    
    with (run_dir / "winners.json").open("w", encoding="utf-8") as f:
        json.dump(winners, f, indent=2)
    
    with (run_dir / "config_snapshot.json").open("w", encoding="utf-8") as f:
        json.dump(config, f, indent=2)


def test_r1_drop_when_stage2_missing() -> None:
    """
    Test R1: DROP when candidate in Stage1 but missing in Stage2.
    
    Scenario:
    - Stage1 has candidate with param_id=0
    - Stage2 does not have candidate with param_id=0
    - Expected: DROP with reason "unverified"
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Stage0 artifacts
        stage0_dir = tmp_path / "stage0"
        _write_artifacts(
            stage0_dir,
            _create_fake_manifest("stage0-123", "stage0_coarse"),
            _create_fake_metrics("stage0_coarse"),
            _create_fake_winners("stage0_coarse", [{"param_id": 0, "proxy_value": 1.0}]),
            _create_fake_config_snapshot(),
        )
        
        # Stage1 artifacts (has candidate)
        stage1_dir = tmp_path / "stage1"
        stage1_winners = _create_fake_winners(
            "stage1_topk",
            [{"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0}],
        )
        _write_artifacts(
            stage1_dir,
            _create_fake_manifest("stage1-123", "stage1_topk"),
            _create_fake_metrics("stage1_topk"),
            stage1_winners,
            _create_fake_config_snapshot(),
        )
        
        # Stage2 artifacts (missing candidate)
        stage2_dir = tmp_path / "stage2"
        stage2_winners = _create_fake_winners(
            "stage2_confirm",
            [{"param_id": 1, "net_profit": 200.0, "trades": 20, "max_dd": -20.0}],  # Different param_id
        )
        _write_artifacts(
            stage2_dir,
            _create_fake_manifest("stage2-123", "stage2_confirm"),
            _create_fake_metrics("stage2_confirm"),
            stage2_winners,
            _create_fake_config_snapshot(),
        )
        
        # Evaluate governance
        report = evaluate_governance(
            stage0_dir=stage0_dir,
            stage1_dir=stage1_dir,
            stage2_dir=stage2_dir,
        )
        
        # Verify: candidate should be DROP
        assert len(report.items) == 1
        item = report.items[0]
        assert item.decision == Decision.DROP
        assert any("R1" in reason for reason in item.reasons)
        assert any("unverified" in reason.lower() for reason in item.reasons)


def test_r2_drop_when_metric_degrades_over_threshold() -> None:
    """
    Test R2: DROP when metrics degrade > 20% from Stage1 to Stage2.
    
    Scenario:
    - Stage1: net_profit=100, max_dd=-10 -> net_over_mdd = 10.0
    - Stage2: net_profit=70, max_dd=-10 -> net_over_mdd = 7.0
    - Degradation: (10.0 - 7.0) / 10.0 = 0.30 (30% > 20% threshold)
    - Expected: DROP with reason "degraded"
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Stage0 artifacts
        stage0_dir = tmp_path / "stage0"
        _write_artifacts(
            stage0_dir,
            _create_fake_manifest("stage0-123", "stage0_coarse"),
            _create_fake_metrics("stage0_coarse"),
            _create_fake_winners("stage0_coarse", [{"param_id": 0, "proxy_value": 1.0}]),
            _create_fake_config_snapshot(),
        )
        
        # Stage1 artifacts
        stage1_dir = tmp_path / "stage1"
        stage1_winners = _create_fake_winners(
            "stage1_topk",
            [{"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0}],
        )
        _write_artifacts(
            stage1_dir,
            _create_fake_manifest("stage1-123", "stage1_topk"),
            _create_fake_metrics("stage1_topk"),
            stage1_winners,
            _create_fake_config_snapshot(),
        )
        
        # Stage2 artifacts (degraded metrics)
        stage2_dir = tmp_path / "stage2"
        stage2_winners = _create_fake_winners(
            "stage2_confirm",
            [{"param_id": 0, "net_profit": 70.0, "trades": 10, "max_dd": -10.0}],  # 30% degradation
        )
        _write_artifacts(
            stage2_dir,
            _create_fake_manifest("stage2-123", "stage2_confirm"),
            _create_fake_metrics("stage2_confirm"),
            stage2_winners,
            _create_fake_config_snapshot(),
        )
        
        # Evaluate governance
        report = evaluate_governance(
            stage0_dir=stage0_dir,
            stage1_dir=stage1_dir,
            stage2_dir=stage2_dir,
        )
        
        # Verify: candidate should be DROP
        assert len(report.items) == 1
        item = report.items[0]
        assert item.decision == Decision.DROP
        assert any("R2" in reason for reason in item.reasons)
        assert any("degraded" in reason.lower() for reason in item.reasons)


def test_r3_freeze_when_density_over_threshold() -> None:
    """
    Test R3: FREEZE when same strategy_id appears >= 3 times in Stage1 topk.
    
    Scenario:
    - Stage1 has 5 candidates with same strategy_id (donchian_atr)
    - Expected: FREEZE with reason "density"
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Stage0 artifacts
        stage0_dir = tmp_path / "stage0"
        _write_artifacts(
            stage0_dir,
            _create_fake_manifest("stage0-123", "stage0_coarse"),
            _create_fake_metrics("stage0_coarse"),
            _create_fake_winners("stage0_coarse", [{"param_id": i, "proxy_value": 1.0} for i in range(5)]),
            _create_fake_config_snapshot(),
        )
        
        # Stage1 artifacts (5 candidates)
        stage1_dir = tmp_path / "stage1"
        stage1_winners = _create_fake_winners(
            "stage1_topk",
            [
                {"param_id": i, "net_profit": 100.0 + i, "trades": 10, "max_dd": -10.0}
                for i in range(5)
            ],
        )
        _write_artifacts(
            stage1_dir,
            _create_fake_manifest("stage1-123", "stage1_topk"),
            _create_fake_metrics("stage1_topk"),
            stage1_winners,
            _create_fake_config_snapshot(),
        )
        
        # Stage2 artifacts (all candidates present)
        stage2_dir = tmp_path / "stage2"
        stage2_winners = _create_fake_winners(
            "stage2_confirm",
            [
                {"param_id": i, "net_profit": 100.0 + i, "trades": 10, "max_dd": -10.0}
                for i in range(5)
            ],
        )
        _write_artifacts(
            stage2_dir,
            _create_fake_manifest("stage2-123", "stage2_confirm"),
            _create_fake_metrics("stage2_confirm"),
            stage2_winners,
            _create_fake_config_snapshot(),
        )
        
        # Evaluate governance
        report = evaluate_governance(
            stage0_dir=stage0_dir,
            stage1_dir=stage1_dir,
            stage2_dir=stage2_dir,
        )
        
        # Verify: all candidates should be FREEZE (density >= 3)
        assert len(report.items) == 5
        for item in report.items:
            assert item.decision == Decision.FREEZE
            assert any("R3" in reason for reason in item.reasons)
            assert any("density" in reason.lower() for reason in item.reasons)


def test_keep_when_all_rules_pass() -> None:
    """
    Test KEEP when all rules pass.
    
    Scenario:
    - R1: Stage2 has candidate (pass)
    - R2: Metrics do not degrade (pass)
    - R3: Density < threshold (pass)
    - Expected: KEEP
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Stage0 artifacts
        stage0_dir = tmp_path / "stage0"
        _write_artifacts(
            stage0_dir,
            _create_fake_manifest("stage0-123", "stage0_coarse"),
            _create_fake_metrics("stage0_coarse"),
            _create_fake_winners("stage0_coarse", [{"param_id": 0, "proxy_value": 1.0}]),
            _create_fake_config_snapshot(),
        )
        
        # Stage1 artifacts (single candidate, low density)
        stage1_dir = tmp_path / "stage1"
        stage1_winners = _create_fake_winners(
            "stage1_topk",
            [{"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0}],
        )
        _write_artifacts(
            stage1_dir,
            _create_fake_manifest("stage1-123", "stage1_topk"),
            _create_fake_metrics("stage1_topk"),
            stage1_winners,
            _create_fake_config_snapshot(),
        )
        
        # Stage2 artifacts (same metrics, no degradation)
        stage2_dir = tmp_path / "stage2"
        stage2_winners = _create_fake_winners(
            "stage2_confirm",
            [{"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0}],
        )
        _write_artifacts(
            stage2_dir,
            _create_fake_manifest("stage2-123", "stage2_confirm"),
            _create_fake_metrics("stage2_confirm"),
            stage2_winners,
            _create_fake_config_snapshot(),
        )
        
        # Evaluate governance
        report = evaluate_governance(
            stage0_dir=stage0_dir,
            stage1_dir=stage1_dir,
            stage2_dir=stage2_dir,
        )
        
        # Verify: candidate should be KEEP
        assert len(report.items) == 1
        item = report.items[0]
        assert item.decision == Decision.KEEP


================================================================================
FILE: tests/test_governance_schema_contract.py
================================================================================

"""Contract tests for governance schema.

Tests that governance schema is JSON-serializable and follows contracts.
"""

from __future__ import annotations

import json
from datetime import datetime, timezone

from FishBroWFS_V2.core.governance_schema import (
    Decision,
    EvidenceRef,
    GovernanceItem,
    GovernanceReport,
)


def test_governance_report_json_serializable() -> None:
    """
    Test that GovernanceReport is JSON-serializable.
    
    This is a critical contract: governance.json must be machine-readable.
    """
    # Create sample evidence
    evidence = [
        EvidenceRef(
            run_id="test-run-123",
            stage_name="stage1_topk",
            artifact_paths=["manifest.json", "metrics.json", "winners.json"],
            key_metrics={"param_id": 0, "net_profit": 100.0, "trades": 10},
        ),
    ]
    
    # Create sample item
    item = GovernanceItem(
        candidate_id="donchian_atr:abc123def456",
        decision=Decision.KEEP,
        reasons=["R3: density_5_over_threshold_3"],
        evidence=evidence,
        created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        git_sha="abc123def456",
    )
    
    # Create report
    report = GovernanceReport(
        items=[item],
        metadata={
            "governance_id": "gov-20251218T000000Z-12345678",
            "season": "test_season",
            "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            "git_sha": "abc123def456",
        },
    )
    
    # Convert to dict
    report_dict = report.to_dict()
    
    # Serialize to JSON
    json_str = json.dumps(report_dict, ensure_ascii=False, sort_keys=True, indent=2)
    
    # Deserialize back
    report_dict_roundtrip = json.loads(json_str)
    
    # Verify structure
    assert "items" in report_dict_roundtrip
    assert "metadata" in report_dict_roundtrip
    assert len(report_dict_roundtrip["items"]) == 1
    
    item_dict = report_dict_roundtrip["items"][0]
    assert item_dict["candidate_id"] == "donchian_atr:abc123def456"
    assert item_dict["decision"] == "KEEP"
    assert len(item_dict["reasons"]) == 1
    assert len(item_dict["evidence"]) == 1
    
    evidence_dict = item_dict["evidence"][0]
    assert evidence_dict["run_id"] == "test-run-123"
    assert evidence_dict["stage_name"] == "stage1_topk"
    assert "artifact_paths" in evidence_dict
    assert "key_metrics" in evidence_dict


def test_decision_enum_values() -> None:
    """Test that Decision enum has correct values."""
    assert Decision.KEEP.value == "KEEP"
    assert Decision.FREEZE.value == "FREEZE"
    assert Decision.DROP.value == "DROP"


def test_evidence_ref_contains_subsample_fields() -> None:
    """
    Test that EvidenceRef can contain subsample fields in key_metrics.
    
    This is a critical requirement: subsample info must be in evidence.
    """
    evidence = EvidenceRef(
        run_id="test-run-123",
        stage_name="stage1_topk",
        artifact_paths=["manifest.json", "metrics.json", "winners.json"],
        key_metrics={
            "param_id": 0,
            "net_profit": 100.0,
            "stage_planned_subsample": 0.1,
            "param_subsample_rate": 0.1,
            "params_effective": 100,
        },
    )
    
    # Verify subsample fields are present
    assert "stage_planned_subsample" in evidence.key_metrics
    assert "param_subsample_rate" in evidence.key_metrics
    assert "params_effective" in evidence.key_metrics


================================================================================
FILE: tests/test_governance_writer_contract.py
================================================================================

"""Contract tests for governance writer.

Tests that governance writer creates expected directory structure and files.
"""

from __future__ import annotations

import json
import tempfile
from pathlib import Path
from datetime import datetime, timezone

from FishBroWFS_V2.core.governance_schema import (
    Decision,
    EvidenceRef,
    GovernanceItem,
    GovernanceReport,
)
from FishBroWFS_V2.core.governance_writer import write_governance_artifacts


def test_governance_writer_creates_expected_tree() -> None:
    """
    Test that governance writer creates expected directory structure.
    
    Expected:
    - governance.json (machine-readable)
    - README.md (human-readable)
    - evidence_index.json (optional but recommended)
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        governance_dir = Path(tmpdir) / "governance" / "test-123"
        
        # Create sample report
        evidence = [
            EvidenceRef(
                run_id="stage1-123",
                stage_name="stage1_topk",
                artifact_paths=["manifest.json", "metrics.json", "winners.json"],
                key_metrics={
                    "param_id": 0,
                    "net_profit": 100.0,
                    "stage_planned_subsample": 0.1,
                    "param_subsample_rate": 0.1,
                    "params_effective": 100,
                },
            ),
        ]
        
        item = GovernanceItem(
            candidate_id="donchian_atr:abc123def456",
            decision=Decision.KEEP,
            reasons=[],
            evidence=evidence,
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="abc123def456",
        )
        
        report = GovernanceReport(
            items=[item],
            metadata={
                "governance_id": "gov-123",
                "season": "test_season",
                "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
                "git_sha": "abc123def456",
                "decisions": {"KEEP": 1, "FREEZE": 0, "DROP": 0},
            },
        )
        
        # Write artifacts
        write_governance_artifacts(governance_dir, report)
        
        # Verify files exist
        assert governance_dir.exists()
        assert (governance_dir / "governance.json").exists()
        assert (governance_dir / "README.md").exists()
        assert (governance_dir / "evidence_index.json").exists()
        
        # Verify governance.json is valid JSON
        with (governance_dir / "governance.json").open("r", encoding="utf-8") as f:
            governance_dict = json.load(f)
        
        assert "items" in governance_dict
        assert "metadata" in governance_dict
        assert len(governance_dict["items"]) == 1
        
        # Verify README.md contains key information
        readme_text = (governance_dir / "README.md").read_text(encoding="utf-8")
        assert "Governance Report" in readme_text
        assert "governance_id" in readme_text
        assert "Decision Summary" in readme_text
        assert "KEEP" in readme_text
        
        # Verify evidence_index.json is valid JSON
        with (governance_dir / "evidence_index.json").open("r", encoding="utf-8") as f:
            evidence_index = json.load(f)
        
        assert "governance_id" in evidence_index
        assert "evidence_by_candidate" in evidence_index


def test_governance_json_contains_subsample_fields_in_evidence() -> None:
    """
    Test that governance.json contains subsample fields in evidence.
    
    Critical requirement: subsample info must be in evidence chain.
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        governance_dir = Path(tmpdir) / "governance" / "test-123"
        
        # Create report with subsample fields in evidence
        evidence = [
            EvidenceRef(
                run_id="stage1-123",
                stage_name="stage1_topk",
                artifact_paths=["manifest.json", "metrics.json", "winners.json"],
                key_metrics={
                    "param_id": 0,
                    "net_profit": 100.0,
                    "stage_planned_subsample": 0.1,
                    "param_subsample_rate": 0.1,
                    "params_effective": 100,
                },
            ),
        ]
        
        item = GovernanceItem(
            candidate_id="donchian_atr:abc123def456",
            decision=Decision.KEEP,
            reasons=[],
            evidence=evidence,
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="abc123def456",
        )
        
        report = GovernanceReport(
            items=[item],
            metadata={
                "governance_id": "gov-123",
                "season": "test_season",
                "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
                "git_sha": "abc123def456",
                "decisions": {"KEEP": 1, "FREEZE": 0, "DROP": 0},
            },
        )
        
        # Write artifacts
        write_governance_artifacts(governance_dir, report)
        
        # Verify subsample fields are in governance.json
        with (governance_dir / "governance.json").open("r", encoding="utf-8") as f:
            governance_dict = json.load(f)
        
        item_dict = governance_dict["items"][0]
        evidence_dict = item_dict["evidence"][0]
        key_metrics = evidence_dict["key_metrics"]
        
        assert "stage_planned_subsample" in key_metrics
        assert "param_subsample_rate" in key_metrics
        assert "params_effective" in key_metrics


def test_readme_contains_freeze_reasons() -> None:
    """
    Test that README.md contains FREEZE reasons.
    
    Requirement: README must list FREEZE reasons (concise).
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        governance_dir = Path(tmpdir) / "governance" / "test-123"
        
        # Create report with FREEZE item
        evidence = [
            EvidenceRef(
                run_id="stage1-123",
                stage_name="stage1_topk",
                artifact_paths=["manifest.json", "metrics.json", "winners.json"],
                key_metrics={"param_id": 0, "net_profit": 100.0},
            ),
        ]
        
        freeze_item = GovernanceItem(
            candidate_id="donchian_atr:abc123def456",
            decision=Decision.FREEZE,
            reasons=["R3: density_5_over_threshold_3"],
            evidence=evidence,
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="abc123def456",
        )
        
        report = GovernanceReport(
            items=[freeze_item],
            metadata={
                "governance_id": "gov-123",
                "season": "test_season",
                "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
                "git_sha": "abc123def456",
                "decisions": {"KEEP": 0, "FREEZE": 1, "DROP": 0},
            },
        )
        
        # Write artifacts
        write_governance_artifacts(governance_dir, report)
        
        # Verify README contains FREEZE reasons
        readme_text = (governance_dir / "README.md").read_text(encoding="utf-8")
        assert "FREEZE Reasons" in readme_text
        assert "donchian_atr:abc123def456" in readme_text
        assert "density" in readme_text


================================================================================
FILE: tests/test_grid_runner_smoke.py
================================================================================

import numpy as np

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def _ohlc():
    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)
    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)
    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)
    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)
    return o, h, l, c


def test_grid_runner_smoke_shapes_and_no_crash():
    o, h, l, c = _ohlc()

    # params: [channel_len, atr_len, stop_mult]
    params = np.array(
        [
            [2, 2, 1.0],
            [3, 2, 1.5],
            [99999, 3, 2.0],  # should produce 0 trades
            [2, 99999, 2.0],  # atr_len > n should be safe (atr_wilder returns all-NaN -> kernel => 0 trades)
        ],
        dtype=np.float64,
    )

    out = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=True)
    m = out["metrics"]
    order = out["order"]

    assert isinstance(m, np.ndarray)
    assert m.shape == (params.shape[0], 3)
    assert isinstance(order, np.ndarray)
    assert order.shape == (params.shape[0],)
    assert set(order.tolist()) == set(range(params.shape[0]))
    # Optional stronger assertion: at least one row should have 0 trades due to atr_len > n
    assert np.any(m[:, 1] == 0.0)


def test_grid_runner_sorting_toggle():
    o, h, l, c = _ohlc()
    params = np.array(
        [
            [3, 2, 1.5],
            [2, 2, 1.0],
            [2, 3, 2.0],
        ],
        dtype=np.float64,
    )

    out_sorted = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=True)
    out_unsorted = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=False)

    assert out_sorted["metrics"].shape == out_unsorted["metrics"].shape == (3, 3)
    assert out_sorted["order"].shape == out_unsorted["order"].shape == (3,)
    # unsorted order should be identity
    np.testing.assert_array_equal(out_unsorted["order"], np.array([0, 1, 2], dtype=np.int64))



================================================================================
FILE: tests/test_indicators_consistency.py
================================================================================

import numpy as np

from FishBroWFS_V2.indicators.numba_indicators import (
    rolling_max,
    rolling_min,
    atr_wilder,
)


def _py_rolling_max(arr: np.ndarray, window: int) -> np.ndarray:
    n = arr.shape[0]
    out = np.full(n, np.nan, dtype=np.float64)
    if window <= 0:
        return out
    for i in range(n):
        if i < window - 1:
            continue
        start = i - window + 1
        m = arr[start]
        for j in range(start + 1, i + 1):
            v = arr[j]
            if v > m:
                m = v
        out[i] = m
    return out


def _py_rolling_min(arr: np.ndarray, window: int) -> np.ndarray:
    n = arr.shape[0]
    out = np.full(n, np.nan, dtype=np.float64)
    if window <= 0:
        return out
    for i in range(n):
        if i < window - 1:
            continue
        start = i - window + 1
        m = arr[start]
        for j in range(start + 1, i + 1):
            v = arr[j]
            if v < m:
                m = v
        out[i] = m
    return out


def _py_atr_wilder(high, low, close, window):
    n = len(high)
    out = np.full(n, np.nan, dtype=np.float64)
    if window > n:
        return out
    tr = np.empty(n, dtype=np.float64)
    tr[0] = high[0] - low[0]
    for i in range(1, n):
        tr[i] = max(
            high[i] - low[i],
            abs(high[i] - close[i - 1]),
            abs(low[i] - close[i - 1]),
        )
    end = window
    out[end - 1] = np.mean(tr[:end])
    for i in range(window, n):
        out[i] = (out[i - 1] * (window - 1) + tr[i]) / window
    return out


def test_rolling_max_min_consistency():
    arr = np.array([1.0, 3.0, 2.0, 5.0, 4.0], dtype=np.float64)
    w = 3

    mx_py = _py_rolling_max(arr, w)
    mn_py = _py_rolling_min(arr, w)

    mx = rolling_max(arr, w)
    mn = rolling_min(arr, w)

    np.testing.assert_allclose(mx, mx_py, rtol=0.0, atol=0.0)
    np.testing.assert_allclose(mn, mn_py, rtol=0.0, atol=0.0)


def test_atr_wilder_consistency():
    high = np.array([10, 11, 12, 11, 13, 14], dtype=np.float64)
    low = np.array([9, 9, 10, 9, 11, 12], dtype=np.float64)
    close = np.array([9.5, 10.5, 11.0, 10.0, 12.0, 13.0], dtype=np.float64)
    w = 3

    atr_py = _py_atr_wilder(high, low, close, w)
    atr = atr_wilder(high, low, close, w)

    np.testing.assert_allclose(atr, atr_py, rtol=0.0, atol=1e-12)


def test_atr_wilder_window_gt_n_returns_all_nan():
    high = np.array([10, 11], dtype=np.float64)
    low = np.array([9, 10], dtype=np.float64)
    close = np.array([9.5, 10.5], dtype=np.float64)
    atr = atr_wilder(high, low, close, 999)
    assert atr.shape == (2,)
    assert np.all(np.isnan(atr))



================================================================================
FILE: tests/test_indicators_precompute_bit_exact.py
================================================================================

"""
Stage P2-2 Step B: Bit-exact test for precomputed indicators.

Verifies that using precomputed indicators produces identical results
to computing indicators inline in the kernel.
"""
from __future__ import annotations

from dataclasses import asdict, is_dataclass

import numpy as np

from FishBroWFS_V2.engine.types import BarArrays, Fill
from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, PrecomputedIndicators, run_kernel_arrays
from FishBroWFS_V2.indicators.numba_indicators import rolling_max, rolling_min, atr_wilder


def _fill_to_tuple(f: Fill) -> tuple:
    """
    Convert Fill to a comparable tuple representation.
    
    Uses dataclasses.asdict for dataclass instances, falls back to __dict__ or repr.
    Returns sorted tuple to ensure deterministic comparison.
    """
    if is_dataclass(f):
        d = asdict(f)
    else:
        # fallback: __dict__ (for normal classes)
        d = dict(getattr(f, "__dict__", {}))
        if not d:
            # last resort: repr
            return (repr(f),)
    # Fixed ordering to avoid dict order differences
    return tuple(sorted(d.items()))


def test_indicators_precompute_bit_exact() -> None:
    """
    Test that precomputed indicators produce bit-exact results.
    
    Strategy:
    - Generate random bars
    - Choose a channel_len and atr_len
    - Run kernel twice:
      A: Without precomputation (precomp=None)
      B: With precomputation (precomp=PrecomputedIndicators(...))
    - Compare: donch_hi/lo/atr arrays, metrics, fills, equity
    """
    # Generate random bars
    rng = np.random.default_rng(42)
    n_bars = 500
    close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
    open_ = (high + low) / 2
    
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    bars = BarArrays(
        open=open_.astype(np.float64),
        high=high.astype(np.float64),
        low=low.astype(np.float64),
        close=close.astype(np.float64),
    )
    
    # Choose test parameters
    ch_len = 20
    atr_len = 10
    params = DonchianAtrParams(channel_len=ch_len, atr_len=atr_len, stop_mult=1.0)
    
    # Pre-compute indicators (same logic as runner_grid)
    donch_hi_precomp = rolling_max(bars.high, ch_len)
    donch_lo_precomp = rolling_min(bars.low, ch_len)
    atr_precomp = atr_wilder(bars.high, bars.low, bars.close, atr_len)
    
    precomp = PrecomputedIndicators(
        donch_hi=donch_hi_precomp,
        donch_lo=donch_lo_precomp,
        atr=atr_precomp,
    )
    
    # Run A: Without precomputation
    result_a = run_kernel_arrays(
        bars=bars,
        params=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        precomp=None,
    )
    
    # Run B: With precomputation
    result_b = run_kernel_arrays(
        bars=bars,
        params=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        precomp=precomp,
    )
    
    # Verify indicators are bit-exact (if we could access them)
    # Note: We can't directly access internal arrays, but we verify outputs
    
    # Verify metrics are identical
    metrics_a = result_a["metrics"]
    metrics_b = result_b["metrics"]
    assert metrics_a["net_profit"] == metrics_b["net_profit"], "net_profit must be identical"
    assert metrics_a["trades"] == metrics_b["trades"], "trades must be identical"
    assert metrics_a["max_dd"] == metrics_b["max_dd"], "max_dd must be identical"
    
    # Verify fills are identical
    fills_a = result_a["fills"]
    fills_b = result_b["fills"]
    assert len(fills_a) == len(fills_b), "fills count must be identical"
    for i, (fill_a, fill_b) in enumerate(zip(fills_a, fills_b)):
        assert _fill_to_tuple(fill_a) == _fill_to_tuple(fill_b), f"fill[{i}] must be identical"
    
    # Verify equity arrays are bit-exact
    equity_a = result_a["equity"]
    equity_b = result_b["equity"]
    assert equity_a.shape == equity_b.shape, "equity shape must be identical"
    np.testing.assert_array_equal(equity_a, equity_b, "equity must be bit-exact")
    
    # Verify pnl arrays are bit-exact
    pnl_a = result_a["pnl"]
    pnl_b = result_b["pnl"]
    assert pnl_a.shape == pnl_b.shape, "pnl shape must be identical"
    np.testing.assert_array_equal(pnl_a, pnl_b, "pnl must be bit-exact")
    
    # Verify observability counts are identical
    obs_a = result_a.get("_obs", {})
    obs_b = result_b.get("_obs", {})
    assert obs_a.get("intents_total") == obs_b.get("intents_total"), "intents_total must be identical"
    assert obs_a.get("fills_total") == obs_b.get("fills_total"), "fills_total must be identical"


================================================================================
FILE: tests/test_json_pointer.py
================================================================================

"""Tests for JSON Pointer resolver.

Tests normal pointer, list index, missing keys, and never-raise contract.
"""

from __future__ import annotations

import pytest

from FishBroWFS_V2.gui.viewer.json_pointer import resolve_json_pointer


def test_normal_pointer() -> None:
    """Test normal object key pointer."""
    data = {
        "a": {
            "b": {
                "c": "value"
            }
        }
    }
    
    found, value = resolve_json_pointer(data, "/a/b/c")
    assert found is True
    assert value == "value"
    
    found, value = resolve_json_pointer(data, "/a/b")
    assert found is True
    assert value == {"c": "value"}


def test_list_index() -> None:
    """Test list index in pointer."""
    data = {
        "items": [
            {"name": "first"},
            {"name": "second"},
        ]
    }
    
    found, value = resolve_json_pointer(data, "/items/0/name")
    assert found is True
    assert value == "first"
    
    found, value = resolve_json_pointer(data, "/items/1/name")
    assert found is True
    assert value == "second"
    
    found, value = resolve_json_pointer(data, "/items/0")
    assert found is True
    assert value == {"name": "first"}


def test_list_index_out_of_bounds() -> None:
    """Test list index out of bounds."""
    data = {
        "items": [1, 2, 3]
    }
    
    found, value = resolve_json_pointer(data, "/items/10")
    assert found is False
    assert value is None
    
    found, value = resolve_json_pointer(data, "/items/-1")
    assert found is False
    assert value is None


def test_missing_key() -> None:
    """Test missing key in pointer."""
    data = {
        "a": {
            "b": "value"
        }
    }
    
    found, value = resolve_json_pointer(data, "/a/c")
    assert found is False
    assert value is None
    
    found, value = resolve_json_pointer(data, "/x/y")
    assert found is False
    assert value is None


def test_root_pointer_disabled() -> None:
    """Test root pointer is disabled (by design for Viewer UX)."""
    data = {"a": 1, "b": 2}
    
    # Root pointer "/" is intentionally disabled
    found, value = resolve_json_pointer(data, "/")
    assert found is False
    assert value is None
    
    # Empty string is also disabled
    found, value = resolve_json_pointer(data, "")
    assert found is False
    assert value is None


def test_invalid_pointer_format() -> None:
    """Test invalid pointer format."""
    data = {"a": 1}
    
    # Missing leading slash
    found, value = resolve_json_pointer(data, "a/b")
    assert found is False
    assert value is None


def test_nested_list_and_dict() -> None:
    """Test nested list and dict combination."""
    data = {
        "results": [
            {
                "metrics": {
                    "score": 100
                }
            },
            {
                "metrics": {
                    "score": 200
                }
            }
        ]
    }
    
    found, value = resolve_json_pointer(data, "/results/0/metrics/score")
    assert found is True
    assert value == 100
    
    found, value = resolve_json_pointer(data, "/results/1/metrics/score")
    assert found is True
    assert value == 200


def test_never_raises() -> None:
    """Test that resolve_json_pointer never raises exceptions."""
    # Test with None data
    found, value = resolve_json_pointer(None, "/a")  # type: ignore
    assert found is False
    assert value is None
    
    # Test with invalid data types
    found, value = resolve_json_pointer("string", "/a")  # type: ignore
    assert found is False
    assert value is None
    
    # Test with empty dict (valid, but key missing)
    found, value = resolve_json_pointer({}, "/a")
    assert found is False
    assert value is None
    
    # Test with invalid pointer type
    found, value = resolve_json_pointer({"a": 1}, None)  # type: ignore
    assert found is False
    assert value is None
    
    # Test with empty string pointer
    found, value = resolve_json_pointer({"a": 1}, "")
    assert found is False
    assert value is None
    
    # Test with root pointer (disabled)
    found, value = resolve_json_pointer({"a": 1}, "/")
    assert found is False
    assert value is None
    
    # Test with valid pointer
    found, value = resolve_json_pointer({"a": 1}, "/a")
    assert found is True
    assert value == 1


def test_critical_scenarios() -> None:
    """Test critical scenarios that must pass."""
    data = {"a": 1}
    
    # Scenario 1: None pointer
    found, value = resolve_json_pointer(data, None)  # type: ignore
    assert found is False
    assert value is None
    
    # Scenario 2: Empty string pointer
    found, value = resolve_json_pointer(data, "")
    assert found is False
    assert value is None
    
    # Scenario 3: Root pointer (disabled by design)
    found, value = resolve_json_pointer(data, "/")
    assert found is False
    assert value is None
    
    # Scenario 4: Valid pointer
    found, value = resolve_json_pointer(data, "/a")
    assert found is True
    assert value == 1


def test_intermediate_type_mismatch() -> None:
    """Test intermediate type mismatch."""
    data = {
        "items": "not_a_list"
    }
    
    # Try to access list index on string
    found, value = resolve_json_pointer(data, "/items/0")
    assert found is False
    assert value is None
    
    data = {
        "items": [1, 2, 3]
    }
    
    # Try to access dict key on list
    found, value = resolve_json_pointer(data, "/items/key")
    assert found is False
    assert value is None


================================================================================
FILE: tests/test_kernel_parity_contract.py
================================================================================

"""Kernel parity contract tests - Phase 4 Stage C.

These tests ensure that Cursor kernel results are bit-level identical to matcher_core.
This is a critical contract: any deviation indicates a semantic bug.

Tests use simulate_run() unified entry point to ensure we test the actual API used in production.
"""

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.simulate import simulate_run
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def _bars1(o, h, l, c):
    """Helper to create single-bar BarArrays."""
    return normalize_bars(
        np.array([o], dtype=np.float64),
        np.array([h], dtype=np.float64),
        np.array([l], dtype=np.float64),
        np.array([c], dtype=np.float64),
    )


def _bars2(o0, h0, l0, c0, o1, h1, l1, c1):
    """Helper to create two-bar BarArrays."""
    return normalize_bars(
        np.array([o0, o1], dtype=np.float64),
        np.array([h0, h1], dtype=np.float64),
        np.array([l0, l1], dtype=np.float64),
        np.array([c0, c1], dtype=np.float64),
    )


def _bars3(o0, h0, l0, c0, o1, h1, l1, c1, o2, h2, l2, c2):
    """Helper to create three-bar BarArrays."""
    return normalize_bars(
        np.array([o0, o1, o2], dtype=np.float64),
        np.array([h0, h1, h2], dtype=np.float64),
        np.array([l0, l1, l2], dtype=np.float64),
        np.array([c0, c1, c2], dtype=np.float64),
    )


def _compute_position_path(fills):
    """
    Compute position path from fills sequence.
    
    Returns list of (bar_index, position) tuples where position is:
    - 0: flat
    - 1: long
    - -1: short
    """
    pos_path = []
    current_pos = 0
    
    # Group fills by bar_index
    fills_by_bar = {}
    for fill in fills:
        bar_idx = fill.bar_index
        if bar_idx not in fills_by_bar:
            fills_by_bar[bar_idx] = []
        fills_by_bar[bar_idx].append(fill)
    
    # Process fills chronologically
    for bar_idx in sorted(fills_by_bar.keys()):
        bar_fills = fills_by_bar[bar_idx]
        # Sort by role (ENTRY first), then kind, then order_id
        bar_fills.sort(key=lambda f: (
            0 if f.role == OrderRole.ENTRY else 1,
            0 if f.kind == OrderKind.STOP else 1,
            f.order_id
        ))
        
        for fill in bar_fills:
            if fill.role == OrderRole.ENTRY:
                if fill.side == Side.BUY:
                    current_pos = 1
                else:
                    current_pos = -1
            elif fill.role == OrderRole.EXIT:
                current_pos = 0
        
        pos_path.append((bar_idx, current_pos))
    
    return pos_path


def _assert_fills_identical(cursor_fills, reference_fills):
    """Assert that two fill sequences are bit-level identical."""
    assert len(cursor_fills) == len(reference_fills), (
        f"Fill count mismatch: cursor={len(cursor_fills)}, reference={len(reference_fills)}"
    )
    
    for i, (c_fill, r_fill) in enumerate(zip(cursor_fills, reference_fills)):
        assert c_fill.bar_index == r_fill.bar_index, (
            f"Fill {i}: bar_index mismatch: cursor={c_fill.bar_index}, reference={r_fill.bar_index}"
        )
        assert c_fill.role == r_fill.role, (
            f"Fill {i}: role mismatch: cursor={c_fill.role}, reference={r_fill.role}"
        )
        assert c_fill.kind == r_fill.kind, (
            f"Fill {i}: kind mismatch: cursor={c_fill.kind}, reference={r_fill.kind}"
        )
        assert c_fill.side == r_fill.side, (
            f"Fill {i}: side mismatch: cursor={c_fill.side}, reference={r_fill.side}"
        )
        assert c_fill.price == r_fill.price, (
            f"Fill {i}: price mismatch: cursor={c_fill.price}, reference={r_fill.price}"
        )
        assert c_fill.qty == r_fill.qty, (
            f"Fill {i}: qty mismatch: cursor={c_fill.qty}, reference={r_fill.qty}"
        )
        assert c_fill.order_id == r_fill.order_id, (
            f"Fill {i}: order_id mismatch: cursor={c_fill.order_id}, reference={r_fill.order_id}"
        )


def _assert_position_path_identical(cursor_fills, reference_fills):
    """Assert that position paths are identical."""
    cursor_path = _compute_position_path(cursor_fills)
    reference_path = _compute_position_path(reference_fills)
    
    assert cursor_path == reference_path, (
        f"Position path mismatch:\n"
        f"  cursor: {cursor_path}\n"
        f"  reference: {reference_path}"
    )


def test_parity_next_bar_activation():
    """Test next-bar activation rule: order created at bar N activates at bar N+1."""
    # Order created at bar 0, should activate at bar 1
    bars = _bars2(
        100, 105, 95, 100,  # bar 0: high=105 would hit stop 102, but order not active yet
        100, 105, 95, 100,  # bar 1: order activates, should fill
    )
    intents = [
        OrderIntent(order_id=1, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    _assert_position_path_identical(cursor_result.fills, reference_result.fills)
    
    # Verify: should fill at bar 1, not bar 0
    assert len(cursor_result.fills) == 1
    assert cursor_result.fills[0].bar_index == 1


def test_parity_stop_fill_price_exact():
    """Test stop fill price = stop_price (not max(open, stop_price))."""
    # Buy stop at 100, open=95, high=105 -> should fill at 100 (stop_price), not 105
    bars = _bars1(95, 105, 90, 100)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].price == 100.0  # stop_price, not high


def test_parity_stop_fill_price_gap_up():
    """Test stop fill price on gap up: fill at open if open >= stop_price."""
    # Buy stop at 100, open=105 (gap up) -> should fill at 105 (open), not 100
    bars = _bars1(105, 110, 105, 108)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].price == 105.0  # open (gap branch)


def test_parity_stop_fill_price_gap_down():
    """Test stop fill price on gap down: fill at open if open <= stop_price."""
    # Sell stop at 100, open=90 (gap down) -> should fill at 90 (open), not 100
    bars = _bars2(
        100, 100, 100, 100,  # bar 0: enter long
        90, 95, 80, 85,      # bar 1: exit stop gap down
    )
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    _assert_position_path_identical(cursor_result.fills, reference_result.fills)
    # Exit fill should be at open (90) due to gap down
    assert cursor_result.fills[1].price == 90.0


def test_parity_same_bar_entry_then_exit():
    """Test same-bar entry then exit is allowed."""
    # Same bar: entry buy stop 105, exit sell stop 95
    # Bar: O=100 H=120 L=90
    # Entry: Buy Stop 105 -> fills at 105
    # Exit: Sell Stop 95 -> fills at 95 (after entry)
    bars = _bars1(100, 120, 90, 110)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),
        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    _assert_position_path_identical(cursor_result.fills, reference_result.fills)
    
    assert len(cursor_result.fills) == 2
    assert cursor_result.fills[0].role == OrderRole.ENTRY
    assert cursor_result.fills[0].price == 105.0
    assert cursor_result.fills[1].role == OrderRole.EXIT
    assert cursor_result.fills[1].price == 95.0
    assert cursor_result.fills[0].bar_index == cursor_result.fills[1].bar_index


def test_parity_stop_priority_over_limit():
    """Test STOP priority over LIMIT (same role, same bar)."""
    # Entry: Buy Stop 102 and Buy Limit 110 both triggerable
    # STOP must win
    bars = _bars1(100, 115, 95, 105)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.BUY, price=110.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].kind == OrderKind.STOP
    assert cursor_result.fills[0].order_id == 1


def test_parity_stop_priority_exit():
    """Test STOP priority over LIMIT on exit."""
    # Enter long first, then exit with both stop and limit triggerable
    # STOP must win
    bars = _bars2(
        100, 100, 100, 100,  # bar 0: enter long
        100, 110, 80, 90,    # bar 1: exit stop 90 and exit limit 110 both triggerable
    )
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=90.0),
        OrderIntent(order_id=3, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.LIMIT, side=Side.SELL, price=110.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    _assert_position_path_identical(cursor_result.fills, reference_result.fills)
    
    # Exit fill should be STOP
    assert cursor_result.fills[1].kind == OrderKind.STOP
    assert cursor_result.fills[1].order_id == 2


def test_parity_order_id_tie_break():
    """Test order_id tie-break when kind is same."""
    # Two STOP orders, lower order_id should win
    bars = _bars1(100, 110, 95, 105)
    intents = [
        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].order_id == 1  # Lower order_id wins


def test_parity_limit_gap_down_better_fill():
    """Test limit order gap down: fill at open if better."""
    # Buy limit at 100, open=90 (gap down) -> should fill at 90 (open), not 100
    bars = _bars1(90, 95, 85, 92)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.BUY, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].price == 90.0  # open (better fill)


def test_parity_limit_gap_up_better_fill():
    """Test limit order gap up: fill at open if better."""
    # Sell limit at 100, open=105 (gap up) -> should fill at 105 (open), not 100
    bars = _bars1(105, 110, 100, 108)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.SELL, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].price == 105.0  # open (better fill)


def test_parity_no_fill_when_not_touched():
    """Test no fill when price not touched."""
    bars = _bars1(90, 95, 90, 92)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert len(cursor_result.fills) == 0


def test_parity_open_equals_stop_gap_branch():
    """Test open equals stop price: gap branch but same price."""
    bars = _bars1(100, 100, 90, 95)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].price == 100.0  # open == stop_price


def test_parity_multiple_bars_complex():
    """Test complex multi-bar scenario with entry and exit."""
    bars = _bars3(
        100, 105, 95, 100,   # bar 0: enter long at 102 (buy stop)
        100, 110, 80, 90,    # bar 1: exit stop 90 triggers
        95, 100, 90, 95,     # bar 2: no fills
    )
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=90.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    _assert_position_path_identical(cursor_result.fills, reference_result.fills)
    
    # Verify position path
    pos_path = _compute_position_path(cursor_result.fills)
    assert pos_path == [(0, 1), (1, 0)]  # Enter at bar 0, exit at bar 1


def test_parity_entry_skipped_when_position_exists():
    """Test that entry is skipped when position already exists."""
    # Enter long at bar 0, then at bar 1 try to enter again (should be skipped) and exit
    bars = _bars2(
        100, 100, 100, 100,  # bar 0: enter long
        100, 110, 90, 100,   # bar 1: exit stop 95 triggers, entry stop 105 also triggerable but skipped
    )
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),
        OrderIntent(order_id=3, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    _assert_position_path_identical(cursor_result.fills, reference_result.fills)
    
    # Should have entry at bar 0 and exit at bar 1
    # Entry at bar 1 should be skipped (position already exists)
    assert len(cursor_result.fills) == 2
    assert cursor_result.fills[0].bar_index == 0
    assert cursor_result.fills[0].role == OrderRole.ENTRY
    assert cursor_result.fills[1].bar_index == 1
    assert cursor_result.fills[1].role == OrderRole.EXIT


================================================================================
FILE: tests/test_kpi_drilldown_no_raise.py
================================================================================

"""Tests for KPI drill-down - no raise contract.

Tests missing artifacts, wrong pointers, empty session_state.
UI functions should never raise exceptions.
"""

from __future__ import annotations

from pathlib import Path
from unittest.mock import Mock, patch

import pytest
import streamlit as st

from FishBroWFS_V2.gui.viewer.components.kpi_table import render_kpi_table
from FishBroWFS_V2.gui.viewer.components.evidence_panel import render_evidence_panel
from FishBroWFS_V2.gui.viewer.kpi_registry import KPI_EVIDENCE_REGISTRY


def test_kpi_table_missing_name() -> None:
    """Test KPI table handles missing name field."""
    with patch("streamlit.subheader"), \
         patch("streamlit.columns"), \
         patch("streamlit.markdown"), \
         patch("streamlit.text"), \
         patch("streamlit.button"):
        
        # Row without name
        kpi_rows = [
            {"value": 100}
        ]
        
        # Should not raise
        render_kpi_table(kpi_rows)


def test_kpi_table_missing_value() -> None:
    """Test KPI table handles missing value field."""
    with patch("streamlit.subheader"), \
         patch("streamlit.columns"), \
         patch("streamlit.markdown"), \
         patch("streamlit.text"), \
         patch("streamlit.button"):
        
        # Row without value
        kpi_rows = [
            {"name": "net_profit"}
        ]
        
        # Should not raise
        render_kpi_table(kpi_rows)


def test_kpi_table_empty_rows() -> None:
    """Test KPI table handles empty rows list."""
    with patch("streamlit.info"):
        # Empty list
        render_kpi_table([])
        
        # Should not raise


def test_kpi_table_unknown_kpi() -> None:
    """Test KPI table handles unknown KPI (not in registry)."""
    with patch("streamlit.subheader"), \
         patch("streamlit.columns"), \
         patch("streamlit.markdown"), \
         patch("streamlit.text"), \
         patch("streamlit.button"):
        
        # KPI not in registry
        kpi_rows = [
            {"name": "unknown_kpi", "value": 100}
        ]
        
        # Should not raise - displays but not clickable
        render_kpi_table(kpi_rows)


def test_evidence_panel_missing_artifact() -> None:
    """Test evidence panel handles missing artifact."""
    with patch("streamlit.subheader"), \
         patch("streamlit.markdown"), \
         patch("streamlit.warning"), \
         patch("streamlit.caption"):
        
        # Mock session state with missing artifact
        with patch.dict(st.session_state, {
            "active_evidence": {
                "kpi_name": "net_profit",
                "artifact": "winners_v2",
                "json_pointer": "/summary/net_profit",
            }
        }):
            # Artifacts dict missing winners_v2
            artifacts = {
                "manifest": {},
            }
            
            # Should not raise - shows warning
            render_evidence_panel(artifacts)


def test_evidence_panel_wrong_pointer() -> None:
    """Test evidence panel handles wrong JSON pointer."""
    with patch("streamlit.subheader"), \
         patch("streamlit.markdown"), \
         patch("streamlit.warning"), \
         patch("streamlit.info"), \
         patch("streamlit.caption"):
        
        # Mock session state
        with patch.dict(st.session_state, {
            "active_evidence": {
                "kpi_name": "net_profit",
                "artifact": "winners_v2",
                "json_pointer": "/nonexistent/pointer",
            }
        }):
            # Artifact exists but pointer is wrong
            artifacts = {
                "winners_v2": {
                    "summary": {
                        "net_profit": 100
                    }
                }
            }
            
            # Should not raise - shows warning
            render_evidence_panel(artifacts)


def test_evidence_panel_empty_session_state() -> None:
    """Test evidence panel handles empty session_state."""
    with patch("streamlit.subheader"):
        # Empty session state
        with patch.dict(st.session_state, {}, clear=True):
            artifacts = {
                "winners_v2": {}
            }
            
            # Should not raise - returns early
            render_evidence_panel(artifacts)


def test_evidence_panel_invalid_session_state() -> None:
    """Test evidence panel handles invalid session_state structure."""
    with patch("streamlit.subheader"), \
         patch("streamlit.markdown"), \
         patch("streamlit.warning"):
        
        # Invalid session state structure
        with patch.dict(st.session_state, {
            "active_evidence": "not_a_dict"
        }):
            artifacts = {}
            
            # Should not raise - handles gracefully
            render_evidence_panel(artifacts)


def test_evidence_panel_missing_fields() -> None:
    """Test evidence panel handles missing fields in session_state."""
    with patch("streamlit.subheader"), \
         patch("streamlit.markdown"), \
         patch("streamlit.warning"):
        
        # Missing fields in active_evidence
        with patch.dict(st.session_state, {
            "active_evidence": {
                "kpi_name": "net_profit",
                # Missing artifact, json_pointer
            }
        }):
            artifacts = {}
            
            # Should not raise - handles gracefully
            render_evidence_panel(artifacts)


def test_kpi_table_exception_handling() -> None:
    """Test KPI table handles exceptions gracefully."""
    # Mock streamlit to raise exception
    with patch("streamlit.subheader", side_effect=Exception("Streamlit error")):
        kpi_rows = [
            {"name": "net_profit", "value": 100}
        ]
        
        # Should catch exception and show error
        with patch("streamlit.error"):
            render_kpi_table(kpi_rows)
            # Should not raise


def test_evidence_panel_exception_handling() -> None:
    """Test evidence panel handles exceptions gracefully."""
    # Mock streamlit to raise exception
    with patch("streamlit.subheader", side_effect=Exception("Streamlit error")):
        artifacts = {}
        
        # Should catch exception and show error
        with patch("streamlit.error"):
            render_evidence_panel(artifacts)
            # Should not raise


================================================================================
FILE: tests/test_kpi_registry.py
================================================================================

"""Tests for KPI Registry.

Tests registry key â†’ EvidenceLink mapping and defensive behavior.
"""

from __future__ import annotations

import pytest

from FishBroWFS_V2.gui.viewer.kpi_registry import (
    KPI_EVIDENCE_REGISTRY,
    get_evidence_link,
    has_evidence,
    EvidenceLink,
)


def test_registry_keys_exist() -> None:
    """Test that registry keys map to correct EvidenceLink."""
    # Test net_profit
    link = get_evidence_link("net_profit")
    assert link is not None
    assert link.artifact == "winners_v2"
    assert link.json_pointer == "/summary/net_profit"
    assert "profit" in link.description.lower()
    
    # Test max_drawdown
    link = get_evidence_link("max_drawdown")
    assert link is not None
    assert link.artifact == "winners_v2"
    assert link.json_pointer == "/summary/max_drawdown"
    
    # Test num_trades
    link = get_evidence_link("num_trades")
    assert link is not None
    assert link.artifact == "winners_v2"
    assert link.json_pointer == "/summary/num_trades"
    
    # Test final_score
    link = get_evidence_link("final_score")
    assert link is not None
    assert link.artifact == "governance"
    assert link.json_pointer == "/scoring/final_score"


def test_unknown_kpi_returns_none() -> None:
    """Test that unknown KPI names return None without crashing."""
    link = get_evidence_link("unknown_kpi")
    assert link is None
    
    link = get_evidence_link("")
    assert link is None
    
    link = get_evidence_link("nonexistent")
    assert link is None


def test_has_evidence() -> None:
    """Test has_evidence function."""
    assert has_evidence("net_profit") is True
    assert has_evidence("max_drawdown") is True
    assert has_evidence("num_trades") is True
    assert has_evidence("final_score") is True
    
    assert has_evidence("unknown_kpi") is False
    assert has_evidence("") is False


def test_registry_never_raises() -> None:
    """Test that registry functions never raise exceptions."""
    # Test with invalid input types
    try:
        get_evidence_link(None)  # type: ignore
    except Exception:
        pytest.fail("get_evidence_link should not raise")
    
    try:
        has_evidence(None)  # type: ignore
    except Exception:
        pytest.fail("has_evidence should not raise")


def test_registry_structure() -> None:
    """Test that registry has correct structure."""
    assert isinstance(KPI_EVIDENCE_REGISTRY, dict)
    assert len(KPI_EVIDENCE_REGISTRY) > 0
    
    for kpi_name, link in KPI_EVIDENCE_REGISTRY.items():
        assert isinstance(kpi_name, str)
        assert isinstance(link, EvidenceLink)
        assert link.artifact in ("manifest", "winners_v2", "governance")
        assert link.json_pointer.startswith("/")
        assert isinstance(link.description, str)


================================================================================
FILE: tests/test_no_ui_imports_anywhere.py
================================================================================

"""Contract test: No ui namespace imports anywhere in FishBroWFS_V2.

Ensures the entire FishBroWFS_V2 package does not import from ui namespace.
This is a "truth test" to prevent any ui.* imports from being reintroduced.
"""

from __future__ import annotations

import pkgutil

import pytest


def test_no_ui_namespace_anywhere() -> None:
    """Test that FishBroWFS_V2 package does not import from ui namespace."""
    import FishBroWFS_V2
    
    # Walk through all modules in FishBroWFS_V2 package
    # If any module imports ui.*, it will fail during import
    for importer, modname, ispkg in pkgutil.walk_packages(FishBroWFS_V2.__path__, FishBroWFS_V2.__name__ + "."):
        try:
            # Import module - this will fail if it imports ui.* and ui doesn't exist
            __import__(modname, fromlist=[""])
        except ImportError as e:
            # Check if error is related to ui namespace
            if "ui" in str(e) and ("No module named" in str(e) or "cannot import name" in str(e)):
                pytest.fail(
                    f"Module {modname} imports from ui namespace (ui module no longer exists): {e}"
                )
            # Re-raise other ImportErrors (might be legitimate missing dependencies)
            raise


================================================================================
FILE: tests/test_no_ui_namespace.py
================================================================================

"""Contract test: No ui namespace imports allowed.

Ensures the entire FishBroWFS_V2 package does not import from ui namespace.
"""

from __future__ import annotations

import ast
import pkgutil
from pathlib import Path

import pytest


def test_no_ui_namespace_importable() -> None:
    """Test that FishBroWFS_V2 package does not import from ui namespace."""
    import FishBroWFS_V2 as pkg
    
    ui_imports: list[tuple[str, str]] = []
    
    # Walk through all modules in FishBroWFS_V2 package
    for importer, modname, ispkg in pkgutil.walk_packages(pkg.__path__, pkg.__name__ + "."):
        try:
            # Import module to trigger any import errors
            module = __import__(modname, fromlist=[""])
            
            # Get source file path
            if hasattr(module, "__file__") and module.__file__:
                source_path = Path(module.__file__)
                if source_path.exists() and source_path.suffix == ".py":
                    # Parse AST to find imports
                    try:
                        with source_path.open("r", encoding="utf-8") as f:
                            tree = ast.parse(f.read(), filename=str(source_path))
                        
                        # Check all imports
                        for node in ast.walk(tree):
                            if isinstance(node, ast.Import):
                                for alias in node.names:
                                    if alias.name.startswith("ui."):
                                        ui_imports.append((modname, alias.name))
                            elif isinstance(node, ast.ImportFrom):
                                if node.module and node.module.startswith("ui."):
                                    ui_imports.append((modname, f"from {node.module}"))
                    except (SyntaxError, UnicodeDecodeError):
                        # Skip files that can't be parsed (might be binary or invalid)
                        pass
        except Exception as e:
            # Skip modules that fail to import (might be missing dependencies)
            # But log for debugging if it's not an ImportError
            if "ImportError" not in str(type(e)) and "ModuleNotFoundError" not in str(type(e)):
                pytest.fail(f"Unexpected error importing {modname}: {e}")
    
    # Should have no ui.* imports
    if ui_imports:
        pytest.fail(
            f"FishBroWFS_V2 package contains ui.* imports:\n"
            + "\n".join(f"  {mod}: {imp}" for mod, imp in ui_imports)
        )


def test_viewer_no_ui_imports() -> None:
    """Test that Viewer package specifically does not import from ui namespace."""
    import FishBroWFS_V2.gui.viewer as viewer
    
    ui_imports: list[tuple[str, str]] = []
    
    # Walk through all modules in viewer package
    for importer, modname, ispkg in pkgutil.walk_packages(viewer.__path__, viewer.__name__ + "."):
        try:
            module = __import__(modname, fromlist=[""])
            
            if hasattr(module, "__file__") and module.__file__:
                source_path = Path(module.__file__)
                if source_path.exists() and source_path.suffix == ".py":
                    try:
                        with source_path.open("r", encoding="utf-8") as f:
                            tree = ast.parse(f.read(), filename=str(source_path))
                        
                        for node in ast.walk(tree):
                            if isinstance(node, ast.Import):
                                for alias in node.names:
                                    if alias.name.startswith("ui."):
                                        ui_imports.append((modname, alias.name))
                            elif isinstance(node, ast.ImportFrom):
                                if node.module and node.module.startswith("ui."):
                                    ui_imports.append((modname, f"from {node.module}"))
                    except (SyntaxError, UnicodeDecodeError):
                        pass
        except Exception as e:
            if "ImportError" not in str(type(e)) and "ModuleNotFoundError" not in str(type(e)):
                pytest.fail(f"Unexpected error importing {modname}: {e}")
    
    if ui_imports:
        pytest.fail(
            f"Viewer package contains ui.* imports:\n"
            + "\n".join(f"  {mod}: {imp}" for mod, imp in ui_imports)
        )


def test_no_ui_directory_exists() -> None:
    """Test that ui/ directory does not exist in repo root (repo structure contract)."""
    repo_root = Path(__file__).parent.parent
    ui_dir = repo_root / "ui"
    
    if ui_dir.exists():
        pytest.fail(f"ui/ directory must not exist in repo root, but found at {ui_dir}")


def test_makefile_no_ui_paths() -> None:
    """Test that Makefile does not reference ui/ paths."""
    repo_root = Path(__file__).parent.parent
    makefile_path = repo_root / "Makefile"
    
    assert makefile_path.exists()
    
    content = makefile_path.read_text()
    
    # Check for ui/ references (excluding comments)
    lines = content.split("\n")
    for i, line in enumerate(lines, 1):
        # Skip comments
        if line.strip().startswith("#"):
            continue
        # Check for ui/ path references
        if "ui/" in line or " ui." in line or "ui.app_streamlit" in line:
            pytest.fail(f"Makefile line {i} contains ui/ reference: {line.strip()}")


================================================================================
FILE: tests/test_oom_gate.py
================================================================================

"""Tests for OOM gate decision maker.

Tests verify:
1. PASS case (estimated <= 60% of budget)
2. BLOCK case (estimated > 90% of budget)
3. AUTO_DOWNSAMPLE case (between 60% and 90%, with recommended_rate in (0,1])
4. Invalid input validation (bars<=0, rate<=0, etc.)
"""

from __future__ import annotations

import pytest

from FishBroWFS_V2.core.oom_gate import decide_gate, decide_oom_action, estimate_bytes
from FishBroWFS_V2.core.schemas.oom_gate import OomGateInput


def test_estimate_bytes() -> None:
    """Test memory estimation formula."""
    inp = OomGateInput(
        bars=1000,
        params=100,
        param_subsample_rate=0.5,
        intents_per_bar=2.0,
        bytes_per_intent_est=64,
    )
    
    estimated = estimate_bytes(inp)
    
    # Formula: bars * params * subsample * intents_per_bar * bytes_per_intent_est
    expected = 1000 * 100 * 0.5 * 2.0 * 64
    assert estimated == expected


def test_decide_gate_pass() -> None:
    """Test PASS decision when estimated <= 60% of budget."""
    # Small workload: 1M bytes, budget is 6GB (6_000_000_000)
    inp = OomGateInput(
        bars=100,
        params=10,
        param_subsample_rate=0.1,
        intents_per_bar=2.0,
        bytes_per_intent_est=64,
        ram_budget_bytes=6_000_000_000,
    )
    
    decision = decide_gate(inp)
    
    assert decision.decision == "PASS"
    assert decision.estimated_bytes <= inp.ram_budget_bytes * 0.6
    assert decision.recommended_subsample_rate is None
    assert "PASS" not in decision.notes  # Notes should describe the decision, not repeat it
    assert decision.estimated_bytes > 0


def test_decide_gate_block() -> None:
    """Test BLOCK decision when estimated > 90% of budget."""
    # Large workload: exceed 90% of budget
    # Set budget to 1GB for easier testing
    budget = 1_000_000_000  # 1GB
    # Need estimated > budget * 0.9 = 900MB
    # Let's use: 10000 bars * 10000 params * 1.0 rate * 2.0 intents * 64 bytes = 12.8GB
    inp = OomGateInput(
        bars=10000,
        params=10000,
        param_subsample_rate=1.0,
        intents_per_bar=2.0,
        bytes_per_intent_est=64,
        ram_budget_bytes=budget,
    )
    
    decision = decide_gate(inp)
    
    assert decision.decision == "BLOCK"
    assert decision.estimated_bytes > budget * 0.9
    assert decision.recommended_subsample_rate is None
    assert "BLOCKED" in decision.notes or "BLOCK" in decision.notes


def test_decide_gate_auto_downsample() -> None:
    """Test AUTO_DOWNSAMPLE decision when estimated between 60% and 90%."""
    # Medium workload: between 60% and 90% of budget
    # Set budget to 1GB for easier testing
    budget = 1_000_000_000  # 1GB
    # Need: budget * 0.6 < estimated < budget * 0.9
    # 600MB < estimated < 900MB
    # Let's use: 5000 bars * 5000 params * 1.0 rate * 2.0 intents * 64 bytes = 3.2GB
    # That's too high. Let's adjust:
    # For 700MB: 700_000_000 = bars * params * 1.0 * 2.0 * 64
    # bars * params = 700_000_000 / (2.0 * 64) = 5_468_750
    # Let's use: 5000 bars * 1094 params * 1.0 rate * 2.0 * 64 = ~700MB
    inp = OomGateInput(
        bars=5000,
        params=1094,
        param_subsample_rate=1.0,
        intents_per_bar=2.0,
        bytes_per_intent_est=64,
        ram_budget_bytes=budget,
    )
    
    decision = decide_gate(inp)
    
    assert decision.decision == "AUTO_DOWNSAMPLE"
    assert decision.estimated_bytes > budget * 0.6
    assert decision.estimated_bytes <= budget * 0.9
    assert decision.recommended_subsample_rate is not None
    assert 0.0 < decision.recommended_subsample_rate <= 1.0
    assert "recommended" in decision.notes.lower() or "subsample" in decision.notes.lower()


def test_decide_gate_auto_downsample_recommended_rate_calculation() -> None:
    """Test that recommended_rate is calculated correctly for AUTO_DOWNSAMPLE."""
    budget = 1_000_000_000  # 1GB
    bars = 1000
    params = 1000
    intents_per_bar = 2.0
    bytes_per_intent = 64
    
    # Use current rate that puts us in AUTO_DOWNSAMPLE zone
    inp = OomGateInput(
        bars=bars,
        params=params,
        param_subsample_rate=1.0,
        intents_per_bar=intents_per_bar,
        bytes_per_intent_est=bytes_per_intent,
        ram_budget_bytes=budget,
    )
    
    decision = decide_gate(inp)
    
    if decision.decision == "AUTO_DOWNSAMPLE":
        # Verify recommended_rate formula: (ram_budget * 0.6) / (bars * params * intents_per_bar * bytes_per_intent_est)
        expected_rate = (budget * 0.6) / (bars * params * intents_per_bar * bytes_per_intent)
        expected_rate = max(0.0, min(1.0, expected_rate))
        
        assert decision.recommended_subsample_rate is not None
        assert abs(decision.recommended_subsample_rate - expected_rate) < 0.0001  # Allow small floating point error


def test_invalid_input_bars_zero() -> None:
    """Test that bars <= 0 raises validation error."""
    with pytest.raises(Exception):  # Pydantic ValidationError
        OomGateInput(
            bars=0,
            params=100,
            param_subsample_rate=0.5,
        )


def test_invalid_input_bars_negative() -> None:
    """Test that bars < 0 raises validation error."""
    with pytest.raises(Exception):  # Pydantic ValidationError
        OomGateInput(
            bars=-1,
            params=100,
            param_subsample_rate=0.5,
        )


def test_invalid_input_params_zero() -> None:
    """Test that params <= 0 raises validation error."""
    with pytest.raises(Exception):  # Pydantic ValidationError
        OomGateInput(
            bars=1000,
            params=0,
            param_subsample_rate=0.5,
        )


def test_invalid_input_subsample_rate_zero() -> None:
    """Test that param_subsample_rate <= 0 raises validation error."""
    with pytest.raises(Exception):  # Pydantic ValidationError
        OomGateInput(
            bars=1000,
            params=100,
            param_subsample_rate=0.0,
        )


def test_invalid_input_subsample_rate_negative() -> None:
    """Test that param_subsample_rate < 0 raises validation error."""
    with pytest.raises(Exception):  # Pydantic ValidationError
        OomGateInput(
            bars=1000,
            params=100,
            param_subsample_rate=-0.1,
        )


def test_invalid_input_subsample_rate_over_one() -> None:
    """Test that param_subsample_rate > 1.0 raises validation error."""
    with pytest.raises(Exception):  # Pydantic ValidationError
        OomGateInput(
            bars=1000,
            params=100,
            param_subsample_rate=1.1,
        )


def test_default_values() -> None:
    """Test that default values work correctly."""
    inp = OomGateInput(
        bars=1000,
        params=100,
        param_subsample_rate=0.5,
    )
    
    assert inp.intents_per_bar == 2.0
    assert inp.bytes_per_intent_est == 64
    assert inp.ram_budget_bytes == 6_000_000_000  # 6GB
    
    decision = decide_gate(inp)
    assert decision.decision in ("PASS", "BLOCK", "AUTO_DOWNSAMPLE")
    assert decision.estimated_bytes >= 0
    assert decision.ram_budget_bytes == inp.ram_budget_bytes


def test_decide_oom_action_returns_dict_schema() -> None:
    """Test legacy decide_oom_action() returns dict schema."""
    cfg = {"bars": 1000, "params_total": 100, "param_subsample_rate": 0.1}
    res = decide_oom_action(cfg, mem_limit_mb=10_000.0)
    
    assert isinstance(res, dict)
    assert res["action"] in {"PASS", "BLOCK", "AUTO_DOWNSAMPLE"}
    assert "estimated_bytes" in res
    assert "estimated_mb" in res
    assert "mem_limit_mb" in res
    assert "mem_limit_bytes" in res
    assert "original_subsample" in res  # Contract key name
    assert "final_subsample" in res  # Contract key name
    assert "params_total" in res
    assert "params_effective" in res
    assert "reason" in res


================================================================================
FILE: tests/test_oom_gate_contract.py
================================================================================

"""Contract tests for OOM gate.

Tests verify:
1. Gate PASS when under limit
2. Gate BLOCK when over limit and no auto-downsample
3. Gate AUTO_DOWNSAMPLE when allowed
"""

from __future__ import annotations

import numpy as np
import pytest

from FishBroWFS_V2.core.oom_gate import decide_oom_action
from FishBroWFS_V2.core.oom_cost_model import estimate_memory_bytes, summarize_estimates


def test_oom_gate_pass_when_under_limit():
    """Test that gate PASSes when memory estimate is under limit."""
    cfg = {
        "bars": 1000,
        "params_total": 100,
        "param_subsample_rate": 0.1,
        "open_": np.random.randn(1000).astype(np.float64),
        "high": np.random.randn(1000).astype(np.float64),
        "low": np.random.randn(1000).astype(np.float64),
        "close": np.random.randn(1000).astype(np.float64),
        "params_matrix": np.random.randn(100, 3).astype(np.float64),
    }
    
    # Use a very high limit to ensure PASS
    mem_limit_mb = 10000.0
    
    result = decide_oom_action(cfg, mem_limit_mb=mem_limit_mb)
    
    assert result["action"] == "PASS"
    assert result["original_subsample"] == 0.1
    assert result["final_subsample"] == 0.1
    assert "estimates" in result
    assert result["estimates"]["mem_est_mb"] <= mem_limit_mb


def test_oom_gate_block_when_over_limit_and_no_auto():
    """Test that gate BLOCKs when over limit and auto-downsample is disabled."""
    cfg = {
        "bars": 100000,
        "params_total": 10000,
        "param_subsample_rate": 1.0,
        "open_": np.random.randn(100000).astype(np.float64),
        "high": np.random.randn(100000).astype(np.float64),
        "low": np.random.randn(100000).astype(np.float64),
        "close": np.random.randn(100000).astype(np.float64),
        "params_matrix": np.random.randn(10000, 3).astype(np.float64),
    }
    
    # Use a very low limit to ensure BLOCK
    mem_limit_mb = 1.0
    
    result = decide_oom_action(
        cfg,
        mem_limit_mb=mem_limit_mb,
        allow_auto_downsample=False,
    )
    
    assert result["action"] == "BLOCK"
    assert result["original_subsample"] == 1.0
    assert result["final_subsample"] == 1.0  # Not changed
    assert "reason" in result
    assert "mem_est_mb" in result["reason"] or "limit" in result["reason"]


def test_oom_gate_auto_downsample_when_allowed(monkeypatch):
    """Test that gate AUTO_DOWNSAMPLEs when allowed and over limit."""
    # Monkeypatch estimate_memory_bytes to make it subsample-sensitive for testing
    def mock_estimate_memory_bytes(cfg, work_factor=2.0):
        """Mock that makes memory estimate sensitive to subsample."""
        bars = int(cfg.get("bars", 0))
        params_total = int(cfg.get("params_total", 0))
        subsample_rate = float(cfg.get("param_subsample_rate", 1.0))
        params_effective = int(params_total * subsample_rate)
        
        # Simplified: mem scales with bars and effective params
        base_mem = bars * 8 * 4  # 4 price arrays
        params_mem = params_effective * 3 * 8  # params_matrix
        total_mem = (base_mem + params_mem) * work_factor
        return int(total_mem)
    
    monkeypatch.setattr(
        "FishBroWFS_V2.core.oom_cost_model.estimate_memory_bytes",
        mock_estimate_memory_bytes,
    )
    
    cfg = {
        "bars": 10000,
        "params_total": 1000,
        "param_subsample_rate": 0.5,  # Start at 50%
        "open_": np.random.randn(10000).astype(np.float64),
        "high": np.random.randn(10000).astype(np.float64),
        "low": np.random.randn(10000).astype(np.float64),
        "close": np.random.randn(10000).astype(np.float64),
        "params_matrix": np.random.randn(1000, 3).astype(np.float64),
    }
    
    # Dynamic calculation: compute mem_mb for two subsample rates, use midpoint
    def _mem_mb(cfg_dict):
        b = mock_estimate_memory_bytes(cfg_dict, work_factor=2.0)
        return b / (1024.0 * 1024.0)
    
    cfg_half = dict(cfg)
    cfg_half["param_subsample_rate"] = 0.5
    cfg_quarter = dict(cfg)
    cfg_quarter["param_subsample_rate"] = 0.25
    
    mb_half = _mem_mb(cfg_half)  # ~0.633
    mb_quarter = _mem_mb(cfg_quarter)  # ~0.622
    
    # Set limit between these two values â†’ guaranteed to trigger AUTO_DOWNSAMPLE
    mem_limit_mb = (mb_half + mb_quarter) / 2.0
    
    result = decide_oom_action(
        cfg,
        mem_limit_mb=mem_limit_mb,
        allow_auto_downsample=True,
        auto_downsample_step=0.5,
        auto_downsample_min=0.02,
    )
    
    assert result["action"] == "AUTO_DOWNSAMPLE"
    assert result["original_subsample"] == 0.5
    assert result["final_subsample"] < result["original_subsample"]
    assert result["final_subsample"] >= 0.02  # Above minimum
    assert "reason" in result
    assert "auto-downsample" in result["reason"].lower()
    assert result["estimates"]["mem_est_mb"] <= mem_limit_mb


def test_oom_gate_block_when_min_still_over_limit(monkeypatch):
    """Test that gate BLOCKs when even at minimum subsample still over limit."""
    def mock_estimate_memory_bytes(cfg, work_factor=2.0):
        """Mock that always returns high memory."""
        return 100 * 1024 * 1024  # Always 100MB
    
    monkeypatch.setattr(
        "FishBroWFS_V2.core.oom_cost_model.estimate_memory_bytes",
        mock_estimate_memory_bytes,
    )
    
    cfg = {
        "bars": 1000,
        "params_total": 100,
        "param_subsample_rate": 0.5,
        "open_": np.random.randn(1000).astype(np.float64),
        "high": np.random.randn(1000).astype(np.float64),
        "low": np.random.randn(1000).astype(np.float64),
        "close": np.random.randn(1000).astype(np.float64),
        "params_matrix": np.random.randn(100, 3).astype(np.float64),
    }
    
    mem_limit_mb = 50.0  # Lower than mock estimate
    
    result = decide_oom_action(
        cfg,
        mem_limit_mb=mem_limit_mb,
        allow_auto_downsample=True,
        auto_downsample_min=0.02,
    )
    
    assert result["action"] == "BLOCK"
    assert "min_subsample" in result["reason"].lower() or "still too large" in result["reason"].lower()


def test_oom_gate_result_schema():
    """Test that gate result has correct schema."""
    cfg = {
        "bars": 1000,
        "params_total": 100,
        "param_subsample_rate": 0.1,
        "open_": np.random.randn(1000).astype(np.float64),
        "high": np.random.randn(1000).astype(np.float64),
        "low": np.random.randn(1000).astype(np.float64),
        "close": np.random.randn(1000).astype(np.float64),
        "params_matrix": np.random.randn(100, 3).astype(np.float64),
    }
    
    result = decide_oom_action(cfg, mem_limit_mb=10000.0)
    
    # Verify schema
    assert "action" in result
    assert result["action"] in ("PASS", "BLOCK", "AUTO_DOWNSAMPLE")
    assert "reason" in result
    assert isinstance(result["reason"], str)
    assert "original_subsample" in result
    assert "final_subsample" in result
    assert "estimates" in result
    
    # Verify estimates structure
    estimates = result["estimates"]
    assert "mem_est_bytes" in estimates
    assert "mem_est_mb" in estimates
    assert "ops_est" in estimates
    assert "time_est_s" in estimates


================================================================================
FILE: tests/test_perf_breakdown_contract.py
================================================================================

"""
Stage P2-1.8: Contract Tests for Granular Breakdown and Extended Observability

Tests that verify:
- Granular timing keys exist and are non-negative floats
- Extended observability keys exist (entry/exit intents/fills totals)
- Accounting consistency (intents_total == entry + exit, fills_total == entry + exit)
- run_grid output contains timing keys in perf dict
"""
from __future__ import annotations

import os
import numpy as np

from FishBroWFS_V2.strategy.kernel import run_kernel_arrays, DonchianAtrParams
from FishBroWFS_V2.engine.types import BarArrays
from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_perf_breakdown_keys_existence() -> None:
    """
    D1: Contract test - Verify granular timing keys exist in _obs and are floats >= 0.0
    Also verify that t_total_kernel_s >= max(stage_times) for sanity check.
    
    Contract: keys always exist, values always float >= 0.0.
    (When perf harness runs with profiling enabled, these will naturally become >0 real data.)
    """
    import os
    # Ensure clean environment for test
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    # Task 2: Kernel profiling is optional - keys will always exist (may be 0.0 if not profiled)
    # We can optionally enable profiling to get real timing data, but it's not required for contract
    old_profile_kernel = os.environ.get("FISHBRO_PROFILE_KERNEL")
    # Optionally enable profiling to get real timing values (not required - keys exist regardless)
    # Uncomment the line below if you want to test with profiling enabled:
    # os.environ["FISHBRO_PROFILE_KERNEL"] = "1"
    
    try:
        n_bars = 200
        warmup = 20
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        bars = BarArrays(
            open=open_.astype(np.float64),
            high=high.astype(np.float64),
            low=low.astype(np.float64),
            close=close.astype(np.float64),
        )
        
        params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)
        
        result = run_kernel_arrays(
            bars=bars,
            params=params,
            commission=0.0,
            slip=0.0,
            order_qty=1,
        )
        
        # Verify _obs exists and contains timing keys
        assert "_obs" in result, "_obs must exist in kernel result"
        obs = result["_obs"]
        assert isinstance(obs, dict), "_obs must be a dict"
        
        # Required timing keys (now in _obs, not _perf)
        # Task 2: Contract - keys always exist, values always float >= 0.0
        timing_keys = [
            "t_calc_indicators_s",
            "t_build_entry_intents_s",
            "t_simulate_entry_s",
            "t_calc_exits_s",
            "t_simulate_exit_s",
            "t_total_kernel_s",
        ]
        
        stage_times = []
        for key in timing_keys:
            assert key in obs, f"{key} must exist in _obs (keys always exist, even if 0.0)"
            value = obs[key]
            assert isinstance(value, float), f"{key} must be float, got {type(value)}"
            assert value >= 0.0, f"{key} must be >= 0.0, got {value}"
            if key != "t_total_kernel_s":
                stage_times.append(value)
        
        # Sanity check: total time should be >= max of individual stage times
        # (allowing some overhead for timer calls and other operations)
        # Note: This check only makes sense if profiling was enabled (values > 0)
        t_total = obs["t_total_kernel_s"]
        if stage_times and t_total > 0.0:
            max_stage = max(stage_times)
            # Allow equality or small overhead
            assert t_total >= max_stage, (
                f"t_total_kernel_s ({t_total}) should be >= max(stage_times) ({max_stage})"
            )
    finally:
        # Restore environment
        # restore trigger rate
        if old_trigger_rate is None:
            os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
        else:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate
        
        # restore kernel profiling flag
        if old_profile_kernel is None:
            os.environ.pop("FISHBRO_PROFILE_KERNEL", None)
        else:
            os.environ["FISHBRO_PROFILE_KERNEL"] = old_profile_kernel


def test_extended_observability_keys_existence() -> None:
    """
    D1: Contract test - Verify extended observability keys exist in _obs
    """
    import os
    # Ensure clean environment for test
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    
    try:
        n_bars = 200
        warmup = 20
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        bars = BarArrays(
            open=open_.astype(np.float64),
            high=high.astype(np.float64),
            low=low.astype(np.float64),
            close=close.astype(np.float64),
        )
        
        params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)
        
        result = run_kernel_arrays(
            bars=bars,
            params=params,
            commission=0.0,
            slip=0.0,
            order_qty=1,
        )
        
        # Verify _obs exists and contains extended keys
        assert "_obs" in result, "_obs must exist in kernel result"
        obs = result["_obs"]
        assert isinstance(obs, dict), "_obs must be a dict"
        
        # Required observability keys
        obs_keys = [
            "entry_intents_total",
            "entry_fills_total",
            "exit_intents_total",
            "exit_fills_total",
        ]
        
        for key in obs_keys:
            assert key in obs, f"{key} must exist in _obs"
            value = obs[key]
            assert isinstance(value, int), f"{key} must be int, got {type(value)}"
            assert value >= 0, f"{key} must be >= 0, got {value}"
    finally:
        # Restore environment
        if old_trigger_rate is not None:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate


def test_accounting_consistency() -> None:
    """
    D2: Contract test - Verify accounting consistency
    intents_total == entry_intents_total + exit_intents_total
    fills_total == entry_fills_total + exit_fills_total
    Also verify entry_intents_total == valid_mask_sum in arrays mode
    """
    import os
    # Ensure clean environment for test
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    
    try:
        n_bars = 200
        warmup = 20
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        bars = BarArrays(
            open=open_.astype(np.float64),
            high=high.astype(np.float64),
            low=low.astype(np.float64),
            close=close.astype(np.float64),
        )
        
        params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)
        
        result = run_kernel_arrays(
            bars=bars,
            params=params,
            commission=0.0,
            slip=0.0,
            order_qty=1,
        )
        
        obs = result["_obs"]
        
        # Verify intents_total consistency
        intents_total = obs.get("intents_total", 0)
        entry_intents_total = obs.get("entry_intents_total", 0)
        exit_intents_total = obs.get("exit_intents_total", 0)
        
        assert intents_total == entry_intents_total + exit_intents_total, (
            f"intents_total ({intents_total}) must equal "
            f"entry_intents_total ({entry_intents_total}) + exit_intents_total ({exit_intents_total})"
        )
        
        # Verify fills_total consistency
        fills_total = obs.get("fills_total", 0)
        entry_fills_total = obs.get("entry_fills_total", 0)
        exit_fills_total = obs.get("exit_fills_total", 0)
        
        assert fills_total == entry_fills_total + exit_fills_total, (
            f"fills_total ({fills_total}) must equal "
            f"entry_fills_total ({entry_fills_total}) + exit_fills_total ({exit_fills_total})"
        )
        
        # Verify entry_intents_total == valid_mask_sum (arrays mode contract)
        if "valid_mask_sum" in obs and "entry_intents_total" in obs:
            valid_mask_sum = obs.get("valid_mask_sum", 0)
            entry_intents = obs.get("entry_intents_total", 0)
            assert entry_intents == valid_mask_sum, (
                f"entry_intents_total ({entry_intents}) must equal valid_mask_sum ({valid_mask_sum})"
            )
    finally:
        # Restore environment
        if old_trigger_rate is not None:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate


def test_run_grid_perf_contains_timing_keys(monkeypatch) -> None:
    """
    Contract test - Verify run_grid output contains timing keys in perf dict.
    This ensures timing aggregation works correctly at grid level.
    """
    # Task 1: Explicitly enable kernel profiling (required for timing collection)
    old_profile_kernel = os.environ.get("FISHBRO_PROFILE_KERNEL")
    os.environ["FISHBRO_PROFILE_KERNEL"] = "1"
    
    # Enable profile mode to ensure timing collection
    monkeypatch.setenv("FISHBRO_PROFILE_GRID", "1")
    
    try:
        n_bars = 200
        n_params = 5
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Generate minimal params
        params = np.array([
            [20, 10, 1.0],
            [25, 12, 1.5],
            [30, 15, 2.0],
            [35, 18, 1.0],
            [40, 20, 1.5],
        ], dtype=np.float64)
        
        result = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=False,
        )
        
        # Verify perf dict exists
        assert "perf" in result, "perf must exist in run_grid result"
        perf = result["perf"]
        assert isinstance(perf, dict), "perf must be a dict"
        
        # Stage P2-2 Step A: Required micro-profiling timing keys (aggregated across params)
        # Task 2: Since profile is enabled, timing keys must exist
        timing_keys = [
            "t_ind_donchian_s",
            "t_ind_atr_s",
            "t_build_entry_intents_s",
            "t_simulate_entry_s",
            "t_calc_exits_s",
            "t_simulate_exit_s",
            "t_total_kernel_s",
        ]
        
        for key in timing_keys:
            assert key in perf, f"{key} must exist in perf dict when profile is enabled"
            value = perf[key]
            assert isinstance(value, float), f"{key} must be float, got {type(value)}"
            assert value >= 0.0, f"{key} must be >= 0.0, got {value}"
        
        # Stage P2-2 Step A: Memoization potential assessment keys
        unique_keys = [
            "unique_channel_len_count",
            "unique_atr_len_count",
            "unique_ch_atr_pair_count",
        ]
        
        for key in unique_keys:
            assert key in perf, f"{key} must exist in perf dict"
            value = perf[key]
            assert isinstance(value, int), f"{key} must be int, got {type(value)}"
            assert value >= 1, f"{key} must be >= 1, got {value}"
    finally:
        # Task 1: Restore FISHBRO_PROFILE_KERNEL environment variable
        if old_profile_kernel is None:
            os.environ.pop("FISHBRO_PROFILE_KERNEL", None)
        else:
            os.environ["FISHBRO_PROFILE_KERNEL"] = old_profile_kernel


================================================================================
FILE: tests/test_perf_env_config_contract.py
================================================================================

"""Test perf harness environment variable configuration contract.

Ensures that FISHBRO_PERF_BARS and FISHBRO_PERF_PARAMS env vars are correctly parsed.
"""

import os
import sys
from pathlib import Path
from unittest.mock import patch


def _get_perf_config():
    """
    Helper to get perf config values by reading the script file.
    This avoids import issues with scripts/ module.
    """
    script_path = Path(__file__).parent.parent / "scripts" / "perf_grid.py"
    
    # Read and parse the constants
    with open(script_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    # Extract default values from the file
    # Look for: TIER_JIT_BARS = int(os.environ.get("FISHBRO_PERF_BARS", "20000"))
    import re
    
    bars_match = re.search(r'TIER_JIT_BARS\s*=\s*int\(os\.environ\.get\("FISHBRO_PERF_BARS",\s*"(\d+)"\)\)', content)
    params_match = re.search(r'TIER_JIT_PARAMS\s*=\s*int\(os\.environ\.get\("FISHBRO_PERF_PARAMS",\s*"(\d+)"\)\)', content)
    
    default_bars = int(bars_match.group(1)) if bars_match else None
    default_params = int(params_match.group(1)) if params_match else None
    
    return default_bars, default_params


def test_perf_env_bars_parsing():
    """Test that FISHBRO_PERF_BARS env var is correctly parsed."""
    with patch.dict(os.environ, {"FISHBRO_PERF_BARS": "50000"}, clear=False):
        # Simulate the parsing logic
        bars = int(os.environ.get("FISHBRO_PERF_BARS", "20000"))
        assert bars == 50000


def test_perf_env_params_parsing():
    """Test that FISHBRO_PERF_PARAMS env var is correctly parsed."""
    with patch.dict(os.environ, {"FISHBRO_PERF_PARAMS": "5000"}, clear=False):
        # Simulate the parsing logic
        params = int(os.environ.get("FISHBRO_PERF_PARAMS", "1000"))
        assert params == 5000


def test_perf_env_both_parsing():
    """Test that both env vars can be set simultaneously."""
    with patch.dict(os.environ, {
        "FISHBRO_PERF_BARS": "30000",
        "FISHBRO_PERF_PARAMS": "3000",
    }, clear=False):
        bars = int(os.environ.get("FISHBRO_PERF_BARS", "20000"))
        params = int(os.environ.get("FISHBRO_PERF_PARAMS", "1000"))
        
        assert bars == 30000
        assert params == 3000


def test_perf_env_defaults():
    """Test that defaults are baseline (20000Ã—1000) when env vars are not set."""
    # Ensure env vars are not set for this test
    env_backup = {}
    for key in ["FISHBRO_PERF_BARS", "FISHBRO_PERF_PARAMS"]:
        if key in os.environ:
            env_backup[key] = os.environ[key]
            del os.environ[key]
    
    try:
        # Check defaults match baseline
        default_bars, default_params = _get_perf_config()
        assert default_bars == 20000, f"Expected default bars=20000, got {default_bars}"
        assert default_params == 1000, f"Expected default params=1000, got {default_params}"
        
        # Verify parsing logic uses defaults
        bars = int(os.environ.get("FISHBRO_PERF_BARS", "20000"))
        params = int(os.environ.get("FISHBRO_PERF_PARAMS", "1000"))
        assert bars == 20000
        assert params == 1000
    finally:
        # Restore env vars
        for key, value in env_backup.items():
            os.environ[key] = value


================================================================================
FILE: tests/test_perf_evidence_chain.py
================================================================================

from __future__ import annotations

import numpy as np

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_perf_evidence_chain_exists() -> None:
    """
    Phase 3.0-D: Contract Test - Evidence Chain Existence
    
    Purpose: Lock down that evidence fields always exist and are non-null.
    This test only verifies evidence existence, not timing or strategy quality.
    """
    # Use minimal data: bars=50, params=3
    n_bars = 50
    n_params = 3
    
    # Generate synthetic OHLC data
    rng = np.random.default_rng(42)
    close = 100.0 + np.cumsum(rng.standard_normal(n_bars)).astype(np.float64)
    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
    open_ = (high + low) / 2 + rng.standard_normal(n_bars) * 0.5
    
    # Ensure high >= max(open, close) and low <= min(open, close)
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    # Generate minimal params: [channel_len, atr_len, stop_mult]
    params = np.array(
        [
            [10, 5, 1.0],
            [15, 7, 1.5],
            [20, 10, 2.0],
        ],
        dtype=np.float64,
    )
    
    # Run grid runner (array path)
    # Note: perf field is always present in runner output (Phase 3.0-B)
    out = run_grid(
        open_=open_,
        high=high,
        low=low,
        close=close,
        params_matrix=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        sort_params=False,
    )
    
    # Verify perf field exists
    assert "perf" in out, "perf field must exist in runner output"
    perf = out["perf"]
    assert isinstance(perf, dict), "perf must be a dict"
    
    # Phase 3.0-D: Assert evidence fields exist and are non-null
    # 1. intent_mode must be "arrays"
    assert "intent_mode" in perf, "intent_mode must exist in perf"
    assert perf["intent_mode"] == "arrays", (
        f"intent_mode expected 'arrays' but got '{perf['intent_mode']}'"
    )
    
    # 2. intents_total must exist, be non-null, and > 0
    assert "intents_total" in perf, "intents_total must exist in perf"
    assert perf["intents_total"] is not None, "intents_total must not be None"
    assert isinstance(perf["intents_total"], (int, np.integer)), (
        f"intents_total must be an integer, got {type(perf['intents_total'])}"
    )
    assert int(perf["intents_total"]) > 0, (
        f"intents_total must be > 0, got {perf['intents_total']}"
    )
    
    # 3. fills_total must exist and be non-null (can be 0, but not None)
    assert "fills_total" in perf, "fills_total must exist in perf"
    assert perf["fills_total"] is not None, "fills_total must not be None"
    assert isinstance(perf["fills_total"], (int, np.integer)), (
        f"fills_total must be an integer, got {type(perf['fills_total'])}"
    )
    # fills_total can be 0 (no trades), but must not be None


================================================================================
FILE: tests/test_perf_grid_profile_report.py
================================================================================

from __future__ import annotations

import cProfile

from FishBroWFS_V2.perf.profile_report import _format_profile_report


def test_profile_report_markers_present() -> None:
    pr = cProfile.Profile()
    pr.enable()
    _ = sum(range(10_000))  # tiny workload, deterministic
    pr.disable()
    report = _format_profile_report(
        lane_id="3",
        n_bars=2000,
        n_params=100,
        jit_enabled=True,
        sort_params=False,
        topn=10,
        mode="",
        pr=pr,
    )
    assert "__PROFILE_START__" in report
    assert "pstats sort: cumtime" in report
    assert "__PROFILE_END__" in report




================================================================================
FILE: tests/test_perf_obs_contract.py
================================================================================

"""Contract tests for perf observability (Stage P2-1.5).

These tests ensure that entry sparse observability fields are correctly
propagated from kernel to perf JSON output.
"""

import numpy as np
import pytest

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_perf_obs_entry_sparse_fields():
    """
    Contract: perf dict must contain entry sparse observability fields.
    
    This test directly calls run_grid (no subprocess) to verify that:
    1. entry_valid_mask_sum is present in perf dict
    2. entry_intents_total is present in perf dict
    3. entry_valid_mask_sum == entry_intents_total (contract)
    4. entry_intents_per_bar_avg is correctly calculated
    """
    # Generate small synthetic data (fast test)
    n_bars = 2000
    n_params = 50
    
    rng = np.random.default_rng(42)
    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10
    high = close + np.abs(rng.standard_normal(n_bars)) * 5
    low = close - np.abs(rng.standard_normal(n_bars)) * 5
    open_ = (high + low) / 2 + rng.standard_normal(n_bars)
    
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    # Generate params matrix (channel_len, atr_len, stop_mult)
    params_matrix = np.column_stack([
        np.random.randint(10, 30, size=n_params),  # channel_len
        np.random.randint(5, 20, size=n_params),   # atr_len
        np.random.uniform(1.0, 2.0, size=n_params),  # stop_mult
    ]).astype(np.float64)
    
    # Call run_grid (will use arrays mode by default)
    result = run_grid(
        open_=open_,
        high=high,
        low=low,
        close=close,
        params_matrix=params_matrix,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        sort_params=False,
    )
    
    # Verify result structure
    assert "perf" in result, "result must contain 'perf' dict"
    perf = result["perf"]
    assert isinstance(perf, dict), "perf must be a dict"
    
    # Verify entry sparse observability fields exist
    assert "entry_valid_mask_sum" in perf, (
        "perf must contain 'entry_valid_mask_sum' field"
    )
    assert "entry_intents_total" in perf, (
        "perf must contain 'entry_intents_total' field"
    )
    
    entry_valid_mask_sum = perf["entry_valid_mask_sum"]
    entry_intents_total = perf["entry_intents_total"]
    
    # Verify types
    assert isinstance(entry_valid_mask_sum, (int, np.integer)), (
        f"entry_valid_mask_sum must be int, got {type(entry_valid_mask_sum)}"
    )
    assert isinstance(entry_intents_total, (int, np.integer)), (
        f"entry_intents_total must be int, got {type(entry_intents_total)}"
    )
    
    # Contract: entry_valid_mask_sum == entry_intents_total
    assert entry_valid_mask_sum == entry_intents_total, (
        f"entry_valid_mask_sum ({entry_valid_mask_sum}) must equal "
        f"entry_intents_total ({entry_intents_total})"
    )
    
    # Verify entry_intents_per_bar_avg if present
    if "entry_intents_per_bar_avg" in perf:
        entry_intents_per_bar_avg = perf["entry_intents_per_bar_avg"]
        assert isinstance(entry_intents_per_bar_avg, (float, np.floating)), (
            f"entry_intents_per_bar_avg must be float, got {type(entry_intents_per_bar_avg)}"
        )
        
        # Verify calculation: entry_intents_per_bar_avg == entry_intents_total / n_bars
        expected_avg = entry_intents_total / n_bars
        assert abs(entry_intents_per_bar_avg - expected_avg) <= 1e-12, (
            f"entry_intents_per_bar_avg ({entry_intents_per_bar_avg}) must equal "
            f"entry_intents_total / n_bars ({expected_avg})"
        )
    
    # Verify intents_total_reported is present (preserves original)
    if "intents_total_reported" in perf:
        intents_total_reported = perf["intents_total_reported"]
        assert isinstance(intents_total_reported, (int, np.integer)), (
            f"intents_total_reported must be int, got {type(intents_total_reported)}"
        )
        # intents_total_reported should equal original intents_total
        if "intents_total" in perf:
            assert intents_total_reported == perf["intents_total"], (
                f"intents_total_reported ({intents_total_reported}) should equal "
                f"intents_total ({perf['intents_total']})"
            )


def test_perf_obs_entry_sparse_non_zero():
    """
    Contract: With valid data, entry sparse fields should be non-zero.
    
    This ensures that sparse masking is actually working and producing
    observable results.
    """
    # Generate data that should produce some valid intents
    n_bars = 1000
    n_params = 20
    
    rng = np.random.default_rng(42)
    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10
    high = close + np.abs(rng.standard_normal(n_bars)) * 5
    low = close - np.abs(rng.standard_normal(n_bars)) * 5
    open_ = (high + low) / 2 + rng.standard_normal(n_bars)
    
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    # Use reasonable params (should produce valid donch_hi)
    params_matrix = np.column_stack([
        np.full(n_params, 20, dtype=np.float64),  # channel_len = 20
        np.full(n_params, 14, dtype=np.float64),  # atr_len = 14
        np.full(n_params, 2.0, dtype=np.float64),  # stop_mult = 2.0
    ])
    
    result = run_grid(
        open_=open_,
        high=high,
        low=low,
        close=close,
        params_matrix=params_matrix,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        sort_params=False,
    )
    
    perf = result.get("perf", {})
    if "entry_valid_mask_sum" in perf and "entry_intents_total" in perf:
        entry_valid_mask_sum = perf["entry_valid_mask_sum"]
        entry_intents_total = perf["entry_intents_total"]
        
        # With valid data and reasonable params, we should have some intents
        # (but allow for edge cases where all are filtered)
        assert entry_valid_mask_sum >= 0, "entry_valid_mask_sum must be non-negative"
        assert entry_intents_total >= 0, "entry_intents_total must be non-negative"
        
        # With n_bars=1000 and channel_len=20, we should have some valid intents
        # after warmup (at least a few)
        if n_bars > 100:  # Only check if we have enough bars
            # Conservative: allow for edge cases but expect some intents
            # In practice, with valid data, we should have >> 0
            pass  # Just verify non-negative, don't enforce minimum


================================================================================
FILE: tests/test_perf_trigger_rate_contract.py
================================================================================

"""
Stage P2-1.6: Contract Tests for Trigger Rate Masking

Tests that verify trigger_rate control works correctly:
- entry_intents_total scales linearly with trigger_rate
- entry_valid_mask_sum == entry_intents_total
- Deterministic behavior (same seed â†’ same result)
"""
from __future__ import annotations

import numpy as np
import os

from FishBroWFS_V2.perf.scenario_control import apply_trigger_rate_mask


def test_trigger_rate_mask_rate_1_0_no_change() -> None:
    """
    Test that trigger_rate=1.0 preserves all valid triggers unchanged.
    """
    n_bars = 2000
    warmup = 100
    
    # Create trigger array: warmup period NaN, rest are valid positive values
    trigger = np.full(n_bars, np.nan, dtype=np.float64)
    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    # Apply mask with rate=1.0
    masked = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=1.0,
        warmup=warmup,
        seed=42,
    )
    
    # Should be unchanged
    assert np.array_equal(trigger, masked, equal_nan=True), (
        "trigger_rate=1.0 should not change trigger array"
    )


def test_trigger_rate_mask_rate_0_05_approximately_5_percent() -> None:
    """
    Test that trigger_rate=0.05 results in approximately 5% of valid triggers.
    Allows Â±20% relative error to account for random fluctuations.
    """
    n_bars = 2000
    warmup = 100
    n_valid_expected = n_bars - warmup  # Valid positions after warmup
    
    # Create trigger array: warmup period NaN, rest are valid positive values
    trigger = np.full(n_bars, np.nan, dtype=np.float64)
    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    # Apply mask with rate=0.05
    masked = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=0.05,
        warmup=warmup,
        seed=42,
    )
    
    # Count valid (finite) positions after warmup
    valid_after_warmup = np.isfinite(masked[warmup:])
    n_valid_actual = int(np.sum(valid_after_warmup))
    
    # Expected: approximately 5% of valid positions
    expected_min = int(n_valid_expected * 0.05 * 0.8)  # 80% of 5% (lower bound)
    expected_max = int(n_valid_expected * 0.05 * 1.2)  # 120% of 5% (upper bound)
    
    assert expected_min <= n_valid_actual <= expected_max, (
        f"Expected ~5% valid triggers ({expected_min}-{expected_max}), "
        f"got {n_valid_actual} ({n_valid_actual/n_valid_expected*100:.2f}%)"
    )


def test_trigger_rate_mask_deterministic() -> None:
    """
    Test that same seed and same input produce identical mask results.
    """
    n_bars = 2000
    warmup = 100
    
    # Create trigger array
    trigger = np.full(n_bars, np.nan, dtype=np.float64)
    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    # Apply mask twice with same parameters
    masked1 = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=0.05,
        warmup=warmup,
        seed=42,
    )
    
    masked2 = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=0.05,
        warmup=warmup,
        seed=42,
    )
    
    # Should be identical
    assert np.array_equal(masked1, masked2, equal_nan=True), (
        "Same seed and input should produce identical mask results"
    )


def test_trigger_rate_mask_different_seeds_different_results() -> None:
    """
    Test that different seeds produce different mask results (when rate < 1.0).
    """
    n_bars = 2000
    warmup = 100
    
    # Create trigger array
    trigger = np.full(n_bars, np.nan, dtype=np.float64)
    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    # Apply mask with different seeds
    masked1 = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=0.05,
        warmup=warmup,
        seed=42,
    )
    
    masked2 = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=0.05,
        warmup=warmup,
        seed=999,
    )
    
    # Should be different (very unlikely to be identical with different seeds)
    assert not np.array_equal(masked1, masked2, equal_nan=True), (
        "Different seeds should produce different mask results"
    )


def test_trigger_rate_mask_preserves_warmup_nan() -> None:
    """
    Test that warmup period NaN positions are preserved (not masked).
    """
    n_bars = 2000
    warmup = 100
    
    # Create trigger array: warmup period NaN, rest are valid
    trigger = np.full(n_bars, np.nan, dtype=np.float64)
    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    # Apply mask
    masked = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=0.05,
        warmup=warmup,
        seed=42,
    )
    
    # Warmup period should remain NaN
    assert np.all(np.isnan(masked[:warmup])), (
        "Warmup period should remain NaN after masking"
    )


def test_trigger_rate_mask_linear_scaling() -> None:
    """
    Test that valid trigger count scales approximately linearly with trigger_rate.
    """
    n_bars = 2000
    warmup = 100
    n_valid_expected = n_bars - warmup
    
    # Create trigger array
    trigger = np.full(n_bars, np.nan, dtype=np.float64)
    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    rates = [0.1, 0.3, 0.5, 0.7, 0.9]
    valid_counts = []
    
    for rate in rates:
        masked = apply_trigger_rate_mask(
            trigger=trigger,
            trigger_rate=rate,
            warmup=warmup,
            seed=42,
        )
        n_valid = int(np.sum(np.isfinite(masked[warmup:])))
        valid_counts.append(n_valid)
    
    # Check approximate linearity: valid_counts[i] / valid_counts[j] â‰ˆ rates[i] / rates[j]
    # Use first and last as reference
    ratio_expected = rates[-1] / rates[0]  # 0.9 / 0.1 = 9.0
    ratio_actual = valid_counts[-1] / valid_counts[0] if valid_counts[0] > 0 else 0
    
    # Allow Â±30% error for random fluctuations
    assert 0.7 * ratio_expected <= ratio_actual <= 1.3 * ratio_expected, (
        f"Valid counts should scale linearly with rate. "
        f"Expected ratio ~{ratio_expected:.2f}, got {ratio_actual:.2f}. "
        f"Counts: {valid_counts}"
    )


def test_trigger_rate_mask_preserves_dtype() -> None:
    """
    Test that masking preserves the input dtype.
    """
    n_bars = 200
    warmup = 20
    
    # Test with float64
    trigger_f64 = np.full(n_bars, np.nan, dtype=np.float64)
    trigger_f64[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    masked_f64 = apply_trigger_rate_mask(
        trigger=trigger_f64,
        trigger_rate=0.5,
        warmup=warmup,
        seed=42,
    )
    
    assert masked_f64.dtype == np.float64, (
        f"Expected float64, got {masked_f64.dtype}"
    )
    
    # Test with float32
    trigger_f32 = np.full(n_bars, np.nan, dtype=np.float32)
    trigger_f32[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float32)
    
    masked_f32 = apply_trigger_rate_mask(
        trigger=trigger_f32,
        trigger_rate=0.5,
        warmup=warmup,
        seed=42,
    )
    
    assert masked_f32.dtype == np.float32, (
        f"Expected float32, got {masked_f32.dtype}"
    )


def test_trigger_rate_mask_integration_with_kernel() -> None:
    """
    Integration test: verify that trigger_rate affects entry_intents_total in run_kernel_arrays.
    This test uses run_kernel_arrays directly (no subprocess) to verify the integration.
    """
    from FishBroWFS_V2.strategy.kernel import run_kernel_arrays, DonchianAtrParams
    from FishBroWFS_V2.engine.types import BarArrays
    
    n_bars = 200
    warmup = 20
    
    # Generate simple OHLC data
    rng = np.random.default_rng(42)
    close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
    open_ = (high + low) / 2
    
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    bars = BarArrays(
        open=open_.astype(np.float64),
        high=high.astype(np.float64),
        low=low.astype(np.float64),
        close=close.astype(np.float64),
    )
    
    params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)
    
    # Test with trigger_rate=1.0 (baseline) - explicitly set to avoid env interference
    os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
    result_1_0 = run_kernel_arrays(
        bars=bars,
        params=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
    )
    
    # Contract test: fail fast if keys missing (no .get() with defaults)
    entry_intents_1_0 = result_1_0["_obs"]["entry_intents_total"]
    valid_mask_sum_1_0 = result_1_0["_obs"]["entry_valid_mask_sum"]
    assert entry_intents_1_0 == valid_mask_sum_1_0
    
    # Test with trigger_rate=0.5
    os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "0.5"
    result_0_5 = run_kernel_arrays(
        bars=bars,
        params=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
    )
    
    # Contract test: fail fast if keys missing (no .get() with defaults)
    entry_intents_0_5 = result_0_5["_obs"]["entry_intents_total"]
    valid_mask_sum_0_5 = result_0_5["_obs"]["entry_valid_mask_sum"]
    assert entry_intents_0_5 == valid_mask_sum_0_5
    
    # Cleanup
    os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    
    # Verify that entry_intents_0_5 is approximately 50% of entry_intents_1_0
    # Allow Â±30% error for random fluctuations and warmup/NaN deterministic effects
    if entry_intents_1_0 > 0:
        ratio = entry_intents_0_5 / entry_intents_1_0
        assert 0.35 <= ratio <= 0.65, (
            f"With trigger_rate=0.5, expected entry_intents ~50% of baseline, "
            f"got {ratio*100:.1f}% (baseline={entry_intents_1_0}, actual={entry_intents_0_5})"
        )


================================================================================
FILE: tests/test_report_link_allows_minimal_artifacts.py
================================================================================

"""Tests for report link allowing minimal artifacts.

Tests that report readiness only checks file existence,
and build_report_link always returns Viewer URL.
"""

from __future__ import annotations

import json
from pathlib import Path

import pytest

from FishBroWFS_V2.control.report_links import (
    build_report_link,
    get_outputs_root,
    is_report_ready,
)


def test_is_report_ready_with_minimal_artifacts(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that is_report_ready returns True with only three files."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    run_dir = tmp_path / run_id
    run_dir.mkdir(parents=True)
    
    # Create only the three required files
    (run_dir / "manifest.json").write_text(json.dumps({"run_id": run_id}))
    # Use winners_v2.json (preferred) or winners.json (fallback)
    (run_dir / "winners_v2.json").write_text(json.dumps({"summary": {}}))
    (run_dir / "governance.json").write_text(json.dumps({"scoring": {}}))
    
    # Should return True
    assert is_report_ready(run_id) is True


def test_is_report_ready_missing_file(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that is_report_ready returns False if any file is missing."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    run_dir = tmp_path / run_id
    run_dir.mkdir(parents=True)
    
    # Create only two files (missing governance.json)
    (run_dir / "manifest.json").write_text(json.dumps({"run_id": run_id}))
    (run_dir / "winners.json").write_text(json.dumps({"summary": {}}))
    
    # Should return False
    assert is_report_ready(run_id) is False


def test_build_report_link_always_returns_url(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that build_report_link always returns Viewer URL."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    
    # Should return URL even if artifacts don't exist
    report_link = build_report_link(run_id)
    
    assert report_link is not None
    assert report_link.startswith("/?")
    assert run_id in report_link
    assert "season" in report_link


def test_build_report_link_no_error_string(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that build_report_link never returns error string."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    
    # Should never return error string
    report_link = build_report_link(run_id)
    
    assert report_link is not None
    assert isinstance(report_link, str)
    assert "error" not in report_link.lower()
    assert "not ready" not in report_link.lower()
    assert "missing" not in report_link.lower()


def test_is_report_ready_never_raises(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that is_report_ready never raises exceptions."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    # Should not raise even with invalid run_id
    result = is_report_ready("nonexistent_run")
    assert isinstance(result, bool)
    
    # Should not raise even with None
    result = is_report_ready(None)  # type: ignore
    assert isinstance(result, bool)


def test_build_report_link_never_raises(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that build_report_link never raises exceptions."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    # Should not raise even with invalid run_id
    report_link = build_report_link("nonexistent_run")
    assert report_link is not None
    assert isinstance(report_link, str)
    
    # Should not raise even with empty string
    report_link = build_report_link("")
    assert report_link is not None
    assert isinstance(report_link, str)


def test_minimal_artifacts_content_not_checked(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that is_report_ready does not check content validity."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    run_dir = tmp_path / run_id
    run_dir.mkdir(parents=True)
    
    # Create files with invalid JSON content
    (run_dir / "manifest.json").write_text("invalid json")
    (run_dir / "winners_v2.json").write_text("not json")
    (run_dir / "governance.json").write_text("{}")
    
    # Should still return True (only checks existence)
    assert is_report_ready(run_id) is True


def test_is_report_ready_accepts_winners_json_fallback(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that is_report_ready accepts winners.json as fallback."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    run_dir = tmp_path / run_id
    run_dir.mkdir(parents=True)
    
    # Create files with winners.json (not winners_v2.json)
    (run_dir / "manifest.json").write_text(json.dumps({"run_id": run_id}))
    (run_dir / "winners.json").write_text(json.dumps({"summary": {}}))
    (run_dir / "governance.json").write_text(json.dumps({"scoring": {}}))
    
    # Should still return True (only checks existence)
    assert is_report_ready(run_id) is True


def test_ui_does_not_block_with_minimal_artifacts(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that UI flow does not block with minimal artifacts."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    run_dir = tmp_path / run_id
    run_dir.mkdir(parents=True)
    
    # Create minimal artifacts
    (run_dir / "manifest.json").write_text(json.dumps({"run_id": run_id}))
    (run_dir / "winners_v2.json").write_text(json.dumps({"summary": {}}))
    (run_dir / "governance.json").write_text(json.dumps({"scoring": {}}))
    
    # build_report_link should work
    report_link = build_report_link(run_id)
    assert report_link is not None
    assert "error" not in report_link.lower()
    
    # is_report_ready should return True
    assert is_report_ready(run_id) is True


================================================================================
FILE: tests/test_runner_adapter_contract.py
================================================================================

"""Contract tests for runner adapter.

Tests verify:
1. Adapter returns data only (no file I/O)
2. Winners schema is stable
3. Metrics structure is consistent
"""

from __future__ import annotations

import tempfile
from pathlib import Path

import numpy as np
import pytest

from FishBroWFS_V2.pipeline.runner_adapter import run_stage_job


def test_runner_adapter_returns_no_files_written():
    """Test that adapter does not write any files."""
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Count files before
        files_before = list(tmp_path.rglob("*"))
        file_count_before = len([f for f in files_before if f.is_file()])
        
        # Run adapter
        cfg = {
            "stage_name": "stage0_coarse",
            "param_subsample_rate": 0.1,
            "topk": 10,
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "params_total": 100,
            "proxy_name": "ma_proxy_v0",
        }
        
        result = run_stage_job(cfg)
        
        # Count files after
        files_after = list(tmp_path.rglob("*"))
        file_count_after = len([f for f in files_after if f.is_file()])
        
        # Verify no new files were created
        assert file_count_after == file_count_before, (
            "Adapter should not write files, but new files were created"
        )
        
        # Verify result structure
        assert "metrics" in result
        assert "winners" in result


def test_winners_schema_is_stable():
    """Test that winners schema is stable across all stages."""
    test_cases = [
        {
            "stage_name": "stage0_coarse",
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "params_total": 100,
            "topk": 10,
        },
        {
            "stage_name": "stage1_topk",
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "params_total": 100,
            "topk": 5,
            "commission": 0.0,
            "slip": 0.0,
        },
        {
            "stage_name": "stage2_confirm",
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "params_total": 100,
            "commission": 0.0,
            "slip": 0.0,
        },
    ]
    
    for cfg in test_cases:
        cfg["param_subsample_rate"] = 1.0  # Use full for simplicity
        
        result = run_stage_job(cfg)
        
        # Verify winners schema
        winners = result.get("winners", {})
        assert "topk" in winners, f"Missing 'topk' in winners for {cfg['stage_name']}"
        assert "notes" in winners, f"Missing 'notes' in winners for {cfg['stage_name']}"
        assert isinstance(winners["topk"], list)
        assert isinstance(winners["notes"], dict)
        assert winners["notes"].get("schema") == "v1"


def test_metrics_structure_is_consistent():
    """Test that metrics structure is consistent across stages."""
    test_cases = [
        {
            "stage_name": "stage0_coarse",
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "params_total": 100,
            "topk": 10,
        },
        {
            "stage_name": "stage1_topk",
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "params_total": 100,
            "topk": 5,
            "commission": 0.0,
            "slip": 0.0,
        },
    ]
    
    required_fields = ["params_total", "params_effective", "bars", "stage_name"]
    
    for cfg in test_cases:
        cfg["param_subsample_rate"] = 0.5
        
        result = run_stage_job(cfg)
        
        metrics = result.get("metrics", {})
        
        # Verify required fields exist
        for field in required_fields:
            assert field in metrics, (
                f"Missing required field '{field}' in metrics for {cfg['stage_name']}"
            )
        
        # Verify stage_name matches
        assert metrics["stage_name"] == cfg["stage_name"]


================================================================================
FILE: tests/test_runner_adapter_input_coercion.py
================================================================================

"""Contract tests for runner adapter input coercion.

Tests verify that input arrays are coerced to np.ndarray float64,
preventing .shape access errors when lists are passed.
"""

from __future__ import annotations

import numpy as np
import pytest

from FishBroWFS_V2.pipeline.runner_adapter import run_stage_job


def test_stage0_coercion_with_lists() -> None:
    """Test that Stage0 accepts list inputs and coerces to np.ndarray."""
    # Use list instead of np.ndarray
    close_list = [100.0 + i * 0.1 for i in range(1000)]
    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5], [20.0, 10.0, 2.0]]
    
    cfg = {
        "stage_name": "stage0_coarse",
        "param_subsample_rate": 1.0,
        "topk": 3,
        "close": close_list,  # List, not np.ndarray
        "params_matrix": params_matrix_list,  # List, not np.ndarray
        "params_total": 3,
        "proxy_name": "ma_proxy_v0",
    }
    
    # Should not raise AttributeError: 'list' object has no attribute 'shape'
    result = run_stage_job(cfg)
    
    # Verify result structure
    assert "metrics" in result
    assert "winners" in result
    
    # Verify that internal arrays are np.ndarray (by checking results work)
    assert isinstance(result["winners"]["topk"], list)
    assert len(result["winners"]["topk"]) <= 3


def test_stage1_coercion_with_lists() -> None:
    """Test that Stage1 accepts list inputs and coerces to np.ndarray."""
    # Use lists instead of np.ndarray
    open_list = [100.0 + i * 0.1 for i in range(100)]
    high_list = [101.0 + i * 0.1 for i in range(100)]
    low_list = [99.0 + i * 0.1 for i in range(100)]
    close_list = [100.0 + i * 0.1 for i in range(100)]
    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5]]
    
    cfg = {
        "stage_name": "stage1_topk",
        "param_subsample_rate": 1.0,
        "topk": 2,
        "open_": open_list,  # List, not np.ndarray
        "high": high_list,  # List, not np.ndarray
        "low": low_list,  # List, not np.ndarray
        "close": close_list,  # List, not np.ndarray
        "params_matrix": params_matrix_list,  # List, not np.ndarray
        "params_total": 2,
        "commission": 0.0,
        "slip": 0.0,
    }
    
    # Should not raise AttributeError: 'list' object has no attribute 'shape'
    result = run_stage_job(cfg)
    
    # Verify result structure
    assert "metrics" in result
    assert "winners" in result
    
    # Verify that internal arrays are np.ndarray (by checking results work)
    assert isinstance(result["winners"]["topk"], list)


def test_stage2_coercion_with_lists() -> None:
    """Test that Stage2 accepts list inputs and coerces to np.ndarray."""
    # Use lists instead of np.ndarray
    open_list = [100.0 + i * 0.1 for i in range(100)]
    high_list = [101.0 + i * 0.1 for i in range(100)]
    low_list = [99.0 + i * 0.1 for i in range(100)]
    close_list = [100.0 + i * 0.1 for i in range(100)]
    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5]]
    
    cfg = {
        "stage_name": "stage2_confirm",
        "param_subsample_rate": 1.0,
        "open_": open_list,  # List, not np.ndarray
        "high": high_list,  # List, not np.ndarray
        "low": low_list,  # List, not np.ndarray
        "close": close_list,  # List, not np.ndarray
        "params_matrix": params_matrix_list,  # List, not np.ndarray
        "params_total": 2,
        "commission": 0.0,
        "slip": 0.0,
    }
    
    # Should not raise AttributeError: 'list' object has no attribute 'shape'
    result = run_stage_job(cfg)
    
    # Verify result structure
    assert "metrics" in result
    assert "winners" in result
    
    # Verify that internal arrays are np.ndarray (by checking results work)
    assert isinstance(result["winners"]["topk"], list)


def test_coercion_preserves_dtype_float64() -> None:
    """Test that coercion produces float64 arrays."""
    # Test with float32 input (should be coerced to float64)
    close_float32 = np.array([100.0, 101.0, 102.0], dtype=np.float32)
    params_matrix_float32 = np.array([[10.0, 5.0, 1.0]], dtype=np.float32)
    
    cfg = {
        "stage_name": "stage0_coarse",
        "param_subsample_rate": 1.0,
        "topk": 1,
        "close": close_float32,
        "params_matrix": params_matrix_float32,
        "params_total": 1,
        "proxy_name": "ma_proxy_v0",
    }
    
    # Should not raise errors
    result = run_stage_job(cfg)
    
    # Verify result structure
    assert "metrics" in result
    assert "winners" in result


def test_coercion_handles_mixed_inputs() -> None:
    """Test that coercion handles mixed list/np.ndarray inputs."""
    # Mix of lists and np.ndarray
    open_list = [100.0 + i * 0.1 for i in range(100)]
    high_array = np.array([101.0 + i * 0.1 for i in range(100)], dtype=np.float64)
    low_list = [99.0 + i * 0.1 for i in range(100)]
    close_array = np.array([100.0 + i * 0.1 for i in range(100)], dtype=np.float32)
    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5]]
    
    cfg = {
        "stage_name": "stage1_topk",
        "param_subsample_rate": 1.0,
        "topk": 2,
        "open_": open_list,  # List
        "high": high_array,  # np.ndarray float64
        "low": low_list,  # List
        "close": close_array,  # np.ndarray float32 (should be coerced to float64)
        "params_matrix": params_matrix_list,  # List
        "params_total": 2,
        "commission": 0.0,
        "slip": 0.0,
    }
    
    # Should not raise errors
    result = run_stage_job(cfg)
    
    # Verify result structure
    assert "metrics" in result
    assert "winners" in result


================================================================================
FILE: tests/test_runner_grid_perf_observability.py
================================================================================

from __future__ import annotations

import numpy as np

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_run_grid_perf_fields_present_and_non_negative(monkeypatch) -> None:
    # Enable perf observability.
    monkeypatch.setenv("FISHBRO_PROFILE_GRID", "1")

    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)
    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)
    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)
    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)

    params = np.array([[2, 2, 1.0], [3, 2, 1.5]], dtype=np.float64)
    out = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=False)

    assert "perf" in out
    perf = out["perf"]
    assert isinstance(perf, dict)

    for k in ("t_features", "t_indicators", "t_intent_gen", "t_simulate"):
        assert k in perf
        # allow None (JSON null) when measurement is unavailable; never assume 0 is meaningful
        if perf[k] is not None:
            assert float(perf[k]) >= 0.0

    assert "simulate_impl" in perf
    assert perf["simulate_impl"] in ("jit", "py")

    assert "intents_total" in perf
    if perf["intents_total"] is not None:
        assert int(perf["intents_total"]) >= 0

    # Perf harness hook: confirm we can observe intent mode when profiling is enabled.
    assert "intent_mode" in perf
    if perf["intent_mode"] is not None:
        assert perf["intent_mode"] in ("arrays", "objects")




================================================================================
FILE: tests/test_seed_demo_run.py
================================================================================

"""Tests for seed_demo_run.

Tests that seed_demo_run creates demo job and artifacts correctly.
"""

from __future__ import annotations

import json
import sqlite3
from pathlib import Path

import pytest

from FishBroWFS_V2.control.seed_demo_run import main, get_db_path


def test_seed_demo_run_no_raise(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that seed_demo_run does not raise exceptions."""
    # Set outputs root to tmp_path
    monkeypatch.chdir(tmp_path)
    monkeypatch.setenv("JOBS_DB_PATH", str(tmp_path / "jobs.db"))
    
    # Should not raise
    run_id = main()
    
    assert run_id.startswith("demo_")
    assert len(run_id) > 5


def test_outputs_directory_created(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that outputs/<season>/runs/<run_id>/ directory is created."""
    monkeypatch.chdir(tmp_path)
    monkeypatch.setenv("JOBS_DB_PATH", str(tmp_path / "jobs.db"))
    
    run_id = main()
    
    # Standard path structure: outputs/<season>/runs/<run_id>/
    run_dir = tmp_path / "outputs" / "seasons" / "2026Q1" / "runs" / run_id
    assert run_dir.exists()
    assert run_dir.is_dir()


def test_artifacts_exist(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that all required artifacts are created."""
    monkeypatch.chdir(tmp_path)
    monkeypatch.setenv("JOBS_DB_PATH", str(tmp_path / "jobs.db"))
    
    run_id = main()
    # Standard path structure: outputs/<season>/runs/<run_id>/
    run_dir = tmp_path / "outputs" / "seasons" / "2026Q1" / "runs" / run_id
    
    # Check manifest.json
    manifest_path = run_dir / "manifest.json"
    assert manifest_path.exists()
    with manifest_path.open("r", encoding="utf-8") as f:
        manifest = json.load(f)
    assert manifest["run_id"] == run_id
    assert "created_at" in manifest
    
    # Check winners_v2.json
    winners_path = run_dir / "winners_v2.json"
    assert winners_path.exists()
    
    # Check governance.json
    governance_path = run_dir / "governance.json"
    assert governance_path.exists()
    
    # Check kpi.json (KPIå”¯ä¸€ä¾†æº)
    kpi_path = run_dir / "kpi.json"
    assert kpi_path.exists()
    with kpi_path.open("r", encoding="utf-8") as f:
        kpi = json.load(f)
    assert "net_profit" in kpi
    assert "max_drawdown" in kpi
    assert "num_trades" in kpi
    assert "final_score" in kpi


def test_job_in_db(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that job is created in database with DONE status."""
    monkeypatch.chdir(tmp_path)
    db_path = tmp_path / "jobs.db"
    monkeypatch.setenv("JOBS_DB_PATH", str(db_path))
    
    run_id = main()
    
    # Check database
    conn = sqlite3.connect(str(db_path))
    try:
        cursor = conn.execute("SELECT status, run_id, report_link FROM jobs WHERE run_id = ?", (run_id,))
        row = cursor.fetchone()
        assert row is not None
        
        status, db_run_id, report_link = row
        assert status == "DONE"
        assert db_run_id == run_id
        assert report_link is not None
        assert report_link.startswith("/b5?")
        assert run_id in report_link
        assert "season=2026Q1" in report_link
    finally:
        conn.close()


def test_report_link_not_none(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that report_link is not None."""
    monkeypatch.chdir(tmp_path)
    db_path = tmp_path / "jobs.db"
    monkeypatch.setenv("JOBS_DB_PATH", str(db_path))
    
    run_id = main()
    
    conn = sqlite3.connect(str(db_path))
    try:
        cursor = conn.execute("SELECT report_link FROM jobs WHERE run_id = ?", (run_id,))
        row = cursor.fetchone()
        assert row is not None
        
        report_link = row[0]
        assert report_link is not None
        assert len(report_link) > 0
    finally:
        conn.close()


def test_kpi_values_aligned(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that KPI values align with Phase 6.1 registry."""
    monkeypatch.chdir(tmp_path)
    monkeypatch.setenv("JOBS_DB_PATH", str(tmp_path / "jobs.db"))
    
    run_id = main()
    # Standard path structure: outputs/<season>/runs/<run_id>/
    run_dir = tmp_path / "outputs" / "seasons" / "2026Q1" / "runs" / run_id
    
    # Check kpi.json exists and has required KPIs (KPIå”¯ä¸€ä¾†æº)
    kpi_path = run_dir / "kpi.json"
    assert kpi_path.exists()
    with kpi_path.open("r", encoding="utf-8") as f:
        kpi = json.load(f)
    
    assert "net_profit" in kpi
    assert "max_drawdown" in kpi
    assert "num_trades" in kpi
    assert "final_score" in kpi
    
    # Verify KPI values match expected
    assert kpi["net_profit"] == 123456
    assert kpi["max_drawdown"] == -0.18
    assert kpi["num_trades"] == 42
    assert kpi["final_score"] == 1.23


================================================================================
FILE: tests/test_sparse_intents_contract.py
================================================================================

"""
Stage P2-3A: Contract Tests for Sparse Entry Intents (Grid Level)

Verifies that entry intents are truly sparse at grid level:
- entry_intents_total == entry_valid_mask_sum (not Bars Ã— Params)
- Sparse builder produces identical results to dense builder (same triggers)
"""
from __future__ import annotations

from dataclasses import asdict, is_dataclass

import numpy as np
import os

from FishBroWFS_V2.engine.types import Fill
from FishBroWFS_V2.pipeline.runner_grid import run_grid


def _fill_to_tuple(f: Fill) -> tuple:
    """
    Convert Fill to a comparable tuple representation.
    
    Uses dataclasses.asdict for dataclass instances, falls back to __dict__ or repr.
    Returns sorted tuple to ensure deterministic comparison.
    """
    if is_dataclass(f):
        d = asdict(f)
    else:
        # fallback: __dict__ (for normal classes)
        d = dict(getattr(f, "__dict__", {}))
        if not d:
            # last resort: repr
            return (repr(f),)
    # Fixed ordering to avoid dict order differences
    return tuple(sorted(d.items()))


def test_grid_sparse_intents_count() -> None:
    """
    Test that grid-level entry intents count scales with trigger_rate (param-subsample).
    
    This test verifies the core sparse contract at grid level:
    - entry_intents_total == entry_valid_mask_sum
    - entry_intents_total scales approximately linearly with trigger_rate
    """
    # Ensure clean environment
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    old_param_subsample_rate = os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
    old_profile_grid = os.environ.pop("FISHBRO_PROFILE_GRID", None)
    
    try:
        n_bars = 500
        n_params = 30  # Enough params to make "unique repetition" meaningful
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Generate params matrix (at least 10-50 params for meaningful unique repetition)
        params_list = []
        for i in range(n_params):
            ch_len = 20 + (i % 10)  # Vary channel_len (20-29)
            atr_len = 10 + (i % 5)  # Vary atr_len (10-14)
            stop_mult = 1.0 + (i % 3) * 0.5  # Vary stop_mult (1.0, 1.5, 2.0)
            params_list.append([ch_len, atr_len, stop_mult])
        
        params_matrix = np.array(params_list, dtype=np.float64)
        
        # Fix param_subsample_rate=1.0 (all params) to test trigger_rate effect on intents
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = "1.0"
        os.environ["FISHBRO_PROFILE_GRID"] = "1"
        
        # Run Dense (trigger_rate=1.0) - baseline
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
        
        result_dense = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Run Sparse (trigger_rate=0.05) - bar/intent-level sparsity
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "0.05"
        
        result_sparse = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Verify perf dicts exist
        perf_dense = result_dense.get("perf", {})
        perf_sparse = result_sparse.get("perf", {})
        
        assert isinstance(perf_dense, dict), "perf_dense must be a dict"
        assert isinstance(perf_sparse, dict), "perf_sparse must be a dict"
        
        # Core contract: entry_intents_total == entry_valid_mask_sum (both runs)
        entry_intents_dense = perf_dense.get("entry_intents_total")
        entry_valid_mask_dense = perf_dense.get("entry_valid_mask_sum")
        entry_intents_sparse = perf_sparse.get("entry_intents_total")
        entry_valid_mask_sparse = perf_sparse.get("entry_valid_mask_sum")
        
        assert entry_intents_dense == entry_valid_mask_dense, (
            f"Dense: entry_intents_total ({entry_intents_dense}) "
            f"must equal entry_valid_mask_sum ({entry_valid_mask_dense})"
        )
        assert entry_intents_sparse == entry_valid_mask_sparse, (
            f"Sparse: entry_intents_total ({entry_intents_sparse}) "
            f"must equal entry_valid_mask_sum ({entry_valid_mask_sparse})"
        )
        
        # Contract: entry_intents_sparse should be approximately trigger_rate * entry_intents_dense
        # With trigger_rate=0.05, we expect approximately 5% of dense baseline
        # Allow wide tolerance: [0.02, 0.08] (2% to 8% of dense)
        if entry_intents_dense is not None and entry_intents_dense > 0:
            ratio = entry_intents_sparse / entry_intents_dense
            assert 0.02 <= ratio <= 0.08, (
                f"With trigger_rate=0.05, entry_intents_sparse ({entry_intents_sparse}) "
                f"should be approximately 5% of entry_intents_dense ({entry_intents_dense}), "
                f"got ratio {ratio:.4f} (expected [0.02, 0.08])"
            )
        
    finally:
        # Restore environment
        if old_trigger_rate is None:
            os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
        else:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate
        
        if old_param_subsample_rate is None:
            os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
        else:
            os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = old_param_subsample_rate
        
        if old_profile_grid is None:
            os.environ.pop("FISHBRO_PROFILE_GRID", None)
        else:
            os.environ["FISHBRO_PROFILE_GRID"] = old_profile_grid


def test_sparse_vs_dense_builder_parity() -> None:
    """
    Test that sparse builder produces identical results to dense builder (same triggers).
    
    This test verifies determinism parity:
    - Same triggers set â†’ same results (metrics, fills)
    - Order ID determinism
    - Bit-exact parity
    
    Uses FISHBRO_FORCE_SPARSE_BUILDER=1 to test numba builder vs python builder.
    """
    # Ensure clean environment
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    old_force_sparse = os.environ.pop("FISHBRO_FORCE_SPARSE_BUILDER", None)
    
    try:
        n_bars = 300
        n_params = 20
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Generate params matrix
        params_list = []
        for i in range(n_params):
            ch_len = 20 + (i % 10)
            atr_len = 10 + (i % 5)
            stop_mult = 1.0 + (i % 3) * 0.5
            params_list.append([ch_len, atr_len, stop_mult])
        
        params_matrix = np.array(params_list, dtype=np.float64)
        
        # Run A: trigger_rate=1.0, force_sparse=0 (Python builder)
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
        os.environ.pop("FISHBRO_FORCE_SPARSE_BUILDER", None)  # Ensure not set
        
        result_a = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Run B: trigger_rate=1.0, force_sparse=1 (Numba builder, same triggers)
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
        os.environ["FISHBRO_FORCE_SPARSE_BUILDER"] = "1"
        
        result_b = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Verify metrics are identical (bit-exact)
        metrics_a = result_a.get("metrics")
        metrics_b = result_b.get("metrics")
        
        assert metrics_a is not None, "metrics_a must exist"
        assert metrics_b is not None, "metrics_b must exist"
        
        # Compare metrics arrays (should be bit-exact)
        np.testing.assert_array_equal(metrics_a, metrics_b, "metrics must be bit-exact")
        
        # Verify sparse contract holds in both runs
        perf_a = result_a.get("perf", {})
        perf_b = result_b.get("perf", {})
        
        if isinstance(perf_a, dict) and isinstance(perf_b, dict):
            entry_intents_a = perf_a.get("entry_intents_total")
            entry_intents_b = perf_b.get("entry_intents_total")
            
            if entry_intents_a is not None and entry_intents_b is not None:
                assert entry_intents_a == entry_intents_b, (
                    f"entry_intents_total should be identical (same triggers): "
                    f"A={entry_intents_a}, B={entry_intents_b}"
                )
        
    finally:
        # Restore environment
        if old_trigger_rate is None:
            os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
        else:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate
        
        if old_force_sparse is None:
            os.environ.pop("FISHBRO_FORCE_SPARSE_BUILDER", None)
        else:
            os.environ["FISHBRO_FORCE_SPARSE_BUILDER"] = old_force_sparse


def test_created_bar_sorted() -> None:
    """
    Test that created_bar arrays are sorted (ascending).
    
    Note: This test verifies the sparse builder contract that created_bar must be
    sorted. We verify this indirectly through the sparse contract consistency.
    """
    # Ensure clean environment
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    
    try:
        n_bars = 200
        n_params = 10
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Generate params matrix
        params_list = []
        for i in range(n_params):
            ch_len = 20 + (i % 5)
            atr_len = 10 + (i % 3)
            stop_mult = 1.0
            params_list.append([ch_len, atr_len, stop_mult])
        
        params_matrix = np.array(params_list, dtype=np.float64)
        
        # Run grid
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
        
        result = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Verify sparse contract: entry_intents_total == entry_valid_mask_sum
        perf = result.get("perf", {})
        if isinstance(perf, dict):
            entry_intents_total = perf.get("entry_intents_total")
            entry_valid_mask_sum = perf.get("entry_valid_mask_sum")
            
            if entry_intents_total is not None and entry_valid_mask_sum is not None:
                assert entry_intents_total == entry_valid_mask_sum, (
                    f"Sparse contract: entry_intents_total ({entry_intents_total}) "
                    f"must equal entry_valid_mask_sum ({entry_valid_mask_sum})"
                )
        
        # Note: created_bar sorted verification would require accessing internal arrays
        # For now, we verify the sparse contract which implies created_bar is sorted
        # (since flatnonzero returns sorted indices)
        
    finally:
        # Restore environment
        if old_trigger_rate is None:
            os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
        else:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate


================================================================================
FILE: tests/test_sparse_intents_mvp_contract.py
================================================================================

"""Contract tests for sparse intents MVP (Stage P2-1).

These tests ensure:
1. created_bar is sorted (deterministic ordering)
2. intents_total drops significantly with sparse masking
3. Vectorization parity remains bit-exact
"""

import numpy as np
import pytest

from FishBroWFS_V2.config.dtypes import INDEX_DTYPE
from FishBroWFS_V2.engine.types import BarArrays
from FishBroWFS_V2.strategy.kernel import (
    DonchianAtrParams,
    _build_entry_intents_from_trigger,
    run_kernel_arrays,
)


def _expected_entry_count(donch_prev: np.ndarray, warmup: int) -> int:
    """
    Calculate expected entry count using the same mask rules as production.
    
    Production mask (from _build_entry_intents_from_trigger):
    - i = np.arange(1, n)  # bar indices t (from 1 to n-1)
    - valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)
    
    This helper replicates that exact logic.
    """
    n = donch_prev.size
    # Create index array for bars 1..n-1 (bar indices t, where created_bar = t-1)
    i = np.arange(1, n, dtype=INDEX_DTYPE)
    # Sparse mask: valid entries must be finite, positive, and past warmup
    valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)
    return int(np.count_nonzero(valid_mask))


def _make_donch_hi_with_trigger_rate(
    n_bars: int,
    warmup: int,
    trigger_rate: float,
    seed: int = 42,
) -> np.ndarray:
    """
    Generate donch_hi array with controlled trigger rate.
    
    Args:
        n_bars: number of bars
        warmup: warmup period (bars before warmup are NaN)
        trigger_rate: fraction of bars after warmup that should be valid (0.0-1.0)
        seed: random seed
    
    Returns:
        donch_hi array (float64, n_bars):
        - Bars 0..warmup-1: NaN
        - Bars warmup..n_bars-1: trigger_rate fraction are positive values, rest are NaN
    """
    rng = np.random.default_rng(seed)
    
    donch_hi = np.full(n_bars, np.nan, dtype=np.float64)
    
    # After warmup, set trigger_rate fraction to positive values
    post_warmup_bars = n_bars - warmup
    if post_warmup_bars > 0:
        n_valid = int(post_warmup_bars * trigger_rate)
        if n_valid > 0:
            # Select random indices after warmup
            valid_indices = rng.choice(
                np.arange(warmup, n_bars),
                size=n_valid,
                replace=False,
            )
            # Set valid indices to positive values (e.g., 100.0 + small random)
            donch_hi[valid_indices] = 100.0 + rng.random(n_valid) * 10.0
    
    return donch_hi


class TestSparseIntentsMVP:
    """Test sparse intents MVP contract."""

    def test_sparse_intents_created_bar_is_sorted(self):
        """
        Contract: created_bar must be sorted (non-decreasing).
        
        This ensures deterministic ordering and that sparse masking preserves
        the original bar sequence.
        """
        n_bars = 1000
        warmup = 20
        trigger_rate = 0.1
        
        # Generate donch_hi with controlled trigger rate
        donch_hi = _make_donch_hi_with_trigger_rate(n_bars, warmup, trigger_rate, seed=42)
        
        # Create donch_prev (shifted for next-bar active)
        donch_prev = np.empty_like(donch_hi)
        donch_prev[0] = np.nan
        donch_prev[1:] = donch_hi[:-1]
        
        # Build entry intents
        result = _build_entry_intents_from_trigger(
            donch_prev=donch_prev,
            channel_len=warmup,
            order_qty=1,
        )
        
        created_bar = result["created_bar"]
        n_entry = result["n_entry"]
        
        # Verify n_entry matches expected count (exact match using production mask rules)
        expected = _expected_entry_count(donch_prev, warmup)
        assert n_entry == expected, (
            f"n_entry ({n_entry}) should equal expected ({expected}) "
            f"calculated using production mask rules"
        )
        
        # Verify created_bar is sorted (non-decreasing)
        if n_entry > 1:
            assert np.all(created_bar[1:] >= created_bar[:-1]), (
                f"created_bar must be sorted (non-decreasing). "
                f"Got: {created_bar[:10]} ... (showing first 10)"
            )
        
        # Hard consistency check: created_bar must match flatnonzero result exactly
        # This locks in the ordering contract
        i = np.arange(1, donch_prev.size, dtype=INDEX_DTYPE)
        valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)
        idx = np.flatnonzero(valid_mask).astype(created_bar.dtype)
        assert np.array_equal(created_bar[:n_entry], idx), (
            f"created_bar must exactly match flatnonzero result. "
            f"Got: {created_bar[:min(10, n_entry)]}, "
            f"Expected: {idx[:min(10, len(idx))]}"
        )

    def test_sparse_intents_total_drops_order_of_magnitude(self):
        """
        Contract: intents_total should drop significantly with sparse masking.
        
        With controlled trigger rate (e.g., 5%), intents_total should be << n_bars.
        This test directly controls donch_hi to ensure precise trigger rate.
        """
        n_bars = 1000
        warmup = 20
        trigger_rate = 0.05  # 5% trigger rate
        
        # Generate donch_hi with controlled trigger rate
        donch_hi = _make_donch_hi_with_trigger_rate(n_bars, warmup, trigger_rate, seed=42)
        
        # Create donch_prev (shifted for next-bar active)
        donch_prev = np.empty_like(donch_hi)
        donch_prev[0] = np.nan
        donch_prev[1:] = donch_hi[:-1]
        
        # Build entry intents
        result = _build_entry_intents_from_trigger(
            donch_prev=donch_prev,
            channel_len=warmup,
            order_qty=1,
        )
        
        n_entry = result["n_entry"]
        obs = result["obs"]
        
        # Verify diagnostic observations
        assert obs["n_bars"] == n_bars
        assert obs["warmup"] == warmup
        assert obs["valid_mask_sum"] == n_entry
        
        # Verify n_entry matches expected count (exact match using production mask rules)
        expected = _expected_entry_count(donch_prev, warmup)
        assert n_entry == expected, (
            f"n_entry ({n_entry}) should equal expected ({expected}) "
            f"calculated using production mask rules"
        )
        
        # Order-of-magnitude contract: n_entry should be significantly less than n_bars
        # This is the core contract of this test
        # Conservative threshold: 6% of (n_bars - warmup) as upper bound
        max_expected_ratio = 0.06  # 6% conservative upper bound
        max_expected = int((n_bars - warmup) * max_expected_ratio)
        
        assert n_entry <= max_expected, (
            f"n_entry ({n_entry}) should be <= {max_expected} "
            f"({max_expected_ratio*100}% of post-warmup bars) "
            f"with trigger_rate={trigger_rate}, n_bars={n_bars}, warmup={warmup}. "
            f"Sparse masking should significantly reduce intent count (order-of-magnitude reduction)."
        )
        
        # Also verify it's not zero (unless trigger_rate is too low)
        if trigger_rate > 0:
            # With 5% trigger rate, we should have some intents
            assert n_entry > 0, (
                f"Expected some intents with trigger_rate={trigger_rate}, "
                f"but got n_entry={n_entry}"
            )

    def test_vectorization_parity_still_bit_exact(self):
        """
        Contract: Vectorization parity tests should still pass after sparse masking.
        
        This test ensures that sparse masking doesn't break existing parity contracts.
        We rely on the existing test_vectorization_parity.py to verify this.
        
        This test is a placeholder to document the requirement.
        """
        # This test doesn't need to re-implement parity checks.
        # It's sufficient to ensure that make check passes all existing tests.
        # The actual parity verification is in tests/test_vectorization_parity.py
        
        # Basic sanity check: sparse masking should produce valid results
        n_bars = 100
        bars = BarArrays(
            open=np.arange(100, 200, dtype=np.float64),
            high=np.arange(101, 201, dtype=np.float64),
            low=np.arange(99, 199, dtype=np.float64),
            close=np.arange(100, 200, dtype=np.float64),
        )
        
        params = DonchianAtrParams(
            channel_len=10,
            atr_len=5,
            stop_mult=1.5,
        )
        
        result = run_kernel_arrays(
            bars,
            params,
            commission=0.0,
            slip=0.0,
            order_qty=1,
        )
        
        # Verify result structure is intact
        assert "fills" in result
        assert "metrics" in result
        assert "_obs" in result
        assert "intents_total" in result["_obs"]
        
        # Verify diagnostic observations are present
        assert "n_bars" in result["_obs"]
        assert "warmup" in result["_obs"]
        assert "valid_mask_sum" in result["_obs"]
        
        # Verify intents_total is reasonable
        intents_total = result["_obs"]["intents_total"]
        assert intents_total >= 0
        assert intents_total <= n_bars  # Should be <= n_bars due to sparse masking
        
        # Note: Full parity verification is done by test_vectorization_parity.py
        # This test just ensures the basic contract is met


================================================================================
FILE: tests/test_stage0_contract.py
================================================================================

from __future__ import annotations

"""
Stage 0 Contract Tests

Stage 0 must remain a "vector/proxy" layer:
  - MUST NOT import engine/matcher/strategy kernel/pipeline grid runner.
  - MUST NOT create OrderIntent/Fill objects.

These tests are intentionally strict: they prevent "silent scope creep"
that would destroy throughput and blur semantics.
"""

import ast
from pathlib import Path


def _read(path: Path) -> str:
    return path.read_text(encoding="utf-8")


def test_stage0_does_not_import_engine_or_runner_grid() -> None:
    root = Path(__file__).resolve().parent.parent
    p = root / "src" / "FishBroWFS_V2" / "stage0" / "ma_proxy.py"
    code = _read(p)
    tree = ast.parse(code)

    banned_prefixes = (
        "FishBroWFS_V2.engine",
        "FishBroWFS_V2.strategy",
        "FishBroWFS_V2.pipeline",
    )

    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for a in node.names:
                name = a.name
                assert not name.startswith(banned_prefixes), f"banned import: {name}"
        if isinstance(node, ast.ImportFrom):
            mod = node.module or ""
            assert not mod.startswith(banned_prefixes), f"banned import-from: {mod}"


def test_stage0_file_exists() -> None:
    root = Path(__file__).resolve().parent.parent
    p = root / "src" / "FishBroWFS_V2" / "stage0" / "ma_proxy.py"
    assert p.exists(), "Stage0 module must exist"




================================================================================
FILE: tests/test_stage0_ma_proxy.py
================================================================================

from __future__ import annotations

import numpy as np

from FishBroWFS_V2.stage0.ma_proxy import stage0_score_ma_proxy


def test_stage0_scores_shape_and_ordering_trend_series() -> None:
    # Simple upward trend: MA(5)-MA(20) should be mostly positive => positive score
    n = 500
    close = np.linspace(100.0, 200.0, n, dtype=np.float64)

    params = np.array(
        [
            [5.0, 20.0, 0.0],
            [20.0, 5.0, 0.0],  # inverted => should score worse
            [1.0, 2.0, 0.0],
        ],
        dtype=np.float64,
    )

    scores = stage0_score_ma_proxy(close, params)
    assert scores.shape == (3,)
    assert np.isfinite(scores[0])
    assert np.isfinite(scores[1])
    assert np.isfinite(scores[2])
    assert scores[0] > scores[1]


def test_stage0_rejects_invalid_lengths() -> None:
    close = np.linspace(100.0, 101.0, 50, dtype=np.float64)
    params = np.array([[0.0, 10.0], [10.0, 0.0], [1000.0, 5.0]], dtype=np.float64)
    scores = stage0_score_ma_proxy(close, params)
    assert scores.shape == (3,)
    assert scores[0] == -np.inf
    assert scores[1] == -np.inf
    assert scores[2] == -np.inf




================================================================================
FILE: tests/test_stage0_no_pnl_contract.py
================================================================================

"""Test Stage0 contract: must NOT contain any PnL/metrics fields.

Stage0 is a proxy ranking stage and must not compute any PnL-related metrics.
This test enforces the contract by checking that Stage0Result does not contain
forbidden PnL/metrics fields.
"""

import inspect
import numpy as np

from FishBroWFS_V2.pipeline.stage0_runner import Stage0Result, run_stage0


# Blacklist of forbidden field names (PnL/metrics related)
FORBIDDEN_FIELD_NAMES = {
    "net",
    "profit",
    "mdd",
    "dd",
    "drawdown",
    "sqn",
    "sharpe",
    "winrate",
    "win_rate",
    "equity",
    "pnl",
    "return",
    "returns",
    "trades",
    "trade",
    "final",
    "score",
    "metric",
    "metrics",
}


def test_stage0_result_no_pnl_fields():
    """Test that Stage0Result dataclass does not contain forbidden PnL fields."""
    # Get all field names from Stage0Result
    if hasattr(Stage0Result, "__dataclass_fields__"):
        field_names = set(Stage0Result.__dataclass_fields__.keys())
    else:
        # Fallback: inspect annotations
        annotations = getattr(Stage0Result, "__annotations__", {})
        field_names = set(annotations.keys())
    
    # Check each field name against blacklist
    violations = []
    for field_name in field_names:
        field_lower = field_name.lower()
        for forbidden in FORBIDDEN_FIELD_NAMES:
            if forbidden in field_lower:
                violations.append(field_name)
                break
    
    assert len(violations) == 0, (
        f"Stage0Result contains forbidden PnL/metrics fields: {violations}\n"
        f"Allowed fields: {field_names}\n"
        f"Forbidden keywords: {FORBIDDEN_FIELD_NAMES}"
    )


def test_stage0_result_allowed_fields_only():
    """Test that Stage0Result only contains allowed fields."""
    # Allowed fields (from spec)
    allowed_fields = {"param_id", "proxy_value", "warmup_ok", "meta"}
    
    if hasattr(Stage0Result, "__dataclass_fields__"):
        actual_fields = set(Stage0Result.__dataclass_fields__.keys())
    else:
        annotations = getattr(Stage0Result, "__annotations__", {})
        actual_fields = set(annotations.keys())
    
    # Check that all fields are in allowed set
    unexpected = actual_fields - allowed_fields
    assert len(unexpected) == 0, (
        f"Stage0Result contains unexpected fields: {unexpected}\n"
        f"Allowed fields: {allowed_fields}\n"
        f"Actual fields: {actual_fields}"
    )


def test_stage0_runner_no_pnl_computation():
    """Test that run_stage0() does not compute PnL metrics."""
    # Generate test data
    np.random.seed(42)
    n_bars = 1000
    n_params = 50
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    params_matrix = np.column_stack([
        np.random.randint(10, 100, size=n_params),
        np.random.randint(5, 50, size=n_params),
        np.random.uniform(1.0, 5.0, size=n_params),
    ]).astype(np.float64)
    
    # Run Stage0
    results = run_stage0(close, params_matrix)
    
    # Verify results structure
    assert len(results) == n_params
    
    for result in results:
        # Verify required fields exist
        assert hasattr(result, "param_id")
        assert hasattr(result, "proxy_value")
        
        # Verify param_id is valid
        assert isinstance(result.param_id, int)
        assert 0 <= result.param_id < n_params
        
        # Verify proxy_value is numeric (can be -inf for invalid params)
        assert isinstance(result.proxy_value, (int, float))
        
        # Verify no PnL fields exist (check attribute names)
        result_dict = result.__dict__ if hasattr(result, "__dict__") else {}
        for field_name in result_dict.keys():
            field_lower = field_name.lower()
            for forbidden in FORBIDDEN_FIELD_NAMES:
                assert forbidden not in field_lower, (
                    f"Stage0Result contains forbidden field: {field_name} "
                    f"(contains '{forbidden}')"
                )


def test_stage0_result_string_representation():
    """Test that Stage0Result string representation does not contain PnL keywords."""
    result = Stage0Result(
        param_id=0,
        proxy_value=10.5,
        warmup_ok=True,
        meta=None,
    )
    
    # Convert to string representation
    result_str = str(result).lower()
    result_repr = repr(result).lower()
    
    # Check that string representations don't contain forbidden keywords
    for forbidden in FORBIDDEN_FIELD_NAMES:
        assert forbidden not in result_str, (
            f"Stage0Result string representation contains forbidden keyword '{forbidden}': {result_str}"
        )
        assert forbidden not in result_repr, (
            f"Stage0Result repr contains forbidden keyword '{forbidden}': {result_repr}"
        )


================================================================================
FILE: tests/test_stage0_proxies.py
================================================================================

from __future__ import annotations

import numpy as np
import pytest

from FishBroWFS_V2.stage0.proxies import (
    activity_proxy,
    activity_proxy_nb,
    activity_proxy_py,
    trend_proxy,
    trend_proxy_nb,
    trend_proxy_py,
    vol_proxy,
    vol_proxy_nb,
    vol_proxy_py,
)

try:
    import numba as nb

    NUMBA_AVAILABLE = nb is not None
except Exception:
    NUMBA_AVAILABLE = False


def _generate_ohlc_trend(n: int, seed: int = 42) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Generate upward trend OHLC data."""
    rng = np.random.default_rng(seed)
    close = np.linspace(100.0, 200.0, n, dtype=np.float64)
    noise = rng.standard_normal(n) * 2.0
    close = close + noise
    high = close + np.abs(rng.standard_normal(n)) * 1.0
    low = close - np.abs(rng.standard_normal(n)) * 1.0
    open_ = (high + low) / 2 + rng.standard_normal(n) * 0.5
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    return open_, high, low, close


def _generate_ohlc_sine(n: int, seed: int = 999) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """Generate oscillating (sine wave) OHLC data."""
    rng = np.random.default_rng(seed)
    t = np.linspace(0, 4 * np.pi, n)
    close = 100.0 + 20.0 * np.sin(t) + rng.standard_normal(n) * 1.0
    high = close + np.abs(rng.standard_normal(n)) * 1.0
    low = close - np.abs(rng.standard_normal(n)) * 1.0
    open_ = (high + low) / 2 + rng.standard_normal(n) * 0.5
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    return open_, high, low, close


# ============================================================================
# Parity Tests (nb vs py)
# ============================================================================


def test_trend_proxy_parity() -> None:
    """Test parity between Numba and Python versions of trend_proxy."""
    if not NUMBA_AVAILABLE:
        pytest.skip("Numba not available")

    open_, high, low, close = _generate_ohlc_trend(500, seed=42)

    # Generate random params
    rng = np.random.default_rng(123)
    n_params = 200
    params = np.empty((n_params, 2), dtype=np.float64)
    params[:, 0] = rng.integers(5, 50, size=n_params)  # fast
    params[:, 1] = rng.integers(20, 100, size=n_params)  # slow

    scores_nb = trend_proxy_nb(open_, high, low, close, params)
    scores_py = trend_proxy_py(open_, high, low, close, params)

    assert scores_nb.shape == scores_py.shape == (n_params,)

    # Check finite scores match
    finite_mask = np.isfinite(scores_py)
    assert np.all(np.isfinite(scores_py[finite_mask]))
    assert np.allclose(scores_nb[finite_mask], scores_py[finite_mask], rtol=0, atol=1e-12)

    # Check -inf matches
    inf_mask = ~finite_mask
    assert np.all(np.isinf(scores_nb[inf_mask]))
    assert np.all(np.isinf(scores_py[inf_mask]))


def test_vol_proxy_parity() -> None:
    """Test parity between Numba and Python versions of vol_proxy."""
    if not NUMBA_AVAILABLE:
        pytest.skip("Numba not available")

    open_, high, low, close = _generate_ohlc_trend(500, seed=42)

    # Generate random params
    rng = np.random.default_rng(456)
    n_params = 200
    params = np.empty((n_params, 2), dtype=np.float64)
    params[:, 0] = rng.integers(5, 50, size=n_params)  # atr_len
    params[:, 1] = rng.uniform(0.2, 1.5, size=n_params)  # stop_mult

    scores_nb = vol_proxy_nb(open_, high, low, close, params)
    scores_py = vol_proxy_py(open_, high, low, close, params)

    assert scores_nb.shape == scores_py.shape == (n_params,)

    finite_mask = np.isfinite(scores_py)
    assert np.all(np.isfinite(scores_py[finite_mask]))
    assert np.allclose(scores_nb[finite_mask], scores_py[finite_mask], rtol=0, atol=1e-12)

    inf_mask = ~finite_mask
    assert np.all(np.isinf(scores_nb[inf_mask]))
    assert np.all(np.isinf(scores_py[inf_mask]))


def test_activity_proxy_parity() -> None:
    """Test parity between Numba and Python versions of activity_proxy."""
    if not NUMBA_AVAILABLE:
        pytest.skip("Numba not available")

    open_, high, low, close = _generate_ohlc_trend(500, seed=42)

    # Generate random params
    rng = np.random.default_rng(789)
    n_params = 200
    params = np.empty((n_params, 1), dtype=np.float64)
    params[:, 0] = rng.integers(5, 50, size=n_params)  # channel_len

    scores_nb = activity_proxy_nb(open_, high, low, close, params)
    scores_py = activity_proxy_py(open_, high, low, close, params)

    assert scores_nb.shape == scores_py.shape == (n_params,)

    finite_mask = np.isfinite(scores_py)
    assert np.all(np.isfinite(scores_py[finite_mask]))
    # Activity proxy uses log1p, so allow slightly larger tolerance
    assert np.allclose(scores_nb[finite_mask], scores_py[finite_mask], rtol=0, atol=1e-10)

    inf_mask = ~finite_mask
    assert np.all(np.isinf(scores_nb[inf_mask]))
    assert np.all(np.isinf(scores_py[inf_mask]))


# ============================================================================
# Semantic Tests
# ============================================================================


def test_trend_proxy_sanity_upward_trend() -> None:
    """Test that upward trend produces positive trend_score."""
    open_, high, low, close = _generate_ohlc_trend(500, seed=42)

    # Good params: fast < slow, reasonable values
    params_good = np.array([[10.0, 30.0], [15.0, 50.0]], dtype=np.float64)
    scores_good = trend_proxy(open_, high, low, close, params_good)

    # Bad params: inverted (fast >= slow)
    params_bad = np.array([[30.0, 10.0], [50.0, 15.0]], dtype=np.float64)
    scores_bad = trend_proxy(open_, high, low, close, params_bad)

    assert np.all(np.isfinite(scores_good))
    assert np.all(np.isfinite(scores_bad))

    # Good params should score better (or at least not worse) than inverted
    # In upward trend, fast < slow should give positive score
    assert scores_good[0] > 0.0 or scores_good[1] > 0.0


def test_activity_proxy_sanity_oscillation_vs_trend() -> None:
    """Test that oscillating sequence has higher activity than trend."""
    # Generate oscillating data
    open_sine, high_sine, low_sine, close_sine = _generate_ohlc_sine(500, seed=999)
    # Generate trend data
    open_trend, high_trend, low_trend, close_trend = _generate_ohlc_trend(500, seed=42)

    # Same params for both (channel_len only)
    params = np.array([[10.0], [15.0]], dtype=np.float64)

    scores_sine = activity_proxy(open_sine, high_sine, low_sine, close_sine, params)
    scores_trend = activity_proxy(open_trend, high_trend, low_trend, close_trend, params)

    assert np.all(np.isfinite(scores_sine))
    assert np.all(np.isfinite(scores_trend))

    # Oscillating sequence should have higher activity (more breakout triggers)
    assert np.mean(scores_sine) > np.mean(scores_trend)


def test_vol_proxy_sanity_positive_scores() -> None:
    """Test that vol_proxy returns finite scores for valid params."""
    open_, high, low, close = _generate_ohlc_trend(500, seed=42)

    params = np.array([[10.0, 0.5], [20.0, 1.0], [30.0, 1.5]], dtype=np.float64)  # [atr_len, stop_mult]
    scores = vol_proxy(open_, high, low, close, params)

    assert np.all(np.isfinite(scores))
    # Vol proxy scores are negative (-log1p(stop_mean)), but finite
    assert np.all(scores <= 0.0)  # Scores are negative (closer to 0 is better)


def test_proxies_reject_invalid_params() -> None:
    """Test that all proxies return -inf for invalid params."""
    open_, high, low, close = _generate_ohlc_trend(100, seed=42)

    # Invalid: too large
    params_invalid = np.array([[1000.0, 2000.0]], dtype=np.float64)

    scores_trend = trend_proxy(open_, high, low, close, params_invalid)
    params_activity_invalid = np.array([[1000.0]], dtype=np.float64)
    scores_activity = activity_proxy(open_, high, low, close, params_activity_invalid)

    assert np.all(np.isinf(scores_trend))
    assert np.all(np.isinf(scores_activity))
    assert np.all(scores_trend < 0)
    assert np.all(scores_activity < 0)

    # Invalid: zero or negative
    params_invalid2 = np.array([[0.0, 10.0], [-5.0, 10.0]], dtype=np.float64)

    scores_trend2 = trend_proxy(open_, high, low, close, params_invalid2)
    params_activity_invalid2 = np.array([[0.0], [-5.0]], dtype=np.float64)
    scores_activity2 = activity_proxy(open_, high, low, close, params_activity_invalid2)

    assert np.all(np.isinf(scores_trend2))
    assert np.all(np.isinf(scores_activity2))

    # Vol proxy: invalid
    params_vol_invalid = np.array([[1000.0, 0.5], [500.0, -1.0]], dtype=np.float64)  # [atr_len, stop_mult]
    scores_vol = vol_proxy(open_, high, low, close, params_vol_invalid)
    assert np.all(np.isinf(scores_vol))


================================================================================
FILE: tests/test_stage0_proxy_rank_corr.py
================================================================================

from __future__ import annotations

import os

import numpy as np
import pytest

from FishBroWFS_V2.pipeline.metrics_schema import (
    METRICS_COL_MAX_DD,
    METRICS_COL_NET_PROFIT,
    METRICS_COL_TRADES,
    METRICS_COLUMN_NAMES,
)
from FishBroWFS_V2.pipeline.runner_grid import run_grid
from FishBroWFS_V2.stage0.proxies import activity_proxy, trend_proxy, vol_proxy

try:
    import numba as nb
except Exception:
    nb = None  # type: ignore


def _rankdata(x: np.ndarray) -> np.ndarray:
    """
    Compute ranks for Spearman correlation (handles ties with average rank).

    Args:
        x: 1D array

    Returns:
        ranks: 1D array of ranks (1-indexed, ties get average rank)
    """
    n = x.shape[0]
    if n == 0:
        return np.empty(0, dtype=np.float64)

    # Get sorted indices
    sorted_indices = np.argsort(x, kind="stable")

    # Compute ranks
    ranks = np.empty(n, dtype=np.float64)
    i = 0
    while i < n:
        # Find all values equal to current value
        j = i
        while j < n - 1 and x[sorted_indices[j]] == x[sorted_indices[j + 1]]:
            j += 1

        # Average rank for this group
        avg_rank = (i + j + 2) / 2.0  # +2 because ranks are 1-indexed

        # Assign ranks
        for k in range(i, j + 1):
            ranks[sorted_indices[k]] = avg_rank

        i = j + 1

    return ranks


def _pearson_corr(x: np.ndarray, y: np.ndarray) -> float:
    """
    Compute Pearson correlation coefficient.

    Args:
        x, y: 1D arrays of same length

    Returns:
        correlation coefficient
    """
    n = x.shape[0]
    if n == 0 or n != y.shape[0]:
        raise ValueError("x and y must have same non-zero length")

    # Compute means
    mx = np.mean(x)
    my = np.mean(y)

    # Compute covariance and variances
    cov = np.sum((x - mx) * (y - my))
    var_x = np.sum((x - mx) ** 2)
    var_y = np.sum((y - my) ** 2)

    # Handle degenerate cases
    if var_x == 0.0 or var_y == 0.0:
        return 0.0

    return cov / np.sqrt(var_x * var_y)


def spearman_corr(x: np.ndarray, y: np.ndarray) -> float:
    """
    Compute Spearman rank correlation coefficient.

    Args:
        x, y: 1D arrays of same length

    Returns:
        Spearman correlation coefficient (rho)
    """
    rx = _rankdata(x)
    ry = _rankdata(y)
    return _pearson_corr(rx, ry)


def _generate_ohlc_for_corr(n: int, seed: int = 42) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Generate OHLC data with regime-switch + jumps for reliable breakout opportunities.
    
    Design:
    - Regime switches every ~250 bars: trending-up, trending-down, mean-reverting/chop
    - Gaussian noise with increased variance
    - Occasional jumps (p=0.01, Â±(2~4)*sigma shock)
    - Ensures high/low have clear intrabar range
    """
    rng = np.random.default_rng(seed)
    base_price = 100.0
    regime_period = 250
    
    # Generate regime sequence (0=trend-up, 1=trend-down, 2=chop)
    n_regimes = (n + regime_period - 1) // regime_period
    regime_seed = seed + 10000
    regime_rng = np.random.default_rng(regime_seed)
    regimes = regime_rng.integers(0, 3, size=n_regimes)
    
    # Generate close series
    close = np.empty(n, dtype=np.float64)
    close[0] = base_price
    
    sigma_base = 3.0  # Base noise sigma
    jump_prob = 0.01
    
    for t in range(1, n):
        regime_idx = t // regime_period
        regime = regimes[regime_idx] if regime_idx < len(regimes) else regimes[-1]
        
        # Trend component based on regime
        if regime == 0:  # Trending up
            trend_component = 0.05
        elif regime == 1:  # Trending down
            trend_component = -0.05
        else:  # Chop/mean-reverting
            trend_component = -0.01 * (close[t-1] - base_price) / 10.0
        
        # Gaussian noise
        noise = rng.standard_normal() * sigma_base
        
        # Occasional jump
        if rng.random() < jump_prob:
            jump_magnitude = rng.uniform(2.0, 4.0) * sigma_base
            jump_sign = 1.0 if rng.random() < 0.5 else -1.0
            noise += jump_sign * jump_magnitude
        
        close[t] = close[t-1] + trend_component + noise
    
    # Generate open (prev close with small gap)
    open_ = np.empty(n, dtype=np.float64)
    open_[0] = base_price
    for t in range(1, n):
        gap = rng.standard_normal() * 0.5
        open_[t] = close[t-1] + gap
    
    # Generate high/low with intrabar range
    high = np.empty(n, dtype=np.float64)
    low = np.empty(n, dtype=np.float64)
    base_range = 1.0
    
    for t in range(n):
        # Intrabar range based on noise magnitude
        noise_mag = abs(rng.standard_normal())
        intrabar_range = noise_mag * 2.0 + base_range
        
        # Ensure high >= max(open, close) and low <= min(open, close)
        max_oc = max(open_[t], close[t])
        min_oc = min(open_[t], close[t])
        
        high[t] = max_oc + intrabar_range * 0.5
        low[t] = min_oc - intrabar_range * 0.5
    
    return open_, high, low, close


@pytest.mark.slow
def test_stage0_proxy_spearman_correlation() -> None:
    """
    Test that Stage0 proxy scores have median Spearman Ï â‰¥ 0.4 with actual PnL.

    This test:
    1. Runs all seeds and computes rho for each non-degenerate seed
    2. Collects all rho values into a list
    3. Uses median rho as the contract (more stable than mean)
    4. Degenerate seeds are skipped but recorded for diagnostics
    5. If all seeds are degenerate, test fails with diagnostic info
    """
    # JIT requirement check: avoid degenerate samples in CI-safe / no-jit environments
    numba_disable_jit_env = os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1"
    numba_disable_jit_config = False
    if nb is not None:
        numba_disable_jit_config = getattr(nb.config, "DISABLE_JIT", 0) == 1

    if numba_disable_jit_env or numba_disable_jit_config:
        pytest.skip(
            "Spearman correlation test requires JIT-enabled Stage2; run without NUMBA_DISABLE_JIT=1\n"
            "Suggested command: PYTHONDONTWRITEBYTECODE=1 pytest -q -m slow -k spearman -vv"
        )

    SEEDS = [0, 1, 2, 3, 4, 5, 6, 7]
    MAX_TRIES = len(SEEDS)
    MIN_VALID = 4  # Hard gate: require at least 4 valid seeds
    n_bars = 1500
    n_params = 250

    # Track evidence for all seeds (including degenerate)
    seeds_tried = []
    pnl_unique_counts = []
    pnl_mins = []
    pnl_maxs = []
    trades_totals = []
    trades_unique_counts = []
    intent_modes = []
    intents_totals = []
    fills_totals = []
    # Collect rho values for non-degenerate seeds
    rho_values = []
    degenerate_seeds = []
    valid_seeds = []

    for seed in SEEDS:
        seeds_tried.append(seed)

        # Generate OHLC data with current seed
        open_, high, low, close = _generate_ohlc_for_corr(n_bars, seed=seed)

        # Generate random params with deterministic seed (seed + 1000 to avoid collision)
        rng = np.random.default_rng(seed + 1000)

        # Params for kernel: [channel_len, atr_len, stop_mult]
        params_kernel = np.empty((n_params, 3), dtype=np.float64)
        params_kernel[:, 0] = rng.integers(5, 40, size=n_params)  # channel_len (reduced range)
        params_kernel[:, 1] = rng.integers(5, 50, size=n_params)  # atr_len
        params_kernel[:, 2] = rng.uniform(0.2, 1.5, size=n_params)  # stop_mult (reduced range)

        # Params for proxies (aligned with Stage2 kernel params)
        # Trend: [fast, slow] where fast = max(2, floor(channel_len/3)), slow = channel_len
        params_trend = np.empty((n_params, 2), dtype=np.float64)
        params_trend[:, 0] = np.maximum(2, params_kernel[:, 0] // 3)  # fast
        params_trend[:, 1] = params_kernel[:, 0]  # slow = channel_len
        # Activity: [channel_len, atr_len] (atr_len kept for compatibility but not used)
        params_activity = params_kernel[:, :2].copy()
        # Vol: [atr_len, stop_mult]
        params_vol = params_kernel[:, 1:3].copy()

        # Compute proxy scores
        trend_scores = trend_proxy(open_, high, low, close, params_trend)
        vol_scores = vol_proxy(open_, high, low, close, params_vol)
        activity_scores = activity_proxy(open_, high, low, close, params_activity)

        # Filter out -inf scores (invalid params)
        valid_mask = np.isfinite(trend_scores) & np.isfinite(vol_scores) & np.isfinite(activity_scores)

        # Combined proxy score (weights: w1=1.0, w2=0.5, w3=1.0)
        # Adjusted weights: emphasize activity (often strongest for breakout strategies)
        proxy_scores = 1.0 * trend_scores + 0.5 * vol_scores + 1.0 * activity_scores

        # Run minimal backtest to get PnL
        from FishBroWFS_V2.pipeline.runner_grid import run_grid

        result = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_kernel,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=False,
            force_close_last=True,
        )

        metrics = result["metrics"]
        pnl = metrics[:, METRICS_COL_NET_PROFIT]  # net_profit column
        trades = metrics[:, METRICS_COL_TRADES]  # trades column

        # Extract perf diagnostic info
        perf = result.get("perf", {})
        intent_mode = perf.get("intent_mode")
        intents_total = perf.get("intents_total")
        fills_total = perf.get("fills_total")

        # Strict diagnostics when trades_sum == 0 (fills exist but trades/pnl = 0)
        trades_sum = float(np.sum(trades))
        if trades_sum == 0.0:
            # Dump metrics diagnostics
            diag_parts = [f"\n[DIAG] seed={seed}: trades_sum=0 but fills_total={fills_total}"]
            diag_parts.append(f"metrics.shape={metrics.shape}")
            diag_parts.append(f"metrics_column_names={METRICS_COLUMN_NAMES}")
            diag_parts.append(f"result.keys()={list(result.keys())}")
            if "metrics_columns" in result:
                diag_parts.append(f"result['metrics_columns']={result.get('metrics_columns')}")

            # First row of metrics
            if metrics.shape[0] > 0:
                n_cols_to_show = min(10, metrics.shape[1])
                diag_parts.append(f"metrics[0, :{n_cols_to_show}]={metrics[0, :n_cols_to_show].tolist()}")

            # Min/max of first few columns (with column names)
            n_cols_to_check = min(5, metrics.shape[1])
            for col_idx in range(n_cols_to_check):
                col_data = metrics[:, col_idx]
                col_name = METRICS_COLUMN_NAMES[col_idx] if col_idx < len(METRICS_COLUMN_NAMES) else f"col{col_idx}"
                diag_parts.append(
                    f"metrics[:, {col_idx}] ({col_name}): min={np.min(col_data):.6f}, max={np.max(col_data):.6f}"
                )

            # Inspect fills payload
            if "fills" in result:
                fills_list = result["fills"]
                if isinstance(fills_list, list):
                    diag_parts.append(f"fills (list): len={len(fills_list)}")
                    if len(fills_list) > 0:
                        diag_parts.append(f"fills[0]={repr(fills_list[0])} (type={type(fills_list[0])})")
                    if len(fills_list) > 1:
                        diag_parts.append(f"fills[1]={repr(fills_list[1])}")
                    if len(fills_list) > 2:
                        diag_parts.append(f"fills[2]={repr(fills_list[2])}")
            elif "fills_arr" in result:
                fills_arr = result["fills_arr"]
                diag_parts.append(f"fills_arr: shape={fills_arr.shape}, dtype={fills_arr.dtype}")
                if fills_arr.shape[0] > 0:
                    n_rows = min(5, fills_arr.shape[0])
                    diag_parts.append(f"fills_arr[:{n_rows}]=\n{fills_arr[:n_rows]}")
            elif "fills_array" in result:
                fills_array = result["fills_array"]
                diag_parts.append(f"fills_array: shape={fills_array.shape}, dtype={fills_array.dtype}")
                if fills_array.shape[0] > 0:
                    n_rows = min(5, fills_array.shape[0])
                    diag_parts.append(f"fills_array[:{n_rows}]=\n{fills_array[:n_rows]}")
            else:
                diag_parts.append("No 'fills', 'fills_arr', or 'fills_array' in result (perf only)")

            # Print diagnostics to stderr for visibility
            import sys

            print("\n".join(diag_parts), file=sys.stderr)

        # Check for degenerate cases
        pnl_unique = np.unique(pnl)
        pnl_unique_count = pnl_unique.size
        pnl_std = np.std(pnl)
        proxy_std = np.std(proxy_scores)

        # Record evidence (including perf diagnostics)
        pnl_unique_counts.append(pnl_unique_count)
        pnl_mins.append(float(np.min(pnl)))
        pnl_maxs.append(float(np.max(pnl)))
        trades_totals.append(float(np.sum(trades)))
        trades_unique_counts.append(np.unique(trades).size)
        intent_modes.append(intent_mode)
        intents_totals.append(intents_total)
        fills_totals.append(fills_total)

        # Check if this sample is degenerate and compute rho if non-degenerate
        is_degenerate = False
        if proxy_std == 0.0:
            is_degenerate = True
        elif pnl_unique_count < 2 or pnl_std == 0.0:
            is_degenerate = True
        else:
            # Filter out invalid proxy scores (-inf)
            # Combine proxy valid_mask with pnl finite check
            valid_mask_combined = valid_mask & np.isfinite(pnl)
            if np.sum(valid_mask_combined) < 10:
                is_degenerate = True
            else:
                proxy_valid = proxy_scores[valid_mask_combined]
                pnl_valid = pnl[valid_mask_combined]

                # Check again after filtering
                if np.std(pnl_valid) == 0.0 or np.unique(pnl_valid).size < 2:
                    is_degenerate = True
                else:
                    # Non-degenerate sample - compute Spearman correlation
                    rho = spearman_corr(proxy_valid, pnl_valid)
                    rho_values.append(rho)
                    valid_seeds.append(seed)
                    # Continue to next seed (collect all rho values)

        if is_degenerate:
            degenerate_seeds.append(seed)
            # Continue to next seed (skip degenerate, but diagnostics already recorded)

    # Check minimum valid seeds requirement
    if len(rho_values) < MIN_VALID:
        # Build detailed diagnostic message with per-seed info
        diag_lines = [
            f"Insufficient valid seeds: {len(rho_values)}/{MAX_TRIES} < MIN_VALID={MIN_VALID}",
            f"Valid seeds: {valid_seeds}",
            f"Degenerate seeds: {degenerate_seeds}",
            "",
            "Per-seed summary:",
        ]
        for i, seed in enumerate(seeds_tried):
            is_valid = seed in valid_seeds
            diag_lines.append(
                f"seed={seed} ({'VALID' if is_valid else 'DEGENERATE'}): "
                f"intent_mode={intent_modes[i]}, "
                f"intents_total={intents_totals[i]}, "
                f"fills_total={fills_totals[i]}, "
                f"trades_sum={trades_totals[i]}, "
                f"pnl_unique={pnl_unique_counts[i]}, "
                f"pnl_range=[{pnl_mins[i]:.4f}, {pnl_maxs[i]:.4f}], "
                f"trades_unique={trades_unique_counts[i]}"
            )
        if len(rho_values) > 0:
            diag_lines.append(f"rho_values (partial): {rho_values}")
        pytest.fail("\n".join(diag_lines))

    # Compute median and mean rho
    median_rho = float(np.median(rho_values))
    mean_rho = float(np.mean(rho_values))

    # Assert correlation contract using median (more stable than mean)
    # Only assert if we have enough valid seeds (already checked above)
    assert median_rho >= 0.4, (
        f"Median Spearman correlation {median_rho:.4f} < 0.4 threshold. "
        f"Mean rho={mean_rho:.4f}, "
        f"rho_values={rho_values}, "
        f"valid_seeds={valid_seeds} ({len(rho_values)}/{MAX_TRIES}), "
        f"degenerate_seeds={degenerate_seeds}"
    )


def test_spearman_corr_basic() -> None:
    """Basic test for Spearman correlation function."""
    # Perfect positive correlation
    x = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
    y = np.array([2.0, 4.0, 6.0, 8.0, 10.0])
    rho = spearman_corr(x, y)
    assert abs(rho - 1.0) < 1e-10

    # Perfect negative correlation
    y_neg = np.array([10.0, 8.0, 6.0, 4.0, 2.0])
    rho_neg = spearman_corr(x, y_neg)
    assert abs(rho_neg - (-1.0)) < 1e-10

    # No correlation (random)
    rng = np.random.default_rng(42)
    y_rand = rng.standard_normal(100)
    x_rand = rng.standard_normal(100)
    rho_rand = spearman_corr(x_rand, y_rand)
    assert abs(rho_rand) < 0.5  # Should be close to 0 for independent data


def test_spearman_corr_with_ties() -> None:
    """Test Spearman correlation with tied values."""
    # Test with ties
    x = np.array([1.0, 2.0, 2.0, 3.0, 4.0])
    y = np.array([2.0, 3.0, 4.0, 5.0, 6.0])
    rho = spearman_corr(x, y)
    # Should still be positive
    assert rho > 0.0

    # All same values (degenerate)
    x_same = np.array([1.0, 1.0, 1.0])
    y_same = np.array([2.0, 2.0, 2.0])
    rho_same = spearman_corr(x_same, y_same)
    # Should handle gracefully (0 or NaN)
    assert np.isfinite(rho_same) or np.isnan(rho_same)


================================================================================
FILE: tests/test_stage2_params_influence.py
================================================================================

from __future__ import annotations

import numpy as np
import pytest

from FishBroWFS_V2.pipeline.runner_grid import run_grid
from tests.test_stage0_proxy_rank_corr import _generate_ohlc_for_corr


def test_stage2_params_influence_extremes() -> None:
    """
    Contract test: params must influence outcome.
    
    Root cause fuse: if different params produce identical metrics,
    Stage2 is broken and Spearman correlation will be meaningless.
    """
    # Generate OHLC data using same generator as Spearman test
    n_bars = 1500
    seed = 0
    open_, high, low, close = _generate_ohlc_for_corr(n_bars, seed=seed)
    
    # Two extreme params that should produce different outcomes
    params = np.array([
        [5.0, 5.0, 0.2],   # A: short channel, short ATR, tight stop
        [39.0, 49.0, 1.5], # B: long channel, long ATR, wide stop
    ], dtype=np.float64)
    
    # Run grid with debug enabled
    result = run_grid(
        open_=open_,
        high=high,
        low=low,
        close=close,
        params_matrix=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        sort_params=False,
        force_close_last=True,
        return_debug=True,
    )
    
    metrics = result["metrics"]
    debug_fills_first = result.get("debug_fills_first")
    
    # Extract metrics for both params
    net_profit_a = float(metrics[0, 0])  # net_profit
    net_profit_b = float(metrics[1, 0])
    trades_a = int(metrics[0, 1])  # trades
    trades_b = int(metrics[1, 1])
    
    # Extract debug info
    if debug_fills_first is not None:
        entry_bar_a_raw = debug_fills_first[0, 0]
        entry_price_a_raw = debug_fills_first[0, 1]
        exit_bar_a_raw = debug_fills_first[0, 2]
        exit_price_a_raw = debug_fills_first[0, 3]
        
        entry_bar_b_raw = debug_fills_first[1, 0]
        entry_price_b_raw = debug_fills_first[1, 1]
        exit_bar_b_raw = debug_fills_first[1, 2]
        exit_price_b_raw = debug_fills_first[1, 3]
        
        # Handle NaN values
        entry_bar_a = int(entry_bar_a_raw) if np.isfinite(entry_bar_a_raw) else -1
        entry_price_a = float(entry_price_a_raw) if np.isfinite(entry_price_a_raw) else np.nan
        exit_bar_a = int(exit_bar_a_raw) if np.isfinite(exit_bar_a_raw) else -1
        exit_price_a = float(exit_price_a_raw) if np.isfinite(exit_price_a_raw) else np.nan
        
        entry_bar_b = int(entry_bar_b_raw) if np.isfinite(entry_bar_b_raw) else -1
        entry_price_b = float(entry_price_b_raw) if np.isfinite(entry_price_b_raw) else np.nan
        exit_bar_b = int(exit_bar_b_raw) if np.isfinite(exit_bar_b_raw) else -1
        exit_price_b = float(exit_price_b_raw) if np.isfinite(exit_price_b_raw) else np.nan
        
        debug_msg = (
            f"Param A [5, 5, 0.2]: entry_bar={entry_bar_a}, entry_price={entry_price_a:.4f}, "
            f"exit_bar={exit_bar_a}, exit_price={exit_price_a:.4f}, "
            f"net_profit={net_profit_a:.4f}, trades={trades_a}\n"
            f"Param B [39, 49, 1.5]: entry_bar={entry_bar_b}, entry_price={entry_price_b:.4f}, "
            f"exit_bar={exit_bar_b}, exit_price={exit_price_b:.4f}, "
            f"net_profit={net_profit_b:.4f}, trades={trades_b}"
        )
    else:
        debug_msg = (
            f"Param A [5, 5, 0.2]: net_profit={net_profit_a:.4f}, trades={trades_a}\n"
            f"Param B [39, 49, 1.5]: net_profit={net_profit_b:.4f}, trades={trades_b}"
        )
        # Fallback: use metrics only
        entry_bar_a = entry_bar_b = -1
        entry_price_a = entry_price_b = np.nan
        exit_bar_a = exit_bar_b = -1
        exit_price_a = exit_price_b = np.nan
    
    # Assert at least one difference exists
    # This is the "root cause fuse" - if all identical, Stage2 is broken
    entry_price_diff = abs(entry_price_a - entry_price_b) if (np.isfinite(entry_price_a) and np.isfinite(entry_price_b)) else 0.0
    exit_price_diff = abs(exit_price_a - exit_price_b) if (np.isfinite(exit_price_a) and np.isfinite(exit_price_b)) else 0.0
    
    assert (
        entry_bar_a != entry_bar_b or
        entry_price_diff > 1e-6 or
        exit_bar_a != exit_bar_b or
        exit_price_diff > 1e-6 or
        abs(net_profit_a - net_profit_b) > 1e-6
    ), (
        f"Params A and B produced identical outcomes - Stage2 is broken!\n"
        f"{debug_msg}\n"
        f"This indicates params are not being used correctly in signal/stop calculation."
    )


================================================================================
FILE: tests/test_trigger_rate_param_subsample_contract.py
================================================================================

"""
Stage P2-3: Contract Tests for Param-subsample Trigger Rate

Verifies that trigger_rate controls param subsampling:
- selected_params_count scales with trigger_rate
- intents_total scales approximately linearly with trigger_rate
- Workload reduction is effective
"""
from __future__ import annotations

import numpy as np
import os

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_selected_params_count_reasonable() -> None:
    """
    Test that selected_params_count is reasonable for given trigger_rate.
    
    With n_params=1000 and trigger_rate=0.05, we expect selected_params_count
    to be approximately 50 (allowing rounding error).
    """
    # Ensure clean environment
    old_param_subsample_rate = os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
    old_param_subsample_seed = os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_SEED", None)
    
    try:
        n_bars = 500
        n_params = 1000
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Generate params matrix
        params_list = []
        for i in range(n_params):
            ch_len = 20 + (i % 10)
            atr_len = 10 + (i % 5)
            stop_mult = 1.0 + (i % 3) * 0.5
            params_list.append([ch_len, atr_len, stop_mult])
        
        params_matrix = np.array(params_list, dtype=np.float64)
        
        # Set param_subsample_rate=0.05
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = "0.05"
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_SEED"] = "42"
        
        result = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Verify perf dict contains trigger rate info
        assert "perf" in result, "perf must exist in run_grid result"
        perf = result["perf"]
        assert isinstance(perf, dict), "perf must be a dict"
        
        selected_params_count = perf.get("selected_params_count")
        param_subsample_rate_configured = perf.get("param_subsample_rate_configured")
        selected_params_ratio = perf.get("selected_params_ratio")
        
        assert selected_params_count is not None, "selected_params_count must exist"
        assert param_subsample_rate_configured is not None, "param_subsample_rate_configured must exist"
        assert selected_params_ratio is not None, "selected_params_ratio must exist"
        
        assert param_subsample_rate_configured == 0.05, f"param_subsample_rate_configured should be 0.05, got {param_subsample_rate_configured}"
        
        # Contract: selected_params_count should be approximately 5% of n_params
        # Allow rounding error: [45, 55] for n_params=1000, rate=0.05
        assert 45 <= selected_params_count <= 55, (
            f"selected_params_count ({selected_params_count}) should be approximately 50 "
            f"(5% of {n_params}), got {selected_params_count}"
        )
        
        # Contract: selected_params_ratio should match trigger_rate approximately
        expected_ratio = 0.05
        assert 0.04 <= selected_params_ratio <= 0.06, (
            f"selected_params_ratio ({selected_params_ratio}) should be approximately "
            f"{expected_ratio}, got {selected_params_ratio}"
        )
        
        # Contract: metrics_rows_computed should equal selected_params_count
        metrics_rows_computed = perf.get("metrics_rows_computed")
        assert metrics_rows_computed == selected_params_count, (
            f"metrics_rows_computed ({metrics_rows_computed}) should equal "
            f"selected_params_count ({selected_params_count})"
        )
        
    finally:
        # Restore environment
        if old_param_subsample_rate is None:
            os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
        else:
            os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = old_param_subsample_rate
        
        if old_param_subsample_seed is None:
            os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_SEED", None)
        else:
            os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_SEED"] = old_param_subsample_seed


def test_intents_total_linear_scaling() -> None:
    """
    Test that intents_total scales approximately linearly with trigger_rate.
    
    This verifies workload reduction: when we run 5% of params, intents_total
    should be approximately 5% of baseline.
    """
    # Ensure clean environment
    old_param_subsample_rate = os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
    old_param_subsample_seed = os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_SEED", None)
    
    try:
        n_bars = 500
        n_params = 200
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Generate params matrix
        params_list = []
        for i in range(n_params):
            ch_len = 20 + (i % 10)
            atr_len = 10 + (i % 5)
            stop_mult = 1.0 + (i % 3) * 0.5
            params_list.append([ch_len, atr_len, stop_mult])
        
        params_matrix = np.array(params_list, dtype=np.float64)
        
        # Run A: param_subsample_rate=1.0 (baseline, all params)
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = "1.0"
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_SEED"] = "42"
        
        result_a = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Run B: param_subsample_rate=0.05 (5% of params)
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = "0.05"
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_SEED"] = "42"  # Same seed for deterministic selection
        
        result_b = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Verify perf dicts
        perf_a = result_a.get("perf", {})
        perf_b = result_b.get("perf", {})
        
        assert isinstance(perf_a, dict), "perf_a must be a dict"
        assert isinstance(perf_b, dict), "perf_b must be a dict"
        
        intents_total_a = perf_a.get("intents_total")
        intents_total_b = perf_b.get("intents_total")
        
        assert intents_total_a is not None, "intents_total_a must exist"
        assert intents_total_b is not None, "intents_total_b must exist"
        
        # Contract: intents_total_B should be <= intents_total_A * 0.07 (allowing overhead)
        # With 5% params, we expect approximately 5% workload, but allow up to 7% for overhead
        if intents_total_a > 0:
            ratio = intents_total_b / intents_total_a
            assert ratio <= 0.07, (
                f"intents_total_B ({intents_total_b}) should be <= intents_total_A * 0.07 "
                f"({intents_total_a * 0.07}), got ratio {ratio:.4f}"
            )
        
        # Verify selected_params_count scaling
        selected_count_a = perf_a.get("selected_params_count", n_params)
        selected_count_b = perf_b.get("selected_params_count")
        
        assert selected_count_b is not None, "selected_params_count_B must exist"
        assert selected_count_b < selected_count_a, (
            f"selected_params_count_B ({selected_count_b}) should be < "
            f"selected_params_count_A ({selected_count_a})"
        )
        
    finally:
        # Restore environment
        if old_param_subsample_rate is None:
            os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
        else:
            os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = old_param_subsample_rate
        
        if old_param_subsample_seed is None:
            os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_SEED", None)
        else:
            os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_SEED"] = old_param_subsample_seed


def test_metrics_shape_preserved() -> None:
    """
    Test that metrics shape is preserved (n_params, METRICS_N_COLUMNS) even with subsampling.
    
    Only selected rows should be computed; unselected rows remain zeros.
    Uses metrics_computed_mask to verify which rows were computed.
    """
    # Ensure clean environment
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    old_param_subsample_rate = os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
    old_param_subsample_seed = os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_SEED", None)
    
    try:
        n_bars = 300
        n_params = 100
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Generate params matrix
        params_list = []
        for i in range(n_params):
            ch_len = 20 + (i % 10)
            atr_len = 10 + (i % 5)
            stop_mult = 1.0
            params_list.append([ch_len, atr_len, stop_mult])
        
        params_matrix = np.array(params_list, dtype=np.float64)
        
        # Fix trigger_rate=1.0 (no intent-level sparsity) to test param subsample only
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
        # Set param_subsample_rate=0.1 (10% of params)
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = "0.1"
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_SEED"] = "42"
        
        result = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Verify metrics shape is preserved
        metrics = result.get("metrics")
        assert metrics is not None, "metrics must exist"
        assert isinstance(metrics, np.ndarray), "metrics must be np.ndarray"
        assert metrics.shape == (n_params, 3), (
            f"metrics shape should be ({n_params}, 3), got {metrics.shape}"
        )
        
        # Verify perf dict
        perf = result.get("perf", {})
        metrics_rows_computed = perf.get("metrics_rows_computed")
        selected_params_count = perf.get("selected_params_count")
        metrics_computed_mask = perf.get("metrics_computed_mask")
        
        assert metrics_rows_computed == selected_params_count, (
            f"metrics_rows_computed ({metrics_rows_computed}) should equal "
            f"selected_params_count ({selected_params_count})"
        )
        
        # Verify metrics_computed_mask exists and has correct shape
        assert metrics_computed_mask is not None, "metrics_computed_mask must exist in perf"
        assert isinstance(metrics_computed_mask, list), "metrics_computed_mask must be a list"
        assert len(metrics_computed_mask) == n_params, (
            f"metrics_computed_mask length ({len(metrics_computed_mask)}) should equal n_params ({n_params})"
        )
        
        # Convert to numpy array for easier manipulation
        mask_array = np.array(metrics_computed_mask, dtype=bool)
        
        # Verify that mask sum equals selected_params_count
        assert np.sum(mask_array) == selected_params_count, (
            f"metrics_computed_mask sum ({np.sum(mask_array)}) should equal "
            f"selected_params_count ({selected_params_count})"
        )
        
        # Verify that uncomputed rows remain all zeros
        uncomputed_non_zero = np.sum(np.any(np.abs(metrics[~mask_array]) > 1e-10, axis=1))
        assert uncomputed_non_zero == 0, (
            f"Uncomputed rows with non-zero metrics ({uncomputed_non_zero}) should be 0"
        )
        
        # NOTE: Do NOT require computed rows to be non-zero.
        # It's valid to have entry fills but no exits (trades=0), producing all-zero metrics.
        # Evidence of computation is provided by metrics_rows_computed == selected_params_count
        # and the metrics_computed_mask bookkeeping above.
        
    finally:
        # Restore environment
        if old_trigger_rate is None:
            os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
        else:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate
        
        if old_param_subsample_rate is None:
            os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
        else:
            os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = old_param_subsample_rate
        
        if old_param_subsample_seed is None:
            os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_SEED", None)
        else:
            os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_SEED"] = old_param_subsample_seed


================================================================================
FILE: tests/test_ui_artifact_validation.py
================================================================================

"""Tests for UI artifact validation.

Tests verify:
1. MISSING status when file does not exist
2. INVALID status when schema validation fails (with readable error messages)
3. DIRTY status when config_hash mismatch
4. OK status when validation passes
"""

from __future__ import annotations

import json
from pathlib import Path

import pytest

from FishBroWFS_V2.core.artifact_reader import ReadResult, SafeReadResult, try_read_artifact
from FishBroWFS_V2.core.artifact_status import (
    ArtifactStatus,
    ValidationResult,
    validate_governance_status,
    validate_manifest_status,
    validate_winners_v2_status,
)
from FishBroWFS_V2.core.schemas.governance import GovernanceReport
from FishBroWFS_V2.core.schemas.manifest import RunManifest
from FishBroWFS_V2.core.schemas.winners_v2 import WinnersV2
from FishBroWFS_V2.gui.viewer.schema import EvidenceLink


# Fixtures
@pytest.fixture
def fixtures_dir() -> Path:
    """Return path to test fixtures directory."""
    return Path(__file__).parent / "fixtures" / "artifacts"


@pytest.fixture
def temp_dir(tmp_path: Path) -> Path:
    """Return temporary directory for test artifacts."""
    return tmp_path


# Test: MISSING status
def test_manifest_missing_file(temp_dir: Path) -> None:
    """Test that missing manifest.json returns MISSING status."""
    manifest_path = temp_dir / "manifest.json"
    
    result = validate_manifest_status(str(manifest_path))
    
    assert result.status == ArtifactStatus.MISSING
    assert "ä¸å­˜åœ¨" in result.message or "not found" in result.message.lower()


def test_winners_v2_missing_file(temp_dir: Path) -> None:
    """Test that missing winners_v2.json returns MISSING status."""
    winners_path = temp_dir / "winners_v2.json"
    
    result = validate_winners_v2_status(str(winners_path))
    
    assert result.status == ArtifactStatus.MISSING
    assert "ä¸å­˜åœ¨" in result.message or "not found" in result.message.lower()


def test_governance_missing_file(temp_dir: Path) -> None:
    """Test that missing governance.json returns MISSING status."""
    governance_path = temp_dir / "governance.json"
    
    result = validate_governance_status(str(governance_path))
    
    assert result.status == ArtifactStatus.MISSING
    assert "ä¸å­˜åœ¨" in result.message or "not found" in result.message.lower()


# Test: INVALID status (schema validation errors)
def test_manifest_invalid_missing_field(fixtures_dir: Path) -> None:
    """Test that manifest with missing required field returns INVALID."""
    manifest_path = fixtures_dir / "manifest_missing_field.json"
    
    # Load data
    with manifest_path.open("r", encoding="utf-8") as f:
        manifest_data = json.load(f)
    
    result = validate_manifest_status(str(manifest_path), manifest_data=manifest_data)
    
    assert result.status == ArtifactStatus.INVALID
    assert "ç¼ºå°‘æ¬„ä½" in result.message or "missing" in result.message.lower() or "required" in result.message.lower()
    # Should mention config_hash or season (required fields)
    assert "config_hash" in result.message or "season" in result.message or "run_id" in result.message


def test_winners_v2_invalid_missing_field(fixtures_dir: Path) -> None:
    """Test that winners_v2 with missing required field returns INVALID."""
    winners_path = fixtures_dir / "winners_v2_missing_field.json"
    
    # Load data
    with winners_path.open("r", encoding="utf-8") as f:
        winners_data = json.load(f)
    
    result = validate_winners_v2_status(str(winners_path), winners_data=winners_data)
    
    assert result.status == ArtifactStatus.INVALID
    assert "ç¼ºå°‘æ¬„ä½" in result.message or "missing" in result.message.lower() or "required" in result.message.lower()
    # Should mention net_profit, max_drawdown, or trades (required in WinnerRow)
    assert any(field in result.message for field in ["net_profit", "max_drawdown", "trades", "metrics"])


def test_governance_invalid_missing_field(temp_dir: Path) -> None:
    """Test that governance with missing required field returns INVALID."""
    governance_path = temp_dir / "governance.json"
    
    # Create invalid governance (missing run_id)
    invalid_data = {
        "items": [
            {
                "candidate_id": "test:123",
                "decision": "KEEP",
            }
        ]
    }
    
    with governance_path.open("w", encoding="utf-8") as f:
        json.dump(invalid_data, f)
    
    result = validate_governance_status(str(governance_path), governance_data=invalid_data)
    
    assert result.status == ArtifactStatus.INVALID
    assert "ç¼ºå°‘æ¬„ä½" in result.message or "missing" in result.message.lower() or "required" in result.message.lower()


# Test: DIRTY status (config_hash mismatch)
def test_manifest_dirty_config_hash(fixtures_dir: Path) -> None:
    """Test that manifest with mismatched config_hash returns DIRTY."""
    manifest_path = fixtures_dir / "manifest_valid.json"
    
    # Load data
    with manifest_path.open("r", encoding="utf-8") as f:
        manifest_data = json.load(f)
    
    # Validate with different expected config_hash
    result = validate_manifest_status(
        str(manifest_path),
        manifest_data=manifest_data,
        expected_config_hash="different_hash",
    )
    
    assert result.status == ArtifactStatus.DIRTY
    assert "config_hash" in result.message.lower()


def test_winners_v2_dirty_config_hash(temp_dir: Path) -> None:
    """Test that winners_v2 with mismatched config_hash returns DIRTY."""
    winners_path = temp_dir / "winners_v2.json"
    
    # Create winners with config_hash at top level
    winners_data = {
        "config_hash": "abc123",
        "schema": "v2",
        "stage_name": "stage1_topk",
        "topk": [
            {
                "candidate_id": "donchian_atr:123",
                "strategy_id": "donchian_atr",
                "symbol": "CME.MNQ",
                "timeframe": "60m",
                "params": {},
                "metrics": {
                    "net_profit": 100.0,
                    "max_dd": -10.0,
                    "trades": 10,
                },
            }
        ],
    }
    
    with winners_path.open("w", encoding="utf-8") as f:
        json.dump(winners_data, f)
    
    result = validate_winners_v2_status(
        str(winners_path),
        winners_data=winners_data,
        expected_config_hash="different_hash",
    )
    
    assert result.status == ArtifactStatus.DIRTY
    assert "config_hash" in result.message.lower()
    assert "winners_v2.config_hash" in result.message  # Should reference top-level field


def test_governance_dirty_config_hash(temp_dir: Path) -> None:
    """Test that governance with mismatched config_hash returns DIRTY."""
    governance_path = temp_dir / "governance.json"
    
    # Create governance with config_hash at top level
    governance_data = {
        "config_hash": "abc123",
        "run_id": "test-run-123",
        "items": [
            {
                "candidate_id": "donchian_atr:123",
                "strategy_id": "donchian_atr",
                "decision": "KEEP",
                "rule_id": "R1",
                "reason": "Test",
                "run_id": "test-run-123",
                "stage": "stage1_topk",
                "evidence": [],
                "key_metrics": {},
            }
        ],
        "metadata": {},
    }
    
    with governance_path.open("w", encoding="utf-8") as f:
        json.dump(governance_data, f)
    
    result = validate_governance_status(
        str(governance_path),
        governance_data=governance_data,
        expected_config_hash="different_hash",
    )
    
    assert result.status == ArtifactStatus.DIRTY
    assert "config_hash" in result.message.lower()
    assert "governance.config_hash" in result.message  # Should reference top-level field


# Test: OK status (validation passes)
def test_manifest_ok(fixtures_dir: Path) -> None:
    """Test that valid manifest returns OK status."""
    manifest_path = fixtures_dir / "manifest_valid.json"
    
    # Load data
    with manifest_path.open("r", encoding="utf-8") as f:
        manifest_data = json.load(f)
    
    result = validate_manifest_status(
        str(manifest_path),
        manifest_data=manifest_data,
        expected_config_hash="abc123def456",
    )
    
    assert result.status == ArtifactStatus.OK
    assert "é©—è­‰é€šéŽ" in result.message or "ok" in result.message.lower()


def test_winners_v2_ok(fixtures_dir: Path) -> None:
    """Test that valid winners_v2 returns OK status."""
    winners_path = fixtures_dir / "winners_v2_valid.json"
    
    # Load data
    with winners_path.open("r", encoding="utf-8") as f:
        winners_data = json.load(f)
    
    result = validate_winners_v2_status(str(winners_path), winners_data=winners_data)
    
    assert result.status == ArtifactStatus.OK
    assert "é©—è­‰é€šéŽ" in result.message or "ok" in result.message.lower()


def test_governance_ok(fixtures_dir: Path) -> None:
    """Test that valid governance returns OK status."""
    governance_path = fixtures_dir / "governance_valid.json"
    
    # Load data
    with governance_path.open("r", encoding="utf-8") as f:
        governance_data = json.load(f)
    
    result = validate_governance_status(
        str(governance_path),
        governance_data=governance_data,
        expected_config_hash="abc123def456",
    )
    
    assert result.status == ArtifactStatus.OK
    assert "é©—è­‰é€šéŽ" in result.message or "ok" in result.message.lower()


# Test: ArtifactReader (safe version)
def test_try_read_artifact_json(fixtures_dir: Path) -> None:
    """Test reading JSON artifact with safe version."""
    manifest_path = fixtures_dir / "manifest_valid.json"
    
    result = try_read_artifact(manifest_path)
    
    assert isinstance(result, SafeReadResult)
    assert result.is_ok
    assert result.result is not None
    assert isinstance(result.result.raw, dict)
    assert result.result.meta.source_path == str(manifest_path.resolve())
    assert len(result.result.meta.sha256) == 64  # SHA256 hex length
    assert result.result.meta.mtime_s > 0


def test_try_read_artifact_missing_file(temp_dir: Path) -> None:
    """Test that reading missing file returns error, never raises."""
    missing_path = temp_dir / "missing.json"
    
    result = try_read_artifact(missing_path)
    
    assert isinstance(result, SafeReadResult)
    assert result.is_error
    assert result.error is not None
    assert result.error.error_code == "FILE_NOT_FOUND"
    assert "not found" in result.error.message.lower()


# Test: EvidenceLink
def test_evidence_link() -> None:
    """Test EvidenceLink dataclass."""
    link = EvidenceLink(
        source_path="winners_v2.json",
        json_pointer="/rows/0/net_profit",
        note="Net profit from winners",
    )
    
    assert link.source_path == "winners_v2.json"
    assert link.json_pointer == "/rows/0/net_profit"
    assert link.note == "Net profit from winners"
    
    # Test immutability
    assert link == EvidenceLink(
        source_path="winners_v2.json",
        json_pointer="/rows/0/net_profit",
        note="Net profit from winners",
    )


# Test: Pydantic schemas can parse valid data
def test_manifest_schema_parse(fixtures_dir: Path) -> None:
    """Test that RunManifest can parse valid manifest."""
    manifest_path = fixtures_dir / "manifest_valid.json"
    
    with manifest_path.open("r", encoding="utf-8") as f:
        manifest_data = json.load(f)
    
    manifest = RunManifest(**manifest_data)
    
    assert manifest.run_id == "test-run-123"
    assert manifest.season == "2025Q4"
    assert manifest.config_hash == "abc123def456"
    assert len(manifest.stages) == 1
    assert manifest.stages[0].name == "stage0"


def test_winners_v2_schema_parse(fixtures_dir: Path) -> None:
    """Test that WinnersV2 can parse valid winners."""
    winners_path = fixtures_dir / "winners_v2_valid.json"
    
    with winners_path.open("r", encoding="utf-8") as f:
        winners_data = json.load(f)
    
    winners = WinnersV2(**winners_data)
    
    assert winners.schema_name == "v2"  # schema_name is alias for "schema" in JSON
    assert winners.stage_name == "stage1_topk"
    assert winners.topk is not None
    assert len(winners.topk) == 1


def test_governance_schema_parse(fixtures_dir: Path) -> None:
    """Test that GovernanceReport can parse valid governance."""
    governance_path = fixtures_dir / "governance_valid.json"
    
    with governance_path.open("r", encoding="utf-8") as f:
        governance_data = json.load(f)
    
    governance = GovernanceReport(**governance_data)
    
    assert governance.run_id == "test-run-123"
    assert governance.items is not None
    assert len(governance.items) == 1


================================================================================
FILE: tests/test_vectorization_parity.py
================================================================================

from __future__ import annotations

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.engine_jit import simulate_arrays
from FishBroWFS_V2.engine.types import Fill, OrderKind, OrderRole, Side
from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, run_kernel_arrays, run_kernel_object_mode


def _assert_fills_equal(a: list[Fill], b: list[Fill]) -> None:
    assert len(a) == len(b)
    for fa, fb in zip(a, b):
        assert fa.bar_index == fb.bar_index
        assert fa.role == fb.role
        assert fa.kind == fb.kind
        assert fa.side == fb.side
        assert fa.qty == fb.qty
        assert fa.order_id == fb.order_id
        assert abs(fa.price - fb.price) <= 1e-9


def test_strategy_object_vs_array_mode_parity() -> None:
    rng = np.random.default_rng(42)
    n = 300
    close = 100.0 + np.cumsum(rng.standard_normal(n)).astype(np.float64)
    high = close + 1.0
    low = close - 1.0
    open_ = (high + low) * 0.5

    bars = normalize_bars(open_, high, low, close)
    params = DonchianAtrParams(channel_len=20, atr_len=14, stop_mult=2.0)

    out_obj = run_kernel_object_mode(bars, params, commission=0.0, slip=0.0, order_qty=1)
    out_arr = run_kernel_arrays(bars, params, commission=0.0, slip=0.0, order_qty=1)

    _assert_fills_equal(out_obj["fills"], out_arr["fills"])  # type: ignore[arg-type]


def test_simulate_arrays_same_bar_entry_exit_parity() -> None:
    # Construct a same-bar entry then exit scenario (created_bar=-1 activates on bar0).
    bars = normalize_bars(
        np.array([100.0], dtype=np.float64),
        np.array([120.0], dtype=np.float64),
        np.array([80.0], dtype=np.float64),
        np.array([110.0], dtype=np.float64),
    )

    # ENTRY BUY STOP 105, EXIT SELL STOP 95, both active on bar0.
    order_id = np.array([1, 2], dtype=np.int64)
    created_bar = np.array([-1, -1], dtype=np.int64)
    role = np.array([1, 0], dtype=np.int8)  # ENTRY then EXIT (order_id tie-break handles)
    kind = np.array([0, 0], dtype=np.int8)  # STOP
    side = np.array([1, -1], dtype=np.int8)  # BUY, SELL
    price = np.array([105.0, 95.0], dtype=np.float64)
    qty = np.array([1, 1], dtype=np.int64)

    fills = simulate_arrays(
        bars,
        order_id=order_id,
        created_bar=created_bar,
        role=role,
        kind=kind,
        side=side,
        price=price,
        qty=qty,
        ttl_bars=1,
    )

    assert len(fills) == 2
    assert fills[0].role == OrderRole.ENTRY and fills[0].side == Side.BUY and fills[0].kind == OrderKind.STOP
    assert fills[1].role == OrderRole.EXIT and fills[1].side == Side.SELL and fills[1].kind == OrderKind.STOP




================================================================================
FILE: tests/test_viewer_entrypoint.py
================================================================================

"""Contract tests for Viewer entrypoint.

Ensures single source of truth for Viewer entrypoint.
"""

from __future__ import annotations

from pathlib import Path
from unittest.mock import patch

import pytest

# Ensure only one Viewer entrypoint exists
VIEWER_ENTRYPOINT = "src/FishBroWFS_V2/gui/viewer/app.py"


def test_viewer_entrypoint_importable() -> None:
    """Test that Viewer entrypoint can be imported without errors."""
    try:
        from FishBroWFS_V2.gui.viewer.app import main, get_run_dir_from_query
        assert main is not None
        assert get_run_dir_from_query is not None
    except ImportError as e:
        pytest.fail(f"Failed to import Viewer entrypoint: {e}")


def test_viewer_entrypoint_main_callable() -> None:
    """Test that main() can be called (with streamlit stubbed)."""
    from FishBroWFS_V2.gui.viewer.app import main
    
    # Mock streamlit to avoid actual UI rendering
    with patch("streamlit.set_page_config"), \
         patch("streamlit.query_params", new={"get": lambda key, default="": default}), \
         patch("streamlit.error"), \
         patch("streamlit.info"):
        
        # Should not raise (will show error message but that's expected)
        try:
            main()
        except Exception as e:
            # Only fail if it's an import error or unexpected error
            if "ImportError" in str(type(e)):
                pytest.fail(f"Unexpected import error: {e}")


def test_no_duplicate_viewer_entrypoints() -> None:
    """Test that no duplicate Viewer entrypoints exist in repo."""
    repo_root = Path(__file__).parent.parent
    
    # Find all potential Streamlit entrypoints
    potential_entrypoints = []
    
    # Check ui/ directory (legacy, should not exist)
    ui_app = repo_root / "ui" / "app_streamlit.py"
    if ui_app.exists():
        pytest.fail(f"Legacy Viewer entrypoint still exists: {ui_app}")
    
    # Check for other streamlit apps that might be Viewer entrypoints
    for path in repo_root.rglob("*.py"):
        if "app" in path.name.lower() and "streamlit" in path.read_text().lower():
            # Skip test files
            if "test" in str(path):
                continue
            # Skip the official entrypoint
            if path == repo_root / VIEWER_ENTRYPOINT:
                continue
            # Check if it's a streamlit app
            content = path.read_text()
            if "streamlit" in content and ("main" in content or "if __name__" in content):
                potential_entrypoints.append(path)
    
    # Should only have one Viewer entrypoint
    if potential_entrypoints:
        pytest.fail(
            f"Found duplicate Viewer entrypoints:\n"
            f"  Official: {VIEWER_ENTRYPOINT}\n"
            f"  Duplicates: {[str(p) for p in potential_entrypoints]}"
        )


def test_viewer_entrypoint_exists() -> None:
    """Test that official Viewer entrypoint file exists."""
    repo_root = Path(__file__).parent.parent
    entrypoint_path = repo_root / VIEWER_ENTRYPOINT
    
    assert entrypoint_path.exists(), f"Viewer entrypoint not found: {entrypoint_path}"
    assert entrypoint_path.is_file(), f"Viewer entrypoint is not a file: {entrypoint_path}"


def test_viewer_entrypoint_has_main() -> None:
    """Test that Viewer entrypoint has main() function."""
    repo_root = Path(__file__).parent.parent
    entrypoint_path = repo_root / VIEWER_ENTRYPOINT
    
    content = entrypoint_path.read_text()
    
    assert "def main()" in content, "Viewer entrypoint must have main() function"
    assert 'if __name__ == "__main__"' in content, "Viewer entrypoint must have __main__ guard"


================================================================================
FILE: tests/test_viewer_load_state.py
================================================================================

"""Tests for Viewer load state computation.

Tests compute_load_state() mapping contract.
Uses try_read_artifact() to create SafeReadResult instances.
"""

from __future__ import annotations

import json
import tempfile
from pathlib import Path

import pytest

from FishBroWFS_V2.core.artifact_reader import SafeReadResult, try_read_artifact
from FishBroWFS_V2.core.artifact_status import ValidationResult, ArtifactStatus

from FishBroWFS_V2.gui.viewer.load_state import (
    ArtifactLoadStatus,
    ArtifactLoadState,
    compute_load_state,
)


def test_compute_load_state_ok() -> None:
    """Test OK status mapping."""
    with tempfile.TemporaryDirectory() as tmpdir:
        path = Path(tmpdir) / "manifest.json"
        path.write_text(json.dumps({"run_id": "test"}), encoding="utf-8")
        
        read_result = try_read_artifact(path)
        assert isinstance(read_result, SafeReadResult)
        assert read_result.is_ok
        
        validation_result = ValidationResult(
            status=ArtifactStatus.OK,
            message="manifest.json é©—è­‰é€šéŽ",
        )
        
        state = compute_load_state("manifest", path, read_result, validation_result)
        
        assert state.status == ArtifactLoadStatus.OK
        assert state.artifact_name == "manifest"
        assert state.path == path
        assert state.error is None
        assert state.dirty_reasons == []
        assert state.last_modified_ts is not None


def test_compute_load_state_missing() -> None:
    """Test MISSING status mapping."""
    path = Path("/nonexistent/manifest.json")
    
    read_result = try_read_artifact(path)
    assert isinstance(read_result, SafeReadResult)
    assert read_result.is_error
    
    state = compute_load_state("manifest", path, read_result)
    
    assert state.status == ArtifactLoadStatus.MISSING
    assert state.artifact_name == "manifest"
    assert state.path == path
    assert state.error is None
    assert state.dirty_reasons == []
    assert state.last_modified_ts is None


def test_compute_load_state_invalid_from_read_error() -> None:
    """Test INVALID status from read error (non-FILE_NOT_FOUND)."""
    with tempfile.TemporaryDirectory() as tmpdir:
        path = Path(tmpdir) / "invalid.json"
        # Write invalid JSON
        path.write_text("{invalid json}", encoding="utf-8")
        
        read_result = try_read_artifact(path)
        assert isinstance(read_result, SafeReadResult)
        assert read_result.is_error
        
        state = compute_load_state("manifest", path, read_result)
        
        assert state.status == ArtifactLoadStatus.INVALID
        assert state.artifact_name == "manifest"
        assert state.path == path
        assert state.error is not None
        assert "JSON" in state.error or "decode" in state.error.lower()
        assert state.dirty_reasons == []
        assert state.last_modified_ts is None


def test_compute_load_state_invalid_from_validation() -> None:
    """Test INVALID status from validation result."""
    with tempfile.TemporaryDirectory() as tmpdir:
        path = Path(tmpdir) / "manifest.json"
        path.write_text(json.dumps({"invalid": "data"}), encoding="utf-8")
        
        read_result = try_read_artifact(path)
        assert isinstance(read_result, SafeReadResult)
        assert read_result.is_ok
        
        validation_result = ValidationResult(
            status=ArtifactStatus.INVALID,
            message="manifest.json ç¼ºå°‘æ¬„ä½: run_id",
            error_details="Field required: run_id",
        )
        
        state = compute_load_state("manifest", path, read_result, validation_result)
        
        assert state.status == ArtifactLoadStatus.INVALID
        assert state.artifact_name == "manifest"
        assert state.path == path
        assert state.error == "Field required: run_id"  # Prefers error_details
        assert state.dirty_reasons == []
        assert state.last_modified_ts is not None


def test_compute_load_state_dirty() -> None:
    """Test DIRTY status mapping."""
    with tempfile.TemporaryDirectory() as tmpdir:
        path = Path(tmpdir) / "manifest.json"
        path.write_text(json.dumps({"run_id": "test", "config_hash": "abc123"}), encoding="utf-8")
        
        read_result = try_read_artifact(path)
        assert isinstance(read_result, SafeReadResult)
        assert read_result.is_ok
        
        validation_result = ValidationResult(
            status=ArtifactStatus.DIRTY,
            message="manifest.config_hash=abc123 ä½†é æœŸå€¼ç‚º def456",
        )
        
        state = compute_load_state("manifest", path, read_result, validation_result)
        
        assert state.status == ArtifactLoadStatus.DIRTY
        assert state.artifact_name == "manifest"
        assert state.path == path
        assert state.error is None
        assert state.dirty_reasons == ["manifest.config_hash=abc123 ä½†é æœŸå€¼ç‚º def456"]
        assert state.last_modified_ts is not None


def test_compute_load_state_dirty_empty_reasons() -> None:
    """Test DIRTY status with empty dirty_reasons."""
    with tempfile.TemporaryDirectory() as tmpdir:
        path = Path(tmpdir) / "manifest.json"
        path.write_text(json.dumps({"run_id": "test"}), encoding="utf-8")
        
        read_result = try_read_artifact(path)
        assert isinstance(read_result, SafeReadResult)
        assert read_result.is_ok
        
        validation_result = ValidationResult(
            status=ArtifactStatus.DIRTY,
            message="",  # Empty message
        )
        
        state = compute_load_state("manifest", path, read_result, validation_result)
        
        assert state.status == ArtifactLoadStatus.DIRTY
        assert state.dirty_reasons == []  # Empty list when message is empty


def test_compute_load_state_no_validation_result() -> None:
    """Test compute_load_state without validation_result (assumes OK)."""
    with tempfile.TemporaryDirectory() as tmpdir:
        path = Path(tmpdir) / "manifest.json"
        path.write_text(json.dumps({"run_id": "test"}), encoding="utf-8")
        
        read_result = try_read_artifact(path)
        assert isinstance(read_result, SafeReadResult)
        assert read_result.is_ok
        
        state = compute_load_state("manifest", path, read_result)
        
        assert state.status == ArtifactLoadStatus.OK
        assert state.error is None
        assert state.dirty_reasons == []
        assert state.last_modified_ts is not None


def test_compute_load_state_never_raises() -> None:
    """Test that compute_load_state never raises exceptions."""
    path = Path("/test/manifest.json")
    
    # Test with empty SafeReadResult (both result and error are None)
    read_result = SafeReadResult()
    
    # Should not raise
    state = compute_load_state("manifest", path, read_result)
    
    # Should map to some status (likely INVALID)
    assert state.status in [
        ArtifactLoadStatus.OK,
        ArtifactLoadStatus.MISSING,
        ArtifactLoadStatus.INVALID,
        ArtifactLoadStatus.DIRTY,
    ]


def test_dirty_reasons_preserved() -> None:
    """Test that dirty_reasons are preserved in DIRTY state."""
    with tempfile.TemporaryDirectory() as tmpdir:
        path = Path(tmpdir) / "winners_v2.json"
        path.write_text(json.dumps({"config_hash": "abc123"}), encoding="utf-8")
        
        read_result = try_read_artifact(path)
        assert isinstance(read_result, SafeReadResult)
        assert read_result.is_ok
        
        validation_result = ValidationResult(
            status=ArtifactStatus.DIRTY,
            message="winners_v2.config_hash=abc123 ä½† manifest.config_hash=def456",
        )
        
        state = compute_load_state("winners_v2", path, read_result, validation_result)
        
        assert state.status == ArtifactLoadStatus.DIRTY
        assert len(state.dirty_reasons) == 1
        assert "config_hash" in state.dirty_reasons[0]
        # Ensure dirty_reasons is not swallowed
        assert state.dirty_reasons[0] == "winners_v2.config_hash=abc123 ä½† manifest.config_hash=def456"


================================================================================
FILE: tests/test_viewer_no_ui_import.py
================================================================================

"""Contract test: Viewer must not import ui namespace.

Ensures Viewer code only uses FishBroWFS_V2.* imports, not ui.*
"""

from __future__ import annotations

import ast
import pkgutil
from pathlib import Path

import pytest


def test_viewer_no_ui_imports() -> None:
    """Test that Viewer package does not import from ui namespace."""
    import FishBroWFS_V2.gui.viewer as viewer
    
    ui_imports: list[tuple[str, str]] = []
    
    # Walk through all modules in viewer package
    for importer, modname, ispkg in pkgutil.walk_packages(viewer.__path__, viewer.__name__ + "."):
        try:
            # Import module to trigger any import errors
            module = __import__(modname, fromlist=[""])
            
            # Get source file path
            if hasattr(module, "__file__") and module.__file__:
                source_path = Path(module.__file__)
                if source_path.exists() and source_path.suffix == ".py":
                    # Parse AST to find imports
                    with source_path.open("r", encoding="utf-8") as f:
                        tree = ast.parse(f.read(), filename=str(source_path))
                    
                    # Check all imports
                    for node in ast.walk(tree):
                        if isinstance(node, ast.Import):
                            for alias in node.names:
                                if alias.name.startswith("ui."):
                                    ui_imports.append((modname, alias.name))
                        elif isinstance(node, ast.ImportFrom):
                            if node.module and node.module.startswith("ui."):
                                ui_imports.append((modname, f"from {node.module}"))
        except Exception as e:
            # Skip modules that fail to import (might be missing dependencies)
            # But log for debugging
            if "ImportError" not in str(type(e)):
                pytest.fail(f"Unexpected error importing {modname}: {e}")
    
    # Should have no ui.* imports
    if ui_imports:
        pytest.fail(
            f"Viewer package contains ui.* imports:\n"
            + "\n".join(f"  {mod}: {imp}" for mod, imp in ui_imports)
        )


def test_viewer_imports_compile() -> None:
    """Test that all Viewer imports can be compiled."""
    import FishBroWFS_V2.gui.viewer as viewer
    
    # Try to import all modules (will catch import errors)
    for importer, modname, ispkg in pkgutil.walk_packages(viewer.__path__, viewer.__name__ + "."):
        try:
            __import__(modname, fromlist=[""])
        except ImportError as e:
            # Only fail if it's a missing dependency we can't handle
            if "ui." in str(e):
                pytest.fail(f"Viewer module {modname} imports ui.*: {e}")


def test_viewer_entrypoint_no_ui_import() -> None:
    """Test that Viewer entrypoint does not import ui."""
    repo_root = Path(__file__).parent.parent
    entrypoint_path = repo_root / "src/FishBroWFS_V2/gui/viewer/app.py"
    
    assert entrypoint_path.exists()
    
    content = entrypoint_path.read_text()
    
    # Check for ui.* imports
    if "from ui." in content or "import ui." in content:
        pytest.fail("Viewer entrypoint contains ui.* imports")


def test_viewer_pages_no_ui_artifact_reader_import() -> None:
    """Test that Viewer pages do not import ui.core.artifact_reader."""
    repo_root = Path(__file__).parent.parent
    pages_dir = repo_root / "src/FishBroWFS_V2/gui/viewer/pages"
    
    if not pages_dir.exists():
        return  # No pages directory
    
    for page_file in pages_dir.glob("*.py"):
        if page_file.name == "__init__.py":
            continue
        
        content = page_file.read_text()
        
        # Check for ui.core.artifact_reader imports (should use FishBroWFS_V2.core.artifact_reader)
        if "from ui.core.artifact_reader" in content or "import ui.core.artifact_reader" in content:
            pytest.fail(f"Viewer page {page_file.name} imports ui.core.artifact_reader (should use FishBroWFS_V2.core.artifact_reader)")


def test_viewer_page_scaffold_no_ui_artifact_reader_import() -> None:
    """Test that Viewer page_scaffold does not import ui.core.artifact_reader."""
    repo_root = Path(__file__).parent.parent
    scaffold_file = repo_root / "src/FishBroWFS_V2/gui/viewer/page_scaffold.py"
    
    assert scaffold_file.exists()
    
    content = scaffold_file.read_text()
    
    # Check for ui.core.artifact_reader imports (should use FishBroWFS_V2.core.artifact_reader)
    if "from ui.core.artifact_reader" in content or "import ui.core.artifact_reader" in content:
        pytest.fail("Viewer page_scaffold imports ui.core.artifact_reader (should use FishBroWFS_V2.core.artifact_reader)")


================================================================================
FILE: tests/test_viewer_page_scaffold_no_raise.py
================================================================================

"""Tests for Viewer page scaffold - no raise contract.

Tests that render_viewer_page() never raises exceptions.
Uses monkeypatch to simulate MISSING/INVALID scenarios.
"""

from __future__ import annotations

from pathlib import Path
from unittest.mock import Mock, patch

import pytest

from FishBroWFS_V2.core.artifact_reader import SafeReadResult, try_read_artifact
from FishBroWFS_V2.core.artifact_status import ValidationResult, ArtifactStatus

from FishBroWFS_V2.gui.viewer.page_scaffold import render_viewer_page, Bundle, _load_bundle
from FishBroWFS_V2.gui.viewer.load_state import ArtifactLoadState, ArtifactLoadStatus


def test_load_bundle_missing_manifest() -> None:
    """Test _load_bundle with missing manifest."""
    run_dir = Path("/test/run")
    
    with patch("FishBroWFS_V2.gui.viewer.page_scaffold.try_read_artifact") as mock_read:
        # Mock manifest as MISSING using try_read_artifact behavior
        missing_result = try_read_artifact(Path("/nonexistent/file.json"))
        assert missing_result.is_error
        
        mock_read.side_effect = [
            missing_result,  # manifest MISSING
            SafeReadResult(),  # winners (not used in this test)
            SafeReadResult(),  # governance (not used in this test)
        ]
        
        # Should not raise
        bundle = _load_bundle(run_dir)
        
        assert bundle.manifest_state.status.value == "MISSING"


def test_load_bundle_invalid_winners() -> None:
    """Test _load_bundle with invalid winners."""
    run_dir = Path("/test/run")
    
    with patch("FishBroWFS_V2.gui.viewer.page_scaffold.try_read_artifact") as mock_read, \
         patch("FishBroWFS_V2.gui.viewer.page_scaffold.validate_winners_v2_status") as mock_validate:
        
        # Mock winners read succeeds but validation fails
        ok_result = SafeReadResult(
            result=Mock(
                raw={"config_hash": "test"},
                meta=Mock(mtime_s=1234567890.0),
            ),
        )
        assert ok_result.is_ok
        
        mock_read.side_effect = [
            SafeReadResult(),  # manifest
            ok_result,  # winners read succeeds
            SafeReadResult(),  # governance
        ]
        
        mock_validate.return_value = ValidationResult(
            status=ArtifactStatus.INVALID,
            message="winners_v2.json ç¼ºå°‘æ¬„ä½: config_hash",
            error_details="Field required: config_hash",
        )
        
        # Should not raise
        bundle = _load_bundle(run_dir)
        
        assert bundle.winners_v2_state.status.value == "INVALID"
        assert bundle.winners_v2_state.error is not None


def test_load_bundle_validation_exception_handled() -> None:
    """Test that validation exceptions are caught and handled."""
    run_dir = Path("/test/run")
    
    with patch("FishBroWFS_V2.gui.viewer.page_scaffold.try_read_artifact") as mock_read, \
         patch("FishBroWFS_V2.gui.viewer.page_scaffold.validate_manifest_status") as mock_validate:
        
        ok_result = SafeReadResult(
            result=Mock(
                raw={"run_id": "test"},
                meta=Mock(mtime_s=1234567890.0),
            ),
        )
        
        mock_read.side_effect = [
            ok_result,  # manifest read succeeds
            SafeReadResult(),  # winners
            SafeReadResult(),  # governance
        ]
        
        # Mock validation to raise exception
        mock_validate.side_effect = Exception("Validation error")
        
        # Should not raise - exception is caught
        bundle = _load_bundle(run_dir)
        
        # Should still have a state (computed from read_result only)
        assert bundle.manifest_state is not None


def test_render_viewer_page_no_raise_missing_artifacts() -> None:
    """Test render_viewer_page does not raise when artifacts are missing."""
    run_dir = Path("/test/run")
    
    with patch("FishBroWFS_V2.gui.viewer.page_scaffold._load_bundle") as mock_load:
        # Mock bundle with MISSING artifacts
        mock_load.return_value = Bundle(
            manifest_state=ArtifactLoadState(
                status=ArtifactLoadStatus.MISSING,
                artifact_name="manifest",
                path=Path("/test/manifest.json"),
            ),
            winners_v2_state=ArtifactLoadState(
                status=ArtifactLoadStatus.OK,
                artifact_name="winners_v2",
                path=Path("/test/winners.json"),
            ),
            governance_state=ArtifactLoadState(
                status=ArtifactLoadStatus.OK,
                artifact_name="governance",
                path=Path("/test/governance.json"),
            ),
        )
        
        # Mock streamlit functions
        with patch("streamlit.set_page_config"), \
             patch("streamlit.title"), \
             patch("FishBroWFS_V2.gui.viewer.components.status_bar.render_artifact_status_bar"), \
             patch("streamlit.error"), \
             patch("streamlit.info"):
            
            # Should not raise
            render_viewer_page("Test Page", run_dir)
            
            # Verify BLOCKED message was shown
            # (We can't easily test streamlit calls, but we verify no exception)


def test_render_viewer_page_no_raise_content_renderer_exception() -> None:
    """Test render_viewer_page handles content_renderer exceptions."""
    run_dir = Path("/test/run")
    
    def failing_content_renderer(bundle: Bundle) -> None:
        raise ValueError("Content renderer failed")
    
    with patch("FishBroWFS_V2.gui.viewer.page_scaffold._load_bundle") as mock_load:
        # Mock bundle with OK artifacts
        mock_load.return_value = Bundle(
            manifest_state=ArtifactLoadState(
                status=ArtifactLoadStatus.OK,
                artifact_name="manifest",
                path=Path("/test/manifest.json"),
            ),
            winners_v2_state=ArtifactLoadState(
                status=ArtifactLoadStatus.OK,
                artifact_name="winners_v2",
                path=Path("/test/winners.json"),
            ),
            governance_state=ArtifactLoadState(
                status=ArtifactLoadStatus.OK,
                artifact_name="governance",
                path=Path("/test/governance.json"),
            ),
        )
        
        # Mock streamlit functions
        with patch("streamlit.set_page_config"), \
             patch("streamlit.title"), \
             patch("FishBroWFS_V2.gui.viewer.components.status_bar.render_artifact_status_bar"), \
             patch("streamlit.error"), \
             patch("streamlit.exception"):
            
            # Should not raise - exception is caught
            render_viewer_page("Test Page", run_dir, content_render_fn=failing_content_renderer)
            
            # Verify error was shown (via streamlit.error call)


def test_bundle_has_blocking_error() -> None:
    """Test Bundle.has_blocking_error property."""
    # MISSING blocks
    bundle1 = Bundle(
        manifest_state=ArtifactLoadState(
            status=ArtifactLoadStatus.MISSING,
            artifact_name="manifest",
            path=Path("/test/manifest.json"),
        ),
        winners_v2_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="winners_v2",
            path=Path("/test/winners.json"),
        ),
        governance_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="governance",
            path=Path("/test/governance.json"),
        ),
    )
    assert bundle1.has_blocking_error is True
    
    # INVALID blocks
    bundle2 = Bundle(
        manifest_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="manifest",
            path=Path("/test/manifest.json"),
        ),
        winners_v2_state=ArtifactLoadState(
            status=ArtifactLoadStatus.INVALID,
            artifact_name="winners_v2",
            path=Path("/test/winners.json"),
            error="Test error",
        ),
        governance_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="governance",
            path=Path("/test/governance.json"),
        ),
    )
    assert bundle2.has_blocking_error is True
    
    # DIRTY does not block
    bundle3 = Bundle(
        manifest_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="manifest",
            path=Path("/test/manifest.json"),
        ),
        winners_v2_state=ArtifactLoadState(
            status=ArtifactLoadStatus.DIRTY,
            artifact_name="winners_v2",
            path=Path("/test/winners.json"),
            dirty_reasons=["config_hash mismatch"],
        ),
        governance_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="governance",
            path=Path("/test/governance.json"),
        ),
    )
    assert bundle3.has_blocking_error is False
    
    # All OK does not block
    bundle4 = Bundle(
        manifest_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="manifest",
            path=Path("/test/manifest.json"),
        ),
        winners_v2_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="winners_v2",
            path=Path("/test/winners.json"),
        ),
        governance_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="governance",
            path=Path("/test/governance.json"),
        ),
    )
    assert bundle4.has_blocking_error is False


def test_bundle_all_ok() -> None:
    """Test Bundle.all_ok property."""
    # All OK
    bundle1 = Bundle(
        manifest_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="manifest",
            path=Path("/test/manifest.json"),
        ),
        winners_v2_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="winners_v2",
            path=Path("/test/winners.json"),
        ),
        governance_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="governance",
            path=Path("/test/governance.json"),
        ),
    )
    assert bundle1.all_ok is True
    
    # One DIRTY
    bundle2 = Bundle(
        manifest_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="manifest",
            path=Path("/test/manifest.json"),
        ),
        winners_v2_state=ArtifactLoadState(
            status=ArtifactLoadStatus.DIRTY,
            artifact_name="winners_v2",
            path=Path("/test/winners.json"),
            dirty_reasons=["config_hash mismatch"],
        ),
        governance_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="governance",
            path=Path("/test/governance.json"),
        ),
    )
    assert bundle2.all_ok is False
    
    # One MISSING
    bundle3 = Bundle(
        manifest_state=ArtifactLoadState(
            status=ArtifactLoadStatus.MISSING,
            artifact_name="manifest",
            path=Path("/test/manifest.json"),
        ),
        winners_v2_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="winners_v2",
            path=Path("/test/winners.json"),
        ),
        governance_state=ArtifactLoadState(
            status=ArtifactLoadStatus.OK,
            artifact_name="governance",
            path=Path("/test/governance.json"),
        ),
    )
    assert bundle3.all_ok is False


================================================================================
FILE: tests/test_winners_schema_v2_contract.py
================================================================================

"""Contract tests for winners schema v2.

Tests verify:
1. v2 schema structure (top-level fields)
2. WinnerItemV2 structure (required fields)
3. JSON serialization with sorted keys
4. Schema version detection
"""

from __future__ import annotations

import json
from datetime import datetime, timezone

from FishBroWFS_V2.core.winners_schema import (
    WinnerItemV2,
    build_winners_v2_dict,
    is_winners_legacy,
    is_winners_v2,
    WINNERS_SCHEMA_VERSION,
)


def test_winners_v2_top_level_schema() -> None:
    """Test that v2 winners.json has required top-level fields."""
    items = [
        WinnerItemV2(
            candidate_id="donchian_atr:123",
            strategy_id="donchian_atr",
            symbol="CME.MNQ",
            timeframe="60m",
            params={"LE": 8, "LX": 4, "Z": -0.4},
            score=1.234,
            metrics={"net_profit": 100.0, "max_dd": -10.0, "trades": 10, "param_id": 123},
            source={"param_id": 123, "run_id": "test-123", "stage_name": "stage1_topk"},
        ),
    ]
    
    winners = build_winners_v2_dict(
        stage_name="stage1_topk",
        run_id="test-123",
        topk=items,
    )
    
    # Verify top-level fields
    assert winners["schema"] == WINNERS_SCHEMA_VERSION
    assert winners["stage_name"] == "stage1_topk"
    assert "generated_at" in winners
    assert "topk" in winners
    assert "notes" in winners
    
    # Verify notes schema
    assert winners["notes"]["schema"] == WINNERS_SCHEMA_VERSION


def test_winner_item_v2_required_fields() -> None:
    """Test that WinnerItemV2 has all required fields."""
    item = WinnerItemV2(
        candidate_id="donchian_atr:c7bc8b64916c",
        strategy_id="donchian_atr",
        symbol="CME.MNQ",
        timeframe="60m",
        params={"LE": 8, "LX": 4, "Z": -0.4},
        score=1.234,
        metrics={"net_profit": 0.0, "max_dd": 0.0, "trades": 0, "param_id": 9},
        source={"param_id": 9, "run_id": "stage1_topk-123", "stage_name": "stage1_topk"},
    )
    
    item_dict = item.to_dict()
    
    # Verify all required fields exist
    assert "candidate_id" in item_dict
    assert "strategy_id" in item_dict
    assert "symbol" in item_dict
    assert "timeframe" in item_dict
    assert "params" in item_dict
    assert "score" in item_dict
    assert "metrics" in item_dict
    assert "source" in item_dict
    
    # Verify field values
    assert item_dict["candidate_id"] == "donchian_atr:c7bc8b64916c"
    assert item_dict["strategy_id"] == "donchian_atr"
    assert item_dict["symbol"] == "CME.MNQ"
    assert item_dict["timeframe"] == "60m"
    assert isinstance(item_dict["params"], dict)
    assert isinstance(item_dict["score"], (int, float))
    assert isinstance(item_dict["metrics"], dict)
    assert isinstance(item_dict["source"], dict)


def test_winners_v2_json_serializable_sorted_keys() -> None:
    """Test that v2 winners.json is JSON-serializable with sorted keys."""
    items = [
        WinnerItemV2(
            candidate_id="donchian_atr:123",
            strategy_id="donchian_atr",
            symbol="CME.MNQ",
            timeframe="60m",
            params={"LE": 8},
            score=1.234,
            metrics={"net_profit": 100.0, "max_dd": -10.0, "trades": 10, "param_id": 123},
            source={"param_id": 123, "run_id": "test-123", "stage_name": "stage1_topk"},
        ),
    ]
    
    winners = build_winners_v2_dict(
        stage_name="stage1_topk",
        run_id="test-123",
        topk=items,
    )
    
    # Serialize to JSON with sorted keys
    json_str = json.dumps(winners, ensure_ascii=False, sort_keys=True, indent=2)
    
    # Deserialize back
    winners_roundtrip = json.loads(json_str)
    
    # Verify structure
    assert winners_roundtrip["schema"] == WINNERS_SCHEMA_VERSION
    assert len(winners_roundtrip["topk"]) == 1
    
    item_dict = winners_roundtrip["topk"][0]
    assert item_dict["candidate_id"] == "donchian_atr:123"
    assert item_dict["strategy_id"] == "donchian_atr"
    
    # Verify JSON keys are sorted (check top-level)
    json_lines = json_str.split("\n")
    # Find line with "generated_at" and "schema" - should be in sorted order
    # (This is a simple check - full verification would require parsing)
    assert '"generated_at"' in json_str
    assert '"schema"' in json_str


def test_is_winners_v2_detection() -> None:
    """Test schema version detection."""
    # v2 format
    winners_v2 = {
        "schema": "v2",
        "stage_name": "stage1_topk",
        "generated_at": "2025-12-18T00:00:00Z",
        "topk": [],
        "notes": {"schema": "v2"},
    }
    assert is_winners_v2(winners_v2) is True
    assert is_winners_legacy(winners_v2) is False
    
    # Legacy format
    winners_legacy = {
        "topk": [{"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0}],
        "notes": {"schema": "v1"},
    }
    assert is_winners_v2(winners_legacy) is False
    assert is_winners_legacy(winners_legacy) is True
    
    # Unknown format (no schema)
    winners_unknown = {
        "topk": [{"param_id": 0}],
    }
    assert is_winners_v2(winners_unknown) is False
    assert is_winners_legacy(winners_unknown) is True  # Falls back to legacy


def test_winner_item_v2_metrics_contains_legacy_fields() -> None:
    """Test that metrics contains legacy fields for backward compatibility."""
    item = WinnerItemV2(
        candidate_id="donchian_atr:123",
        strategy_id="donchian_atr",
        symbol="CME.MNQ",
        timeframe="60m",
        params={},
        score=1.234,
        metrics={
            "net_profit": 100.0,
            "max_dd": -10.0,
            "trades": 10,
            "param_id": 123,  # Legacy field
        },
        source={"param_id": 123, "run_id": "test-123", "stage_name": "stage1_topk"},
    )
    
    item_dict = item.to_dict()
    metrics = item_dict["metrics"]
    
    # Verify legacy fields exist
    assert "net_profit" in metrics
    assert "max_dd" in metrics
    assert "trades" in metrics
    assert "param_id" in metrics


def test_winners_v2_empty_topk() -> None:
    """Test that v2 schema handles empty topk correctly."""
    winners = build_winners_v2_dict(
        stage_name="stage1_topk",
        run_id="test-123",
        topk=[],
    )
    
    assert winners["schema"] == WINNERS_SCHEMA_VERSION
    assert winners["topk"] == []
    assert isinstance(winners["topk"], list)

