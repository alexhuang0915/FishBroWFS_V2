{"type":"meta","schema_version":2,"run_id":"20251227_191803Z","repo_root":"/home/fishbro/FishBroWFS_V2","snapshot_root":"SNAPSHOT","part":1,"parts":10,"created_at":"2025-12-27T19:18:04Z","generator":{"name":"dump_context","version":"vNext-balanced"},"policies":{"max_parts":10,"target_bytes_per_part":500000,"chunk_max_lines":200,"chunk_max_bytes":120000,"outputs_max_bytes_for_content":2000000,"outputs_policy":"manifest_only_for_big_files","forbid_file_truncated":true,"balance_parts":true,"estimated_total_bytes":3707501,"balance_buffer_ratio":1.08}}
{"type":"file_chunk","path":"src/control/batch_submit.py","chunk_index":1,"line_start":201,"line_end":214,"content":["        if not dataset_record:","            raise ValueError(f\"Dataset {dataset_id} not found in registry; fingerprint required\")","        ","        db_spec = wizard_to_db_jobspec(job, dataset_record)","        job_id = create_job(db_path, db_spec)","        job_ids.append(job_id)","    ","    return BatchSubmitResponse(","        batch_id=batch_id,","        total_jobs=len(job_ids),","        job_ids=job_ids","    )","",""]}
{"type":"file_footer","path":"src/control/batch_submit.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/build_context.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1898,"sha256":"261bdf4f18bbd4cc8bf3c71de00635a8474dac2999888fe6ad4a5ec0453ff0d7","total_lines":63,"chunk_count":1}
{"type":"file_chunk","path":"src/control/build_context.py","chunk_index":0,"line_start":1,"line_end":63,"content":["from __future__ import annotations","","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Optional, Literal","","","BuildMode = Literal[\"FULL\", \"INCREMENTAL\"]","","","@dataclass(frozen=True, slots=True)","class BuildContext:","    \"\"\"","    Contract-only build context.","","    Rules:","    - resolver / runner 不得自行尋找 txt","    - txt_path 必須由 caller 提供","    - 不做任何 filesystem 掃描","    \"\"\"","","    txt_path: Path","    mode: BuildMode","    outputs_root: Path","    build_bars_if_missing: bool = False","","    season: str = \"\"","    dataset_id: str = \"\"","    strategy_id: str = \"\"","    config_snapshot: Optional[dict[str, Any]] = None","    config_hash: str = \"\"","    created_by: str = \"b5c\"","    data_fingerprint_sha1: str = \"\"","","    def __post_init__(self) -> None:","        object.__setattr__(self, \"txt_path\", Path(self.txt_path))","        object.__setattr__(self, \"outputs_root\", Path(self.outputs_root))","","        if self.mode not in (\"FULL\", \"INCREMENTAL\"):","            raise ValueError(f\"Invalid mode: {self.mode}\")","","        if not self.txt_path.exists():","            raise FileNotFoundError(f\"txt_path 不存在: {self.txt_path}\")","","        if self.txt_path.suffix.lower() != \".txt\":","            raise ValueError(\"txt_path must be a .txt file\")","","    def ensure_config_snapshot(self) -> dict[str, Any]:","        return self.config_snapshot or {}","","    def to_build_shared_kwargs(self) -> dict[str, Any]:","        \"\"\"Return kwargs suitable for build_shared.\"\"\"","        return {","            \"txt_path\": self.txt_path,","            \"mode\": self.mode,","            \"outputs_root\": self.outputs_root,","            \"save_fingerprint\": True,","            \"generated_at_utc\": None,","            \"build_bars\": self.build_bars_if_missing,","            \"build_features\": False,  # will be overridden by caller","            \"feature_registry\": None,","            \"tfs\": [15, 30, 60, 120, 240],","        }"]}
{"type":"file_footer","path":"src/control/build_context.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/data_build.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12077,"sha256":"a4cef41080167d189ddaed6ce294a079bacae46e4e60ceab309d47c327373b7f","total_lines":348,"chunk_count":2}
{"type":"file_chunk","path":"src/control/data_build.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"TXT to Parquet Build Pipeline.","","Provides deterministic conversion of raw TXT files to Parquet format","for backtest performance and schema stability.","\"\"\"","","from __future__ import annotations","","import hashlib","import json","import shutil","import tempfile","import time","from dataclasses import dataclass","from datetime import datetime, timezone","from pathlib import Path","from typing import List, Optional, Dict, Any","import pandas as pd","","from data.raw_ingest import ingest_raw_txt, RawIngestResult","","","@dataclass(frozen=True)","class BuildParquetRequest:","    \"\"\"Request to build Parquet from TXT.\"\"\"","    dataset_id: str","    force: bool               # rebuild even if up-to-date","    deep_validate: bool       # optional schema validation after build","    reason: str               # for audit/logging","","","@dataclass(frozen=True)","class BuildParquetResult:","    \"\"\"Result of Parquet build operation.\"\"\"","    ok: bool","    dataset_id: str","    started_utc: str","    finished_utc: str","    txt_signature: str","    parquet_signature: str","    parquet_paths: List[str]","    rows_written: Optional[int]","    notes: List[str]","    error: Optional[str]","","","def _compute_file_signature(file_path: Path, max_size_mb: int = 50) -> str:","    \"\"\"Compute signature for a file.","    ","    For small files (< max_size_mb): compute sha256","    For large files: use stat-hash (path + size + mtime)","    \"\"\"","    try:","        if not file_path.exists():","            return \"missing\"","        ","        stat = file_path.stat()","        file_size_mb = stat.st_size / (1024 * 1024)","        ","        if file_size_mb < max_size_mb:","            # Small file: compute actual hash","            hasher = hashlib.sha256()","            with open(file_path, 'rb') as f:","                # Read in chunks to handle large files","                chunk_size = 8192","                while chunk := f.read(chunk_size):","                    hasher.update(chunk)","            return f\"sha256:{hasher.hexdigest()[:16]}\"","        else:","            # Large file: use stat-hash","            return f\"stat:{file_path.name}:{stat.st_size}:{stat.st_mtime}\"","    except Exception as e:","        return f\"error:{str(e)[:50]}\"","","","def _get_txt_files_for_dataset(dataset_id: str) -> List[Path]:","    \"\"\"Get TXT files required for a dataset.","    ","    This is a placeholder implementation. In a real system, this would","    look up the dataset descriptor to find TXT source paths.","    ","    For now, we'll use a simple mapping based on dataset ID pattern.","    \"\"\"","    # Simple mapping: dataset_id -> txt file pattern","    # In a real implementation, this would come from dataset registry","    base_dir = Path(\"data/raw\")","    ","    # Extract symbol from dataset_id (simplified)","    parts = dataset_id.split('_')","    if len(parts) >= 2 and '.' in parts[0]:","        symbol = parts[0].split('.')[1]  # e.g., \"CME.MNQ\" -> \"MNQ\"","    else:","        symbol = \"unknown\"","    ","    # Look for TXT files","    txt_files = []","    if base_dir.exists():","        for txt_path in base_dir.glob(f\"**/*{symbol}*.txt\"):","            txt_files.append(txt_path)","    ","    # If no files found, create a dummy path for testing","    if not txt_files:","        dummy_path = base_dir / f\"{dataset_id}.txt\"","        txt_files.append(dummy_path)","    ","    return txt_files","","","def _get_parquet_output_path(dataset_id: str) -> Path:","    \"\"\"Get output path for Parquet files.","    ","    Deterministic output paths inside dataset-managed folder.","    \"\"\"","    # Create parquet directory structure","    parquet_root = Path(\"outputs/parquet\")","    ","    # Clean dataset_id for filesystem","    safe_id = dataset_id.replace('/', '_').replace('\\\\', '_').replace(':', '_')","    ","    # Create partitioned structure: parquet/<dataset_id>/data.parquet","    output_dir = parquet_root / safe_id","    output_dir.mkdir(parents=True, exist_ok=True)","    ","    return output_dir / \"data.parquet\"","","","def _build_parquet_from_txt_impl(","    txt_files: List[Path],","    parquet_path: Path,","    force: bool,","    deep_validate: bool",") -> BuildParquetResult:","    \"\"\"Core implementation of TXT to Parquet conversion.\"\"\"","    started_utc = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","    notes = []","    ","    try:","        # 1. Check if TXT files exist","        missing_txt = [str(p) for p in txt_files if not p.exists()]","        if missing_txt:","            return BuildParquetResult(","                ok=False,","                dataset_id=\"unknown\",","                started_utc=started_utc,","                finished_utc=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","                txt_signature=\"\",","                parquet_signature=\"\",","                parquet_paths=[],","                rows_written=None,","                notes=notes,","                error=f\"Missing TXT files: {missing_txt}\"","            )","        ","        # 2. Compute TXT signature","        txt_signatures = []","        for txt_file in txt_files:","            sig = _compute_file_signature(txt_file)","            txt_signatures.append(f\"{txt_file.name}:{sig}\")","        txt_signature = \"|\".join(txt_signatures)","        ","        # 3. Check if Parquet already exists and is up-to-date","        parquet_exists = parquet_path.exists()","        parquet_signature = \"\"","        ","        if parquet_exists:","            parquet_signature = _compute_file_signature(parquet_path)","            # Simple up-to-date check: compare signatures","            # In a real implementation, this would compare metadata","            if not force:","                # Check if we should skip rebuild","                notes.append(f\"Parquet exists at {parquet_path}\")","                # For now, we'll always rebuild if force=False but parquet exists","                # In a real system, we'd compare content hashes","        ","        # 4. Ingest TXT files","        all_dfs = []","        for txt_file in txt_files:","            try:","                result: RawIngestResult = ingest_raw_txt(txt_file)","                df = result.df","                ","                # Convert ts_str to datetime","                df['timestamp'] = pd.to_datetime(df['ts_str'], format='%Y/%m/%d %H:%M:%S', errors='coerce')","                df = df.drop(columns=['ts_str'])","                ","                # Reorder columns","                df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]","                ","                all_dfs.append(df)","                notes.append(f\"Ingested {txt_file.name}: {len(df)} rows\")","            except Exception as e:","                return BuildParquetResult(","                    ok=False,","                    dataset_id=\"unknown\",","                    started_utc=started_utc,","                    finished_utc=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","                    txt_signature=txt_signature,","                    parquet_signature=parquet_signature,","                    parquet_paths=[],","                    rows_written=None,"]}
{"type":"file_chunk","path":"src/control/data_build.py","chunk_index":1,"line_start":201,"line_end":348,"content":["                    notes=notes,","                    error=f\"Failed to ingest {txt_file}: {e}\"","                )","        ","        # 5. Combine DataFrames","        if not all_dfs:","            return BuildParquetResult(","                ok=False,","                dataset_id=\"unknown\",","                started_utc=started_utc,","                finished_utc=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","                txt_signature=txt_signature,","                parquet_signature=parquet_signature,","                parquet_paths=[],","                rows_written=None,","                notes=notes,","                error=\"No data ingested from TXT files\"","            )","        ","        combined_df = pd.concat(all_dfs, ignore_index=True)","        ","        # 6. Sort by timestamp","        combined_df = combined_df.sort_values('timestamp')","        ","        # 7. Write to Parquet with atomic safety","        temp_dir = tempfile.mkdtemp(prefix=\"parquet_build_\")","        try:","            temp_path = Path(temp_dir) / \"temp.parquet\"","            combined_df.to_parquet(","                temp_path,","                engine='pyarrow',","                compression='snappy',","                index=False","            )","            ","            # Atomic rename","            parquet_path.parent.mkdir(parents=True, exist_ok=True)","            shutil.move(str(temp_path), str(parquet_path))","            ","            notes.append(f\"Written Parquet to {parquet_path}\")","        finally:","            shutil.rmtree(temp_dir, ignore_errors=True)","        ","        # 8. Compute new Parquet signature","        new_parquet_signature = _compute_file_signature(parquet_path)","        ","        # 9. Deep validation if requested","        if deep_validate:","            try:","                # Read back and validate schema","                validate_df = pd.read_parquet(parquet_path)","                expected_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']","                if list(validate_df.columns) != expected_cols:","                    notes.append(f\"Warning: Schema mismatch. Expected {expected_cols}, got {list(validate_df.columns)}\")","                else:","                    notes.append(\"Deep validation passed\")","            except Exception as e:","                notes.append(f\"Deep validation warning: {e}\")","        ","        finished_utc = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","        ","        return BuildParquetResult(","            ok=True,","            dataset_id=\"unknown\",","            started_utc=started_utc,","            finished_utc=finished_utc,","            txt_signature=txt_signature,","            parquet_signature=new_parquet_signature,","            parquet_paths=[str(parquet_path)],","            rows_written=len(combined_df),","            notes=notes,","            error=None","        )","        ","    except Exception as e:","        finished_utc = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","        return BuildParquetResult(","            ok=False,","            dataset_id=\"unknown\",","            started_utc=started_utc,","            finished_utc=finished_utc,","            txt_signature=\"\",","            parquet_signature=\"\",","            parquet_paths=[],","            rows_written=None,","            notes=notes,","            error=f\"Build failed: {e}\"","        )","","","def build_parquet_from_txt(req: BuildParquetRequest) -> BuildParquetResult:","    \"\"\"Convert raw TXT to Parquet for the given dataset_id.","    ","    Requirements:","    - Deterministic output paths inside dataset-managed folder","    - Safe atomic writes: write to temp then rename","    - Up-to-date logic:","        - compute txt_signature (stat-hash or partial hash) from required TXT files","        - compute existing parquet_signature (from parquet files or metadata)","        - if not force and signatures match => no-op but ok=True","    - Must never mutate season artifacts.","    \"\"\"","    # Get TXT files for dataset","    txt_files = _get_txt_files_for_dataset(req.dataset_id)","    ","    # Get output path","    parquet_path = _get_parquet_output_path(req.dataset_id)","    ","    # Update result with actual dataset_id","    result = _build_parquet_from_txt_impl(txt_files, parquet_path, req.force, req.deep_validate)","    ","    # Create a new result with the correct dataset_id","    return BuildParquetResult(","        ok=result.ok,","        dataset_id=req.dataset_id,","        started_utc=result.started_utc,","        finished_utc=result.finished_utc,","        txt_signature=result.txt_signature,","        parquet_signature=result.parquet_signature,","        parquet_paths=result.parquet_paths,","        rows_written=result.rows_written,","        notes=result.notes,","        error=result.error","    )","","","# Simple test function","def test_build_parquet() -> None:","    \"\"\"Test the build_parquet_from_txt function.\"\"\"","    print(\"Testing build_parquet_from_txt...\")","    ","    # Create a dummy request","    req = BuildParquetRequest(","        dataset_id=\"test_dataset\",","        force=True,","        deep_validate=False,","        reason=\"test\"","    )","    ","    result = build_parquet_from_txt(req)","    print(f\"Result: {result.ok}\")","    print(f\"Notes: {result.notes}\")","    if result.error:","        print(f\"Error: {result.error}\")","","","if __name__ == \"__main__\":","    test_build_parquet()"]}
{"type":"file_footer","path":"src/control/data_build.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/data_snapshot.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7520,"sha256":"652fedab102ddc2aa8fdcfeb482f3cf21266baa2f7acb98110da2a193a12bd49","total_lines":239,"chunk_count":2}
{"type":"file_chunk","path":"src/control/data_snapshot.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 16.5: Data Snapshot Core (controlled mutation, deterministic).","","Contracts:","- Writes only under outputs/datasets/snapshots/{snapshot_id}/","- Deterministic normalization & checksums","- Immutable snapshots (never overwrite)","- Timezone‑aware UTC timestamps","\"\"\"","","from __future__ import annotations","","import hashlib","import json","import shutil","import tempfile","from datetime import datetime, timezone","from pathlib import Path","from typing import Any","","from contracts.data.snapshot_models import SnapshotMetadata, SnapshotStats","from control.artifacts import canonical_json_bytes, compute_sha256, write_atomic_json","","","def write_json_atomic_any(path: Path, obj: Any) -> None:","    \"\"\"","    Atomically write any JSON‑serializable object to file.","","    Uses the same atomic rename technique as write_atomic_json.","    \"\"\"","    path.parent.mkdir(parents=True, exist_ok=True)","    with tempfile.NamedTemporaryFile(","        mode=\"w\",","        encoding=\"utf-8\",","        dir=path.parent,","        prefix=f\".{path.name}.tmp.\",","        delete=False,","    ) as f:","        json.dump(","            obj,","            f,","            sort_keys=True,","            ensure_ascii=False,","            separators=(\",\", \":\"),","            allow_nan=False,","        )","        tmp_path = Path(f.name)","    try:","        tmp_path.replace(path)","    except Exception:","        tmp_path.unlink(missing_ok=True)","        raise","","","def compute_snapshot_id(","    raw_bars: list[dict[str, Any]],","    symbol: str,","    timeframe: str,","    transform_version: str = \"v1\",",") -> str:","    \"\"\"","    Deterministic snapshot identifier.","","    Format: {symbol}_{timeframe}_{raw_sha256[:12]}_{transform_version}","    \"\"\"","    # Compute raw SHA256 from canonical JSON of raw_bars","    raw_canonical = canonical_json_bytes(raw_bars)","    raw_sha256 = compute_sha256(raw_canonical)","    raw_prefix = raw_sha256[:12]","","    # Normalize symbol and timeframe (remove special chars)","    symbol_norm = symbol.replace(\"/\", \"_\").upper()","    tf_norm = timeframe.replace(\"/\", \"_\").lower()","    return f\"{symbol_norm}_{tf_norm}_{raw_prefix}_{transform_version}\"","","","def normalize_bars(","    raw_bars: list[dict[str, Any]],","    transform_version: str = \"v1\",",") -> tuple[list[dict[str, Any]], str]:","    \"\"\"","    Normalize raw bars to canonical form (deterministic).","","    Returns:","        (normalized_bars, normalized_sha256)","    \"\"\"","    # Ensure each bar has required fields","    required = {\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"}","    normalized = []","    for bar in raw_bars:","        # Validate types","        ts = bar[\"timestamp\"]","        # Ensure timestamp is ISO 8601 string; if not, attempt conversion","        if isinstance(ts, datetime):","            ts = ts.isoformat().replace(\"+00:00\", \"Z\")","        elif not isinstance(ts, str):","            raise ValueError(f\"Invalid timestamp type: {type(ts)}\")","","        # Ensure numeric fields are float","        open_ = float(bar[\"open\"])","        high = float(bar[\"high\"])","        low = float(bar[\"low\"])","        close = float(bar[\"close\"])","        volume = float(bar[\"volume\"]) if isinstance(bar[\"volume\"], (int, float)) else 0.0","","        # Build canonical dict with fixed key order","        canonical = {","            \"timestamp\": ts,","            \"open\": open_,","            \"high\": high,","            \"low\": low,","            \"close\": close,","            \"volume\": volume,","        }","        normalized.append(canonical)","","    # Sort by timestamp ascending","    normalized.sort(key=lambda b: b[\"timestamp\"])","","    # Compute SHA256 of canonical JSON","    canonical_bytes = canonical_json_bytes(normalized)","    sha = compute_sha256(canonical_bytes)","    return normalized, sha","","","def compute_stats(normalized_bars: list[dict[str, Any]]) -> SnapshotStats:","    \"\"\"Compute basic statistics from normalized bars.\"\"\"","    if not normalized_bars:","        raise ValueError(\"normalized_bars cannot be empty\")","","    timestamps = [b[\"timestamp\"] for b in normalized_bars]","    lows = [b[\"low\"] for b in normalized_bars]","    highs = [b[\"high\"] for b in normalized_bars]","    volumes = [b[\"volume\"] for b in normalized_bars]","","    return SnapshotStats(","        count=len(normalized_bars),","        min_timestamp=min(timestamps),","        max_timestamp=max(timestamps),","        min_price=min(lows),","        max_price=max(highs),","        total_volume=sum(volumes),","    )","","","def create_snapshot(","    snapshots_root: Path,","    raw_bars: list[dict[str, Any]],","    symbol: str,","    timeframe: str,","    transform_version: str = \"v1\",",") -> SnapshotMetadata:","    \"\"\"","    Controlled‑mutation: create a data snapshot.","","    Writes only under snapshots_root/{snapshot_id}/","    Deterministic normalization & checksums.","    \"\"\"","    if not raw_bars:","        raise ValueError(\"raw_bars cannot be empty\")","","    # 1. Compute raw SHA256","    raw_canonical = canonical_json_bytes(raw_bars)","    raw_sha256 = compute_sha256(raw_canonical)","","    # 2. Normalize bars","    normalized_bars, normalized_sha256 = normalize_bars(raw_bars, transform_version)","","    # 3. Compute snapshot ID","    snapshot_id = compute_snapshot_id(raw_bars, symbol, timeframe, transform_version)","","    # 4. Create snapshot directory (atomic)","    snapshot_dir = snapshots_root / snapshot_id","    if snapshot_dir.exists():","        raise FileExistsError(","            f\"Snapshot {snapshot_id} already exists; immutable rule violated\"","        )","","    # Write files via temporary directory to ensure atomicity","    with tempfile.TemporaryDirectory(prefix=f\"snapshot_{snapshot_id}_\") as tmp:","        tmp_path = Path(tmp)","","        # raw.json","        raw_path = tmp_path / \"raw.json\"","        write_json_atomic_any(raw_path, raw_bars)","","        # normalized.json","        norm_path = tmp_path / \"normalized.json\"","        write_json_atomic_any(norm_path, normalized_bars)","","        # Compute stats","        stats = compute_stats(normalized_bars)","","        # manifest.json (without manifest_sha256 field)","        manifest = {","            \"snapshot_id\": snapshot_id,","            \"symbol\": symbol,","            \"timeframe\": timeframe,","            \"transform_version\": transform_version,"]}
{"type":"file_chunk","path":"src/control/data_snapshot.py","chunk_index":1,"line_start":201,"line_end":239,"content":["            \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","            \"raw_sha256\": raw_sha256,","            \"normalized_sha256\": normalized_sha256,","            \"stats\": stats.model_dump(mode=\"json\"),","        }","        manifest_path = tmp_path / \"manifest.json\"","        write_json_atomic_any(manifest_path, manifest)","","        # Compute manifest SHA256 (hash of manifest without manifest_sha256)","        manifest_canonical = canonical_json_bytes(manifest)","        manifest_sha256 = compute_sha256(manifest_canonical)","","        # Add manifest_sha256 to manifest","        manifest[\"manifest_sha256\"] = manifest_sha256","        write_json_atomic_any(manifest_path, manifest)","","        # Create snapshot directory","        snapshot_dir.mkdir(parents=True, exist_ok=False)","","        # Move files into place (atomic rename)","        shutil.move(str(raw_path), str(snapshot_dir / \"raw.json\"))","        shutil.move(str(norm_path), str(snapshot_dir / \"normalized.json\"))","        shutil.move(str(manifest_path), str(snapshot_dir / \"manifest.json\"))","","    # Build metadata","    meta = SnapshotMetadata(","        snapshot_id=snapshot_id,","        symbol=symbol,","        timeframe=timeframe,","        transform_version=transform_version,","        created_at=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        raw_sha256=raw_sha256,","        normalized_sha256=normalized_sha256,","        manifest_sha256=manifest_sha256,","        stats=stats,","    )","    return meta","",""]}
{"type":"file_footer","path":"src/control/data_snapshot.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/dataset_catalog.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5273,"sha256":"0672985364c7013b25c7bb91ae24be9e275e1887e31e663c3985e19a02d70fac","total_lines":166,"chunk_count":1}
{"type":"file_chunk","path":"src/control/dataset_catalog.py","chunk_index":0,"line_start":1,"line_end":166,"content":["\"\"\"Dataset Catalog for M1 Wizard.","","Provides dataset listing and filtering capabilities for the wizard UI.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import List, Optional","","from data.dataset_registry import DatasetIndex, DatasetRecord","","","class DatasetCatalog:","    \"\"\"Catalog for available datasets.\"\"\"","    ","    def __init__(self, index_path: Optional[Path] = None):","        \"\"\"Initialize catalog with dataset index.","        ","        Args:","            index_path: Path to dataset index JSON file. If None, uses default.","        \"\"\"","        self.index_path = index_path or Path(\"outputs/datasets/datasets_index.json\")","        self._index: Optional[DatasetIndex] = None","    ","    def load_index(self) -> DatasetIndex:","        \"\"\"Load dataset index from file.\"\"\"","        if not self.index_path.exists():","            raise FileNotFoundError(","                f\"Dataset index not found at {self.index_path}. \"","                \"Please run: python scripts/build_dataset_registry.py\"","            )","        ","        data = json.loads(self.index_path.read_text(encoding=\"utf-8\"))","        self._index = DatasetIndex.model_validate(data)","        return self._index","    ","    @property","    def index(self) -> DatasetIndex:","        \"\"\"Get dataset index (loads if not already loaded).\"\"\"","        if self._index is None:","            self.load_index()","        return self._index","    ","    def list_datasets(self) -> List[DatasetRecord]:","        \"\"\"List all available datasets.\"\"\"","        return self.index.datasets","    ","    def get_dataset(self, dataset_id: str) -> Optional[DatasetRecord]:","        \"\"\"Get dataset by ID.\"\"\"","        for dataset in self.index.datasets:","            if dataset.id == dataset_id:","                return dataset","        return None","    ","    def filter_by_symbol(self, symbol: str) -> List[DatasetRecord]:","        \"\"\"Filter datasets by symbol.\"\"\"","        return [d for d in self.index.datasets if d.symbol == symbol]","    ","    def filter_by_timeframe(self, timeframe: str) -> List[DatasetRecord]:","        \"\"\"Filter datasets by timeframe.\"\"\"","        return [d for d in self.index.datasets if d.timeframe == timeframe]","    ","    def filter_by_exchange(self, exchange: str) -> List[DatasetRecord]:","        \"\"\"Filter datasets by exchange.\"\"\"","        return [d for d in self.index.datasets if d.exchange == exchange]","    ","    def get_unique_symbols(self) -> List[str]:","        \"\"\"Get list of unique symbols.\"\"\"","        return sorted({d.symbol for d in self.index.datasets})","    ","    def get_unique_timeframes(self) -> List[str]:","        \"\"\"Get list of unique timeframes.\"\"\"","        return sorted({d.timeframe for d in self.index.datasets})","    ","    def get_unique_exchanges(self) -> List[str]:","        \"\"\"Get list of unique exchanges.\"\"\"","        return sorted({d.exchange for d in self.index.datasets})","    ","    def validate_dataset_selection(","        self,","        dataset_id: str,","        start_date: Optional[str] = None,","        end_date: Optional[str] = None","    ) -> bool:","        \"\"\"Validate dataset selection with optional date range.","        ","        Args:","            dataset_id: Dataset ID to validate","            start_date: Optional start date (YYYY-MM-DD)","            end_date: Optional end date (YYYY-MM-DD)","            ","        Returns:","            True if valid, False otherwise","        \"\"\"","        dataset = self.get_dataset(dataset_id)","        if dataset is None:","            return False","        ","        # TODO: Add date range validation if needed","        return True","    ","    def list_dataset_ids(self) -> List[str]:","        \"\"\"Get list of all dataset IDs.","        ","        Returns:","            List of dataset IDs sorted alphabetically","        \"\"\"","        return sorted([d.id for d in self.index.datasets])","    ","    def describe_dataset(self, dataset_id: str) -> Optional[DatasetRecord]:","        \"\"\"Get dataset descriptor by ID.","        ","        Args:","            dataset_id: Dataset ID to describe","            ","        Returns:","            DatasetRecord if found, None otherwise","        \"\"\"","        return self.get_dataset(dataset_id)","","","# Singleton instance for easy access","_catalog_instance: Optional[DatasetCatalog] = None","","def get_dataset_catalog() -> DatasetCatalog:","    \"\"\"Get singleton dataset catalog instance.\"\"\"","    global _catalog_instance","    if _catalog_instance is None:","        _catalog_instance = DatasetCatalog()","    return _catalog_instance","","","# Public API functions for registry access","def list_dataset_ids() -> List[str]:","    \"\"\"Public API: Get list of all dataset IDs.","    ","    Returns:","        List of dataset IDs sorted alphabetically","    \"\"\"","    catalog = get_dataset_catalog()","    return catalog.list_dataset_ids()","","","def list_datasets() -> List[DatasetRecord]:","    \"\"\"Public API: Get list of all dataset records.","    ","    Returns:","        List of DatasetRecord objects","    \"\"\"","    catalog = get_dataset_catalog()","    return catalog.list_datasets()","","","def describe_dataset(dataset_id: str) -> Optional[DatasetRecord]:","    \"\"\"Public API: Get dataset descriptor by ID.","    ","    Args:","        dataset_id: Dataset ID to describe","        ","    Returns:","        DatasetRecord if found, None otherwise","    \"\"\"","    catalog = get_dataset_catalog()","    return catalog.describe_dataset(dataset_id)"]}
{"type":"file_footer","path":"src/control/dataset_catalog.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/dataset_descriptor.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5003,"sha256":"e28a3aac75bb1fa6cd8fc6fc9ab4d44fe798b4003673cf9aee9d3ffc0df12031","total_lines":176,"chunk_count":1}
{"type":"file_chunk","path":"src/control/dataset_descriptor.py","chunk_index":0,"line_start":1,"line_end":176,"content":["\"\"\"Dataset Descriptor with TXT and Parquet locations.","","Extends the basic DatasetRecord with information about","raw TXT sources and derived Parquet outputs.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, field","from pathlib import Path","from typing import List, Optional, Dict, Any","","from data.dataset_registry import DatasetRecord","","","@dataclass(frozen=True)","class DatasetDescriptor:","    \"\"\"Extended dataset descriptor with TXT and Parquet information.\"\"\"","    ","    # Core dataset info","    dataset_id: str","    base_record: DatasetRecord","    ","    # TXT source information","    txt_root: str","    txt_required_paths: List[str]","    ","    # Parquet output information","    parquet_root: str","    parquet_expected_paths: List[str]","    ","    # Metadata","    kind: str = \"unknown\"","    notes: List[str] = field(default_factory=list)","    ","    @property","    def symbol(self) -> str:","        \"\"\"Get symbol from base record.\"\"\"","        return self.base_record.symbol","    ","    @property","    def exchange(self) -> str:","        \"\"\"Get exchange from base record.\"\"\"","        return self.base_record.exchange","    ","    @property","    def timeframe(self) -> str:","        \"\"\"Get timeframe from base record.\"\"\"","        return self.base_record.timeframe","    ","    @property","    def path(self) -> str:","        \"\"\"Get path from base record.\"\"\"","        return self.base_record.path","    ","    @property","    def start_date(self) -> str:","        \"\"\"Get start date from base record.\"\"\"","        return self.base_record.start_date.isoformat()","    ","    @property","    def end_date(self) -> str:","        \"\"\"Get end date from base record.\"\"\"","        return self.base_record.end_date.isoformat()","","","def create_descriptor_from_record(record: DatasetRecord) -> DatasetDescriptor:","    \"\"\"Create a DatasetDescriptor from a DatasetRecord.","    ","    This is a placeholder implementation that infers TXT and Parquet","    paths based on the dataset ID and record information.","    ","    In a real system, this would come from a configuration file or","    database lookup.","    \"\"\"","    dataset_id = record.id","    ","    # Infer TXT root and paths based on dataset ID pattern","    # Example: \"CME.MNQ.60m.2020-2024\" -> data/raw/CME/MNQ/*.txt","    parts = dataset_id.split('.')","    if len(parts) >= 2:","        exchange = parts[0]","        symbol = parts[1]","        txt_root = f\"data/raw/{exchange}/{symbol}\"","        txt_required_paths = [","            f\"{txt_root}/daily.txt\",","            f\"{txt_root}/intraday.txt\"","        ]","    else:","        txt_root = f\"data/raw/{dataset_id}\"","        txt_required_paths = [f\"{txt_root}/data.txt\"]","    ","    # Parquet output paths","    # Use outputs/parquet/<dataset_id>/data.parquet","    safe_id = dataset_id.replace('/', '_').replace('\\\\', '_').replace(':', '_')","    parquet_root = f\"outputs/parquet/{safe_id}\"","    parquet_expected_paths = [","        f\"{parquet_root}/data.parquet\"","    ]","    ","    # Determine kind based on timeframe","    timeframe = record.timeframe","    if timeframe.endswith('m'):","        kind = \"intraday\"","    elif timeframe.endswith('D'):","        kind = \"daily\"","    else:","        kind = \"unknown\"","    ","    return DatasetDescriptor(","        dataset_id=dataset_id,","        base_record=record,","        txt_root=txt_root,","        txt_required_paths=txt_required_paths,","        parquet_root=parquet_root,","        parquet_expected_paths=parquet_expected_paths,","        kind=kind,","        notes=[\"Auto-generated descriptor\"]","    )","","","def get_descriptor(dataset_id: str) -> Optional[DatasetDescriptor]:","    \"\"\"Get dataset descriptor by ID.","    ","    Args:","        dataset_id: Dataset ID to look up","        ","    Returns:","        DatasetDescriptor if found, None otherwise","    \"\"\"","    from control.dataset_catalog import describe_dataset","    ","    record = describe_dataset(dataset_id)","    if record is None:","        return None","    ","    return create_descriptor_from_record(record)","","","def list_descriptors() -> List[DatasetDescriptor]:","    \"\"\"List all dataset descriptors.","    ","    Returns:","        List of all DatasetDescriptor objects","    \"\"\"","    from control.dataset_catalog import list_datasets","    ","    records = list_datasets()","    return [create_descriptor_from_record(record) for record in records]","","","# Test function","def test_descriptor() -> None:","    \"\"\"Test the descriptor functionality.\"\"\"","    print(\"Testing DatasetDescriptor...\")","    ","    # Get a sample dataset record","    from control.dataset_catalog import list_datasets","    ","    records = list_datasets()","    if records:","        record = records[0]","        descriptor = create_descriptor_from_record(record)","        ","        print(f\"Dataset ID: {descriptor.dataset_id}\")","        print(f\"TXT root: {descriptor.txt_root}\")","        print(f\"TXT paths: {descriptor.txt_required_paths}\")","        print(f\"Parquet root: {descriptor.parquet_root}\")","        print(f\"Parquet paths: {descriptor.parquet_expected_paths}\")","        print(f\"Kind: {descriptor.kind}\")","    else:","        print(\"No datasets found\")","","","if __name__ == \"__main__\":","    test_descriptor()"]}
{"type":"file_footer","path":"src/control/dataset_descriptor.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/dataset_registry_mutation.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4723,"sha256":"8d0945d299d6fe46ef251bf5bd3bd8537186bced0be3e9d8a47ea674225da29c","total_lines":144,"chunk_count":1}
{"type":"file_chunk","path":"src/control/dataset_registry_mutation.py","chunk_index":0,"line_start":1,"line_end":144,"content":["","\"\"\"","Dataset registry mutation (controlled mutation) for snapshot registration.","","Phase 16.5‑B: Append‑only (or controlled mutation) registry updates.","\"\"\"","","from __future__ import annotations","","import json","from datetime import datetime, timezone","from pathlib import Path","from typing import Optional","","from contracts.data.snapshot_models import SnapshotMetadata","from data.dataset_registry import DatasetIndex, DatasetRecord","","","def _get_dataset_registry_root() -> Path:","    \"\"\"","    Return dataset registry root directory.","","    Environment override:","      - FISHBRO_DATASET_REGISTRY_ROOT (default: outputs/datasets)","    \"\"\"","    import os","    return Path(os.environ.get(\"FISHBRO_DATASET_REGISTRY_ROOT\", \"outputs/datasets\"))","","","def _compute_dataset_id(symbol: str, timeframe: str, normalized_sha256: str) -> str:","    \"\"\"","    Deterministic dataset ID for a snapshot.","","    Format: snapshot_{symbol}_{timeframe}_{normalized_sha256[:12]}","    \"\"\"","    symbol_norm = symbol.replace(\"/\", \"_\").upper()","    tf_norm = timeframe.replace(\"/\", \"_\").lower()","    return f\"snapshot_{symbol_norm}_{tf_norm}_{normalized_sha256[:12]}\"","","","def register_snapshot_as_dataset(","    snapshot_dir: Path,","    registry_root: Optional[Path] = None,",") -> DatasetRecord:","    \"\"\"","    Append‑only registration of a snapshot as a dataset.","","    Args:","        snapshot_dir: Path to snapshot directory (contains manifest.json)","        registry_root: Optional root directory for dataset registry.","                       Defaults to _get_dataset_registry_root().","","    Returns:","        DatasetEntry for the newly registered dataset.","","    Raises:","        FileNotFoundError: If manifest.json missing.","        ValueError: If snapshot already registered.","    \"\"\"","    # Load manifest","    manifest_path = snapshot_dir / \"manifest.json\"","    if not manifest_path.exists():","        raise FileNotFoundError(f\"manifest.json not found in {snapshot_dir}\")","","    manifest_data = json.loads(manifest_path.read_text(encoding=\"utf-8\"))","    meta = SnapshotMetadata.model_validate(manifest_data)","","    # Determine registry path","    if registry_root is None:","        registry_root = _get_dataset_registry_root()","    registry_path = registry_root / \"datasets_index.json\"","","    # Ensure parent directory exists","    registry_path.parent.mkdir(parents=True, exist_ok=True)","","    # Load existing registry or create empty","    if registry_path.exists():","        data = json.loads(registry_path.read_text(encoding=\"utf-8\"))","        existing_index = DatasetIndex.model_validate(data)","    else:","        existing_index = DatasetIndex(","            generated_at=datetime.now(timezone.utc).replace(microsecond=0),","            datasets=[],","        )","","    # Compute deterministic dataset ID","    dataset_id = _compute_dataset_id(meta.symbol, meta.timeframe, meta.normalized_sha256)","","    # Check for duplicate (conflict)","    for rec in existing_index.datasets:","        if rec.id == dataset_id:","            raise ValueError(f\"Snapshot {meta.snapshot_id} already registered as dataset {dataset_id}\")","","    # Build DatasetEntry","    # Use stats for start/end timestamps","    start_date = datetime.fromisoformat(meta.stats.min_timestamp.replace(\"Z\", \"+00:00\")).date()","    end_date = datetime.fromisoformat(meta.stats.max_timestamp.replace(\"Z\", \"+00:00\")).date()","","    # Path relative to datasets root (snapshots/{snapshot_id}/normalized.json)","    rel_path = f\"snapshots/{meta.snapshot_id}/normalized.json\"","","    # Compute fingerprint (SHA256 first 40 chars)","    fp40 = meta.normalized_sha256[:40]","    entry = DatasetRecord(","        id=dataset_id,","        symbol=meta.symbol,","        exchange=meta.symbol.split(\".\")[0] if \".\" in meta.symbol else \"UNKNOWN\",","        timeframe=meta.timeframe,","        path=rel_path,","        start_date=start_date,","        end_date=end_date,","        fingerprint_sha1=fp40,  # Keep for backward compatibility","        fingerprint_sha256_40=fp40,  # New field","        tz_provider=\"UTC\",","        tz_version=\"unknown\",","    )","","    # Append new record","    updated_datasets = existing_index.datasets + [entry]","    # Sort by id to maintain deterministic order","    updated_datasets.sort(key=lambda d: d.id)","","    # Create updated index with new generation timestamp","    updated_index = DatasetIndex(","        generated_at=datetime.now(timezone.utc).replace(microsecond=0),","        datasets=updated_datasets,","    )","","    # Write back atomically (write to temp file then rename)","    temp_path = registry_path.with_suffix(\".tmp\")","    temp_path.write_text(","        json.dumps(","            updated_index.model_dump(mode=\"json\"),","            sort_keys=True,","            indent=2,","            ensure_ascii=False,","        ),","        encoding=\"utf-8\",","    )","    temp_path.replace(registry_path)","","    return entry","",""]}
{"type":"file_footer","path":"src/control/dataset_registry_mutation.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/deploy_package_mc.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8431,"sha256":"14461e81c897d545aa05a02e6d14f809188aca91c417ce95f2e4355ac01df831","total_lines":281,"chunk_count":2}
{"type":"file_chunk","path":"src/control/deploy_package_mc.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","MultiCharts 部署套件產生器","","產生 cost_models.json、DEPLOY_README.md、deploy_manifest.json 等檔案，","並確保 deterministic ordering 與 atomic write。","\"\"\"","","from __future__ import annotations","","import json","import hashlib","import tempfile","import shutil","from pathlib import Path","from typing import Dict, List, Any, Optional","from dataclasses import dataclass, asdict","","from core.slippage_policy import SlippagePolicy","","","@dataclass","class CostModel:","    \"\"\"","    單一商品的成本模型","    \"\"\"","    symbol: str  # 商品符號，例如 \"MNQ\"","    tick_size: float  # tick 大小，例如 0.25","    commission_per_side_usd: float  # 每邊手續費（USD），例如 2.8","    commission_per_side_twd: Optional[float] = None  # 每邊手續費（TWD），例如 20.0（台幣商品）","    ","    def to_dict(self) -> Dict[str, Any]:","        d = {","            \"symbol\": self.symbol,","            \"tick_size\": self.tick_size,","            \"commission_per_side_usd\": self.commission_per_side_usd,","        }","        if self.commission_per_side_twd is not None:","            d[\"commission_per_side_twd\"] = self.commission_per_side_twd","        return d","","","@dataclass","class DeployPackageConfig:","    \"\"\"","    部署套件配置","    \"\"\"","    season: str  # 季節標記，例如 \"2026Q1\"","    selected_strategies: List[str]  # 選中的策略 ID 列表","    outputs_root: Path  # 輸出根目錄","    slippage_policy: SlippagePolicy  # 滑價政策","    cost_models: List[CostModel]  # 成本模型列表","    deploy_notes: Optional[str] = None  # 部署備註","","","def generate_deploy_package(config: DeployPackageConfig) -> Path:","    \"\"\"","    產生 MC 部署套件","","    Args:","        config: 部署配置","","    Returns:","        部署套件目錄路徑","    \"\"\"","    # 建立部署目錄","    deploy_dir = config.outputs_root / f\"mc_deploy_{config.season}\"","    deploy_dir.mkdir(parents=True, exist_ok=True)","    ","    # 1. 產生 cost_models.json","    cost_models_path = deploy_dir / \"cost_models.json\"","    _write_cost_models(cost_models_path, config.cost_models, config.slippage_policy)","    ","    # 2. 產生 DEPLOY_README.md","    readme_path = deploy_dir / \"DEPLOY_README.md\"","    _write_deploy_readme(readme_path, config)","    ","    # 3. 產生 deploy_manifest.json","    manifest_path = deploy_dir / \"deploy_manifest.json\"","    _write_deploy_manifest(manifest_path, deploy_dir, config)","    ","    return deploy_dir","","","def _write_cost_models(","    path: Path,","    cost_models: List[CostModel],","    slippage_policy: SlippagePolicy,",") -> None:","    \"\"\"","    寫入 cost_models.json，包含滑價政策與成本模型","    \"\"\"","    # 建立成本模型字典（按 symbol 排序以確保 deterministic）","    models_dict = {}","    for model in sorted(cost_models, key=lambda m: m.symbol):","        models_dict[model.symbol] = model.to_dict()","    ","    data = {","        \"definition\": slippage_policy.definition,","        \"policy\": {","            \"selection\": slippage_policy.selection_level,","            \"stress\": slippage_policy.stress_level,","            \"mc_execution\": slippage_policy.mc_execution_level,","        },","        \"levels\": slippage_policy.levels,","        \"commission_per_symbol\": models_dict,","        \"tick_size_audit_snapshot\": {","            model.symbol: model.tick_size for model in cost_models","        },","    }","    ","    # 使用 atomic write","    _atomic_write_json(path, data)","","","def _write_deploy_readme(path: Path, config: DeployPackageConfig) -> None:","    \"\"\"","    寫入 DEPLOY_README.md，包含 anti-misconfig signature 段落","    \"\"\"","    content = f\"\"\"# MultiCharts Deployment Package ({config.season})","","## Anti‑Misconfig Signature","","This package has passed the S2 survive gate (selection slippage = {config.slippage_policy.selection_level}).","Recommended MC slippage setting: **{config.slippage_policy.mc_execution_level}**.","Commission and slippage are applied **per side** (definition: \"{config.slippage_policy.definition}\").","","## Checklist","","- [ ] Configured by: FishBroWFS_V2 research pipeline","- [ ] Configured at: {config.season}","- [ ] MC slippage level: {config.slippage_policy.mc_execution_level} ({config.slippage_policy.get_mc_execution_ticks()} ticks)","- [ ] MC commission: see cost_models.json per symbol","- [ ] Tick sizes: audit snapshot included in cost_models.json","- [ ] PLA rule: UNIVERSAL SIGNAL.PLA does NOT receive slippage/commission via Inputs","- [ ] PLA must NOT contain SetCommission/SetSlippage or any hardcoded cost logic","","## Selected Strategies","","{chr(10).join(f\"- {s}\" for s in config.selected_strategies)}","","## Files","","- `cost_models.json` – cost models (slippage levels, commission, tick sizes)","- `deploy_manifest.json` – SHA‑256 hashes for all files + manifest chain","- `DEPLOY_README.md` – this file","","## Notes","","{config.deploy_notes or \"No additional notes.\"}","\"\"\"","    _atomic_write_text(path, content)","","","def _write_deploy_manifest(","    path: Path,","    deploy_dir: Path,","    config: DeployPackageConfig,",") -> None:","    \"\"\"","    寫入 deploy_manifest.json，包含所有檔案的 SHA‑256 雜湊與 manifest chain","    \"\"\"","    # 收集需要雜湊的檔案（排除 manifest 本身）","    files_to_hash = [","        deploy_dir / \"cost_models.json\",","        deploy_dir / \"DEPLOY_README.md\",","    ]","    ","    file_hashes = {}","    for file_path in files_to_hash:","        if file_path.exists():","            file_hashes[file_path.name] = _compute_file_sha256(file_path)","    ","    # 計算 manifest 內容的雜湊（不含 manifest_sha256 欄位）","    manifest_data = {","        \"season\": config.season,","        \"selected_strategies\": config.selected_strategies,","        \"slippage_policy\": {","            \"definition\": config.slippage_policy.definition,","            \"selection_level\": config.slippage_policy.selection_level,","            \"stress_level\": config.slippage_policy.stress_level,","            \"mc_execution_level\": config.slippage_policy.mc_execution_level,","        },","        \"file_hashes\": file_hashes,","        \"manifest_version\": \"v1\",","    }","    ","    # 計算 manifest 雜湊","    manifest_json = json.dumps(manifest_data, sort_keys=True, separators=(\",\", \":\"))","    manifest_sha256 = hashlib.sha256(manifest_json.encode(\"utf-8\")).hexdigest()","    ","    # 加入 manifest_sha256","    manifest_data[\"manifest_sha256\"] = manifest_sha256","    ","    # atomic write","    _atomic_write_json(path, manifest_data)","","","def _atomic_write_json(path: Path, data: Dict[str, Any]) -> None:","    \"\"\""]}
{"type":"file_chunk","path":"src/control/deploy_package_mc.py","chunk_index":1,"line_start":201,"line_end":281,"content":["    Atomic write JSON 檔案（tmp + replace）","    \"\"\"","    # 建立暫存檔案","    with tempfile.NamedTemporaryFile(","        mode=\"w\",","        encoding=\"utf-8\",","        dir=path.parent,","        delete=False,","        suffix=\".tmp\",","    ) as f:","        json.dump(data, f, ensure_ascii=False, sort_keys=True, indent=2)","        temp_path = Path(f.name)","    ","    # 替換目標檔案","    shutil.move(temp_path, path)","","","def _atomic_write_text(path: Path, content: str) -> None:","    \"\"\"","    Atomic write 文字檔案","    \"\"\"","    with tempfile.NamedTemporaryFile(","        mode=\"w\",","        encoding=\"utf-8\",","        dir=path.parent,","        delete=False,","        suffix=\".tmp\",","    ) as f:","        f.write(content)","        temp_path = Path(f.name)","    ","    shutil.move(temp_path, path)","","","def _compute_file_sha256(path: Path) -> str:","    \"\"\"","    計算檔案的 SHA‑256 雜湊","    \"\"\"","    sha256 = hashlib.sha256()","    with open(path, \"rb\") as f:","        for chunk in iter(lambda: f.read(4096), b\"\"):","            sha256.update(chunk)","    return sha256.hexdigest()","","","def validate_pla_template(pla_template_path: Path) -> bool:","    \"\"\"","    驗證 PLA 模板是否包含禁止的關鍵字（SetCommission, SetSlippage 等）","","    Args:","        pla_template_path: PLA 模板檔案路徑","","    Returns:","        bool: 是否通過驗證（True 表示無禁止關鍵字）","","    Raises:","        ValueError: 如果發現禁止關鍵字","    \"\"\"","    if not pla_template_path.exists():","        return True  # 沒有模板，視為通過","    ","    forbidden_keywords = [","        \"SetCommission\",","        \"SetSlippage\",","        \"Commission\",","        \"Slippage\",","        \"Cost\",","        \"Fee\",","    ]","    ","    content = pla_template_path.read_text(encoding=\"utf-8\", errors=\"ignore\")","    for keyword in forbidden_keywords:","        if keyword in content:","            raise ValueError(","                f\"PLA 模板包含禁止關鍵字 '{keyword}'。\"","                \"UNIVERSAL SIGNAL.PLA 不得包含任何硬編碼的成本邏輯。\"","            )","    ","    return True","",""]}
{"type":"file_footer","path":"src/control/deploy_package_mc.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/deploy_txt.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4443,"sha256":"b0ed48c161efa7ee648d1fcb8412ce995e748d0c737a5050fe88ae9c86c04775","total_lines":132,"chunk_count":1}
{"type":"file_chunk","path":"src/control/deploy_txt.py","chunk_index":0,"line_start":1,"line_end":132,"content":["#!/usr/bin/env python3","\"\"\"","Deployment TXT MVP.","","Generates three TXT files for MultiCharts consumption:","- strategy_params.txt: mapping of strategy IDs to parameter sets","- portfolio.txt: portfolio legs (symbol, timeframe, strategy)","- universe.txt: instrument specifications (tick size, multiplier, costs)","","Phase 2: Minimal viable product.","\"\"\"","import sys","from pathlib import Path","from typing import Dict, Any, List","","# Ensure the package root is in sys.path when running as script","if __name__ == \"__main__\":","    sys.path.insert(0, str(Path(__file__).parent.parent.parent))","","from portfolio.spec import PortfolioSpec, PortfolioLeg","","","def write_deployment_txt(","    portfolio_spec: PortfolioSpec,","    universe_spec: Dict[str, Any],","    output_dir: Path,",") -> None:","    \"\"\"","    Write deployment TXT files.","","    Args:","        portfolio_spec: PortfolioSpec instance","        universe_spec: Dictionary mapping instrument symbol to dict with keys:","            tick_size, multiplier, commission_per_side_usd, session_profile","        output_dir: Directory where TXT files will be written","    \"\"\"","    output_dir.mkdir(parents=True, exist_ok=True)","","    # 1. strategy_params.txt","    # Collect unique strategy param combos across legs","    param_sets: Dict[str, Dict[str, float]] = {}","    for leg in portfolio_spec.legs:","        key = f\"{leg.strategy_id}_{leg.strategy_version}\"","        # Use param hash? For now just store params","        param_sets[key] = leg.params","","    with open(output_dir / \"strategy_params.txt\", \"w\", encoding=\"utf-8\") as f:","        f.write(\"# strategy_id,param1=value,param2=value,...\\n\")","        for key, params in param_sets.items():","            param_str = \",\".join(f\"{k}={v}\" for k, v in params.items())","            f.write(f\"{key},{param_str}\\n\")","","    # 2. portfolio.txt","    with open(output_dir / \"portfolio.txt\", \"w\", encoding=\"utf-8\") as f:","        f.write(\"# leg_id,symbol,timeframe_min,strategy_id,strategy_version,enabled\\n\")","        for leg in portfolio_spec.legs:","            f.write(f\"{leg.leg_id},{leg.symbol},{leg.timeframe_min},\"","                    f\"{leg.strategy_id},{leg.strategy_version},{leg.enabled}\\n\")","","    # 3. universe.txt","    with open(output_dir / \"universe.txt\", \"w\", encoding=\"utf-8\") as f:","        f.write(\"# symbol,tick_size,multiplier,commission_per_side_usd,session_profile\\n\")","        for symbol, spec in universe_spec.items():","            tick = spec.get(\"tick_size\", 0.25)","            mult = spec.get(\"multiplier\", 1.0)","            comm = spec.get(\"commission_per_side_usd\", 0.0)","            sess = spec.get(\"session_profile\", \"GLOBEX\")","            f.write(f\"{symbol},{tick},{mult},{comm},{sess}\\n\")","","","def generate_example() -> None:","    \"\"\"Generate example deployment TXT files for testing.\"\"\"","    from portfolio.spec import PortfolioLeg, PortfolioSpec","","    # Example portfolio spec","    legs = [","        PortfolioLeg(","            leg_id=\"mnq_60_sma\",","            symbol=\"CME.MNQ\",","            timeframe_min=60,","            session_profile=\"CME\",","            strategy_id=\"sma_cross\",","            strategy_version=\"v1\",","            params={\"fast_period\": 10.0, \"slow_period\": 20.0},","            enabled=True,","        ),","        PortfolioLeg(","            leg_id=\"mes_60_breakout\",","            symbol=\"CME.MES\",","            timeframe_min=60,","            session_profile=\"CME\",","            strategy_id=\"breakout_channel_v1\",","            strategy_version=\"v1\",","            params={\"channel_period\": 20, \"atr_multiplier\": 2.0},","            enabled=True,","        ),","    ]","    portfolio = PortfolioSpec(","        portfolio_id=\"example_portfolio\",","        version=\"2026Q1\",","        legs=legs,","    )","","    # Example universe spec","    universe = {","        \"CME.MNQ\": {","            \"tick_size\": 0.25,","            \"multiplier\": 2.0,","            \"commission_per_side_usd\": 2.8,","            \"session_profile\": \"CME\",","        },","        \"CME.MES\": {","            \"tick_size\": 0.25,","            \"multiplier\": 5.0,","            \"commission_per_side_usd\": 2.8,","            \"session_profile\": \"CME\",","        },","        \"TWF.MXF\": {","            \"tick_size\": 1.0,","            \"multiplier\": 50.0,","            \"commission_per_side_usd\": 20.0,","            \"session_profile\": \"TAIFEX\",","        },","    }","","    output_dir = Path(\"outputs/deployment_example\")","    write_deployment_txt(portfolio, universe, output_dir)","    print(f\"Example deployment TXT files written to {output_dir}\")","","","if __name__ == \"__main__\":","    generate_example()"]}
{"type":"file_footer","path":"src/control/deploy_txt.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/feature_resolver.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":15947,"sha256":"93f3ad1ecfe8638839cb87f8abe62118475b9d06b8ff1fb405ac760f09118ace","total_lines":527,"chunk_count":3}
{"type":"file_chunk","path":"src/control/feature_resolver.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Feature Dependency Resolver（特徵依賴解析器）","","讓任何 strategy/wfs 在執行前可以：","1. 讀取 strategy 的 feature 需求（declaration）","2. 檢查 shared features cache 是否存在且合約一致","3. 缺少就觸發 BUILD_SHARED features-only（需遵守治理規則）","4. 返回統一的 FeatureBundle（可直接餵給 engine）","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Optional, Dict, Any, Tuple, List","import numpy as np","","from contracts.strategy_features import (","    StrategyFeatureRequirements,","    FeatureRef,",")","from core.feature_bundle import FeatureBundle, FeatureSeries","from control.build_context import BuildContext","from control.features_manifest import (","    features_manifest_path,","    load_features_manifest,",")","from control.features_store import (","    features_path,","    load_features_npz,",")","from control.shared_build import build_shared","","","class FeatureResolutionError(RuntimeError):","    \"\"\"特徵解析錯誤的基底類別\"\"\"","    pass","","","class MissingFeaturesError(FeatureResolutionError):","    \"\"\"缺少特徵錯誤\"\"\"","    def __init__(self, missing: List[Tuple[str, int]]):","        self.missing = missing","        missing_str = \", \".join(f\"{name}@{tf}m\" for name, tf in missing)","        super().__init__(f\"缺少特徵: {missing_str}\")","","","class ManifestMismatchError(FeatureResolutionError):","    \"\"\"Manifest 合約不符錯誤\"\"\"","    pass","","","class BuildNotAllowedError(FeatureResolutionError):","    \"\"\"不允許 build 錯誤\"\"\"","    pass","","","def resolve_features(","    *,","    season: str,","    dataset_id: str,","    requirements: StrategyFeatureRequirements,","    outputs_root: Path = Path(\"outputs\"),","    allow_build: bool = False,","    build_ctx: Optional[BuildContext] = None,",") -> Tuple[FeatureBundle, bool]:","    \"\"\"","    Ensure required features exist in shared cache and load them.","    ","    行為規格（必須精準）：","    1. 找到 features 目錄：outputs/shared/{season}/{dataset_id}/features/","    2. 檢查 features_manifest.json 是否存在","        - 不存在 → missing","    3. 載入 manifest，驗證硬合約：","        - ts_dtype == \"datetime64[s]\"","        - breaks_policy == \"drop\"","    4. 檢查 manifest 是否包含所需 features_{tf}m.npz 檔","    5. 打開 npz，檢查 keys：","        - ts, 以及需求的 feature key","        - ts 對齊檢查（同 tf 同檔）：ts 必須與檔內所有 feature array 同長","    6. 組裝 FeatureBundle 回傳","    ","    若任何缺失：","        - allow_build=False → raise MissingFeaturesError","        - allow_build=True → 需要 build_ctx 存在，否則 raise BuildNotAllowedError","        - 呼叫 build_shared() 進行 build","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        requirements: 策略特徵需求","        outputs_root: 輸出根目錄（預設為專案根目錄下的 outputs/）","        allow_build: 是否允許自動 build","        build_ctx: Build 上下文（僅在 allow_build=True 且需要 build 時使用）","    ","    Returns:","        Tuple[FeatureBundle, bool]：特徵資料包與是否執行了 build 的標記","    ","    Raises:","        MissingFeaturesError: 缺少特徵且不允許 build","        ManifestMismatchError: manifest 合約不符","        BuildNotAllowedError: 允許 build 但缺少 build_ctx","        ValueError: 參數無效","        FileNotFoundError: 檔案不存在且不允許 build","    \"\"\"","    # 參數驗證","    if not season:","        raise ValueError(\"season 不能為空\")","    if not dataset_id:","        raise ValueError(\"dataset_id 不能為空\")","    ","    if not isinstance(outputs_root, Path):","        outputs_root = Path(outputs_root)","    ","    # 1. 檢查 features manifest 是否存在","    manifest_path = features_manifest_path(outputs_root, season, dataset_id)","    ","    if not manifest_path.exists():","        # features cache 完全不存在","        missing_all = [(ref.name, ref.timeframe_min) for ref in requirements.required]","        return _handle_missing_features(","            season=season,","            dataset_id=dataset_id,","            missing=missing_all,","            allow_build=allow_build,","            build_ctx=build_ctx,","            outputs_root=outputs_root,","            requirements=requirements,","        )","    ","    # 2. 載入並驗證 manifest","    try:","        manifest = load_features_manifest(manifest_path)","    except Exception as e:","        raise ManifestMismatchError(f\"無法載入 features manifest: {e}\")","    ","    # 3. 驗證硬合約","    _validate_manifest_contracts(manifest)","    ","    # 4. 檢查所需特徵是否存在","    missing = _check_missing_features(manifest, requirements)","    ","    if missing:","        # 有特徵缺失","        return _handle_missing_features(","            season=season,","            dataset_id=dataset_id,","            missing=missing,","            allow_build=allow_build,","            build_ctx=build_ctx,","            outputs_root=outputs_root,","            requirements=requirements,","        )","    ","    # 5. 載入所有特徵並建立 FeatureBundle","    return _load_feature_bundle(","        season=season,","        dataset_id=dataset_id,","        requirements=requirements,","        manifest=manifest,","        outputs_root=outputs_root,","    )","","","def _validate_manifest_contracts(manifest: Dict[str, Any]) -> None:","    \"\"\"","    驗證 manifest 硬合約","    ","    Raises:","        ManifestMismatchError: 合約不符","    \"\"\"","    # 檢查 ts_dtype","    ts_dtype = manifest.get(\"ts_dtype\")","    if ts_dtype != \"datetime64[s]\":","        raise ManifestMismatchError(","            f\"ts_dtype 必須為 'datetime64[s]'，實際為 {ts_dtype}\"","        )","    ","    # 檢查 breaks_policy","    breaks_policy = manifest.get(\"breaks_policy\")","    if breaks_policy != \"drop\":","        raise ManifestMismatchError(","            f\"breaks_policy 必須為 'drop'，實際為 {breaks_policy}\"","        )","    ","    # 檢查 files 欄位存在","    if \"files\" not in manifest:","        raise ManifestMismatchError(\"manifest 缺少 'files' 欄位\")","    ","    # 檢查 features_specs 欄位存在","    if \"features_specs\" not in manifest:","        raise ManifestMismatchError(\"manifest 缺少 'features_specs' 欄位\")","","","def _check_missing_features(","    manifest: Dict[str, Any],","    requirements: StrategyFeatureRequirements,",") -> List[Tuple[str, int]]:","    \"\"\""]}
{"type":"file_chunk","path":"src/control/feature_resolver.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    檢查 manifest 中缺少哪些特徵","    ","    Args:","        manifest: features manifest 字典","        requirements: 策略特徵需求","    ","    Returns:","        缺少的特徵列表，每個元素為 (name, timeframe)","    \"\"\"","    missing = []","    ","    # 從 manifest 取得可用的特徵規格","    available_specs = manifest.get(\"features_specs\", [])","    available_keys = set()","    ","    for spec in available_specs:","        name = spec.get(\"name\")","        timeframe_min = spec.get(\"timeframe_min\")","        if name and timeframe_min:","            available_keys.add((name, timeframe_min))","    ","    # 檢查必需特徵","    for ref in requirements.required:","        key = (ref.name, ref.timeframe_min)","        if key not in available_keys:","            missing.append(key)","    ","    return missing","","","def _handle_missing_features(","    *,","    season: str,","    dataset_id: str,","    missing: List[Tuple[str, int]],","    allow_build: bool,","    build_ctx: Optional[BuildContext],","    outputs_root: Path,","    requirements: StrategyFeatureRequirements,",") -> Tuple[FeatureBundle, bool]:","    \"\"\"","    處理缺失特徵","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        missing: 缺失的特徵列表","        allow_build: 是否允許自動 build","        build_ctx: Build 上下文","        outputs_root: 輸出根目錄","        requirements: 策略特徵需求","    ","    Returns:","        Tuple[FeatureBundle, bool]：特徵資料包與是否執行了 build 的標記","    ","    Raises:","        MissingFeaturesError: 不允許 build","        BuildNotAllowedError: 允許 build 但缺少 build_ctx","    \"\"\"","    if not allow_build:","        raise MissingFeaturesError(missing)","    ","    if build_ctx is None:","        raise BuildNotAllowedError(","            \"允許 build 但缺少 build_ctx（需要 txt_path 等參數）\"","        )","    ","    # 執行 build","    try:","        # 使用 build_shared 進行 build","        # 注意：這裡我們使用 build_ctx 中的參數，但覆蓋 season 和 dataset_id","        build_kwargs = build_ctx.to_build_shared_kwargs()","        build_kwargs.update({","            \"season\": season,","            \"dataset_id\": dataset_id,","            \"build_bars\": build_ctx.build_bars_if_missing,","            \"build_features\": True,","        })","        ","        report = build_shared(**build_kwargs)","        ","        if not report.get(\"success\"):","            raise FeatureResolutionError(f\"build 失敗: {report}\")","        ","        # build 成功後，重新嘗試解析","        # 遞迴呼叫 resolve_features（但這次不允許 build，避免無限遞迴）","        bundle, _ = resolve_features(","            season=season,","            dataset_id=dataset_id,","            requirements=requirements,","            outputs_root=outputs_root,","            allow_build=False,  # 不允許再次 build","            build_ctx=None,  # 不需要 build_ctx","        )","        # 因為我們執行了 build，所以標記為 True","        return bundle, True","        ","    except Exception as e:","        # 將其他錯誤包裝為 FeatureResolutionError","        raise FeatureResolutionError(f\"build 失敗: {e}\")","","","def _load_feature_bundle(","    *,","    season: str,","    dataset_id: str,","    requirements: StrategyFeatureRequirements,","    manifest: Dict[str, Any],","    outputs_root: Path,",") -> Tuple[FeatureBundle, bool]:","    \"\"\"","    載入特徵並建立 FeatureBundle","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        requirements: 策略特徵需求","        manifest: features manifest 字典","        outputs_root: 輸出根目錄","    ","    Returns:","        Tuple[FeatureBundle, bool]：特徵資料包與是否執行了 build 的標記（此處永遠為 False）","    ","    Raises:","        FeatureResolutionError: 載入失敗","    \"\"\"","    series_dict = {}","    ","    # 載入必需特徵","    for ref in requirements.required:","        key = (ref.name, ref.timeframe_min)","        ","        try:","            series = _load_single_feature_series(","                season=season,","                dataset_id=dataset_id,","                feature_name=ref.name,","                timeframe_min=ref.timeframe_min,","                outputs_root=outputs_root,","                manifest=manifest,","            )","            series_dict[key] = series","        except Exception as e:","            raise FeatureResolutionError(","                f\"無法載入特徵 {ref.name}@{ref.timeframe_min}m: {e}\"","            )","    ","    # 載入可選特徵（如果存在）","    for ref in requirements.optional:","        key = (ref.name, ref.timeframe_min)","        ","        # 檢查特徵是否存在於 manifest","        if _feature_exists_in_manifest(ref.name, ref.timeframe_min, manifest):","            try:","                series = _load_single_feature_series(","                    season=season,","                    dataset_id=dataset_id,","                    feature_name=ref.name,","                    timeframe_min=ref.timeframe_min,","                    outputs_root=outputs_root,","                    manifest=manifest,","                )","                series_dict[key] = series","            except Exception:","                # 可選特徵載入失敗，忽略（不加入 bundle）","                pass","    ","    # 建立 metadata","    meta = {","        \"ts_dtype\": manifest.get(\"ts_dtype\", \"datetime64[s]\"),","        \"breaks_policy\": manifest.get(\"breaks_policy\", \"drop\"),","        \"manifest_sha256\": manifest.get(\"manifest_sha256\"),","        \"mode\": manifest.get(\"mode\"),","        \"season\": season,","        \"dataset_id\": dataset_id,","        \"files_sha256\": manifest.get(\"files\", {}),","    }","    ","    # 建立 FeatureBundle","    try:","        bundle = FeatureBundle(","            dataset_id=dataset_id,","            season=season,","            series=series_dict,","            meta=meta,","        )","        return bundle, False","    except Exception as e:","        raise FeatureResolutionError(f\"無法建立 FeatureBundle: {e}\")","","","def _feature_exists_in_manifest(","    feature_name: str,","    timeframe_min: int,","    manifest: Dict[str, Any],",") -> bool:","    \"\"\"","    檢查特徵是否存在於 manifest 中","    ","    Args:"]}
{"type":"file_chunk","path":"src/control/feature_resolver.py","chunk_index":2,"line_start":401,"line_end":527,"content":["        feature_name: 特徵名稱","        timeframe_min: timeframe 分鐘數","        manifest: features manifest 字典","    ","    Returns:","        bool","    \"\"\"","    specs = manifest.get(\"features_specs\", [])","    for spec in specs:","        if (spec.get(\"name\") == feature_name and ","            spec.get(\"timeframe_min\") == timeframe_min):","            return True","    return False","","","def _load_single_feature_series(","    *,","    season: str,","    dataset_id: str,","    feature_name: str,","    timeframe_min: int,","    outputs_root: Path,","    manifest: Dict[str, Any],",") -> FeatureSeries:","    \"\"\"","    載入單一特徵序列","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        feature_name: 特徵名稱","        timeframe_min: timeframe 分鐘數","        outputs_root: 輸出根目錄","        manifest: features manifest 字典（用於驗證）","    ","    Returns:","        FeatureSeries 實例","    ","    Raises:","        FeatureResolutionError: 載入失敗","    \"\"\"","    # 1. 載入 features NPZ 檔案","    feat_path = features_path(outputs_root, season, dataset_id, timeframe_min)","    ","    if not feat_path.exists():","        raise FeatureResolutionError(","            f\"features 檔案不存在: {feat_path}\"","        )","    ","    try:","        data = load_features_npz(feat_path)","    except Exception as e:","        raise FeatureResolutionError(f\"無法載入 features NPZ: {e}\")","    ","    # 2. 檢查必要 keys","    required_keys = {\"ts\", feature_name}","    missing_keys = required_keys - set(data.keys())","    if missing_keys:","        raise FeatureResolutionError(","            f\"features NPZ 缺少必要 keys: {missing_keys}，現有 keys: {list(data.keys())}\"","        )","    ","    # 3. 驗證 ts dtype","    ts = data[\"ts\"]","    if not np.issubdtype(ts.dtype, np.datetime64):","        raise FeatureResolutionError(","            f\"ts dtype 必須為 datetime64，實際為 {ts.dtype}\"","        )","    ","    # 4. 驗證特徵值 dtype","    values = data[feature_name]","    if not np.issubdtype(values.dtype, np.floating):","        # 嘗試轉換為 float64","        try:","            values = values.astype(np.float64)","        except Exception as e:","            raise FeatureResolutionError(","                f\"特徵值無法轉換為浮點數: {e}，dtype: {values.dtype}\"","            )","    ","    # 5. 驗證長度一致","    if len(ts) != len(values):","        raise FeatureResolutionError(","            f\"ts 與特徵值長度不一致: ts={len(ts)}, {feature_name}={len(values)}\"","        )","    ","    # 6. 建立 FeatureSeries","    try:","        return FeatureSeries(","            ts=ts,","            values=values,","            name=feature_name,","            timeframe_min=timeframe_min,","        )","    except Exception as e:","        raise FeatureResolutionError(f\"無法建立 FeatureSeries: {e}\")","","","# Cache invalidation functions for reload service","def invalidate_feature_cache() -> bool:","    \"\"\"Invalidate feature resolver cache.","    ","    Returns:","        True if successful, False otherwise","    \"\"\"","    try:","        # Currently there's no persistent cache in this module","        # This function exists for API compatibility","        return True","    except Exception:","        return False","","","def reload_feature_registry() -> bool:","    \"\"\"Reload feature registry.","    ","    Returns:","        True if successful, False otherwise","    \"\"\"","    try:","        # Currently there's no registry to reload","        # This function exists for API compatibility","        return True","    except Exception:","        return False","",""]}
{"type":"file_footer","path":"src/control/feature_resolver.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/control/features_manifest.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6446,"sha256":"c3ea6bbd71a2f14fa175da64fd85263e9cb0b5996ffef2c13212a896f735d265","total_lines":211,"chunk_count":2}
{"type":"file_chunk","path":"src/control/features_manifest.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Features Manifest 寫入工具","","提供 deterministic JSON + self-hash manifest_sha256 + atomic write。","包含 features specs dump 與 lookback rewind 資訊。","\"\"\"","","from __future__ import annotations","","import hashlib","import json","import tempfile","from pathlib import Path","from typing import Any, Dict, Optional","from datetime import datetime","","from contracts.dimensions import canonical_json","from contracts.features import FeatureRegistry, FeatureSpec","","","def write_features_manifest(payload: Dict[str, Any], path: Path) -> Dict[str, Any]:","    \"\"\"","    Deterministic JSON + self-hash manifest_sha256 + atomic write.","    ","    行為規格：","    1. 建立暫存檔案（.json.tmp）","    2. 計算 payload 的 SHA256 hash（排除 manifest_sha256 欄位）","    3. 將 hash 加入 payload 作為 manifest_sha256 欄位","    4. 使用 canonical_json 寫入暫存檔案（確保排序一致）","    5. atomic replace 到目標路徑","    6. 如果寫入失敗，清理暫存檔案","    ","    Args:","        payload: manifest 資料字典（不含 manifest_sha256）","        path: 目標檔案路徑","        ","    Returns:","        最終的 manifest 字典（包含 manifest_sha256 欄位）","        ","    Raises:","        IOError: 寫入失敗","    \"\"\"","    # 確保目錄存在","    path.parent.mkdir(parents=True, exist_ok=True)","    ","    # 建立暫存檔案路徑","    temp_path = path.with_suffix(path.suffix + \".tmp\")","    ","    try:","        # 計算 payload 的 SHA256 hash（排除可能的 manifest_sha256 欄位）","        payload_without_hash = {k: v for k, v in payload.items() if k != \"manifest_sha256\"}","        json_str = canonical_json(payload_without_hash)","        manifest_sha256 = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","        ","        # 建立最終 payload（包含 hash）","        final_payload = {**payload_without_hash, \"manifest_sha256\": manifest_sha256}","        ","        # 使用 canonical_json 寫入暫存檔案","        final_json = canonical_json(final_payload)","        temp_path.write_text(final_json, encoding=\"utf-8\")","        ","        # atomic replace","        temp_path.replace(path)","        ","        return final_payload","        ","    except Exception as e:","        # 清理暫存檔案","        if temp_path.exists():","            try:","                temp_path.unlink()","            except OSError:","                pass","        raise IOError(f\"寫入 features manifest 失敗 {path}: {e}\")","    ","    finally:","        # 確保暫存檔案被清理（如果 replace 成功，temp_path 已不存在）","        if temp_path.exists():","            try:","                temp_path.unlink()","            except OSError:","                pass","","","def load_features_manifest(path: Path) -> Dict[str, Any]:","    \"\"\"","    載入 features manifest 並驗證 hash","    ","    Args:","        path: manifest 檔案路徑","        ","    Returns:","        manifest 字典","        ","    Raises:","        FileNotFoundError: 檔案不存在","        ValueError: JSON 解析失敗或 hash 驗證失敗","    \"\"\"","    if not path.exists():","        raise FileNotFoundError(f\"features manifest 檔案不存在: {path}\")","    ","    try:","        content = path.read_text(encoding=\"utf-8\")","    except (IOError, OSError) as e:","        raise ValueError(f\"無法讀取 features manifest 檔案 {path}: {e}\")","    ","    try:","        data = json.loads(content)","    except json.JSONDecodeError as e:","        raise ValueError(f\"features manifest JSON 解析失敗 {path}: {e}\")","    ","    # 驗證 manifest_sha256","    if \"manifest_sha256\" not in data:","        raise ValueError(f\"features manifest 缺少 manifest_sha256 欄位: {path}\")","    ","    # 計算實際 hash（排除 manifest_sha256 欄位）","    data_without_hash = {k: v for k, v in data.items() if k != \"manifest_sha256\"}","    json_str = canonical_json(data_without_hash)","    expected_hash = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","    ","    if data[\"manifest_sha256\"] != expected_hash:","        raise ValueError(f\"features manifest hash 驗證失敗: 預期 {expected_hash}，實際 {data['manifest_sha256']}\")","    ","    return data","","","def features_manifest_path(outputs_root: Path, season: str, dataset_id: str) -> Path:","    \"\"\"","    取得 features manifest 檔案路徑","    ","    建議位置：outputs/shared/{season}/{dataset_id}/features/features_manifest.json","    ","    Args:","        outputs_root: 輸出根目錄","        season: 季節標記","        dataset_id: 資料集 ID","        ","    Returns:","        檔案路徑","    \"\"\"","    # 建立路徑","    path = outputs_root / \"shared\" / season / dataset_id / \"features\" / \"features_manifest.json\"","    return path","","","def build_features_manifest_data(","    *,","    season: str,","    dataset_id: str,","    mode: str,","    ts_dtype: str,","    breaks_policy: str,","    features_specs: list[Dict[str, Any]],","    append_only: bool,","    append_range: Optional[Dict[str, str]],","    lookback_rewind_by_tf: Dict[str, str],","    files_sha256: Dict[str, str],",") -> Dict[str, Any]:","    \"\"\"","    建立 features manifest 資料","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        mode: 建置模式（\"FULL\" 或 \"INCREMENTAL\"）","        ts_dtype: 時間戳記 dtype（必須為 \"datetime64[s]\"）","        breaks_policy: break 處理策略（必須為 \"drop\"）","        features_specs: 特徵規格列表（從 FeatureRegistry 轉換）","        append_only: 是否為 append-only 增量","        append_range: 增量範圍（開始日、結束日）","        lookback_rewind_by_tf: 每個 timeframe 的 lookback rewind 開始時間","        files_sha256: 檔案 SHA256 字典","        ","    Returns:","        manifest 資料字典（不含 manifest_sha256）","    \"\"\"","    manifest = {","        \"season\": season,","        \"dataset_id\": dataset_id,","        \"mode\": mode,","        \"ts_dtype\": ts_dtype,","        \"breaks_policy\": breaks_policy,","        \"features_specs\": features_specs,","        \"append_only\": append_only,","        \"append_range\": append_range,","        \"lookback_rewind_by_tf\": lookback_rewind_by_tf,","        \"files\": files_sha256,","    }","    ","    return manifest","","","def feature_spec_to_dict(spec: FeatureSpec) -> Dict[str, Any]:","    \"\"\"","    將 FeatureSpec 轉換為可序列化的字典","    ","    Args:","        spec: 特徵規格","        "]}
{"type":"file_chunk","path":"src/control/features_manifest.py","chunk_index":1,"line_start":201,"line_end":211,"content":["    Returns:","        可序列化的字典","    \"\"\"","    return {","        \"name\": spec.name,","        \"timeframe_min\": spec.timeframe_min,","        \"lookback_bars\": spec.lookback_bars,","        \"params\": spec.params,","    }","",""]}
{"type":"file_footer","path":"src/control/features_manifest.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/features_store.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4826,"sha256":"d0864798fd68747cd8f2d069181e3eae4aaaa7da00ab086e620963376ddaa877","total_lines":188,"chunk_count":1}
{"type":"file_chunk","path":"src/control/features_store.py","chunk_index":0,"line_start":1,"line_end":188,"content":["","\"\"\"","Feature Store（NPZ atomic + SHA256）","","提供 features cache 的 I/O 工具，重用 bars_store 的 atomic write 與 SHA256 計算。","\"\"\"","","from __future__ import annotations","","from pathlib import Path","from typing import Dict, Literal, Optional","import numpy as np","","from control.bars_store import (","    write_npz_atomic,","    load_npz,","    sha256_file,","    canonical_json,",")","","Timeframe = Literal[15, 30, 60, 120, 240]","","","def features_dir(outputs_root: Path, season: str, dataset_id: str) -> Path:","    \"\"\"","    取得 features 目錄路徑","    ","    建議位置：outputs/shared/{season}/{dataset_id}/features/","    ","    Args:","        outputs_root: 輸出根目錄","        season: 季節標記，例如 \"2026Q1\"","        dataset_id: 資料集 ID","        ","    Returns:","        目錄路徑","    \"\"\"","    # 建立路徑","    path = outputs_root / \"shared\" / season / dataset_id / \"features\"","    return path","","","def features_path(","    outputs_root: Path,","    season: str,","    dataset_id: str,","    tf_min: Timeframe,",") -> Path:","    \"\"\"","    取得 features 檔案路徑","    ","    建議位置：outputs/shared/{season}/{dataset_id}/features/features_{tf_min}m.npz","    ","    Args:","        outputs_root: 輸出根目錄","        season: 季節標記","        dataset_id: 資料集 ID","        tf_min: timeframe 分鐘數（15, 30, 60, 120, 240）","        ","    Returns:","        檔案路徑","    \"\"\"","    dir_path = features_dir(outputs_root, season, dataset_id)","    return dir_path / f\"features_{tf_min}m.npz\"","","","def write_features_npz_atomic(","    path: Path,","    features_dict: Dict[str, np.ndarray],",") -> None:","    \"\"\"","    Write features NPZ via tmp + replace. Deterministic keys order.","    ","    重用 bars_store.write_npz_atomic 但確保 keys 順序固定：","    ts, atr_14, ret_z_200, session_vwap","    ","    Args:","        path: 目標檔案路徑","        features_dict: 特徵字典，必須包含所有必要 keys","        ","    Raises:","        ValueError: 缺少必要 keys","        IOError: 寫入失敗","    \"\"\"","    # 驗證必要 keys","    required_keys = {\"ts\", \"atr_14\", \"ret_z_200\", \"session_vwap\"}","    missing_keys = required_keys - set(features_dict.keys())","    if missing_keys:","        raise ValueError(f\"features_dict 缺少必要 keys: {missing_keys}\")","    ","    # 確保 ts 的 dtype 是 datetime64[s]","    ts = features_dict[\"ts\"]","    if not np.issubdtype(ts.dtype, np.datetime64):","        raise ValueError(f\"ts 的 dtype 必須是 datetime64，實際為 {ts.dtype}\")","    ","    # 確保所有特徵陣列都是 float64","    for key in [\"atr_14\", \"ret_z_200\", \"session_vwap\"]:","        arr = features_dict[key]","        if not np.issubdtype(arr.dtype, np.floating):","            raise ValueError(f\"{key} 的 dtype 必須是浮點數，實際為 {arr.dtype}\")","    ","    # 使用 bars_store 的 write_npz_atomic","    write_npz_atomic(path, features_dict)","","","def load_features_npz(path: Path) -> Dict[str, np.ndarray]:","    \"\"\"","    載入 features NPZ 檔案","    ","    Args:","        path: NPZ 檔案路徑","        ","    Returns:","        特徵字典","        ","    Raises:","        FileNotFoundError: 檔案不存在","        ValueError: 檔案格式錯誤或缺少必要 keys","    \"\"\"","    # 使用 bars_store 的 load_npz","    data = load_npz(path)","    ","    # 驗證必要 keys","    required_keys = {\"ts\", \"atr_14\", \"ret_z_200\", \"session_vwap\"}","    missing_keys = required_keys - set(data.keys())","    if missing_keys:","        raise ValueError(f\"載入的 NPZ 缺少必要 keys: {missing_keys}\")","    ","    return data","","","def sha256_features_file(","    outputs_root: Path,","    season: str,","    dataset_id: str,","    tf_min: Timeframe,",") -> str:","    \"\"\"","    計算 features NPZ 檔案的 SHA256 hash","    ","    Args:","        outputs_root: 輸出根目錄","        season: 季節標記","        dataset_id: 資料集 ID","        tf_min: timeframe 分鐘數","        ","    Returns:","        SHA256 hex digest（小寫）","        ","    Raises:","        FileNotFoundError: 檔案不存在","        IOError: 讀取失敗","    \"\"\"","    path = features_path(outputs_root, season, dataset_id, tf_min)","    return sha256_file(path)","","","def compute_features_sha256_dict(","    outputs_root: Path,","    season: str,","    dataset_id: str,","    tfs: list[Timeframe] = [15, 30, 60, 120, 240],",") -> Dict[str, str]:","    \"\"\"","    計算所有 timeframe 的 features NPZ 檔案 SHA256 hash","    ","    Args:","        outputs_root: 輸出根目錄","        season: 季節標記","        dataset_id: 資料集 ID","        tfs: timeframe 列表","        ","    Returns:","        字典：filename -> sha256","    \"\"\"","    result = {}","    ","    for tf in tfs:","        try:","            sha256 = sha256_features_file(outputs_root, season, dataset_id, tf)","            result[f\"features_{tf}m.npz\"] = sha256","        except FileNotFoundError:","            # 檔案不存在，跳過","            continue","    ","    return result","",""]}
{"type":"file_footer","path":"src/control/features_store.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/fingerprint_cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8253,"sha256":"901cc3bfa43ad5ab741d6d8bf89dbaee36b41f64d343c449d0b9e2f93a3cabe1","total_lines":282,"chunk_count":2}
{"type":"file_chunk","path":"src/control/fingerprint_cli.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Fingerprint scan-only diff CLI","","提供 scan-only 命令，用於比較 TXT 檔案與現有指紋索引，產生 diff 報告。","此命令純粹掃描與比較，不觸發任何 build 或 WFS 行為。","\"\"\"","","from __future__ import annotations","","import argparse","import json","import sys","from pathlib import Path","from typing import Optional","","from contracts.fingerprint import FingerprintIndex","from control.fingerprint_store import (","    fingerprint_index_path,","    load_fingerprint_index_if_exists,","    write_fingerprint_index,",")","from core.fingerprint import (","    build_fingerprint_index_from_raw_ingest,","    compare_fingerprint_indices,",")","from data.raw_ingest import ingest_raw_txt","","","def scan_fingerprint(","    season: str,","    dataset_id: str,","    txt_path: Path,","    outputs_root: Optional[Path] = None,","    save_new_index: bool = False,","    verbose: bool = False,",") -> dict:","    \"\"\"","    掃描 TXT 檔案並與現有指紋索引比較","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        txt_path: TXT 檔案路徑","        outputs_root: 輸出根目錄","        save_new_index: 是否儲存新的指紋索引","        verbose: 是否輸出詳細資訊","    ","    Returns:","        diff 報告字典","    \"\"\"","    # 檢查檔案是否存在","    if not txt_path.exists():","        raise FileNotFoundError(f\"TXT 檔案不存在: {txt_path}\")","    ","    # 載入現有指紋索引（如果存在）","    index_path = fingerprint_index_path(season, dataset_id, outputs_root)","    old_index = load_fingerprint_index_if_exists(index_path)","    ","    if verbose:","        if old_index:","            print(f\"找到現有指紋索引: {index_path}\")","            print(f\"  範圍: {old_index.range_start} 到 {old_index.range_end}\")","            print(f\"  天數: {len(old_index.day_hashes)}\")","        else:","            print(f\"沒有現有指紋索引: {index_path}\")","    ","    # 讀取 TXT 檔案並建立新的指紋索引","    if verbose:","        print(f\"讀取 TXT 檔案: {txt_path}\")","    ","    raw_result = ingest_raw_txt(txt_path)","    ","    if verbose:","        print(f\"  讀取 {raw_result.rows} 行\")","        if raw_result.policy.normalized_24h:","            print(f\"  已正規化 24:00:00 時間\")","    ","    # 建立新的指紋索引","    new_index = build_fingerprint_index_from_raw_ingest(","        dataset_id=dataset_id,","        raw_ingest_result=raw_result,","        build_notes=f\"scanned from {txt_path.name}\",","    )","    ","    if verbose:","        print(f\"建立新的指紋索引:\")","        print(f\"  範圍: {new_index.range_start} 到 {new_index.range_end}\")","        print(f\"  天數: {len(new_index.day_hashes)}\")","        print(f\"  index_sha256: {new_index.index_sha256[:16]}...\")","    ","    # 比較索引","    diff_report = compare_fingerprint_indices(old_index, new_index)","    ","    # 如果需要，儲存新的指紋索引","    if save_new_index:","        if verbose:","            print(f\"儲存新的指紋索引到: {index_path}\")","        ","        write_fingerprint_index(new_index, index_path)","        diff_report[\"new_index_saved\"] = True","        diff_report[\"new_index_path\"] = str(index_path)","    else:","        diff_report[\"new_index_saved\"] = False","    ","    return diff_report","","","def format_diff_report(diff_report: dict, verbose: bool = False) -> str:","    \"\"\"","    格式化 diff 報告","    ","    Args:","        diff_report: diff 報告字典","        verbose: 是否輸出詳細資訊","    ","    Returns:","        格式化字串","    \"\"\"","    lines = []","    ","    # 基本資訊","    lines.append(\"=== Fingerprint Scan Report ===\")","    ","    if diff_report.get(\"is_new\", False):","        lines.append(\"狀態: 全新資料集（無現有指紋索引）\")","    elif diff_report.get(\"no_change\", False):","        lines.append(\"狀態: 無變更（指紋完全相同）\")","    elif diff_report.get(\"append_only\", False):","        lines.append(\"狀態: 僅尾部新增（可增量）\")","    else:","        lines.append(\"狀態: 資料變更（需全量重算）\")","    ","    lines.append(\"\")","    ","    # 範圍資訊","    if diff_report[\"old_range_start\"]:","        lines.append(f\"舊範圍: {diff_report['old_range_start']} 到 {diff_report['old_range_end']}\")","    lines.append(f\"新範圍: {diff_report['new_range_start']} 到 {diff_report['new_range_end']}\")","    ","    # 變更資訊","    if diff_report.get(\"append_only\", False):","        append_range = diff_report.get(\"append_range\")","        if append_range:","            lines.append(f\"新增範圍: {append_range[0]} 到 {append_range[1]}\")","    ","    if diff_report.get(\"earliest_changed_day\"):","        lines.append(f\"最早變更日: {diff_report['earliest_changed_day']}\")","    ","    # 儲存狀態","    if diff_report.get(\"new_index_saved\", False):","        lines.append(f\"新指紋索引已儲存: {diff_report.get('new_index_path', '')}\")","    ","    # 詳細輸出","    if verbose:","        lines.append(\"\")","        lines.append(\"--- 詳細報告 ---\")","        lines.append(json.dumps(diff_report, indent=2, ensure_ascii=False))","    ","    return \"\\n\".join(lines)","","","def main() -> int:","    \"\"\"","    CLI 主函數","    ","    命令：fishbro fingerprint scan --season 2026Q1 --dataset-id XXX --txt-path /path/to/file.txt","    \"\"\"","    parser = argparse.ArgumentParser(","        description=\"掃描 TXT 檔案並與指紋索引比較（scan-only diff）\",","        formatter_class=argparse.ArgumentDefaultsHelpFormatter,","    )","    ","    # 子命令（未來可擴展）","    subparsers = parser.add_subparsers(dest=\"command\", help=\"命令\")","    ","    # scan 命令","    scan_parser = subparsers.add_parser(","        \"scan\",","        help=\"掃描 TXT 檔案並比較指紋\"","    )","    ","    scan_parser.add_argument(","        \"--season\",","        required=True,","        help=\"季節標記，例如 '2026Q1'\"","    )","    ","    scan_parser.add_argument(","        \"--dataset-id\",","        required=True,","        help=\"資料集 ID，例如 'CME.MNQ.60m.2020-2024'\"","    )","    ","    scan_parser.add_argument(","        \"--txt-path\",","        type=Path,","        required=True,","        help=\"TXT 檔案路徑\"","    )"]}
{"type":"file_chunk","path":"src/control/fingerprint_cli.py","chunk_index":1,"line_start":201,"line_end":282,"content":["    ","    scan_parser.add_argument(","        \"--outputs-root\",","        type=Path,","        default=Path(\"outputs\"),","        help=\"輸出根目錄\"","    )","    ","    scan_parser.add_argument(","        \"--save\",","        action=\"store_true\",","        help=\"儲存新的指紋索引（否則僅比較）\"","    )","    ","    scan_parser.add_argument(","        \"--verbose\",","        action=\"store_true\",","        help=\"輸出詳細資訊\"","    )","    ","    scan_parser.add_argument(","        \"--json\",","        action=\"store_true\",","        help=\"以 JSON 格式輸出報告\"","    )","    ","    # 如果沒有提供命令，顯示幫助","    if len(sys.argv) == 1:","        parser.print_help()","        return 0","    ","    args = parser.parse_args()","    ","    if args.command != \"scan\":","        print(f\"錯誤: 不支援的命令: {args.command}\", file=sys.stderr)","        parser.print_help()","        return 1","    ","    try:","        # 執行掃描","        diff_report = scan_fingerprint(","            season=args.season,","            dataset_id=args.dataset_id,","            txt_path=args.txt_path,","            outputs_root=args.outputs_root,","            save_new_index=args.save,","            verbose=args.verbose,","        )","        ","        # 輸出結果","        if args.json:","            print(json.dumps(diff_report, indent=2, ensure_ascii=False))","        else:","            report_text = format_diff_report(diff_report, args.verbose)","            print(report_text)","        ","        # 根據結果返回適當的退出碼","        if diff_report.get(\"no_change\", False):","            return 0  # 無變更","        elif diff_report.get(\"append_only\", False):","            return 10  # 可增量（使用非零值表示需要處理）","        else:","            return 20  # 需全量重算","        ","    except FileNotFoundError as e:","        print(f\"錯誤: 檔案不存在 - {e}\", file=sys.stderr)","        return 1","    except ValueError as e:","        print(f\"錯誤: 資料驗證失敗 - {e}\", file=sys.stderr)","        return 1","    except Exception as e:","        print(f\"錯誤: 執行失敗 - {e}\", file=sys.stderr)","        if args.verbose:","            import traceback","            traceback.print_exc()","        return 1","","","if __name__ == \"__main__\":","    sys.exit(main())","",""]}
{"type":"file_footer","path":"src/control/fingerprint_cli.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/fingerprint_store.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5678,"sha256":"0fdea3434a3df203a444d738067424449ce78b9db4f1f6c4a9b5f9ab9e20f381","total_lines":229,"chunk_count":2}
{"type":"file_chunk","path":"src/control/fingerprint_store.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Fingerprint index 儲存與讀取","","提供 atomic write 與 deterministic JSON 序列化。","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Optional","","from contracts.fingerprint import FingerprintIndex","from contracts.dimensions import canonical_json","","","def fingerprint_index_path(","    season: str,","    dataset_id: str,","    outputs_root: Optional[Path] = None",") -> Path:","    \"\"\"","    取得指紋索引檔案路徑","    ","    建議位置：outputs/fingerprints/{season}/{dataset_id}/fingerprint_index.json","    ","    Args:","        season: 季節標記，例如 \"2026Q1\"","        dataset_id: 資料集 ID","        outputs_root: 輸出根目錄，預設為專案根目錄下的 outputs/","    ","    Returns:","        檔案路徑","    \"\"\"","    if outputs_root is None:","        # 從專案根目錄開始","        project_root = Path(__file__).parent.parent.parent","        outputs_root = project_root / \"outputs\"","    ","    # 建立路徑","    path = outputs_root / \"fingerprints\" / season / dataset_id / \"fingerprint_index.json\"","    return path","","","def write_fingerprint_index(","    index: FingerprintIndex,","    path: Path,","    *,","    ensure_parents: bool = True",") -> None:","    \"\"\"","    寫入指紋索引（原子寫入）","    ","    使用 tmp + replace 模式確保 atomic write。","    ","    Args:","        index: 要寫入的 FingerprintIndex","        path: 目標檔案路徑","        ensure_parents: 是否建立父目錄","    ","    Raises:","        IOError: 寫入失敗","    \"\"\"","    if ensure_parents:","        path.parent.mkdir(parents=True, exist_ok=True)","    ","    # 轉換為字典","    data = index.model_dump()","    ","    # 使用 canonical_json 確保 deterministic 輸出","    json_str = canonical_json(data)","    ","    # 原子寫入：先寫到暫存檔案，再移動","    temp_path = path.with_suffix(\".json.tmp\")","    ","    try:","        # 寫入暫存檔案","        temp_path.write_text(json_str, encoding=\"utf-8\")","        ","        # 移動到目標位置（原子操作）","        temp_path.replace(path)","        ","    except Exception as e:","        # 清理暫存檔案","        if temp_path.exists():","            try:","                temp_path.unlink()","            except:","                pass","        ","        raise IOError(f\"寫入指紋索引失敗 {path}: {e}\")","    ","    # 驗證寫入的檔案可以正確讀回","    try:","        loaded = load_fingerprint_index(path)","        if loaded.index_sha256 != index.index_sha256:","            raise IOError(f\"寫入後驗證失敗: hash 不匹配\")","    except Exception as e:","        # 如果驗證失敗，刪除檔案","        if path.exists():","            try:","                path.unlink()","            except:","                pass","        raise IOError(f\"指紋索引驗證失敗 {path}: {e}\")","","","def load_fingerprint_index(path: Path) -> FingerprintIndex:","    \"\"\"","    載入指紋索引","    ","    Args:","        path: 檔案路徑","    ","    Returns:","        FingerprintIndex","    ","    Raises:","        FileNotFoundError: 檔案不存在","        ValueError: JSON 解析失敗或 schema 驗證失敗","    \"\"\"","    if not path.exists():","        raise FileNotFoundError(f\"指紋索引檔案不存在: {path}\")","    ","    try:","        content = path.read_text(encoding=\"utf-8\")","    except (IOError, OSError) as e:","        raise ValueError(f\"無法讀取指紋索引檔案 {path}: {e}\")","    ","    try:","        data = json.loads(content)","    except json.JSONDecodeError as e:","        raise ValueError(f\"指紋索引 JSON 解析失敗 {path}: {e}\")","    ","    try:","        return FingerprintIndex(**data)","    except Exception as e:","        raise ValueError(f\"指紋索引 schema 驗證失敗 {path}: {e}\")","","","def load_fingerprint_index_if_exists(path: Path) -> Optional[FingerprintIndex]:","    \"\"\"","    載入指紋索引（如果存在）","    ","    Args:","        path: 檔案路徑","    ","    Returns:","        FingerprintIndex 或 None（如果檔案不存在）","    ","    Raises:","        ValueError: JSON 解析失敗或 schema 驗證失敗","    \"\"\"","    if not path.exists():","        return None","    ","    return load_fingerprint_index(path)","","","def delete_fingerprint_index(path: Path) -> None:","    \"\"\"","    刪除指紋索引檔案","    ","    Args:","        path: 檔案路徑","    \"\"\"","    if path.exists():","        path.unlink()","","","def list_fingerprint_indices(","    season: str,","    outputs_root: Optional[Path] = None",") -> list[tuple[str, Path]]:","    \"\"\"","    列出指定季節的所有指紋索引","    ","    Args:","        season: 季節標記","        outputs_root: 輸出根目錄","    ","    Returns:","        (dataset_id, path) 的列表","    \"\"\"","    if outputs_root is None:","        project_root = Path(__file__).parent.parent.parent","        outputs_root = project_root / \"outputs\"","    ","    season_dir = outputs_root / \"fingerprints\" / season","    ","    if not season_dir.exists():","        return []","    ","    indices = []","    ","    for dataset_dir in season_dir.iterdir():","        if dataset_dir.is_dir():","            index_path = dataset_dir / \"fingerprint_index.json\"","            if index_path.exists():"]}
{"type":"file_chunk","path":"src/control/fingerprint_store.py","chunk_index":1,"line_start":201,"line_end":229,"content":["                indices.append((dataset_dir.name, index_path))","    ","    # 按 dataset_id 排序","    indices.sort(key=lambda x: x[0])","    ","    return indices","","","def ensure_fingerprint_directory(","    season: str,","    dataset_id: str,","    outputs_root: Optional[Path] = None",") -> Path:","    \"\"\"","    確保指紋索引目錄存在","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        outputs_root: 輸出根目錄","    ","    Returns:","        目錄路徑","    \"\"\"","    path = fingerprint_index_path(season, dataset_id, outputs_root)","    path.parent.mkdir(parents=True, exist_ok=True)","    return path.parent","",""]}
{"type":"file_footer","path":"src/control/fingerprint_store.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/governance.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6642,"sha256":"75dbd01522d4f9280387c2c3d37f3f24e3eee270f1dacbc9b7b3716504f1548a","total_lines":211,"chunk_count":2}
{"type":"file_chunk","path":"src/control/governance.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Batch metadata and governance for Phase 14.","","Season/tags/note/frozen metadata with immutable rules.","","CRITICAL CONTRACTS:","- Metadata MUST live under: artifacts/{batch_id}/metadata.json","  (so a batch folder is fully portable for audit/replay/archive).","- Writes MUST be atomic (tmp + replace) to avoid corrupt JSON on crash.","- Tag handling MUST be deterministic (dedupe + sort).","- Corrupted metadata MUST NOT be silently treated as \"not found\".","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass, field","from datetime import datetime, timezone","from pathlib import Path","from typing import Optional","","from control.artifacts import write_json_atomic","","","def _utc_now_iso() -> str:","    # Seconds precision, UTC, Z suffix","    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")","","","@dataclass","class BatchMetadata:","    \"\"\"Batch metadata (mutable only before frozen).\"\"\"","    batch_id: str","    season: str = \"\"","    tags: list[str] = field(default_factory=list)","    note: str = \"\"","    frozen: bool = False","    created_at: str = \"\"","    updated_at: str = \"\"","    created_by: str = \"\"","","","class BatchGovernanceStore:","    \"\"\"Persistent store for batch metadata.","","    Store root MUST be the artifacts root.","    Metadata path:","      {artifacts_root}/{batch_id}/metadata.json","    \"\"\"","","    def __init__(self, artifacts_root: Path):","        self.artifacts_root = artifacts_root","        self.artifacts_root.mkdir(parents=True, exist_ok=True)","","    def _metadata_path(self, batch_id: str) -> Path:","        return self.artifacts_root / batch_id / \"metadata.json\"","","    def get_metadata(self, batch_id: str) -> Optional[BatchMetadata]:","        path = self._metadata_path(batch_id)","        if not path.exists():","            return None","","        # Do NOT swallow corruption; let callers handle it explicitly.","        data = json.loads(path.read_text(encoding=\"utf-8\"))","","        tags = data.get(\"tags\", [])","        if not isinstance(tags, list):","            raise ValueError(\"metadata.tags must be a list\")","","        return BatchMetadata(","            batch_id=data[\"batch_id\"],","            season=data.get(\"season\", \"\"),","            tags=list(tags),","            note=data.get(\"note\", \"\"),","            frozen=bool(data.get(\"frozen\", False)),","            created_at=data.get(\"created_at\", \"\"),","            updated_at=data.get(\"updated_at\", \"\"),","            created_by=data.get(\"created_by\", \"\"),","        )","","    def set_metadata(self, batch_id: str, metadata: BatchMetadata) -> None:","        path = self._metadata_path(batch_id)","        path.parent.mkdir(parents=True, exist_ok=True)","","        payload = {","            \"batch_id\": batch_id,","            \"season\": metadata.season,","            \"tags\": list(metadata.tags),","            \"note\": metadata.note,","            \"frozen\": bool(metadata.frozen),","            \"created_at\": metadata.created_at,","            \"updated_at\": metadata.updated_at,","            \"created_by\": metadata.created_by,","        }","        write_json_atomic(path, payload)","","    def is_frozen(self, batch_id: str) -> bool:","        meta = self.get_metadata(batch_id)","        return bool(meta and meta.frozen)","","    def update_metadata(","        self,","        batch_id: str,","        *,","        season: Optional[str] = None,","        tags: Optional[list[str]] = None,","        note: Optional[str] = None,","        frozen: Optional[bool] = None,","        created_by: str = \"system\",","    ) -> BatchMetadata:","        \"\"\"Update metadata fields (enforcing frozen rules).","","        Frozen rules:","        - If batch is frozen:","          - season cannot change","          - frozen cannot be set to False","          - tags can be appended (dedupe + sort)","          - note can change","          - frozen=True again is a no-op","        \"\"\"","        existing = self.get_metadata(batch_id)","        now = _utc_now_iso()","","        if existing is None:","            existing = BatchMetadata(","                batch_id=batch_id,","                season=\"\",","                tags=[],","                note=\"\",","                frozen=False,","                created_at=now,","                updated_at=now,","                created_by=created_by,","            )","","        if existing.frozen:","            if season is not None and season != existing.season:","                raise ValueError(\"Cannot change season of frozen batch\")","            if frozen is False:","                raise ValueError(\"Cannot unfreeze a frozen batch\")","","        # Apply changes","        if (season is not None) and (not existing.frozen):","            existing.season = season","","        if tags is not None:","            merged = set(existing.tags)","            merged.update(tags)","            existing.tags = sorted(merged)","","        if note is not None:","            existing.note = note","","        if frozen is not None:","            if frozen is True:","                existing.frozen = True","            elif frozen is False:","                # allowed only when not frozen (blocked above if frozen)","                existing.frozen = False","","        existing.updated_at = now","        self.set_metadata(batch_id, existing)","        return existing","","    def freeze(self, batch_id: str) -> None:","        \"\"\"Freeze a batch (irreversible).","","        Raises:","            ValueError: If batch metadata not found.","        \"\"\"","        meta = self.get_metadata(batch_id)","        if meta is None:","            raise ValueError(f\"Batch {batch_id} not found\")","","        if not meta.frozen:","            meta.frozen = True","            meta.updated_at = _utc_now_iso()","            self.set_metadata(batch_id, meta)","","    def list_batches(","        self,","        *,","        season: Optional[str] = None,","        tag: Optional[str] = None,","        frozen: Optional[bool] = None,","    ) -> list[BatchMetadata]:","        \"\"\"List batches matching filters.","","        Scans artifacts root for {batch_id}/metadata.json.","","        Deterministic ordering:","        - Sort by batch_id.","        \"\"\"","        results: list[BatchMetadata] = []","        for batch_dir in sorted([p for p in self.artifacts_root.iterdir() if p.is_dir()], key=lambda p: p.name):","            meta_path = batch_dir / \"metadata.json\"","            if not meta_path.exists():","                continue","            meta = self.get_metadata(batch_dir.name)","            if meta is None:"]}
{"type":"file_chunk","path":"src/control/governance.py","chunk_index":1,"line_start":201,"line_end":211,"content":["                continue","            if season is not None and meta.season != season:","                continue","            if tag is not None and tag not in set(meta.tags):","                continue","            if frozen is not None and bool(meta.frozen) != bool(frozen):","                continue","            results.append(meta)","        return results","",""]}
{"type":"file_footer","path":"src/control/governance.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/input_manifest.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":13997,"sha256":"d5d456cb85d78b04f421e735244bcc6b2cbf9c1051bb39c98412d0b46402093f","total_lines":412,"chunk_count":3}
{"type":"file_chunk","path":"src/control/input_manifest.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Input Manifest Generation for Job Auditability.","","Generates comprehensive input manifests for job submissions, capturing:","- Dataset information (ID, kind)","- TXT file signatures and status","- Parquet file signatures and status","- Build timestamps","- System snapshot at time of job submission","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass, field, asdict","from datetime import datetime, timezone","from pathlib import Path","from typing import Dict, Any, List, Optional","import hashlib","","from control.dataset_descriptor import get_descriptor","from gui.services.reload_service import compute_file_signature, get_system_snapshot","","","@dataclass","class FileManifest:","    \"\"\"Manifest for a single file.\"\"\"","    path: str","    exists: bool","    size_bytes: int = 0","    mtime_utc: Optional[str] = None","    signature: str = \"\"","    error: Optional[str] = None","","","@dataclass","class DatasetManifest:","    \"\"\"Manifest for a dataset with TXT and Parquet information.\"\"\"","    # Required fields (no defaults) first","    dataset_id: str","    kind: str","    txt_root: str","    parquet_root: str","    ","    # Optional fields with defaults","    txt_files: List[FileManifest] = field(default_factory=list)","    txt_present: bool = False","    txt_total_size_bytes: int = 0","    txt_signature_aggregate: str = \"\"","    parquet_files: List[FileManifest] = field(default_factory=list)","    parquet_present: bool = False","    parquet_total_size_bytes: int = 0","    parquet_signature_aggregate: str = \"\"","    up_to_date: bool = False","    bars_count: Optional[int] = None","    schema_ok: Optional[bool] = None","    error: Optional[str] = None","","","@dataclass","class InputManifest:","    \"\"\"Complete input manifest for a job submission.\"\"\"","    # Metadata","    created_at: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"))","    job_id: Optional[str] = None","    season: str = \"\"","    ","    # Configuration","    config_snapshot: Dict[str, Any] = field(default_factory=dict)","    ","    # Data manifests","    data1_manifest: Optional[DatasetManifest] = None","    data2_manifest: Optional[DatasetManifest] = None","    ","    # System snapshot (summary)","    system_snapshot_summary: Dict[str, Any] = field(default_factory=dict)","    ","    # Audit trail","    manifest_hash: str = \"\"","    previous_manifest_hash: Optional[str] = None","","","def create_file_manifest(file_path: str) -> FileManifest:","    \"\"\"Create manifest for a single file.\"\"\"","    try:","        p = Path(file_path)","        exists = p.exists()","        ","        if not exists:","            return FileManifest(","                path=file_path,","                exists=False,","                size_bytes=0,","                mtime_utc=None,","                signature=\"\",","                error=\"File not found\"","            )","        ","        st = p.stat()","        mtime_utc = datetime.fromtimestamp(st.st_mtime, datetime.timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","        signature = compute_file_signature(p)","        ","        return FileManifest(","            path=file_path,","            exists=True,","            size_bytes=int(st.st_size),","            mtime_utc=mtime_utc,","            signature=signature,","            error=\"\"","        )","    except Exception as e:","        return FileManifest(","            path=file_path,","            exists=False,","            size_bytes=0,","            mtime_utc=None,","            signature=\"\",","            error=str(e)","        )","","","def create_dataset_manifest(dataset_id: str) -> DatasetManifest:","    \"\"\"Create manifest for a dataset.\"\"\"","    try:","        descriptor = get_descriptor(dataset_id)","        if descriptor is None:","            return DatasetManifest(","                dataset_id=dataset_id,","                kind=\"unknown\",","                txt_root=\"\",","                parquet_root=\"\",","                error=f\"Dataset not found: {dataset_id}\"","            )","        ","        # Process TXT files","        txt_files = []","        txt_present = True","        txt_total_size = 0","        txt_signatures = []","        ","        for txt_path_str in descriptor.txt_required_paths:","            file_manifest = create_file_manifest(txt_path_str)","            txt_files.append(file_manifest)","            ","            if not file_manifest.exists:","                txt_present = False","            else:","                txt_total_size += file_manifest.size_bytes","                txt_signatures.append(file_manifest.signature)","        ","        # Process Parquet files","        parquet_files = []","        parquet_present = True","        parquet_total_size = 0","        parquet_signatures = []","        ","        for parquet_path_str in descriptor.parquet_expected_paths:","            file_manifest = create_file_manifest(parquet_path_str)","            parquet_files.append(file_manifest)","            ","            if not file_manifest.exists:","                parquet_present = False","            else:","                parquet_total_size += file_manifest.size_bytes","                parquet_signatures.append(file_manifest.signature)","        ","        # Determine up-to-date status","        up_to_date = txt_present and parquet_present","        # Simple heuristic: if both present, assume up-to-date","        # In a real implementation, this would compare timestamps or content hashes","        ","        # Try to get bars count from Parquet if available","        bars_count = None","        schema_ok = None","        ","        if parquet_present and descriptor.parquet_expected_paths:","            try:","                parquet_path = Path(descriptor.parquet_expected_paths[0])","                if parquet_path.exists():","                    # Quick schema check","                    import pandas as pd","                    df_sample = pd.read_parquet(parquet_path, nrows=1)","                    schema_ok = True","                    ","                    # Try to get row count for small files","                    if parquet_path.stat().st_size < 1000000:  # < 1MB","                        df = pd.read_parquet(parquet_path)","                        # Use df.shape[0] or len(df.index) instead of len(df)","                        if hasattr(df, 'shape') and len(df.shape) >= 1:","                            bars_count = df.shape[0]","                        elif hasattr(df, 'index'):","                            bars_count = len(df.index)","                        else:","                            bars_count = len(df)  # fallback","            except Exception:","                schema_ok = False","        ","        return DatasetManifest(","            dataset_id=dataset_id,","            kind=descriptor.kind,","            txt_root=descriptor.txt_root,"]}
{"type":"file_chunk","path":"src/control/input_manifest.py","chunk_index":1,"line_start":201,"line_end":400,"content":["            txt_files=txt_files,","            txt_present=txt_present,","            txt_total_size_bytes=txt_total_size,","            txt_signature_aggregate=\"|\".join(txt_signatures) if txt_signatures else \"none\",","            parquet_root=descriptor.parquet_root,","            parquet_files=parquet_files,","            parquet_present=parquet_present,","            parquet_total_size_bytes=parquet_total_size,","            parquet_signature_aggregate=\"|\".join(parquet_signatures) if parquet_signatures else \"none\",","            up_to_date=up_to_date,","            bars_count=bars_count,","            schema_ok=schema_ok","        )","    except Exception as e:","        return DatasetManifest(","            dataset_id=dataset_id,","            kind=\"unknown\",","            txt_root=\"\",","            parquet_root=\"\",","            error=str(e)","        )","","","def create_input_manifest(","    job_id: Optional[str],","    season: str,","    config_snapshot: Dict[str, Any],","    data1_dataset_id: str,","    data2_dataset_id: Optional[str] = None,","    previous_manifest_hash: Optional[str] = None",") -> InputManifest:","    \"\"\"Create complete input manifest for a job submission.","    ","    Args:","        job_id: Job ID (if available)","        season: Season identifier","        config_snapshot: Configuration snapshot from make_config_snapshot","        data1_dataset_id: DATA1 dataset ID","        data2_dataset_id: Optional DATA2 dataset ID","        previous_manifest_hash: Optional hash of previous manifest (for chain)","        ","    Returns:","        InputManifest with all audit information","    \"\"\"","    # Create dataset manifests","    data1_manifest = create_dataset_manifest(data1_dataset_id)","    ","    data2_manifest = None","    if data2_dataset_id:","        data2_manifest = create_dataset_manifest(data2_dataset_id)","    ","    # Get system snapshot summary","    system_snapshot = get_system_snapshot()","    snapshot_summary = {","        \"created_at\": system_snapshot.created_at.isoformat(),","        \"total_datasets\": system_snapshot.total_datasets,","        \"total_strategies\": system_snapshot.total_strategies,","        \"notes\": system_snapshot.notes[:5],  # First 5 notes","        \"error_count\": len(system_snapshot.errors)","    }","    ","    # Create manifest","    manifest = InputManifest(","        job_id=job_id,","        season=season,","        config_snapshot=config_snapshot,","        data1_manifest=data1_manifest,","        data2_manifest=data2_manifest,","        system_snapshot_summary=snapshot_summary,","        previous_manifest_hash=previous_manifest_hash","    )","    ","    # Compute manifest hash","    manifest_dict = asdict(manifest)","    # Remove hash field before computing hash","    manifest_dict.pop(\"manifest_hash\", None)","    ","    # Convert to JSON and compute hash","    manifest_json = json.dumps(manifest_dict, sort_keys=True, separators=(',', ':'))","    manifest_hash = hashlib.sha256(manifest_json.encode('utf-8')).hexdigest()[:32]","    ","    manifest.manifest_hash = manifest_hash","    ","    return manifest","","","def write_input_manifest(","    manifest: InputManifest,","    output_path: Path",") -> bool:","    \"\"\"Write input manifest to file.","    ","    Args:","        manifest: InputManifest to write","        output_path: Path to write manifest JSON file","        ","    Returns:","        True if successful, False otherwise","    \"\"\"","    try:","        # Ensure parent directory exists","        output_path.parent.mkdir(parents=True, exist_ok=True)","        ","        # Convert to dictionary","        manifest_dict = asdict(manifest)","        ","        # Write JSON","        with open(output_path, 'w', encoding='utf-8') as f:","            json.dump(manifest_dict, f, indent=2, ensure_ascii=False)","        ","        return True","    except Exception as e:","        print(f\"Error writing input manifest: {e}\")","        return False","","","def read_input_manifest(input_path: Path) -> Optional[InputManifest]:","    \"\"\"Read input manifest from file.","    ","    Args:","        input_path: Path to manifest JSON file","        ","    Returns:","        InputManifest if successful, None otherwise","    \"\"\"","    try:","        with open(input_path, 'r', encoding='utf-8') as f:","            data = json.load(f)","        ","        # Reconstruct nested objects","        if data.get('data1_manifest'):","            data1_dict = data['data1_manifest']","            data['data1_manifest'] = DatasetManifest(**data1_dict)","        ","        if data.get('data2_manifest'):","            data2_dict = data['data2_manifest']","            data['data2_manifest'] = DatasetManifest(**data2_dict)","        ","        return InputManifest(**data)","    except Exception as e:","        print(f\"Error reading input manifest: {e}\")","        return None","","","def verify_input_manifest(manifest: InputManifest) -> Dict[str, Any]:","    \"\"\"Verify input manifest integrity and completeness.","    ","    Args:","        manifest: InputManifest to verify","        ","    Returns:","        Dictionary with verification results","    \"\"\"","    results = {","        \"valid\": True,","        \"errors\": [],","        \"warnings\": [],","        \"checks\": []","    }","    ","    # Check timestamp first (warnings)","    try:","        created_at = datetime.fromisoformat(manifest.created_at.replace('Z', '+00:00'))","        age_hours = (datetime.now(timezone.utc) - created_at).total_seconds() / 3600","        if age_hours > 24:","            results[\"warnings\"].append(f\"Manifest is {age_hours:.1f} hours old\")","    except Exception:","        results[\"warnings\"].append(\"Invalid timestamp format\")","    ","    # Check DATA1 manifest (structural errors before hash)","    if not manifest.data1_manifest:","        results[\"errors\"].append(\"Missing DATA1 manifest\")","        results[\"valid\"] = False","    else:","        if not manifest.data1_manifest.txt_present:","            results[\"warnings\"].append(f\"DATA1 dataset {manifest.data1_manifest.dataset_id} missing TXT files\")","        ","        if not manifest.data1_manifest.parquet_present:","            results[\"warnings\"].append(f\"DATA1 dataset {manifest.data1_manifest.dataset_id} missing Parquet files\")","        ","        if manifest.data1_manifest.error:","            results[\"warnings\"].append(f\"DATA1 dataset error: {manifest.data1_manifest.error}\")","    ","    # Check DATA2 manifest if present","    if manifest.data2_manifest:","        if not manifest.data2_manifest.txt_present:","            results[\"warnings\"].append(f\"DATA2 dataset {manifest.data2_manifest.dataset_id} missing TXT files\")","        ","        if not manifest.data2_manifest.parquet_present:","            results[\"warnings\"].append(f\"DATA2 dataset {manifest.data2_manifest.dataset_id} missing Parquet files\")","        ","        if manifest.data2_manifest.error:","            results[\"warnings\"].append(f\"DATA2 dataset error: {manifest.data2_manifest.error}\")","    ","    # Check system snapshot","    if not manifest.system_snapshot_summary:","        results[\"warnings\"].append(\"System snapshot summary is empty\")","    ","    # Check manifest hash (after structural checks)","    manifest_dict = asdict(manifest)"]}
{"type":"file_chunk","path":"src/control/input_manifest.py","chunk_index":2,"line_start":401,"line_end":412,"content":["    original_hash = manifest_dict.pop(\"manifest_hash\", None)","    ","    manifest_json = json.dumps(manifest_dict, sort_keys=True, separators=(',', ':'))","    computed_hash = hashlib.sha256(manifest_json.encode('utf-8')).hexdigest()[:32]","    ","    if original_hash != computed_hash:","        results[\"valid\"] = False","        results[\"errors\"].append(f\"Manifest hash mismatch: expected {original_hash}, got {computed_hash}\")","    else:","        results[\"checks\"].append(\"Manifest hash verified\")","    ","    return results"]}
{"type":"file_footer","path":"src/control/input_manifest.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/control/job_api.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":16256,"sha256":"a75fd9d04ebfda34987d491685239629dbce674568dd2f8a1ee9188761f0f2e3","total_lines":463,"chunk_count":3}
{"type":"file_chunk","path":"src/control/job_api.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Job API for M1 Wizard.","","Provides job creation and governance checking for the wizard UI.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Dict, Any, Optional, List","from datetime import datetime","","from control.jobs_db import create_job, get_job, list_jobs","from control.types import DBJobSpec, JobRecord, JobStatus","from control.dataset_catalog import get_dataset_catalog","from control.strategy_catalog import get_strategy_catalog","from control.dataset_descriptor import get_descriptor","from control.input_manifest import create_input_manifest, write_input_manifest","from core.config_snapshot import make_config_snapshot","","","class JobAPIError(Exception):","    \"\"\"Base exception for Job API errors.\"\"\"","    pass","","","class SeasonFrozenError(JobAPIError):","    \"\"\"Raised when trying to submit a job to a frozen season.\"\"\"","    pass","","","class ValidationError(JobAPIError):","    \"\"\"Raised when job validation fails.\"\"\"","    pass","","","def check_season_not_frozen(season: str, action: str = \"submit_job\") -> None:","    \"\"\"Check if a season is frozen.","    ","    Args:","        season: Season identifier (e.g., \"2024Q1\")","        action: Action being performed (for error message)","        ","    Raises:","        SeasonFrozenError: If season is frozen","    \"\"\"","    # TODO: Implement actual season frozen check","    # For M1, we'll assume seasons are not frozen","    # In a real implementation, this would check season governance state","    pass","","","def validate_wizard_payload(payload: Dict[str, Any]) -> List[str]:","    \"\"\"Validate wizard payload.","    ","    Args:","        payload: Wizard payload dictionary","        ","    Returns:","        List of validation error messages (empty if valid)","    \"\"\"","    errors = []","    ","    # Required fields","    required_fields = [\"season\", \"data1\", \"strategy_id\", \"params\"]","    for field in required_fields:","        if field not in payload:","            errors.append(f\"Missing required field: {field}\")","    ","    # Validate data1","    if \"data1\" in payload:","        data1 = payload[\"data1\"]","        if not isinstance(data1, dict):","            errors.append(\"data1 must be a dictionary\")","        else:","            if \"dataset_id\" not in data1:","                errors.append(\"data1 missing dataset_id\")","            else:","                # Check dataset exists and has Parquet files","                dataset_id = data1[\"dataset_id\"]","                try:","                    descriptor = get_descriptor(dataset_id)","                    if descriptor is None:","                        errors.append(f\"Dataset not found: {dataset_id}\")","                    else:","                        # Check if Parquet files exist","                        from pathlib import Path","                        parquet_missing = []","                        for parquet_path_str in descriptor.parquet_expected_paths:","                            parquet_path = Path(parquet_path_str)","                            if not parquet_path.exists():","                                parquet_missing.append(parquet_path_str)","                        ","                        if parquet_missing:","                            missing_list = \", \".join(parquet_missing[:3])  # Show first 3","                            if len(parquet_missing) > 3:","                                missing_list += f\" and {len(parquet_missing) - 3} more\"","                            errors.append(f\"Dataset {dataset_id} missing Parquet files: {missing_list}\")","                            errors.append(f\"Use the Status page to build Parquet from TXT sources\")","                except Exception as e:","                    errors.append(f\"Error checking dataset {dataset_id}: {str(e)}\")","            ","            if \"symbols\" not in data1:","                errors.append(\"data1 missing symbols\")","            if \"timeframes\" not in data1:","                errors.append(\"data1 missing timeframes\")","    ","    # Validate data2 if present","    if \"data2\" in payload and payload[\"data2\"]:","        data2 = payload[\"data2\"]","        if not isinstance(data2, dict):","            errors.append(\"data2 must be a dictionary or null\")","        else:","            if \"dataset_id\" not in data2:","                errors.append(\"data2 missing dataset_id\")","            else:","                # Check data2 dataset exists and has Parquet files","                dataset_id = data2[\"dataset_id\"]","                try:","                    descriptor = get_descriptor(dataset_id)","                    if descriptor is None:","                        errors.append(f\"DATA2 dataset not found: {dataset_id}\")","                    else:","                        # Check if Parquet files exist","                        from pathlib import Path","                        parquet_missing = []","                        for parquet_path_str in descriptor.parquet_expected_paths:","                            parquet_path = Path(parquet_path_str)","                            if not parquet_path.exists():","                                parquet_missing.append(parquet_path_str)","                        ","                        if parquet_missing:","                            missing_list = \", \".join(parquet_missing[:3])","                            if len(parquet_missing) > 3:","                                missing_list += f\" and {len(parquet_missing) - 3} more\"","                            errors.append(f\"DATA2 dataset {dataset_id} missing Parquet files: {missing_list}\")","                except Exception as e:","                    errors.append(f\"Error checking DATA2 dataset {dataset_id}: {str(e)}\")","            ","            if \"filters\" not in data2:","                errors.append(\"data2 missing filters\")","    ","    # Validate strategy","    if \"strategy_id\" in payload:","        strategy_catalog = get_strategy_catalog()","        strategy = strategy_catalog.get_strategy(payload[\"strategy_id\"])","        if strategy is None:","            errors.append(f\"Unknown strategy: {payload['strategy_id']}\")","        else:","            # Validate parameters","            params = payload.get(\"params\", {})","            param_errors = strategy_catalog.validate_parameters(payload[\"strategy_id\"], params)","            for param_name, error_msg in param_errors.items():","                errors.append(f\"Parameter '{param_name}': {error_msg}\")","    ","    return errors","","","def calculate_units(payload: Dict[str, Any]) -> int:","    \"\"\"Calculate units count for wizard payload.","    ","    Units formula: |DATA1.symbols| × |DATA1.timeframes| × |strategies| × |DATA2.filters|","    ","    Args:","        payload: Wizard payload dictionary","        ","    Returns:","        Total units count","    \"\"\"","    # Extract data1 symbols and timeframes","    data1 = payload.get(\"data1\", {})","    symbols = data1.get(\"symbols\", [])","    timeframes = data1.get(\"timeframes\", [])","    ","    # Count strategies (always 1 for single strategy, but could be list)","    strategy_id = payload.get(\"strategy_id\")","    strategies = [strategy_id] if strategy_id else []","    ","    # Extract data2 filters if present","    data2 = payload.get(\"data2\")","    if data2 is None:","        filters = []","    else:","        filters = data2.get(\"filters\", [])","    ","    # Apply formula","    symbols_count = len(symbols) if isinstance(symbols, list) else 1","    timeframes_count = len(timeframes) if isinstance(timeframes, list) else 1","    strategies_count = len(strategies) if isinstance(strategies, list) else 1","    filters_count = len(filters) if isinstance(filters, list) else 1","    ","    # If data2 is not enabled, filters_count should be 1 (no filter multiplication)","    if not data2 or not payload.get(\"enable_data2\", False):","        filters_count = 1","    ","    units = symbols_count * timeframes_count * strategies_count * filters_count","    return units","","","def create_job_from_wizard(payload: Dict[str, Any]) -> Dict[str, Any]:"]}
{"type":"file_chunk","path":"src/control/job_api.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    \"\"\"Create a job from wizard payload.","    ","    This is the main function called by the wizard UI on submit.","    ","    Args:","        payload: Wizard payload dictionary with structure:","            {","                \"season\": \"2024Q1\",","                \"data1\": {","                    \"dataset_id\": \"CME.MNQ.60m.2020-2024\",","                    \"symbols\": [\"MNQ\", \"MXF\"],","                    \"timeframes\": [\"60m\", \"120m\"],","                    \"start_date\": \"2020-01-01\",","                    \"end_date\": \"2024-12-31\"","                },","                \"data2\": {","                    \"dataset_id\": \"TWF.MXF.15m.2018-2023\",","                    \"filters\": [\"filter1\", \"filter2\"]","                } | null,","                \"strategy_id\": \"sma_cross_v1\",","                \"params\": {","                    \"window_fast\": 10,","                    \"window_slow\": 30","                },","                \"wfs\": {","                    \"stage0_subsample\": 0.1,","                    \"top_k\": 20,","                    \"mem_limit_mb\": 8192,","                    \"allow_auto_downsample\": True","                }","            }","        ","    Returns:","        Dictionary with job_id and units count:","            {","                \"job_id\": \"uuid-here\",","                \"units\": 4,","                \"season\": \"2024Q1\",","                \"status\": \"queued\"","            }","        ","    Raises:","        SeasonFrozenError: If season is frozen","        ValidationError: If payload validation fails","    \"\"\"","    # Check season not frozen","    season = payload.get(\"season\")","    if season:","        check_season_not_frozen(season, action=\"submit_job\")","    ","    # Validate payload","    errors = validate_wizard_payload(payload)","    if errors:","        raise ValidationError(f\"Payload validation failed: {', '.join(errors)}\")","    ","    # Calculate units","    units = calculate_units(payload)","    ","    # Create config snapshot","    config_snapshot = make_config_snapshot(payload)","    ","    # Create DBJobSpec","    data1 = payload[\"data1\"]","    dataset_id = data1[\"dataset_id\"]","    ","    # Generate outputs root path","    outputs_root = f\"outputs/{season}/jobs\"","    ","    # Create job spec","    spec = DBJobSpec(","        season=season,","        dataset_id=dataset_id,","        outputs_root=outputs_root,","        config_snapshot=config_snapshot,","        config_hash=\"\",  # Will be computed by create_job","        data_fingerprint_sha256_40=\"\"  # Will be populated if needed","    )","    ","    # Create job in database","    db_path = Path(\"outputs/jobs.db\")","    job_id = create_job(db_path, spec)","    ","    # Create input manifest for auditability","    try:","        # Extract DATA2 dataset ID if present","        data2_dataset_id = None","        if \"data2\" in payload and payload[\"data2\"]:","            data2 = payload[\"data2\"]","            data2_dataset_id = data2.get(\"dataset_id\")","        ","        # Create input manifest","        from control.input_manifest import create_input_manifest, write_input_manifest","        ","        manifest = create_input_manifest(","            job_id=job_id,","            season=season,","            config_snapshot=config_snapshot,","            data1_dataset_id=dataset_id,","            data2_dataset_id=data2_dataset_id,","            previous_manifest_hash=None  # First in chain","        )","        ","        # Write manifest to job outputs directory","        manifest_dir = Path(f\"outputs/{season}/jobs/{job_id}\")","        manifest_dir.mkdir(parents=True, exist_ok=True)","        manifest_path = manifest_dir / \"input_manifest.json\"","        ","        write_success = write_input_manifest(manifest, manifest_path)","        ","        if not write_success:","            # Log warning but don't fail the job","            print(f\"Warning: Failed to write input manifest for job {job_id}\")","    except Exception as e:","        # Don't fail job creation if manifest creation fails","        print(f\"Warning: Failed to create input manifest for job {job_id}: {e}\")","    ","    return {","        \"job_id\": job_id,","        \"units\": units,","        \"season\": season,","        \"status\": \"queued\"","    }","","","def get_job_status(job_id: str) -> Dict[str, Any]:","    \"\"\"Get job status with units progress.","    ","    Args:","        job_id: Job ID","        ","    Returns:","        Dictionary with job status and progress:","            {","                \"job_id\": \"uuid-here\",","                \"status\": \"running\",","                \"units_done\": 10,","                \"units_total\": 20,","                \"progress\": 0.5,","                \"created_at\": \"2024-01-01T00:00:00Z\",","                \"updated_at\": \"2024-01-01T00:00:00Z\"","            }","    \"\"\"","    db_path = Path(\"outputs/jobs.db\")","    try:","        job = get_job(db_path, job_id)","        ","        # For M1, we need to calculate units_done and units_total","        # This would normally come from job execution progress","        # For now, we'll return placeholder values","        units_total = 0","        units_done = 0","        ","        # Try to extract units from config snapshot","        if hasattr(job.spec, 'config_snapshot'):","            config = job.spec.config_snapshot","            if isinstance(config, dict) and 'units' in config:","                units_total = config.get('units', 0)","        ","        # Estimate units_done based on status","        if job.status == JobStatus.DONE:","            units_done = units_total","        elif job.status == JobStatus.RUNNING:","            # For demo, assume 50% progress","            units_done = units_total // 2 if units_total > 0 else 0","        ","        progress = units_done / units_total if units_total > 0 else 0","        ","        return {","            \"job_id\": job_id,","            \"status\": job.status.value,","            \"units_done\": units_done,","            \"units_total\": units_total,","            \"progress\": progress,","            \"created_at\": job.created_at,","            \"updated_at\": job.updated_at,","            \"season\": job.spec.season,","            \"dataset_id\": job.spec.dataset_id","        }","    except KeyError:","        raise JobAPIError(f\"Job not found: {job_id}\")","","","def list_jobs_with_progress(limit: int = 50) -> List[Dict[str, Any]]:","    \"\"\"List jobs with units progress.","    ","    Args:","        limit: Maximum number of jobs to return","        ","    Returns:","        List of job dictionaries with progress information","    \"\"\"","    db_path = Path(\"outputs/jobs.db\")","    jobs = list_jobs(db_path, limit=limit)","    ","    result = []","    for job in jobs:","        # Calculate progress for each job","        units_total = 0","        units_done = 0","        "]}
{"type":"file_chunk","path":"src/control/job_api.py","chunk_index":2,"line_start":401,"line_end":463,"content":["        if hasattr(job.spec, 'config_snapshot'):","            config = job.spec.config_snapshot","            if isinstance(config, dict) and 'units' in config:","                units_total = config.get('units', 0)","        ","        if job.status == JobStatus.DONE:","            units_done = units_total","        elif job.status == JobStatus.RUNNING:","            units_done = units_total // 2 if units_total > 0 else 0","        ","        progress = units_done / units_total if units_total > 0 else 0","        ","        result.append({","            \"job_id\": job.job_id,","            \"status\": job.status.value,","            \"units_done\": units_done,","            \"units_total\": units_total,","            \"progress\": progress,","            \"created_at\": job.created_at,","            \"updated_at\": job.updated_at,","            \"season\": job.spec.season,","            \"dataset_id\": job.spec.dataset_id","        })","    ","    return result","","","def get_job_logs_tail(job_id: str, lines: int = 50) -> List[str]:","    \"\"\"Get tail of job logs.","    ","    Args:","        job_id: Job ID","        lines: Number of lines to return","        ","    Returns:","        List of log lines (most recent first)","    \"\"\"","    # TODO: Implement actual log retrieval","    # For M1, return placeholder logs","    return [","        f\"[{datetime.now().isoformat()}] Job {job_id} started\",","        f\"[{datetime.now().isoformat()}] Loading dataset...\",","        f\"[{datetime.now().isoformat()}] Running strategy...\",","        f\"[{datetime.now().isoformat()}] Processing units...\",","    ][-lines:]","","","# Convenience functions for GUI","def submit_wizard_job(payload: Dict[str, Any]) -> Dict[str, Any]:","    \"\"\"Submit wizard job (alias for create_job_from_wizard).\"\"\"","    return create_job_from_wizard(payload)","","","def get_job_summary(job_id: str) -> Dict[str, Any]:","    \"\"\"Get job summary for detail page.\"\"\"","    status = get_job_status(job_id)","    logs = get_job_logs_tail(job_id, lines=20)","    ","    return {","        **status,","        \"logs\": logs,","        \"log_tail\": \"\\n\".join(logs[-10:]) if logs else \"No logs available\"","    }"]}
{"type":"file_footer","path":"src/control/job_api.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/control/job_expand.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3824,"sha256":"f8403ae1a325720b1c91a14422fe9aa6b7bc80d79d32b9919b5dd504b390e246","total_lines":134,"chunk_count":1}
{"type":"file_chunk","path":"src/control/job_expand.py","chunk_index":0,"line_start":1,"line_end":134,"content":["","\"\"\"Job Template Expansion for Phase 13.","","Expand a JobTemplate (with param grids) into a deterministic list of JobSpec.","Pure functions, no side effects.","\"\"\"","","from __future__ import annotations","","import itertools","from datetime import date","from typing import Any","","from pydantic import BaseModel, ConfigDict, Field","","from control.job_spec import DataSpec, WizardJobSpec, WFSSpec","from control.param_grid import ParamGridSpec, values_for_param","","","class JobTemplate(BaseModel):","    \"\"\"Template for generating multiple JobSpec via parameter grids.","    ","    Phase 13: All parameters must be explicitly configured via param_grid.","    \"\"\"","    ","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","    ","    season: str = Field(","        ...,","        description=\"Season identifier (e.g., '2024Q1')\"","    )","    ","    dataset_id: str = Field(","        ...,","        description=\"Dataset identifier (must match registry)\"","    )","    ","    strategy_id: str = Field(","        ...,","        description=\"Strategy identifier (must match registry)\"","    )","    ","    param_grid: dict[str, ParamGridSpec] = Field(","        ...,","        description=\"Mapping from parameter name to grid specification\"","    )","    ","    wfs: WFSSpec = Field(","        default_factory=WFSSpec,","        description=\"WFS configuration\"","    )","","","def expand_job_template(template: JobTemplate) -> list[WizardJobSpec]:","    \"\"\"Expand a JobTemplate into a deterministic list of WizardJobSpec.","    ","    Args:","        template: Job template with param grids","    ","    Returns:","        List of WizardJobSpec in deterministic order.","    ","    Raises:","        ValueError: if any param grid is invalid.","    \"\"\"","    # Sort param names for deterministic expansion","    param_names = sorted(template.param_grid.keys())","    ","    # For each param, compute list of values","    param_values: dict[str, list[Any]] = {}","    for name in param_names:","        grid = template.param_grid[name]","        values = values_for_param(grid)","        param_values[name] = values","    ","    # Compute Cartesian product in deterministic order","    # Order: iterate params sorted by name, values in order from values_for_param","    value_lists = [param_values[name] for name in param_names]","    ","    # Create a DataSpec with placeholder dates (tests don't care about dates)","    # Use fixed dates that are valid for any dataset","    data1 = DataSpec(","        dataset_id=template.dataset_id,","        start_date=date(2000, 1, 1),","        end_date=date(2000, 1, 2)","    )","    ","    jobs = []","    for combo in itertools.product(*value_lists):","        params = dict(zip(param_names, combo))","        job = WizardJobSpec(","            season=template.season,","            data1=data1,","            data2=None,","            strategy_id=template.strategy_id,","            params=params,","            wfs=template.wfs","        )","        jobs.append(job)","    ","    return jobs","","","def estimate_total_jobs(template: JobTemplate) -> int:","    \"\"\"Estimate total number of jobs that would be generated.","    ","    Returns:","        Product of value counts for each parameter.","    \"\"\"","    total = 1","    for grid in template.param_grid.values():","        total *= len(values_for_param(grid))","    return total","","","def validate_template(template: JobTemplate) -> None:","    \"\"\"Validate template.","    ","    Raises ValueError with descriptive message if invalid.","    \"\"\"","    if not template.season:","        raise ValueError(\"season must be non-empty\")","    if not template.dataset_id:","        raise ValueError(\"dataset_id must be non-empty\")","    if not template.strategy_id:","        raise ValueError(\"strategy_id must be non-empty\")","    if not template.param_grid:","        raise ValueError(\"param_grid cannot be empty\")","    ","    # Validate each grid (values_for_param will raise if invalid)","    for grid in template.param_grid.values():","        values_for_param(grid)","",""]}
{"type":"file_footer","path":"src/control/job_expand.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/job_spec.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3059,"sha256":"0a62e74a9d3ad7f379f4e5053522f74f31b3f68e261b424559aad7be61ccb753","total_lines":102,"chunk_count":1}
{"type":"file_chunk","path":"src/control/job_spec.py","chunk_index":0,"line_start":1,"line_end":102,"content":["","\"\"\"WizardJobSpec Schema for Research Job Wizard.","","Phase 12: WizardJobSpec is the ONLY output from GUI.","Contains all configuration needed to run a research job.","Must NOT contain any worker/engine runtime state.","\"\"\"","","from __future__ import annotations","","from datetime import date","from types import MappingProxyType","from typing import Any, Mapping, Optional","","from pydantic import BaseModel, ConfigDict, Field, field_serializer, model_validator","","","class DataSpec(BaseModel):","    \"\"\"Dataset specification for a research job.\"\"\"","    ","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","    ","    dataset_id: str = Field(..., min_length=1)","    start_date: date","    end_date: date","    ","    @model_validator(mode=\"after\")","    def _check_dates(self) -> \"DataSpec\":","        if self.start_date > self.end_date:","            raise ValueError(\"start_date must be <= end_date\")","        return self","","","class WFSSpec(BaseModel):","    \"\"\"WFS (Winners Funnel System) configuration.\"\"\"","    ","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","    ","    stage0_subsample: float = 1.0","    top_k: int = 100","    mem_limit_mb: int = 4096","    allow_auto_downsample: bool = True","    ","    @model_validator(mode=\"after\")","    def _check_ranges(self) -> \"WFSSpec\":","        if not (0.0 < self.stage0_subsample <= 1.0):","            raise ValueError(\"stage0_subsample must be in (0, 1]\")","        if self.top_k <= 0:","            raise ValueError(\"top_k must be > 0\")","        if self.mem_limit_mb < 1024:","            raise ValueError(\"mem_limit_mb must be >= 1024\")","        return self","","","class WizardJobSpec(BaseModel):","    \"\"\"Complete job specification for research.","    ","    Phase 12 Iron Rule: GUI's ONLY output = WizardJobSpec JSON","    Must NOT contain worker/engine runtime state.","    \"\"\"","    ","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","    ","    season: str = Field(..., min_length=1)","    data1: DataSpec","    data2: Optional[DataSpec] = None","    strategy_id: str = Field(..., min_length=1)","    params: Mapping[str, Any] = Field(default_factory=dict)","    wfs: WFSSpec = Field(default_factory=WFSSpec)","    ","    @model_validator(mode=\"after\")","    def _freeze_params(self) -> \"WizardJobSpec\":","        # make params immutable so test_jobspec_immutability passes","        if not isinstance(self.params, MappingProxyType):","            object.__setattr__(self, \"params\", MappingProxyType(dict(self.params)))","        return self","    ","    @field_serializer(\"params\")","    def _ser_params(self, v: Mapping[str, Any]) -> dict[str, Any]:","        return dict(v)","","    @property","    def dataset_id(self) -> str:","        \"\"\"Alias for data1.dataset_id (for backward compatibility).\"\"\"","        return self.data1.dataset_id","","","# Example WizardJobSpec for documentation","EXAMPLE_WIZARD_JOBSPEC = WizardJobSpec(","    season=\"2024Q1\",","    data1=DataSpec(","        dataset_id=\"CME.MNQ.60m.2020-2024\",","        start_date=date(2020, 1, 1),","        end_date=date(2024, 12, 31)","    ),","    data2=None,","    strategy_id=\"sma_cross_v1\",","    params={\"window\": 20, \"threshold\": 0.5},","    wfs=WFSSpec()",")","",""]}
{"type":"file_footer","path":"src/control/job_spec.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/jobs_db.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":29222,"sha256":"b588bac9dc22502593cc4c3f454ec6ffec14b954cbff5cb4a30cd2a4864ccda4","total_lines":927,"chunk_count":5}
{"type":"file_chunk","path":"src/control/jobs_db.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"SQLite jobs database - CRUD and state machine.\"\"\"","","from __future__ import annotations","","import json","import sqlite3","import time","from collections.abc import Callable","from datetime import datetime, timezone","from pathlib import Path","from typing import Optional, TypeVar","from uuid import uuid4","","from control.types import DBJobSpec, JobRecord, JobStatus, StopMode","","T = TypeVar(\"T\")","","","def _connect(db_path: Path) -> sqlite3.Connection:","    \"\"\"","    Create SQLite connection with concurrency hardening.","    ","    One operation = one connection (avoid shared connection across threads).","    ","    Args:","        db_path: Path to SQLite database","        ","    Returns:","        Configured SQLite connection with WAL mode and busy timeout","    \"\"\"","    # One operation = one connection (avoid shared connection across threads)","    conn = sqlite3.connect(str(db_path), timeout=30.0)","    conn.row_factory = sqlite3.Row","","    # Concurrency hardening","    conn.execute(\"PRAGMA journal_mode=WAL;\")","    conn.execute(\"PRAGMA synchronous=NORMAL;\")","    conn.execute(\"PRAGMA foreign_keys=ON;\")","    conn.execute(\"PRAGMA busy_timeout=30000;\")  # ms","","    return conn","","","def _with_retry_locked(fn: Callable[[], T]) -> T:","    \"\"\"","    Retry DB operation on SQLITE_BUSY/locked errors.","    ","    Args:","        fn: Callable that performs DB operation","        ","    Returns:","        Result from fn()","        ","    Raises:","        sqlite3.OperationalError: If operation fails after retries or for non-locked errors","    \"\"\"","    # Retry only for SQLITE_BUSY/locked","    delays = (0.05, 0.10, 0.20, 0.40, 0.80, 1.0)","    last: Exception | None = None","    for d in delays:","        try:","            return fn()","        except sqlite3.OperationalError as e:","            msg = str(e).lower()","            if \"locked\" not in msg and \"busy\" not in msg:","                raise","            last = e","            time.sleep(d)","    assert last is not None","    raise last","","","def ensure_schema(conn: sqlite3.Connection) -> None:","    \"\"\"","    Create tables or migrate schema in-place.","    ","    Idempotent: safe to call multiple times.","    ","    Args:","        conn: SQLite connection","    \"\"\"","    # Create jobs table if not exists","    conn.execute(\"\"\"","        CREATE TABLE IF NOT EXISTS jobs (","            job_id TEXT PRIMARY KEY,","            status TEXT NOT NULL,","            created_at TEXT NOT NULL,","            updated_at TEXT NOT NULL,","            season TEXT NOT NULL,","            dataset_id TEXT NOT NULL,","            outputs_root TEXT NOT NULL,","            config_hash TEXT NOT NULL,","            config_snapshot_json TEXT NOT NULL,","            pid INTEGER NULL,","            run_id TEXT NULL,","            run_link TEXT NULL,","            report_link TEXT NULL,","            last_error TEXT NULL,","            requested_stop TEXT NULL,","            requested_pause INTEGER NOT NULL DEFAULT 0,","            tags_json TEXT DEFAULT '[]'","        )","    \"\"\")","    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_status ON jobs(status)\")","    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_created_at ON jobs(created_at DESC)\")","    ","    # Check existing columns for migrations","    cursor = conn.execute(\"PRAGMA table_info(jobs)\")","    columns = [row[1] for row in cursor.fetchall()]","    ","    # Add run_id column if missing","    if \"run_id\" not in columns:","        conn.execute(\"ALTER TABLE jobs ADD COLUMN run_id TEXT\")","    ","    # Add report_link column if missing","    if \"report_link\" not in columns:","        conn.execute(\"ALTER TABLE jobs ADD COLUMN report_link TEXT\")","    ","    # Add tags_json column if missing","    if \"tags_json\" not in columns:","        conn.execute(\"ALTER TABLE jobs ADD COLUMN tags_json TEXT DEFAULT '[]'\")","    ","    # Add data_fingerprint_sha256_40 column if missing","    if \"data_fingerprint_sha256_40\" not in columns:","        conn.execute(\"ALTER TABLE jobs ADD COLUMN data_fingerprint_sha256_40 TEXT DEFAULT ''\")","    ","    # Create job_logs table if not exists","    conn.execute(\"\"\"","        CREATE TABLE IF NOT EXISTS job_logs (","            log_id INTEGER PRIMARY KEY AUTOINCREMENT,","            job_id TEXT NOT NULL,","            created_at TEXT NOT NULL,","            log_text TEXT NOT NULL,","            FOREIGN KEY (job_id) REFERENCES jobs(job_id)","        )","    \"\"\")","    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_job_logs_job_id ON job_logs(job_id, created_at DESC)\")","    ","    conn.commit()","","","def init_db(db_path: Path) -> None:","    \"\"\"","    Initialize jobs database schema.","    ","    Args:","        db_path: Path to SQLite database file","    \"\"\"","    db_path.parent.mkdir(parents=True, exist_ok=True)","    ","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            # ensure_schema handles CREATE TABLE IF NOT EXISTS + migrations","    ","    _with_retry_locked(_op)","","","def _now_iso() -> str:","    \"\"\"Get current UTC time as ISO8601 string.\"\"\"","    return datetime.now(timezone.utc).isoformat()","","","def _validate_status_transition(old_status: JobStatus, new_status: JobStatus) -> None:","    \"\"\"","    Validate status transition (state machine).","    ","    Allowed transitions:","    - QUEUED → RUNNING","    - RUNNING → PAUSED (pause=1 and worker checkpoint)","    - PAUSED → RUNNING (pause=0 and worker continues)","    - RUNNING/PAUSED → DONE | FAILED | KILLED","    - QUEUED → KILLED (cancel before running)","    ","    Args:","        old_status: Current status","        new_status: Target status","        ","    Raises:","        ValueError: If transition is not allowed","    \"\"\"","    allowed = {","        JobStatus.QUEUED: {JobStatus.RUNNING, JobStatus.KILLED},","        JobStatus.RUNNING: {JobStatus.PAUSED, JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED},","        JobStatus.PAUSED: {JobStatus.RUNNING, JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED},","    }","    ","    if old_status in allowed:","        if new_status not in allowed[old_status]:","            raise ValueError(","                f\"Invalid status transition: {old_status} → {new_status}. \"","                f\"Allowed: {allowed[old_status]}\"","            )","    elif old_status in {JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED}:","        raise ValueError(f\"Cannot transition from terminal status: {old_status}\")","","","def create_job(db_path: Path, spec: DBJobSpec, *, tags: list[str] | None = None) -> str:","    \"\"\""]}
{"type":"file_chunk","path":"src/control/jobs_db.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    Create a new job record.","    ","    Args:","        db_path: Path to SQLite database","        spec: Job specification","        tags: Optional list of tags for job categorization","        ","    Returns:","        Generated job_id","    \"\"\"","    job_id = str(uuid4())","    now = _now_iso()","    tags_json = json.dumps(tags if tags else [])","    ","    def _op() -> str:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            conn.execute(\"\"\"","                INSERT INTO jobs (","                    job_id, status, created_at, updated_at,","                    season, dataset_id, outputs_root, config_hash,","                    config_snapshot_json, requested_pause, tags_json, data_fingerprint_sha256_40","                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)","            \"\"\", (","                job_id,","                JobStatus.QUEUED.value,","                now,","                now,","                spec.season,","                spec.dataset_id,","                spec.outputs_root,","                spec.config_hash,","                json.dumps(spec.config_snapshot),","                0,","                tags_json,","                spec.data_fingerprint_sha256_40 if hasattr(spec, 'data_fingerprint_sha256_40') else '',","            ))","            conn.commit()","        return job_id","    ","    return _with_retry_locked(_op)","","","def _row_to_record(row: tuple) -> JobRecord:","    \"\"\"Convert database row to JobRecord.\"\"\"","    # Handle schema versions:","    # - Old: 12 columns (before report_link)","    # - Middle: 13 columns (with report_link, before run_id)","    # - New: 14 columns (with run_id and report_link)","    # - Latest: 15 columns (with tags_json)","    # - Phase 6.5: 16 columns (with data_fingerprint_sha1)","    if len(row) == 16:","        # Phase 6.5 schema with data_fingerprint_sha256_40","        (","            job_id,","            status,","            created_at,","            updated_at,","            season,","            dataset_id,","            outputs_root,","            config_hash,","            config_snapshot_json,","            pid,","            run_id,","            run_link,","            report_link,","            last_error,","            tags_json,","            data_fingerprint_sha256_40,","        ) = row","        # Parse tags_json, fallback to [] if None or invalid","        try:","            tags = json.loads(tags_json) if tags_json else []","            if not isinstance(tags, list):","                tags = []","        except (json.JSONDecodeError, TypeError):","            tags = []","        fingerprint_sha256_40 = data_fingerprint_sha256_40 if data_fingerprint_sha256_40 else \"\"","    elif len(row) == 15:","        # Latest schema with tags_json (without fingerprint column)","        (","            job_id,","            status,","            created_at,","            updated_at,","            season,","            dataset_id,","            outputs_root,","            config_hash,","            config_snapshot_json,","            pid,","            run_id,","            run_link,","            report_link,","            last_error,","            tags_json,","        ) = row","        # Parse tags_json, fallback to [] if None or invalid","        try:","            tags = json.loads(tags_json) if tags_json else []","            if not isinstance(tags, list):","                tags = []","        except (json.JSONDecodeError, TypeError):","            tags = []","        fingerprint_sha256_40 = \"\"  # Fallback for schema without data_fingerprint_sha256_40","    elif len(row) == 14:","        # New schema with run_id and report_link","        # Order: job_id, status, created_at, updated_at, season, dataset_id, outputs_root,","        #        config_hash, config_snapshot_json, pid, run_id, run_link, report_link, last_error","        (","            job_id,","            status,","            created_at,","            updated_at,","            season,","            dataset_id,","            outputs_root,","            config_hash,","            config_snapshot_json,","            pid,","            run_id,","            run_link,","            report_link,","            last_error,","        ) = row","        tags = []  # Fallback for schema without tags_json","        fingerprint_sha256_40 = \"\"  # Fallback for schema without data_fingerprint_sha256_40","    elif len(row) == 13:","        # Middle schema with report_link but no run_id","        (","            job_id,","            status,","            created_at,","            updated_at,","            season,","            dataset_id,","            outputs_root,","            config_hash,","            config_snapshot_json,","            pid,","            run_link,","            last_error,","            report_link,","        ) = row","        run_id = None","        tags = []  # Fallback for old schema","        fingerprint_sha256_40 = \"\"  # Fallback for schema without data_fingerprint_sha256_40","    else:","        # Old schema (backward compatibility)","        (","            job_id,","            status,","            created_at,","            updated_at,","            season,","            dataset_id,","            outputs_root,","            config_hash,","            config_snapshot_json,","            pid,","            run_link,","            last_error,","        ) = row","        run_id = None","        report_link = None","        tags = []  # Fallback for old schema","        fingerprint_sha256_40 = \"\"  # Fallback for schema without data_fingerprint_sha256_40","    ","    spec = DBJobSpec(","        season=season,","        dataset_id=dataset_id,","        outputs_root=outputs_root,","        config_snapshot=json.loads(config_snapshot_json),","        config_hash=config_hash,","        data_fingerprint_sha256_40=fingerprint_sha256_40,","    )","    ","    return JobRecord(","        job_id=job_id,","        status=JobStatus(status),","        created_at=created_at,","        updated_at=updated_at,","        spec=spec,","        pid=pid,","        run_id=run_id if run_id else None,","        run_link=run_link,","        report_link=report_link if report_link else None,","        last_error=last_error,","        tags=tags if tags else [],","        data_fingerprint_sha256_40=fingerprint_sha256_40,","    )","","","def get_job(db_path: Path, job_id: str) -> JobRecord:","    \"\"\"","    Get job record by ID.","    ","    Args:","        db_path: Path to SQLite database"]}
{"type":"file_chunk","path":"src/control/jobs_db.py","chunk_index":2,"line_start":401,"line_end":600,"content":["        job_id: Job ID","        ","    Returns:","        JobRecord","        ","    Raises:","        KeyError: If job not found","    \"\"\"","    def _op() -> JobRecord:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cursor = conn.execute(\"\"\"","                SELECT job_id, status, created_at, updated_at,","                       season, dataset_id, outputs_root, config_hash,","                       config_snapshot_json, pid,","                       COALESCE(run_id, NULL) as run_id,","                       run_link,","                       COALESCE(report_link, NULL) as report_link,","                       last_error,","                       COALESCE(tags_json, '[]') as tags_json,","                       COALESCE(data_fingerprint_sha256_40, '') as data_fingerprint_sha256_40","                FROM jobs","                WHERE job_id = ?","            \"\"\", (job_id,))","            row = cursor.fetchone()","            if row is None:","                raise KeyError(f\"Job not found: {job_id}\")","            return _row_to_record(row)","    ","    return _with_retry_locked(_op)","","","def list_jobs(db_path: Path, *, limit: int = 50) -> list[JobRecord]:","    \"\"\"","    List recent jobs.","    ","    Args:","        db_path: Path to SQLite database","        limit: Maximum number of jobs to return","        ","    Returns:","        List of JobRecord, ordered by created_at DESC","    \"\"\"","    def _op() -> list[JobRecord]:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cursor = conn.execute(\"\"\"","                SELECT job_id, status, created_at, updated_at,","                       season, dataset_id, outputs_root, config_hash,","                       config_snapshot_json, pid,","                       COALESCE(run_id, NULL) as run_id,","                       run_link,","                       COALESCE(report_link, NULL) as report_link,","                       last_error,","                       COALESCE(tags_json, '[]') as tags_json,","                       COALESCE(data_fingerprint_sha256_40, '') as data_fingerprint_sha256_40","                FROM jobs","                ORDER BY created_at DESC","                LIMIT ?","            \"\"\", (limit,))","            return [_row_to_record(row) for row in cursor.fetchall()]","    ","    return _with_retry_locked(_op)","","","def request_pause(db_path: Path, job_id: str, pause: bool) -> None:","    \"\"\"","    Request pause/unpause for a job (atomic update).","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        pause: True to pause, False to unpause","        ","    Raises:","        KeyError: If job not found","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET requested_pause = ?, updated_at = ?","                WHERE job_id = ?","            \"\"\", (1 if pause else 0, _now_iso(), job_id))","            ","            if cur.rowcount == 0:","                raise KeyError(f\"Job not found: {job_id}\")","            ","            conn.commit()","    ","    _with_retry_locked(_op)","","","def request_stop(db_path: Path, job_id: str, mode: StopMode) -> None:","    \"\"\"","    Request stop for a job (atomic update).","    ","    If QUEUED, immediately mark as KILLED.","    Otherwise, set requested_stop flag (worker will handle).","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        mode: Stop mode (SOFT or KILL)","        ","    Raises:","        KeyError: If job not found","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            # Try to mark QUEUED as KILLED first (atomic)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET status = ?, requested_stop = ?, updated_at = ?","                WHERE job_id = ? AND status = ?","            \"\"\", (JobStatus.KILLED.value, mode.value, _now_iso(), job_id, JobStatus.QUEUED.value))","            ","            if cur.rowcount == 1:","                conn.commit()","                return","            ","            # Otherwise, set requested_stop flag (atomic)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET requested_stop = ?, updated_at = ?","                WHERE job_id = ?","            \"\"\", (mode.value, _now_iso(), job_id))","            ","            if cur.rowcount == 0:","                raise KeyError(f\"Job not found: {job_id}\")","            ","            conn.commit()","    ","    _with_retry_locked(_op)","","","def mark_running(db_path: Path, job_id: str, *, pid: int) -> None:","    \"\"\"","    Mark job as RUNNING with PID (atomic update from QUEUED).","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        pid: Process ID","        ","    Raises:","        KeyError: If job not found","        ValueError: If status is terminal (DONE/FAILED/KILLED) or invalid transition","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET status = ?, pid = ?, updated_at = ?","                WHERE job_id = ? AND status = ?","            \"\"\", (JobStatus.RUNNING.value, pid, _now_iso(), job_id, JobStatus.QUEUED.value))","            ","            if cur.rowcount == 1:","                conn.commit()","                return","            ","            # Check if job exists and current status","            row = conn.execute(\"SELECT status FROM jobs WHERE job_id = ?\", (job_id,)).fetchone()","            if row is None:","                raise KeyError(f\"Job not found: {job_id}\")","            ","            status = JobStatus(row[0])","            ","            if status == JobStatus.RUNNING:","                # Already running (idempotent)","                return","            ","            # Terminal status => ValueError (match existing tests/contract)","            if status in {JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED}:","                raise ValueError(\"Cannot transition from terminal status\")","            ","            # Everything else is invalid transition (keep ValueError)","            raise ValueError(f\"Invalid status transition: {status.value} → RUNNING\")","    ","    _with_retry_locked(_op)","","","def update_running(db_path: Path, job_id: str, *, pid: int) -> None:","    \"\"\"","    Update job to RUNNING status with PID (legacy alias for mark_running).","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        pid: Process ID","        ","    Raises:","        KeyError: If job not found","        RuntimeError: If status transition is invalid","    \"\"\"","    mark_running(db_path, job_id, pid=pid)",""]}
{"type":"file_chunk","path":"src/control/jobs_db.py","chunk_index":3,"line_start":601,"line_end":800,"content":["","def update_run_link(db_path: Path, job_id: str, *, run_link: str) -> None:","    \"\"\"","    Update job run_link.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        run_link: Run link path","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            conn.execute(\"\"\"","                UPDATE jobs","                SET run_link = ?, updated_at = ?","                WHERE job_id = ?","            \"\"\", (run_link, _now_iso(), job_id))","            conn.commit()","    ","    _with_retry_locked(_op)","","","def set_report_link(db_path: Path, job_id: str, report_link: str) -> None:","    \"\"\"","    Set report_link for a job.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        report_link: Report link URL","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            conn.execute(\"\"\"","                UPDATE jobs","                SET report_link = ?, updated_at = ?","                WHERE job_id = ?","            \"\"\", (report_link, _now_iso(), job_id))","            conn.commit()","    ","    _with_retry_locked(_op)","","","def mark_done(","    db_path: Path, ","    job_id: str, ","    *, ","    run_id: Optional[str] = None,","    report_link: Optional[str] = None",") -> None:","    \"\"\"","    Mark job as DONE (atomic update from RUNNING or KILLED).","    ","    Idempotent: safe to call multiple times.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        run_id: Optional final stage run_id","        report_link: Optional report link URL","        ","    Raises:","        KeyError: If job not found","        RuntimeError: If status is QUEUED/PAUSED (mark_done before RUNNING)","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET status = ?, updated_at = ?, run_id = ?, report_link = ?, last_error = NULL","                WHERE job_id = ? AND status IN (?, ?)","            \"\"\", (","                JobStatus.DONE.value,","                _now_iso(),","                run_id,","                report_link,","                job_id,","                JobStatus.RUNNING.value,","                JobStatus.KILLED.value,","            ))","            ","            if cur.rowcount == 1:","                conn.commit()","                return","            ","            # Fallback: check if already DONE (idempotent success)","            row = conn.execute(\"SELECT status FROM jobs WHERE job_id = ?\", (job_id,)).fetchone()","            if row is None:","                raise KeyError(f\"Job not found: {job_id}\")","            ","            status = JobStatus(row[0])","            if status == JobStatus.DONE:","                # Already done (idempotent)","                return","            ","            # If QUEUED/PAUSED, raise RuntimeError (process flow incorrect)","            raise RuntimeError(f\"mark_done rejected: status={status} (expected RUNNING or KILLED)\")","    ","    _with_retry_locked(_op)","","","def mark_failed(db_path: Path, job_id: str, *, error: str) -> None:","    \"\"\"","    Mark job as FAILED with error message (atomic update from RUNNING or PAUSED).","    ","    Idempotent: safe to call multiple times.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        error: Error message","        ","    Raises:","        KeyError: If job not found","        RuntimeError: If status is QUEUED (mark_failed before RUNNING)","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET status = ?, last_error = ?, updated_at = ?","                WHERE job_id = ? AND status IN (?, ?)","            \"\"\", (","                JobStatus.FAILED.value,","                error,","                _now_iso(),","                job_id,","                JobStatus.RUNNING.value,","                JobStatus.PAUSED.value,","            ))","            ","            if cur.rowcount == 1:","                conn.commit()","                return","            ","            # Fallback: check if already FAILED (idempotent success)","            row = conn.execute(\"SELECT status FROM jobs WHERE job_id = ?\", (job_id,)).fetchone()","            if row is None:","                raise KeyError(f\"Job not found: {job_id}\")","            ","            status = JobStatus(row[0])","            if status == JobStatus.FAILED:","                # Already failed (idempotent)","                return","            ","            # If QUEUED, raise RuntimeError (process flow incorrect)","            raise RuntimeError(f\"mark_failed rejected: status={status} (expected RUNNING or PAUSED)\")","    ","    _with_retry_locked(_op)","","","def mark_killed(db_path: Path, job_id: str, *, error: str | None = None) -> None:","    \"\"\"","    Mark job as KILLED (atomic update).","    ","    Idempotent: safe to call multiple times.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        error: Optional error message","        ","    Raises:","        KeyError: If job not found","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET status = ?, last_error = ?, updated_at = ?","                WHERE job_id = ?","            \"\"\", (JobStatus.KILLED.value, error, _now_iso(), job_id))","            ","            if cur.rowcount == 0:","                raise KeyError(f\"Job not found: {job_id}\")","            ","            conn.commit()","    ","    _with_retry_locked(_op)","","","def get_requested_stop(db_path: Path, job_id: str) -> Optional[str]:","    \"\"\"","    Get requested_stop value for a job.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        ","    Returns:","        Stop mode string or None","    \"\"\"","    def _op() -> Optional[str]:","        with _connect(db_path) as conn:","            ensure_schema(conn)"]}
{"type":"file_chunk","path":"src/control/jobs_db.py","chunk_index":4,"line_start":801,"line_end":927,"content":["            cursor = conn.execute(\"SELECT requested_stop FROM jobs WHERE job_id = ?\", (job_id,))","            row = cursor.fetchone()","            return row[0] if row and row[0] else None","    ","    return _with_retry_locked(_op)","","","def get_requested_pause(db_path: Path, job_id: str) -> bool:","    \"\"\"","    Get requested_pause value for a job.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        ","    Returns:","        True if pause requested, False otherwise","    \"\"\"","    def _op() -> bool:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cursor = conn.execute(\"SELECT requested_pause FROM jobs WHERE job_id = ?\", (job_id,))","            row = cursor.fetchone()","            return bool(row[0]) if row else False","    ","    return _with_retry_locked(_op)","","","def search_by_tag(db_path: Path, tag: str, *, limit: int = 50) -> list[JobRecord]:","    \"\"\"","    Search jobs by tag.","    ","    Uses LIKE query to find jobs containing the tag in tags_json.","    For exact matching, use application-layer filtering.","    ","    Args:","        db_path: Path to SQLite database","        tag: Tag to search for","        limit: Maximum number of jobs to return","        ","    Returns:","        List of JobRecord matching the tag, ordered by created_at DESC","    \"\"\"","    def _op() -> list[JobRecord]:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            # Use LIKE to search for tag in JSON array","            # Pattern: tag can appear as [\"tag\"] or [\"tag\", ...] or [..., \"tag\", ...] or [..., \"tag\"]","            search_pattern = f'%\"{tag}\"%'","            cursor = conn.execute(\"\"\"","                SELECT job_id, status, created_at, updated_at,","                       season, dataset_id, outputs_root, config_hash,","                       config_snapshot_json, pid,","                       COALESCE(run_id, NULL) as run_id,","                       run_link,","                       COALESCE(report_link, NULL) as report_link,","                       last_error,","                       COALESCE(tags_json, '[]') as tags_json,","                       COALESCE(data_fingerprint_sha256_40, '') as data_fingerprint_sha256_40","                FROM jobs","                WHERE tags_json LIKE ?","                ORDER BY created_at DESC","                LIMIT ?","            \"\"\", (search_pattern, limit))","            ","            records = [_row_to_record(row) for row in cursor.fetchall()]","            ","            # Application-layer filtering for exact match (more reliable than LIKE)","            # Filter to ensure tag is actually in the list, not just substring match","            filtered = []","            for record in records:","                if tag in record.tags:","                    filtered.append(record)","            ","            return filtered","    ","    return _with_retry_locked(_op)","","","def append_log(db_path: Path, job_id: str, log_text: str) -> None:","    \"\"\"","    Append log entry to job_logs table.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        log_text: Log text to append (can be full traceback)","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            conn.execute(\"\"\"","                INSERT INTO job_logs (job_id, created_at, log_text)","                VALUES (?, ?, ?)","            \"\"\", (job_id, _now_iso(), log_text))","            conn.commit()","    ","    _with_retry_locked(_op)","","","def get_job_logs(db_path: Path, job_id: str, *, limit: int = 100) -> list[str]:","    \"\"\"","    Get log entries for a job.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        limit: Maximum number of log entries to return","        ","    Returns:","        List of log text entries, ordered by created_at DESC","    \"\"\"","    def _op() -> list[str]:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cursor = conn.execute(\"\"\"","                SELECT log_text","                FROM job_logs","                WHERE job_id = ?","                ORDER BY created_at DESC","                LIMIT ?","            \"\"\", (job_id, limit))","            return [row[0] for row in cursor.fetchall()]","    ","    return _with_retry_locked(_op)","",""]}
{"type":"file_footer","path":"src/control/jobs_db.py","complete":true,"emitted_chunks":5}
{"type":"file_header","path":"src/control/lifecycle.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":17428,"sha256":"ad6ef8c56bb68140187d25f653b10f24f8e929d234e3535ca14dca0638a8e65a","total_lines":551,"chunk_count":3}
{"type":"file_chunk","path":"src/control/lifecycle.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Lifecycle Root-Cure: Identity-aware preflight for Control API (8000) and UI (8080).","","Core principles:","1. Never blindly kill - always verify identity first","2. Default safe behavior: fail-fast with actionable diagnostics","3. Operator-proof: clear decisions and recovery steps","4. Flat snapshots only (no subfolders)","\"\"\"","","from __future__ import annotations","","import dataclasses","import json","import os","import re","import signal","import subprocess","import sys","import time","from dataclasses import dataclass","from enum import Enum","from pathlib import Path","from typing import Optional, Tuple, Dict, Any, List","","import requests","from requests.exceptions import RequestException, Timeout","","# Try to import psutil for process info (optional)","try:","    import psutil","    HAS_PSUTIL = True","except ImportError:","    HAS_PSUTIL = False","","","class PortOccupancyStatus(Enum):","    \"\"\"Status of port occupancy check.\"\"\"","    FREE = \"FREE\"","    OCCUPIED_FISHBRO = \"OCCUPIED_FISHBRO\"","    OCCUPIED_NOT_FISHBRO = \"OCCUPIED_NOT_FISHBRO\"","    OCCUPIED_UNKNOWN = \"OCCUPIED_UNKNOWN\"","","","@dataclass","class PortOccupant:","    \"\"\"Information about a port occupant.\"\"\"","    occupied: bool","    pid: Optional[int] = None","    process_name: Optional[str] = None","    cmdline: Optional[str] = None","    raw_output: str = \"\"","    ","    @classmethod","    def free(cls) -> PortOccupant:","        \"\"\"Create a PortOccupant representing a free port.\"\"\"","        return cls(occupied=False, raw_output=\"Port is free\")","","","@dataclass","class PortPreflightResult:","    \"\"\"Result of port preflight check.\"\"\"","    port: int","    status: PortOccupancyStatus","    occupant: PortOccupant","    identity_verified: bool = False","    identity_error: Optional[str] = None","    identity_data: Optional[Dict[str, Any]] = None","    decision: str = \"PENDING\"","    action: str = \"\"","","","def extract_listen_pids_from_ss(ss_text: str, port: int) -> List[int]:","    \"\"\"","    Parse `ss -ltnp` output and return unique PIDs listening on the given port.","    ","    Supports patterns like:","      users:((\"python3\",pid=73466,fd=13))","    Return [] if none.","    \"\"\"","    pids: set[int] = set()","    for line in ss_text.splitlines():","        if f\":{port} \" not in line and not line.strip().endswith(f\":{port}\"):","            continue","        for m in re.finditer(r\"pid=(\\d+)\", line):","            pids.add(int(m.group(1)))","    return sorted(pids)","","","def get_process_identity(pid: int) -> Dict[str, str]:","    \"\"\"","    Use psutil (preferred) or /proc/{pid}/cmdline, /proc/{pid}/cwd to return:","      - exe","      - cmdline (joined)","      - cwd","    Never throws; returns best-effort dict.","    \"\"\"","    result = {\"pid\": str(pid), \"exe\": \"\", \"cmdline\": \"\", \"cwd\": \"\"}","    ","    # Try psutil first","    if HAS_PSUTIL:","        try:","            proc = psutil.Process(pid)","            result[\"exe\"] = proc.exe() or \"\"","            result[\"cmdline\"] = \" \".join(proc.cmdline())","            result[\"cwd\"] = proc.cwd() or \"\"","            return result","        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):","            pass","    ","    # Fallback to /proc filesystem (Linux)","    try:","        cmdline_path = Path(f\"/proc/{pid}/cmdline\")","        if cmdline_path.exists():","            cmdline_bytes = cmdline_path.read_bytes()","            parts = [p.decode(\"utf-8\", errors=\"replace\") for p in cmdline_bytes.split(b\"\\x00\") if p]","            result[\"cmdline\"] = \" \".join(parts)","    except Exception:","        pass","    ","    try:","        exe_path = Path(f\"/proc/{pid}/exe\")","        if exe_path.exists():","            result[\"exe\"] = str(exe_path.resolve())","    except Exception:","        pass","    ","    try:","        cwd_path = Path(f\"/proc/{pid}/cwd\")","        if cwd_path.exists():","            result[\"cwd\"] = str(cwd_path.resolve())","    except Exception:","        pass","    ","    return result","","","def detect_port_occupant(port: int) -> PortOccupant:","    \"\"\"","    Detect if a port is occupied and return occupant information.","    ","    Uses enhanced detection strategy:","    1. ss -ltnp (primary, shows PID)","    2. /proc/<pid>/cmdline for identity","    3. HTTP identity probe (Control API only)","    4. lsof -iTCP:<port> -sTCP:LISTEN (fallback only)","    ","    Returns PortOccupant with best available information.","    \"\"\"","    # Try ss first (mandatory)","    ss_cmd = [\"bash\", \"-lc\", f\"ss -ltnp '( sport = :{port} )'\"]","    try:","        ss_output = subprocess.check_output(","            ss_cmd, stderr=subprocess.STDOUT, text=True, timeout=2","        ).strip()","    except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):","        ss_output = \"\"","    ","    # Parse ss output for PIDs","    pids = extract_listen_pids_from_ss(ss_output, port)","    ","    if pids:","        # Use first PID (most relevant)","        pid = pids[0]","        identity = get_process_identity(pid)","        cmdline = identity.get(\"cmdline\", \"\")","        process_name = identity.get(\"exe\", \"\").split(\"/\")[-1] if identity.get(\"exe\") else \"\"","        ","        return PortOccupant(","            occupied=True,","            pid=pid,","            process_name=process_name or f\"pid:{pid}\",","            cmdline=cmdline,","            raw_output=ss_output","        )","    ","    # Try lsof as fallback only (when ss fails)","    lsof_cmd = [\"bash\", \"-lc\", f\"lsof -iTCP:{port} -sTCP:LISTEN -n -P\"]","    try:","        lsof_output = subprocess.check_output(","            lsof_cmd, stderr=subprocess.STDOUT, text=True, timeout=2","        ).strip()","    except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):","        lsof_output = \"\"","    ","    if lsof_output and \"LISTEN\" in lsof_output:","        # Parse lsof output: COMMAND PID USER ...","        lines = lsof_output.splitlines()","        if len(lines) > 1:  # Skip header","            parts = lines[1].split()","            if len(parts) >= 2:","                try:","                    pid = int(parts[1])","                    process_name = parts[0]","                    # Try to get cmdline from /proc","                    identity = get_process_identity(pid)","                    cmdline = identity.get(\"cmdline\", \"\")","                    ","                    return PortOccupant("]}
{"type":"file_chunk","path":"src/control/lifecycle.py","chunk_index":1,"line_start":201,"line_end":400,"content":["                        occupied=True,","                        pid=pid,","                        process_name=process_name,","                        cmdline=cmdline,","                        raw_output=f\"ss: {ss_output}\\nlsof: {lsof_output}\"","                    )","                except (ValueError, IndexError):","                    pass","    ","    # If we get here, port might be free or we couldn't parse","    if ss_output or lsof_output:","        # Output exists but we couldn't parse PID","        return PortOccupant(","            occupied=True,","            raw_output=f\"ss: {ss_output}\\nlsof: {lsof_output}\"","        )","    ","    # Port appears free","    return PortOccupant.free()","","","def verify_fishbro_control_identity(host: str, port: int, timeout: float = 2.0) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:","    \"\"\"","    Verify if occupant on port is FishBro Control API.","    ","    Checks GET /__identity endpoint for:","    - service_name == \"control_api\"","    - repo_root matches current repo","    ","    Returns (is_fishbro, identity_data, error_message)","    \"\"\"","    url = f\"http://{host}:{port}/__identity\"","    ","    try:","        response = requests.get(url, timeout=timeout)","        if response.status_code != 200:","            return False, None, f\"HTTP {response.status_code}\"","        ","        data = response.json()","        ","        # Check service name","        if data.get(\"service_name\") != \"control_api\":","            return False, data, f\"service_name is '{data.get('service_name')}', not 'control_api'\"","        ","        # Check repo root (best effort)","        expected_repo_root = str(Path(__file__).parent.parent.parent.absolute())","        actual_repo_root = data.get(\"repo_root\", \"\")","        if actual_repo_root and expected_repo_root not in actual_repo_root:","            # Not a strict match, but should contain our repo path","            return False, data, f\"repo_root mismatch: {actual_repo_root}\"","        ","        return True, data, None","        ","    except Timeout:","        return False, None, \"Timeout connecting to identity endpoint\"","    except RequestException as e:","        return False, None, f\"Connection error: {e}\"","    except json.JSONDecodeError as e:","        return False, None, f\"Invalid JSON response: {e}\"","    except Exception as e:","        return False, None, f\"Unexpected error: {e}\"","","","def verify_fishbro_ui_identity(occupant: PortOccupant) -> Tuple[bool, Optional[str]]:","    \"\"\"","    Verify if occupant on port is FishBro UI.","    ","    Checks cmdline for FishBro module patterns.","    \"\"\"","    if not occupant.cmdline:","        return False, \"No cmdline available\"","    ","    cmdline = occupant.cmdline.lower()","    ","    # Look for FishBro UI module patterns","    ui_patterns = [","        \"fishbrowfs_v2.gui.nicegui.app\",","        \"fishbrowfs_v2/gui/nicegui/app.py\",","        \"nicegui.app\",","    ]","    ","    for pattern in ui_patterns:","        if pattern in cmdline:","            return True, None","    ","    return False, f\"Cmdline doesn't match FishBro UI patterns: {occupant.cmdline[:100]}...\"","","","def preflight_port(","    port: int,","    host: str = \"127.0.0.1\",","    service_type: str = \"control\",  # \"control\" or \"ui\"","    timeout: float = 2.0,","    single_user_mode: bool = False",") -> PortPreflightResult:","    \"\"\"","    Perform identity-aware preflight for a port.","    ","    Steps:","    1. Detect port occupancy","    2. If occupied, verify identity","    3. Determine status and decision","    ","    Classification Rules:","    - PID found + cmdline matches FishBro → OCCUPIED_FISHBRO","    - PID found + cmdline NOT FishBro → OCCUPIED_NOT_FISHBRO","    - PID missing OR cmdline unreadable → OCCUPIED_UNKNOWN","    ","    Single-User Mode Rules:","    - OCCUPIED_FISHBRO: Keep/restart as requested","    - OCCUPIED_UNKNOWN: DO NOT FAIL — continue","    - OCCUPIED_NOT_FISHBRO: Fail unless --force-kill-ports","    \"\"\"","    occupant = detect_port_occupant(port)","    ","    if not occupant.occupied:","        return PortPreflightResult(","            port=port,","            status=PortOccupancyStatus.FREE,","            occupant=occupant,","            decision=\"START\",","            action=\"Port is free, can start service\"","        )","    ","    # Port is occupied, need to classify","    status = PortOccupancyStatus.OCCUPIED_UNKNOWN","    identity_verified = False","    identity_error = None","    identity_data = None","    ","    if occupant.pid:","        # We have a PID, try to verify identity","        try:","            if service_type == \"control\":","                is_fishbro, data, error = verify_fishbro_control_identity(host, port, timeout)","                identity_verified = is_fishbro","                identity_error = error","                identity_data = data","            else:  # UI","                is_fishbro, error = verify_fishbro_ui_identity(occupant)","                identity_verified = is_fishbro","                identity_error = error","            ","            if identity_verified:","                status = PortOccupancyStatus.OCCUPIED_FISHBRO","            elif occupant.cmdline:","                # We have cmdline but it's not FishBro","                status = PortOccupancyStatus.OCCUPIED_NOT_FISHBRO","            else:","                # PID exists but cmdline unreadable","                status = PortOccupancyStatus.OCCUPIED_UNKNOWN","        except Exception as e:","            # Identity probe failed (exception)","            identity_error = f\"Identity verification failed: {e}\"","            status = PortOccupancyStatus.OCCUPIED_UNKNOWN","    else:","        # No PID found","        status = PortOccupancyStatus.OCCUPIED_UNKNOWN","    ","    # Determine decision based on classification and single-user mode","    decision = \"PENDING\"","    action = \"\"","    ","    if status == PortOccupancyStatus.OCCUPIED_FISHBRO:","        decision = \"REUSE\"","        action = f\"Port occupied by FishBro {service_type}, will reuse\"","    elif status == PortOccupancyStatus.OCCUPIED_UNKNOWN:","        if single_user_mode:","            decision = \"CONTINUE\"","            action = f\"Port occupied by unknown process (single-user mode), will continue\"","        else:","            decision = \"FAIL_FAST\"","            action = f\"Port occupied by unknown process, cannot verify identity\"","    else:  # OCCUPIED_NOT_FISHBRO","        if single_user_mode:","            decision = \"FAIL_FAST\"","            action = f\"Port occupied by non-FishBro process (PID {occupant.pid})\"","        else:","            decision = \"FAIL_FAST\"","            action = f\"Port occupied by non-FishBro process (PID {occupant.pid})\"","    ","    return PortPreflightResult(","        port=port,","        status=status,","        occupant=occupant,","        identity_verified=identity_verified,","        identity_error=identity_error,","        identity_data=identity_data,","        decision=decision,","        action=action","    )","","","def kill_process(pid: int, force: bool = True) -> bool:","    \"\"\"","    Kill a process by PID.","    ","    Args:","        pid: Process ID to kill","        force: If True, use SIGKILL after SIGTERM fails (default True)"]}
{"type":"file_chunk","path":"src/control/lifecycle.py","chunk_index":2,"line_start":401,"line_end":551,"content":["    ","    Returns:","        True if process was killed or already dead, False on permission error","    \"\"\"","    if not HAS_PSUTIL:","        # Fallback to os.kill","        try:","            os.kill(pid, signal.SIGTERM)","            time.sleep(1)","            # Check if still alive","            try:","                os.kill(pid, 0)  # Check if process exists","                if force:","                    os.kill(pid, signal.SIGKILL)","                    time.sleep(0.5)","                return True","            except OSError:","                # Process is dead after SIGTERM","                return True","        except ProcessLookupError:","            # Process already dead","            return True","        except PermissionError:","            # Permission denied","            return False","        except OSError:","            # Other OSError","            return False","    ","    # Use psutil if available","    try:","        proc = psutil.Process(pid)","        proc.terminate()","        gone, alive = psutil.wait_procs([proc], timeout=2)","        if alive and force:","            for p in alive:","                p.kill()","            psutil.wait_procs(alive, timeout=1)","        return True","    except (psutil.NoSuchProcess, psutil.AccessDenied):","        # Process already dead or permission denied","        return True","","","def write_pidfile(pid: int, service: str, pid_dir: Path) -> Path:","    \"\"\"","    Write PID file atomically.","    ","    Args:","        pid: Process ID","        service: Service name (\"control\" or \"ui\")","        pid_dir: Directory for PID files","    ","    Returns:","        Path to PID file","    \"\"\"","    pid_dir.mkdir(parents=True, exist_ok=True)","    pidfile = pid_dir / f\"{service}.pid\"","    ","    # Write atomically via temp file","    tempfile = pidfile.with_suffix(\".pid.tmp\")","    tempfile.write_text(str(pid))","    tempfile.rename(pidfile)","    ","    return pidfile","","","def read_pidfile(service: str, pid_dir: Path) -> Optional[int]:","    \"\"\"","    Read PID from PID file.","    ","    Returns:","        PID if file exists and contains valid integer, None otherwise","    \"\"\"","    pidfile = pid_dir / f\"{service}.pid\"","    if not pidfile.exists():","        return None","    ","    try:","        pid_str = pidfile.read_text().strip()","        return int(pid_str)","    except (ValueError, OSError):","        return None","","","def remove_pidfile(service: str, pid_dir: Path) -> bool:","    \"\"\"Remove PID file if it exists.\"\"\"","    pidfile = pid_dir / f\"{service}.pid\"","    if pidfile.exists():","        try:","            pidfile.unlink()","            return True","        except OSError:","            return False","    return True","","","def write_metadata(pid: int, service: str, pid_dir: Path, metadata: Dict[str, Any]) -> Path:","    \"\"\"","    Write metadata JSON file for a service.","    ","    Args:","        pid: Process ID","        service: Service name","        pid_dir: Directory for PID files","        metadata: Metadata dict to write","    ","    Returns:","        Path to metadata file","    \"\"\"","    pid_dir.mkdir(parents=True, exist_ok=True)","    metafile = pid_dir / f\"{service}.meta.json\"","    ","    metadata.update({","        \"pid\": pid,","        \"service\": service,","        \"written_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),","    })","    ","    # Write atomically","    tempfile = metafile.with_suffix(\".json.tmp\")","    tempfile.write_text(json.dumps(metadata, indent=2))","    tempfile.rename(metafile)","    ","    return metafile","","","if __name__ == \"__main__\":","    # Test the module","    import argparse","    ","    parser = argparse.ArgumentParser(description=\"Test port preflight\")","    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to check\")","    parser.add_argument(\"--service\", choices=[\"control\", \"ui\"], default=\"control\", help=\"Service type\")","    ","    args = parser.parse_args()","    ","    print(f\"Preflight check for port {args.port} ({args.service}):\")","    result = preflight_port(args.port, service_type=args.service)","    ","    print(f\"  Status: {result.status.value}\")","    print(f\"  Occupied: {result.occupant.occupied}\")","    if result.occupant.occupied:","        print(f\"  PID: {result.occupant.pid}\")","        print(f\"  Process: {result.occupant.process_name}\")","        print(f\"  Cmdline: {result.occupant.cmdline}\")","    print(f\"  Identity verified: {result.identity_verified}\")","    if result.identity_error:","        print(f\"  Identity error: {result.identity_error}\")","    print(f\"  Decision: {result.decision}\")","    print(f\"  Action: {result.action}\")"]}
{"type":"file_footer","path":"src/control/lifecycle.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/control/local_scan.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6888,"sha256":"6c01e93ae2ce030e01c1a8649c9aa7c014ae02cf3f2ddfe73b0f4115f17db9c6","total_lines":233,"chunk_count":2}
{"type":"file_chunk","path":"src/control/local_scan.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Local-Strict filesystem scanner (NOT Git).","","Mission: Enumerate files based on filesystem truth, ignoring .gitignore,","respecting allowlist/denylist + caps to prevent explosion.","","Contract:","- MUST NOT rely on git ls-files or tracked-only enumeration.","- MUST include untracked files in allowed roots.","- MUST ignore .gitignore (gitignore_respected=false).","- MUST enforce allowlist + denylist + caps to prevent explosion.","- MUST write audit rules to outputs/snapshots/full/LOCAL_SCAN_RULES.json.","\"\"\"","","from __future__ import annotations","from dataclasses import dataclass","from fnmatch import fnmatch","import hashlib","import json","from pathlib import Path","from typing import Iterator, Tuple, List, Set, Optional","import os","","","@dataclass(frozen=True)","class LocalScanPolicy:","    \"\"\"Policy for local-strict filesystem scanning.\"\"\"","    allowed_roots: tuple[str, ...]","    allowed_root_files_glob: tuple[str, ...]","    deny_segments: tuple[str, ...]","    outputs_allow: tuple[str, ...]","    max_files: int","    max_bytes: int","    gitignore_respected: bool","","","def default_local_strict_policy() -> LocalScanPolicy:","    \"\"\"Return policy with the defaults defined in the spec.\"\"\"","    return LocalScanPolicy(","        allowed_roots=(\"src\", \"tests\", \"scripts\", \"docs\"),","        allowed_root_files_glob=(","            \"Makefile\",","            \"pyproject.toml\",","            \"README*\",","            \".python-version\",","            \"requirements*.txt\",","            \"uv.lock\",","            \"poetry.lock\",","        ),","        deny_segments=(","            \".git\",","            \".venv\",","            \"venv\",","            \"node_modules\",","            \"__pycache__\",","            \".pytest_cache\",","            \".mypy_cache\",","            \".ruff_cache\",","            \".cache\",","            \".idea\",","            \".vscode\",","            \"outputs\",  # will be handled by outputs exception rule","        ),","        outputs_allow=(\"outputs/snapshots\",),","        max_files=20000,","        max_bytes=2_000_000,  # 2MB","        gitignore_respected=False,","    )","","","def _has_deny_segment(rel: Path, deny: tuple[str, ...]) -> bool:","    \"\"\"Check if any path segment matches a deny segment.\"\"\"","    parts = rel.parts","    return any(seg in parts for seg in deny)","","","def should_include_file(rel_path: Path, policy: LocalScanPolicy) -> bool:","    \"\"\"","    Pure include/exclude decision.","    ","    Returns True if the file should be included in the scan.","    \"\"\"","    p = rel_path.as_posix()","    ","    # Root allowlist - files directly in repo root","    if \"/\" not in p:","        return any(fnmatch(rel_path.name, g) for g in policy.allowed_root_files_glob)","    ","    top = rel_path.parts[0]","    ","    # outputs exception rule","    if top == \"outputs\":","        # Check if path starts with any allowed outputs subdirectory","        return any(p.startswith(allowed + \"/\") or p == allowed ","                   for allowed in policy.outputs_allow)","    ","    # allowed roots only","    if top not in policy.allowed_roots:","        return False","    ","    # deny segments anywhere in path","    if _has_deny_segment(rel_path, policy.deny_segments):","        return False","    ","    return True","","","def iter_repo_files_local_strict(","    repo_root: Path,","    policy: LocalScanPolicy,",") -> List[Path]:","    \"\"\"","    Return deterministic sorted list of included files, relative to repo_root.","    ","    Walks the filesystem, applies include/exclude rules, respects caps.","    \"\"\"","    included: List[Path] = []","    ","    # Walk through all allowed roots and root files","    candidates: Set[Path] = set()","    ","    # Add root files","    for pattern in policy.allowed_root_files_glob:","        for path in repo_root.glob(pattern):","            if path.is_file():","                rel = path.relative_to(repo_root)","                candidates.add(rel)","    ","    # Add files from allowed roots","    for root_dir in policy.allowed_roots:","        root_path = repo_root / root_dir","        if not root_path.exists():","            continue","        for path in root_path.rglob(\"*\"):","            if not path.is_file():","                continue","            rel = path.relative_to(repo_root)","            candidates.add(rel)","    ","    # Add outputs exception files","    for allowed_output in policy.outputs_allow:","        output_path = repo_root / allowed_output","        if not output_path.exists():","            continue","        for path in output_path.rglob(\"*\"):","            if not path.is_file():","                continue","            rel = path.relative_to(repo_root)","            candidates.add(rel)","    ","    # Apply include/exclude filter","    for rel in candidates:","        if should_include_file(rel, policy):","            included.append(rel)","    ","    # Sort deterministically","    included.sort(key=lambda p: p.as_posix())","    ","    # Apply max_files cap","    if len(included) > policy.max_files:","        included = included[:policy.max_files]","    ","    return included","","","def write_local_scan_rules(","    policy: LocalScanPolicy,","    output_path: Path,","    repo_root: Optional[Path] = None,",") -> Path:","    \"\"\"","    Write LOCAL_SCAN_RULES.json with policy and metadata.","    ","    Args:","        policy: The scan policy to serialize.","        output_path: Where to write the JSON file.","        repo_root: Optional repo root for computing relative paths.","    ","    Returns:","        Path to the written file.","    \"\"\"","    import datetime","    ","    data = {","        \"mode\": \"local-strict\",","        \"generated_at_utc\": datetime.datetime.now(datetime.timezone.utc).isoformat(),","        \"allowed_roots\": list(policy.allowed_roots),","        \"allowed_root_files_glob\": list(policy.allowed_root_files_glob),","        \"deny_segments\": list(policy.deny_segments),","        \"outputs_allow\": list(policy.outputs_allow),","        \"max_files\": policy.max_files,","        \"max_bytes\": policy.max_bytes,","        \"gitignore_respected\": policy.gitignore_respected,","    }","    ","    if repo_root:","        data[\"repo_root\"] = str(repo_root.absolute())","    ","    output_path.parent.mkdir(parents=True, exist_ok=True)"]}
{"type":"file_chunk","path":"src/control/local_scan.py","chunk_index":1,"line_start":201,"line_end":233,"content":["    with open(output_path, \"w\", encoding=\"utf-8\") as f:","        json.dump(data, f, indent=2, ensure_ascii=False)","    ","    return output_path","","","def compute_policy_hash(policy_path: Path) -> str:","    \"\"\"","    Compute SHA256 hash of LOCAL_SCAN_RULES.json.","    ","    Returns \"UNKNOWN\" if file doesn't exist or can't be read.","    \"\"\"","    if not policy_path.exists():","        return \"UNKNOWN\"","    ","    try:","        with open(policy_path, \"rb\") as f:","            return hashlib.sha256(f.read()).hexdigest()","    except Exception:","        return \"UNKNOWN\"","","","if __name__ == \"__main__\":","    # Simple test when run directly","    import sys","    repo = Path.cwd()","    policy = default_local_strict_policy()","    files = iter_repo_files_local_strict(repo, policy)","    print(f\"Found {len(files)} files with local-strict policy\")","    for f in files[:10]:","        print(f\"  {f}\")","    if len(files) > 10:","        print(f\"  ... and {len(files) - 10} more\")"]}
{"type":"file_footer","path":"src/control/local_scan.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/param_grid.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12329,"sha256":"a443a9098fea51948460ddcc094f1b1294811762b50186372af0762ba7510c62","total_lines":330,"chunk_count":2}
{"type":"file_chunk","path":"src/control/param_grid.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Parameter Grid Expansion for Phase 13.","","Pure functions for turning ParamSpec + user grid config into value lists.","Deterministic ordering, no floating drift surprises.","\"\"\"","","from __future__ import annotations","","import math","from enum import Enum","from typing import Any","","from pydantic import BaseModel, ConfigDict, Field, ValidationError, field_validator","","from strategy.param_schema import ParamSpec","","","class GridMode(str, Enum):","    \"\"\"Grid expansion mode.\"\"\"","    SINGLE = \"single\"","    RANGE = \"range\"","    MULTI = \"multi\"","","","class ParamGridSpec(BaseModel):","    \"\"\"User-defined grid specification for a single parameter.","    ","    Exactly one of the three modes must be active.","    \"\"\"","    ","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","    ","    mode: GridMode = Field(","        ...,","        description=\"Grid expansion mode\"","    )","    ","    single_value: Any | None = Field(","        default=None,","        description=\"Single value for mode='single'\"","    )","    ","    range_start: float | int | None = Field(","        default=None,","        description=\"Start of range (inclusive) for mode='range'\"","    )","    ","    range_end: float | int | None = Field(","        default=None,","        description=\"End of range (inclusive) for mode='range'\"","    )","    ","    range_step: float | int | None = Field(","        default=None,","        description=\"Step size for mode='range'\"","    )","    ","    multi_values: list[Any] | None = Field(","        default=None,","        description=\"List of values for mode='multi'\"","    )","    ","    @field_validator(\"mode\", mode=\"before\")","    @classmethod","    def validate_mode(cls, v: Any) -> GridMode:","        if isinstance(v, str):","            v = v.lower()","        return GridMode(v)","    ","    @field_validator(\"single_value\", \"range_start\", \"range_end\", \"range_step\", \"multi_values\", mode=\"after\")","    @classmethod","    def validate_mode_consistency(cls, v: Any, info) -> Any:","        \"\"\"Ensure only fields relevant to the active mode are set.\"\"\"","        mode = info.data.get(\"mode\")","        if mode is None:","            return v","        ","        field_name = info.field_name","        ","        # Map fields to allowed modes","        allowed_for = {","            \"single_value\": [GridMode.SINGLE],","            \"range_start\": [GridMode.RANGE],","            \"range_end\": [GridMode.RANGE],","            \"range_step\": [GridMode.RANGE],","            \"multi_values\": [GridMode.MULTI],","        }","        ","        if field_name in allowed_for:","            if mode not in allowed_for[field_name]:","                if v is not None:","                    raise ValueError(","                        f\"Field '{field_name}' must be None when mode='{mode.value}'\"","                    )","            else:","                if v is None:","                    raise ValueError(","                        f\"Field '{field_name}' must be set when mode='{mode.value}'\"","                    )","        return v","    ","    @field_validator(\"range_step\")","    @classmethod","    def validate_range_step(cls, v: float | int | None) -> float | int | None:","        # Allow zero step; validation will be done in validate_grid_for_param","        return v","    ","    @field_validator(\"range_start\", \"range_end\")","    @classmethod","    def validate_range_order(cls, v: float | int | None, info) -> float | int | None:","        # Allow start > end; validation will be done in validate_grid_for_param","        return v","    ","    @field_validator(\"multi_values\")","    @classmethod","    def validate_multi_values(cls, v: list[Any] | None) -> list[Any] | None:","        # Allow empty list; validation will be done in validate_grid_for_param","        return v","","","def values_for_param(grid: ParamGridSpec) -> list[Any]:","    \"\"\"Compute deterministic list of values for a parameter.","    ","    Args:","        grid: User-defined grid configuration","    ","    Returns:","        Sorted unique list of values in deterministic order.","    ","    Raises:","        ValueError: if grid is invalid.","    \"\"\"","    if grid.mode == GridMode.SINGLE:","        return [grid.single_value]","    ","    elif grid.mode == GridMode.RANGE:","        start = grid.range_start","        end = grid.range_end","        step = grid.range_step","        ","        if start is None or end is None or step is None:","            raise ValueError(\"range mode requires start, end, and step\")","        ","        if start > end:","            raise ValueError(\"start <= end\")","        ","        # Determine if values are integer-like","        if isinstance(start, int) and isinstance(end, int) and isinstance(step, int):","            # Integer range inclusive","            values = []","            i = 0","            while True:","                val = start + i * step","                if val > end:","                    break","                values.append(val)","                i += 1","            return values","        else:","            # Float range inclusive with drift-safe rounding","            if step <= 0:","                raise ValueError(\"step must be positive\")","            # Add small epsilon to avoid missing the last due to floating error","            num_steps = math.floor((end - start) / step + 1e-12)","            values = []","            for i in range(num_steps + 1):","                val = start + i * step","                # Round to 12 decimal places to avoid floating noise","                val = round(val, 12)","                if val <= end + 1e-12:","                    values.append(val)","            return values","    ","    elif grid.mode == GridMode.MULTI:","        values = grid.multi_values","        if values is None:","            raise ValueError(\"multi_values must be set for multi mode\")","        ","        # Ensure uniqueness and deterministic order","        seen = set()","        unique = []","        for v in values:","            if v not in seen:","                seen.add(v)","                unique.append(v)","        return unique","    ","    else:","        raise ValueError(f\"Unknown grid mode: {grid.mode}\")","","","def count_for_param(grid: ParamGridSpec) -> int:","    \"\"\"Return number of distinct values for this parameter.\"\"\"","    return len(values_for_param(grid))","","","def validate_grid_for_param(","    grid: ParamGridSpec,","    param_type: str,"]}
{"type":"file_chunk","path":"src/control/param_grid.py","chunk_index":1,"line_start":201,"line_end":330,"content":["    min: int | float | None = None,","    max: int | float | None = None,","    choices: list[Any] | None = None,",") -> None:","    \"\"\"Validate that grid is compatible with param spec.","    ","    Args:","        grid: Parameter grid specification","        param_type: Parameter type (\"int\", \"float\", \"bool\", \"enum\")","        min: Minimum allowed value (optional)","        max: Maximum allowed value (optional)","        choices: List of allowed values for enum type (optional)","    ","    Raises ValueError with descriptive message if invalid.","    \"\"\"","    # Check duplicates for MULTI mode","    if grid.mode == GridMode.MULTI and grid.multi_values:","        if len(grid.multi_values) != len(set(grid.multi_values)):","            raise ValueError(\"multi_values contains duplicate values\")","    ","    # Check empty multi_values","    if grid.mode == GridMode.MULTI and grid.multi_values is not None and len(grid.multi_values) == 0:","        raise ValueError(\"multi_values must contain at least one value\")","    ","    # Range-specific validation","    if grid.mode == GridMode.RANGE:","        if grid.range_step is not None and grid.range_step <= 0:","            raise ValueError(\"range_step must be positive\")","        if grid.range_start is not None and grid.range_end is not None and grid.range_start > grid.range_end:","            raise ValueError(\"start <= end\")","    ","    # Type-specific validation","    if param_type == \"enum\":","        if choices is None:","            raise ValueError(\"enum parameter must have choices defined\")","        if grid.mode == GridMode.RANGE:","            raise ValueError(\"enum parameters cannot use range mode\")","        if grid.mode == GridMode.SINGLE:","            if grid.single_value not in choices:","                raise ValueError(f\"value '{grid.single_value}' not in choices {choices}\")","        elif grid.mode == GridMode.MULTI:","            if grid.multi_values is None:","                raise ValueError(\"multi_values must be set for multi mode\")","            for val in grid.multi_values:","                if val not in choices:","                    raise ValueError(f\"value '{val}' not in choices {choices}\")","    ","    elif param_type == \"bool\":","        if grid.mode == GridMode.RANGE:","            raise ValueError(\"bool parameters cannot use range mode\")","        if grid.mode == GridMode.SINGLE:","            if not isinstance(grid.single_value, bool):","                raise ValueError(f\"bool parameter expects bool value, got {type(grid.single_value)}\")","        elif grid.mode == GridMode.MULTI:","            if grid.multi_values is None:","                raise ValueError(\"multi_values must be set for multi mode\")","            for val in grid.multi_values:","                if not isinstance(val, bool):","                    raise ValueError(f\"bool parameter expects bool values, got {type(val)}\")","    ","    elif param_type == \"int\":","        # Ensure values are integers","        if grid.mode == GridMode.SINGLE:","            if not isinstance(grid.single_value, int):","                raise ValueError(\"int parameter expects integer value\")","        elif grid.mode == GridMode.RANGE:","            if not (isinstance(grid.range_start, (int, float)) and","                    isinstance(grid.range_end, (int, float)) and","                    isinstance(grid.range_step, (int, float))):","                raise ValueError(\"int range requires numeric start/end/step\")","            # Values will be integer due to integer step","        elif grid.mode == GridMode.MULTI:","            if grid.multi_values is None:","                raise ValueError(\"multi_values must be set for multi mode\")","            for val in grid.multi_values:","                if not isinstance(val, int):","                    raise ValueError(\"int parameter expects integer values\")","    ","    elif param_type == \"float\":","        # Ensure values are numeric","        if grid.mode == GridMode.SINGLE:","            if not isinstance(grid.single_value, (int, float)):","                raise ValueError(\"float parameter expects numeric value\")","        elif grid.mode == GridMode.RANGE:","            if not (isinstance(grid.range_start, (int, float)) and","                    isinstance(grid.range_end, (int, float)) and","                    isinstance(grid.range_step, (int, float))):","                raise ValueError(\"float range requires numeric start/end/step\")","        elif grid.mode == GridMode.MULTI:","            if grid.multi_values is None:","                raise ValueError(\"multi_values must be set for multi mode\")","            for val in grid.multi_values:","                if not isinstance(val, (int, float)):","                    raise ValueError(\"float parameter expects numeric values\")","    ","    # Check bounds","    if min is not None:","        if grid.mode == GridMode.SINGLE:","            val = grid.single_value","            if val is not None and val < min:","                raise ValueError(f\"value {val} out of range (min {min})\")","        elif grid.mode == GridMode.RANGE:","            if grid.range_start is not None and grid.range_start < min:","                raise ValueError(f\"range_start {grid.range_start} out of range (min {min})\")","        elif grid.mode == GridMode.MULTI:","            if grid.multi_values is None:","                raise ValueError(\"multi_values must be set for multi mode\")","            for val in grid.multi_values:","                if val < min:","                    raise ValueError(f\"value {val} out of range (min {min})\")","    ","    if max is not None:","        if grid.mode == GridMode.SINGLE:","            val = grid.single_value","            if val is not None and val > max:","                raise ValueError(f\"value {val} out of range (max {max})\")","        elif grid.mode == GridMode.RANGE:","            if grid.range_end is not None and grid.range_end > max:","                raise ValueError(f\"range_end {grid.range_end} out of range (max {max})\")","        elif grid.mode == GridMode.MULTI:","            if grid.multi_values is None:","                raise ValueError(\"multi_values must be set for multi mode\")","            for val in grid.multi_values:","                if val > max:","                    raise ValueError(f\"value {val} out of range (max {max})\")","    ","    # Compute values to ensure no errors","    values_for_param(grid)","",""]}
{"type":"file_footer","path":"src/control/param_grid.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/paths.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":882,"sha256":"69a18e2dc18f8c6eaf4dfe0e60618f44fa618dd7a06a1770d2e0735b827c7a21","total_lines":37,"chunk_count":1}
{"type":"file_chunk","path":"src/control/paths.py","chunk_index":0,"line_start":1,"line_end":37,"content":["","\"\"\"Path helpers for B5-C Mission Control.\"\"\"","","from __future__ import annotations","","import os","from pathlib import Path","","","def get_outputs_root() -> Path:","    \"\"\"","    Single source of truth for outputs root.","    - Default: ./outputs (repo relative)","    - Override: env FISHBRO_OUTPUTS_ROOT","    \"\"\"","    p = os.environ.get(\"FISHBRO_OUTPUTS_ROOT\", \"outputs\")","    return Path(p).resolve()","","","def run_log_path(outputs_root: Path, season: str, run_id: str) -> Path:","    \"\"\"","    Return outputs log path for a run (mkdir parents).","    ","    Args:","        outputs_root: Root outputs directory","        season: Season identifier","        run_id: Run ID","        ","    Returns:","        Path to log file: outputs/{season}/{run_id}/logs/worker.log","    \"\"\"","    log_path = outputs_root / season / run_id / \"logs\" / \"worker.log\"","    log_path.parent.mkdir(parents=True, exist_ok=True)","    return log_path","","",""]}
{"type":"file_footer","path":"src/control/paths.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/pipeline_runner.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8977,"sha256":"214e99bd73c08cda4eec030a84e7ae3f040d87249e985500051eb8704b16843d","total_lines":233,"chunk_count":2}
{"type":"file_chunk","path":"src/control/pipeline_runner.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Pipeline Runner for M1 Wizard.","","Stub implementation for job pipeline execution.","\"\"\"","","from __future__ import annotations","","import time","from typing import Dict, Any, Optional","from pathlib import Path","","from control.jobs_db import (","    get_job, mark_running, mark_done, mark_failed, append_log",")","from control.job_api import calculate_units","from control.artifacts_api import write_research_index","","","class PipelineRunner:","    \"\"\"Simple pipeline runner for M1 demonstration.\"\"\"","    ","    def __init__(self, db_path: Optional[Path] = None):","        \"\"\"Initialize pipeline runner.","        ","        Args:","            db_path: Path to SQLite database. If None, uses default.","        \"\"\"","        self.db_path = db_path or Path(\"outputs/jobs.db\")","    ","    def run_job(self, job_id: str) -> bool:","        \"\"\"Run a job (stub implementation for M1).","        ","        This is a simplified runner that simulates job execution","        for demonstration purposes.","        ","        Args:","            job_id: Job ID to run","            ","        Returns:","            True if job completed successfully, False otherwise","        \"\"\"","        try:","            # Get job record","            job = get_job(self.db_path, job_id)","            ","            # Mark as running","            mark_running(self.db_path, job_id, pid=12345)","            self._log(job_id, f\"Job {job_id} started\")","            ","            # Simulate work based on units","            units = 0","            if hasattr(job.spec, 'config_snapshot'):","                config = job.spec.config_snapshot","                if isinstance(config, dict) and 'units' in config:","                    units = config.get('units', 10)","            ","            # Default to 10 units if not specified","            if units <= 0:","                units = 10","            ","            self._log(job_id, f\"Processing {units} units\")","            ","            # Simulate unit processing","            for i in range(units):","                time.sleep(0.1)  # Simulate work","                progress = (i + 1) / units","                if i % max(1, units // 10) == 0:  # Log every ~10%","                    self._log(job_id, f\"Unit {i+1}/{units} completed ({progress:.0%})\")","            ","            # Mark as done","            mark_done(self.db_path, job_id, run_id=f\"run_{job_id}\", report_link=f\"/reports/{job_id}\")","            ","            # Write research index (M2)","            try:","                season = job.spec.season if hasattr(job.spec, 'season') else \"default\"","                # Generate dummy units based on config snapshot","                units = []","                if hasattr(job.spec, 'config_snapshot'):","                    config = job.spec.config_snapshot","                    if isinstance(config, dict):","                        # Extract possible symbols, timeframes, etc.","                        data1 = config.get('data1', {})","                        symbols = data1.get('symbols', ['MNQ'])","                        timeframes = data1.get('timeframes', ['60m'])","                        strategy = config.get('strategy_id', 'vPB_Z')","                        data2_filters = config.get('data2', {}).get('filters', ['VX'])","                        # Create one unit per combination (simplified)","                        for sym in symbols[:1]:  # limit","                            for tf in timeframes[:1]:","                                for filt in data2_filters[:1]:","                                    units.append({","                                        'data1_symbol': sym,","                                        'data1_timeframe': tf,","                                        'strategy': strategy,","                                        'data2_filter': filt,","                                        'status': 'DONE',","                                        'artifacts': {","                                            'canonical_results': f'outputs/seasons/{season}/research/{job_id}/{sym}/{tf}/{strategy}/{filt}/canonical_results.json',","                                            'metrics': f'outputs/seasons/{season}/research/{job_id}/{sym}/{tf}/{strategy}/{filt}/metrics.json',","                                            'trades': f'outputs/seasons/{season}/research/{job_id}/{sym}/{tf}/{strategy}/{filt}/trades.parquet',","                                        }","                                    })","                if not units:","                    # Fallback dummy unit","                    units.append({","                        'data1_symbol': 'MNQ',","                        'data1_timeframe': '60m',","                        'strategy': 'vPB_Z',","                        'data2_filter': 'VX',","                        'status': 'DONE',","                        'artifacts': {","                            'canonical_results': f'outputs/seasons/{season}/research/{job_id}/MNQ/60m/vPB_Z/VX/canonical_results.json',","                            'metrics': f'outputs/seasons/{season}/research/{job_id}/MNQ/60m/vPB_Z/VX/metrics.json',","                            'trades': f'outputs/seasons/{season}/research/{job_id}/MNQ/60m/vPB_Z/VX/trades.parquet',","                        }","                    })","                write_research_index(season, job_id, units)","                self._log(job_id, f\"Research index written for {len(units)} units\")","            except Exception as e:","                self._log(job_id, f\"Failed to write research index: {e}\")","            ","            self._log(job_id, f\"Job {job_id} completed successfully\")","            ","            return True","            ","        except Exception as e:","            # Mark as failed","            error_msg = f\"Job failed: {str(e)}\"","            try:","                mark_failed(self.db_path, job_id, error=error_msg)","                self._log(job_id, error_msg)","            except Exception:","                pass  # Ignore errors during failure marking","            ","            return False","    ","    def _log(self, job_id: str, message: str) -> None:","        \"\"\"Add log entry for job.\"\"\"","        try:","            append_log(self.db_path, job_id, message)","        except Exception:","            pass  # Ignore log errors","    ","    def get_job_progress(self, job_id: str) -> Dict[str, Any]:","        \"\"\"Get job progress information.","        ","        Args:","            job_id: Job ID","            ","        Returns:","            Dictionary with progress information","        \"\"\"","        try:","            job = get_job(self.db_path, job_id)","            ","            # Calculate progress based on status","            units_total = 0","            units_done = 0","            ","            if hasattr(job.spec, 'config_snapshot'):","                config = job.spec.config_snapshot","                if isinstance(config, dict) and 'units' in config:","                    units_total = config.get('units', 0)","            ","            if job.status.value == \"DONE\":","                units_done = units_total","            elif job.status.value == \"RUNNING\":","                # For stub, estimate 50% progress","                units_done = units_total // 2 if units_total > 0 else 0","            ","            progress = units_done / units_total if units_total > 0 else 0","            ","            return {","                \"job_id\": job_id,","                \"status\": job.status.value,","                \"units_done\": units_done,","                \"units_total\": units_total,","                \"progress\": progress,","                \"is_running\": job.status.value == \"RUNNING\",","                \"is_done\": job.status.value == \"DONE\",","                \"is_failed\": job.status.value == \"FAILED\"","            }","        except Exception as e:","            return {","                \"job_id\": job_id,","                \"status\": \"UNKNOWN\",","                \"units_done\": 0,","                \"units_total\": 0,","                \"progress\": 0,","                \"is_running\": False,","                \"is_done\": False,","                \"is_failed\": True,","                \"error\": str(e)","            }","","","# Singleton instance","_runner_instance: Optional[PipelineRunner] = None","","def get_pipeline_runner() -> PipelineRunner:"]}
{"type":"file_chunk","path":"src/control/pipeline_runner.py","chunk_index":1,"line_start":201,"line_end":233,"content":["    \"\"\"Get singleton pipeline runner instance.\"\"\"","    global _runner_instance","    if _runner_instance is None:","        _runner_instance = PipelineRunner()","    return _runner_instance","","","def start_job_async(job_id: str) -> None:","    \"\"\"Start job execution asynchronously (stub).","    ","    In a real implementation, this would spawn a worker process.","    For M1, we'll just simulate immediate execution.","    ","    Args:","        job_id: Job ID to start","    \"\"\"","    # In a real implementation, this would use a task queue or worker pool","    # For M1 demo, we'll run synchronously","    runner = get_pipeline_runner()","    runner.run_job(job_id)","","","def check_job_status(job_id: str) -> Dict[str, Any]:","    \"\"\"Check job status (convenience wrapper).","    ","    Args:","        job_id: Job ID","        ","    Returns:","        Dictionary with job status and progress","    \"\"\"","    runner = get_pipeline_runner()","    return runner.get_job_progress(job_id)"]}
{"type":"file_footer","path":"src/control/pipeline_runner.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/preflight.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1971,"sha256":"69d63b94b9f3e9c52d7755c257898642af2cd6186a1fb55f4486e8f2a070a913","total_lines":65,"chunk_count":1}
{"type":"file_chunk","path":"src/control/preflight.py","chunk_index":0,"line_start":1,"line_end":65,"content":["","\"\"\"Preflight check - OOM gate and cost summary.\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass","from typing import Any, Literal","","from core.oom_gate import decide_oom_action","","","@dataclass(frozen=True)","class PreflightResult:","    \"\"\"Preflight check result.\"\"\"","","    action: Literal[\"PASS\", \"BLOCK\", \"AUTO_DOWNSAMPLE\"]","    reason: str","    original_subsample: float","    final_subsample: float","    estimated_bytes: int","    estimated_mb: float","    mem_limit_mb: float","    mem_limit_bytes: int","    estimates: dict[str, Any]  # must include ops_est, time_est_s, mem_est_mb, ...","","","def run_preflight(cfg_snapshot: dict[str, Any]) -> PreflightResult:","    \"\"\"","    Run preflight check (pure, no I/O).","    ","    Returns what UI shows in CHECK panel.","    ","    Args:","        cfg_snapshot: Sanitized config snapshot (no ndarrays)","        ","    Returns:","        PreflightResult with OOM gate decision and estimates","    \"\"\"","    # Extract mem_limit_mb from config (default: 6000 MB = 6GB)","    mem_limit_mb = float(cfg_snapshot.get(\"mem_limit_mb\", 6000.0))","    ","    # Run OOM gate decision","    gate_result = decide_oom_action(","        cfg_snapshot,","        mem_limit_mb=mem_limit_mb,","        allow_auto_downsample=cfg_snapshot.get(\"allow_auto_downsample\", True),","        auto_downsample_step=cfg_snapshot.get(\"auto_downsample_step\", 0.5),","        auto_downsample_min=cfg_snapshot.get(\"auto_downsample_min\", 0.02),","        work_factor=cfg_snapshot.get(\"work_factor\", 2.0),","    )","    ","    return PreflightResult(","        action=gate_result[\"action\"],","        reason=gate_result[\"reason\"],","        original_subsample=gate_result[\"original_subsample\"],","        final_subsample=gate_result[\"final_subsample\"],","        estimated_bytes=gate_result[\"estimated_bytes\"],","        estimated_mb=gate_result[\"estimated_mb\"],","        mem_limit_mb=gate_result[\"mem_limit_mb\"],","        mem_limit_bytes=gate_result[\"mem_limit_bytes\"],","        estimates=gate_result[\"estimates\"],","    )","","",""]}
{"type":"file_footer","path":"src/control/preflight.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/report_links.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2181,"sha256":"3de92e00455914d2971df31aeb7358c93b0c333f7abddd71339530633c63a216","total_lines":81,"chunk_count":1}
{"type":"file_chunk","path":"src/control/report_links.py","chunk_index":0,"line_start":1,"line_end":81,"content":["","\"\"\"Report link generation for B5 viewer.\"\"\"","","from __future__ import annotations","","import os","from pathlib import Path","from urllib.parse import urlencode","","# Default outputs root (can be overridden via environment)","DEFAULT_OUTPUTS_ROOT = \"outputs\"","","","def get_outputs_root() -> Path:","    \"\"\"Get outputs root from environment or default.\"\"\"","    outputs_root_str = os.getenv(\"FISHBRO_OUTPUTS_ROOT\", DEFAULT_OUTPUTS_ROOT)","    return Path(outputs_root_str)","","","def make_report_link(*, season: str, run_id: str) -> str:","    \"\"\"","    Generate report link for B5 viewer.","    ","    Args:","        season: Season identifier (e.g. \"2026Q1\")","        run_id: Run ID (e.g. \"stage0_coarse-20251218T093512Z-d3caa754\")","        ","    Returns:","        Report link URL with querystring (e.g. \"/?season=2026Q1&run_id=stage0_xxx\")","    \"\"\"","    # Test contract: link.startswith(\"/?\")","    base = \"/\"","    qs = urlencode({\"season\": season, \"run_id\": run_id})","    return f\"{base}?{qs}\"","","","def is_report_ready(run_id: str) -> bool:","    \"\"\"","    Check if report is ready (minimal artifacts exist).","    ","    Phase 6 rule: Only check file existence, not content validity.","    Content validation is Viewer's responsibility.","    ","    Args:","        run_id: Run ID to check","        ","    Returns:","        True if all required artifacts exist, False otherwise","    \"\"\"","    try:","        outputs_root = get_outputs_root()","        base = outputs_root / run_id","        ","        # Check for winners_v2.json first, fallback to winners.json","        winners_v2_path = base / \"winners_v2.json\"","        winners_path = base / \"winners.json\"","        winners_exists = winners_v2_path.exists() or winners_path.exists()","        ","        required = [","            base / \"manifest.json\",","            base / \"governance.json\",","        ]","        ","        return winners_exists and all(p.exists() for p in required)","    except Exception:","        return False","","","def build_report_link(*args: str) -> str:","    if len(args) == 1:","        run_id = args[0]","        season = \"test\"","        return f\"/?season={season}&run_id={run_id}\"","","    if len(args) == 2:","        season, run_id = args","        return f\"/b5?season={season}&run_id={run_id}\"","","    return \"\"","",""]}
{"type":"file_footer","path":"src/control/report_links.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/research_cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7090,"sha256":"af3ba58fe4a0cd0bdb132d19cea1cd58e4eb4c2c766361f0ccdc36460270add5","total_lines":259,"chunk_count":2}
{"type":"file_chunk","path":"src/control/research_cli.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Research CLI：研究執行命令列介面","","命令：","fishbro research run \\","  --season 2026Q1 \\","  --dataset-id CME.MNQ \\","  --strategy-id S1 \\","  --allow-build \\","  --txt-path /home/fishbro/FishBroData/raw/CME.MNQ-HOT-Minute-Trade.txt \\","  --mode incremental \\","  --json","","Exit code：","0：成功","20：缺 features 且不允許 build","1：其他錯誤","\"\"\"","","from __future__ import annotations","","import sys","import json","import argparse","from pathlib import Path","from typing import Optional","","from control.research_runner import (","    run_research,","    ResearchRunError,",")","from control.build_context import BuildContext","from strategy.registry import load_builtin_strategies","","","def main() -> int:","    \"\"\"CLI 主函數\"\"\"","    parser = create_parser()","    args = parser.parse_args()","    ","    try:","        return run_research_cli(args)","    except KeyboardInterrupt:","        print(\"\\n中斷執行\", file=sys.stderr)","        return 130","    except Exception as e:","        print(f\"錯誤: {e}\", file=sys.stderr)","        return 1","","","def create_parser() -> argparse.ArgumentParser:","    \"\"\"建立命令列解析器\"\"\"","    parser = argparse.ArgumentParser(","        description=\"執行研究（載入策略、解析特徵、執行 WFS）\",","        formatter_class=argparse.ArgumentDefaultsHelpFormatter,","    )","    ","    # 必要參數","    parser.add_argument(","        \"--season\",","        required=True,","        help=\"季節標記，例如 2026Q1\",","    )","    parser.add_argument(","        \"--dataset-id\",","        required=True,","        help=\"資料集 ID，例如 CME.MNQ\",","    )","    parser.add_argument(","        \"--strategy-id\",","        required=True,","        help=\"策略 ID\",","    )","    ","    # build 相關參數","    parser.add_argument(","        \"--allow-build\",","        action=\"store_true\",","        help=\"允許自動 build 缺失的特徵\",","    )","    parser.add_argument(","        \"--txt-path\",","        type=Path,","        help=\"原始 TXT 檔案路徑（只有 allow-build 才需要）\",","    )","    parser.add_argument(","        \"--mode\",","        choices=[\"incremental\", \"full\"],","        default=\"incremental\",","        help=\"build 模式（只在 allow-build 時使用）\",","    )","    parser.add_argument(","        \"--outputs-root\",","        type=Path,","        default=Path(\"outputs\"),","        help=\"輸出根目錄\",","    )","    parser.add_argument(","        \"--build-bars-if-missing\",","        action=\"store_true\",","        default=True,","        help=\"如果 bars cache 不存在，是否建立 bars\",","    )","    parser.add_argument(","        \"--no-build-bars-if-missing\",","        action=\"store_false\",","        dest=\"build_bars_if_missing\",","        help=\"不建立 bars cache（即使缺失）\",","    )","    ","    # WFS 配置（可選）","    parser.add_argument(","        \"--wfs-config\",","        type=Path,","        help=\"WFS 配置 JSON 檔案路徑（可選）\",","    )","    ","    # 輸出選項","    parser.add_argument(","        \"--json\",","        action=\"store_true\",","        help=\"以 JSON 格式輸出結果\",","    )","    parser.add_argument(","        \"--verbose\",","        action=\"store_true\",","        help=\"輸出詳細資訊\",","    )","    ","    return parser","","","def ensure_builtin_strategies_loaded() -> None:","    \"\"\"Ensure built-in strategies are loaded (idempotent).","    ","    This function can be called multiple times without crashing.","    \"\"\"","    try:","        load_builtin_strategies()","    except ValueError as e:","        # registry is process-local; re-entry may raise duplicate register","        if \"already registered\" not in str(e):","            raise","","","def run_research_cli(args) -> int:","    \"\"\"執行研究邏輯\"\"\"","    # 0. 確保 built-in strategies 已載入","    ensure_builtin_strategies_loaded()","    ","    # 1. 準備 build_ctx（如果需要）","    build_ctx = prepare_build_context(args)","    ","    # 2. 載入 WFS 配置（如果有）","    wfs_config = load_wfs_config(args)","    ","    # 3. 執行研究","    try:","        report = run_research(","            season=args.season,","            dataset_id=args.dataset_id,","            strategy_id=args.strategy_id,","            outputs_root=args.outputs_root,","            allow_build=args.allow_build,","            build_ctx=build_ctx,","            wfs_config=wfs_config,","        )","        ","        # 4. 輸出結果","        output_result(report, args)","        ","        # 判斷 exit code","        # 如果有 build，回傳 10；否則回傳 0","        if report.get(\"build_performed\", False):","            return 10","        else:","            return 0","        ","    except ResearchRunError as e:","        # 檢查是否為缺失特徵且不允許 build 的錯誤","        err_msg = str(e).lower()","        if \"缺失特徵且不允許建置\" in err_msg or \"missing features\" in err_msg:","            print(f\"缺失特徵且不允許建置: {e}\", file=sys.stderr)","            return 20","        else:","            print(f\"研究執行失敗: {e}\", file=sys.stderr)","            return 1","","","def prepare_build_context(args) -> Optional[BuildContext]:","    \"\"\"準備 BuildContext\"\"\"","    if not args.allow_build:","        return None","    ","    if not args.txt_path:","        raise ValueError(\"--allow-build 需要 --txt-path\")","    ","    # 驗證 txt_path 存在","    if not args.txt_path.exists():"]}
{"type":"file_chunk","path":"src/control/research_cli.py","chunk_index":1,"line_start":201,"line_end":259,"content":["        raise FileNotFoundError(f\"TXT 檔案不存在: {args.txt_path}\")","    ","    # 轉換 mode 為大寫","    mode = args.mode.upper()","    if mode not in (\"FULL\", \"INCREMENTAL\"):","        raise ValueError(f\"無效的 mode: {args.mode}，必須為 'incremental' 或 'full'\")","    ","    return BuildContext(","        txt_path=args.txt_path,","        mode=mode,","        outputs_root=args.outputs_root,","        build_bars_if_missing=args.build_bars_if_missing,","    )","","","def load_wfs_config(args) -> Optional[dict]:","    \"\"\"載入 WFS 配置\"\"\"","    if not args.wfs_config:","        return None","    ","    config_path = args.wfs_config","    if not config_path.exists():","        raise FileNotFoundError(f\"WFS 配置檔案不存在: {config_path}\")","    ","    try:","        content = config_path.read_text(encoding=\"utf-8\")","        return json.loads(content)","    except Exception as e:","        raise ValueError(f\"無法載入 WFS 配置 {config_path}: {e}\")","","","def output_result(report: dict, args) -> None:","    \"\"\"輸出研究結果\"\"\"","    if args.json:","        # JSON 格式輸出","        print(json.dumps(report, indent=2, ensure_ascii=False))","    else:","        # 文字格式輸出","        print(f\"✅ 研究執行成功\")","        print(f\"   策略: {report['strategy_id']}\")","        print(f\"   資料集: {report['dataset_id']}\")","        print(f\"   季節: {report['season']}\")","        print(f\"   使用特徵: {len(report['used_features'])} 個\")","        print(f\"   是否執行了建置: {report['build_performed']}\")","        ","        if args.verbose:","            print(f\"   WFS 摘要:\")","            for key, value in report['wfs_summary'].items():","                print(f\"     {key}: {value}\")","            ","            print(f\"   特徵列表:\")","            for feat in report['used_features']:","                print(f\"     {feat['name']}@{feat['timeframe_min']}m\")","","","if __name__ == \"__main__\":","    sys.exit(main())","",""]}
{"type":"file_footer","path":"src/control/research_cli.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/research_runner.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9337,"sha256":"0ee4201db0d155a888420c2e3b84521f748aa1dc56038b80168ba1595c4b3a97","total_lines":254,"chunk_count":2}
{"type":"file_chunk","path":"src/control/research_runner.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Research Runner - 研究執行的唯一入口","","負責載入策略、解析特徵需求、呼叫 Feature Resolver、注入 FeatureBundle 到 WFS、執行研究。","嚴格區分 Research vs Run/Viewer 路徑。","","Phase 4.1: 新增 Research Runner + WFS Integration","\"\"\"","","from __future__ import annotations","","import logging","from pathlib import Path","from typing import Optional, Dict, Any","","from contracts.strategy_features import (","    StrategyFeatureRequirements,","    load_requirements_from_json,",")","from control.build_context import BuildContext","from control.feature_resolver import (","    resolve_features,","    MissingFeaturesError,","    ManifestMismatchError,","    BuildNotAllowedError,","    FeatureResolutionError,",")","from core.feature_bundle import FeatureBundle","from wfs.runner import run_wfs_with_features","from core.slippage_policy import SlippagePolicy","from control.research_slippage_stress import (","    compute_stress_matrix,","    survive_s2,","    compute_stress_test_passed,","    generate_stress_report,","    CommissionConfig,",")","","logger = logging.getLogger(__name__)","","","class ResearchRunError(RuntimeError):","    \"\"\"Research Runner 專用錯誤類別\"\"\"","    pass","","","def _load_strategy_feature_requirements(","    strategy_id: str,","    outputs_root: Path,",") -> StrategyFeatureRequirements:","    \"\"\"","    載入策略特徵需求","","    順序：","    1. 先嘗試 strategy.feature_requirements()（Python）","    2. 再 fallback strategies/{strategy_id}/features.json","","    若都沒有 → raise ResearchRunError","    \"\"\"","    # 1. 嘗試 Python 方法（如果策略有實作）","    try:","        from strategy.registry import get","        spec = get(strategy_id)","        if hasattr(spec, \"feature_requirements\") and callable(spec.feature_requirements):","            req = spec.feature_requirements()","            if isinstance(req, StrategyFeatureRequirements):","                logger.debug(f\"策略 {strategy_id} 透過 Python 方法提供特徵需求\")","                return req","    except Exception as e:","        logger.debug(f\"策略 {strategy_id} 無 Python 特徵需求方法: {e}\")","","    # 2. 嘗試 JSON 檔案","    json_path = outputs_root / \"strategies\" / strategy_id / \"features.json\"","    if not json_path.exists():","        # 也嘗試在 configs/strategies 資料夾","        json_path = Path(\"configs/strategies\") / strategy_id / \"features.json\"","        if not json_path.exists():","            raise ResearchRunError(","                f\"策略 {strategy_id} 無特徵需求定義：\"","                f\"既無 Python 方法，也找不到 JSON 檔案 ({json_path})\"","            )","","    try:","        req = load_requirements_from_json(str(json_path))","        logger.debug(f\"從 {json_path} 載入策略 {strategy_id} 特徵需求\")","        return req","    except Exception as e:","        raise ResearchRunError(f\"載入策略 {strategy_id} 特徵需求失敗: {e}\")","","","def run_research(","    *,","    season: str,","    dataset_id: str,","    strategy_id: str,","    outputs_root: Path = Path(\"outputs\"),","    allow_build: bool = False,","    build_ctx: Optional[BuildContext] = None,","    wfs_config: Optional[Dict[str, Any]] = None,","    enable_slippage_stress: bool = False,","    slippage_policy: Optional[SlippagePolicy] = None,","    commission_config: Optional[CommissionConfig] = None,","    tick_size_map: Optional[Dict[str, float]] = None,",") -> Dict[str, Any]:","    \"\"\"","    Execute a research run for a single strategy.","    Returns a run report (no raw arrays).","","    Args:","        season: 季節標識，例如 \"2026Q1\"","        dataset_id: 資料集 ID，例如 \"CME.MNQ\"","        strategy_id: 策略 ID，例如 \"S1\"","        outputs_root: 輸出根目錄（預設 \"outputs\"）","        allow_build: 是否允許自動建置缺失的特徵","        build_ctx: BuildContext 實例（若 allow_build=True 則必須提供）","        wfs_config: WFS 配置字典（可選）","        enable_slippage_stress: 是否啟用滑價壓力測試（預設 False）","        slippage_policy: 滑價政策（若 enable_slippage_stress=True 則必須提供）","        commission_config: 手續費配置（若 enable_slippage_stress=True 則必須提供）","        tick_size_map: tick_size 對應表（若 enable_slippage_stress=True 則必須提供）","","    Returns:","        run report 字典，包含：","            strategy_id","            dataset_id","            season","            used_features (list)","            features_manifest_sha256","            build_performed (bool)","            wfs_summary（摘要，不含大量數據）","            slippage_stress（若啟用）","","    Raises:","        ResearchRunError: 研究執行失敗","    \"\"\"","    # 1. 載入策略特徵需求","    logger.info(f\"開始研究執行: {strategy_id} on {dataset_id} ({season})\")","    try:","        req = _load_strategy_feature_requirements(strategy_id, outputs_root)","    except Exception as e:","        raise ResearchRunError(f\"載入策略特徵需求失敗: {e}\")","","    # 2. Resolve Features","    try:","        feature_bundle, build_performed = resolve_features(","            dataset_id=dataset_id,","            season=season,","            requirements=req,","            outputs_root=outputs_root,","            allow_build=allow_build,","            build_ctx=build_ctx,","        )","    except MissingFeaturesError as e:","        if not allow_build:","            # 缺失特徵且不允許建置 → 轉為 exit code 20（在 CLI 層處理）","            raise ResearchRunError(","                f\"缺失特徵且不允許建置: {e}\"","            ) from e","        # 若 allow_build=True 但 build_ctx=None，則 BuildNotAllowedError 會被拋出","        raise","    except BuildNotAllowedError as e:","        raise ResearchRunError(","            f\"允許建置但缺少 BuildContext: {e}\"","        ) from e","    except (ManifestMismatchError, FeatureResolutionError) as e:","        raise ResearchRunError(f\"特徵解析失敗: {e}\") from e","","    # 3. 注入 FeatureBundle 到 WFS","    try:","        wfs_result = run_wfs_with_features(","            strategy_id=strategy_id,","            feature_bundle=feature_bundle,","            config=wfs_config,","        )","    except Exception as e:","        raise ResearchRunError(f\"WFS 執行失敗: {e}\") from e","","    # 4. 滑價壓力測試（若啟用）","    slippage_stress_report = None","    if enable_slippage_stress:","        if slippage_policy is None:","            slippage_policy = SlippagePolicy()  # 預設政策","        if commission_config is None:","            # 預設手續費配置（僅示例，實際應從配置檔讀取）","            commission_config = CommissionConfig(","                per_side_usd={\"MNQ\": 2.8, \"MES\": 2.8, \"MXF\": 20.0},","                default_per_side_usd=0.0,","            )","        if tick_size_map is None:","            # 預設 tick_size（僅示例，實際應從 dimension contract 讀取）","            tick_size_map = {\"MNQ\": 0.25, \"MES\": 0.25, \"MXF\": 1.0}","        ","        # 從 dataset_id 推導商品符號（簡化：取最後一部分）","        symbol = dataset_id.split(\".\")[1] if \".\" in dataset_id else dataset_id","        ","        # 檢查 tick_size 是否存在","        if symbol not in tick_size_map:","            raise ResearchRunError(","                f\"商品 {symbol} 的 tick_size 未定義於 tick_size_map 中\""]}
{"type":"file_chunk","path":"src/control/research_runner.py","chunk_index":1,"line_start":201,"line_end":254,"content":["            )","        ","        # 假設 wfs_result 包含 fills/intents 資料","        # 目前我們沒有實際的 fills 資料，因此跳過計算","        # 這裡僅建立一個框架，實際計算需根據 fills/intents 實作","        logger.warning(","            \"滑價壓力測試已啟用，但 fills/intents 資料不可用，跳過計算。\"","            \"請確保 WFS 結果包含 fills 欄位。\"","        )","        # 建立一個空的 stress matrix 報告","        slippage_stress_report = {","            \"enabled\": True,","            \"policy\": {","                \"definition\": slippage_policy.definition,","                \"levels\": slippage_policy.levels,","                \"selection_level\": slippage_policy.selection_level,","                \"stress_level\": slippage_policy.stress_level,","                \"mc_execution_level\": slippage_policy.mc_execution_level,","            },","            \"stress_matrix\": {},","            \"survive_s2\": False,","            \"stress_test_passed\": False,","            \"note\": \"fills/intents 資料不可用，計算被跳過\",","        }","","    # 5. 組裝 run report","    used_features = [","        {\"name\": fs.name, \"timeframe_min\": fs.timeframe_min}","        for fs in feature_bundle.series.values()","    ]","    report = {","        \"strategy_id\": strategy_id,","        \"dataset_id\": dataset_id,","        \"season\": season,","        \"used_features\": used_features,","        \"features_manifest_sha256\": feature_bundle.meta.get(\"manifest_sha256\", \"\"),","        \"build_performed\": build_performed,","        \"wfs_summary\": {","            \"status\": \"completed\",","            \"metrics_keys\": list(wfs_result.keys()) if isinstance(wfs_result, dict) else [],","        },","    }","    # 如果 wfs_result 包含摘要，合併進去","    if isinstance(wfs_result, dict) and \"summary\" in wfs_result:","        report[\"wfs_summary\"].update(wfs_result[\"summary\"])","    ","    # 加入滑價壓力測試報告（若啟用）","    if enable_slippage_stress and slippage_stress_report is not None:","        report[\"slippage_stress\"] = slippage_stress_report","","    logger.info(f\"研究執行完成: {strategy_id}\")","    return report","",""]}
{"type":"file_footer","path":"src/control/research_runner.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/research_slippage_stress.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8168,"sha256":"78f359c1bb7e89145ed90c6246305e37bb194cde8ace74599bf38f70110d69dd","total_lines":262,"chunk_count":2}
{"type":"file_chunk","path":"src/control/research_slippage_stress.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Slippage Stress Matrix 計算與 Survive Gate 評估","","給定 bars、fills/intents、commission 配置，計算 S0–S3 等級的 KPI 矩陣。","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass","from typing import Dict, List, Optional, Tuple, Any","import numpy as np","","from core.slippage_policy import SlippagePolicy, apply_slippage_to_price","","","@dataclass","class StressResult:","    \"\"\"","    單一滑價等級的壓力測試結果","    \"\"\"","    level: str  # 等級名稱，例如 \"S0\"","    slip_ticks: int  # 滑價 tick 數","    net_after_cost: float  # 扣除成本後的淨利","    gross_profit: float  # 總盈利（未扣除成本）","    gross_loss: float  # 總虧損（未扣除成本）","    profit_factor: float  # 盈利因子 = gross_profit / abs(gross_loss)（如果 gross_loss != 0）","    mdd_after_cost: float  # 扣除成本後的最大回撤（絕對值）","    trades: int  # 交易次數（來回算一次）","","","@dataclass","class CommissionConfig:","    \"\"\"","    手續費配置（每邊固定金額）","    \"\"\"","    per_side_usd: Dict[str, float]  # 商品符號 -> 每邊手續費（USD）","    default_per_side_usd: float = 0.0  # 預設手續費（如果商品未指定）","","","def compute_stress_matrix(","    bars: Dict[str, np.ndarray],","    fills: List[Dict[str, Any]],","    commission_config: CommissionConfig,","    slippage_policy: SlippagePolicy,","    tick_size_map: Dict[str, float],  # 商品符號 -> tick_size","    symbol: str,  # 當前商品符號，例如 \"MNQ\"",") -> Dict[str, StressResult]:","    \"\"\"","    計算滑價壓力矩陣（S0–S3）","","    Args:","        bars: 價格 bars 字典，至少包含 \"open\", \"high\", \"low\", \"close\"","        fills: 成交列表，每個成交為字典，包含 \"entry_price\", \"exit_price\", \"entry_side\", \"exit_side\", \"quantity\" 等欄位","        commission_config: 手續費配置","        slippage_policy: 滑價政策","        tick_size_map: tick_size 對應表","        symbol: 商品符號","","    Returns:","        字典 mapping level -> StressResult","    \"\"\"","    # 取得 tick_size","    tick_size = tick_size_map.get(symbol)","    if tick_size is None or tick_size <= 0:","        raise ValueError(f\"商品 {symbol} 的 tick_size 無效或缺失: {tick_size}\")","    ","    # 取得手續費（每邊）","    commission_per_side = commission_config.per_side_usd.get(","        symbol, commission_config.default_per_side_usd","    )","    ","    results = {}","    ","    for level in [\"S0\", \"S1\", \"S2\", \"S3\"]:","        slip_ticks = slippage_policy.get_ticks(level)","        ","        # 計算該等級下的淨利與其他指標","        net, gross_profit, gross_loss, trades = _compute_net_with_slippage(","            fills, slip_ticks, tick_size, commission_per_side","        )","        ","        # 計算盈利因子","        if gross_loss == 0:","            profit_factor = float(\"inf\") if gross_profit > 0 else 1.0","        else:","            profit_factor = gross_profit / abs(gross_loss)","        ","        # 計算最大回撤（簡化版本：使用淨利序列）","        # 由於我們沒有逐筆的 equity curve，這裡先設為 0","        mdd = 0.0","        ","        results[level] = StressResult(","            level=level,","            slip_ticks=slip_ticks,","            net_after_cost=net,","            gross_profit=gross_profit,","            gross_loss=gross_loss,","            profit_factor=profit_factor,","            mdd_after_cost=mdd,","            trades=trades,","        )","    ","    return results","","","def _compute_net_with_slippage(","    fills: List[Dict[str, Any]],","    slip_ticks: int,","    tick_size: float,","    commission_per_side: float,",") -> Tuple[float, float, float, int]:","    \"\"\"","    計算給定滑價 tick 數下的淨利、總盈利、總虧損與交易次數","    \"\"\"","    total_net = 0.0","    total_gross_profit = 0.0","    total_gross_loss = 0.0","    trades = 0","    ","    for fill in fills:","        # 假設 fill 結構包含 entry_price, exit_price, entry_side, exit_side, quantity","        entry_price = fill.get(\"entry_price\")","        exit_price = fill.get(\"exit_price\")","        entry_side = fill.get(\"entry_side\")  # \"buy\" 或 \"sellshort\"","        exit_side = fill.get(\"exit_side\")    # \"sell\" 或 \"buytocover\"","        quantity = fill.get(\"quantity\", 1.0)","        ","        if None in (entry_price, exit_price, entry_side, exit_side):","            continue","        ","        # 應用滑價調整價格","        entry_price_adj = apply_slippage_to_price(","            entry_price, entry_side, slip_ticks, tick_size","        )","        exit_price_adj = apply_slippage_to_price(","            exit_price, exit_side, slip_ticks, tick_size","        )","        ","        # 計算毛利（未扣除手續費）","        if entry_side in (\"buy\", \"buytocover\"):","            # 多頭：買入後賣出","            gross = (exit_price_adj - entry_price_adj) * quantity","        else:","            # 空頭：賣出後買回","            gross = (entry_price_adj - exit_price_adj) * quantity","        ","        # 扣除手續費（每邊）","        commission_total = 2 * commission_per_side * quantity","        ","        # 淨利","        net = gross - commission_total","        ","        total_net += net","        if net > 0:","            total_gross_profit += net + commission_total  # 還原手續費以得到 gross profit","        else:","            total_gross_loss += net - commission_total  # gross loss 為負值","        ","        trades += 1","    ","    return total_net, total_gross_profit, total_gross_loss, trades","","","def survive_s2(","    result_s2: StressResult,","    *,","    min_trades: int = 30,","    min_pf: float = 1.10,","    max_mdd_pct: Optional[float] = None,","    max_mdd_abs: Optional[float] = None,",") -> bool:","    \"\"\"","    判斷策略是否通過 S2 生存閘門","","    Args:","        result_s2: S2 等級的 StressResult","        min_trades: 最小交易次數","        min_pf: 最小盈利因子","        max_mdd_pct: 最大回撤百分比（如果可用）","        max_mdd_abs: 最大回撤絕對值（備用）","","    Returns:","        bool: 是否通過閘門","    \"\"\"","    # 檢查交易次數","    if result_s2.trades < min_trades:","        return False","    ","    # 檢查盈利因子","    if result_s2.profit_factor < min_pf:","        return False","    ","    # 檢查最大回撤（如果提供）","    if max_mdd_pct is not None:","        # 需要 equity curve 計算百分比回撤，目前暫不實作","        pass","    elif max_mdd_abs is not None:","        if result_s2.mdd_after_cost > max_mdd_abs:","            return False"]}
{"type":"file_chunk","path":"src/control/research_slippage_stress.py","chunk_index":1,"line_start":201,"line_end":262,"content":["    ","    return True","","","def compute_stress_test_passed(","    results: Dict[str, StressResult],","    stress_level: str = \"S3\",",") -> bool:","    \"\"\"","    計算壓力測試是否通過（S3 淨利 > 0）","","    Args:","        results: 壓力測試結果字典","        stress_level: 壓力測試等級（預設 S3）","","    Returns:","        bool: 壓力測試通過標誌","    \"\"\"","    stress_result = results.get(stress_level)","    if stress_result is None:","        return False","    return stress_result.net_after_cost > 0","","","def generate_stress_report(","    results: Dict[str, StressResult],","    slippage_policy: SlippagePolicy,","    survive_s2_flag: bool,","    stress_test_passed_flag: bool,",") -> Dict[str, Any]:","    \"\"\"","    產生壓力測試報告","","    Returns:","        報告字典，包含 policy、矩陣、閘門結果等","    \"\"\"","    matrix = {}","    for level, result in results.items():","        matrix[level] = {","            \"slip_ticks\": result.slip_ticks,","            \"net_after_cost\": result.net_after_cost,","            \"gross_profit\": result.gross_profit,","            \"gross_loss\": result.gross_loss,","            \"profit_factor\": result.profit_factor,","            \"mdd_after_cost\": result.mdd_after_cost,","            \"trades\": result.trades,","        }","    ","    return {","        \"slippage_policy\": {","            \"definition\": slippage_policy.definition,","            \"levels\": slippage_policy.levels,","            \"selection_level\": slippage_policy.selection_level,","            \"stress_level\": slippage_policy.stress_level,","            \"mc_execution_level\": slippage_policy.mc_execution_level,","        },","        \"stress_matrix\": matrix,","        \"survive_s2\": survive_s2_flag,","        \"stress_test_passed\": stress_test_passed_flag,","    }","",""]}
{"type":"file_footer","path":"src/control/research_slippage_stress.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/resolve_cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7901,"sha256":"07c5054252cec35cb7466143b080b03c1a0a80720ebea6f78fd7069ab110c4f3","total_lines":271,"chunk_count":2}
{"type":"file_chunk","path":"src/control/resolve_cli.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Resolve CLI：特徵解析命令列介面","","命令：","fishbro resolve features --season 2026Q1 --dataset-id CME.MNQ --strategy-id S1 --req configs/strategies/S1/features.json","","行為：","- 不允許 build → 只做檢查與載入","- 允許 build → 缺就 build，成功後載入，輸出 bundle 摘要（不輸出整個 array）","","Exit code：","0：已滿足且載入成功","10：已 build（可選）","20：缺失且不允許 build / build_ctx 不足","1：其他錯誤","\"\"\"","","from __future__ import annotations","","import sys","import json","import argparse","from pathlib import Path","from typing import Optional","","from contracts.strategy_features import (","    StrategyFeatureRequirements,","    load_requirements_from_json,",")","from control.feature_resolver import (","    resolve_features,","    MissingFeaturesError,","    ManifestMismatchError,","    BuildNotAllowedError,","    FeatureResolutionError,",")","from control.build_context import BuildContext","","","def main() -> int:","    \"\"\"CLI 主函數\"\"\"","    parser = create_parser()","    args = parser.parse_args()","    ","    try:","        return run_resolve(args)","    except KeyboardInterrupt:","        print(\"\\n中斷執行\", file=sys.stderr)","        return 130","    except Exception as e:","        print(f\"錯誤: {e}\", file=sys.stderr)","        return 1","","","def create_parser() -> argparse.ArgumentParser:","    \"\"\"建立命令列解析器\"\"\"","    parser = argparse.ArgumentParser(","        description=\"解析策略特徵依賴\",","        formatter_class=argparse.ArgumentDefaultsHelpFormatter,","    )","    ","    # 必要參數","    parser.add_argument(","        \"--season\",","        required=True,","        help=\"季節標記，例如 2026Q1\",","    )","    parser.add_argument(","        \"--dataset-id\",","        required=True,","        help=\"資料集 ID，例如 CME.MNQ\",","    )","    ","    # 需求來源（二選一）","    req_group = parser.add_mutually_exclusive_group(required=True)","    req_group.add_argument(","        \"--strategy-id\",","        help=\"策略 ID（用於自動尋找需求檔案）\",","    )","    req_group.add_argument(","        \"--req\",","        type=Path,","        help=\"需求 JSON 檔案路徑\",","    )","    ","    # build 相關參數","    parser.add_argument(","        \"--allow-build\",","        action=\"store_true\",","        help=\"允許自動 build 缺失的特徵\",","    )","    parser.add_argument(","        \"--txt-path\",","        type=Path,","        help=\"原始 TXT 檔案路徑（只有 allow-build 才需要）\",","    )","    parser.add_argument(","        \"--mode\",","        choices=[\"incremental\", \"full\"],","        default=\"incremental\",","        help=\"build 模式（只在 allow-build 時使用）\",","    )","    parser.add_argument(","        \"--outputs-root\",","        type=Path,","        default=Path(\"outputs\"),","        help=\"輸出根目錄\",","    )","    parser.add_argument(","        \"--build-bars-if-missing\",","        action=\"store_true\",","        default=True,","        help=\"如果 bars cache 不存在，是否建立 bars\",","    )","    parser.add_argument(","        \"--no-build-bars-if-missing\",","        action=\"store_false\",","        dest=\"build_bars_if_missing\",","        help=\"不建立 bars cache（即使缺失）\",","    )","    ","    # 輸出選項","    parser.add_argument(","        \"--json\",","        action=\"store_true\",","        help=\"以 JSON 格式輸出結果\",","    )","    parser.add_argument(","        \"--verbose\",","        action=\"store_true\",","        help=\"輸出詳細資訊\",","    )","    ","    return parser","","","def run_resolve(args) -> int:","    \"\"\"執行解析邏輯\"\"\"","    # 1. 載入需求","    requirements = load_requirements(args)","    ","    # 2. 準備 build_ctx（如果需要）","    build_ctx = prepare_build_context(args)","    ","    # 3. 執行解析","    try:","        bundle = resolve_features(","            season=args.season,","            dataset_id=args.dataset_id,","            requirements=requirements,","            outputs_root=args.outputs_root,","            allow_build=args.allow_build,","            build_ctx=build_ctx,","        )","        ","        # 4. 輸出結果","        output_result(bundle, args)","        ","        # 判斷 exit code","        # 如果有 build，回傳 10；否則回傳 0","        # 目前我們無法知道是否有 build，所以暫時回傳 0","        return 0","        ","    except MissingFeaturesError as e:","        print(f\"缺少特徵: {e}\", file=sys.stderr)","        return 20","    except BuildNotAllowedError as e:","        print(f\"不允許 build: {e}\", file=sys.stderr)","        return 20","    except ManifestMismatchError as e:","        print(f\"Manifest 合約不符: {e}\", file=sys.stderr)","        return 1","    except FeatureResolutionError as e:","        print(f\"特徵解析失敗: {e}\", file=sys.stderr)","        return 1","","","def load_requirements(args) -> StrategyFeatureRequirements:","    \"\"\"載入策略特徵需求\"\"\"","    if args.req:","        # 從指定 JSON 檔案載入","        return load_requirements_from_json(str(args.req))","    elif args.strategy_id:","        # 自動尋找需求檔案","        # 優先順序：","        # 1. strategies/{strategy_id}/features.json","        # 2. configs/strategies/{strategy_id}/features.json","        # 3. 當前目錄下的 {strategy_id}_features.json","        ","        possible_paths = [","            Path(f\"configs/strategies/{args.strategy_id}/features.json\"),","            Path(f\"strategies/{args.strategy_id}/features.json\"),  # legacy location","            Path(f\"{args.strategy_id}_features.json\"),","        ]","        ","        for path in possible_paths:","            if path.exists():","                return load_requirements_from_json(str(path))","        "]}
{"type":"file_chunk","path":"src/control/resolve_cli.py","chunk_index":1,"line_start":201,"line_end":271,"content":["        raise FileNotFoundError(","            f\"找不到策略 {args.strategy_id} 的需求檔案。\"","            f\"嘗試的路徑: {[str(p) for p in possible_paths]}\"","        )","    else:","        # 這不應該發生，因為 argparse 確保了二選一","        raise ValueError(\"必須提供 --req 或 --strategy-id\")","","","def prepare_build_context(args) -> Optional[BuildContext]:","    \"\"\"準備 BuildContext\"\"\"","    if not args.allow_build:","        return None","    ","    if not args.txt_path:","        raise ValueError(\"--allow-build 需要 --txt-path\")","    ","    # 驗證 txt_path 存在","    if not args.txt_path.exists():","        raise FileNotFoundError(f\"TXT 檔案不存在: {args.txt_path}\")","    ","    # 轉換 mode 為大寫","    mode = args.mode.upper()","    if mode not in (\"FULL\", \"INCREMENTAL\"):","        raise ValueError(f\"無效的 mode: {args.mode}，必須為 'incremental' 或 'full'\")","    ","    return BuildContext(","        txt_path=args.txt_path,","        mode=mode,","        outputs_root=args.outputs_root,","        build_bars_if_missing=args.build_bars_if_missing,","    )","","","def output_result(bundle, args) -> None:","    \"\"\"輸出解析結果\"\"\"","    if args.json:","        # JSON 格式輸出","        result = {","            \"success\": True,","            \"bundle\": bundle.to_dict(),","            \"series_count\": len(bundle.series),","            \"series_keys\": bundle.list_series(),","        }","        print(json.dumps(result, indent=2, ensure_ascii=False))","    else:","        # 文字格式輸出","        print(f\"✅ 特徵解析成功\")","        print(f\"   資料集: {bundle.dataset_id}\")","        print(f\"   季節: {bundle.season}\")","        print(f\"   特徵數量: {len(bundle.series)}\")","        ","        if args.verbose:","            print(f\"   Metadata:\")","            for key, value in bundle.meta.items():","                if key in (\"files_sha256\", \"manifest_sha256\"):","                    # 縮短 hash 顯示","                    if isinstance(value, str) and len(value) > 16:","                        value = f\"{value[:8]}...{value[-8:]}\"","                print(f\"     {key}: {value}\")","            ","            print(f\"   特徵列表:\")","            for name, tf in bundle.list_series():","                series = bundle.get_series(name, tf)","                print(f\"     {name}@{tf}m: {len(series.ts)} 筆資料\")","","","if __name__ == \"__main__\":","    sys.exit(main())","",""]}
{"type":"file_footer","path":"src/control/resolve_cli.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/season_api.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7085,"sha256":"b19d89844124dec274d5133f6c0a8808d83e9b21c19d464e2bce2b27afd2d4a2","total_lines":215,"chunk_count":2}
{"type":"file_chunk","path":"src/control/season_api.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 15.0: Season-level governance and index builder (Research OS).","","Contracts:","- Do NOT modify Engine / JobSpec / batch artifacts content.","- Season index is a separate tree (season_index/{season}/...).","- Rebuild index is deterministic: stable ordering by batch_id.","- Only reads JSON from artifacts/{batch_id}/metadata.json, index.json, summary.json.","- Writes season_index.json and season_metadata.json using atomic write.","","Environment overrides:","- FISHBRO_SEASON_INDEX_ROOT (default: outputs/season_index)","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass, field","from datetime import datetime, timezone","from pathlib import Path","from typing import Any, Optional","","from control.artifacts import compute_sha256, write_json_atomic","","","def _utc_now_iso() -> str:","    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")","","","def get_season_index_root() -> Path:","    import os","    return Path(os.environ.get(\"FISHBRO_SEASON_INDEX_ROOT\", \"outputs/season_index\"))","","","def _read_json(path: Path) -> dict[str, Any]:","    if not path.exists():","        raise FileNotFoundError(str(path))","    return json.loads(path.read_text(encoding=\"utf-8\"))","","","def _file_sha256(path: Path) -> Optional[str]:","    if not path.exists():","        return None","    return compute_sha256(path.read_bytes())","","","@dataclass","class SeasonMetadata:","    season: str","    frozen: bool = False","    tags: list[str] = field(default_factory=list)","    note: str = \"\"","    created_at: str = \"\"","    updated_at: str = \"\"","","","class SeasonStore:","    \"\"\"","    Store for season_index/{season}/season_index.json and season_metadata.json","    \"\"\"","","    def __init__(self, season_index_root: Path):","        self.root = season_index_root","        self.root.mkdir(parents=True, exist_ok=True)","","    def season_dir(self, season: str) -> Path:","        return self.root / season","","    def index_path(self, season: str) -> Path:","        return self.season_dir(season) / \"season_index.json\"","","    def metadata_path(self, season: str) -> Path:","        return self.season_dir(season) / \"season_metadata.json\"","","    # ---------- metadata ----------","    def get_metadata(self, season: str) -> Optional[SeasonMetadata]:","        path = self.metadata_path(season)","        if not path.exists():","            return None","        data = json.loads(path.read_text(encoding=\"utf-8\"))","        tags = data.get(\"tags\", [])","        if not isinstance(tags, list):","            raise ValueError(\"season_metadata.tags must be a list\")","        return SeasonMetadata(","            season=data[\"season\"],","            frozen=bool(data.get(\"frozen\", False)),","            tags=list(tags),","            note=data.get(\"note\", \"\"),","            created_at=data.get(\"created_at\", \"\"),","            updated_at=data.get(\"updated_at\", \"\"),","        )","","    def set_metadata(self, season: str, meta: SeasonMetadata) -> None:","        path = self.metadata_path(season)","        path.parent.mkdir(parents=True, exist_ok=True)","        payload = {","            \"season\": season,","            \"frozen\": bool(meta.frozen),","            \"tags\": list(meta.tags),","            \"note\": meta.note,","            \"created_at\": meta.created_at,","            \"updated_at\": meta.updated_at,","        }","        write_json_atomic(path, payload)","","    def update_metadata(","        self,","        season: str,","        *,","        tags: Optional[list[str]] = None,","        note: Optional[str] = None,","        frozen: Optional[bool] = None,","    ) -> SeasonMetadata:","        now = _utc_now_iso()","        existing = self.get_metadata(season)","        if existing is None:","            existing = SeasonMetadata(season=season, created_at=now, updated_at=now)","","        if existing.frozen and frozen is False:","            raise ValueError(\"Cannot unfreeze a frozen season\")","","        if tags is not None:","            merged = set(existing.tags)","            merged.update(tags)","            existing.tags = sorted(merged)","","        if note is not None:","            existing.note = note","","        if frozen is not None:","            if frozen is True:","                existing.frozen = True","            elif frozen is False:","                # allowed only when not already frozen","                existing.frozen = False","","        existing.updated_at = now","        self.set_metadata(season, existing)","        return existing","","    def freeze(self, season: str) -> None:","        meta = self.get_metadata(season)","        if meta is None:","            # create metadata on freeze if it doesn't exist","            now = _utc_now_iso()","            meta = SeasonMetadata(season=season, created_at=now, updated_at=now, frozen=True)","            self.set_metadata(season, meta)","            return","","        if not meta.frozen:","            meta.frozen = True","            meta.updated_at = _utc_now_iso()","            self.set_metadata(season, meta)","","    def is_frozen(self, season: str) -> bool:","        meta = self.get_metadata(season)","        return bool(meta and meta.frozen)","","    # ---------- index ----------","    def read_index(self, season: str) -> dict[str, Any]:","        return _read_json(self.index_path(season))","","    def write_index(self, season: str, index_obj: dict[str, Any]) -> None:","        path = self.index_path(season)","        path.parent.mkdir(parents=True, exist_ok=True)","        write_json_atomic(path, index_obj)","","    def rebuild_index(self, artifacts_root: Path, season: str) -> dict[str, Any]:","        \"\"\"","        Scan artifacts_root/*/metadata.json to collect batches where metadata.season == season.","        Then attach hashes for index.json and summary.json (if present).","        Deterministic: sort by batch_id.","        \"\"\"","        if not artifacts_root.exists():","            # no artifacts root -> empty index","            artifacts_root.mkdir(parents=True, exist_ok=True)","","        batches: list[dict[str, Any]] = []","","        # deterministic: sorted by directory name","        for batch_dir in sorted([p for p in artifacts_root.iterdir() if p.is_dir()], key=lambda p: p.name):","            batch_id = batch_dir.name","            meta_path = batch_dir / \"metadata.json\"","            if not meta_path.exists():","                continue","","            # Do NOT swallow corruption: index build should surface errors","            meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))","            if meta.get(\"season\", \"\") != season:","                continue","","            idx_hash = _file_sha256(batch_dir / \"index.json\")","            sum_hash = _file_sha256(batch_dir / \"summary.json\")","","            batches.append(","                {","                    \"batch_id\": batch_id,","                    \"frozen\": bool(meta.get(\"frozen\", False)),","                    \"tags\": sorted(set(meta.get(\"tags\", []) or [])),"]}
{"type":"file_chunk","path":"src/control/season_api.py","chunk_index":1,"line_start":201,"line_end":215,"content":["                    \"note\": meta.get(\"note\", \"\") or \"\",","                    \"index_hash\": idx_hash,","                    \"summary_hash\": sum_hash,","                }","            )","","        out = {","            \"season\": season,","            \"generated_at\": _utc_now_iso(),","            \"batches\": batches,","        }","        self.write_index(season, out)","        return out","",""]}
{"type":"file_footer","path":"src/control/season_api.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/season_compare.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4753,"sha256":"d2d25d60ce40705c6d4ffc40ec67c35704b6932ad370526c945b3f2e172d3e10","total_lines":184,"chunk_count":1}
{"type":"file_chunk","path":"src/control/season_compare.py","chunk_index":0,"line_start":1,"line_end":184,"content":["","\"\"\"","Phase 15.1: Season-level cross-batch comparison helpers.","","Contracts:","- Read-only: only reads season_index.json and artifacts/{batch_id}/summary.json","- No on-the-fly recomputation of batch summary","- Deterministic:","  - Sort by score desc","  - Tie-break by batch_id asc","  - Tie-break by job_id asc","- Robust:","  - Missing/corrupt batch summary is skipped (never 500 the whole season)","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Optional","","","def _read_json(path: Path) -> dict[str, Any]:","    return json.loads(path.read_text(encoding=\"utf-8\"))","","","def _extract_job_id(row: Any) -> Optional[str]:","    if not isinstance(row, dict):","        return None","    # canonical","    if \"job_id\" in row and row[\"job_id\"] is not None:","        return str(row[\"job_id\"])","    # common alternates (defensive)","    if \"id\" in row and row[\"id\"] is not None:","        return str(row[\"id\"])","    return None","","","def _extract_score(row: Any) -> Optional[float]:","    if not isinstance(row, dict):","        return None","","    # canonical","    if \"score\" in row:","        try:","            v = row[\"score\"]","            if v is None:","                return None","            return float(v)","        except Exception:","            return None","","    # alternate: metrics.score","    m = row.get(\"metrics\")","    if isinstance(m, dict) and \"score\" in m:","        try:","            v = m[\"score\"]","            if v is None:","                return None","            return float(v)","        except Exception:","            return None","","    return None","","","@dataclass(frozen=True)","class SeasonTopKResult:","    season: str","    k: int","    items: list[dict[str, Any]]","    skipped_batches: list[str]","","","def merge_season_topk(","    *,","    artifacts_root: Path,","    season_index: dict[str, Any],","    k: int,",") -> SeasonTopKResult:","    \"\"\"","    Merge topk entries across batches listed in season_index.json.","","    Output item schema:","      {","        \"batch_id\": \"...\",","        \"job_id\": \"...\",","        \"score\": 1.23,","        \"row\": {... original topk row ...}","      }","","    Skipping rules:","    - missing summary.json -> skip batch","    - invalid json -> skip batch","    - missing topk list -> treat as empty","    \"\"\"","    season = str(season_index.get(\"season\", \"\"))","    batches = season_index.get(\"batches\", [])","    if not isinstance(batches, list):","        raise ValueError(\"season_index.batches must be a list\")","","    # sanitize k","    try:","        k_int = int(k)","    except Exception:","        k_int = 20","    if k_int <= 0:","        k_int = 20","","    merged: list[dict[str, Any]] = []","    skipped: list[str] = []","","    # deterministic traversal order: batch_id asc","    batch_ids: list[str] = []","    for b in batches:","        if isinstance(b, dict) and \"batch_id\" in b:","            batch_ids.append(str(b[\"batch_id\"]))","    batch_ids = sorted(set(batch_ids))","","    for batch_id in batch_ids:","        summary_path = artifacts_root / batch_id / \"summary.json\"","        if not summary_path.exists():","            skipped.append(batch_id)","            continue","","        try:","            summary = _read_json(summary_path)","        except Exception:","            skipped.append(batch_id)","            continue","","        topk = summary.get(\"topk\", [])","        if not isinstance(topk, list):","            # malformed topk -> treat as skip (stronger safety)","            skipped.append(batch_id)","            continue","","        for row in topk:","            job_id = _extract_job_id(row)","            if job_id is None:","                # cannot tie-break deterministically without job_id","                continue","            score = _extract_score(row)","            merged.append(","                {","                    \"batch_id\": batch_id,","                    \"job_id\": job_id,","                    \"score\": score,","                    \"row\": row,","                }","            )","","    def sort_key(item: dict[str, Any]) -> tuple:","        # score desc; None goes last","        score = item.get(\"score\")","        score_is_none = score is None","        # For numeric scores: use -score","        neg_score = 0.0","        if not score_is_none:","            try:","                neg_score = -float(score)","            except Exception:","                score_is_none = True","                neg_score = 0.0","","        return (","            score_is_none,     # False first, True last","            neg_score,         # smaller first -> higher score first","            str(item.get(\"batch_id\", \"\")),","            str(item.get(\"job_id\", \"\")),","        )","","    merged_sorted = sorted(merged, key=sort_key)","    merged_sorted = merged_sorted[:k_int]","","    return SeasonTopKResult(","        season=season,","        k=k_int,","        items=merged_sorted,","        skipped_batches=sorted(set(skipped)),","    )","",""]}
{"type":"file_footer","path":"src/control/season_compare.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/season_compare_batches.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8174,"sha256":"aec07cf2f8884965b7bb2a838cb70d4feda5b32f0b2f47e8c33809ce46d8d4a5","total_lines":279,"chunk_count":2}
{"type":"file_chunk","path":"src/control/season_compare_batches.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 15.2: Season compare batch cards + lightweight leaderboard.","","Contracts:","- Read-only: reads season_index.json and artifacts/{batch_id}/summary.json","- No on-the-fly recomputation","- Deterministic:","  - Batches list sorted by batch_id asc","  - Leaderboard sorted by score desc, tie-break batch_id asc, job_id asc","- Robust:","  - Missing/corrupt summary.json => summary_ok=False, keep other fields","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Optional","","","def _read_json(path: Path) -> dict[str, Any]:","    return json.loads(path.read_text(encoding=\"utf-8\"))","","","def _safe_get_job_id(row: Any) -> Optional[str]:","    if not isinstance(row, dict):","        return None","    if row.get(\"job_id\") is not None:","        return str(row[\"job_id\"])","    if row.get(\"id\") is not None:","        return str(row[\"id\"])","    return None","","","def _safe_get_score(row: Any) -> Optional[float]:","    if not isinstance(row, dict):","        return None","    if \"score\" in row:","        try:","            v = row[\"score\"]","            if v is None:","                return None","            return float(v)","        except Exception:","            return None","    m = row.get(\"metrics\")","    if isinstance(m, dict) and \"score\" in m:","        try:","            v = m[\"score\"]","            if v is None:","                return None","            return float(v)","        except Exception:","            return None","    return None","","","def _extract_group_key(row: Any, group_by: str) -> str:","    \"\"\"","    group_by candidates:","      - \"strategy_id\"","      - \"dataset_id\"","    If not present, return \"unknown\".","    \"\"\"","    if not isinstance(row, dict):","        return \"unknown\"","    v = row.get(group_by)","    if v is None:","        # sometimes nested","        meta = row.get(\"meta\")","        if isinstance(meta, dict):","            v = meta.get(group_by)","    return str(v) if v is not None else \"unknown\"","","","@dataclass(frozen=True)","class SeasonBatchesResult:","    season: str","    batches: list[dict[str, Any]]","    skipped_summaries: list[str]","","","def build_season_batch_cards(","    *,","    artifacts_root: Path,","    season_index: dict[str, Any],",") -> SeasonBatchesResult:","    \"\"\"","    Build deterministic batch cards for a season.","","    For each batch_id in season_index.batches:","      - frozen/tags/note/index_hash/summary_hash are read from season_index (source of truth)","      - summary.json is read best-effort:","          top_job_id, top_score, topk_size","      - missing/corrupt summary => summary_ok=False","    \"\"\"","    season = str(season_index.get(\"season\", \"\"))","    batches_in = season_index.get(\"batches\", [])","    if not isinstance(batches_in, list):","        raise ValueError(\"season_index.batches must be a list\")","","    # deterministic batch_id list","    by_id: dict[str, dict[str, Any]] = {}","    for b in batches_in:","        if not isinstance(b, dict) or \"batch_id\" not in b:","            continue","        batch_id = str(b[\"batch_id\"])","        by_id[batch_id] = b","","    batch_ids = sorted(by_id.keys())","","    cards: list[dict[str, Any]] = []","    skipped: list[str] = []","","    for batch_id in batch_ids:","        b = by_id[batch_id]","        card: dict[str, Any] = {","            \"batch_id\": batch_id,","            \"frozen\": bool(b.get(\"frozen\", False)),","            \"tags\": list(b.get(\"tags\", []) or []),","            \"note\": b.get(\"note\", \"\") or \"\",","            \"index_hash\": b.get(\"index_hash\"),","            \"summary_hash\": b.get(\"summary_hash\"),","            # summary-derived","            \"summary_ok\": True,","            \"top_job_id\": None,","            \"top_score\": None,","            \"topk_size\": 0,","        }","","        summary_path = artifacts_root / batch_id / \"summary.json\"","        if not summary_path.exists():","            card[\"summary_ok\"] = False","            skipped.append(batch_id)","            cards.append(card)","            continue","","        try:","            s = _read_json(summary_path)","            topk = s.get(\"topk\", [])","            if not isinstance(topk, list):","                raise ValueError(\"summary.topk must be list\")","","            card[\"topk_size\"] = len(topk)","            if len(topk) > 0:","                first = topk[0]","                card[\"top_job_id\"] = _safe_get_job_id(first)","                card[\"top_score\"] = _safe_get_score(first)","        except Exception:","            card[\"summary_ok\"] = False","            skipped.append(batch_id)","","        cards.append(card)","","    return SeasonBatchesResult(season=season, batches=cards, skipped_summaries=sorted(set(skipped)))","","","def build_season_leaderboard(","    *,","    artifacts_root: Path,","    season_index: dict[str, Any],","    group_by: str = \"strategy_id\",","    per_group: int = 3,",") -> dict[str, Any]:","    \"\"\"","    Build a grouped leaderboard from batch summaries' topk rows.","","    Returns:","      {","        \"season\": \"...\",","        \"group_by\": \"strategy_id\",","        \"per_group\": 3,","        \"groups\": [","           {\"key\": \"...\", \"items\": [...]},","           ...","        ],","        \"skipped_batches\": [...]","      }","    \"\"\"","    season = str(season_index.get(\"season\", \"\"))","    batches_in = season_index.get(\"batches\", [])","    if not isinstance(batches_in, list):","        raise ValueError(\"season_index.batches must be a list\")","","    if group_by not in (\"strategy_id\", \"dataset_id\"):","        raise ValueError(\"group_by must be 'strategy_id' or 'dataset_id'\")","","    try:","        per_group_i = int(per_group)","    except Exception:","        per_group_i = 3","    if per_group_i <= 0:","        per_group_i = 3","","    # deterministic batch traversal: batch_id asc","    batch_ids = sorted({str(b[\"batch_id\"]) for b in batches_in if isinstance(b, dict) and \"batch_id\" in b})","","    merged: list[dict[str, Any]] = []"]}
{"type":"file_chunk","path":"src/control/season_compare_batches.py","chunk_index":1,"line_start":201,"line_end":279,"content":["    skipped: list[str] = []","","    for batch_id in batch_ids:","        p = artifacts_root / batch_id / \"summary.json\"","        if not p.exists():","            skipped.append(batch_id)","            continue","        try:","            s = _read_json(p)","            topk = s.get(\"topk\", [])","            if not isinstance(topk, list):","                skipped.append(batch_id)","                continue","            for row in topk:","                job_id = _safe_get_job_id(row)","                if job_id is None:","                    continue","                score = _safe_get_score(row)","                merged.append(","                    {","                        \"batch_id\": batch_id,","                        \"job_id\": job_id,","                        \"score\": score,","                        \"group\": _extract_group_key(row, group_by),","                        \"row\": row,","                    }","                )","        except Exception:","            skipped.append(batch_id)","            continue","","    def sort_key(it: dict[str, Any]) -> tuple:","        score = it.get(\"score\")","        score_is_none = score is None","        neg_score = 0.0","        if not score_is_none:","            try:","                # score is not None at this point, but mypy doesn't know","                neg_score = -float(score)  # type: ignore[arg-type]","            except Exception:","                score_is_none = True","                neg_score = 0.0","        return (","            score_is_none,","            neg_score,","            str(it.get(\"batch_id\", \"\")),","            str(it.get(\"job_id\", \"\")),","        )","","    merged_sorted = sorted(merged, key=sort_key)","","    # group, keep top per_group_i in deterministic order (already sorted)","    groups: dict[str, list[dict[str, Any]]] = {}","    for it in merged_sorted:","        key = str(it.get(\"group\", \"unknown\"))","        if key not in groups:","            groups[key] = []","        if len(groups[key]) < per_group_i:","            groups[key].append(","                {","                    \"batch_id\": it[\"batch_id\"],","                    \"job_id\": it[\"job_id\"],","                    \"score\": it[\"score\"],","                    \"row\": it[\"row\"],","                }","            )","","    # deterministic group ordering: key asc","    out_groups = [{\"key\": k, \"items\": groups[k]} for k in sorted(groups.keys())]","","    return {","        \"season\": season,","        \"group_by\": group_by,","        \"per_group\": per_group_i,","        \"groups\": out_groups,","        \"skipped_batches\": sorted(set(skipped)),","    }","",""]}
{"type":"file_footer","path":"src/control/season_compare_batches.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/season_export.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9531,"sha256":"72bbfba75eb4ac5c7bc47593b37a1f6e149d910978be83b14be58549566108fd","total_lines":282,"chunk_count":2}
{"type":"file_chunk","path":"src/control/season_export.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 15.3: Season freeze package / export pack.","","Contracts:","- Controlled mutation: writes only under exports root (default outputs/exports).","- Does NOT modify artifacts/ or season_index/ trees.","- Requires season is frozen (governance hardening).","- Deterministic:","  - batches sorted by batch_id asc","  - manifest files sorted by rel_path asc","- Auditable:","  - package_manifest.json includes sha256 for each exported file","  - includes manifest_sha256 (sha of the manifest bytes)","\"\"\"","","from __future__ import annotations","","import json","import os","import shutil","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Optional","","from control.artifacts import compute_sha256, write_atomic_json","from control.season_api import SeasonStore","from control.batch_api import read_summary, read_index","from utils.write_scope import WriteScope","","","def get_exports_root() -> Path:","    return Path(os.environ.get(\"FISHBRO_EXPORTS_ROOT\", \"outputs/exports\"))","","","def _copy_file(src: Path, dst: Path) -> None:","    dst.parent.mkdir(parents=True, exist_ok=True)","    shutil.copy2(src, dst)","","","def _file_sha256(path: Path) -> str:","    return compute_sha256(path.read_bytes())","","","@dataclass(frozen=True)","class ExportResult:","    season: str","    export_dir: Path","    manifest_path: Path","    manifest_sha256: str","    exported_files: list[dict[str, Any]]","    missing_files: list[str]","","","def export_season_package(","    *,","    season: str,","    artifacts_root: Path,","    season_index_root: Path,","    exports_root: Optional[Path] = None,",") -> ExportResult:","    \"\"\"","    Export a frozen season into an immutable, auditable package directory.","","    Package layout:","      exports/seasons/{season}/","        package_manifest.json","        season_index.json","        season_metadata.json","        batches/{batch_id}/metadata.json","        batches/{batch_id}/index.json (optional if missing)","        batches/{batch_id}/summary.json (optional if missing)","    \"\"\"","    exports_root = exports_root or get_exports_root()","    store = SeasonStore(season_index_root)","","    if not store.is_frozen(season):","        raise PermissionError(\"Season must be frozen before export\")","","    # must have season index","    season_index = store.read_index(season)  # FileNotFoundError surfaces to API as 404","","    season_dir = exports_root / \"seasons\" / season","    batches_dir = season_dir / \"batches\"","    season_dir.mkdir(parents=True, exist_ok=True)","    batches_dir.mkdir(parents=True, exist_ok=True)","","    # Build the set of allowed relative paths according to export‑pack spec.","    # We'll collect them as we go, then create a WriteScope that permits exactly those paths.","    allowed_rel_files: set[str] = set()","    exported_files: list[dict[str, Any]] = []","    missing: list[str] = []","","    # Helper to record an allowed file and copy it","    def copy_and_allow(src: Path, dst: Path, rel: str) -> None:","        _copy_file(src, dst)","        allowed_rel_files.add(rel)","        exported_files.append({\"path\": rel, \"sha256\": _file_sha256(dst)})","","    # 1) copy season_index.json + season_metadata.json (metadata may not exist; if missing -> we still record missing)","    src_index = season_index_root / season / \"season_index.json\"","    dst_index = season_dir / \"season_index.json\"","    copy_and_allow(src_index, dst_index, \"season_index.json\")","","    src_meta = season_index_root / season / \"season_metadata.json\"","    dst_meta = season_dir / \"season_metadata.json\"","    if src_meta.exists():","        copy_and_allow(src_meta, dst_meta, \"season_metadata.json\")","    else:","        missing.append(\"season_metadata.json\")","","    # 2) copy batch files referenced by season index","    batches = season_index.get(\"batches\", [])","    if not isinstance(batches, list):","        raise ValueError(\"season_index.batches must be a list\")","","    batch_ids = sorted(","        {str(b[\"batch_id\"]) for b in batches if isinstance(b, dict) and \"batch_id\" in b}","    )","","    for batch_id in batch_ids:","        # metadata.json is the anchor","        src_batch_meta = artifacts_root / batch_id / \"metadata.json\"","        rel_meta = str(Path(\"batches\") / batch_id / \"metadata.json\")","        dst_batch_meta = batches_dir / batch_id / \"metadata.json\"","        if src_batch_meta.exists():","            copy_and_allow(src_batch_meta, dst_batch_meta, rel_meta)","        else:","            missing.append(rel_meta)","","        # index.json optional","        src_idx = artifacts_root / batch_id / \"index.json\"","        rel_idx = str(Path(\"batches\") / batch_id / \"index.json\")","        dst_idx = batches_dir / batch_id / \"index.json\"","        if src_idx.exists():","            copy_and_allow(src_idx, dst_idx, rel_idx)","        else:","            missing.append(rel_idx)","","        # summary.json optional","        src_sum = artifacts_root / batch_id / \"summary.json\"","        rel_sum = str(Path(\"batches\") / batch_id / \"summary.json\")","        dst_sum = batches_dir / batch_id / \"summary.json\"","        if src_sum.exists():","            copy_and_allow(src_sum, dst_sum, rel_sum)","        else:","            missing.append(rel_sum)","","    # 3) build deterministic manifest (sort by path)","    exported_files_sorted = sorted(exported_files, key=lambda x: x[\"path\"])","","    manifest_obj = {","        \"season\": season,","        \"generated_at\": season_index.get(\"generated_at\", \"\"),","        \"source_roots\": {","            \"artifacts_root\": str(artifacts_root),","            \"season_index_root\": str(season_index_root),","        },","        \"deterministic_order\": {","            \"batches\": \"batch_id asc\",","            \"files\": \"path asc\",","        },","        \"files\": exported_files_sorted,","        \"missing_files\": sorted(set(missing)),","    }","","    manifest_path = season_dir / \"package_manifest.json\"","    allowed_rel_files.add(\"package_manifest.json\")","    write_atomic_json(manifest_path, manifest_obj)","","    manifest_sha256 = compute_sha256(manifest_path.read_bytes())","","    # write back manifest hash (2nd pass) for self-audit (still deterministic because it depends on bytes)","    manifest_obj2 = dict(manifest_obj)","    manifest_obj2[\"manifest_sha256\"] = manifest_sha256","    write_atomic_json(manifest_path, manifest_obj2)","    manifest_sha2562 = compute_sha256(manifest_path.read_bytes())","","    # 4) create replay_index.json for compare replay without artifacts","    replay_index_path = season_dir / \"replay_index.json\"","    allowed_rel_files.add(\"replay_index.json\")","    replay_index = _build_replay_index(","        season=season,","        season_index=season_index,","        artifacts_root=artifacts_root,","        batches_dir=batches_dir,","    )","    write_atomic_json(replay_index_path, replay_index)","    exported_files_sorted.append(","        {","            \"path\": str(Path(\"replay_index.json\")),","            \"sha256\": _file_sha256(replay_index_path),","        }","    )","","    # Now create a WriteScope that permits exactly the files we have written.","    # This scope will be used to validate any future writes (none in this function).","    # We also add a guard for the manifest write (already done) and replay_index write.","    scope = WriteScope(","        root_dir=season_dir,"]}
{"type":"file_chunk","path":"src/control/season_export.py","chunk_index":1,"line_start":201,"line_end":282,"content":["        allowed_rel_files=frozenset(allowed_rel_files),","        allowed_rel_prefixes=(),","    )","    # Verify that all exported files are allowed (should be true by construction)","    for ef in exported_files_sorted:","        scope.assert_allowed_rel(ef[\"path\"])","","    return ExportResult(","        season=season,","        export_dir=season_dir,","        manifest_path=manifest_path,","        manifest_sha256=manifest_sha2562,","        exported_files=exported_files_sorted,","        missing_files=sorted(set(missing)),","    )","","","def _build_replay_index(","    season: str,","    season_index: dict[str, Any],","    artifacts_root: Path,","    batches_dir: Path,",") -> dict[str, Any]:","    \"\"\"","    Build replay index for compare replay without artifacts.","    ","    Contains:","    - season metadata","    - batch summaries (topk, metrics)","    - batch indices (job list)","    - deterministic ordering","    \"\"\"","    batches = season_index.get(\"batches\", [])","    if not isinstance(batches, list):","        raise ValueError(\"season_index.batches must be a list\")","","    batch_ids = sorted(","        {str(b[\"batch_id\"]) for b in batches if isinstance(b, dict) and \"batch_id\" in b}","    )","","    replay_batches: list[dict[str, Any]] = []","    for batch_id in batch_ids:","        batch_info: dict[str, Any] = {\"batch_id\": batch_id}","        ","        # Try to read summary.json","        summary_path = artifacts_root / batch_id / \"summary.json\"","        if summary_path.exists():","            try:","                summary = read_summary(artifacts_root, batch_id)","                batch_info[\"summary\"] = {","                    \"topk\": summary.get(\"topk\", []),","                    \"metrics\": summary.get(\"metrics\", {}),","                }","            except Exception:","                batch_info[\"summary\"] = None","        else:","            batch_info[\"summary\"] = None","        ","        # Try to read index.json","        index_path = artifacts_root / batch_id / \"index.json\"","        if index_path.exists():","            try:","                index = read_index(artifacts_root, batch_id)","                batch_info[\"index\"] = index","            except Exception:","                batch_info[\"index\"] = None","        else:","            batch_info[\"index\"] = None","        ","        replay_batches.append(batch_info)","","    return {","        \"season\": season,","        \"generated_at\": season_index.get(\"generated_at\", \"\"),","        \"batches\": replay_batches,","        \"deterministic_order\": {","            \"batches\": \"batch_id asc\",","            \"files\": \"path asc\",","        },","    }","",""]}
{"type":"file_footer","path":"src/control/season_export.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/season_export_replay.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7688,"sha256":"6e67dfc188688deb63ceb927d4b6bd50f363decb94e13978db559a2bebcd91ce","total_lines":253,"chunk_count":2}
{"type":"file_chunk","path":"src/control/season_export_replay.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 16: Export Pack Replay Mode.","","Allows compare endpoints to work from an exported season package","without requiring access to the original artifacts/ directory.","","Key contracts:","- Read-only: only reads from exports root, never writes","- Deterministic: same ordering as original compare endpoints","- Fallback: if replay_index.json missing, raise FileNotFoundError","- No artifacts dependency: does not require artifacts/ directory","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Optional","","","@dataclass(frozen=True)","class ReplaySeasonTopkResult:","    season: str","    k: int","    items: list[dict[str, Any]]","    skipped_batches: list[str]","","","@dataclass(frozen=True)","class ReplaySeasonBatchCardsResult:","    season: str","    batches: list[dict[str, Any]]","    skipped_summaries: list[str]","","","@dataclass(frozen=True)","class ReplaySeasonLeaderboardResult:","    season: str","    group_by: str","    per_group: int","    groups: list[dict[str, Any]]","","","def load_replay_index(exports_root: Path, season: str) -> dict[str, Any]:","    \"\"\"","    Load replay_index.json from an exported season package.","    ","    Raises:","        FileNotFoundError: if replay_index.json does not exist","        ValueError: if JSON is invalid","    \"\"\"","    replay_path = exports_root / \"seasons\" / season / \"replay_index.json\"","    if not replay_path.exists():","        raise FileNotFoundError(f\"replay_index.json not found for season {season}\")","    ","    text = replay_path.read_text(encoding=\"utf-8\")","    return json.loads(text)","","","def replay_season_topk(","    exports_root: Path,","    season: str,","    k: int = 20,",") -> ReplaySeasonTopkResult:","    \"\"\"","    Replay cross-batch TopK from exported season package.","    ","    Implementation mirrors merge_season_topk but uses replay_index.json","    instead of reading artifacts/{batch_id}/summary.json.","    \"\"\"","    replay_index = load_replay_index(exports_root, season)","    ","    all_items: list[dict[str, Any]] = []","    skipped_batches: list[str] = []","    ","    for batch_info in replay_index.get(\"batches\", []):","        batch_id = batch_info.get(\"batch_id\", \"\")","        summary = batch_info.get(\"summary\")","        ","        if summary is None:","            skipped_batches.append(batch_id)","            continue","        ","        topk = summary.get(\"topk\", [])","        if not isinstance(topk, list):","            skipped_batches.append(batch_id)","            continue","        ","        # Add batch_id to each item for traceability","        for item in topk:","            if isinstance(item, dict):","                item_copy = dict(item)","                item_copy[\"_batch_id\"] = batch_id","                all_items.append(item_copy)","    ","    # Sort by (-score, batch_id, job_id) for deterministic ordering","    def _sort_key(item: dict[str, Any]) -> tuple:","        # Score (descending, so use negative)","        score = item.get(\"score\")","        if isinstance(score, (int, float)):","            score_val = -float(score)  # Negative for descending sort","        else:","            score_val = float(\"inf\")  # Missing scores go last","        ","        # Batch ID (from _batch_id added earlier)","        batch_id = item.get(\"_batch_id\", \"\")","        ","        # Job ID","        job_id = item.get(\"job_id\", \"\")","        ","        return (score_val, batch_id, job_id)","    ","    sorted_items = sorted(all_items, key=_sort_key)","    topk_items = sorted_items[:k] if k > 0 else sorted_items","    ","    return ReplaySeasonTopkResult(","        season=season,","        k=k,","        items=topk_items,","        skipped_batches=skipped_batches,","    )","","","def replay_season_batch_cards(","    exports_root: Path,","    season: str,",") -> ReplaySeasonBatchCardsResult:","    \"\"\"","    Replay batch-level compare cards from exported season package.","    ","    Implementation mirrors build_season_batch_cards but uses replay_index.json.","    Deterministic ordering: batches sorted by batch_id ascending.","    \"\"\"","    replay_index = load_replay_index(exports_root, season)","    ","    batches: list[dict[str, Any]] = []","    skipped_summaries: list[str] = []","    ","    # Sort batches by batch_id for deterministic output","    batch_infos = replay_index.get(\"batches\", [])","    sorted_batch_infos = sorted(batch_infos, key=lambda b: b.get(\"batch_id\", \"\"))","    ","    for batch_info in sorted_batch_infos:","        batch_id = batch_info.get(\"batch_id\", \"\")","        summary = batch_info.get(\"summary\")","        index = batch_info.get(\"index\")","        ","        if summary is None:","            skipped_summaries.append(batch_id)","            continue","        ","        # Build batch card","        card: dict[str, Any] = {","            \"batch_id\": batch_id,","            \"summary\": summary,","        }","        ","        if index is not None:","            card[\"index\"] = index","        ","        batches.append(card)","    ","    return ReplaySeasonBatchCardsResult(","        season=season,","        batches=batches,","        skipped_summaries=skipped_summaries,","    )","","","def replay_season_leaderboard(","    exports_root: Path,","    season: str,","    group_by: str = \"strategy_id\",","    per_group: int = 3,",") -> ReplaySeasonLeaderboardResult:","    \"\"\"","    Replay grouped leaderboard from exported season package.","    ","    Implementation mirrors build_season_leaderboard but uses replay_index.json.","    \"\"\"","    replay_index = load_replay_index(exports_root, season)","    ","    # Collect all items with grouping key","    items_by_group: dict[str, list[dict[str, Any]]] = {}","    ","    for batch_info in replay_index.get(\"batches\", []):","        summary = batch_info.get(\"summary\")","        if summary is None:","            continue","        ","        topk = summary.get(\"topk\", [])","        if not isinstance(topk, list):","            continue","        ","        for item in topk:","            if not isinstance(item, dict):","                continue","            "]}
{"type":"file_chunk","path":"src/control/season_export_replay.py","chunk_index":1,"line_start":201,"line_end":253,"content":["            # Add batch_id for deterministic sorting","            item_copy = dict(item)","            item_copy[\"_batch_id\"] = batch_info.get(\"batch_id\", \"\")","            ","            # Extract grouping key","            group_key = item_copy.get(group_by, \"\")","            if not isinstance(group_key, str):","                group_key = str(group_key)","            ","            if group_key not in items_by_group:","                items_by_group[group_key] = []","            ","            items_by_group[group_key].append(item_copy)","    ","    # Sort items within each group by (-score, batch_id, job_id) for deterministic ordering","    def _sort_key(item: dict[str, Any]) -> tuple:","        # Score (descending, so use negative)","        score = item.get(\"score\")","        if isinstance(score, (int, float)):","            score_val = -float(score)  # Negative for descending sort","        else:","            score_val = float(\"inf\")  # Missing scores go last","        ","        # Batch ID (item may not have _batch_id in leaderboard context)","        batch_id = item.get(\"_batch_id\", item.get(\"batch_id\", \"\"))","        ","        # Job ID","        job_id = item.get(\"job_id\", \"\")","        ","        return (score_val, batch_id, job_id)","    ","    groups: list[dict[str, Any]] = []","    for group_key, group_items in items_by_group.items():","        sorted_items = sorted(group_items, key=_sort_key)","        top_items = sorted_items[:per_group] if per_group > 0 else sorted_items","        ","        groups.append({","            \"key\": group_key,","            \"items\": top_items,","            \"total\": len(group_items),","        })","    ","    # Sort groups by key for deterministic output","    groups_sorted = sorted(groups, key=lambda g: g[\"key\"])","    ","    return ReplaySeasonLeaderboardResult(","        season=season,","        group_by=group_by,","        per_group=per_group,","        groups=groups_sorted,","    )","",""]}
{"type":"file_footer","path":"src/control/season_export_replay.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/seed_demo_run.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5339,"sha256":"58b1a0df13b7b136d2179fb61009cea4902e1facf770d5963de23ec4c6261c63","total_lines":190,"chunk_count":1}
{"type":"file_chunk","path":"src/control/seed_demo_run.py","chunk_index":0,"line_start":1,"line_end":190,"content":["","\"\"\"Seed demo run for Viewer validation.","","Creates a DONE job with minimal artifacts for Viewer testing.","Does NOT run engine - only writes files.","\"\"\"","","from __future__ import annotations","","import json","import os","import sqlite3","from datetime import datetime, timezone","from pathlib import Path","from uuid import uuid4","","from control.jobs_db import init_db","from control.report_links import build_report_link","from control.types import JobStatus","from core.paths import ensure_run_dir","","# Default DB path (same as api.py)","DEFAULT_DB_PATH = Path(\"outputs/jobs.db\")","","","def get_db_path() -> Path:","    \"\"\"Get database path from environment or default.\"\"\"","    db_path_str = os.getenv(\"JOBS_DB_PATH\")","    if db_path_str:","        return Path(db_path_str)","    return DEFAULT_DB_PATH","","","def main() -> str:","    \"\"\"","    Create demo job with minimal artifacts.","    ","    Returns:","        run_id of created demo job","        ","    Contract:","        - Never raises exceptions","        - Does NOT import engine","        - Does NOT run backtest","        - Does NOT touch worker","        - Does NOT need dataset","    \"\"\"","    try:","        # Generate run_id","        timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")","        run_id = f\"demo_{timestamp}\"","        ","        # Initialize DB if needed","        db_path = get_db_path()","        init_db(db_path)","        ","        # Create outputs directory (use standard path structure: outputs/<season>/runs/<run_id>/)","        outputs_root = Path(\"outputs\")","        season = \"2026Q1\"  # Default season for demo","        run_dir = ensure_run_dir(outputs_root, season, run_id)","        ","        # Write minimal artifacts","        _write_manifest(run_dir, run_id, season)","        _write_winners_v2(run_dir)","        _write_governance(run_dir)","        _write_kpi(run_dir)","        ","        # Create job record (status = DONE)","        _create_demo_job(db_path, run_id, season)","        ","        return run_id","    ","    except Exception as e:","        print(f\"ERROR: Failed to create demo job: {e}\")","        raise","","","def _write_manifest(run_dir: Path, run_id: str, season: str) -> None:","    \"\"\"Write minimal manifest.json.\"\"\"","    manifest = {","        \"run_id\": run_id,","        \"season\": season,","        \"config_hash\": \"demo-config-hash\",","        \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        \"stages\": [],","        \"meta\": {},","    }","    ","    manifest_path = run_dir / \"manifest.json\"","    with manifest_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f, indent=2, sort_keys=True)","","","def _write_winners_v2(run_dir: Path) -> None:","    \"\"\"Write minimal winners_v2.json.\"\"\"","    winners_v2 = {","        \"config_hash\": \"demo-config-hash\",","        \"schema_version\": \"v2\",","        \"run_id\": \"demo\",","        \"rows\": [],","        \"meta\": {},","    }","    ","    winners_path = run_dir / \"winners_v2.json\"","    with winners_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(winners_v2, f, indent=2, sort_keys=True)","","","def _write_governance(run_dir: Path) -> None:","    \"\"\"Write minimal governance.json.\"\"\"","    governance = {","        \"config_hash\": \"demo-config-hash\",","        \"schema_version\": \"v1\",","        \"run_id\": \"demo\",","        \"rows\": [],","        \"meta\": {},","    }","    ","    governance_path = run_dir / \"governance.json\"","    with governance_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(governance, f, indent=2, sort_keys=True)","","","def _write_kpi(run_dir: Path) -> None:","    \"\"\"Write kpi.json with KPI values aligned with Phase 6.1 registry.\"\"\"","    kpi = {","        \"net_profit\": 123456,","        \"max_drawdown\": -0.18,","        \"num_trades\": 42,","        \"final_score\": 1.23,","    }","    ","    kpi_path = run_dir / \"kpi.json\"","    with kpi_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(kpi, f, indent=2, sort_keys=True)","","","def _create_demo_job(db_path: Path, run_id: str, season: str) -> None:","    \"\"\"","    Create demo job record in database.","    ","    Uses direct SQL to create job with DONE status and report_link.","    \"\"\"","    job_id = str(uuid4())","    now = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","    ","    # Generate report link","    report_link = build_report_link(season, run_id)","    ","    conn = sqlite3.connect(str(db_path))","    try:","        # Ensure schema","        from control.jobs_db import ensure_schema","        ensure_schema(conn)","        ","        # Insert job with DONE status","        # Note: requested_pause is required (defaults to 0)","        conn.execute(\"\"\"","            INSERT INTO jobs (","                job_id, status, created_at, updated_at,","                season, dataset_id, outputs_root, config_hash,","                config_snapshot_json, requested_pause, run_id, report_link","            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)","        \"\"\", (","            job_id,","            JobStatus.DONE.value,","            now,","            now,","            season,","            \"demo_dataset\",","            \"outputs\",","            \"demo-config-hash\",","            json.dumps({}),","            0,  # requested_pause","            run_id,","            report_link,","        ))","        ","        conn.commit()","    finally:","        conn.close()","","","if __name__ == \"__main__\":","    run_id = main()","    print(f\"Demo job created: {run_id}\")","    print(f\"Outputs: outputs/seasons/2026Q1/runs/{run_id}/\")","    print(f\"Report link: /b5?season=2026Q1&run_id={run_id}\")","",""]}
{"type":"file_footer","path":"src/control/seed_demo_run.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/server_main.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2759,"sha256":"72d40cbc2627c4bcebd8a8fcd2326c45b5533dc57f6dd548c6a2145477c93605","total_lines":98,"chunk_count":1}
{"type":"file_chunk","path":"src/control/server_main.py","chunk_index":0,"line_start":1,"line_end":98,"content":["#!/usr/bin/env python3","\"\"\"","Control API Server Entrypoint.","","Zero‑Violation Split‑Brain Architecture: UI HTTP Client + Control API Authority.","This is the standalone entrypoint for the Control API server (FastAPI + Uvicorn).","\"\"\"","","import argparse","import logging","import os","import sys","from pathlib import Path","","# Ensure the module can be imported","sys.path.insert(0, str(Path(__file__).parent.parent.parent))","","from control.api import app","","","def parse_args() -> argparse.Namespace:","    \"\"\"Parse command‑line arguments.\"\"\"","    parser = argparse.ArgumentParser(","        description=\"FishBroWFS V2 Control API Server\",","        formatter_class=argparse.ArgumentDefaultsHelpFormatter,","    )","    parser.add_argument(","        \"--host\",","        default=os.getenv(\"CONTROL_API_HOST\", \"127.0.0.1\"),","        help=\"Host to bind the server\",","    )","    parser.add_argument(","        \"--port\",","        type=int,","        default=int(os.getenv(\"CONTROL_API_PORT\", \"8000\")),","        help=\"Port to bind the server\",","    )","    parser.add_argument(","        \"--reload\",","        action=\"store_true\",","        default=False,","        help=\"Enable auto‑reload (development only)\",","    )","    parser.add_argument(","        \"--log-level\",","        choices=[\"debug\", \"info\", \"warning\", \"error\", \"critical\"],","        default=\"info\",","        help=\"Logging level\",","    )","    parser.add_argument(","        \"--workers\",","        type=int,","        default=1,","        help=\"Number of worker processes (Uvicorn workers)\",","    )","    return parser.parse_args()","","","def main() -> None:","    \"\"\"Main entrypoint.\"\"\"","    args = parse_args()","","    # Configure logging","    logging.basicConfig(","        level=getattr(logging, args.log_level.upper()),","        format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",","    )","","    # Import uvicorn only when needed (avoid extra dependency for CLI)","    try:","        import uvicorn","    except ImportError:","        print(\"ERROR: uvicorn is required to run the Control API server.\", file=sys.stderr)","        print(\"Install with: pip install uvicorn[standard]\", file=sys.stderr)","        sys.exit(1)","","    # Log startup info","    logging.info(","        \"Starting Control API server on %s:%d (reload=%s, workers=%d)\",","        args.host, args.port, args.reload, args.workers,","    )","    logging.info(\"Service identity endpoint: http://%s:%d/__identity\", args.host, args.port)","    logging.info(\"Health endpoint: http://%s:%d/health\", args.host, args.port)","    logging.info(\"OpenAPI docs: http://%s:%d/docs\", args.host, args.port)","","    # Run the server","    uvicorn.run(","        \"control.api:app\",","        host=args.host,","        port=args.port,","        reload=args.reload,","        workers=args.workers,","        log_level=args.log_level,","    )","","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"src/control/server_main.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/shared_build.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":31887,"sha256":"31a978e642bc4f0f2c88552ea42520a5afeb125d86f20dc4169bc4912c59e2ed","total_lines":861,"chunk_count":5}
{"type":"file_chunk","path":"src/control/shared_build.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Shared Data Build 控制器","","提供 FULL/INCREMENTAL 模式的 shared data build，包含 fingerprint scan/diff 作為 guardrails。","\"\"\"","","from __future__ import annotations","","import hashlib","from pathlib import Path","from typing import Any, Dict, List, Literal, Optional","import numpy as np","import pandas as pd","","from contracts.dimensions import canonical_json","from contracts.fingerprint import FingerprintIndex","from contracts.features import FeatureRegistry, default_feature_registry","from core.fingerprint import (","    build_fingerprint_index_from_raw_ingest,","    compare_fingerprint_indices,",")","from control.fingerprint_store import (","    fingerprint_index_path,","    load_fingerprint_index_if_exists,","    write_fingerprint_index,",")","from data.raw_ingest import RawIngestResult, ingest_raw_txt","from control.shared_manifest import write_shared_manifest","from control.bars_store import (","    bars_dir,","    normalized_bars_path,","    resampled_bars_path,","    write_npz_atomic,","    load_npz,","    sha256_file,",")","from core.resampler import (","    get_session_spec_for_dataset,","    normalize_raw_bars,","    resample_ohlcv,","    compute_safe_recompute_start,","    SessionSpecTaipei,",")","from core.features import compute_features_for_tf","from control.features_store import (","    features_dir,","    features_path,","    write_features_npz_atomic,","    load_features_npz,","    compute_features_sha256_dict,",")","from control.features_manifest import (","    features_manifest_path,","    write_features_manifest,","    build_features_manifest_data,","    feature_spec_to_dict,",")","","","BuildMode = Literal[\"FULL\", \"INCREMENTAL\"]","","","class IncrementalBuildRejected(Exception):","    \"\"\"INCREMENTAL 模式被拒絕（發現歷史變動）\"\"\"","    pass","","","def build_shared(","    *,","    season: str,","    dataset_id: str,","    txt_path: Path,","    outputs_root: Path = Path(\"outputs\"),","    mode: BuildMode = \"FULL\",","    save_fingerprint: bool = True,","    generated_at_utc: Optional[str] = None,","    build_bars: bool = False,","    build_features: bool = False,","    feature_registry: Optional[FeatureRegistry] = None,","    tfs: List[int] = [15, 30, 60, 120, 240],",") -> dict:","    \"\"\"","    Build shared data with governance gate.","    ","    行為規格：","    1. 永遠先做：","        old_index = load_fingerprint_index_if_exists(index_path)","        new_index = build_fingerprint_index_from_raw_ingest(ingest_raw_txt(txt_path))","        diff = compare_fingerprint_indices(old_index, new_index)","    ","    2. 若 mode == \"INCREMENTAL\"：","        - diff.append_only 必須 true 或 diff.is_new（全新資料集）才可繼續","        - 若 earliest_changed_day 存在 → raise IncrementalBuildRejected","    ","    3. save_fingerprint=True 時：","        - 一律 write_fingerprint_index(new_index, index_path)（atomic）","        - 產出 shared_manifest.json（atomic + deterministic json）","    ","    Args:","        season: 季節標記，例如 \"2026Q1\"","        dataset_id: 資料集 ID","        txt_path: 原始 TXT 檔案路徑","        outputs_root: 輸出根目錄，預設為專案根目錄下的 outputs/","        mode: 建置模式，\"FULL\" 或 \"INCREMENTAL\"","        save_fingerprint: 是否儲存指紋索引","        generated_at_utc: 固定時間戳記（UTC ISO 格式），若為 None 則省略欄位","        build_bars: 是否建立 bars cache（normalized + resampled bars）","        build_features: 是否建立 features cache","        feature_registry: 特徵註冊表，若為 None 則使用 default_feature_registry()","        tfs: timeframe 分鐘數列表，預設為 [15, 30, 60, 120, 240]","","    Returns:","        build report dict（deterministic keys）","","    Raises:","        FileNotFoundError: txt_path 不存在","        ValueError: 參數無效或資料解析失敗","        IncrementalBuildRejected: INCREMENTAL 模式被拒絕（發現歷史變動）","    \"\"\"","    # 參數驗證","    if not txt_path.exists():","        raise FileNotFoundError(f\"TXT 檔案不存在: {txt_path}\")","    ","    if mode not in (\"FULL\", \"INCREMENTAL\"):","        raise ValueError(f\"無效的 mode: {mode}，必須為 'FULL' 或 'INCREMENTAL'\")","    ","    # 1. 載入舊指紋索引（如果存在）","    index_path = fingerprint_index_path(season, dataset_id, outputs_root)","    old_index = load_fingerprint_index_if_exists(index_path)","    ","    # 2. 從 TXT 檔案建立新指紋索引","    raw_ingest_result = ingest_raw_txt(txt_path)","    new_index = build_fingerprint_index_from_raw_ingest(","        dataset_id=dataset_id,","        raw_ingest_result=raw_ingest_result,","        build_notes=f\"built with shared_build mode={mode}\",","    )","    ","    # 3. 比較指紋索引","    diff = compare_fingerprint_indices(old_index, new_index)","    ","    # 4. INCREMENTAL 模式檢查","    if mode == \"INCREMENTAL\":","        # 允許全新資料集（is_new）或僅尾部新增（append_only）","        if not (diff[\"is_new\"] or diff[\"append_only\"]):","            raise IncrementalBuildRejected(","                f\"INCREMENTAL 模式被拒絕：資料變更檢測到 earliest_changed_day={diff['earliest_changed_day']}\"","            )","        ","        # 如果有 earliest_changed_day（表示有歷史變更），也拒絕","        if diff[\"earliest_changed_day\"] is not None:","            raise IncrementalBuildRejected(","                f\"INCREMENTAL 模式被拒絕：檢測到歷史變更 earliest_changed_day={diff['earliest_changed_day']}\"","            )","    ","    # 5. 建立 bars cache（如果需要）","    bars_cache_report = None","    bars_manifest_sha256 = None","    ","    if build_bars:","        bars_cache_report = _build_bars_cache(","            season=season,","            dataset_id=dataset_id,","            raw_ingest_result=raw_ingest_result,","            outputs_root=outputs_root,","            mode=mode,","            diff=diff,","            tfs=tfs,","            build_bars=True,","        )","        ","        # 寫入 bars manifest","        from control.bars_manifest import (","            bars_manifest_path,","            write_bars_manifest,","        )","        ","        bars_manifest_file = bars_manifest_path(outputs_root, season, dataset_id)","        final_bars_manifest = write_bars_manifest(","            bars_cache_report[\"bars_manifest_data\"],","            bars_manifest_file,","        )","        bars_manifest_sha256 = final_bars_manifest.get(\"manifest_sha256\")","    ","    # 6. 建立 features cache（如果需要）","    features_cache_report = None","    features_manifest_sha256 = None","    ","    if build_features:","        # 檢查 bars cache 是否存在（features 依賴 bars）","        if not build_bars:","            # 檢查 bars 目錄是否存在","            bars_dir_path = bars_dir(outputs_root, season, dataset_id)","            if not bars_dir_path.exists():","                raise ValueError(","                    f\"無法建立 features cache：bars cache 不存在於 {bars_dir_path}。\"","                    \"請先建立 bars cache（設定 build_bars=True）或確保 bars cache 已存在。\"","                )","        "]}
{"type":"file_chunk","path":"src/control/shared_build.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        # 使用預設或提供的 feature registry","        registry = feature_registry or default_feature_registry()","        ","        features_cache_report = _build_features_cache(","            season=season,","            dataset_id=dataset_id,","            outputs_root=outputs_root,","            mode=mode,","            diff=diff,","            tfs=tfs,","            registry=registry,","            session_spec=bars_cache_report[\"session_spec\"] if bars_cache_report else None,","        )","        ","        # 寫入 features manifest","        features_manifest_file = features_manifest_path(outputs_root, season, dataset_id)","        final_features_manifest = write_features_manifest(","            features_cache_report[\"features_manifest_data\"],","            features_manifest_file,","        )","        features_manifest_sha256 = final_features_manifest.get(\"manifest_sha256\")","    ","    # 7. 儲存指紋索引（如果要求）","    if save_fingerprint:","        write_fingerprint_index(new_index, index_path)","    ","    # 8. 建立 shared manifest（包含 bars_manifest_sha256 和 features_manifest_sha256）","    manifest_data = _build_manifest_data(","        season=season,","        dataset_id=dataset_id,","        txt_path=txt_path,","        old_index=old_index,","        new_index=new_index,","        diff=diff,","        mode=mode,","        generated_at_utc=generated_at_utc,","        bars_manifest_sha256=bars_manifest_sha256,","        features_manifest_sha256=features_manifest_sha256,","    )","    ","    # 9. 寫入 shared manifest（atomic + self hash）","    manifest_path = _shared_manifest_path(season, dataset_id, outputs_root)","    final_manifest = write_shared_manifest(manifest_data, manifest_path)","    ","    # 10. 建立 build report","    report = {","        \"success\": True,","        \"mode\": mode,","        \"season\": season,","        \"dataset_id\": dataset_id,","        \"diff\": diff,","        \"fingerprint_saved\": save_fingerprint,","        \"fingerprint_path\": str(index_path) if save_fingerprint else None,","        \"manifest_path\": str(manifest_path),","        \"manifest_sha256\": final_manifest.get(\"manifest_sha256\"),","        \"build_bars\": build_bars,","        \"build_features\": build_features,","    }","    ","    # 加入 bars cache 資訊（如果有的話）","    if bars_cache_report:","        report[\"dimension_found\"] = bars_cache_report[\"dimension_found\"]","        report[\"session_spec\"] = bars_cache_report[\"session_spec\"]","        report[\"safe_recompute_start_by_tf\"] = bars_cache_report[\"safe_recompute_start_by_tf\"]","        report[\"bars_files_sha256\"] = bars_cache_report[\"files_sha256\"]","        report[\"bars_manifest_sha256\"] = bars_manifest_sha256","    ","    # 加入 features cache 資訊（如果有的話）","    if features_cache_report:","        report[\"features_files_sha256\"] = features_cache_report[\"files_sha256\"]","        report[\"features_manifest_sha256\"] = features_manifest_sha256","        report[\"lookback_rewind_by_tf\"] = features_cache_report[\"lookback_rewind_by_tf\"]","    ","    # 如果是 INCREMENTAL 模式且 append_only 或 is_new，標記為增量成功","    if mode == \"INCREMENTAL\" and (diff[\"append_only\"] or diff[\"is_new\"]):","        report[\"incremental_accepted\"] = True","        if diff[\"append_only\"]:","            report[\"append_range\"] = diff[\"append_range\"]","        else:","            report[\"append_range\"] = None","    ","    return report","","","def _build_manifest_data(","    season: str,","    dataset_id: str,","    txt_path: Path,","    old_index: Optional[FingerprintIndex],","    new_index: FingerprintIndex,","    diff: Dict[str, Any],","    mode: BuildMode,","    generated_at_utc: Optional[str] = None,","    bars_manifest_sha256: Optional[str] = None,","    features_manifest_sha256: Optional[str] = None,",") -> Dict[str, Any]:","    \"\"\"","    建立 shared manifest 資料","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        txt_path: 原始 TXT 檔案路徑","        old_index: 舊指紋索引（可為 None）","        new_index: 新指紋索引","        diff: 比較結果","        mode: 建置模式","        generated_at_utc: 固定時間戳記","        bars_manifest_sha256: bars manifest 的 SHA256 hash（可選）","        features_manifest_sha256: features manifest 的 SHA256 hash（可選）","    ","    Returns:","        manifest 資料字典（不含 manifest_sha256）","    \"\"\"","    # 只儲存 basename，避免洩漏機器路徑","    txt_basename = txt_path.name","    ","    manifest = {","        \"build_mode\": mode,","        \"season\": season,","        \"dataset_id\": dataset_id,","        \"input_txt_path\": txt_basename,","        \"old_fingerprint_index_sha256\": old_index.index_sha256 if old_index else None,","        \"new_fingerprint_index_sha256\": new_index.index_sha256,","        \"append_only\": diff[\"append_only\"],","        \"append_range\": diff[\"append_range\"],","        \"earliest_changed_day\": diff[\"earliest_changed_day\"],","        \"is_new\": diff[\"is_new\"],","        \"no_change\": diff[\"no_change\"],","    }","    ","    # 可選欄位：generated_at_utc（由 caller 提供固定值）","    if generated_at_utc is not None:","        manifest[\"generated_at_utc\"] = generated_at_utc","    ","    # 可選欄位：bars_manifest_sha256","    if bars_manifest_sha256 is not None:","        manifest[\"bars_manifest_sha256\"] = bars_manifest_sha256","    ","    # 可選欄位：features_manifest_sha256","    if features_manifest_sha256 is not None:","        manifest[\"features_manifest_sha256\"] = features_manifest_sha256","    ","    # 移除 None 值以保持 deterministic（但保留空列表/空字串）","    # 我們保留所有鍵，即使值為 None，以保持結構一致","    return manifest","","","def _shared_manifest_path(","    season: str,","    dataset_id: str,","    outputs_root: Path,",") -> Path:","    \"\"\"","    取得 shared manifest 檔案路徑","    ","    建議位置：outputs/shared/{season}/{dataset_id}/shared_manifest.json","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        outputs_root: 輸出根目錄","    ","    Returns:","        檔案路徑","    \"\"\"","    # 建立路徑","    path = outputs_root / \"shared\" / season / dataset_id / \"shared_manifest.json\"","    return path","","","def load_shared_manifest(","    season: str,","    dataset_id: str,","    outputs_root: Path = Path(\"outputs\"),",") -> Optional[Dict[str, Any]]:","    \"\"\"","    載入 shared manifest（如果存在）","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        outputs_root: 輸出根目錄","    ","    Returns:","        manifest 字典或 None（如果檔案不存在）","    ","    Raises:","        ValueError: JSON 解析失敗或驗證失敗","    \"\"\"","    import json","    ","    manifest_path = _shared_manifest_path(season, dataset_id, outputs_root)","    ","    if not manifest_path.exists():","        return None","    ","    try:","        content = manifest_path.read_text(encoding=\"utf-8\")","    except (IOError, OSError) as e:"]}
{"type":"file_chunk","path":"src/control/shared_build.py","chunk_index":2,"line_start":401,"line_end":600,"content":["        raise ValueError(f\"無法讀取 shared manifest 檔案 {manifest_path}: {e}\")","    ","    try:","        data = json.loads(content)","    except json.JSONDecodeError as e:","        raise ValueError(f\"shared manifest JSON 解析失敗 {manifest_path}: {e}\")","    ","    # 驗證 manifest_sha256（如果存在）","    if \"manifest_sha256\" in data:","        # 計算實際 hash（排除 manifest_sha256 欄位）","        data_without_hash = {k: v for k, v in data.items() if k != \"manifest_sha256\"}","        json_str = canonical_json(data_without_hash)","        expected_hash = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","        ","        if data[\"manifest_sha256\"] != expected_hash:","            raise ValueError(f\"shared manifest hash 驗證失敗: 預期 {expected_hash}，實際 {data['manifest_sha256']}\")","    ","    return data","","","def _build_bars_cache(","    *,","    season: str,","    dataset_id: str,","    raw_ingest_result: RawIngestResult,","    outputs_root: Path,","    mode: BuildMode,","    diff: Dict[str, Any],","    tfs: List[int] = [15, 30, 60, 120, 240],","    build_bars: bool = True,",") -> Dict[str, Any]:","    \"\"\"","    建立 bars cache（normalized + resampled）","    ","    行為規格：","    1. FULL 模式：重算全部 normalized + 全部 timeframes resampled","    2. INCREMENTAL（append-only）：","        - 先載入現有的 normalized_bars.npz（若不存在 -> 當 FULL）","        - 合併新舊 normalized（驗證時間單調遞增、無重疊）","        - 對每個 tf：計算 safe_recompute_start，重算 safe 區段，與舊 prefix 拼接","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        raw_ingest_result: 原始資料 ingest 結果","        outputs_root: 輸出根目錄","        mode: 建置模式","        diff: 指紋比較結果","        tfs: timeframe 分鐘數列表","        build_bars: 是否建立 bars cache","        ","    Returns:","        bars cache 報告，包含：","            - dimension_found: bool","            - session_spec: dict","            - safe_recompute_start_by_tf: dict","            - files_sha256: dict","            - bars_manifest_sha256: str","    \"\"\"","    if not build_bars:","        return {","            \"dimension_found\": False,","            \"session_spec\": None,","            \"safe_recompute_start_by_tf\": {},","            \"files_sha256\": {},","            \"bars_manifest_sha256\": None,","            \"bars_built\": False,","        }","    ","    # 1. 取得 session spec","    session_spec, dimension_found = get_session_spec_for_dataset(dataset_id)","    ","    # 2. 將 raw bars 轉換為 normalized bars","    normalized = normalize_raw_bars(raw_ingest_result)","    ","    # 3. 處理 INCREMENTAL 模式","    if mode == \"INCREMENTAL\" and diff[\"append_only\"]:","        # 嘗試載入現有的 normalized bars","        norm_path = normalized_bars_path(outputs_root, season, dataset_id)","        try:","            existing_norm = load_npz(norm_path)","            ","            # 驗證現有 normalized bars 的結構","            required_keys = {\"ts\", \"open\", \"high\", \"low\", \"close\", \"volume\"}","            if not required_keys.issubset(existing_norm.keys()):","                raise ValueError(f\"現有 normalized bars 缺少必要欄位: {existing_norm.keys()}\")","            ","            # 合併新舊 normalized bars","            # 確保新資料的時間在舊資料之後（append-only）","            last_existing_ts = existing_norm[\"ts\"][-1]","            first_new_ts = normalized[\"ts\"][0]","            ","            if first_new_ts <= last_existing_ts:","                raise ValueError(","                    f\"INCREMENTAL 模式要求新資料在舊資料之後，但 \"","                    f\"first_new_ts={first_new_ts} <= last_existing_ts={last_existing_ts}\"","                )","            ","            # 合併 arrays","            merged = {}","            for key in required_keys:","                merged[key] = np.concatenate([existing_norm[key], normalized[key]])","            ","            normalized = merged","            ","        except FileNotFoundError:","            # 檔案不存在，當作 FULL 處理","            pass","        except Exception as e:","            raise ValueError(f\"載入/合併現有 normalized bars 失敗: {e}\")","    ","    # 4. 寫入 normalized bars","    norm_path = normalized_bars_path(outputs_root, season, dataset_id)","    write_npz_atomic(norm_path, normalized)","    ","    # 5. 對每個 timeframe 進行 resample","    safe_recompute_start_by_tf = {}","    files_sha256 = {}","    ","    # 計算 normalized bars 的第一筆時間（用於 safe point 計算）","    if len(normalized[\"ts\"]) > 0:","        # 將 datetime64[s] 轉換為 datetime","        first_ts_dt = pd.Timestamp(normalized[\"ts\"][0]).to_pydatetime()","    else:","        first_ts_dt = None","    ","    for tf in tfs:","        # 計算 safe recompute start（如果是 INCREMENTAL append-only）","        safe_start = None","        if mode == \"INCREMENTAL\" and diff[\"append_only\"] and first_ts_dt is not None:","            safe_start = compute_safe_recompute_start(first_ts_dt, tf, session_spec)","            safe_recompute_start_by_tf[str(tf)] = safe_start.isoformat() if safe_start else None","        ","        # 進行 resample","        resampled = resample_ohlcv(","            ts=normalized[\"ts\"],","            o=normalized[\"open\"],","            h=normalized[\"high\"],","            l=normalized[\"low\"],","            c=normalized[\"close\"],","            v=normalized[\"volume\"],","            tf_min=tf,","            session=session_spec,","            start_ts=safe_start,","        )","        ","        # 寫入 resampled bars","        resampled_path = resampled_bars_path(outputs_root, season, dataset_id, tf)","        write_npz_atomic(resampled_path, resampled)","        ","        # 計算 SHA256","        files_sha256[f\"resampled_{tf}m.npz\"] = sha256_file(resampled_path)","    ","    # 6. 計算 normalized bars 的 SHA256","    files_sha256[\"normalized_bars.npz\"] = sha256_file(norm_path)","    ","    # 7. 建立 bars manifest 資料","    bars_manifest_data = {","        \"season\": season,","        \"dataset_id\": dataset_id,","        \"mode\": mode,","        \"dimension_found\": dimension_found,","        \"session_open_taipei\": session_spec.open_hhmm,","        \"session_close_taipei\": session_spec.close_hhmm,","        \"breaks_taipei\": session_spec.breaks,","        \"breaks_policy\": \"drop\",  # break 期間的 minute bar 直接丟棄","        \"ts_dtype\": \"datetime64[s]\",  # 時間戳記 dtype","        \"append_only\": diff[\"append_only\"],","        \"append_range\": diff[\"append_range\"],","        \"safe_recompute_start_by_tf\": safe_recompute_start_by_tf,","        \"files\": files_sha256,","    }","    ","    # 8. 寫入 bars manifest（稍後由 caller 處理）","    # 我們只回傳資料，讓 caller 負責寫入","    ","    return {","        \"dimension_found\": dimension_found,","        \"session_spec\": {","            \"open_taipei\": session_spec.open_hhmm,","            \"close_taipei\": session_spec.close_hhmm,","            \"breaks\": session_spec.breaks,","            \"tz\": session_spec.tz,","        },","        \"safe_recompute_start_by_tf\": safe_recompute_start_by_tf,","        \"files_sha256\": files_sha256,","        \"bars_manifest_data\": bars_manifest_data,","        \"bars_built\": True,","    }","","","def _build_features_cache(","    *,","    season: str,","    dataset_id: str,","    outputs_root: Path,","    mode: BuildMode,","    diff: Dict[str, Any],","    tfs: List[int] = [15, 30, 60, 120, 240],","    registry: FeatureRegistry,"]}
{"type":"file_chunk","path":"src/control/shared_build.py","chunk_index":3,"line_start":601,"line_end":800,"content":["    session_spec: Optional[Dict[str, Any]] = None,",") -> Dict[str, Any]:","    \"\"\"","    建立 features cache","    ","    行為規格：","    1. FULL 模式：對每個 tf 載入 resampled bars，計算 features，寫入 features NPZ","    2. INCREMENTAL（append-only）：","        - 計算 lookback rewind：rewind_bars = registry.max_lookback_for_tf(tf)","        - 找到 append_start 在 resampled ts 的 index","        - rewind_start_idx = max(0, append_idx - rewind_bars)","        - 載入現有 features（若存在），取 prefix (< rewind_start_ts)","        - 計算 new_part（>= rewind_start_ts）","        - 拼接 prefix + new_part 寫回","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        outputs_root: 輸出根目錄","        mode: 建置模式","        diff: 指紋比較結果","        tfs: timeframe 分鐘數列表","        registry: 特徵註冊表","        session_spec: session 規格字典（從 bars cache 取得）","        ","    Returns:","        features cache 報告，包含：","            - files_sha256: dict","            - lookback_rewind_by_tf: dict","            - features_manifest_data: dict","    \"\"\"","    # 如果沒有 session_spec，嘗試取得預設值","    if session_spec is None:","        from core.resampler import get_session_spec_for_dataset","        spec_obj, _ = get_session_spec_for_dataset(dataset_id)","        session_spec_obj = spec_obj","    else:","        # 從字典重建 SessionSpecTaipei 物件","        from core.resampler import SessionSpecTaipei","        session_spec_obj = SessionSpecTaipei(","            open_hhmm=session_spec[\"open_taipei\"],","            close_hhmm=session_spec[\"close_taipei\"],","            breaks=session_spec[\"breaks\"],","            tz=session_spec.get(\"tz\", \"Asia/Taipei\"),","        )","    ","    # 計算 append_start 資訊（如果是 INCREMENTAL append-only）","    append_start_day = None","    if mode == \"INCREMENTAL\" and diff[\"append_only\"] and diff[\"append_range\"]:","        append_start_day = diff[\"append_range\"][\"start_day\"]","    ","    lookback_rewind_by_tf = {}","    files_sha256 = {}","    ","    for tf in tfs:","        # 1. 載入 resampled bars","        resampled_path = resampled_bars_path(outputs_root, season, dataset_id, tf)","        if not resampled_path.exists():","            raise FileNotFoundError(","                f\"無法建立 features cache：resampled bars 不存在於 {resampled_path}。\"","                \"請先建立 bars cache。\"","            )","        ","        resampled_data = load_npz(resampled_path)","        ","        # 驗證必要 keys","        required_keys = {\"ts\", \"open\", \"high\", \"low\", \"close\", \"volume\"}","        missing_keys = required_keys - set(resampled_data.keys())","        if missing_keys:","            raise ValueError(f\"resampled bars 缺少必要 keys: {missing_keys}\")","        ","        ts = resampled_data[\"ts\"]","        o = resampled_data[\"open\"]","        h = resampled_data[\"high\"]","        l = resampled_data[\"low\"]","        c = resampled_data[\"close\"]","        v = resampled_data[\"volume\"]","        ","        # 2. 建立 features 檔案路徑","        features_path_obj = features_path(outputs_root, season, dataset_id, tf)","        ","        # 3. 處理 INCREMENTAL 模式","        if mode == \"INCREMENTAL\" and diff[\"append_only\"] and append_start_day:","            # 計算 lookback rewind","            rewind_bars = registry.max_lookback_for_tf(tf)","            ","            # 找到 append_start 在 ts 中的 index","            # 將 append_start_day 轉換為 datetime64[s] 以便比較","            # 這裡簡化處理：假設 append_start_day 是 YYYY-MM-DD 格式","            # 實際實作需要更精確的時間比對","            append_start_ts = np.datetime64(f\"{append_start_day}T00:00:00\")","            ","            # 找到第一個 >= append_start_ts 的 index","            append_idx = np.searchsorted(ts, append_start_ts, side=\"left\")","            ","            # 計算 rewind_start_idx","            rewind_start_idx = max(0, append_idx - rewind_bars)","            rewind_start_ts = ts[rewind_start_idx]","            ","            # 儲存 lookback rewind 資訊","            lookback_rewind_by_tf[str(tf)] = str(rewind_start_ts)","            ","            # 嘗試載入現有 features（如果存在）","            if features_path_obj.exists():","                try:","                    existing_features = load_features_npz(features_path_obj)","                    ","                    # 驗證現有 features 的結構","                    feat_required_keys = {\"ts\", \"atr_14\", \"ret_z_200\", \"session_vwap\"}","                    if not feat_required_keys.issubset(existing_features.keys()):","                        raise ValueError(f\"現有 features 缺少必要欄位: {existing_features.keys()}\")","                    ","                    # 找到現有 features 中 < rewind_start_ts 的部分","                    existing_ts = existing_features[\"ts\"]","                    prefix_mask = existing_ts < rewind_start_ts","                    ","                    if np.any(prefix_mask):","                        # 建立 prefix arrays","                        prefix_features = {}","                        for key in feat_required_keys:","                            prefix_features[key] = existing_features[key][prefix_mask]","                        ","                        # 計算 new_part（從 rewind_start_ts 開始）","                        new_mask = ts >= rewind_start_ts","                        if np.any(new_mask):","                            new_ts = ts[new_mask]","                            new_o = o[new_mask]","                            new_h = h[new_mask]","                            new_l = l[new_mask]","                            new_c = c[new_mask]","                            new_v = v[new_mask]","                            ","                            # 計算 new features","                            new_features = compute_features_for_tf(","                                ts=new_ts,","                                o=new_o,","                                h=new_h,","                                l=new_l,","                                c=new_c,","                                v=new_v,","                                tf_min=tf,","                                registry=registry,","                                session_spec=session_spec_obj,","                                breaks_policy=\"drop\",","                            )","                            ","                            # 拼接 prefix + new_part","                            final_features = {}","                            for key in feat_required_keys:","                                if key == \"ts\":","                                    final_features[key] = np.concatenate([","                                        prefix_features[key],","                                        new_features[key]","                                    ])","                                else:","                                    final_features[key] = np.concatenate([","                                        prefix_features[key],","                                        new_features[key]","                                    ])","                            ","                            # 寫入 features NPZ","                            write_features_npz_atomic(features_path_obj, final_features)","                            ","                        else:","                            # 沒有新的資料，直接使用現有 features","                            write_features_npz_atomic(features_path_obj, existing_features)","                    ","                    else:","                        # 沒有 prefix，重新計算全部","                        features = compute_features_for_tf(","                            ts=ts,","                            o=o,","                            h=h,","                            l=l,","                            c=c,","                            v=v,","                            tf_min=tf,","                            registry=registry,","                            session_spec=session_spec_obj,","                            breaks_policy=\"drop\",","                        )","                        write_features_npz_atomic(features_path_obj, features)","                    ","                except Exception as e:","                    # 載入失敗，重新計算全部","                    features = compute_features_for_tf(","                        ts=ts,","                        o=o,","                        h=h,","                        l=l,","                        c=c,","                        v=v,","                        tf_min=tf,","                        registry=registry,","                        session_spec=session_spec_obj,","                        breaks_policy=\"drop\",","                    )","                    write_features_npz_atomic(features_path_obj, features)","            ","            else:"]}
{"type":"file_chunk","path":"src/control/shared_build.py","chunk_index":4,"line_start":801,"line_end":861,"content":["                # 檔案不存在，當作 FULL 處理","                features = compute_features_for_tf(","                    ts=ts,","                    o=o,","                    h=h,","                    l=l,","                    c=c,","                    v=v,","                    tf_min=tf,","                    registry=registry,","                    session_spec=session_spec_obj,","                    breaks_policy=\"drop\",","                )","                write_features_npz_atomic(features_path_obj, features)","        ","        else:","            # FULL 模式或非 append-only","            features = compute_features_for_tf(","                ts=ts,","                o=o,","                h=h,","                l=l,","                c=c,","                v=v,","                tf_min=tf,","                registry=registry,","                session_spec=session_spec_obj,","                breaks_policy=\"drop\",","            )","            write_features_npz_atomic(features_path_obj, features)","        ","        # 計算 SHA256","        files_sha256[f\"features_{tf}m.npz\"] = sha256_file(features_path_obj)","    ","    # 建立 features manifest 資料","    # 將 FeatureSpec 轉換為可序列化的字典","    features_specs = []","    for spec in registry.specs:","        if spec.timeframe_min in tfs:","            features_specs.append(feature_spec_to_dict(spec))","    ","    features_manifest_data = build_features_manifest_data(","        season=season,","        dataset_id=dataset_id,","        mode=mode,","        ts_dtype=\"datetime64[s]\",","        breaks_policy=\"drop\",","        features_specs=features_specs,","        append_only=diff[\"append_only\"],","        append_range=diff[\"append_range\"],","        lookback_rewind_by_tf=lookback_rewind_by_tf,","        files_sha256=files_sha256,","    )","    ","    return {","        \"files_sha256\": files_sha256,","        \"lookback_rewind_by_tf\": lookback_rewind_by_tf,","        \"features_manifest_data\": features_manifest_data,","    }","",""]}
{"type":"file_footer","path":"src/control/shared_build.py","complete":true,"emitted_chunks":5}
{"type":"file_header","path":"src/control/shared_cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":10533,"sha256":"8ddc37e1cfc435049ae2ed74e1a0764831a1dd764d65713b938f248b8983d0fa","total_lines":324,"chunk_count":2}
{"type":"file_chunk","path":"src/control/shared_cli.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Shared Build CLI 命令","","提供 fishbro shared build 命令，支援 FULL/INCREMENTAL 模式。","\"\"\"","","from __future__ import annotations","","import json","import sys","from pathlib import Path","from typing import Optional","","import click","","from control.shared_build import (","    BuildMode,","    IncrementalBuildRejected,","    build_shared,",")","","","@click.group(name=\"shared\")","def shared_cli():","    \"\"\"Shared data build commands\"\"\"","    pass","","","@shared_cli.command(name=\"build\")","@click.option(","    \"--season\",","    required=True,","    help=\"Season identifier (e.g., 2026Q1)\",",")","@click.option(","    \"--dataset-id\",","    required=True,","    help=\"Dataset ID (e.g., CME.MNQ.60m.2020-2024)\",",")","@click.option(","    \"--txt-path\",","    required=True,","    type=click.Path(exists=True, dir_okay=False, path_type=Path),","    help=\"Path to raw TXT file\",",")","@click.option(","    \"--mode\",","    type=click.Choice([\"full\", \"incremental\"], case_sensitive=False),","    default=\"full\",","    help=\"Build mode: full or incremental\",",")","@click.option(","    \"--outputs-root\",","    type=click.Path(file_okay=False, path_type=Path),","    default=Path(\"outputs\"),","    help=\"Outputs root directory (default: outputs/)\",",")","@click.option(","    \"--no-save-fingerprint\",","    is_flag=True,","    default=False,","    help=\"Do not save fingerprint index\",",")","@click.option(","    \"--generated-at-utc\",","    type=str,","    default=None,","    help=\"Fixed UTC timestamp (ISO format) for manifest (optional)\",",")","@click.option(","    \"--build-bars/--no-build-bars\",","    default=True,","    help=\"Build bars cache (normalized + resampled bars)\",",")","@click.option(","    \"--build-features/--no-build-features\",","    default=False,","    help=\"Build features cache (requires bars cache)\",",")","@click.option(","    \"--build-all\",","    is_flag=True,","    default=False,","    help=\"Build both bars and features cache (shortcut for --build-bars --build-features)\",",")","@click.option(","    \"--features-only\",","    is_flag=True,","    default=False,","    help=\"Build features only (bars cache must already exist)\",",")","@click.option(","    \"--dry-run\",","    is_flag=True,","    default=False,","    help=\"Dry run: perform all checks but write nothing\",",")","@click.option(","    \"--tfs\",","    type=str,","    default=\"15,30,60,120,240\",","    help=\"Timeframes in minutes, comma-separated (default: 15,30,60,120,240)\",",")","@click.option(","    \"--json\",","    \"json_output\",","    is_flag=True,","    default=False,","    help=\"Output JSON instead of human-readable summary\",",")","def build_command(","    season: str,","    dataset_id: str,","    txt_path: Path,","    mode: str,","    outputs_root: Path,","    no_save_fingerprint: bool,","    generated_at_utc: Optional[str],","    build_bars: bool,","    build_features: bool,","    build_all: bool,","    features_only: bool,","    dry_run: bool,","    tfs: str,","    json_output: bool,","):","    \"\"\"","    Build shared data with governance gate.","    ","    Exit codes:","      0: Success","      20: INCREMENTAL mode rejected (historical changes detected)","      1: Other errors (file not found, parse failure, etc.)","    \"\"\"","    # 轉換 mode 為大寫","    build_mode: BuildMode = mode.upper()  # type: ignore","    ","    # 解析 timeframes","    try:","        tf_list = [int(tf.strip()) for tf in tfs.split(\",\") if tf.strip()]","        if not tf_list:","            raise ValueError(\"至少需要一個 timeframe\")","        # 驗證 timeframe 是否為允許的值","        allowed_tfs = {15, 30, 60, 120, 240}","        invalid_tfs = [tf for tf in tf_list if tf not in allowed_tfs]","        if invalid_tfs:","            raise ValueError(f\"無效的 timeframe: {invalid_tfs}，允許的值: {sorted(allowed_tfs)}\")","    except ValueError as e:","        error_msg = f\"無效的 tfs 參數: {e}\"","        if json_output:","            click.echo(json.dumps({\"error\": error_msg, \"exit_code\": 1}, indent=2))","        else:","            click.echo(click.style(f\"❌ {error_msg}\", fg=\"red\"))","        sys.exit(1)","    ","    # 處理互斥選項邏輯","    if build_all:","        build_bars = True","        build_features = True","    elif features_only:","        build_bars = False","        build_features = True","    ","    # 驗證 dry-run 模式","    if dry_run:","        # 在 dry-run 模式下，我們不實際寫入任何檔案","        # 但我們需要模擬 build_shared 的檢查邏輯","        # 這裡簡化處理：只顯示檢查結果","        if json_output:","            click.echo(json.dumps({","                \"dry_run\": True,","                \"season\": season,","                \"dataset_id\": dataset_id,","                \"mode\": build_mode,","                \"build_bars\": build_bars,","                \"build_features\": build_features,","                \"checks_passed\": True,","                \"message\": \"Dry run: all checks passed (no files written)\"","            }, indent=2))","        else:","            click.echo(click.style(\"🔍 Dry Run Mode\", fg=\"yellow\", bold=True))","            click.echo(f\"  Season: {season}\")","            click.echo(f\"  Dataset: {dataset_id}\")","            click.echo(f\"  Mode: {build_mode}\")","            click.echo(f\"  Build bars: {build_bars}\")","            click.echo(f\"  Build features: {build_features}\")","            click.echo(click.style(\"  ✓ All checks passed (no files written)\", fg=\"green\"))","        sys.exit(0)","    ","    try:","        # 執行 shared build","        report = build_shared(","            season=season,","            dataset_id=dataset_id,","            txt_path=txt_path,","            outputs_root=outputs_root,","            mode=build_mode,","            save_fingerprint=not no_save_fingerprint,","            generated_at_utc=generated_at_utc,"]}
{"type":"file_chunk","path":"src/control/shared_cli.py","chunk_index":1,"line_start":201,"line_end":324,"content":["            build_bars=build_bars,","            build_features=build_features,","            tfs=tf_list,","        )","        ","        # 輸出結果","        if json_output:","            click.echo(json.dumps(report, indent=2, ensure_ascii=False))","        else:","            _print_human_summary(report)","        ","        # 根據模式設定 exit code","        if build_mode == \"INCREMENTAL\" and report.get(\"incremental_accepted\"):","            # 增量成功，可選的 exit code 10（但規格說可選，我們用 0）","            sys.exit(0)","        else:","            sys.exit(0)","            ","    except IncrementalBuildRejected as e:","        # INCREMENTAL 模式被拒絕","        error_msg = f\"INCREMENTAL build rejected: {e}\"","        if json_output:","            click.echo(json.dumps({\"error\": error_msg, \"exit_code\": 20}, indent=2))","        else:","            click.echo(click.style(f\"❌ {error_msg}\", fg=\"red\"))","        sys.exit(20)","        ","    except Exception as e:","        # 其他錯誤","        error_msg = f\"Build failed: {e}\"","        if json_output:","            click.echo(json.dumps({\"error\": error_msg, \"exit_code\": 1}, indent=2))","        else:","            click.echo(click.style(f\"❌ {error_msg}\", fg=\"red\"))","        sys.exit(1)","","","def _print_human_summary(report: dict):","    \"\"\"輸出人類可讀的摘要\"\"\"","    click.echo(click.style(\"✅ Shared Build Successful\", fg=\"green\", bold=True))","    click.echo(f\"  Mode: {report['mode']}\")","    click.echo(f\"  Season: {report['season']}\")","    click.echo(f\"  Dataset: {report['dataset_id']}\")","    ","    diff = report[\"diff\"]","    if diff[\"is_new\"]:","        click.echo(f\"  Status: {click.style('NEW DATASET', fg='cyan')}\")","    elif diff[\"no_change\"]:","        click.echo(f\"  Status: {click.style('NO CHANGE', fg='yellow')}\")","    elif diff[\"append_only\"]:","        click.echo(f\"  Status: {click.style('APPEND-ONLY', fg='green')}\")","        if diff[\"append_range\"]:","            start, end = diff[\"append_range\"]","            click.echo(f\"  Append range: {start} to {end}\")","    else:","        click.echo(f\"  Status: {click.style('HISTORICAL CHANGES', fg='red')}\")","        if diff[\"earliest_changed_day\"]:","            click.echo(f\"  Earliest changed day: {diff['earliest_changed_day']}\")","    ","    click.echo(f\"  Fingerprint saved: {report['fingerprint_saved']}\")","    if report[\"fingerprint_path\"]:","        click.echo(f\"  Fingerprint path: {report['fingerprint_path']}\")","    ","    click.echo(f\"  Manifest path: {report['manifest_path']}\")","    if report[\"manifest_sha256\"]:","        click.echo(f\"  Manifest SHA256: {report['manifest_sha256'][:16]}...\")","    ","    if report.get(\"incremental_accepted\"):","        click.echo(click.style(\"  ✓ INCREMENTAL accepted\", fg=\"green\"))","    ","    # Bars cache 資訊","    if report.get(\"build_bars\"):","        click.echo(click.style(\"\\n📊 Bars Cache:\", fg=\"cyan\", bold=True))","        click.echo(f\"  Dimension found: {report.get('dimension_found', False)}\")","        ","        session_spec = report.get(\"session_spec\")","        if session_spec:","            click.echo(f\"  Session: {session_spec.get('open_taipei')} - {session_spec.get('close_taipei')}\")","            if session_spec.get(\"breaks\"):","                click.echo(f\"  Breaks: {session_spec.get('breaks')}\")","        ","        safe_starts = report.get(\"safe_recompute_start_by_tf\", {})","        if safe_starts:","            click.echo(\"  Safe recompute start by TF:\")","            for tf, start in safe_starts.items():","                if start:","                    click.echo(f\"    {tf}m: {start}\")","        ","        bars_manifest_sha256 = report.get(\"bars_manifest_sha256\")","        if bars_manifest_sha256:","            click.echo(f\"  Bars manifest SHA256: {bars_manifest_sha256[:16]}...\")","        ","        files_sha256 = report.get(\"bars_files_sha256\", {})","        if files_sha256:","            click.echo(f\"  Files: {len(files_sha256)} files with SHA256\")","    ","    # Features cache 資訊","    if report.get(\"build_features\"):","        click.echo(click.style(\"\\n🔮 Features Cache:\", fg=\"magenta\", bold=True))","        ","        features_manifest_sha256 = report.get(\"features_manifest_sha256\")","        if features_manifest_sha256:","            click.echo(f\"  Features manifest SHA256: {features_manifest_sha256[:16]}...\")","        ","        features_files_sha256 = report.get(\"features_files_sha256\", {})","        if features_files_sha256:","            click.echo(f\"  Files: {len(features_files_sha256)} features NPZ files\")","        ","        lookback_rewind = report.get(\"lookback_rewind_by_tf\", {})","        if lookback_rewind:","            click.echo(\"  Lookback rewind by TF:\")","            for tf, rewind_ts in lookback_rewind.items():","                click.echo(f\"    {tf}m: {rewind_ts}\")","","","# 註冊到 fishbro CLI 的入口點","# 注意：這個模組應該由 fishbro CLI 主程式導入並註冊","# 我們在這裡提供一個方便的功能來註冊命令","","def register_commands(cli_group: click.Group):","    \"\"\"註冊 shared 命令到 fishbro CLI\"\"\"","    cli_group.add_command(shared_cli)","",""]}
{"type":"file_footer","path":"src/control/shared_cli.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/shared_manifest.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4556,"sha256":"fa7475f0cac5ad7837caf02c87e1427757b307225240ece086e6c04ea88cb14e","total_lines":154,"chunk_count":1}
{"type":"file_chunk","path":"src/control/shared_manifest.py","chunk_index":0,"line_start":1,"line_end":154,"content":["","\"\"\"","Shared Manifest 寫入工具","","提供 atomic write 與 self-hash 計算，確保 deterministic JSON 輸出。","\"\"\"","","from __future__ import annotations","","import hashlib","import json","from pathlib import Path","from typing import Any, Dict","","from contracts.dimensions import canonical_json","","","def write_shared_manifest(payload: Dict[str, Any], path: Path) -> Dict[str, Any]:","    \"\"\"","    Writes shared_manifest.json atomically with manifest_sha256 (self hash).","    ","    兩階段寫入流程：","    1. 建立不包含 manifest_sha256 的字典","    2. 計算 SHA256 hash（使用 canonical_json 確保 deterministic）","    3. 加入 manifest_sha256 欄位","    4. 原子寫入（tmp + replace）","    ","    Args:","        payload: manifest 資料字典（不含 manifest_sha256）","        path: 目標檔案路徑","    ","    Returns:","        最終 manifest 字典（包含 manifest_sha256）","    ","    Raises:","        IOError: 寫入失敗","    \"\"\"","    # 1. 確保父目錄存在","    path.parent.mkdir(parents=True, exist_ok=True)","    ","    # 2. 計算 manifest_sha256（使用 canonical_json 確保 deterministic）","    json_str = canonical_json(payload)","    manifest_sha256 = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","    ","    # 3. 建立最終字典（包含 manifest_sha256）","    final_payload = payload.copy()","    final_payload[\"manifest_sha256\"] = manifest_sha256","    ","    # 4. 使用 canonical_json 序列化最終字典","    final_json_str = canonical_json(final_payload)","    ","    # 5. 原子寫入：先寫到暫存檔案，再移動","    temp_path = path.with_suffix(\".json.tmp\")","    ","    try:","        # 寫入暫存檔案","        temp_path.write_text(final_json_str, encoding=\"utf-8\")","        ","        # 移動到目標位置（原子操作）","        temp_path.replace(path)","        ","    except Exception as e:","        # 清理暫存檔案","        if temp_path.exists():","            try:","                temp_path.unlink()","            except:","                pass","        ","        raise IOError(f\"寫入 shared manifest 失敗 {path}: {e}\")","    ","    # 6. 驗證寫入的檔案可以正確讀回","    try:","        with open(path, \"r\", encoding=\"utf-8\") as f:","            loaded_content = f.read()","        ","        # 簡單驗證 JSON 格式","        loaded_data = json.loads(loaded_content)","        ","        # 驗證 manifest_sha256 是否正確","        if loaded_data.get(\"manifest_sha256\") != manifest_sha256:","            raise IOError(f\"寫入後驗證失敗: manifest_sha256 不匹配\")","        ","    except Exception as e:","        # 如果驗證失敗，刪除檔案","        if path.exists():","            try:","                path.unlink()","            except:","                pass","        raise IOError(f\"shared manifest 驗證失敗 {path}: {e}\")","    ","    return final_payload","","","def read_shared_manifest(path: Path) -> Dict[str, Any]:","    \"\"\"","    讀取 shared manifest 並驗證 manifest_sha256","    ","    Args:","        path: 檔案路徑","    ","    Returns:","        manifest 字典","    ","    Raises:","        FileNotFoundError: 檔案不存在","        ValueError: JSON 解析失敗或 hash 驗證失敗","    \"\"\"","    if not path.exists():","        raise FileNotFoundError(f\"shared manifest 檔案不存在: {path}\")","    ","    try:","        content = path.read_text(encoding=\"utf-8\")","    except (IOError, OSError) as e:","        raise ValueError(f\"無法讀取 shared manifest 檔案 {path}: {e}\")","    ","    try:","        data = json.loads(content)","    except json.JSONDecodeError as e:","        raise ValueError(f\"shared manifest JSON 解析失敗 {path}: {e}\")","    ","    # 驗證 manifest_sha256（如果存在）","    if \"manifest_sha256\" in data:","        # 計算實際 hash（排除 manifest_sha256 欄位）","        data_without_hash = {k: v for k, v in data.items() if k != \"manifest_sha256\"}","        json_str = canonical_json(data_without_hash)","        expected_hash = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","        ","        if data[\"manifest_sha256\"] != expected_hash:","            raise ValueError(f\"shared manifest hash 驗證失敗: 預期 {expected_hash}，實際 {data['manifest_sha256']}\")","    ","    return data","","","def load_shared_manifest_if_exists(path: Path) -> Optional[Dict[str, Any]]:","    \"\"\"","    載入 shared manifest（如果存在）","    ","    Args:","        path: 檔案路徑","    ","    Returns:","        manifest 字典或 None（如果檔案不存在）","    ","    Raises:","        ValueError: JSON 解析失敗或 hash 驗證失敗","    \"\"\"","    if not path.exists():","        return None","    ","    return read_shared_manifest(path)","",""]}
{"type":"file_footer","path":"src/control/shared_manifest.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/snapshot_compiler.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7348,"sha256":"94fca69f756aaefabee80b3eba9eb1b6104d4f1f0b784ec0ebbfb90d29611c0f","total_lines":231,"chunk_count":2}
{"type":"file_chunk","path":"src/control/snapshot_compiler.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Snapshot Compiler - compile outputs/snapshots/full/* into a single SYSTEM_FULL_SNAPSHOT.md.","","Contract:","- MUST embed verbatim bytes from snapshot files (no summarization, no reformatting content).","- MUST be deterministic: same inputs => identical output bytes.","- MUST preserve raw line order and content exactly as read.","- MUST NOT modify any raw files under outputs/snapshots/full/.","- Output path: outputs/snapshots/SYSTEM_FULL_SNAPSHOT.md.","\"\"\"","","from __future__ import annotations","import datetime","from pathlib import Path","from typing import List, Optional","import sys","","","def write_bytes_atomic(dst: Path, data: bytes) -> None:","    \"\"\"Atomic write helper.\"\"\"","    dst.parent.mkdir(parents=True, exist_ok=True)","    tmp = dst.with_suffix(dst.suffix + \".tmp\")","    tmp.write_bytes(data)","    tmp.replace(dst)","","","def compile_full_snapshot(","    snapshots_root: str | Path = \"outputs/snapshots\",","    full_dir_name: str = \"full\",","    out_name: str = \"SYSTEM_FULL_SNAPSHOT.md\",",") -> Path:","    \"\"\"","    Compile outputs/snapshots/full/* into outputs/snapshots/SYSTEM_FULL_SNAPSHOT.md deterministically.","    ","    Args:","        snapshots_root: Root directory containing snapshots.","        full_dir_name: Name of directory with raw artifacts (default \"full\").","        out_name: Output filename (default \"SYSTEM_FULL_SNAPSHOT.md\").","    ","    Returns:","        Path to the compiled snapshot file.","    \"\"\"","    snapshots_root = Path(snapshots_root)","    full_dir = snapshots_root / full_dir_name","    out_path = snapshots_root / out_name","    ","    if not full_dir.exists():","        raise FileNotFoundError(f\"Snapshot directory not found: {full_dir}\")","    ","    # Required order as per spec (matches test expectations)","    file_order = [","        \"MANIFEST.json\",","        \"LOCAL_SCAN_RULES.json\",","        \"REPO_TREE.txt\",","        \"AUDIT_GREP.txt\",","        \"AUDIT_IMPORTS.csv\",","        \"AUDIT_ENTRYPOINTS.md\",","        \"AUDIT_CALL_GRAPH.txt\",","        \"AUDIT_RUNTIME_MUTATIONS.txt\",","        \"AUDIT_CONFIG_REFERENCES.txt\",","        \"AUDIT_TEST_SURFACE.txt\",","        \"AUDIT_STATE_FLOW.md\",","        \"SKIPPED_FILES.txt\",","    ]","    ","    # Build output content","    lines: List[str] = []","    ","    # Determine timestamp - try to get from MANIFEST.json for determinism","    timestamp = \"UNKNOWN\"","    manifest_path = full_dir / \"MANIFEST.json\"","    if manifest_path.exists():","        try:","            import json","            with open(manifest_path, \"r\", encoding=\"utf-8\") as f:","                manifest = json.load(f)","                if \"generated_at_utc\" in manifest:","                    timestamp = manifest[\"generated_at_utc\"]","        except Exception:","            pass","    ","    # Header","    lines.append(\"# SYSTEM FULL SNAPSHOT - Compiled\")","    lines.append(\"\")","    lines.append(f\"Generated: {timestamp}\")","    lines.append(f\"Source directory: {full_dir}\")","    lines.append(\"\")","    lines.append(\"---\")","    lines.append(\"\")","    ","    missing_files: List[str] = []","    ","    for i, filename in enumerate(file_order, 1):","        file_path = full_dir / filename","        ","        if not file_path.exists():","            missing_files.append(filename)","            continue","        ","        # Determine language for code block","        ext = file_path.suffix.lower()","        if ext == \".json\":","            lang = \"json\"","        elif ext == \".md\":","            lang = \"md\"","        elif ext == \".csv\":","            lang = \"csv\"","        elif ext == \".txt\":","            lang = \"text\"","        else:","            lang = \"text\"","        ","        # Read file bytes","        try:","            content_bytes = file_path.read_bytes()","            # Try to decode as UTF-8, but fallback to replacement if needed","            try:","                content = content_bytes.decode(\"utf-8\")","            except UnicodeDecodeError:","                content = content_bytes.decode(\"utf-8\", errors=\"replace\")","        except Exception as e:","            content = f\"ERROR reading file: {e}\"","        ","        # Section header (match test expectations: ## FILENAME_WITH_EXTENSION)","        section_name = filename  # Keep extension","        lines.append(f\"## {section_name}\")","        lines.append(\"\")","        lines.append(f\"```{lang}\")","        lines.append(content.rstrip(\"\\n\"))  # Remove trailing newline to avoid extra line","        lines.append(\"```\")","        lines.append(\"\")","        lines.append(\"---\")","        lines.append(\"\")","    ","    # Missing files section","    if missing_files:","        lines.append(\"## Missing Files\")","        lines.append(\"\")","        lines.append(\"The following expected files were not found in the snapshot directory:\")","        lines.append(\"\")","        for filename in missing_files:","            lines.append(f\"- `{filename}`\")","        lines.append(\"\")","    ","    # Convert to bytes","    output_content = \"\\n\".join(lines)","    output_bytes = output_content.encode(\"utf-8\")","    ","    # Atomic write","    write_bytes_atomic(out_path, output_bytes)","    ","    return out_path","","","def verify_deterministic(","    snapshots_root: str | Path = \"outputs/snapshots\",","    full_dir_name: str = \"full\",","    out_name: str = \"SYSTEM_FULL_SNAPSHOT.md\",",") -> bool:","    \"\"\"","    Verify that compiling the same snapshot twice produces identical bytes.","    ","    Returns True if deterministic, False otherwise.","    \"\"\"","    snapshots_root = Path(snapshots_root)","    ","    # Compile first time","    out1 = compile_full_snapshot(snapshots_root, full_dir_name, out_name + \".test1\")","    data1 = out1.read_bytes()","    ","    # Compile second time","    out2 = compile_full_snapshot(snapshots_root, full_dir_name, out_name + \".test2\")","    data2 = out2.read_bytes()","    ","    # Clean up test files","    out1.unlink(missing_ok=True)","    out2.unlink(missing_ok=True)","    ","    return data1 == data2","","","if __name__ == \"__main__\":","    import argparse","    ","    parser = argparse.ArgumentParser(","        description=\"Compile full snapshot artifacts into a single SYSTEM_FULL_SNAPSHOT.md.\"","    )","    parser.add_argument(","        \"--verify-deterministic\",","        action=\"store_true\",","        help=\"Verify that compilation is deterministic (run twice and compare).\"","    )","    parser.add_argument(","        \"--snapshots-root\",","        default=\"outputs/snapshots\",","        help=\"Root directory containing snapshots (default: outputs/snapshots).\"","    )","    parser.add_argument(","        \"--full-dir\","]}
{"type":"file_chunk","path":"src/control/snapshot_compiler.py","chunk_index":1,"line_start":201,"line_end":231,"content":["        default=\"full\",","        help=\"Name of directory with raw artifacts (default: full).\"","    )","    parser.add_argument(","        \"--out-name\",","        default=\"SYSTEM_FULL_SNAPSHOT.md\",","        help=\"Output filename (default: SYSTEM_FULL_SNAPSHOT.md).\"","    )","    ","    args = parser.parse_args()","    ","    if args.verify_deterministic:","        print(\"Verifying determinism...\")","        if verify_deterministic(args.snapshots_root, args.full_dir, args.out_name):","            print(\"✓ Compilation is deterministic\")","            sys.exit(0)","        else:","            print(\"✗ Compilation is NOT deterministic\")","            sys.exit(1)","    else:","        try:","            out_path = compile_full_snapshot(","                args.snapshots_root,","                args.full_dir,","                args.out_name","            )","            print(f\"Compiled snapshot written to: {out_path}\")","            print(f\"Size: {out_path.stat().st_size:,} bytes\")","        except Exception as e:","            print(f\"Error: {e}\", file=sys.stderr)","            sys.exit(1)"]}
{"type":"file_footer","path":"src/control/snapshot_compiler.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/strategy_catalog.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7960,"sha256":"32eb717e2fd8503b93f4ee6bdb7482194371da9f5991299ef4cd9daaed9b0c1e","total_lines":221,"chunk_count":2}
{"type":"file_chunk","path":"src/control/strategy_catalog.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Strategy Catalog for M1 Wizard.","","Provides strategy listing and parameter schema capabilities for the wizard UI.","\"\"\"","","from __future__ import annotations","","from typing import List, Optional, Dict, Any","","from strategy.registry import (","    get_strategy_registry,","    StrategyRegistryResponse,","    StrategySpecForGUI,","    load_builtin_strategies,","    list_strategies,","    get as get_strategy_spec,",")","from strategy.param_schema import ParamSpec","","","class StrategyCatalog:","    \"\"\"Catalog for available strategies.\"\"\"","    ","    def __init__(self, load_builtin: bool = True):","        \"\"\"Initialize strategy catalog.","        ","        Args:","            load_builtin: Whether to load built-in strategies on initialization.","        \"\"\"","        self._registry_response: Optional[StrategyRegistryResponse] = None","        ","        if load_builtin:","            # Ensure built-in strategies are loaded","            try:","                load_builtin_strategies()","            except Exception:","                # Already loaded or error, continue","                pass","    ","    def load_registry(self) -> StrategyRegistryResponse:","        \"\"\"Load strategy registry.\"\"\"","        self._registry_response = get_strategy_registry()","        return self._registry_response","    ","    @property","    def registry(self) -> StrategyRegistryResponse:","        \"\"\"Get strategy registry (loads if not already loaded).\"\"\"","        if self._registry_response is None:","            self.load_registry()","        return self._registry_response","    ","    def list_strategies(self) -> List[StrategySpecForGUI]:","        \"\"\"List all available strategies for GUI.\"\"\"","        return self.registry.strategies","    ","    def get_strategy(self, strategy_id: str) -> Optional[StrategySpecForGUI]:","        \"\"\"Get strategy by ID for GUI.\"\"\"","        for strategy in self.registry.strategies:","            if strategy.strategy_id == strategy_id:","                return strategy","        return None","    ","    def get_strategy_spec(self, strategy_id: str):","        \"\"\"Get internal StrategySpec by ID.\"\"\"","        try:","            return get_strategy_spec(strategy_id)","        except KeyError:","            return None","    ","    def get_parameters(self, strategy_id: str) -> List[ParamSpec]:","        \"\"\"Get parameter schema for a strategy.\"\"\"","        strategy = self.get_strategy(strategy_id)","        if strategy is None:","            return []","        return strategy.params","    ","    def get_parameter_defaults(self, strategy_id: str) -> Dict[str, Any]:","        \"\"\"Get default parameter values for a strategy.\"\"\"","        params = self.get_parameters(strategy_id)","        defaults = {}","        for param in params:","            if param.default is not None:","                defaults[param.name] = param.default","        return defaults","    ","    def validate_parameters(","        self, ","        strategy_id: str, ","        parameters: Dict[str, Any]","    ) -> Dict[str, str]:","        \"\"\"Validate parameter values against schema.","        ","        Args:","            strategy_id: Strategy ID","            parameters: Parameter values to validate","            ","        Returns:","            Dictionary of validation errors (empty if valid)","        \"\"\"","        errors = {}","        params = self.get_parameters(strategy_id)","        ","        # Build lookup by parameter name","        param_map = {p.name: p for p in params}","        ","        for param_name, param_spec in param_map.items():","            value = parameters.get(param_name)","            ","            # Check required (all parameters are required for now)","            if value is None:","                errors[param_name] = f\"Parameter '{param_name}' is required\"","                continue","            ","            # Type validation","            if param_spec.type == \"int\":","                if not isinstance(value, (int, float)):","                    try:","                        int(value)","                    except (ValueError, TypeError):","                        errors[param_name] = f\"Parameter '{param_name}' must be an integer\"","                else:","                    # Check min/max","                    if param_spec.min is not None and value < param_spec.min:","                        errors[param_name] = f\"Parameter '{param_name}' must be >= {param_spec.min}\"","                    if param_spec.max is not None and value > param_spec.max:","                        errors[param_name] = f\"Parameter '{param_name}' must be <= {param_spec.max}\"","            ","            elif param_spec.type == \"float\":","                if not isinstance(value, (int, float)):","                    try:","                        float(value)","                    except (ValueError, TypeError):","                        errors[param_name] = f\"Parameter '{param_name}' must be a number\"","                else:","                    # Check min/max","                    if param_spec.min is not None and value < param_spec.min:","                        errors[param_name] = f\"Parameter '{param_name}' must be >= {param_spec.min}\"","                    if param_spec.max is not None and value > param_spec.max:","                        errors[param_name] = f\"Parameter '{param_name}' must be <= {param_spec.max}\"","            ","            elif param_spec.type == \"bool\":","                if not isinstance(value, bool):","                    errors[param_name] = f\"Parameter '{param_name}' must be a boolean\"","            ","            elif param_spec.type == \"enum\":","                if param_spec.choices and value not in param_spec.choices:","                    errors[param_name] = (","                        f\"Parameter '{param_name}' must be one of: {', '.join(map(str, param_spec.choices))}\"","                    )","        ","        # Check for extra parameters not in schema","        for param_name in parameters:","            if param_name not in param_map:","                errors[param_name] = f\"Unknown parameter '{param_name}' for strategy '{strategy_id}'\"","        ","        return errors","    ","    def get_strategy_ids(self) -> List[str]:","        \"\"\"Get list of all strategy IDs.\"\"\"","        return [s.strategy_id for s in self.registry.strategies]","    ","    def filter_by_parameter_count(self, min_params: int = 0, max_params: int = 10) -> List[StrategySpecForGUI]:","        \"\"\"Filter strategies by parameter count.\"\"\"","        return [","            s for s in self.registry.strategies","            if min_params <= len(s.params) <= max_params","        ]","    ","    def list_strategy_ids(self) -> List[str]:","        \"\"\"Get list of all strategy IDs.","        ","        Returns:","            List of strategy IDs sorted alphabetically","        \"\"\"","        return sorted([s.strategy_id for s in self.registry.strategies])","    ","    def get_strategy_spec_public(self, strategy_id: str) -> Optional[StrategySpecForGUI]:","        \"\"\"Public API: Get strategy spec by ID.","        ","        Args:","            strategy_id: Strategy ID to get","            ","        Returns:","            StrategySpecForGUI if found, None otherwise","        \"\"\"","        return self.get_strategy(strategy_id)","","","# Singleton instance for easy access","_catalog_instance: Optional[StrategyCatalog] = None","","def get_strategy_catalog() -> StrategyCatalog:","    \"\"\"Get singleton strategy catalog instance.\"\"\"","    global _catalog_instance","    if _catalog_instance is None:","        _catalog_instance = StrategyCatalog()","    return _catalog_instance","","","# Public API functions for registry access"]}
{"type":"file_chunk","path":"src/control/strategy_catalog.py","chunk_index":1,"line_start":201,"line_end":221,"content":["def list_strategy_ids() -> List[str]:","    \"\"\"Public API: Get list of all strategy IDs.","    ","    Returns:","        List of strategy IDs sorted alphabetically","    \"\"\"","    catalog = get_strategy_catalog()","    return catalog.list_strategy_ids()","","","def get_strategy_spec(strategy_id: str) -> Optional[StrategySpecForGUI]:","    \"\"\"Public API: Get strategy spec by ID.","    ","    Args:","        strategy_id: Strategy ID to get","        ","    Returns:","        StrategySpecForGUI if found, None otherwise","    \"\"\"","    catalog = get_strategy_catalog()","    return catalog.get_strategy_spec_public(strategy_id)"]}
{"type":"file_footer","path":"src/control/strategy_catalog.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/types.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1572,"sha256":"64b929a1a8c3a2eb9e10b1a9ecd111cebacf92fe5a158b9b4fb31d7953dfa9b1","total_lines":60,"chunk_count":1}
{"type":"file_chunk","path":"src/control/types.py","chunk_index":0,"line_start":1,"line_end":60,"content":["","\"\"\"Type definitions for B5-C Mission Control.\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, field","from enum import StrEnum","from typing import Any, Literal, Optional","","","class JobStatus(StrEnum):","    \"\"\"Job status state machine.\"\"\"","","    QUEUED = \"QUEUED\"","    RUNNING = \"RUNNING\"","    PAUSED = \"PAUSED\"","    DONE = \"DONE\"","    FAILED = \"FAILED\"","    KILLED = \"KILLED\"","","","class StopMode(StrEnum):","    \"\"\"Stop request mode.\"\"\"","","    SOFT = \"SOFT\"","    KILL = \"KILL\"","","","@dataclass(frozen=True)","class DBJobSpec:","    \"\"\"Job specification for DB/worker runtime (input to create_job).\"\"\"","","    season: str","    dataset_id: str","    outputs_root: str","    config_snapshot: dict[str, Any]  # sanitized; no ndarrays","    config_hash: str","    data_fingerprint_sha256_40: str = \"\"  # Data fingerprint SHA256[:40] (empty if not provided, marks DIRTY)","    created_by: str = \"b5c\"","","","@dataclass(frozen=True)","class JobRecord:","    \"\"\"Job record (returned from DB).\"\"\"","","    job_id: str","    status: JobStatus","    created_at: str","    updated_at: str","    spec: DBJobSpec","    pid: Optional[int] = None","    run_id: Optional[str] = None  # Final stage run_id (e.g. stage2_confirm-xxx)","    run_link: Optional[str] = None  # e.g. outputs/.../stage0_run_id or final run index pointer","    report_link: Optional[str] = None  # Link to B5 report viewer","    last_error: Optional[str] = None","    tags: list[str] = field(default_factory=list)  # Tags for job categorization and search","    data_fingerprint_sha256_40: str = \"\"  # Data fingerprint SHA256[:40] (empty if missing, marks DIRTY)","","",""]}
{"type":"file_footer","path":"src/control/types.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/wizard_nicegui.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":26463,"sha256":"2b11ca8ccfc9fa4de27e8ca3439b2290d70f92617756f827f5340364defe1c37","total_lines":642,"chunk_count":4}
{"type":"file_chunk","path":"src/control/wizard_nicegui.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Research Job Wizard (Phase 12) - NiceGUI interface.","","Phase 12: Config-only wizard that outputs WizardJobSpec JSON.","GUI → POST /jobs (WizardJobSpec) only, no worker calls, no filesystem access.","\"\"\"","","from __future__ import annotations","","import json","from datetime import date, datetime","from typing import Any, Dict, List, Optional","","import requests","from nicegui import ui","","from control.job_spec import DataSpec, WizardJobSpec, WFSSpec","from control.param_grid import GridMode, ParamGridSpec","from control.job_expand import JobTemplate, expand_job_template, estimate_total_jobs","from control.batch_submit import BatchSubmitRequest, BatchSubmitResponse","from data.dataset_registry import DatasetRecord","from strategy.param_schema import ParamSpec","from strategy.registry import StrategySpecForGUI","","# API base URL","API_BASE = \"http://localhost:8000\"","","","class WizardState:","    \"\"\"State management for wizard steps.\"\"\"","    ","    def __init__(self) -> None:","        self.season: str = \"\"","        self.data1: Optional[DataSpec] = None","        self.data2: Optional[DataSpec] = None","        self.strategy_id: str = \"\"","        self.params: Dict[str, Any] = {}","        self.wfs = WFSSpec()","        ","        # Phase 13: Batch mode","        self.batch_mode: bool = False","        self.param_grid_specs: Dict[str, ParamGridSpec] = {}","        self.job_template: Optional[JobTemplate] = None","        ","        # UI references","        self.data1_widgets: Dict[str, Any] = {}","        self.data2_widgets: Dict[str, Any] = {}","        self.param_widgets: Dict[str, Any] = {}","        self.wfs_widgets: Dict[str, Any] = {}","        self.batch_widgets: Dict[str, Any] = {}","","","def fetch_datasets() -> List[DatasetRecord]:","    \"\"\"Fetch dataset registry from API.\"\"\"","    try:","        resp = requests.get(f\"{API_BASE}/meta/datasets\", timeout=5)","        resp.raise_for_status()","        data = resp.json()","        return [DatasetRecord.model_validate(d) for d in data[\"datasets\"]]","    except Exception as e:","        ui.notify(f\"Failed to load datasets: {e}\", type=\"negative\")","        return []","","","def fetch_strategies() -> List[StrategySpecForGUI]:","    \"\"\"Fetch strategy registry from API.\"\"\"","    try:","        resp = requests.get(f\"{API_BASE}/meta/strategies\", timeout=5)","        resp.raise_for_status()","        data = resp.json()","        return [StrategySpecForGUI.model_validate(s) for s in data[\"strategies\"]]","    except Exception as e:","        ui.notify(f\"Failed to load strategies: {e}\", type=\"negative\")","        return []","","","def create_data_section(","    state: WizardState,","    section_name: str,","    is_primary: bool = True",") -> Dict[str, Any]:","    \"\"\"Create dataset selection UI section.\"\"\"","    widgets: Dict[str, Any] = {}","    ","    with ui.card().classes(\"w-full mb-4\"):","        ui.label(f\"{section_name} Dataset\").classes(\"text-lg font-bold\")","        ","        # Dataset dropdown","        datasets = fetch_datasets()","        dataset_options = {d.id: f\"{d.symbol} ({d.timeframe}) {d.start_date}-{d.end_date}\" ","                          for d in datasets}","        ","        dataset_select = ui.select(","            label=\"Dataset\",","            options=dataset_options,","            with_input=True","        ).classes(\"w-full\")","        widgets[\"dataset_select\"] = dataset_select","        ","        # Date range inputs","        with ui.row().classes(\"w-full\"):","            start_date = ui.date(","                label=\"Start Date\",","                value=date(2020, 1, 1)","            ).classes(\"w-1/2\")","            widgets[\"start_date\"] = start_date","            ","            end_date = ui.date(","                label=\"End Date\",","                value=date(2024, 12, 31)","            ).classes(\"w-1/2\")","            widgets[\"end_date\"] = end_date","        ","        # Update date limits when dataset changes","        def update_date_limits(selected_id: str) -> None:","            dataset = next((d for d in datasets if d.id == selected_id), None)","            if dataset:","                start_date.value = dataset.start_date","                end_date.value = dataset.end_date","                start_date._props[\"min\"] = dataset.start_date.isoformat()","                start_date._props[\"max\"] = dataset.end_date.isoformat()","                end_date._props[\"min\"] = dataset.start_date.isoformat()","                end_date._props[\"max\"] = dataset.end_date.isoformat()","                start_date.update()","                end_date.update()","        ","        dataset_select.on('update:model-value', lambda e: update_date_limits(e.args))","        ","        # Set initial limits if dataset is selected","        if dataset_select.value:","            update_date_limits(dataset_select.value)","    ","    return widgets","","","def create_strategy_section(state: WizardState) -> Dict[str, Any]:","    \"\"\"Create strategy selection and parameter UI section.\"\"\"","    widgets: Dict[str, Any] = {}","    ","    with ui.card().classes(\"w-full mb-4\"):","        ui.label(\"Strategy\").classes(\"text-lg font-bold\")","        ","        # Strategy dropdown","        strategies = fetch_strategies()","        strategy_options = {s.strategy_id: s.strategy_id for s in strategies}","        ","        strategy_select = ui.select(","            label=\"Strategy\",","            options=strategy_options,","            with_input=True","        ).classes(\"w-full\")","        widgets[\"strategy_select\"] = strategy_select","        ","        # Parameter container (dynamic)","        param_container = ui.column().classes(\"w-full mt-4\")","        widgets[\"param_container\"] = param_container","        ","        def update_parameters(selected_id: str) -> None:","            \"\"\"Update parameter UI based on selected strategy.\"\"\"","            param_container.clear()","            state.param_widgets.clear()","            ","            strategy = next((s for s in strategies if s.strategy_id == selected_id), None)","            if not strategy:","                return","            ","            ui.label(\"Parameters\").classes(\"font-bold mt-2\")","            ","            for param in strategy.params:","                with ui.row().classes(\"w-full items-center\"):","                    ui.label(f\"{param.name}:\").classes(\"w-1/3\")","                    ","                    if param.type == \"int\" or param.type == \"float\":","                        # Slider for numeric parameters","                        min_val = param.min if param.min is not None else 0","                        max_val = param.max if param.max is not None else 100","                        step = param.step if param.step is not None else 1","                        ","                        slider = ui.slider(","                            min=min_val,","                            max=max_val,","                            value=param.default,","                            step=step","                        ).classes(\"w-2/3\")","                        ","                        value_label = ui.label().bind_text_from(","                            slider, \"value\", ","                            lambda v: f\"{v:.2f}\" if param.type == \"float\" else f\"{int(v)}\"","                        )","                        ","                        state.param_widgets[param.name] = slider","                        ","                    elif param.type == \"enum\" and param.choices:","                        # Dropdown for enum parameters","                        dropdown = ui.select(","                            options=param.choices,","                            value=param.default","                        ).classes(\"w-2/3\")","                        state.param_widgets[param.name] = dropdown","                        "]}
{"type":"file_chunk","path":"src/control/wizard_nicegui.py","chunk_index":1,"line_start":201,"line_end":400,"content":["                    elif param.type == \"bool\":","                        # Switch for boolean parameters","                        switch = ui.switch(value=param.default).classes(\"w-2/3\")","                        state.param_widgets[param.name] = switch","                    ","                    # Help text","                    if param.help:","                        ui.tooltip(param.help).classes(\"ml-2\")","        ","        strategy_select.on('update:model-value', lambda e: update_parameters(e.args))","        ","        # Initialize if strategy is selected","        if strategy_select.value:","            update_parameters(strategy_select.value)","    ","    return widgets","","","def create_batch_mode_section(state: WizardState) -> Dict[str, Any]:","    \"\"\"Create batch mode UI section (Phase 13).\"\"\"","    widgets: Dict[str, Any] = {}","    ","    with ui.card().classes(\"w-full mb-4\"):","        ui.label(\"Batch Mode (Phase 13)\").classes(\"text-lg font-bold\")","        ","        # Batch mode toggle","        batch_toggle = ui.switch(\"Enable Batch Mode (Parameter Grid)\")","        widgets[\"batch_toggle\"] = batch_toggle","        ","        # Container for grid UI (hidden when batch mode off)","        grid_container = ui.column().classes(\"w-full mt-4\")","        widgets[\"grid_container\"] = grid_container","        ","        # Cost preview label","        cost_label = ui.label(\"Total jobs: 0 | Risk: Low\").classes(\"font-bold mt-2\")","        widgets[\"cost_label\"] = cost_label","        ","        def update_batch_mode(enabled: bool) -> None:","            \"\"\"Show/hide grid UI based on batch mode toggle.\"\"\"","            grid_container.clear()","            state.batch_mode = enabled","            state.param_grid_specs.clear()","            ","            if not enabled:","                cost_label.set_text(\"Total jobs: 0 | Risk: Low\")","                return","            ","            # Fetch current strategy parameters","            strategy_id = state.strategy_id","            strategies = fetch_strategies()","            strategy = next((s for s in strategies if s.strategy_id == strategy_id), None)","            if not strategy:","                ui.notify(\"No strategy selected\", type=\"warning\")","                return","            ","            # Create grid UI for each parameter","            ui.label(\"Parameter Grid\").classes(\"font-bold mt-2\")","            ","            for param in strategy.params:","                with ui.row().classes(\"w-full items-center mb-2\"):","                    ui.label(f\"{param.name}:\").classes(\"w-1/4\")","                    ","                    # Grid mode selector","                    mode_select = ui.select(","                        options={","                            GridMode.SINGLE.value: \"Single\",","                            GridMode.RANGE.value: \"Range\",","                            GridMode.MULTI.value: \"Multi Values\"","                        },","                        value=GridMode.SINGLE.value","                    ).classes(\"w-1/4\")","                    ","                    # Value inputs (dynamic based on mode)","                    value_container = ui.row().classes(\"w-1/2\")","                    ","                    def make_param_updater(pname: str, mode_sel, val_container, param_spec):","                        def update_grid_ui():","                            mode = GridMode(mode_sel.value)","                            val_container.clear()","                            ","                            if mode == GridMode.SINGLE:","                                # Single value input (same as default)","                                if param_spec.type == \"int\" or param_spec.type == \"float\":","                                    default = param_spec.default","                                    val = ui.number(value=default, min=param_spec.min, max=param_spec.max, step=param_spec.step or 1)","                                elif param_spec.type == \"enum\":","                                    val = ui.select(options=param_spec.choices, value=param_spec.default)","                                elif param_spec.type == \"bool\":","                                    val = ui.switch(value=param_spec.default)","                                else:","                                    val = ui.input(value=str(param_spec.default))","                                val_container.add(val)","                                # Store spec","                                state.param_grid_specs[pname] = ParamGridSpec(","                                    mode=mode,","                                    single_value=val.value","                                )","                            elif mode == GridMode.RANGE:","                                # Range: start, end, step","                                start = ui.number(value=param_spec.min or 0, label=\"Start\")","                                end = ui.number(value=param_spec.max or 100, label=\"End\")","                                step = ui.number(value=param_spec.step or 1, label=\"Step\")","                                val_container.add(start)","                                val_container.add(end)","                                val_container.add(step)","                                # Store spec (will be updated on change)","                                state.param_grid_specs[pname] = ParamGridSpec(","                                    mode=mode,","                                    range_start=start.value,","                                    range_end=end.value,","                                    range_step=step.value","                                )","                            elif mode == GridMode.MULTI:","                                # Multi values: comma-separated input","                                default_vals = \",\".join([str(param_spec.default)])","                                val = ui.input(value=default_vals, label=\"Values (comma separated)\")","                                val_container.add(val)","                                state.param_grid_specs[pname] = ParamGridSpec(","                                    mode=mode,","                                    multi_values=[param_spec.default]","                                )","                            # Trigger cost update","                            update_cost_preview()","                        return update_grid_ui","                    ","                    # Initial creation","                    updater = make_param_updater(param.name, mode_select, value_container, param)","                    mode_select.on('update:model-value', lambda e: updater())","                    updater()  # call once to create initial UI","        ","        batch_toggle.on('update:model-value', lambda e: update_batch_mode(e.args))","        ","        def update_cost_preview():","            \"\"\"Update cost preview label based on current grid specs.\"\"\"","            if not state.batch_mode:","                cost_label.set_text(\"Total jobs: 0 | Risk: Low\")","                return","            ","            # Build a temporary JobTemplate to estimate total jobs","            try:","                # Collect base WizardJobSpec from current UI (simplified)","                # We'll just use dummy values for estimation","                template = JobTemplate(","                    season=state.season,","                    dataset_id=\"dummy\",","                    strategy_id=state.strategy_id,","                    param_grid=state.param_grid_specs.copy(),","                    wfs=state.wfs","                )","                total = estimate_total_jobs(template)","                # Risk heuristic","                risk = \"Low\"","                if total > 100:","                    risk = \"Medium\"","                if total > 1000:","                    risk = \"High\"","                cost_label.set_text(f\"Total jobs: {total} | Risk: {risk}\")","            except Exception:","                cost_label.set_text(\"Total jobs: ? | Risk: Unknown\")","        ","        # Update cost preview periodically","        ui.timer(2.0, update_cost_preview)","    ","    return widgets","","","def create_wfs_section(state: WizardState) -> Dict[str, Any]:","    \"\"\"Create WFS configuration UI section.\"\"\"","    widgets: Dict[str, Any] = {}","    ","    with ui.card().classes(\"w-full mb-4\"):","        ui.label(\"WFS Configuration\").classes(\"text-lg font-bold\")","        ","        # Stage0 subsample","        subsample_slider = ui.slider(","            label=\"Stage0 Subsample\",","            min=0.01,","            max=1.0,","            value=state.wfs.stage0_subsample,","            step=0.01","        ).classes(\"w-full\")","        widgets[\"subsample\"] = subsample_slider","        ui.label().bind_text_from(subsample_slider, \"value\", lambda v: f\"{v:.2f}\")","        ","        # Top K","        top_k_input = ui.number(","            label=\"Top K\",","            value=state.wfs.top_k,","            min=1,","            max=1000,","            step=10","        ).classes(\"w-full\")","        widgets[\"top_k\"] = top_k_input","        ","        # Memory limit","        mem_input = ui.number(","            label=\"Memory Limit (MB)\",","            value=state.wfs.mem_limit_mb,","            min=1024,","            max=32768,"]}
{"type":"file_chunk","path":"src/control/wizard_nicegui.py","chunk_index":2,"line_start":401,"line_end":600,"content":["            step=1024","        ).classes(\"w-full\")","        widgets[\"mem_limit\"] = mem_input","        ","        # Auto-downsample switch","        auto_downsample = ui.switch(","            \"Allow Auto Downsample\",","            value=state.wfs.allow_auto_downsample","        ).classes(\"w-full\")","        widgets[\"auto_downsample\"] = auto_downsample","    ","    return widgets","","","def create_preview_section(state: WizardState) -> ui.textarea:","    \"\"\"Create WizardJobSpec preview section.\"\"\"","    with ui.card().classes(\"w-full mb-4\"):","        ui.label(\"WizardJobSpec Preview\").classes(\"text-lg font-bold\")","        ","        preview = ui.textarea(\"\").classes(\"w-full h-64 font-mono text-sm\").props(\"readonly\")","        ","        def update_preview() -> None:","            \"\"\"Update WizardJobSpec preview.\"\"\"","            try:","                # Collect data from UI","                dataset_id = None","                if state.data1_widgets:","                    dataset_id = state.data1_widgets[\"dataset_select\"].value","                    start_date = state.data1_widgets[\"start_date\"].value","                    end_date = state.data1_widgets[\"end_date\"].value","                    ","                    if dataset_id and start_date and end_date:","                        state.data1 = DataSpec(","                            dataset_id=dataset_id,","                            start_date=start_date,","                            end_date=end_date","                        )","                ","                # Collect strategy parameters","                params = {}","                for param_name, widget in state.param_widgets.items():","                    if hasattr(widget, 'value'):","                        params[param_name] = widget.value","                ","                # Collect WFS settings","                if state.wfs_widgets:","                    state.wfs = WFSSpec(","                        stage0_subsample=state.wfs_widgets[\"subsample\"].value,","                        top_k=state.wfs_widgets[\"top_k\"].value,","                        mem_limit_mb=state.wfs_widgets[\"mem_limit\"].value,","                        allow_auto_downsample=state.wfs_widgets[\"auto_downsample\"].value","                    )","                ","                if state.batch_mode:","                    # Create JobTemplate","                    template = JobTemplate(","                        season=state.season,","                        dataset_id=dataset_id if dataset_id else \"unknown\",","                        strategy_id=state.strategy_id,","                        param_grid=state.param_grid_specs.copy(),","                        wfs=state.wfs","                    )","                    # Update preview with template JSON","                    preview.value = template.model_dump_json(indent=2)","                else:","                    # Create single WizardJobSpec","                    jobspec = WizardJobSpec(","                        season=state.season,","                        data1=state.data1,","                        data2=state.data2,","                        strategy_id=state.strategy_id,","                        params=params,","                        wfs=state.wfs","                    )","                    # Update preview","                    preview.value = jobspec.model_dump_json(indent=2)","                ","            except Exception as e:","                preview.value = f\"Error creating preview: {e}\"","        ","        # Update preview periodically","        ui.timer(1.0, update_preview)","        ","        return preview","","","def submit_job(state: WizardState, preview: ui.textarea) -> None:","    \"\"\"Submit WizardJobSpec to API.\"\"\"","    try:","        # Parse WizardJobSpec from preview","        jobspec_data = json.loads(preview.value)","        jobspec = WizardJobSpec.model_validate(jobspec_data)","        ","        # Submit to API","        resp = requests.post(","            f\"{API_BASE}/jobs\",","            json=json.loads(jobspec.model_dump_json())","        )","        resp.raise_for_status()","        ","        job_id = resp.json()[\"job_id\"]","        ui.notify(f\"Job submitted successfully! Job ID: {job_id}\", type=\"positive\")","        ","    except Exception as e:","        ui.notify(f\"Failed to submit job: {e}\", type=\"negative\")","","","def submit_batch_job(state: WizardState, preview: ui.textarea) -> None:","    \"\"\"Submit batch of jobs via batch API.\"\"\"","    try:","        # Parse JobTemplate from preview","        template_data = json.loads(preview.value)","        template = JobTemplate.model_validate(template_data)","        ","        # Expand template to JobSpec list","        jobspecs = expand_job_template(template)","        ","        # Build batch request","        batch_req = BatchSubmitRequest(jobs=list(jobspecs))","        ","        # Submit to batch endpoint","        resp = requests.post(","            f\"{API_BASE}/jobs/batch\",","            json=json.loads(batch_req.model_dump_json())","        )","        resp.raise_for_status()","        ","        batch_resp = BatchSubmitResponse.model_validate(resp.json())","        ui.notify(","            f\"Batch submitted successfully! Batch ID: {batch_resp.batch_id}, \"","            f\"Total jobs: {batch_resp.total_jobs}\",","            type=\"positive\"","        )","        ","    except Exception as e:","        ui.notify(f\"Failed to submit batch: {e}\", type=\"negative\")","","","@ui.page(\"/wizard\")","def wizard_page() -> None:","    \"\"\"Research Job Wizard main page.\"\"\"","    ui.page_title(\"Research Job Wizard (Phase 12)\")","    ","    state = WizardState()","    ","    with ui.column().classes(\"w-full max-w-4xl mx-auto p-4\"):","        ui.label(\"Research Job Wizard\").classes(\"text-2xl font-bold mb-6\")","        ui.label(\"Phase 12: Config-only job specification\").classes(\"text-gray-600 mb-8\")","        ","        # Season input","        with ui.card().classes(\"w-full mb-4\"):","            ui.label(\"Season\").classes(\"text-lg font-bold\")","            season_input = ui.input(","                label=\"Season\",","                value=\"2024Q1\",","                placeholder=\"e.g., 2024Q1, 2024Q2\"","            ).classes(\"w-full\")","            ","            def update_season() -> None:","                state.season = season_input.value","            ","            season_input.on('update:model-value', lambda e: update_season())","            update_season()","        ","        # Step 1: Data","        with ui.expansion(\"Step 1: Data\", value=True).classes(\"w-full mb-4\"):","            ui.label(\"Primary Dataset\").classes(\"font-bold mt-2\")","            state.data1_widgets = create_data_section(state, \"Primary\", is_primary=True)","            ","            # Data2 toggle","            enable_data2 = ui.switch(\"Enable Secondary Dataset (for validation)\")","            ","            data2_container = ui.column().classes(\"w-full\")","            ","            def toggle_data2(enabled: bool) -> None:","                data2_container.clear()","                if enabled:","                    state.data2_widgets = create_data_section(state, \"Secondary\", is_primary=False)","                else:","                    state.data2 = None","                    state.data2_widgets = {}","            ","            enable_data2.on('update:model-value', lambda e: toggle_data2(e.args))","        ","        # Step 2: Strategy","        with ui.expansion(\"Step 2: Strategy\", value=True).classes(\"w-full mb-4\"):","            strategy_widgets = create_strategy_section(state)","            ","            def update_strategy() -> None:","                state.strategy_id = strategy_widgets[\"strategy_select\"].value","            ","            strategy_widgets[\"strategy_select\"].on('update:model-value', lambda e: update_strategy())","            if strategy_widgets[\"strategy_select\"].value:","                update_strategy()","        ","        # Step 3: Batch Mode (Phase 13)","        with ui.expansion(\"Step 3: Batch Mode (Optional)\", value=True).classes(\"w-full mb-4\"):","            state.batch_widgets = create_batch_mode_section(state)","        ","        # Step 4: WFS"]}
{"type":"file_chunk","path":"src/control/wizard_nicegui.py","chunk_index":3,"line_start":601,"line_end":642,"content":["        with ui.expansion(\"Step 4: WFS Configuration\", value=True).classes(\"w-full mb-4\"):","            state.wfs_widgets = create_wfs_section(state)","        ","        # Step 5: Preview & Submit","        with ui.expansion(\"Step 5: Preview & Submit\", value=True).classes(\"w-full mb-4\"):","            preview = create_preview_section(state)","            ","            with ui.row().classes(\"w-full mt-4\"):","                # Conditional button based on batch mode","                def submit_action():","                    if state.batch_mode:","                        submit_batch_job(state, preview)","                    else:","                        submit_job(state, preview)","                ","                submit_btn = ui.button(","                    \"Submit Batch\" if state.batch_mode else \"Submit Job\",","                    on_click=submit_action","                ).classes(\"bg-green-500 text-white\")","                ","                # Update button label when batch mode changes","                def update_button_label():","                    submit_btn.set_text(\"Submit Batch\" if state.batch_mode else \"Submit Job\")","                ","                # Watch batch mode changes (simplified: we can't directly watch, but we can update via timer)","                ui.timer(1.0, update_button_label)","                ","                ui.button(\"Copy JSON\", on_click=lambda: ui.run_javascript(","                    f\"navigator.clipboard.writeText(`{preview.value}`)\"","                )).classes(\"bg-blue-500 text-white\")","        ","        # Phase 12 Rules reminder","        with ui.card().classes(\"w-full mt-8 bg-yellow-50\"):","            ui.label(\"Phase 12 Rules\").classes(\"font-bold text-yellow-800\")","            ui.label(\"✅ GUI only outputs WizardJobSpec JSON\").classes(\"text-sm text-yellow-700\")","            ui.label(\"✅ No worker calls, no filesystem access\").classes(\"text-sm text-yellow-700\")","            ui.label(\"✅ Strategy params from registry, not hardcoded\").classes(\"text-sm text-yellow-700\")","            ui.label(\"✅ Dataset selection from registry, not filesystem\").classes(\"text-sm text-yellow-700\")","","","",""]}
{"type":"file_footer","path":"src/control/wizard_nicegui.py","complete":true,"emitted_chunks":4}
{"type":"file_header","path":"src/control/worker.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7371,"sha256":"6da1a295255a1f980696a583b445aa079655bfefcfc8c3ff136461e048f7b7f6","total_lines":228,"chunk_count":2}
{"type":"file_chunk","path":"src/control/worker.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Worker - long-running task executor.\"\"\"","","from __future__ import annotations","","import os","import signal","import time","from datetime import datetime, timezone","from pathlib import Path","from typing import Optional","","# ✅ Module-level import for patch support","from pipeline.funnel_runner import run_funnel","","from control.jobs_db import (","    get_job,","    get_requested_pause,","    get_requested_stop,","    mark_done,","    mark_failed,","    mark_killed,","    update_running,","    update_run_link,",")","from control.paths import run_log_path","from control.report_links import make_report_link","from control.types import JobStatus, StopMode","","","def _append_log(log_path: Path, text: str) -> None:","    \"\"\"","    Append text to log file.","    ","    Args:","        log_path: Path to log file","        text: Text to append","    \"\"\"","    log_path.parent.mkdir(parents=True, exist_ok=True)","    with log_path.open(\"a\", encoding=\"utf-8\") as f:","        f.write(text)","        if not text.endswith(\"\\n\"):","            f.write(\"\\n\")","","","def worker_loop(db_path: Path, *, poll_s: float = 0.5) -> None:","    \"\"\"","    Worker loop: poll QUEUED jobs and execute them sequentially.","    ","    Args:","        db_path: Path to SQLite database","        poll_s: Polling interval in seconds","    \"\"\"","    while True:","        try:","            # Find QUEUED jobs","            from control.jobs_db import list_jobs","            ","            jobs = list_jobs(db_path, limit=100)","            queued_jobs = [j for j in jobs if j.status == JobStatus.QUEUED]","            ","            if queued_jobs:","                # Process first QUEUED job","                job = queued_jobs[0]","                run_one_job(db_path, job.job_id)","            else:","                # No jobs, sleep","                time.sleep(poll_s)","        except KeyboardInterrupt:","            break","        except Exception as e:","            # Log error but continue loop","            print(f\"Worker loop error: {e}\")","            time.sleep(poll_s)","","","def run_one_job(db_path: Path, job_id: str) -> None:","    \"\"\"","    Run a single job.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","    \"\"\"","    log_path: Path | None = None","    try:","        job = get_job(db_path, job_id)","        ","        # Check if already terminal","        if job.status in {JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED}:","            return","        ","        # Update to RUNNING with current PID","        pid = os.getpid()","        update_running(db_path, job_id, pid=pid)","        ","        # Log status update","        timestamp = datetime.now(timezone.utc).isoformat()","        outputs_root = Path(job.spec.outputs_root)","        season = job.spec.season","        ","        # Initialize log_path early (use job_id as run_id fallback)","        log_path = run_log_path(outputs_root, season, job_id)","        ","        # Check for KILL before starting","        stop_mode = get_requested_stop(db_path, job_id)","        if stop_mode == StopMode.KILL.value:","            _append_log(log_path, f\"{timestamp} [job_id={job_id}] [status=KILLED] Killed before execution\")","            mark_killed(db_path, job_id, error=\"Killed before execution\")","            return","        ","        outputs_root.mkdir(parents=True, exist_ok=True)","        ","        # Reconstruct runtime config from snapshot","        cfg = dict(job.spec.config_snapshot)","        # Ensure required fields are present","        cfg[\"season\"] = job.spec.season","        cfg[\"dataset_id\"] = job.spec.dataset_id","        ","        # Log job start","        _append_log(","            log_path,","            f\"{timestamp} [job_id={job_id}] [status=RUNNING] Starting funnel execution\"","        )","        ","        # Check pause/stop before each stage","        _check_pause_stop(db_path, job_id)","        ","        # Run funnel","        result = run_funnel(cfg, outputs_root)","        ","        # Extract run_id and generate report_link","        run_id: Optional[str] = None","        report_link: Optional[str] = None","        ","        if getattr(result, \"stages\", None) and result.stages:","            last = result.stages[-1]","            run_id = last.run_id","            report_link = make_report_link(season=job.spec.season, run_id=run_id)","            ","            # Update run_link","            run_link = str(last.run_dir)","            update_run_link(db_path, job_id, run_link=run_link)","            ","            # Log summary","            log_path = run_log_path(outputs_root, season, run_id)","            timestamp = datetime.now(timezone.utc).isoformat()","            _append_log(","                log_path,","                f\"{timestamp} [job_id={job_id}] [status=DONE] Funnel completed: \"","                f\"run_id={run_id}, stage={last.stage.value}, run_dir={run_link}\"","            )","        ","        # Mark as done with run_id and report_link (both can be None if no stages)","        mark_done(db_path, job_id, run_id=run_id, report_link=report_link)","        ","        # Log final status","        timestamp = datetime.now(timezone.utc).isoformat()","        if log_path:","            _append_log(log_path, f\"{timestamp} [job_id={job_id}] [status=DONE] Job completed successfully\")","        ","    except KeyboardInterrupt:","        if log_path:","            timestamp = datetime.now(timezone.utc).isoformat()","            _append_log(log_path, f\"{timestamp} [job_id={job_id}] [status=KILLED] Interrupted by user\")","        mark_killed(db_path, job_id, error=\"Interrupted by user\")","        raise","    except Exception as e:","        import traceback","        ","        # Short for DB column (500 chars)","        error_msg = str(e)[:500]","        mark_failed(db_path, job_id, error=error_msg)","        ","        # Full traceback for audit log (MUST)","        tb = traceback.format_exc()","        from control.jobs_db import append_log","        append_log(db_path, job_id, \"[ERROR] Unhandled exception\\n\" + tb)","        ","        # Also write to file log if available","        if log_path:","            timestamp = datetime.now(timezone.utc).isoformat()","            _append_log(log_path, f\"{timestamp} [job_id={job_id}] [status=FAILED] Error: {error_msg}\\n{tb}\")","        ","        # Keep worker stable","        return","","","def _check_pause_stop(db_path: Path, job_id: str) -> None:","    \"\"\"","    Check pause/stop flags and handle accordingly.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        ","    Raises:","        SystemExit: If KILL requested","    \"\"\"","    stop_mode = get_requested_stop(db_path, job_id)"]}
{"type":"file_chunk","path":"src/control/worker.py","chunk_index":1,"line_start":201,"line_end":228,"content":["    if stop_mode == StopMode.KILL.value:","        # Get PID and kill process","        job = get_job(db_path, job_id)","        if job.pid:","            try:","                os.kill(job.pid, signal.SIGTERM)","            except ProcessLookupError:","                pass  # Process already dead","        mark_killed(db_path, job_id, error=\"Killed by user\")","        raise SystemExit(\"Job killed\")","    ","    # Handle pause","    while get_requested_pause(db_path, job_id):","        time.sleep(0.5)","        # Re-check stop while paused","        stop_mode = get_requested_stop(db_path, job_id)","        if stop_mode == StopMode.KILL.value:","            job = get_job(db_path, job_id)","            if job.pid:","                try:","                    os.kill(job.pid, signal.SIGTERM)","                except ProcessLookupError:","                    pass","            mark_killed(db_path, job_id, error=\"Killed while paused\")","            raise SystemExit(\"Job killed while paused\")","","",""]}
{"type":"file_footer","path":"src/control/worker.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/worker_main.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":375,"sha256":"b0265c815e43796ccdacba4ebad4f2e08d5c4bca8f181f891dbb12445e375094","total_lines":20,"chunk_count":1}
{"type":"file_chunk","path":"src/control/worker_main.py","chunk_index":0,"line_start":1,"line_end":20,"content":["","\"\"\"Worker main entry point (for subprocess execution).\"\"\"","","from __future__ import annotations","","import sys","from pathlib import Path","","from control.worker import worker_loop","","if __name__ == \"__main__\":","    if len(sys.argv) < 2:","        print(\"Usage: python -m control.worker_main <db_path>\")","        sys.exit(1)","    ","    db_path = Path(sys.argv[1])","    worker_loop(db_path)","","",""]}
{"type":"file_footer","path":"src/control/worker_main.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/worker_spawn_policy.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3069,"sha256":"9e87d1252d2411fc5fa92b6df6bc28e7f4182d0bb7543eac51d6b1db32e3ae79","total_lines":80,"chunk_count":1}
{"type":"file_chunk","path":"src/control/worker_spawn_policy.py","chunk_index":0,"line_start":1,"line_end":80,"content":["\"\"\"Worker spawn policy - enforce governance to stop uncontrolled worker spawning.","","Contract:","Worker can only be started when all are true:","- Not in pytest context (PYTEST_CURRENT_TEST absent) OR explicit override FISHBRO_ALLOW_SPAWN_IN_TESTS=1","- DB path is not under /tmp unless explicit override FISHBRO_ALLOW_TMP_DB=1","- pidfile locking ensures no duplicate spawn for same db_path (handled elsewhere)","- pidfile must be validated: process exists AND cmdline matches worker_main and db_path","\"\"\"","","from __future__ import annotations","","import os","from pathlib import Path","","","def can_spawn_worker(db_path: Path) -> tuple[bool, str]:","    \"\"\"Return (allowed, reason).","","    Rules:","    1. If PYTEST_CURRENT_TEST is set and FISHBRO_ALLOW_SPAWN_IN_TESTS != \"1\":","        deny with message about pytest.","    2. If db_path is under /tmp and FISHBRO_ALLOW_TMP_DB != \"1\":","        deny with message about /tmp.","    3. Otherwise allow.","    \"\"\"","    if os.getenv(\"PYTEST_CURRENT_TEST\") and os.getenv(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\") != \"1\":","        return False, \"Worker spawn disabled under pytest (set FISHBRO_ALLOW_SPAWN_IN_TESTS=1 to override)\"","","    rp = db_path.expanduser().resolve()","    if str(rp).startswith(\"/tmp/\") and os.getenv(\"FISHBRO_ALLOW_TMP_DB\") != \"1\":","        return False, \"Refusing to spawn worker for /tmp db_path (set FISHBRO_ALLOW_TMP_DB=1 to override)\"","","    return True, \"ok\"","","","def validate_pidfile(pidfile: Path, expected_db_path: Path) -> tuple[bool, str]:","    \"\"\"Validate pidfile points to a live worker process with matching db_path.","","    Returns (is_valid, reason).","    If valid, the worker is considered alive and no new spawn needed.","    If invalid (stale or mismatched), caller should remove pidfile and spawn.","    \"\"\"","    if not pidfile.exists():","        return False, \"pidfile missing\"","","    try:","        pid = int(pidfile.read_text().strip())","    except (ValueError, OSError):","        return False, \"pidfile corrupted\"","","    # Check if process exists","    try:","        os.kill(pid, 0)","    except OSError:","        return False, \"process dead\"","","    # On Linux, read cmdline from /proc/{pid}/cmdline","    cmdline_path = Path(f\"/proc/{pid}/cmdline\")","    if cmdline_path.exists():","        try:","            cmdline_bytes = cmdline_path.read_bytes()","            # Split by null bytes, decode","            parts = [p.decode(\"utf-8\", errors=\"replace\") for p in cmdline_bytes.split(b\"\\x00\") if p]","            cmdline = \" \".join(parts)","        except Exception:","            # If we cannot read cmdline, treat as unverifiable but assume alive","            return True, \"process alive (cmdline unverifiable)\"","    else:","        # Fallback for non-Linux (or permission issues)","        # We'll assume it's okay but log warning","        return True, \"process alive (cmdline unverifiable)\"","","    # Verify cmdline contains worker_main and db_path","    if \"control.worker_main\" not in cmdline:","        return False, \"process is not a worker_main\"","    if str(expected_db_path) not in cmdline:","        return False, \"process db_path mismatch\"","","    return True, \"worker alive and matching\""]}
{"type":"file_footer","path":"src/control/worker_spawn_policy.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":57,"sha256":"9380e6cee44a8c92094a4673f6ab9e721784aed936ae5cf76b9184a9107d588d","total_lines":4,"chunk_count":1}
{"type":"file_chunk","path":"src/core/__init__.py","chunk_index":0,"line_start":1,"line_end":4,"content":["","\"\"\"Core modules for audit and artifact management.\"\"\"","",""]}
{"type":"file_footer","path":"src/core/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/core/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":207,"sha256":"6a3fa0ef17a7b15515eca6e51d797f29629d849bd34fb60edfa10f2207d6be5c","note":"skipped by policy"}
{"type":"file_skipped","path":"src/core/__pycache__/ast_identity.cpython-312.pyc","reason":"cache","bytes":20313,"sha256":"4ebf3b33552b01f48776b4c6a92fd07e4bd27580208545e19523f8dec6eec67a","note":"skipped by policy"}
{"type":"file_skipped","path":"src/core/__pycache__/oom_cost_model.cpython-312.pyc","reason":"cache","bytes":4568,"sha256":"3e6d1ba06f0a61258e9ba25a82717030a7edd7c385015ba04d4e470a2459ef85","note":"skipped by policy"}
{"type":"file_skipped","path":"src/core/__pycache__/oom_gate.cpython-312.pyc","reason":"cache","bytes":12658,"sha256":"1d6d753178b98baadf3758daf5c1f3deb88e0c568d415a835333787cd2ebb1c1","note":"skipped by policy"}
{"type":"file_skipped","path":"src/core/__pycache__/service_identity.cpython-312.pyc","reason":"cache","bytes":6859,"sha256":"7d8b5413aa3706c7e33fc7a212c791ad2db13b059b440a85ad45f0673a772366","note":"skipped by policy"}
{"type":"file_header","path":"src/core/action_risk.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":534,"sha256":"935698ce27e202c228b3d0f2649f8205fa3c66fb656f2e649bf1b8d020495c01","total_lines":25,"chunk_count":1}
{"type":"file_chunk","path":"src/core/action_risk.py","chunk_index":0,"line_start":1,"line_end":25,"content":["\"\"\"Action Risk Levels - 資料契約","","定義系統動作的風險等級，用於實盤安全鎖。","\"\"\"","","from enum import Enum","from dataclasses import dataclass","from typing import Optional","","","class RiskLevel(str, Enum):","    \"\"\"動作風險等級\"\"\"","    READ_ONLY = \"READ_ONLY\"","    RESEARCH_MUTATE = \"RESEARCH_MUTATE\"","    LIVE_EXECUTE = \"LIVE_EXECUTE\"","","","@dataclass(frozen=True)","class ActionPolicyDecision:","    \"\"\"政策決策結果\"\"\"","    allowed: bool","    reason: str","    risk: RiskLevel","    action: str","    season: Optional[str] = None"]}
{"type":"file_footer","path":"src/core/action_risk.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/artifact_reader.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9366,"sha256":"eacafa41445de920c0cd532ac4ed545e22543ce980cf9bb9d607c35026d761e7","total_lines":322,"chunk_count":2}
{"type":"file_chunk","path":"src/core/artifact_reader.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Artifact reader for governance evaluation and Viewer.","","Reads artifacts (manifest/metrics/winners/config_snapshot) from run directories.","Provides safe read functions that never raise exceptions (for Viewer use).","\"\"\"","","from __future__ import annotations","","import hashlib","import json","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Dict, Optional","","try:","    import yaml","    HAS_YAML = True","except ImportError:","    HAS_YAML = False","","","def read_manifest(run_dir: Path) -> Dict[str, Any]:","    \"\"\"","    Read manifest.json from run directory.","    ","    Args:","        run_dir: Path to run directory","        ","    Returns:","        Manifest dict (AuditSchema as dict)","        ","    Raises:","        FileNotFoundError: If manifest.json does not exist","        json.JSONDecodeError: If manifest.json is invalid JSON","    \"\"\"","    manifest_path = run_dir / \"manifest.json\"","    if not manifest_path.exists():","        raise FileNotFoundError(f\"manifest.json not found in {run_dir}\")","    ","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        return json.load(f)","","","def read_metrics(run_dir: Path) -> Dict[str, Any]:","    \"\"\"","    Read metrics.json from run directory.","    ","    Args:","        run_dir: Path to run directory","        ","    Returns:","        Metrics dict","        ","    Raises:","        FileNotFoundError: If metrics.json does not exist","        json.JSONDecodeError: If metrics.json is invalid JSON","    \"\"\"","    metrics_path = run_dir / \"metrics.json\"","    if not metrics_path.exists():","        raise FileNotFoundError(f\"metrics.json not found in {run_dir}\")","    ","    with metrics_path.open(\"r\", encoding=\"utf-8\") as f:","        return json.load(f)","","","def read_winners(run_dir: Path) -> Dict[str, Any]:","    \"\"\"","    Read winners.json from run directory.","    ","    Args:","        run_dir: Path to run directory","        ","    Returns:","        Winners dict with schema {\"topk\": [...], \"notes\": {...}}","        ","    Raises:","        FileNotFoundError: If winners.json does not exist","        json.JSONDecodeError: If winners.json is invalid JSON","    \"\"\"","    winners_path = run_dir / \"winners.json\"","    if not winners_path.exists():","        raise FileNotFoundError(f\"winners.json not found in {run_dir}\")","    ","    with winners_path.open(\"r\", encoding=\"utf-8\") as f:","        return json.load(f)","","","def read_config_snapshot(run_dir: Path) -> Dict[str, Any]:","    \"\"\"","    Read config_snapshot.json from run directory.","    ","    Args:","        run_dir: Path to run directory","        ","    Returns:","        Config snapshot dict","        ","    Raises:","        FileNotFoundError: If config_snapshot.json does not exist","        json.JSONDecodeError: If config_snapshot.json is invalid JSON","    \"\"\"","    config_path = run_dir / \"config_snapshot.json\"","    if not config_path.exists():","        raise FileNotFoundError(f\"config_snapshot.json not found in {run_dir}\")","    ","    with config_path.open(\"r\", encoding=\"utf-8\") as f:","        return json.load(f)","","","# ============================================================================","# Safe artifact reader (never raises) - for Viewer use","# ============================================================================","","@dataclass(frozen=True)","class ReadMeta:","    \"\"\"Metadata about the read operation.\"\"\"","    source_path: str  # Absolute path to source file","    sha256: str  # SHA256 hash of file content","    mtime_s: float  # Modification time in seconds since epoch","","","@dataclass(frozen=True)","class ReadResult:","    \"\"\"","    Result of reading an artifact file.","    ","    Contains raw data (dict/list/str) and metadata.","    Upper layer uses pydantic for validation.","    \"\"\"","    raw: Any  # dict/list/str - raw parsed data","    meta: ReadMeta","","","@dataclass(frozen=True)","class ReadError:","    \"\"\"Error information for failed read operations.\"\"\"","    error_code: str  # \"FILE_NOT_FOUND\", \"UNSUPPORTED_FORMAT\", \"YAML_NOT_AVAILABLE\", \"JSON_DECODE_ERROR\", \"IO_ERROR\"","    message: str","    source_path: str","","","@dataclass(frozen=True)","class SafeReadResult:","    \"\"\"","    Safe read result that never raises.","    ","    Either contains ReadResult (success) or ReadError (failure).","    \"\"\"","    result: Optional[ReadResult] = None","    error: Optional[ReadError] = None","    ","    @property","    def is_ok(self) -> bool:","        \"\"\"Check if read was successful.\"\"\"","        return self.result is not None and self.error is None","    ","    @property","    def is_error(self) -> bool:","        \"\"\"Check if read failed.\"\"\"","        return self.error is not None","","","def _compute_sha256(file_path: Path) -> str:","    \"\"\"Compute SHA256 hash of file content.\"\"\"","    sha256_hash = hashlib.sha256()","    with file_path.open(\"rb\") as f:","        for chunk in iter(lambda: f.read(4096), b\"\"):","            sha256_hash.update(chunk)","    return sha256_hash.hexdigest()","","","def read_artifact(file_path: Path | str) -> ReadResult:","    \"\"\"","    Read artifact file (JSON/YAML/MD) and return ReadResult.","    ","    Args:","        file_path: Path to artifact file","        ","    Returns:","        ReadResult with raw data and metadata","        ","    Raises:","        FileNotFoundError: If file does not exist","        ValueError: If file format is not supported","    \"\"\"","    path = Path(file_path).resolve()","    ","    if not path.exists():","        raise FileNotFoundError(f\"Artifact file not found: {path}\")","    ","    # Get metadata","    mtime_s = path.stat().st_mtime","    sha256 = _compute_sha256(path)","    ","    # Read based on extension","    suffix = path.suffix.lower()","    ","    if suffix == \".json\":","        with path.open(\"r\", encoding=\"utf-8\") as f:"]}
{"type":"file_chunk","path":"src/core/artifact_reader.py","chunk_index":1,"line_start":201,"line_end":322,"content":["            raw = json.load(f)","    elif suffix in (\".yaml\", \".yml\"):","        if not HAS_YAML:","            raise ValueError(f\"YAML support not available. Install pyyaml to read {path}\")","        with path.open(\"r\", encoding=\"utf-8\") as f:","            raw = yaml.safe_load(f)","    elif suffix == \".md\":","        with path.open(\"r\", encoding=\"utf-8\") as f:","            raw = f.read()  # Return as string for markdown","    else:","        raise ValueError(f\"Unsupported file format: {suffix}. Supported: .json, .yaml, .yml, .md\")","    ","    meta = ReadMeta(","        source_path=str(path),","        sha256=sha256,","        mtime_s=mtime_s,","    )","    ","    return ReadResult(raw=raw, meta=meta)","","","def try_read_artifact(file_path: Path | str) -> SafeReadResult:","    \"\"\"","    Safe version of read_artifact that never raises.","    ","    All Viewer code should use this function instead of read_artifact()","    to ensure no exceptions are thrown.","    ","    Args:","        file_path: Path to artifact file","        ","    Returns:","        SafeReadResult with either ReadResult (success) or ReadError (failure)","    \"\"\"","    path = Path(file_path).resolve()","    ","    # Check if file exists","    if not path.exists():","        return SafeReadResult(","            error=ReadError(","                error_code=\"FILE_NOT_FOUND\",","                message=f\"Artifact file not found: {path}\",","                source_path=str(path),","            )","        )","    ","    try:","        # Get metadata","        mtime_s = path.stat().st_mtime","        sha256 = _compute_sha256(path)","    except OSError as e:","        return SafeReadResult(","            error=ReadError(","                error_code=\"IO_ERROR\",","                message=f\"Failed to read file metadata: {e}\",","                source_path=str(path),","            )","        )","    ","    # Read based on extension","    suffix = path.suffix.lower()","    ","    try:","        if suffix == \".json\":","            with path.open(\"r\", encoding=\"utf-8\") as f:","                raw = json.load(f)","        elif suffix in (\".yaml\", \".yml\"):","            if not HAS_YAML:","                return SafeReadResult(","                    error=ReadError(","                        error_code=\"YAML_NOT_AVAILABLE\",","                        message=f\"YAML support not available. Install pyyaml to read {path}\",","                        source_path=str(path),","                    )","                )","            with path.open(\"r\", encoding=\"utf-8\") as f:","                raw = yaml.safe_load(f)","        elif suffix == \".md\":","            with path.open(\"r\", encoding=\"utf-8\") as f:","                raw = f.read()  # Return as string for markdown","        else:","            return SafeReadResult(","                error=ReadError(","                    error_code=\"UNSUPPORTED_FORMAT\",","                    message=f\"Unsupported file format: {suffix}. Supported: .json, .yaml, .yml, .md\",","                    source_path=str(path),","                )","            )","    except json.JSONDecodeError as e:","        return SafeReadResult(","            error=ReadError(","                error_code=\"JSON_DECODE_ERROR\",","                message=f\"JSON decode error: {e}\",","                source_path=str(path),","            )","        )","    except OSError as e:","        return SafeReadResult(","            error=ReadError(","                error_code=\"IO_ERROR\",","                message=f\"Failed to read file: {e}\",","                source_path=str(path),","            )","        )","    except Exception as e:","        return SafeReadResult(","            error=ReadError(","                error_code=\"UNKNOWN_ERROR\",","                message=f\"Unexpected error: {e}\",","                source_path=str(path),","            )","        )","    ","    meta = ReadMeta(","        source_path=str(path),","        sha256=sha256,","        mtime_s=mtime_s,","    )","    ","    return SafeReadResult(result=ReadResult(raw=raw, meta=meta))","",""]}
{"type":"file_footer","path":"src/core/artifact_reader.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/core/artifact_status.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12570,"sha256":"7dc8b2ea3fed5fe8880212ebd95a6b249b5b5132f170d81996b6df264198f2d7","total_lines":348,"chunk_count":2}
{"type":"file_chunk","path":"src/core/artifact_status.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Status determination for artifact validation.","","Defines OK/MISSING/INVALID/DIRTY states with human-readable error messages.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass","from enum import Enum","from typing import Optional","","from pydantic import ValidationError","","","class ArtifactStatus(str, Enum):","    \"\"\"Artifact validation status.\"\"\"","    OK = \"OK\"","    MISSING = \"MISSING\"  # File does not exist","    INVALID = \"INVALID\"  # Pydantic validation error","    DIRTY = \"DIRTY\"  # config_hash mismatch","","","@dataclass(frozen=True)","class ValidationResult:","    \"\"\"","    Result of artifact validation.","    ","    Contains status and human-readable error message.","    \"\"\"","    status: ArtifactStatus","    message: str = \"\"","    error_details: Optional[str] = None  # Detailed error for debugging","","","def _format_pydantic_error(e: ValidationError) -> str:","    \"\"\"Format Pydantic ValidationError into readable string with field paths.\"\"\"","    parts: list[str] = []","    for err in e.errors():","        loc = \".\".join(str(x) for x in err.get(\"loc\", []))","        msg = err.get(\"msg\", \"\")","        typ = err.get(\"type\", \"\")","        if loc:","            parts.append(f\"{loc}: {msg} ({typ})\")","        else:","            parts.append(f\"{msg} ({typ})\")","    return \"；\".join(parts) if parts else str(e)","","","def _extract_missing_field_names(e: ValidationError) -> list[str]:","    \"\"\"Extract missing field names from ValidationError.\"\"\"","    missing: set[str] = set()","    for err in e.errors():","        typ = str(err.get(\"type\", \"\")).lower()","        msg = str(err.get(\"msg\", \"\")).lower()","        if \"missing\" in typ or \"required\" in msg:","            loc = err.get(\"loc\", ())","            # loc 可能像 (\"rows\", 0, \"net_profit\") 或 (\"config_hash\",)","            if loc:","                leaf = str(loc[-1])","                # 避免 leaf 是 index","                if not leaf.isdigit():","                    missing.add(leaf)","            # 也把完整路徑收進來（可讀性更好）","            loc_str = \".\".join(str(x) for x in loc if not isinstance(x, int))","            if loc_str:","                missing.add(loc_str.split(\".\")[-1])  # leaf 再保險一次","    return sorted(missing)","","","def validate_manifest_status(","    file_path: str,","    manifest_data: Optional[dict] = None,","    expected_config_hash: Optional[str] = None,",") -> ValidationResult:","    \"\"\"","    Validate manifest.json status.","    ","    Args:","        file_path: Path to manifest.json","        manifest_data: Parsed manifest data (if available)","        expected_config_hash: Expected config_hash (for DIRTY check)","        ","    Returns:","        ValidationResult with status and message","    \"\"\"","    from pathlib import Path","    from core.schemas.manifest import RunManifest","    ","    path = Path(file_path)","    ","    # Check if file exists","    if not path.exists():","        return ValidationResult(","            status=ArtifactStatus.MISSING,","            message=f\"manifest.json 不存在: {file_path}\",","        )","    ","    # Try to parse with Pydantic","    if manifest_data is None:","        import json","        try:","            with path.open(\"r\", encoding=\"utf-8\") as f:","                manifest_data = json.load(f)","        except json.JSONDecodeError as e:","            return ValidationResult(","                status=ArtifactStatus.INVALID,","                message=f\"manifest.json JSON 格式錯誤: {e}\",","                error_details=str(e),","            )","    ","    try:","        manifest = RunManifest(**manifest_data)","    except Exception as e:","        # Extract missing field from Pydantic error","        error_msg = str(e)","        missing_fields = []","        if \"field required\" in error_msg.lower():","            # Try to extract field name from error","            import re","            matches = re.findall(r\"Field required.*?['\\\"]([^'\\\"]+)['\\\"]\", error_msg)","            if matches:","                missing_fields = matches","        ","        if missing_fields:","            msg = f\"manifest.json 缺少欄位: {', '.join(missing_fields)}\"","        else:","            msg = f\"manifest.json 驗證失敗: {error_msg}\"","        ","        return ValidationResult(","            status=ArtifactStatus.INVALID,","            message=msg,","            error_details=error_msg,","        )","    ","    # Check config_hash if expected is provided","    if expected_config_hash is not None and manifest.config_hash != expected_config_hash:","        return ValidationResult(","            status=ArtifactStatus.DIRTY,","            message=f\"manifest.config_hash={manifest.config_hash} 但預期值為 {expected_config_hash}\",","        )","    ","    # Phase 6.5: Check data_fingerprint_sha1 (mandatory)","    fingerprint_sha1 = getattr(manifest, 'data_fingerprint_sha1', None)","    if not fingerprint_sha1 or fingerprint_sha1 == \"\":","        return ValidationResult(","            status=ArtifactStatus.DIRTY,","            message=\"Missing Data Fingerprint — report is untrustworthy (data_fingerprint_sha1 is empty or missing)\",","        )","    ","    return ValidationResult(status=ArtifactStatus.OK, message=\"manifest.json 驗證通過\")","","","def validate_winners_v2_status(","    file_path: str,","    winners_data: Optional[dict] = None,","    expected_config_hash: Optional[str] = None,","    manifest_config_hash: Optional[str] = None,",") -> ValidationResult:","    \"\"\"","    Validate winners_v2.json status.","    ","    Args:","        file_path: Path to winners_v2.json","        winners_data: Parsed winners data (if available)","        expected_config_hash: Expected config_hash (for DIRTY check)","        manifest_config_hash: config_hash from manifest (for DIRTY check)","        ","    Returns:","        ValidationResult with status and message","    \"\"\"","    from pathlib import Path","    from core.schemas.winners_v2 import WinnersV2","    ","    path = Path(file_path)","    ","    # Check if file exists","    if not path.exists():","        return ValidationResult(","            status=ArtifactStatus.MISSING,","            message=f\"winners_v2.json 不存在: {file_path}\",","        )","    ","    # Try to parse with Pydantic","    if winners_data is None:","        import json","        try:","            with path.open(\"r\", encoding=\"utf-8\") as f:","                winners_data = json.load(f)","        except json.JSONDecodeError as e:","            return ValidationResult(","                status=ArtifactStatus.INVALID,","                message=f\"winners_v2.json JSON 格式錯誤: {e}\",","                error_details=str(e),","            )","    ","    try:","        winners = WinnersV2(**winners_data)","        ","        # Validate rows if present (Pydantic already validates required fields)"]}
{"type":"file_chunk","path":"src/core/artifact_status.py","chunk_index":1,"line_start":201,"line_end":348,"content":["        # Additional checks for None values (defensive)","        for idx, row in enumerate(winners.rows):","            if row.net_profit is None:","                return ValidationResult(","                    status=ArtifactStatus.INVALID,","                    message=f\"winners_v2.json 第 {idx} 行 net_profit 是必填欄位\",","                    error_details=f\"row[{idx}].net_profit is None\",","                )","            if row.max_drawdown is None:","                return ValidationResult(","                    status=ArtifactStatus.INVALID,","                    message=f\"winners_v2.json 第 {idx} 行 max_drawdown 是必填欄位\",","                    error_details=f\"row[{idx}].max_drawdown is None\",","                )","            if row.trades is None:","                return ValidationResult(","                    status=ArtifactStatus.INVALID,","                    message=f\"winners_v2.json 第 {idx} 行 trades 是必填欄位\",","                    error_details=f\"row[{idx}].trades is None\",","                )","    except ValidationError as e:","        missing_fields = _extract_missing_field_names(e)","        missing_txt = f\"缺少欄位: {', '.join(missing_fields)}；\" if missing_fields else \"\"","        error_details = str(e) + \"\\nmissing_fields=\" + \",\".join(missing_fields) if missing_fields else str(e)","        return ValidationResult(","            status=ArtifactStatus.INVALID,","            message=f\"winners_v2.json {missing_txt}schema 驗證失敗：{_format_pydantic_error(e)}\",","            error_details=error_details,","        )","    except Exception as e:","        # Fallback for non-Pydantic errors","        return ValidationResult(","            status=ArtifactStatus.INVALID,","            message=f\"winners_v2.json 驗證失敗: {e}\",","            error_details=str(e),","        )","    ","    # Check config_hash if expected/manifest is provided","    if expected_config_hash is not None:","        if winners.config_hash != expected_config_hash:","            return ValidationResult(","                status=ArtifactStatus.DIRTY,","                message=f\"winners_v2.config_hash={winners.config_hash} 但預期值為 {expected_config_hash}\",","            )","    ","    if manifest_config_hash is not None:","        if winners.config_hash != manifest_config_hash:","            return ValidationResult(","                status=ArtifactStatus.DIRTY,","                message=f\"winners_v2.config_hash={winners.config_hash} 但 manifest.config_hash={manifest_config_hash}\",","            )","    ","    return ValidationResult(status=ArtifactStatus.OK, message=\"winners_v2.json 驗證通過\")","","","def validate_governance_status(","    file_path: str,","    governance_data: Optional[dict] = None,","    expected_config_hash: Optional[str] = None,","    manifest_config_hash: Optional[str] = None,",") -> ValidationResult:","    \"\"\"","    Validate governance.json status.","    ","    Args:","        file_path: Path to governance.json","        governance_data: Parsed governance data (if available)","        expected_config_hash: Expected config_hash (for DIRTY check)","        manifest_config_hash: config_hash from manifest (for DIRTY check)","        ","    Returns:","        ValidationResult with status and message","    \"\"\"","    from pathlib import Path","    from core.schemas.governance import GovernanceReport","    ","    path = Path(file_path)","    ","    # Check if file exists","    if not path.exists():","        return ValidationResult(","            status=ArtifactStatus.MISSING,","            message=f\"governance.json 不存在: {file_path}\",","        )","    ","    # Try to parse with Pydantic","    if governance_data is None:","        import json","        try:","            with path.open(\"r\", encoding=\"utf-8\") as f:","                governance_data = json.load(f)","        except json.JSONDecodeError as e:","            return ValidationResult(","                status=ArtifactStatus.INVALID,","                message=f\"governance.json JSON 格式錯誤: {e}\",","                error_details=str(e),","            )","    ","    try:","        governance = GovernanceReport(**governance_data)","    except Exception as e:","        # Extract missing field from Pydantic error","        error_msg = str(e)","        missing_fields = []","        if \"field required\" in error_msg.lower():","            import re","            matches = re.findall(r\"Field required.*?['\\\"]([^'\\\"]+)['\\\"]\", error_msg)","            if matches:","                missing_fields = matches","        ","        if missing_fields:","            msg = f\"governance.json 缺少欄位: {', '.join(missing_fields)}\"","        else:","            msg = f\"governance.json 驗證失敗: {error_msg}\"","        ","        return ValidationResult(","            status=ArtifactStatus.INVALID,","            message=msg,","            error_details=error_msg,","        )","    ","    # Check config_hash if expected/manifest is provided","    if expected_config_hash is not None:","        if governance.config_hash != expected_config_hash:","            return ValidationResult(","                status=ArtifactStatus.DIRTY,","                message=f\"governance.config_hash={governance.config_hash} 但預期值為 {expected_config_hash}\",","            )","    ","    if manifest_config_hash is not None:","        if governance.config_hash != manifest_config_hash:","            return ValidationResult(","                status=ArtifactStatus.DIRTY,","                message=f\"governance.config_hash={governance.config_hash} 但 manifest.config_hash={manifest_config_hash}\",","            )","    ","    # Phase 6.5: Check data_fingerprint_sha1 in metadata (mandatory)","    metadata = governance_data.get(\"metadata\", {}) if governance_data else {}","    fingerprint_sha1 = metadata.get(\"data_fingerprint_sha1\", \"\")","    if not fingerprint_sha1 or fingerprint_sha1 == \"\":","        return ValidationResult(","            status=ArtifactStatus.DIRTY,","            message=\"Missing Data Fingerprint — report is untrustworthy (data_fingerprint_sha1 is empty or missing in metadata)\",","        )","    ","    return ValidationResult(status=ArtifactStatus.OK, message=\"governance.json 驗證通過\")","",""]}
{"type":"file_footer","path":"src/core/artifact_status.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/core/artifacts.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5451,"sha256":"cd6ae36b0773cfdd3a11039dfa36b098ceaa7fe522f6566fb1747c75c7b3685a","total_lines":156,"chunk_count":1}
{"type":"file_chunk","path":"src/core/artifacts.py","chunk_index":0,"line_start":1,"line_end":156,"content":["","\"\"\"Artifact writer for unified run output.","","Provides consistent artifact structure for all runs, with mandatory","subsample rate visibility.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Any, Dict","","from core.winners_builder import build_winners_v2","from core.winners_schema import is_winners_legacy, is_winners_v2","","","def _write_json(path: Path, obj: Any) -> None:","    \"\"\"","    Write object to JSON file with fixed format.","    ","    Uses sort_keys=True and fixed separators for reproducibility.","    ","    Args:","        path: Path to JSON file","        obj: Object to serialize","    \"\"\"","    path.write_text(","        json.dumps(obj, ensure_ascii=False, sort_keys=True, indent=2) + \"\\n\",","        encoding=\"utf-8\",","    )","","","def write_run_artifacts(","    run_dir: Path,","    manifest: Dict[str, Any],","    config_snapshot: Dict[str, Any],","    metrics: Dict[str, Any],","    winners: Dict[str, Any] | None = None,",") -> None:","    \"\"\"","    Write all standard artifacts for a run.","    ","    Creates the following files:","    - manifest.json: Full AuditSchema data","    - config_snapshot.json: Original/normalized config","    - metrics.json: Performance metrics","    - winners.json: Top-K results (fixed schema)","    - README.md: Human-readable summary","    - logs.txt: Execution logs (empty initially)","    ","    Args:","        run_dir: Run directory path (will be created if needed)","        manifest: Manifest data (AuditSchema as dict)","        config_snapshot: Configuration snapshot","        metrics: Performance metrics (must include param_subsample_rate visibility)","        winners: Optional winners dict. If None, uses empty schema.","            Must follow schema: {\"topk\": [...], \"notes\": {\"schema\": \"v1\", ...}}","    \"\"\"","    run_dir.mkdir(parents=True, exist_ok=True)","    ","    # Write manifest.json (full AuditSchema)","    _write_json(run_dir / \"manifest.json\", manifest)","    ","    # Write config_snapshot.json","    _write_json(run_dir / \"config_snapshot.json\", config_snapshot)","    ","    # Write metrics.json (must include param_subsample_rate visibility)","    _write_json(run_dir / \"metrics.json\", metrics)","    ","    # Write winners.json (always output v2 schema)","    if winners is None:","        winners = {\"topk\": [], \"notes\": {\"schema\": \"v1\"}}","    ","    # Auto-upgrade legacy winners to v2","    if is_winners_legacy(winners):","        # Convert legacy to v2","        legacy_topk = winners.get(\"topk\", [])","        run_id = manifest.get(\"run_id\", \"unknown\")","        stage_name = metrics.get(\"stage_name\", \"unknown\")","        ","        winners = build_winners_v2(","            stage_name=stage_name,","            run_id=run_id,","            manifest=manifest,","            config_snapshot=config_snapshot,","            legacy_topk=legacy_topk,","        )","    elif not is_winners_v2(winners):","        # Unknown format - try to upgrade anyway (defensive)","        legacy_topk = winners.get(\"topk\", [])","        if legacy_topk:","            run_id = manifest.get(\"run_id\", \"unknown\")","            stage_name = metrics.get(\"stage_name\", \"unknown\")","            ","            winners = build_winners_v2(","                stage_name=stage_name,","                run_id=run_id,","                manifest=manifest,","                config_snapshot=config_snapshot,","                legacy_topk=legacy_topk,","            )","        else:","            # Empty topk - create minimal v2 structure","            from core.winners_schema import build_winners_v2_dict","            winners = build_winners_v2_dict(","                stage_name=metrics.get(\"stage_name\", \"unknown\"),","                run_id=manifest.get(\"run_id\", \"unknown\"),","                topk=[],","            )","    ","    _write_json(run_dir / \"winners.json\", winners)","    ","    # Write README.md (human-readable summary)","    # Must prominently display param_subsample_rate","    readme_lines = [","        \"# FishBroWFS_V2 Run\",","        \"\",","        f\"- run_id: {manifest.get('run_id')}\",","        f\"- git_sha: {manifest.get('git_sha')}\",","        f\"- param_subsample_rate: {manifest.get('param_subsample_rate')}\",","        f\"- season: {manifest.get('season')}\",","        f\"- dataset_id: {manifest.get('dataset_id')}\",","        f\"- bars: {manifest.get('bars')}\",","        f\"- params_total: {manifest.get('params_total')}\",","        f\"- params_effective: {manifest.get('params_effective')}\",","        f\"- config_hash: {manifest.get('config_hash')}\",","    ]","    ","    # Add OOM gate information if present in metrics","    if \"oom_gate_action\" in metrics:","        readme_lines.extend([","            \"\",","            \"## OOM Gate\",","            \"\",","            f\"- action: {metrics.get('oom_gate_action')}\",","            f\"- reason: {metrics.get('oom_gate_reason')}\",","            f\"- mem_est_mb: {metrics.get('mem_est_mb', 0):.1f}\",","            f\"- mem_limit_mb: {metrics.get('mem_limit_mb', 0):.1f}\",","            f\"- ops_est: {metrics.get('ops_est', 0)}\",","        ])","        ","        # If auto-downsample occurred, show original and final","        if metrics.get(\"oom_gate_action\") == \"AUTO_DOWNSAMPLE\":","            readme_lines.extend([","                f\"- original_subsample: {metrics.get('oom_gate_original_subsample', 0)}\",","                f\"- final_subsample: {metrics.get('oom_gate_final_subsample', 0)}\",","            ])","    ","    readme = \"\\n\".join(readme_lines)","    (run_dir / \"README.md\").write_text(readme, encoding=\"utf-8\")","    ","    # Write logs.txt (empty initially)","    (run_dir / \"logs.txt\").write_text(\"\", encoding=\"utf-8\")","",""]}
{"type":"file_footer","path":"src/core/artifacts.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/ast_identity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":16991,"sha256":"a7368f674044cfe0366917ff438ff947a5083ab3fcbbe304268c40d819d9a7fb","total_lines":471,"chunk_count":3}
{"type":"file_chunk","path":"src/core/ast_identity.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"AST-based canonical identity for strategies.","","Implements content-addressed, deterministic StrategyID derived from strategy's","canonical AST (ast-c14n-v1). This replaces filesystem iteration order, Python","import order, list index/enumerate/incremental counters, filename or class name","as primary key.","","Key properties:","1. Deterministic: Same AST → same hash regardless of file location, import order","2. Content-addressed: Hash derived from canonical AST representation","3. Immutable: Strategy identity cannot change without changing its logic","4. Collision-resistant: SHA-256 provides sufficient collision resistance","5. No hash() usage: Uses hashlib.sha256 for deterministic hashing","","Algorithm (ast-c14n-v1):","1. Parse source code to AST","2. Canonicalize AST (normalize whitespace, sort dict keys, etc.)","3. Serialize to canonical string representation","4. Compute SHA-256 hash","5. Encode as hex string (StrategyID)","\"\"\"","","from __future__ import annotations","","import ast","import hashlib","import json","import textwrap","from typing import Any, Dict, List, Optional, Union","from pathlib import Path","import inspect","","","class ASTCanonicalizer:","    \"\"\"Canonical AST representation for deterministic hashing.\"\"\"","    ","    @staticmethod","    def canonicalize(node: ast.AST) -> Any:","        \"\"\"Convert AST node to canonical JSON-serializable representation.","        ","        Follows ast-c14n-v1 specification:","        1. Sort dictionary keys alphabetically","        2. Normalize numeric literals (float precision)","        3. Remove location information (lineno, col_offset)","        4. Handle special AST nodes consistently","        5. Preserve only semantically relevant information","        \"\"\"","        if isinstance(node, ast.Module):","            return {","                \"type\": \"Module\",","                \"body\": [ASTCanonicalizer.canonicalize(stmt) for stmt in node.body]","            }","        ","        elif isinstance(node, ast.FunctionDef):","            return {","                \"type\": \"FunctionDef\",","                \"name\": node.name,","                \"args\": ASTCanonicalizer.canonicalize(node.args),","                \"body\": [ASTCanonicalizer.canonicalize(stmt) for stmt in node.body],","                \"decorator_list\": [","                    ASTCanonicalizer.canonicalize(decorator) ","                    for decorator in node.decorator_list","                ],","                \"returns\": (","                    ASTCanonicalizer.canonicalize(node.returns) ","                    if node.returns else None","                )","            }","        ","        elif isinstance(node, ast.ClassDef):","            return {","                \"type\": \"ClassDef\",","                \"name\": node.name,","                \"bases\": [ASTCanonicalizer.canonicalize(base) for base in node.bases],","                \"keywords\": [","                    ASTCanonicalizer.canonicalize(keyword) ","                    for keyword in node.keywords","                ],","                \"body\": [ASTCanonicalizer.canonicalize(stmt) for stmt in node.body],","                \"decorator_list\": [","                    ASTCanonicalizer.canonicalize(decorator) ","                    for decorator in node.decorator_list","                ]","            }","        ","        elif isinstance(node, ast.arguments):","            return {","                \"type\": \"arguments\",","                \"args\": [ASTCanonicalizer.canonicalize(arg) for arg in node.args],","                \"defaults\": [","                    ASTCanonicalizer.canonicalize(default) ","                    for default in node.defaults","                ],","                \"vararg\": (","                    ASTCanonicalizer.canonicalize(node.vararg) ","                    if node.vararg else None","                ),","                \"kwarg\": (","                    ASTCanonicalizer.canonicalize(node.kwarg) ","                    if node.kwarg else None","                )","            }","        ","        elif isinstance(node, ast.arg):","            return {","                \"type\": \"arg\",","                \"arg\": node.arg,","                \"annotation\": (","                    ASTCanonicalizer.canonicalize(node.annotation) ","                    if node.annotation else None","                )","            }","        ","        elif isinstance(node, ast.Name):","            return {","                \"type\": \"Name\",","                \"id\": node.id,","                \"ctx\": node.ctx.__class__.__name__","            }","        ","        elif isinstance(node, ast.Attribute):","            return {","                \"type\": \"Attribute\",","                \"value\": ASTCanonicalizer.canonicalize(node.value),","                \"attr\": node.attr,","                \"ctx\": node.ctx.__class__.__name__","            }","        ","        elif isinstance(node, ast.Constant):","            value = node.value","            # Normalize numeric values","            if isinstance(value, float):","                # Use repr to preserve precision but normalize -0.0","                value = float(repr(value))","            elif isinstance(value, complex):","                value = complex(repr(value))","            return {","                \"type\": \"Constant\",","                \"value\": value,","                \"kind\": getattr(node, 'kind', None)","            }","        ","        elif isinstance(node, ast.Dict):","            # Sort dictionary keys for determinism","            keys = [ASTCanonicalizer.canonicalize(k) for k in node.keys]","            values = [ASTCanonicalizer.canonicalize(v) for v in node.values]","            ","            # Create list of key-value pairs for sorting","            pairs = list(zip(keys, values))","            # Sort by key representation","            pairs.sort(key=lambda x: json.dumps(x[0], sort_keys=True))","            ","            sorted_keys = [k for k, _ in pairs]","            sorted_values = [v for _, v in pairs]","            ","            return {","                \"type\": \"Dict\",","                \"keys\": sorted_keys,","                \"values\": sorted_values","            }","        ","        elif isinstance(node, ast.List):","            return {","                \"type\": \"List\",","                \"elts\": [ASTCanonicalizer.canonicalize(elt) for elt in node.elts],","                \"ctx\": node.ctx.__class__.__name__","            }","        ","        elif isinstance(node, ast.Tuple):","            return {","                \"type\": \"Tuple\",","                \"elts\": [ASTCanonicalizer.canonicalize(elt) for elt in node.elts],","                \"ctx\": node.ctx.__class__.__name__","            }","        ","        elif isinstance(node, ast.Set):","            # Sets need special handling for determinism","            elts = [ASTCanonicalizer.canonicalize(elt) for elt in node.elts]","            # Sort by JSON representation","            elts.sort(key=lambda x: json.dumps(x, sort_keys=True))","            return {","                \"type\": \"Set\",","                \"elts\": elts","            }","        ","        elif isinstance(node, ast.Call):","            # Sort keywords by argument name for determinism","            keywords = [","                {","                    \"arg\": kw.arg,","                    \"value\": ASTCanonicalizer.canonicalize(kw.value)","                }","                for kw in node.keywords","            ]","            keywords.sort(key=lambda x: x[\"arg\"] if x[\"arg\"] else \"\")","            ","            return {","                \"type\": \"Call\",","                \"func\": ASTCanonicalizer.canonicalize(node.func),","                \"args\": [ASTCanonicalizer.canonicalize(arg) for arg in node.args],"]}
