{"type":"meta","schema_version":2,"run_id":"20251227_191803Z","repo_root":"/home/fishbro/FishBroWFS_V2","snapshot_root":"SNAPSHOT","part":7,"parts":10,"created_at":"2025-12-27T19:18:04Z","generator":{"name":"dump_context","version":"vNext-balanced"},"policies":{"max_parts":10,"target_bytes_per_part":500000,"chunk_max_lines":200,"chunk_max_bytes":120000,"outputs_max_bytes_for_content":2000000,"outputs_policy":"manifest_only_for_big_files","forbid_file_truncated":true,"balance_parts":true,"estimated_total_bytes":3707501,"balance_buffer_ratio":1.08}}
{"type":"file_chunk","path":"tests/test_research_console_filters.py","chunk_index":1,"line_start":201,"line_end":346,"content":["    # Search in note field","    rows_with_notes = [","        {\"run_id\": \"run1\", \"symbol\": \"AAPL\", \"strategy_id\": \"s1\", \"decision\": \"KEEP\", \"note\": \"good results\"},","        {\"run_id\": \"run2\", \"symbol\": \"GOOG\", \"strategy_id\": \"s2\", \"decision\": \"DROP\", \"note\": \"bad performance\"},","    ]","    result = apply_filters(rows_with_notes, text=\"good\", symbol=None, strategy_id=None, decision=None)","    assert len(result) == 1","    assert result[0][\"run_id\"] == \"run1\"","","","def test_apply_filters_symbol_filter():","    \"\"\"Test symbol filter.\"\"\"","    rows = [","        {\"run_id\": \"run1\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run2\", \"symbol\": \"GOOG\", \"strategy_id\": \"strategy2\", \"decision\": \"DROP\"},","        {\"run_id\": \"run3\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy3\", \"decision\": \"ARCHIVE\"},","    ]","    ","    result = apply_filters(rows, text=None, symbol=\"AAPL\", strategy_id=None, decision=None)","    assert len(result) == 2","    assert all(row[\"symbol\"] == \"AAPL\" for row in result)","","","def test_apply_filters_strategy_filter():","    \"\"\"Test strategy filter.\"\"\"","    rows = [","        {\"run_id\": \"run1\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run2\", \"symbol\": \"GOOG\", \"strategy_id\": \"strategy2\", \"decision\": \"DROP\"},","        {\"run_id\": \"run3\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"ARCHIVE\"},","    ]","    ","    result = apply_filters(rows, text=None, symbol=None, strategy_id=\"strategy1\", decision=None)","    assert len(result) == 2","    assert all(row[\"strategy_id\"] == \"strategy1\" for row in result)","","","def test_apply_filters_decision_filter():","    \"\"\"Test decision filter.\"\"\"","    rows = [","        {\"run_id\": \"run1\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run2\", \"symbol\": \"GOOG\", \"strategy_id\": \"strategy2\", \"decision\": \"DROP\"},","        {\"run_id\": \"run3\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run4\", \"symbol\": \"MSFT\", \"strategy_id\": \"strategy3\", \"decision\": \"ARCHIVE\"},","    ]","    ","    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=\"KEEP\")","    assert len(result) == 2","    assert all(row[\"decision\"] == \"KEEP\" for row in result)","    ","    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=\"DROP\")","    assert len(result) == 1","    assert result[0][\"decision\"] == \"DROP\"","    ","    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=\"ARCHIVE\")","    assert len(result) == 1","    assert result[0][\"decision\"] == \"ARCHIVE\"","","","def test_apply_filters_combined_filters():","    \"\"\"Test multiple filters combined.\"\"\"","    rows = [","        {\"run_id\": \"run_aapl_001\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run_aapl_002\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy2\", \"decision\": \"DROP\"},","        {\"run_id\": \"run_goog_001\", \"symbol\": \"GOOG\", \"strategy_id\": \"strategy1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run_goog_002\", \"symbol\": \"GOOG\", \"strategy_id\": \"strategy2\", \"decision\": \"ARCHIVE\"},","    ]","    ","    # Symbol + Decision filter","    result = apply_filters(","        rows, ","        text=None, ","        symbol=\"AAPL\", ","        strategy_id=None, ","        decision=\"KEEP\"","    )","    assert len(result) == 1","    assert result[0][\"symbol\"] == \"AAPL\"","    assert result[0][\"decision\"] == \"KEEP\"","    ","    # Text + Strategy filter","    result = apply_filters(","        rows,","        text=\"goog\",","        symbol=None,","        strategy_id=\"strategy1\",","        decision=None","    )","    assert len(result) == 1","    assert \"goog\" in result[0][\"run_id\"].lower()","    assert result[0][\"strategy_id\"] == \"strategy1\"","    ","    # All three filters combined","    result = apply_filters(","        rows,","        text=\"aapl\",","        symbol=\"AAPL\",","        strategy_id=\"strategy1\",","        decision=\"KEEP\"","    )","    assert len(result) == 1","    assert result[0][\"run_id\"] == \"run_aapl_001\"","","","def test_apply_filters_missing_fields():","    \"\"\"Test with rows missing some fields.\"\"\"","    rows = [","        {\"run_id\": \"run1\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run2\", \"symbol\": None, \"strategy_id\": \"strategy2\", \"decision\": \"DROP\"},","        {\"run_id\": \"run3\", \"symbol\": \"AAPL\", \"strategy_id\": None, \"decision\": \"ARCHIVE\"},","        {\"run_id\": \"run4\", \"symbol\": \"GOOG\", \"strategy_id\": \"strategy1\", \"decision\": None},","    ]","    ","    # Filter by symbol (should exclude rows with None symbol)","    result = apply_filters(rows, text=None, symbol=\"AAPL\", strategy_id=None, decision=None)","    assert len(result) == 2","    assert all(row[\"symbol\"] == \"AAPL\" for row in result)","    ","    # Filter by strategy (should exclude rows with None strategy_id)","    result = apply_filters(rows, text=None, symbol=None, strategy_id=\"strategy1\", decision=None)","    assert len(result) == 2","    assert all(row[\"strategy_id\"] == \"strategy1\" for row in result)","    ","    # Filter by decision (should exclude rows with None decision)","    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=\"KEEP\")","    assert len(result) == 1","    assert result[0][\"decision\"] == \"KEEP\"","","","def test_apply_filters_deterministic():","    \"\"\"Test that filters are deterministic (same input = same output).\"\"\"","    rows = [","        {\"run_id\": \"run1\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run2\", \"symbol\": \"GOOG\", \"strategy_id\": \"strategy2\", \"decision\": \"DROP\"},","        {\"run_id\": \"run3\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"ARCHIVE\"},","    ]","    ","    # Run filter multiple times","    result1 = apply_filters(rows, text=\"aapl\", symbol=None, strategy_id=None, decision=\"KEEP\")","    result2 = apply_filters(rows, text=\"aapl\", symbol=None, strategy_id=None, decision=\"KEEP\")","    result3 = apply_filters(rows, text=\"aapl\", symbol=None, strategy_id=None, decision=\"KEEP\")","    ","    assert result1 == result2 == result3","    assert len(result1) == 1","    assert result1[0][\"run_id\"] == \"run1\"","",""]}
{"type":"file_footer","path":"tests/test_research_console_filters.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_research_decision.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3709,"sha256":"a52f40d8564f28554f0b4024f6866290515010956a1ee734f0ae635e816028c9","total_lines":116,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_research_decision.py","chunk_index":0,"line_start":1,"line_end":116,"content":["","\"\"\"Tests for research decision module.\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","","import pytest","","from research.decision import append_decision, load_decisions","","","def test_append_decision_new(tmp_path: Path) -> None:","    \"\"\"Test appending a new decision.\"\"\"","    out_dir = tmp_path / \"research\"","    ","    log_path = append_decision(out_dir, \"test-run-123\", \"KEEP\", \"Good results\")","    ","    # Verify log file exists","    assert log_path.exists()","    ","    # Verify log content (JSONL)","    with open(log_path, \"r\", encoding=\"utf-8\") as f:","        lines = [line.strip() for line in f if line.strip()]","        assert len(lines) == 1","        entry = json.loads(lines[0])","        assert entry[\"run_id\"] == \"test-run-123\"","        assert entry[\"decision\"] == \"KEEP\"","        assert entry[\"note\"] == \"Good results\"","        assert \"decided_at\" in entry","","","def test_append_decision_multiple(tmp_path: Path) -> None:","    \"\"\"Test appending multiple decisions (same run_id allowed).\"\"\"","    out_dir = tmp_path / \"research\"","    ","    # Append first decision","    log_path = append_decision(out_dir, \"test-run-123\", \"KEEP\", \"First decision\")","    ","    # Append second decision (same run_id, different decision)","    append_decision(out_dir, \"test-run-123\", \"DROP\", \"Changed mind\")","    ","    # Verify log has 2 lines","    with open(log_path, \"r\", encoding=\"utf-8\") as f:","        lines = [line.strip() for line in f if line.strip()]","        assert len(lines) == 2","    ","    # Verify both entries exist","    entries = []","    with open(log_path, \"r\", encoding=\"utf-8\") as f:","        for line in f:","            line = line.strip()","            if line:","                entries.append(json.loads(line))","    ","    assert len(entries) == 2","    assert entries[0][\"decision\"] == \"KEEP\"","    assert entries[1][\"decision\"] == \"DROP\"","    assert entries[1][\"run_id\"] == \"test-run-123\"","","","def test_load_decisions_empty(tmp_path: Path) -> None:","    \"\"\"Test loading decisions when log doesn't exist.\"\"\"","    out_dir = tmp_path / \"research\"","    ","    decisions = load_decisions(out_dir)","    assert decisions == []","","","def test_load_decisions_multiple(tmp_path: Path) -> None:","    \"\"\"Test loading multiple decisions.\"\"\"","    out_dir = tmp_path / \"research\"","    ","    # Append multiple decisions","    append_decision(out_dir, \"run-1\", \"KEEP\", \"Note 1\")","    append_decision(out_dir, \"run-2\", \"DROP\", \"Note 2\")","    append_decision(out_dir, \"run-3\", \"ARCHIVE\", \"Note 3\")","    ","    # Load decisions","    decisions = load_decisions(out_dir)","    ","    assert len(decisions) == 3","    ","    # Verify all decisions are present","    run_ids = {d[\"run_id\"] for d in decisions}","    assert run_ids == {\"run-1\", \"run-2\", \"run-3\"}","    ","    # Verify decisions","    decision_map = {d[\"run_id\"]: d[\"decision\"] for d in decisions}","    assert decision_map[\"run-1\"] == \"KEEP\"","    assert decision_map[\"run-2\"] == \"DROP\"","    assert decision_map[\"run-3\"] == \"ARCHIVE\"","","","def test_load_decisions_same_run_multiple_times(tmp_path: Path) -> None:","    \"\"\"Test loading decisions when same run_id appears multiple times.\"\"\"","    out_dir = tmp_path / \"research\"","    ","    # Append same run_id multiple times","    append_decision(out_dir, \"run-1\", \"KEEP\", \"First\")","    append_decision(out_dir, \"run-1\", \"DROP\", \"Second\")","    append_decision(out_dir, \"run-1\", \"ARCHIVE\", \"Third\")","    ","    # Load decisions - should return all entries","    decisions = load_decisions(out_dir)","    ","    assert len(decisions) == 3","    # All should have same run_id","    assert all(d[\"run_id\"] == \"run-1\" for d in decisions)","    # Decisions should be in order","    assert decisions[0][\"decision\"] == \"KEEP\"","    assert decisions[1][\"decision\"] == \"DROP\"","    assert decisions[2][\"decision\"] == \"ARCHIVE\"","",""]}
{"type":"file_footer","path":"tests/test_research_decision.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_research_extract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6366,"sha256":"e6c9ef6edb63a64d996268429b67c9c35351a9a19e77912771194e985fa9c7d4","total_lines":208,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_research_extract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Tests for research extract module.\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","","import pytest","","from research.extract import extract_canonical_metrics, ExtractionError","from research.metrics import CanonicalMetrics","","","def test_extract_canonical_metrics_success(tmp_path: Path) -> None:","    \"\"\"Test successful extraction of canonical metrics.\"\"\"","    run_dir = tmp_path / \"run\"","    run_dir.mkdir()","    ","    # Create manifest.json","    manifest = {","        \"run_id\": \"test-run-123\",","        \"bars\": 1000,","        \"created_at\": \"2025-01-01T00:00:00Z\",","    }","    with open(run_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f)","    ","    # Create metrics.json","    metrics_data = {","        \"stage_name\": \"stage2_confirm\",","    }","    with open(run_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(metrics_data, f)","    ","    # Create winners.json with topk","    winners = {","        \"schema\": \"v2\",","        \"stage_name\": \"stage2_confirm\",","        \"topk\": [","            {","                \"candidate_id\": \"test:1\",","                \"strategy_id\": \"donchian_atr\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe\": \"60m\",","                \"metrics\": {","                    \"net_profit\": 100.0,","                    \"max_dd\": -50.0,","                    \"trades\": 10,","                },","                \"score\": 100.0,","            },","            {","                \"candidate_id\": \"test:2\",","                \"strategy_id\": \"donchian_atr\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe\": \"60m\",","                \"metrics\": {","                    \"net_profit\": 50.0,","                    \"max_dd\": -20.0,","                    \"trades\": 5,","                },","                \"score\": 50.0,","            },","        ],","    }","    with open(run_dir / \"winners.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(winners, f)","    ","    # Extract metrics","    metrics = extract_canonical_metrics(run_dir)","    ","    # Verify","    assert metrics.run_id == \"test-run-123\"","    assert metrics.bars == 1000","    assert metrics.trades == 15  # 10 + 5","    assert metrics.net_profit == 150.0  # 100 + 50","    assert metrics.max_drawdown == 50.0  # abs(-50)","    assert metrics.start_date == \"2025-01-01T00:00:00Z\"","    assert metrics.strategy_id == \"donchian_atr\"","    assert metrics.symbol == \"CME.MNQ\"","    assert metrics.timeframe_min == 60","    assert metrics.score_net_mdd == 150.0 / 50.0  # net_profit / max_drawdown","    assert metrics.score_final > 0  # score_net_mdd * (trades ** 0.25)","","","def test_extract_canonical_metrics_missing_artifacts(tmp_path: Path) -> None:","    \"\"\"Test extraction fails when no artifacts exist.\"\"\"","    run_dir = tmp_path / \"run\"","    run_dir.mkdir()","    ","    # No artifacts","    with pytest.raises(ExtractionError, match=\"No artifacts found\"):","        extract_canonical_metrics(run_dir)","","","def test_extract_canonical_metrics_missing_run_id(tmp_path: Path) -> None:","    \"\"\"Test extraction fails when run_id is missing.\"\"\"","    run_dir = tmp_path / \"run\"","    run_dir.mkdir()","    ","    # Create manifest without run_id","    with open(run_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({\"bars\": 100}, f)","    with open(run_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    # Should raise ExtractionError","    with pytest.raises(ExtractionError, match=\"Missing 'run_id'\"):","        extract_canonical_metrics(run_dir)","","","def test_extract_canonical_metrics_missing_bars(tmp_path: Path) -> None:","    \"\"\"Test extraction fails when bars is missing.\"\"\"","    run_dir = tmp_path / \"run\"","    run_dir.mkdir()","    ","    # Create manifest without bars","    with open(run_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({\"run_id\": \"test\"}, f)","    with open(run_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    # Should raise ExtractionError","    with pytest.raises(ExtractionError, match=\"Missing 'bars'\"):","        extract_canonical_metrics(run_dir)","","","def test_extract_canonical_metrics_zero_drawdown_with_profit(tmp_path: Path) -> None:","    \"\"\"Test extraction raises when max_drawdown is 0 but net_profit is non-zero.\"\"\"","    run_dir = tmp_path / \"run\"","    run_dir.mkdir()","    ","    manifest = {","        \"run_id\": \"test-run\",","        \"bars\": 1000,","        \"created_at\": \"2025-01-01T00:00:00Z\",","    }","    with open(run_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f)","    ","    with open(run_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    winners = {","        \"schema\": \"v2\",","        \"topk\": [","            {","                \"candidate_id\": \"test:1\",","                \"metrics\": {","                    \"net_profit\": 100.0,","                    \"max_dd\": 0.0,  # Zero drawdown","                    \"trades\": 10,","                },","            },","        ],","    }","    with open(run_dir / \"winners.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(winners, f)","    ","    # Should raise ExtractionError","    with pytest.raises(ExtractionError, match=\"cannot calculate score_net_mdd\"):","        extract_canonical_metrics(run_dir)","","","def test_extract_canonical_metrics_no_trades(tmp_path: Path) -> None:","    \"\"\"Test extraction with no trades.\"\"\"","    run_dir = tmp_path / \"run\"","    run_dir.mkdir()","    ","    manifest = {","        \"run_id\": \"test-run-no-trades\",","        \"bars\": 1000,","        \"created_at\": \"2025-01-01T00:00:00Z\",","    }","    with open(run_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f)","    ","    with open(run_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    winners = {","        \"schema\": \"v2\",","        \"topk\": [","            {","                \"candidate_id\": \"test:1\",","                \"metrics\": {","                    \"net_profit\": 0.0,","                    \"max_dd\": 0.0,","                    \"trades\": 0,","                },","            },","        ],","    }","    with open(run_dir / \"winners.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(winners, f)","    ","    # Extract metrics","    metrics = extract_canonical_metrics(run_dir)","    "]}
{"type":"file_chunk","path":"tests/test_research_extract.py","chunk_index":1,"line_start":201,"line_end":208,"content":["    # Verify zero metrics","    assert metrics.trades == 0","    assert metrics.net_profit == 0.0","    assert metrics.max_drawdown == 0.0","    assert metrics.score_net_mdd == 0.0","    assert metrics.score_final == 0.0","",""]}
{"type":"file_footer","path":"tests/test_research_extract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_research_registry.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5304,"sha256":"c7a465af2c21df44166467dec0e5659b693c0d7b73a9a8f2adafefe31449b1aa","total_lines":181,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_research_registry.py","chunk_index":0,"line_start":1,"line_end":181,"content":["","\"\"\"Tests for research registry module.\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","","import pytest","","from research.registry import build_research_index","","","def test_build_research_index_empty(tmp_path: Path) -> None:","    \"\"\"Test building index with empty outputs.\"\"\"","    outputs_root = tmp_path / \"outputs\"","    outputs_root.mkdir()","    out_dir = tmp_path / \"research\"","    ","    index_path = build_research_index(outputs_root, out_dir)","    ","    # Verify files created","    assert index_path.exists()","    assert (out_dir / \"canonical_results.json\").exists()","    ","    # Verify content","    with open(index_path, \"r\", encoding=\"utf-8\") as f:","        index_data = json.load(f)","    ","    assert index_data[\"total_runs\"] == 0","    assert index_data[\"entries\"] == []","","","def test_build_research_index_with_runs(tmp_path: Path) -> None:","    \"\"\"Test building index with multiple runs, verify sorting.\"\"\"","    outputs_root = tmp_path / \"outputs\"","    ","    # Create two runs with different scores","    run1_dir = outputs_root / \"seasons\" / \"2026Q1\" / \"runs\" / \"run-1\"","    run1_dir.mkdir(parents=True)","    ","    run2_dir = outputs_root / \"seasons\" / \"2026Q1\" / \"runs\" / \"run-2\"","    run2_dir.mkdir(parents=True)","    ","    # Run 1: Higher score_final","    manifest1 = {","        \"run_id\": \"run-1\",","        \"bars\": 1000,","        \"created_at\": \"2025-01-01T00:00:00Z\",","    }","    with open(run1_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(manifest1, f)","    ","    with open(run1_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    winners1 = {","        \"schema\": \"v2\",","        \"topk\": [","            {","                \"candidate_id\": \"test:1\",","                \"metrics\": {","                    \"net_profit\": 200.0,","                    \"max_dd\": -50.0,","                    \"trades\": 20,  # Higher trades -> higher score_final","                },","            },","        ],","    }","    with open(run1_dir / \"winners.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(winners1, f)","    ","    # Run 2: Lower score_final","    manifest2 = {","        \"run_id\": \"run-2\",","        \"bars\": 1000,","        \"created_at\": \"2025-01-01T00:00:00Z\",","    }","    with open(run2_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(manifest2, f)","    ","    with open(run2_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    winners2 = {","        \"schema\": \"v2\",","        \"topk\": [","            {","                \"candidate_id\": \"test:2\",","                \"metrics\": {","                    \"net_profit\": 100.0,","                    \"max_dd\": -50.0,","                    \"trades\": 10,  # Lower trades -> lower score_final","                },","            },","        ],","    }","    with open(run2_dir / \"winners.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(winners2, f)","    ","    # Build index","    out_dir = tmp_path / \"research\"","    index_path = build_research_index(outputs_root, out_dir)","    ","    # Verify files created","    assert index_path.exists()","    canonical_path = out_dir / \"canonical_results.json\"","    assert canonical_path.exists()","    ","    # Verify canonical_results.json","    with open(canonical_path, \"r\", encoding=\"utf-8\") as f:","        canonical_data = json.load(f)","    ","    assert len(canonical_data) == 2","    ","    # Verify research_index.json is sorted (score_final desc)","    with open(index_path, \"r\", encoding=\"utf-8\") as f:","        index_data = json.load(f)","    ","    assert index_data[\"total_runs\"] == 2","    entries = index_data[\"entries\"]","    assert len(entries) == 2","    ","    # Verify sorting: run-1 should be first (higher score_final)","    assert entries[0][\"run_id\"] == \"run-1\"","    assert entries[1][\"run_id\"] == \"run-2\"","    assert entries[0][\"score_final\"] > entries[1][\"score_final\"]","","","def test_build_research_index_preserves_decisions(tmp_path: Path) -> None:","    \"\"\"Test that building index preserves decisions from decisions.log.\"\"\"","    outputs_root = tmp_path / \"outputs\"","    out_dir = tmp_path / \"research\"","    out_dir.mkdir()","    ","    # Create a run","    run_dir = outputs_root / \"seasons\" / \"2026Q1\" / \"runs\" / \"run-1\"","    run_dir.mkdir(parents=True)","    ","    manifest = {","        \"run_id\": \"run-1\",","        \"bars\": 1000,","        \"created_at\": \"2025-01-01T00:00:00Z\",","    }","    with open(run_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f)","    ","    with open(run_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    winners = {","        \"schema\": \"v2\",","        \"topk\": [","            {","                \"candidate_id\": \"test:1\",","                \"metrics\": {","                    \"net_profit\": 100.0,","                    \"max_dd\": -50.0,","                    \"trades\": 10,","                },","            },","        ],","    }","    with open(run_dir / \"winners.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(winners, f)","    ","    # Add a decision","    from research.decision import append_decision","    ","    append_decision(out_dir, \"run-1\", \"KEEP\", \"Good results\")","    ","    # Build index","    index_path = build_research_index(outputs_root, out_dir)","    ","    # Verify decision is preserved","    with open(index_path, \"r\", encoding=\"utf-8\") as f:","        index_data = json.load(f)","    ","    assert index_data[\"entries\"][0][\"decision\"] == \"KEEP\"","",""]}
{"type":"file_footer","path":"tests/test_research_registry.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_runner_adapter_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4956,"sha256":"288d9ea08e710f3262c1a23c00518877745ce85e37e5a14e65804cf457b9db08","total_lines":148,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_runner_adapter_contract.py","chunk_index":0,"line_start":1,"line_end":148,"content":["","\"\"\"Contract tests for runner adapter.","","Tests verify:","1. Adapter returns data only (no file I/O)","2. Winners schema is stable","3. Metrics structure is consistent","\"\"\"","","from __future__ import annotations","","import tempfile","from pathlib import Path","","import numpy as np","import pytest","","from pipeline.runner_adapter import run_stage_job","","","def test_runner_adapter_returns_no_files_written():","    \"\"\"Test that adapter does not write any files.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Count files before","        files_before = list(tmp_path.rglob(\"*\"))","        file_count_before = len([f for f in files_before if f.is_file()])","        ","        # Run adapter","        cfg = {","            \"stage_name\": \"stage0_coarse\",","            \"param_subsample_rate\": 0.1,","            \"topk\": 10,","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"params_total\": 100,","            \"proxy_name\": \"ma_proxy_v0\",","        }","        ","        result = run_stage_job(cfg)","        ","        # Count files after","        files_after = list(tmp_path.rglob(\"*\"))","        file_count_after = len([f for f in files_after if f.is_file()])","        ","        # Verify no new files were created","        assert file_count_after == file_count_before, (","            \"Adapter should not write files, but new files were created\"","        )","        ","        # Verify result structure","        assert \"metrics\" in result","        assert \"winners\" in result","","","def test_winners_schema_is_stable():","    \"\"\"Test that winners schema is stable across all stages.\"\"\"","    test_cases = [","        {","            \"stage_name\": \"stage0_coarse\",","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"params_total\": 100,","            \"topk\": 10,","        },","        {","            \"stage_name\": \"stage1_topk\",","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"params_total\": 100,","            \"topk\": 5,","            \"commission\": 0.0,","            \"slip\": 0.0,","        },","        {","            \"stage_name\": \"stage2_confirm\",","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"params_total\": 100,","            \"commission\": 0.0,","            \"slip\": 0.0,","        },","    ]","    ","    for cfg in test_cases:","        cfg[\"param_subsample_rate\"] = 1.0  # Use full for simplicity","        ","        result = run_stage_job(cfg)","        ","        # Verify winners schema","        winners = result.get(\"winners\", {})","        assert \"topk\" in winners, f\"Missing 'topk' in winners for {cfg['stage_name']}\"","        assert \"notes\" in winners, f\"Missing 'notes' in winners for {cfg['stage_name']}\"","        assert isinstance(winners[\"topk\"], list)","        assert isinstance(winners[\"notes\"], dict)","        assert winners[\"notes\"].get(\"schema\") == \"v1\"","","","def test_metrics_structure_is_consistent():","    \"\"\"Test that metrics structure is consistent across stages.\"\"\"","    test_cases = [","        {","            \"stage_name\": \"stage0_coarse\",","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"params_total\": 100,","            \"topk\": 10,","        },","        {","            \"stage_name\": \"stage1_topk\",","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"params_total\": 100,","            \"topk\": 5,","            \"commission\": 0.0,","            \"slip\": 0.0,","        },","    ]","    ","    required_fields = [\"params_total\", \"params_effective\", \"bars\", \"stage_name\"]","    ","    for cfg in test_cases:","        cfg[\"param_subsample_rate\"] = 0.5","        ","        result = run_stage_job(cfg)","        ","        metrics = result.get(\"metrics\", {})","        ","        # Verify required fields exist","        for field in required_fields:","            assert field in metrics, (","                f\"Missing required field '{field}' in metrics for {cfg['stage_name']}\"","            )","        ","        # Verify stage_name matches","        assert metrics[\"stage_name\"] == cfg[\"stage_name\"]","",""]}
{"type":"file_footer","path":"tests/test_runner_adapter_contract.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_runner_adapter_input_coercion.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5624,"sha256":"229442c9cfc908d561d6fd064632a0b886cd5cd9b2d6806ff67ddf83552b47e3","total_lines":165,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_runner_adapter_input_coercion.py","chunk_index":0,"line_start":1,"line_end":165,"content":["","\"\"\"Contract tests for runner adapter input coercion.","","Tests verify that input arrays are coerced to np.ndarray float64,","preventing .shape access errors when lists are passed.","\"\"\"","","from __future__ import annotations","","import numpy as np","import pytest","","from pipeline.runner_adapter import run_stage_job","","","def test_stage0_coercion_with_lists() -> None:","    \"\"\"Test that Stage0 accepts list inputs and coerces to np.ndarray.\"\"\"","    # Use list instead of np.ndarray","    close_list = [100.0 + i * 0.1 for i in range(1000)]","    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5], [20.0, 10.0, 2.0]]","    ","    cfg = {","        \"stage_name\": \"stage0_coarse\",","        \"param_subsample_rate\": 1.0,","        \"topk\": 3,","        \"close\": close_list,  # List, not np.ndarray","        \"params_matrix\": params_matrix_list,  # List, not np.ndarray","        \"params_total\": 3,","        \"proxy_name\": \"ma_proxy_v0\",","    }","    ","    # Should not raise AttributeError: 'list' object has no attribute 'shape'","    result = run_stage_job(cfg)","    ","    # Verify result structure","    assert \"metrics\" in result","    assert \"winners\" in result","    ","    # Verify that internal arrays are np.ndarray (by checking results work)","    assert isinstance(result[\"winners\"][\"topk\"], list)","    assert len(result[\"winners\"][\"topk\"]) <= 3","","","def test_stage1_coercion_with_lists() -> None:","    \"\"\"Test that Stage1 accepts list inputs and coerces to np.ndarray.\"\"\"","    # Use lists instead of np.ndarray","    open_list = [100.0 + i * 0.1 for i in range(100)]","    high_list = [101.0 + i * 0.1 for i in range(100)]","    low_list = [99.0 + i * 0.1 for i in range(100)]","    close_list = [100.0 + i * 0.1 for i in range(100)]","    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5]]","    ","    cfg = {","        \"stage_name\": \"stage1_topk\",","        \"param_subsample_rate\": 1.0,","        \"topk\": 2,","        \"open_\": open_list,  # List, not np.ndarray","        \"high\": high_list,  # List, not np.ndarray","        \"low\": low_list,  # List, not np.ndarray","        \"close\": close_list,  # List, not np.ndarray","        \"params_matrix\": params_matrix_list,  # List, not np.ndarray","        \"params_total\": 2,","        \"commission\": 0.0,","        \"slip\": 0.0,","    }","    ","    # Should not raise AttributeError: 'list' object has no attribute 'shape'","    result = run_stage_job(cfg)","    ","    # Verify result structure","    assert \"metrics\" in result","    assert \"winners\" in result","    ","    # Verify that internal arrays are np.ndarray (by checking results work)","    assert isinstance(result[\"winners\"][\"topk\"], list)","","","def test_stage2_coercion_with_lists() -> None:","    \"\"\"Test that Stage2 accepts list inputs and coerces to np.ndarray.\"\"\"","    # Use lists instead of np.ndarray","    open_list = [100.0 + i * 0.1 for i in range(100)]","    high_list = [101.0 + i * 0.1 for i in range(100)]","    low_list = [99.0 + i * 0.1 for i in range(100)]","    close_list = [100.0 + i * 0.1 for i in range(100)]","    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5]]","    ","    cfg = {","        \"stage_name\": \"stage2_confirm\",","        \"param_subsample_rate\": 1.0,","        \"open_\": open_list,  # List, not np.ndarray","        \"high\": high_list,  # List, not np.ndarray","        \"low\": low_list,  # List, not np.ndarray","        \"close\": close_list,  # List, not np.ndarray","        \"params_matrix\": params_matrix_list,  # List, not np.ndarray","        \"params_total\": 2,","        \"commission\": 0.0,","        \"slip\": 0.0,","    }","    ","    # Should not raise AttributeError: 'list' object has no attribute 'shape'","    result = run_stage_job(cfg)","    ","    # Verify result structure","    assert \"metrics\" in result","    assert \"winners\" in result","    ","    # Verify that internal arrays are np.ndarray (by checking results work)","    assert isinstance(result[\"winners\"][\"topk\"], list)","","","def test_coercion_preserves_dtype_float64() -> None:","    \"\"\"Test that coercion produces float64 arrays.\"\"\"","    # Test with float32 input (should be coerced to float64)","    close_float32 = np.array([100.0, 101.0, 102.0], dtype=np.float32)","    params_matrix_float32 = np.array([[10.0, 5.0, 1.0]], dtype=np.float32)","    ","    cfg = {","        \"stage_name\": \"stage0_coarse\",","        \"param_subsample_rate\": 1.0,","        \"topk\": 1,","        \"close\": close_float32,","        \"params_matrix\": params_matrix_float32,","        \"params_total\": 1,","        \"proxy_name\": \"ma_proxy_v0\",","    }","    ","    # Should not raise errors","    result = run_stage_job(cfg)","    ","    # Verify result structure","    assert \"metrics\" in result","    assert \"winners\" in result","","","def test_coercion_handles_mixed_inputs() -> None:","    \"\"\"Test that coercion handles mixed list/np.ndarray inputs.\"\"\"","    # Mix of lists and np.ndarray","    open_list = [100.0 + i * 0.1 for i in range(100)]","    high_array = np.array([101.0 + i * 0.1 for i in range(100)], dtype=np.float64)","    low_list = [99.0 + i * 0.1 for i in range(100)]","    close_array = np.array([100.0 + i * 0.1 for i in range(100)], dtype=np.float32)","    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5]]","    ","    cfg = {","        \"stage_name\": \"stage1_topk\",","        \"param_subsample_rate\": 1.0,","        \"topk\": 2,","        \"open_\": open_list,  # List","        \"high\": high_array,  # np.ndarray float64","        \"low\": low_list,  # List","        \"close\": close_array,  # np.ndarray float32 (should be coerced to float64)","        \"params_matrix\": params_matrix_list,  # List","        \"params_total\": 2,","        \"commission\": 0.0,","        \"slip\": 0.0,","    }","    ","    # Should not raise errors","    result = run_stage_job(cfg)","    ","    # Verify result structure","    assert \"metrics\" in result","    assert \"winners\" in result","",""]}
{"type":"file_footer","path":"tests/test_runner_adapter_input_coercion.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_runner_grid_perf_observability.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1497,"sha256":"ae2e0c54f91ae7f23ce5419ca72e47f7a7bffecc6a938f8766be6eaaa76c9636","total_lines":45,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_runner_grid_perf_observability.py","chunk_index":0,"line_start":1,"line_end":45,"content":["","from __future__ import annotations","","import numpy as np","","from pipeline.runner_grid import run_grid","","","def test_run_grid_perf_fields_present_and_non_negative(monkeypatch) -> None:","    # Enable perf observability.","    monkeypatch.setenv(\"FISHBRO_PROFILE_GRID\", \"1\")","","    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)","    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)","    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)","    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)","","    params = np.array([[2, 2, 1.0], [3, 2, 1.5]], dtype=np.float64)","    out = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=False)","","    assert \"perf\" in out","    perf = out[\"perf\"]","    assert isinstance(perf, dict)","","    for k in (\"t_features\", \"t_indicators\", \"t_intent_gen\", \"t_simulate\"):","        assert k in perf","        # allow None (JSON null) when measurement is unavailable; never assume 0 is meaningful","        if perf[k] is not None:","            assert float(perf[k]) >= 0.0","","    assert \"simulate_impl\" in perf","    assert perf[\"simulate_impl\"] in (\"jit\", \"py\")","","    assert \"intents_total\" in perf","    if perf[\"intents_total\"] is not None:","        assert int(perf[\"intents_total\"]) >= 0","","    # Perf harness hook: confirm we can observe intent mode when profiling is enabled.","    assert \"intent_mode\" in perf","    if perf[\"intent_mode\"] is not None:","        assert perf[\"intent_mode\"] in (\"arrays\", \"objects\")","","","",""]}
{"type":"file_footer","path":"tests/test_runner_grid_perf_observability.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_runtime_context.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9461,"sha256":"abc478c9dee18386ac73602ee53f07b9ffb39a5dd97d69899aaa9b9d02723321","total_lines":287,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_runtime_context.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Test runtime context generation.","","Contract:","- Call write_runtime_context(out_path=tmp_path/...) with dummy entrypoint.","- Monkeypatch subprocess calls to raise; verify file still written with headings and UNKNOWN.","- Assert policy hash section present (UNKNOWN allowed).","\"\"\"","","import tempfile","import json","import subprocess","import hashlib","import os","from pathlib import Path","from unittest.mock import patch, MagicMock","import pytest","","from gui.services.runtime_context import (","    write_runtime_context,","    get_snapshot_timestamp,","    get_git_info,","    get_policy_hash,",")","","","def test_write_runtime_context_basic(tmp_path: Path):","    \"\"\"Basic test that writes a runtime context file.\"\"\"","    out_path = tmp_path / \"RUNTIME_CONTEXT.md\"","    ","    result = write_runtime_context(","        out_path=out_path,","        entrypoint=\"test_entrypoint.py\",","        listen_host=\"127.0.0.1\",","        listen_port=9999,","    )","    ","    assert result == out_path","    assert out_path.exists()","    ","    content = out_path.read_text(encoding=\"utf-8\")","    ","    # Required headings","    assert \"# Runtime Context\" in content","    assert \"## Timestamp\" in content","    assert \"## Process\" in content","    assert \"## Build\" in content","    assert \"## Entrypoint\" in content","    assert \"## Network\" in content","    assert \"## Governance\" in content","    assert \"## Snapshot Policy Binding\" in content","    assert \"## Notes\" in content","    ","    # Specific content","    assert \"test_entrypoint.py\" in content","    assert \"127.0.0.1:9999\" in content or \":9999\" in content","    ","    # Should have PID","    import os","    assert f\"PID: {os.getpid()}\" in content","","","def test_write_runtime_context_no_crash_on_error(tmp_path: Path):","    \"\"\"Test that write_runtime_context never crashes.\"\"\"","    out_path = tmp_path / \"RUNTIME_CONTEXT.md\"","    ","    # Monkeypatch subprocess.check_output to raise","    with patch('subprocess.check_output', side_effect=Exception(\"Mock error\")):","        # Also patch psutil.Process to raise if psutil is available","        # First check if psutil module is imported in the runtime_context module","        import sys","        if 'psutil' in sys.modules:","            with patch('psutil.Process', side_effect=Exception(\"Psutil error\")):","                result = write_runtime_context(","                    out_path=out_path,","                    entrypoint=\"test.py\",","                )","        else:","            # psutil not available, just test without patching it","            result = write_runtime_context(","                out_path=out_path,","                entrypoint=\"test.py\",","            )","    ","    assert result == out_path","    assert out_path.exists()","    ","    content = out_path.read_text(encoding=\"utf-8\")","    # Should still have basic structure","    assert \"# Runtime Context\" in content","    assert \"## Timestamp\" in content","    # Might have error section or minimal info","    assert \"PID:\" in content or \"Error\" in content","","","def test_policy_hash_section(tmp_path: Path):","    \"\"\"Test that policy hash section is present.\"\"\"","    out_path = tmp_path / \"RUNTIME_CONTEXT.md\"","    ","    # Create a dummy LOCAL_SCAN_RULES.json","    policy_dir = tmp_path / \"outputs\" / \"snapshots\" / \"full\"","    policy_dir.mkdir(parents=True)","    policy_content = json.dumps({\"mode\": \"test\", \"allowed_roots\": [\"src\"]})","    (policy_dir / \"LOCAL_SCAN_RULES.json\").write_text(policy_content)","    ","    # Mock the policy path to point to our dummy","    with patch('gui.services.runtime_context.Path') as MockPath:","        mock_path_instance = MagicMock()","        mock_path_instance.exists.return_value = True","        mock_path_instance.__str__.return_value = str(policy_dir / \"LOCAL_SCAN_RULES.json\")","        ","        def side_effect(*args, **kwargs):","            if args[0] == \"outputs/snapshots/full/LOCAL_SCAN_RULES.json\":","                return mock_path_instance","            return Path(*args, **kwargs)","        ","        MockPath.side_effect = side_effect","        ","        result = write_runtime_context(","            out_path=out_path,","            entrypoint=\"test.py\",","        )","    ","    assert out_path.exists()","    content = out_path.read_text(encoding=\"utf-8\")","    ","    # Should have policy hash section","    assert \"## Snapshot Policy Binding\" in content","    assert \"Local scan rules sha256:\" in content","    # Hash could be UNKNOWN or actual hash","    assert \"Local scan rules source:\" in content","","","def test_get_snapshot_timestamp(tmp_path: Path):","    \"\"\"Test snapshot timestamp retrieval.\"\"\"","    # Test with MANIFEST.json","    manifest_dir = tmp_path / \"outputs\" / \"snapshots\" / \"full\"","    manifest_dir.mkdir(parents=True)","    manifest_path = manifest_dir / \"MANIFEST.json\"","    ","    expected_time = \"2025-12-26T12:00:00Z\"","    manifest_path.write_text(json.dumps({\"generated_at_utc\": expected_time}))","    ","    with patch('gui.services.runtime_context.Path') as MockPath:","        def side_effect(*args, **kwargs):","            if args[0] == \"outputs/snapshots/full/MANIFEST.json\":","                return manifest_path","            return Path(*args, **kwargs)","        ","        MockPath.side_effect = side_effect","        ","        timestamp = get_snapshot_timestamp()","        assert timestamp == expected_time","    ","    # Test with SYSTEM_FULL_SNAPSHOT.md mtime","    import time","    snapshot_path = tmp_path / \"SYSTEM_FULL_SNAPSHOT.md\"","    snapshot_path.write_text(\"# Snapshot\")","    expected_mtime = time.time() - 3600","    os.utime(snapshot_path, (expected_mtime, expected_mtime))","    ","    with patch('gui.services.runtime_context.Path') as MockPath:","        def side_effect(*args, **kwargs):","            if args[0] == \"outputs/snapshots/full/MANIFEST.json\":","                return Path(\"/nonexistent\")","            if args[0] == \"outputs/snapshots/SYSTEM_FULL_SNAPSHOT.md\":","                return snapshot_path","            return Path(*args, **kwargs)","        ","        MockPath.side_effect = side_effect","        ","        timestamp = get_snapshot_timestamp()","        # Should be ISO format","        assert \"T\" in timestamp","        assert \"Z\" in timestamp or \"+\" in timestamp","    ","    # Test UNKNOWN when neither exists","    with patch('gui.services.runtime_context.Path') as MockPath:","        MockPath.return_value.exists.return_value = False","        ","        timestamp = get_snapshot_timestamp()","        assert timestamp == \"UNKNOWN\"","","","def test_get_git_info():","    \"\"\"Test git info retrieval.\"\"\"","    with patch('subprocess.check_output') as mock_check_output:","        # Mock successful git commands","        mock_check_output.return_value = b\"abc123\\n\"","        ","        commit, dirty = get_git_info()","        assert commit == \"abc123\"","        # dirty could be \"yes\" or \"no\" depending on mock","        ","        # Test git error","        mock_check_output.side_effect = Exception(\"git not found\")","        commit, dirty = get_git_info()","        assert commit == \"UNKNOWN\"","        assert dirty == \"UNKNOWN\""]}
{"type":"file_chunk","path":"tests/test_runtime_context.py","chunk_index":1,"line_start":201,"line_end":287,"content":["","","# def test_port_occupancy():","#     \"\"\"Test port occupancy checking.\"\"\"","#         mock_run.return_value = \"LISTEN 0 128 0.0.0.0:8080 0.0.0.0:* users:(python)\"","#","#         result = port_occupancy(8080)","#         assert \"8080\" in result or \"python\" in result","#","#         # Test error case","#         mock_run.return_value = \"ERROR: something\"","#         result = port_occupancy(8080)","#         assert \"ERROR\" in result","","","def test_get_policy_hash(tmp_path: Path):","    \"\"\"Test policy hash computation.\"\"\"","    policy_path = tmp_path / \"policy.json\"","    content = b'{\"test\": \"data\"}'","    policy_path.write_bytes(content)","    ","    expected_hash = hashlib.sha256(content).hexdigest()","    ","    hash_val = get_policy_hash(policy_path)","    assert hash_val == expected_hash","    ","    # Test missing file","    missing_path = tmp_path / \"missing.json\"","    hash_val = get_policy_hash(missing_path)","    assert hash_val == \"UNKNOWN\"","    ","    # Test read error","    with patch('builtins.open', side_effect=Exception(\"I/O error\")):","        hash_val = get_policy_hash(policy_path)","        assert hash_val == \"UNKNOWN\"","","","def test_runtime_context_integration(tmp_path: Path):","    \"\"\"Integration test with real file system.\"\"\"","    # Create a minimal repo-like structure","    repo_root = tmp_path / \"repo\"","    repo_root.mkdir()","    ","    # Create outputs/snapshots/full/LOCAL_SCAN_RULES.json","    policy_dir = repo_root / \"outputs\" / \"snapshots\" / \"full\"","    policy_dir.mkdir(parents=True)","    policy_content = json.dumps({","        \"mode\": \"local-strict\",","        \"allowed_roots\": [\"src\", \"tests\"],","        \"max_files\": 20000,","    })","    (policy_dir / \"LOCAL_SCAN_RULES.json\").write_text(policy_content)","    ","    # Create MANIFEST.json","    (policy_dir / \"MANIFEST.json\").write_text(json.dumps({","        \"generated_at_utc\": \"2025-12-26T12:00:00Z\",","        \"git_head\": \"test123\",","    }))","    ","    # Change to repo directory","    import os","    old_cwd = os.getcwd()","    os.chdir(repo_root)","    ","    try:","        out_path = repo_root / \"runtime_test.md\"","        ","        result = write_runtime_context(","            out_path=out_path,","            entrypoint=\"scripts/launch_dashboard.py\",","            listen_port=8080,","        )","        ","        assert result.exists()","        content = result.read_text(encoding=\"utf-8\")","        ","        # Check key sections","        assert \"## Snapshot Policy Binding\" in content","        assert \"Local scan rules sha256:\" in content","        assert \"scripts/launch_dashboard.py\" in content","        ","    finally:","        os.chdir(old_cwd)","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_runtime_context.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_runtime_network_probe.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6035,"sha256":"8e9456b74e262a9ea0c40ca1502e924544ddc120c6765785ea4832a4110e7d0e","total_lines":208,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_runtime_network_probe.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Test runtime network probe hardening.","","Validates dual-probe strategy for port occupancy detection in runtime context.","\"\"\"","","import pytest","","from src.gui.services.runtime_context import (","    _probe_ss,","    _probe_lsof,","    _analyze_port_occupancy,",")","","","def test_probe_ss_mocked_empty(monkeypatch):","    \"\"\"Test when ss returns empty (simulating WSL permission issue).\"\"\"","    ","    def mock_run(cmd):","        return \"\"  # Empty output","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._run\",","        mock_run","    )","    ","    result = _probe_ss(8080)","    assert \"NOT AVAILABLE\" in result","    assert \"ss command failed\" in result or \"empty\" in result","","","def test_probe_lsof_mocked_pid(monkeypatch):","    \"\"\"Test when lsof returns a PID.\"\"\"","    ","    def mock_run(cmd):","        # cmd is a list like [\"bash\", \"-lc\", \"lsof -i :8080 -sTCP:LISTEN -n -P\"]","        # Check if any part of the command contains \"lsof\"","        cmd_str = \" \".join(cmd)","        if \"lsof\" in cmd_str:","            return \"python3 12345 user 3u IPv4 12345 0t0 TCP *:8080 (LISTEN)\"","        return \"\"","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._run\",","        mock_run","    )","    ","    result = _probe_lsof(8080)","    assert \"python3\" in result","    assert \"12345\" in result","    assert \"8080\" in result","","","def test_analyze_port_occupancy_both_fail(monkeypatch):","    \"\"\"Test when both probes fail -> UNRESOLVED.\"\"\"","    ","    def mock_probe_ss(port):","        return \"NOT AVAILABLE (ss command failed)\"","    ","    def mock_probe_lsof(port):","        return \"NOT AVAILABLE (lsof command failed)\"","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_ss\",","        mock_probe_ss","    )","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_lsof\",","        mock_probe_lsof","    )","    ","    ss_out, lsof_out, bound, verdict = _analyze_port_occupancy(8080)","    ","    assert \"NOT AVAILABLE\" in ss_out","    assert \"NOT AVAILABLE\" in lsof_out","    assert bound == \"no\"  # No LISTEN in output","    assert \"UNRESOLVED\" in verdict or \"PORT NOT BOUND\" in verdict","","","def test_analyze_port_occupancy_ss_has_pid(monkeypatch):","    \"\"\"Test when ss returns PID.\"\"\"","    ","    def mock_probe_ss(port):","        return 'State  Recv-Q Send-Q Local Address:Port Peer Address:Port Process\\nLISTEN 0      128    127.0.0.1:8080    0.0.0.0:*      users:((\"python3\",pid=12345,fd=3))'","    ","    def mock_probe_lsof(port):","        return \"NOT AVAILABLE (lsof command failed)\"","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_ss\",","        mock_probe_ss","    )","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_lsof\",","        mock_probe_lsof","    )","    ","    ss_out, lsof_out, bound, verdict = _analyze_port_occupancy(8080)","    ","    assert \"pid=12345\" in ss_out","    assert \"NOT AVAILABLE\" in lsof_out","    assert bound == \"yes\"","    assert \"PID 12345\" in verdict","","","def test_analyze_port_occupancy_lsof_has_pid(monkeypatch):","    \"\"\"Test when lsof returns PID (ss empty).\"\"\"","    ","    def mock_probe_ss(port):","        return \"State  Recv-Q Send-Q Local Address:Port Peer Address:Port\"","    ","    def mock_probe_lsof(port):","        return \"python3   12345  user    3u  IPv4  12345      0t0  TCP *:8080 (LISTEN)\"","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_ss\",","        mock_probe_ss","    )","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_lsof\",","        mock_probe_lsof","    )","    ","    ss_out, lsof_out, bound, verdict = _analyze_port_occupancy(8080)","    ","    assert \"LISTEN\" in ss_out or bound == \"yes\"  # ss shows LISTEN but no PID","    assert \"12345\" in lsof_out","    assert bound == \"yes\"","    assert \"PID 12345\" in verdict","","","def test_analyze_port_occupancy_bound_no_pid(monkeypatch):","    \"\"\"Test when port is bound but no PID identified.\"\"\"","    ","    def mock_probe_ss(port):","        return \"State  Recv-Q Send-Q Local Address:Port Peer Address:Port\\nLISTEN 0      128    127.0.0.1:8080    0.0.0.0:*\"","    ","    def mock_probe_lsof(port):","        return \"COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\"","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_ss\",","        mock_probe_ss","    )","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_lsof\",","        mock_probe_lsof","    )","    ","    ss_out, lsof_out, bound, verdict = _analyze_port_occupancy(8080)","    ","    assert bound == \"yes\"","    assert \"UNRESOLVED\" in verdict","    assert \"bound but no PID\" in verdict","","","def test_write_runtime_context_integration(monkeypatch, tmp_path):","    \"\"\"Integration test: write_runtime_context produces correct Network section.\"\"\"","    ","    # Mock probes to return known values","    def mock_probe_ss(port):","        return \"ss output with pid=9999\"","    ","    def mock_probe_lsof(port):","        return \"lsof output\"","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_ss\",","        mock_probe_ss","    )","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_lsof\",","        mock_probe_lsof","    )","    ","    # Import after monkeypatching","    from src.gui.services.runtime_context import write_runtime_context","    ","    out_path = tmp_path / \"RUNTIME_CONTEXT.md\"","    ","    # Write runtime context","    result_path = write_runtime_context(","        out_path=str(out_path),","        entrypoint=\"test.py\",","        listen_port=9090,","    )","    ","    assert result_path.exists()","    content = result_path.read_text()","    ","    # Check required sections","    assert \"## Network\" in content","    assert \"Listen: :9090\" in content","    assert \"Port occupancy (9090):\" in content","    assert \"### ss\" in content","    assert \"### lsof\" in content","    assert \"### Resolution\" in content","    assert \"- Bound:\" in content","    assert \"- Process identified:\" in content"]}
{"type":"file_chunk","path":"tests/test_runtime_network_probe.py","chunk_index":1,"line_start":201,"line_end":208,"content":["    assert \"- Final verdict:\" in content","    ","    # Check our mocked PID appears","    assert \"pid=9999\" in content or \"PID 9999\" in content","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_runtime_network_probe.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_seed_demo_run.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4831,"sha256":"3d4840b848c0f0c71d0caa10648b80a1fdd9e33e5dc7f65977f8eddc50c9a4c1","total_lines":153,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_seed_demo_run.py","chunk_index":0,"line_start":1,"line_end":153,"content":["","\"\"\"Tests for seed_demo_run.","","Tests that seed_demo_run creates demo job and artifacts correctly.","\"\"\"","","from __future__ import annotations","","import json","import sqlite3","from pathlib import Path","","import pytest","","from control.seed_demo_run import main, get_db_path","","","def test_seed_demo_run_no_raise(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that seed_demo_run does not raise exceptions.\"\"\"","    # Set outputs root to tmp_path","    monkeypatch.chdir(tmp_path)","    monkeypatch.setenv(\"JOBS_DB_PATH\", str(tmp_path / \"jobs.db\"))","    ","    # Should not raise","    run_id = main()","    ","    assert run_id.startswith(\"demo_\")","    assert len(run_id) > 5","","","def test_outputs_directory_created(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that outputs/<season>/runs/<run_id>/ directory is created.\"\"\"","    monkeypatch.chdir(tmp_path)","    monkeypatch.setenv(\"JOBS_DB_PATH\", str(tmp_path / \"jobs.db\"))","    ","    run_id = main()","    ","    # Standard path structure: outputs/<season>/runs/<run_id>/","    run_dir = tmp_path / \"outputs\" / \"seasons\" / \"2026Q1\" / \"runs\" / run_id","    assert run_dir.exists()","    assert run_dir.is_dir()","","","def test_artifacts_exist(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that all required artifacts are created.\"\"\"","    monkeypatch.chdir(tmp_path)","    monkeypatch.setenv(\"JOBS_DB_PATH\", str(tmp_path / \"jobs.db\"))","    ","    run_id = main()","    # Standard path structure: outputs/<season>/runs/<run_id>/","    run_dir = tmp_path / \"outputs\" / \"seasons\" / \"2026Q1\" / \"runs\" / run_id","    ","    # Check manifest.json","    manifest_path = run_dir / \"manifest.json\"","    assert manifest_path.exists()","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        manifest = json.load(f)","    assert manifest[\"run_id\"] == run_id","    assert \"created_at\" in manifest","    ","    # Check winners_v2.json","    winners_path = run_dir / \"winners_v2.json\"","    assert winners_path.exists()","    ","    # Check governance.json","    governance_path = run_dir / \"governance.json\"","    assert governance_path.exists()","    ","    # Check kpi.json (KPI)","    kpi_path = run_dir / \"kpi.json\"","    assert kpi_path.exists()","    with kpi_path.open(\"r\", encoding=\"utf-8\") as f:","        kpi = json.load(f)","    assert \"net_profit\" in kpi","    assert \"max_drawdown\" in kpi","    assert \"num_trades\" in kpi","    assert \"final_score\" in kpi","","","def test_job_in_db(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that job is created in database with DONE status.\"\"\"","    monkeypatch.chdir(tmp_path)","    db_path = tmp_path / \"jobs.db\"","    monkeypatch.setenv(\"JOBS_DB_PATH\", str(db_path))","    ","    run_id = main()","    ","    # Check database","    conn = sqlite3.connect(str(db_path))","    try:","        cursor = conn.execute(\"SELECT status, run_id, report_link FROM jobs WHERE run_id = ?\", (run_id,))","        row = cursor.fetchone()","        assert row is not None","        ","        status, db_run_id, report_link = row","        assert status == \"DONE\"","        assert db_run_id == run_id","        assert report_link is not None","        assert report_link.startswith(\"/b5?\")","        assert run_id in report_link","        assert \"season=2026Q1\" in report_link","    finally:","        conn.close()","","","def test_report_link_not_none(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that report_link is not None.\"\"\"","    monkeypatch.chdir(tmp_path)","    db_path = tmp_path / \"jobs.db\"","    monkeypatch.setenv(\"JOBS_DB_PATH\", str(db_path))","    ","    run_id = main()","    ","    conn = sqlite3.connect(str(db_path))","    try:","        cursor = conn.execute(\"SELECT report_link FROM jobs WHERE run_id = ?\", (run_id,))","        row = cursor.fetchone()","        assert row is not None","        ","        report_link = row[0]","        assert report_link is not None","        assert len(report_link) > 0","    finally:","        conn.close()","","","def test_kpi_values_aligned(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that KPI values align with Phase 6.1 registry.\"\"\"","    monkeypatch.chdir(tmp_path)","    monkeypatch.setenv(\"JOBS_DB_PATH\", str(tmp_path / \"jobs.db\"))","    ","    run_id = main()","    # Standard path structure: outputs/<season>/runs/<run_id>/","    run_dir = tmp_path / \"outputs\" / \"seasons\" / \"2026Q1\" / \"runs\" / run_id","    ","    # Check kpi.json exists and has required KPIs (KPI)","    kpi_path = run_dir / \"kpi.json\"","    assert kpi_path.exists()","    with kpi_path.open(\"r\", encoding=\"utf-8\") as f:","        kpi = json.load(f)","    ","    assert \"net_profit\" in kpi","    assert \"max_drawdown\" in kpi","    assert \"num_trades\" in kpi","    assert \"final_score\" in kpi","    ","    # Verify KPI values match expected","    assert kpi[\"net_profit\"] == 123456","    assert kpi[\"max_drawdown\"] == -0.18","    assert kpi[\"num_trades\"] == 42","    assert kpi[\"final_score\"] == 1.23","",""]}
{"type":"file_footer","path":"tests/test_seed_demo_run.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_service_identity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5938,"sha256":"0d200474f6d5a62558f8b043feea5bf208d2eaaa0a39713f4f9c9bf89fe6fc86","total_lines":166,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_service_identity.py","chunk_index":0,"line_start":1,"line_end":166,"content":["\"\"\"Tests for service_identity module.\"\"\"","","import os","import sys","from pathlib import Path","from unittest.mock import patch, mock_open","import pytest","","from core.service_identity import (","    get_service_identity,","    _safe_cmdline,","    _safe_git_commit,","    _ALLOWED_ENV_KEYS,",")","","","def test_get_service_identity_returns_required_keys():","    \"\"\"Basic smoke test: ensure required keys are present.\"\"\"","    ident = get_service_identity(service_name=\"test\", db_path=None)","    assert isinstance(ident, dict)","    assert ident[\"service_name\"] == \"test\"","    assert ident[\"pid\"] == os.getpid()","    assert ident[\"ppid\"] == os.getppid()","    assert isinstance(ident[\"cmdline\"], str)","    assert ident[\"cwd\"] == str(Path.cwd())","    assert ident[\"python\"] == sys.executable","    assert ident[\"python_version\"] == sys.version","    assert isinstance(ident[\"platform\"], str)","    assert isinstance(ident[\"repo_root\"], str)","    assert \"git_commit\" in ident","    assert isinstance(ident[\"build_time_utc\"], str)","    assert isinstance(ident[\"env\"], dict)","    assert ident[\"jobs_db_path\"] == \"\"","    assert ident[\"jobs_db_parent\"] == \"\"","    assert ident[\"worker_pidfile_path\"] == \"\"","    assert ident[\"worker_log_path\"] == \"\"","","","def test_get_service_identity_with_db_path():","    \"\"\"Test with a db_path.\"\"\"","    db = Path(\"/tmp/test.db\")","    ident = get_service_identity(service_name=\"test\", db_path=db)","    assert ident[\"jobs_db_path\"] == str(db.expanduser().resolve())","    assert ident[\"jobs_db_parent\"] == str(db.expanduser().resolve().parent)","    assert ident[\"worker_pidfile_path\"] == str(db.expanduser().resolve().parent / \"worker.pid\")","    assert ident[\"worker_log_path\"] == str(db.expanduser().resolve().parent / \"worker_process.log\")","","","def test_env_filtering():","    \"\"\"Ensure only allowed env keys appear.\"\"\"","    # Set some env vars","    os.environ[\"PYTHONPATH\"] = \"/some/path\"","    os.environ[\"JOBS_DB_PATH\"] = \"/tmp/db\"","    os.environ[\"FISHBRO_TESTING\"] = \"1\"","    os.environ[\"PYTEST_CURRENT_TEST\"] = \"test\"","    os.environ[\"TMPDIR\"] = \"/tmp\"","    # Set a forbidden key","    os.environ[\"FORBIDDEN_KEY\"] = \"should_not_appear\"","","    ident = get_service_identity(service_name=\"test\", db_path=None)","    env = ident[\"env\"]","    assert \"PYTHONPATH\" in env","    assert \"JOBS_DB_PATH\" in env","    assert \"FISHBRO_TESTING\" in env","    assert \"PYTEST_CURRENT_TEST\" in env","    assert \"TMPDIR\" in env","    assert \"FORBIDDEN_KEY\" not in env","    # Ensure only allowed keys","    for key in env:","        assert key in _ALLOWED_ENV_KEYS","","    # Clean up","    del os.environ[\"FORBIDDEN_KEY\"]","","","def test_git_commit_unknown_when_git_missing():","    \"\"\"Test that git commit returns 'unknown' when .git missing.\"\"\"","    with patch(\"pathlib.Path.exists\", return_value=False):","        commit = _safe_git_commit(Path(\"/nonexistent\"))","        assert commit == \"unknown\"","","","@pytest.mark.xfail(reason=\"Mocking complexity; functionality verified by other tests\")","def test_git_commit_extracts_from_head():","    \"\"\"Mock git HEAD file.\"\"\"","    mock_head = \"ref: refs/heads/main\\n\"","    mock_ref = \"abc123\\n\"","    # Use a simple mock that logs calls","    from unittest.mock import MagicMock","    mock_exists = MagicMock()","    mock_read_text = MagicMock()","    # Configure side effects","    def exists_side(path):","        # path is a Path instance","        return True  # both exist","    def read_text_side(self, *args, **kwargs):","        # self is Path instance","        if self.name == \"HEAD\":","            return mock_head","        else:","            return mock_ref","    mock_exists.side_effect = exists_side","    mock_read_text.side_effect = read_text_side","    with patch(\"core.service_identity.Path.exists\", mock_exists):","        with patch(\"core.service_identity.Path.read_text\", mock_read_text):","            commit = _safe_git_commit(Path(\"/repo\"))","            assert commit == \"abc123\"","","","def test_git_commit_direct_hash():","    \"\"\"Mock HEAD containing direct commit hash.\"\"\"","    mock_head = \"abc456\\n\"","    with patch(\"pathlib.Path.exists\", return_value=True):","        with patch(\"pathlib.Path.read_text\", return_value=mock_head):","            commit = _safe_git_commit(Path(\"/repo\"))","            assert commit == \"abc456\"","","","def test_safe_cmdline_fallback():","    \"\"\"Test cmdline fallback when /proc/self/cmdline not available.\"\"\"","    with patch(\"pathlib.Path.exists\", return_value=False):","        cmd = _safe_cmdline()","        # Should fallback to sys.argv","        assert isinstance(cmd, str)","","","def test_no_exception_on_git_error():","    \"\"\"Ensure git commit extraction never raises.\"\"\"","    with patch(\"pathlib.Path.exists\", side_effect=Exception(\"permission denied\")):","        commit = _safe_git_commit(Path(\"/repo\"))","        assert commit == \"unknown\"","","","def test_repo_root_fallback():","    \"\"\"Test repo root detection falls back to cwd.\"\"\"","    with patch(\"pathlib.Path.exists\", return_value=False):","        # Mock climbing loop","        ident = get_service_identity(service_name=\"test\", db_path=None)","        assert ident[\"repo_root\"] == str(Path.cwd())","","","def test_db_path_expanduser():","    \"\"\"Test that db_path is expanded and resolved.\"\"\"","    # Mock expanduser to return same path","    with patch.object(Path, \"expanduser\", return_value=Path(\"/home/user/test.db\")):","        with patch.object(Path, \"resolve\", return_value=Path(\"/home/user/test.db\")):","            ident = get_service_identity(service_name=\"test\", db_path=Path(\"~/test.db\"))","            assert ident[\"jobs_db_path\"] == \"/home/user/test.db\"","","","def test_env_keys_missing():","    \"\"\"Ensure missing env keys are omitted.\"\"\"","    # Remove some keys","    for key in list(_ALLOWED_ENV_KEYS):","        if key in os.environ:","            del os.environ[key]","    ident = get_service_identity(service_name=\"test\", db_path=None)","    assert ident[\"env\"] == {}","","","def test_identity_json_serializable():","    \"\"\"Ensure identity dict is JSON serializable.\"\"\"","    import json","    ident = get_service_identity(service_name=\"test\", db_path=None)","    # Should not raise","    json.dumps(ident)"]}
{"type":"file_footer","path":"tests/test_service_identity.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_session_classification_mnq.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1840,"sha256":"b2c684c4feef4b91a4d2e42cb703b1f275fe81e27235adf9455e82c61be63235","total_lines":56,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_session_classification_mnq.py","chunk_index":0,"line_start":1,"line_end":56,"content":["","\"\"\"Test session classification for CME.MNQ.\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from data.session.classify import classify_session","from data.session.loader import load_session_profile","","","@pytest.fixture","def mnq_profile(profiles_root: Path) -> Path:","    \"\"\"Load CME.MNQ session profile.\"\"\"","    profile_path = profiles_root / \"CME_MNQ_TPE_v1.yaml\"","    return profile_path","","","def test_mnq_day_session(mnq_profile: Path) -> None:","    \"\"\"Test DAY session classification for CME.MNQ.\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Test DAY session times","    assert classify_session(\"2013/1/1 08:45:00\", profile) == \"DAY\"","    assert classify_session(\"2013/1/1 10:00:00\", profile) == \"DAY\"","    assert classify_session(\"2013/1/1 13:44:59\", profile) == \"DAY\"","    ","    # Test boundary (end is exclusive)","    assert classify_session(\"2013/1/1 13:45:00\", profile) is None","","","def test_mnq_night_session(mnq_profile: Path) -> None:","    \"\"\"Test NIGHT session classification for CME.MNQ.\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Test NIGHT session times (spans midnight)","    assert classify_session(\"2013/1/1 21:00:00\", profile) == \"NIGHT\"","    assert classify_session(\"2013/1/1 23:59:59\", profile) == \"NIGHT\"","    assert classify_session(\"2013/1/2 00:00:00\", profile) == \"NIGHT\"","    assert classify_session(\"2013/1/2 05:59:59\", profile) == \"NIGHT\"","    ","    # Test boundary (end is exclusive)","    assert classify_session(\"2013/1/2 06:00:00\", profile) is None","","","def test_mnq_outside_session(mnq_profile: Path) -> None:","    \"\"\"Test timestamps outside trading sessions.\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Between sessions","    assert classify_session(\"2013/1/1 14:00:00\", profile) is None","    assert classify_session(\"2013/1/1 20:59:59\", profile) is None","",""]}
{"type":"file_footer","path":"tests/test_session_classification_mnq.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_session_classification_mxf.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1840,"sha256":"49de68e848fe7cf62586bd65547849a80a6c673476f4a1ca86287e87c479ceb7","total_lines":56,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_session_classification_mxf.py","chunk_index":0,"line_start":1,"line_end":56,"content":["","\"\"\"Test session classification for TWF.MXF.\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from data.session.classify import classify_session","from data.session.loader import load_session_profile","","","@pytest.fixture","def mxf_profile(profiles_root: Path) -> Path:","    \"\"\"Load TWF.MXF session profile.\"\"\"","    profile_path = profiles_root / \"TWF_MXF_TPE_v1.yaml\"","    return profile_path","","","def test_mxf_day_session(mxf_profile: Path) -> None:","    \"\"\"Test DAY session classification for TWF.MXF.\"\"\"","    profile = load_session_profile(mxf_profile)","    ","    # Test DAY session times","    assert classify_session(\"2013/1/1 08:45:00\", profile) == \"DAY\"","    assert classify_session(\"2013/1/1 10:00:00\", profile) == \"DAY\"","    assert classify_session(\"2013/1/1 13:44:59\", profile) == \"DAY\"","    ","    # Test boundary (end is exclusive)","    assert classify_session(\"2013/1/1 13:45:00\", profile) is None","","","def test_mxf_night_session(mxf_profile: Path) -> None:","    \"\"\"Test NIGHT session classification for TWF.MXF.\"\"\"","    profile = load_session_profile(mxf_profile)","    ","    # Test NIGHT session times (spans midnight)","    assert classify_session(\"2013/1/1 15:00:00\", profile) == \"NIGHT\"","    assert classify_session(\"2013/1/1 23:59:59\", profile) == \"NIGHT\"","    assert classify_session(\"2013/1/2 00:00:00\", profile) == \"NIGHT\"","    assert classify_session(\"2013/1/2 04:59:59\", profile) == \"NIGHT\"","    ","    # Test boundary (end is exclusive)","    assert classify_session(\"2013/1/2 05:00:00\", profile) is None","","","def test_mxf_outside_session(mxf_profile: Path) -> None:","    \"\"\"Test timestamps outside trading sessions.\"\"\"","    profile = load_session_profile(mxf_profile)","    ","    # Between sessions","    assert classify_session(\"2013/1/1 14:00:00\", profile) is None","    assert classify_session(\"2013/1/1 14:59:59\", profile) is None","",""]}
{"type":"file_footer","path":"tests/test_session_classification_mxf.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_session_dst_mnq.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6045,"sha256":"39f02f9d2f9f3f0b16bb3e44faf8430443284fb9ed4a5dc43df249d68ae7d067","total_lines":153,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_session_dst_mnq.py","chunk_index":0,"line_start":1,"line_end":153,"content":["","\"\"\"Test DST boundary handling for CME.MNQ.","","Tests that session classification remains correct across DST transitions.","Uses programmatic timezone conversion to avoid manual TPE time errors.","\"\"\"","","from __future__ import annotations","","from datetime import datetime","from pathlib import Path","","import pytest","from zoneinfo import ZoneInfo","","from data.session.classify import classify_session","from data.session.loader import load_session_profile","","","@pytest.fixture","def mnq_v2_profile(profiles_root: Path) -> Path:","    \"\"\"Load CME.MNQ v2 session profile with windows format.\"\"\"","    profile_path = profiles_root / \"CME_MNQ_v2.yaml\"","    return profile_path","","","def _chicago_to_tpe_ts_str(chicago_time_str: str, date_str: str) -> str:","    \"\"\"Convert Chicago time to Taiwan time ts_str for a given date.","    ","    Args:","        chicago_time_str: Time string \"HH:MM:SS\" in Chicago timezone","        date_str: Date string \"YYYY/M/D\" or \"YYYY/MM/DD\"","        ","    Returns:","        Full ts_str \"YYYY/M/D HH:MM:SS\" in Taiwan timezone","    \"\"\"","    # Parse date (handles non-zero-padded)","    date_parts = date_str.split(\"/\")","    y, m, d = int(date_parts[0]), int(date_parts[1]), int(date_parts[2])","    ","    # Parse Chicago time","    time_parts = chicago_time_str.split(\":\")","    hh, mm, ss = int(time_parts[0]), int(time_parts[1]), int(time_parts[2])","    ","    # Create datetime in Chicago timezone","    chicago_tz = ZoneInfo(\"America/Chicago\")","    dt_chicago = datetime(y, m, d, hh, mm, ss, tzinfo=chicago_tz)","    ","    # Convert to Taiwan time","    tpe_tz = ZoneInfo(\"Asia/Taipei\")","    dt_tpe = dt_chicago.astimezone(tpe_tz)","    ","    # Return as \"YYYY/M/D HH:MM:SS\" string (matching input format)","    return f\"{dt_tpe.year}/{dt_tpe.month}/{dt_tpe.day} {dt_tpe.hour:02d}:{dt_tpe.minute:02d}:{dt_tpe.second:02d}\"","","","def test_dst_spring_forward_break(mnq_v2_profile: Path) -> None:","    \"\"\"Test BREAK session classification during DST spring forward (March).","    ","    CME break: 16:00-17:00 CT (Chicago time)","    During DST transition, this break period maps to different Taiwan times.","    But classification should still correctly identify BREAK session.","    \"\"\"","    profile = load_session_profile(mnq_v2_profile)","    ","    # DST spring forward: Second Sunday in March (2024-03-10)","    # Before DST (Standard Time, UTC-6): 16:00 CT maps to different TPE time","    # After DST (Daylight Time, UTC-5): 16:00 CT maps to different TPE time","    ","    # Calculate TPE ts_str for Chicago 16:00:00 on specific dates","    # Before DST (March 9, 2024 - Saturday)","    tpe_before = _chicago_to_tpe_ts_str(\"16:00:00\", \"2024/3/9\")","    tpe_before_end = _chicago_to_tpe_ts_str(\"16:59:59\", \"2024/3/9\")","    ","    # After DST (March 11, 2024 - Monday)","    tpe_after = _chicago_to_tpe_ts_str(\"16:00:00\", \"2024/3/11\")","    tpe_after_end = _chicago_to_tpe_ts_str(\"16:59:59\", \"2024/3/11\")","    ","    # Test break period before DST","    assert classify_session(tpe_before, profile) == \"BREAK\"","    assert classify_session(tpe_before_end, profile) == \"BREAK\"","    ","    # Test break period after DST","    assert classify_session(tpe_after, profile) == \"BREAK\"","    assert classify_session(tpe_after_end, profile) == \"BREAK\"","    ","    # Verify: Same exchange time (16:00 CT) maps to different Taiwan times,","    # but classification is consistent (both are BREAK)","","","def test_dst_fall_back_break(mnq_v2_profile: Path) -> None:","    \"\"\"Test BREAK session classification during DST fall back (November).","    ","    CME break: 16:00-17:00 CT (Chicago time)","    During DST fall back, this break period maps to different Taiwan times.","    But classification should still correctly identify BREAK session.","    \"\"\"","    profile = load_session_profile(mnq_v2_profile)","    ","    # DST fall back: First Sunday in November (2024-11-03)","    # Before DST (Daylight Time, UTC-5): 16:00 CT maps to different TPE time","    # After DST (Standard Time, UTC-6): 16:00 CT maps to different TPE time","    ","    # Calculate TPE ts_str for Chicago 16:00:00 on specific dates","    # Before DST (November 2, 2024 - Saturday)","    tpe_before = _chicago_to_tpe_ts_str(\"16:00:00\", \"2024/11/2\")","    tpe_before_end = _chicago_to_tpe_ts_str(\"16:59:59\", \"2024/11/2\")","    ","    # After DST (November 4, 2024 - Monday)","    tpe_after = _chicago_to_tpe_ts_str(\"16:00:00\", \"2024/11/4\")","    tpe_after_end = _chicago_to_tpe_ts_str(\"16:59:59\", \"2024/11/4\")","    ","    # Test break period before DST","    assert classify_session(tpe_before, profile) == \"BREAK\"","    assert classify_session(tpe_before_end, profile) == \"BREAK\"","    ","    # Test break period after DST","    assert classify_session(tpe_after, profile) == \"BREAK\"","    assert classify_session(tpe_after_end, profile) == \"BREAK\"","    ","    # Verify: Same exchange time (16:00 CT) maps to different Taiwan times,","    # but classification is consistent (both are BREAK)","","","def test_dst_trading_session_consistency(mnq_v2_profile: Path) -> None:","    \"\"\"Test TRADING session classification remains consistent across DST.","    ","    CME trading: 17:00 CT - 16:00 CT (next day)","    This should be correctly identified regardless of DST transitions.","    \"\"\"","    profile = load_session_profile(mnq_v2_profile)","    ","    # Calculate TPE ts_str for Chicago 17:00:00 on specific dates","    # March (before DST, Standard Time)","    tpe_mar_before = _chicago_to_tpe_ts_str(\"17:00:00\", \"2024/3/9\")","    assert classify_session(tpe_mar_before, profile) == \"TRADING\"","    ","    # March (after DST, Daylight Time)","    tpe_mar_after = _chicago_to_tpe_ts_str(\"17:00:00\", \"2024/3/11\")","    assert classify_session(tpe_mar_after, profile) == \"TRADING\"","    ","    # November (before DST, Daylight Time)","    tpe_nov_before = _chicago_to_tpe_ts_str(\"17:00:00\", \"2024/11/2\")","    assert classify_session(tpe_nov_before, profile) == \"TRADING\"","    ","    # November (after DST, Standard Time)","    tpe_nov_after = _chicago_to_tpe_ts_str(\"17:00:00\", \"2024/11/4\")","    assert classify_session(tpe_nov_after, profile) == \"TRADING\"","    ","    # Verify: Exchange time 17:00 CT is consistently classified as TRADING,","    # regardless of how it maps to Taiwan time due to DST","",""]}
{"type":"file_footer","path":"tests/test_session_dst_mnq.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_snapshot_compiler.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8242,"sha256":"3ebc85bbfbacfa718b87b24bc82ecefc7a26ecc83087c12470fdea3ef580dab7","total_lines":233,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_snapshot_compiler.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Test snapshot compiler deterministic compilation.","","Contract:","- Temp snapshots/full/ with known content files including LOCAL_SCAN_RULES.json","- Run compile_full_snapshot(snapshots_root=tmp_path/...)","- Assert output exists.","- Assert section order includes LOCAL_SCAN_RULES.json after MANIFEST.json.","- Assert raw content substrings match verbatim.","- Determinism: run twice; assert output bytes identical.","\"\"\"","","import tempfile","import json","import hashlib","from pathlib import Path","import pytest","","from control.snapshot_compiler import (","    compile_full_snapshot,","    verify_deterministic,",")","","","def test_compile_full_snapshot_basic(tmp_path: Path):","    \"\"\"Test basic compilation with minimal artifacts.\"\"\"","    snapshots_root = tmp_path / \"snapshots\"","    full_dir = snapshots_root / \"full\"","    full_dir.mkdir(parents=True)","    ","    # Create required files (some may be missing, that's OK)","    (full_dir / \"MANIFEST.json\").write_text(json.dumps({","        \"generated_at_utc\": \"2025-12-26T11:00:00Z\",","        \"git_head\": \"abc123\",","        \"scan_mode\": \"local-strict\",","        \"file_count\": 1,","        \"files\": [],","    }))","    ","    (full_dir / \"LOCAL_SCAN_RULES.json\").write_text(json.dumps({","        \"mode\": \"local-strict\",","        \"allowed_roots\": [\"src\", \"tests\"],","        \"max_files\": 20000,","    }))","    ","    (full_dir / \"REPO_TREE.txt\").write_text(\"src/a.py\\ntests/test.py\\n\")","    (full_dir / \"SKIPPED_FILES.txt\").write_text(\"TOO_LARGE\\tbig.bin\\n\")","    (full_dir / \"AUDIT_IMPORTS.csv\").write_text(\"file,lineno,kind,module,name\\n\")","    (full_dir / \"AUDIT_ENTRYPOINTS.md\").write_text(\"# Entrypoints\\n\")","    (full_dir / \"AUDIT_CALL_GRAPH.txt\").write_text(\"call graph\\n\")","    (full_dir / \"AUDIT_RUNTIME_MUTATIONS.txt\").write_text(\"mutations\\n\")","    (full_dir / \"AUDIT_STATE_FLOW.md\").write_text(\"# State Flow\\n\")","    (full_dir / \"AUDIT_CONFIG_REFERENCES.txt\").write_text(\"config refs\\n\")","    (full_dir / \"AUDIT_TEST_SURFACE.txt\").write_text(\"test surface\\n\")","    ","    # Run compiler","    out_path = compile_full_snapshot(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"SYSTEM_FULL_SNAPSHOT.md\",","    )","    ","    assert out_path.exists()","    assert out_path.name == \"SYSTEM_FULL_SNAPSHOT.md\"","    assert out_path.parent == snapshots_root","    ","    content = out_path.read_text(encoding=\"utf-8\")","    ","    # Check section order","    lines = content.splitlines()","    section_titles = []","    for line in lines:","        if line.startswith(\"## \"):","            section_titles.append(line)","    ","    # Should have sections in order","    assert any(\"MANIFEST.json\" in title for title in section_titles)","    assert any(\"LOCAL_SCAN_RULES.json\" in title for title in section_titles)","    assert any(\"REPO_TREE.txt\" in title for title in section_titles)","    ","    # Check that LOCAL_SCAN_RULES.json appears after MANIFEST.json","    manifest_idx = next(i for i, t in enumerate(section_titles) if \"MANIFEST.json\" in t)","    local_scan_idx = next(i for i, t in enumerate(section_titles) if \"LOCAL_SCAN_RULES.json\" in t)","    assert local_scan_idx > manifest_idx, \"LOCAL_SCAN_RULES.json should be after MANIFEST.json\"","    ","    # Check content is embedded verbatim","    assert '\"allowed_roots\": [\"src\", \"tests\"]' in content","    assert \"src/a.py\" in content","    assert \"TOO_LARGE\" in content","    ","    # Check fenced code blocks","    assert \"```json\" in content","    assert \"```text\" in content or \"```txt\" in content or \"```\" in content","","","def test_compile_deterministic(tmp_path: Path):","    \"\"\"Run compilation twice and ensure identical bytes.\"\"\"","    snapshots_root = tmp_path / \"snapshots\"","    full_dir = snapshots_root / \"full\"","    full_dir.mkdir(parents=True)","    ","    # Create simple files","    (full_dir / \"MANIFEST.json\").write_text(json.dumps({\"test\": 1}))","    (full_dir / \"LOCAL_SCAN_RULES.json\").write_text(json.dumps({\"mode\": \"test\"}))","    (full_dir / \"REPO_TREE.txt\").write_text(\"tree\")","    ","    # First compilation","    out_path = compile_full_snapshot(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_SNAPSHOT.md\",","    )","    ","    first_content = out_path.read_bytes()","    first_hash = hashlib.sha256(first_content).hexdigest()","    ","    # Second compilation (should be identical)","    out_path2 = compile_full_snapshot(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_SNAPSHOT.md\",","    )","    ","    second_content = out_path2.read_bytes()","    second_hash = hashlib.sha256(second_content).hexdigest()","    ","    assert first_hash == second_hash, \"Output should be deterministic\"","    assert first_content == second_content, \"Bytes should be identical\"","","","def test_missing_files_section(tmp_path: Path):","    \"\"\"Test that missing files are listed in Missing Files section.\"\"\"","    snapshots_root = tmp_path / \"snapshots\"","    full_dir = snapshots_root / \"full\"","    full_dir.mkdir(parents=True)","    ","    # Create only one file","    (full_dir / \"MANIFEST.json\").write_text(json.dumps({}))","    ","    out_path = compile_full_snapshot(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_SNAPSHOT.md\",","    )","    ","    content = out_path.read_text(encoding=\"utf-8\")","    ","    # Should have Missing Files section","    assert \"Missing Files\" in content","    # Should list LOCAL_SCAN_RULES.json as missing","    assert \"LOCAL_SCAN_RULES.json\" in content","    assert \"REPO_TREE.txt\" in content","","","def test_verify_deterministic(tmp_path: Path):","    \"\"\"Test the verify_deterministic helper.\"\"\"","    snapshots_root = tmp_path / \"snapshots\"","    full_dir = snapshots_root / \"full\"","    full_dir.mkdir(parents=True)","    ","    (full_dir / \"MANIFEST.json\").write_text(json.dumps({\"a\": 1}))","    (full_dir / \"LOCAL_SCAN_RULES.json\").write_text(json.dumps({\"b\": 2}))","    ","    # Should not raise and return True","    result = verify_deterministic(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_VERIFY.md\",","    )","    assert result is True, \"Should be deterministic\"","    ","    # Now make a non-deterministic change (timestamp in MANIFEST)","    import time","    (full_dir / \"MANIFEST.json\").write_text(json.dumps({\"a\": 1, \"time\": time.time()}))","    ","    # This should still be deterministic because we read the same file twice","    # (content hasn't changed between the two runs)","    result = verify_deterministic(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_VERIFY2.md\",","    )","    assert result is True, \"Should still be deterministic (same input between runs)\"","","","def test_encoding_handling(tmp_path: Path):","    \"\"\"Test that non-UTF-8 files are handled gracefully.\"\"\"","    snapshots_root = tmp_path / \"snapshots\"","    full_dir = snapshots_root / \"full\"","    full_dir.mkdir(parents=True)","    ","    # Create a binary file (simulate corrupted text)","    (full_dir / \"MANIFEST.json\").write_text(json.dumps({\"test\": \"\"}))  # Chinese chars","    (full_dir / \"LOCAL_SCAN_RULES.json\").write_text(json.dumps({\"mode\": \"test\"}))","    ","    # Create a file with invalid UTF-8 sequence","    (full_dir / \"REPO_TREE.txt\").write_bytes(b\"normal text \\xff\\xfe invalid \\x00\")","    ","    # Should not crash"]}
{"type":"file_chunk","path":"tests/test_snapshot_compiler.py","chunk_index":1,"line_start":201,"line_end":233,"content":["    out_path = compile_full_snapshot(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_ENCODING.md\",","    )","    ","    assert out_path.exists()","    # Should contain replacement characters or survive","    content = out_path.read_text(encoding=\"utf-8\", errors=\"ignore\")","    assert \"normal text\" in content","","","def test_empty_snapshots_dir(tmp_path: Path):","    \"\"\"Test with empty snapshots/full directory.\"\"\"","    snapshots_root = tmp_path / \"snapshots\"","    full_dir = snapshots_root / \"full\"","    full_dir.mkdir(parents=True)","    ","    # No files at all","    out_path = compile_full_snapshot(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_EMPTY.md\",","    )","    ","    assert out_path.exists()","    content = out_path.read_text(encoding=\"utf-8\")","    assert \"Missing Files\" in content","    assert all(fname in content for fname in [\"MANIFEST.json\", \"LOCAL_SCAN_RULES.json\", \"REPO_TREE.txt\"])","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_snapshot_compiler.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_sparse_intents_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12830,"sha256":"0a8f0fd239ed566e25bf87ab035e95afef6f43d08c383821d8446325029ec2c6","total_lines":341,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_sparse_intents_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Stage P2-3A: Contract Tests for Sparse Entry Intents (Grid Level)","","Verifies that entry intents are truly sparse at grid level:","- entry_intents_total == entry_valid_mask_sum (not Bars  Params)","- Sparse builder produces identical results to dense builder (same triggers)","\"\"\"","from __future__ import annotations","","from dataclasses import asdict, is_dataclass","","import numpy as np","import os","","from engine.types import Fill","from pipeline.runner_grid import run_grid","","","def _fill_to_tuple(f: Fill) -> tuple:","    \"\"\"","    Convert Fill to a comparable tuple representation.","    ","    Uses dataclasses.asdict for dataclass instances, falls back to __dict__ or repr.","    Returns sorted tuple to ensure deterministic comparison.","    \"\"\"","    if is_dataclass(f):","        d = asdict(f)","    else:","        # fallback: __dict__ (for normal classes)","        d = dict(getattr(f, \"__dict__\", {}))","        if not d:","            # last resort: repr","            return (repr(f),)","    # Fixed ordering to avoid dict order differences","    return tuple(sorted(d.items()))","","","def test_grid_sparse_intents_count() -> None:","    \"\"\"","    Test that grid-level entry intents count scales with trigger_rate (param-subsample).","    ","    This test verifies the core sparse contract at grid level:","    - entry_intents_total == entry_valid_mask_sum","    - entry_intents_total scales approximately linearly with trigger_rate","    \"\"\"","    # Ensure clean environment","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    old_param_subsample_rate = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","    old_profile_grid = os.environ.pop(\"FISHBRO_PROFILE_GRID\", None)","    ","    try:","        n_bars = 500","        n_params = 30  # Enough params to make \"unique repetition\" meaningful","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix (at least 10-50 params for meaningful unique repetition)","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)  # Vary channel_len (20-29)","            atr_len = 10 + (i % 5)  # Vary atr_len (10-14)","            stop_mult = 1.0 + (i % 3) * 0.5  # Vary stop_mult (1.0, 1.5, 2.0)","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Fix param_subsample_rate=1.0 (all params) to test trigger_rate effect on intents","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"1.0\"","        os.environ[\"FISHBRO_PROFILE_GRID\"] = \"1\"","        ","        # Run Dense (trigger_rate=1.0) - baseline","        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","        ","        result_dense = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Run Sparse (trigger_rate=0.05) - bar/intent-level sparsity","        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"0.05\"","        ","        result_sparse = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify perf dicts exist","        perf_dense = result_dense.get(\"perf\", {})","        perf_sparse = result_sparse.get(\"perf\", {})","        ","        assert isinstance(perf_dense, dict), \"perf_dense must be a dict\"","        assert isinstance(perf_sparse, dict), \"perf_sparse must be a dict\"","        ","        # Core contract: entry_intents_total == entry_valid_mask_sum (both runs)","        entry_intents_dense = perf_dense.get(\"entry_intents_total\")","        entry_valid_mask_dense = perf_dense.get(\"entry_valid_mask_sum\")","        entry_intents_sparse = perf_sparse.get(\"entry_intents_total\")","        entry_valid_mask_sparse = perf_sparse.get(\"entry_valid_mask_sum\")","        ","        assert entry_intents_dense == entry_valid_mask_dense, (","            f\"Dense: entry_intents_total ({entry_intents_dense}) \"","            f\"must equal entry_valid_mask_sum ({entry_valid_mask_dense})\"","        )","        assert entry_intents_sparse == entry_valid_mask_sparse, (","            f\"Sparse: entry_intents_total ({entry_intents_sparse}) \"","            f\"must equal entry_valid_mask_sum ({entry_valid_mask_sparse})\"","        )","        ","        # Contract: entry_intents_sparse should be approximately trigger_rate * entry_intents_dense","        # With trigger_rate=0.05, we expect approximately 5% of dense baseline","        # Allow wide tolerance: [0.02, 0.08] (2% to 8% of dense)","        if entry_intents_dense is not None and entry_intents_dense > 0:","            ratio = entry_intents_sparse / entry_intents_dense","            assert 0.02 <= ratio <= 0.08, (","                f\"With trigger_rate=0.05, entry_intents_sparse ({entry_intents_sparse}) \"","                f\"should be approximately 5% of entry_intents_dense ({entry_intents_dense}), \"","                f\"got ratio {ratio:.4f} (expected [0.02, 0.08])\"","            )","        ","    finally:","        # Restore environment","        if old_trigger_rate is None:","            os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","        ","        if old_param_subsample_rate is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = old_param_subsample_rate","        ","        if old_profile_grid is None:","            os.environ.pop(\"FISHBRO_PROFILE_GRID\", None)","        else:","            os.environ[\"FISHBRO_PROFILE_GRID\"] = old_profile_grid","","","def test_sparse_vs_dense_builder_parity() -> None:","    \"\"\"","    Test that sparse builder produces identical results to dense builder (same triggers).","    ","    This test verifies determinism parity:","    - Same triggers set  same results (metrics, fills)","    - Order ID determinism","    - Bit-exact parity","    ","    Uses FISHBRO_FORCE_SPARSE_BUILDER=1 to test numba builder vs python builder.","    \"\"\"","    # Ensure clean environment","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    old_force_sparse = os.environ.pop(\"FISHBRO_FORCE_SPARSE_BUILDER\", None)","    ","    try:","        n_bars = 300","        n_params = 20","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)","            atr_len = 10 + (i % 5)","            stop_mult = 1.0 + (i % 3) * 0.5","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Run A: trigger_rate=1.0, force_sparse=0 (Python builder)"]}
{"type":"file_chunk","path":"tests/test_sparse_intents_contract.py","chunk_index":1,"line_start":201,"line_end":341,"content":["        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","        os.environ.pop(\"FISHBRO_FORCE_SPARSE_BUILDER\", None)  # Ensure not set","        ","        result_a = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Run B: trigger_rate=1.0, force_sparse=1 (Numba builder, same triggers)","        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","        os.environ[\"FISHBRO_FORCE_SPARSE_BUILDER\"] = \"1\"","        ","        result_b = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify metrics are identical (bit-exact)","        metrics_a = result_a.get(\"metrics\")","        metrics_b = result_b.get(\"metrics\")","        ","        assert metrics_a is not None, \"metrics_a must exist\"","        assert metrics_b is not None, \"metrics_b must exist\"","        ","        # Compare metrics arrays (should be bit-exact)","        np.testing.assert_array_equal(metrics_a, metrics_b, \"metrics must be bit-exact\")","        ","        # Verify sparse contract holds in both runs","        perf_a = result_a.get(\"perf\", {})","        perf_b = result_b.get(\"perf\", {})","        ","        if isinstance(perf_a, dict) and isinstance(perf_b, dict):","            entry_intents_a = perf_a.get(\"entry_intents_total\")","            entry_intents_b = perf_b.get(\"entry_intents_total\")","            ","            if entry_intents_a is not None and entry_intents_b is not None:","                assert entry_intents_a == entry_intents_b, (","                    f\"entry_intents_total should be identical (same triggers): \"","                    f\"A={entry_intents_a}, B={entry_intents_b}\"","                )","        ","    finally:","        # Restore environment","        if old_trigger_rate is None:","            os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","        ","        if old_force_sparse is None:","            os.environ.pop(\"FISHBRO_FORCE_SPARSE_BUILDER\", None)","        else:","            os.environ[\"FISHBRO_FORCE_SPARSE_BUILDER\"] = old_force_sparse","","","def test_created_bar_sorted() -> None:","    \"\"\"","    Test that created_bar arrays are sorted (ascending).","    ","    Note: This test verifies the sparse builder contract that created_bar must be","    sorted. We verify this indirectly through the sparse contract consistency.","    \"\"\"","    # Ensure clean environment","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    ","    try:","        n_bars = 200","        n_params = 10","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 5)","            atr_len = 10 + (i % 3)","            stop_mult = 1.0","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Run grid","        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","        ","        result = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify sparse contract: entry_intents_total == entry_valid_mask_sum","        perf = result.get(\"perf\", {})","        if isinstance(perf, dict):","            entry_intents_total = perf.get(\"entry_intents_total\")","            entry_valid_mask_sum = perf.get(\"entry_valid_mask_sum\")","            ","            if entry_intents_total is not None and entry_valid_mask_sum is not None:","                assert entry_intents_total == entry_valid_mask_sum, (","                    f\"Sparse contract: entry_intents_total ({entry_intents_total}) \"","                    f\"must equal entry_valid_mask_sum ({entry_valid_mask_sum})\"","                )","        ","        # Note: created_bar sorted verification would require accessing internal arrays","        # For now, we verify the sparse contract which implies created_bar is sorted","        # (since flatnonzero returns sorted indices)","        ","    finally:","        # Restore environment","        if old_trigger_rate is None:","            os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","",""]}
{"type":"file_footer","path":"tests/test_sparse_intents_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_sparse_intents_mvp_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9494,"sha256":"2e83caf5381232abf321f25a168af841a70f1988f9c3a9f0283e776450811c56","total_lines":255,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_sparse_intents_mvp_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for sparse intents MVP (Stage P2-1).","","These tests ensure:","1. created_bar is sorted (deterministic ordering)","2. intents_total drops significantly with sparse masking","3. Vectorization parity remains bit-exact","\"\"\"","","import numpy as np","import pytest","","from config.dtypes import INDEX_DTYPE","from engine.types import BarArrays","from strategy.kernel import (","    DonchianAtrParams,","    _build_entry_intents_from_trigger,","    run_kernel_arrays,",")","","","def _expected_entry_count(donch_prev: np.ndarray, warmup: int) -> int:","    \"\"\"","    Calculate expected entry count using the same mask rules as production.","    ","    Production mask (from _build_entry_intents_from_trigger):","    - i = np.arange(1, n)  # bar indices t (from 1 to n-1)","    - valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)","    ","    This helper replicates that exact logic.","    \"\"\"","    n = donch_prev.size","    # Create index array for bars 1..n-1 (bar indices t, where created_bar = t-1)","    i = np.arange(1, n, dtype=INDEX_DTYPE)","    # Sparse mask: valid entries must be finite, positive, and past warmup","    valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)","    return int(np.count_nonzero(valid_mask))","","","def _make_donch_hi_with_trigger_rate(","    n_bars: int,","    warmup: int,","    trigger_rate: float,","    seed: int = 42,",") -> np.ndarray:","    \"\"\"","    Generate donch_hi array with controlled trigger rate.","    ","    Args:","        n_bars: number of bars","        warmup: warmup period (bars before warmup are NaN)","        trigger_rate: fraction of bars after warmup that should be valid (0.0-1.0)","        seed: random seed","    ","    Returns:","        donch_hi array (float64, n_bars):","        - Bars 0..warmup-1: NaN","        - Bars warmup..n_bars-1: trigger_rate fraction are positive values, rest are NaN","    \"\"\"","    rng = np.random.default_rng(seed)","    ","    donch_hi = np.full(n_bars, np.nan, dtype=np.float64)","    ","    # After warmup, set trigger_rate fraction to positive values","    post_warmup_bars = n_bars - warmup","    if post_warmup_bars > 0:","        n_valid = int(post_warmup_bars * trigger_rate)","        if n_valid > 0:","            # Select random indices after warmup","            valid_indices = rng.choice(","                np.arange(warmup, n_bars),","                size=n_valid,","                replace=False,","            )","            # Set valid indices to positive values (e.g., 100.0 + small random)","            donch_hi[valid_indices] = 100.0 + rng.random(n_valid) * 10.0","    ","    return donch_hi","","","class TestSparseIntentsMVP:","    \"\"\"Test sparse intents MVP contract.\"\"\"","","    def test_sparse_intents_created_bar_is_sorted(self):","        \"\"\"","        Contract: created_bar must be sorted (non-decreasing).","        ","        This ensures deterministic ordering and that sparse masking preserves","        the original bar sequence.","        \"\"\"","        n_bars = 1000","        warmup = 20","        trigger_rate = 0.1","        ","        # Generate donch_hi with controlled trigger rate","        donch_hi = _make_donch_hi_with_trigger_rate(n_bars, warmup, trigger_rate, seed=42)","        ","        # Create donch_prev (shifted for next-bar active)","        donch_prev = np.empty_like(donch_hi)","        donch_prev[0] = np.nan","        donch_prev[1:] = donch_hi[:-1]","        ","        # Build entry intents","        result = _build_entry_intents_from_trigger(","            donch_prev=donch_prev,","            channel_len=warmup,","            order_qty=1,","        )","        ","        created_bar = result[\"created_bar\"]","        n_entry = result[\"n_entry\"]","        ","        # Verify n_entry matches expected count (exact match using production mask rules)","        expected = _expected_entry_count(donch_prev, warmup)","        assert n_entry == expected, (","            f\"n_entry ({n_entry}) should equal expected ({expected}) \"","            f\"calculated using production mask rules\"","        )","        ","        # Verify created_bar is sorted (non-decreasing)","        if n_entry > 1:","            assert np.all(created_bar[1:] >= created_bar[:-1]), (","                f\"created_bar must be sorted (non-decreasing). \"","                f\"Got: {created_bar[:10]} ... (showing first 10)\"","            )","        ","        # Hard consistency check: created_bar must match flatnonzero result exactly","        # This locks in the ordering contract","        i = np.arange(1, donch_prev.size, dtype=INDEX_DTYPE)","        valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)","        idx = np.flatnonzero(valid_mask).astype(created_bar.dtype)","        assert np.array_equal(created_bar[:n_entry], idx), (","            f\"created_bar must exactly match flatnonzero result. \"","            f\"Got: {created_bar[:min(10, n_entry)]}, \"","            f\"Expected: {idx[:min(10, len(idx))]}\"","        )","","    def test_sparse_intents_total_drops_order_of_magnitude(self):","        \"\"\"","        Contract: intents_total should drop significantly with sparse masking.","        ","        With controlled trigger rate (e.g., 5%), intents_total should be << n_bars.","        This test directly controls donch_hi to ensure precise trigger rate.","        \"\"\"","        n_bars = 1000","        warmup = 20","        trigger_rate = 0.05  # 5% trigger rate","        ","        # Generate donch_hi with controlled trigger rate","        donch_hi = _make_donch_hi_with_trigger_rate(n_bars, warmup, trigger_rate, seed=42)","        ","        # Create donch_prev (shifted for next-bar active)","        donch_prev = np.empty_like(donch_hi)","        donch_prev[0] = np.nan","        donch_prev[1:] = donch_hi[:-1]","        ","        # Build entry intents","        result = _build_entry_intents_from_trigger(","            donch_prev=donch_prev,","            channel_len=warmup,","            order_qty=1,","        )","        ","        n_entry = result[\"n_entry\"]","        obs = result[\"obs\"]","        ","        # Verify diagnostic observations","        assert obs[\"n_bars\"] == n_bars","        assert obs[\"warmup\"] == warmup","        assert obs[\"valid_mask_sum\"] == n_entry","        ","        # Verify n_entry matches expected count (exact match using production mask rules)","        expected = _expected_entry_count(donch_prev, warmup)","        assert n_entry == expected, (","            f\"n_entry ({n_entry}) should equal expected ({expected}) \"","            f\"calculated using production mask rules\"","        )","        ","        # Order-of-magnitude contract: n_entry should be significantly less than n_bars","        # This is the core contract of this test","        # Conservative threshold: 6% of (n_bars - warmup) as upper bound","        max_expected_ratio = 0.06  # 6% conservative upper bound","        max_expected = int((n_bars - warmup) * max_expected_ratio)","        ","        assert n_entry <= max_expected, (","            f\"n_entry ({n_entry}) should be <= {max_expected} \"","            f\"({max_expected_ratio*100}% of post-warmup bars) \"","            f\"with trigger_rate={trigger_rate}, n_bars={n_bars}, warmup={warmup}. \"","            f\"Sparse masking should significantly reduce intent count (order-of-magnitude reduction).\"","        )","        ","        # Also verify it's not zero (unless trigger_rate is too low)","        if trigger_rate > 0:","            # With 5% trigger rate, we should have some intents","            assert n_entry > 0, (","                f\"Expected some intents with trigger_rate={trigger_rate}, \"","                f\"but got n_entry={n_entry}\"","            )","","    def test_vectorization_parity_still_bit_exact(self):"]}
{"type":"file_chunk","path":"tests/test_sparse_intents_mvp_contract.py","chunk_index":1,"line_start":201,"line_end":255,"content":["        \"\"\"","        Contract: Vectorization parity tests should still pass after sparse masking.","        ","        This test ensures that sparse masking doesn't break existing parity contracts.","        We rely on the existing test_vectorization_parity.py to verify this.","        ","        This test is a placeholder to document the requirement.","        \"\"\"","        # This test doesn't need to re-implement parity checks.","        # It's sufficient to ensure that make check passes all existing tests.","        # The actual parity verification is in tests/test_vectorization_parity.py","        ","        # Basic sanity check: sparse masking should produce valid results","        n_bars = 100","        bars = BarArrays(","            open=np.arange(100, 200, dtype=np.float64),","            high=np.arange(101, 201, dtype=np.float64),","            low=np.arange(99, 199, dtype=np.float64),","            close=np.arange(100, 200, dtype=np.float64),","        )","        ","        params = DonchianAtrParams(","            channel_len=10,","            atr_len=5,","            stop_mult=1.5,","        )","        ","        result = run_kernel_arrays(","            bars,","            params,","            commission=0.0,","            slip=0.0,","            order_qty=1,","        )","        ","        # Verify result structure is intact","        assert \"fills\" in result","        assert \"metrics\" in result","        assert \"_obs\" in result","        assert \"intents_total\" in result[\"_obs\"]","        ","        # Verify diagnostic observations are present","        assert \"n_bars\" in result[\"_obs\"]","        assert \"warmup\" in result[\"_obs\"]","        assert \"valid_mask_sum\" in result[\"_obs\"]","        ","        # Verify intents_total is reasonable","        intents_total = result[\"_obs\"][\"intents_total\"]","        assert intents_total >= 0","        assert intents_total <= n_bars  # Should be <= n_bars due to sparse masking","        ","        # Note: Full parity verification is done by test_vectorization_parity.py","        # This test just ensures the basic contract is met","",""]}
{"type":"file_footer","path":"tests/test_sparse_intents_mvp_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_state_processor_serialization.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":10908,"sha256":"5afc673e001c00d7ebf93dbd1206797b7927187d0f8ff3b8180d638ef18d9bd2","total_lines":354,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_state_processor_serialization.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Test StateProcessor serial execution for Attack #9.","","Tests that StateProcessor executes intents sequentially (single consumer)","and produces consistent SystemState snapshots.","\"\"\"","","import pytest","import asyncio","import time","from datetime import date, datetime","from concurrent.futures import ThreadPoolExecutor","","from core.intents import (","    CreateJobIntent, CalculateUnitsIntent, DataSpecIntent,","    IntentStatus, IntentType",")","from control.action_queue import ActionQueue, reset_action_queue","from core.processor import StateProcessor, ProcessingError, get_processor","from core.state import SystemState, JobStatus, create_initial_state","","","@pytest.fixture","def action_queue():","    \"\"\"Create a fresh ActionQueue for each test.\"\"\"","    reset_action_queue()","    queue = ActionQueue(max_size=100)","    yield queue","    queue.clear()","","","@pytest.fixture","def processor(action_queue):","    \"\"\"Create a StateProcessor with fresh queue.\"\"\"","    return StateProcessor(action_queue)","","","@pytest.fixture","def sample_data_spec():","    \"\"\"Create a sample DataSpecIntent for testing.\"\"\"","    return DataSpecIntent(","        dataset_id=\"test_dataset\",","        symbols=[\"MNQ\", \"MXF\"],","        timeframes=[\"60m\", \"120m\"],","        start_date=date(2020, 1, 1),","        end_date=date(2024, 12, 31)","    )","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_sequential_execution(processor, action_queue, sample_data_spec):","    \"\"\"Test that processor executes intents sequentially.\"\"\"","    # Start processor","    await processor.start()","    ","    # Submit multiple intents","    intent_ids = []","    for i in range(5):","        intent = CalculateUnitsIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        intent_id = processor.submit_intent(intent)","        intent_ids.append(intent_id)","    ","    # Wait for all intents to complete","    completed_count = 0","    start_time = time.time()","    timeout = 5.0","    ","    while completed_count < 5 and time.time() - start_time < timeout:","        completed_count = 0","        for intent_id in intent_ids:","            intent = action_queue.get_intent(intent_id)","            if intent and intent.status in [IntentStatus.COMPLETED, IntentStatus.FAILED]:","                completed_count += 1","        await asyncio.sleep(0.1)","    ","    # All intents should be completed","    assert completed_count == 5","    ","    # Check that they were processed in order (FIFO)","    # Since we can't easily track exact order without timestamps in test,","    # we at least verify all were processed","    metrics = action_queue.get_metrics()","    assert metrics[\"processed\"] == 5","    ","    await processor.stop()","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_state_updates(processor, action_queue, sample_data_spec):","    \"\"\"Test that processor updates system state correctly.\"\"\"","    # Start processor","    await processor.start()","    ","    # Get initial state","    initial_state = processor.get_state()","    assert initial_state.metrics.total_jobs == 0","    ","    # Submit a job creation intent","    intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10, \"window_slow\": 30}","    )","    ","    intent_id = processor.submit_intent(intent)","    ","    # Wait for completion","    completed = await processor.wait_for_intent(intent_id, timeout=5.0)","    assert completed is not None","    assert completed.status == IntentStatus.COMPLETED","    ","    # Check that state was updated","    final_state = processor.get_state()","    assert final_state.metrics.total_jobs == 1","    assert final_state.metrics.queued_jobs == 1","    ","    # Job should be in state","    job_id = completed.result[\"job_id\"]","    job = final_state.get_job(job_id)","    assert job is not None","    assert job.season == \"2024Q1\"","    assert job.status == JobStatus.QUEUED","    ","    await processor.stop()","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_error_handling(processor, action_queue):","    \"\"\"Test that processor handles errors gracefully.\"\"\"","    # Start processor","    await processor.start()","    ","    # Submit an invalid intent (missing required fields)","    # We'll create a malformed intent by directly manipulating a valid one","    from core.intents import CreateJobIntent, DataSpecIntent","    ","    # Create a data spec with empty symbols (should fail validation)","    invalid_data_spec = DataSpecIntent(","        dataset_id=\"test_dataset\",","        symbols=[],  # Empty - should fail validation","        timeframes=[\"60m\"],","        start_date=date(2020, 1, 1),","        end_date=date(2024, 12, 31)","    )","    ","    intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=invalid_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    intent_id = processor.submit_intent(intent)","    ","    # Wait for completion (should fail)","    completed = await processor.wait_for_intent(intent_id, timeout=5.0)","    assert completed is not None","    assert completed.status == IntentStatus.FAILED","    assert completed.error_message is not None","    assert \"validation\" in completed.error_message.lower() or \"empty\" in completed.error_message.lower()","    ","    # Check metrics","    state = processor.get_state()","    assert state.intent_queue.failed_count == 1","    ","    await processor.stop()","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_concurrent_submission(processor, action_queue, sample_data_spec):","    \"\"\"Test that processor handles concurrent intent submissions correctly.\"\"\"","    # Start processor","    await processor.start()","    ","    # Submit intents from multiple threads","    intent_ids = []","    ","    async def submit_intent(i: int):","        intent = CalculateUnitsIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        intent_id = processor.submit_intent(intent)","        intent_ids.append(intent_id)","        return intent_id","    ","    # Submit concurrently","    tasks = [submit_intent(i) for i in range(10)]","    await asyncio.gather(*tasks)"]}
{"type":"file_chunk","path":"tests/test_state_processor_serialization.py","chunk_index":1,"line_start":201,"line_end":354,"content":["    ","    # Wait for all to complete","    for intent_id in intent_ids:","        completed = await processor.wait_for_intent(intent_id, timeout=5.0)","        assert completed is not None","        assert completed.status == IntentStatus.COMPLETED","    ","    # All should be processed","    state = processor.get_state()","    assert state.intent_queue.completed_count == 10","    ","    await processor.stop()","","","def test_state_immutability():","    \"\"\"Test that SystemState is immutable (read-only).\"\"\"","    # Create initial state","    state = create_initial_state()","    ","    # Try to modify attributes (should fail or create new object)","    # Since Pydantic models with frozen=True raise ValidationError on modification","    with pytest.raises(Exception):","        state.metrics.total_jobs = 100  # Should fail","    ","    # Verify state hasn't changed","    assert state.metrics.total_jobs == 0","","","def test_state_snapshot_creation():","    \"\"\"Test creating state snapshots with updates.\"\"\"","    # Create initial state","    state = create_initial_state()","    ","    # Create snapshot with updates","    from core.state import create_state_snapshot, SystemMetrics","    ","    new_metrics = SystemMetrics(","        total_jobs=5,","        active_jobs=2,","        queued_jobs=3,","        completed_jobs=0,","        failed_jobs=0,","        total_units_processed=100,","        units_per_second=10.0,","        memory_usage_mb=512.0,","        cpu_usage_percent=25.0,","        disk_usage_gb=5.0,","        snapshot_timestamp=datetime.now(),","        uptime_seconds=3600.0","    )","    ","    new_state = create_state_snapshot(","        state,","        metrics=new_metrics,","        is_healthy=True","    )","    ","    # New state should have updated values","    assert new_state.metrics.total_jobs == 5","    assert new_state.metrics.active_jobs == 2","    assert new_state.is_healthy is True","    ","    # Original state should be unchanged","    assert state.metrics.total_jobs == 0","    assert state.metrics.active_jobs == 0","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_get_state_snapshot(processor):","    \"\"\"Test that get_state() returns consistent snapshots.\"\"\"","    # Start processor","    await processor.start()","    ","    # Get multiple state snapshots","    state1 = processor.get_state()","    await asyncio.sleep(0.1)","    state2 = processor.get_state()","    ","    # Snapshots should be different objects","    assert state1 is not state2","    assert state1.state_id != state2.state_id","    ","    # But should have same basic structure","    assert isinstance(state1, SystemState)","    assert isinstance(state2, SystemState)","    ","    await processor.stop()","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_queue_status_updates(processor, action_queue, sample_data_spec):","    \"\"\"Test that processor updates queue status in state.\"\"\"","    # Start processor","    await processor.start()","    ","    # Submit some intents","    for i in range(3):","        intent = CalculateUnitsIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        processor.submit_intent(intent)","    ","    # Wait a bit for processing to start","    await asyncio.sleep(0.5)","    ","    # Check queue status in state","    state = processor.get_state()","    assert state.intent_queue.queue_size >= 0","    assert state.intent_queue.completed_count >= 0","    ","    # Wait for all to complete","    await asyncio.sleep(2.0)","    ","    # Final state should show all completed","    final_state = processor.get_state()","    assert final_state.intent_queue.completed_count == 3","    ","    await processor.stop()","","","def test_processor_singleton():","    \"\"\"Test that get_processor() returns singleton instance.\"\"\"","    # Reset to ensure clean state","    reset_action_queue()","    ","    # First call should create instance","    processor1 = get_processor()","    assert processor1 is not None","    ","    # Second call should return same instance","    processor2 = get_processor()","    assert processor2 is processor1","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_stop_before_start():","    \"\"\"Test that processor can be stopped even if not started.\"\"\"","    from control.action_queue import ActionQueue","    from core.processor import StateProcessor","    ","    queue = ActionQueue()","    processor = StateProcessor(queue)","    ","    # Should not raise exception","    await processor.stop()","","","if __name__ == \"__main__\":","    # Run tests","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_state_processor_serialization.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_strategy_contract_purity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3435,"sha256":"d2ce83161e6bf0994ed3e0794396eaad44a609238fcf10846bf313f48b83c47f","total_lines":133,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_strategy_contract_purity.py","chunk_index":0,"line_start":1,"line_end":133,"content":["","\"\"\"Test strategy contract purity.","","Phase 7: Test that same input produces same output (deterministic).","\"\"\"","","from __future__ import annotations","","import numpy as np","import pytest","","from strategy.registry import get, load_builtin_strategies, clear","from engine.types import OrderIntent","","","@pytest.fixture(autouse=True)","def setup_registry() -> None:","    \"\"\"Setup registry before each test.\"\"\"","    clear()","    load_builtin_strategies()","    yield","    clear()","","","def test_sma_cross_purity() -> None:","    \"\"\"Test SMA cross strategy is deterministic.\"\"\"","    spec = get(\"sma_cross\")","    ","    # Create test features","    sma_fast = np.array([10.0, 11.0, 12.0, 13.0, 14.0])","    sma_slow = np.array([15.0, 14.0, 13.0, 12.0, 11.0])  # Cross at index 3","    ","    context = {","        \"bar_index\": 3,","        \"order_qty\": 1,","        \"features\": {","            \"sma_fast\": sma_fast,","            \"sma_slow\": sma_slow,","        },","    }","    ","    params = {","        \"fast_period\": 10.0,","        \"slow_period\": 20.0,","    }","    ","    # Run multiple times","    result1 = spec.fn(context, params)","    result2 = spec.fn(context, params)","    result3 = spec.fn(context, params)","    ","    # All results should be identical","    assert result1 == result2 == result3","    ","    # Check intents are identical","    intents1 = result1[\"intents\"]","    intents2 = result2[\"intents\"]","    intents3 = result3[\"intents\"]","    ","    assert len(intents1) == len(intents2) == len(intents3)","    ","    if len(intents1) > 0:","        # Compare intent attributes","        for i, (i1, i2, i3) in enumerate(zip(intents1, intents2, intents3)):","            assert i1.order_id == i2.order_id == i3.order_id","            assert i1.created_bar == i2.created_bar == i3.created_bar","            assert i1.role == i2.role == i3.role","            assert i1.kind == i2.kind == i3.kind","            assert i1.side == i2.side == i3.side","            assert i1.price == i2.price == i3.price","            assert i1.qty == i2.qty == i3.qty","","","def test_breakout_channel_purity() -> None:","    \"\"\"Test breakout channel strategy is deterministic.\"\"\"","    spec = get(\"breakout_channel\")","    ","    # Create test features","    high = np.array([100.0, 101.0, 102.0, 103.0, 105.0])","    close = np.array([99.0, 100.0, 101.0, 102.0, 104.0])","    channel_high = np.array([102.0, 102.0, 102.0, 102.0, 102.0])","    ","    context = {","        \"bar_index\": 4,","        \"order_qty\": 1,","        \"features\": {","            \"high\": high,","            \"close\": close,","            \"channel_high\": channel_high,","        },","    }","    ","    params = {","        \"channel_period\": 20.0,","    }","    ","    # Run multiple times","    result1 = spec.fn(context, params)","    result2 = spec.fn(context, params)","    ","    # Results should be identical","    assert result1 == result2","","","def test_mean_revert_zscore_purity() -> None:","    \"\"\"Test mean reversion z-score strategy is deterministic.\"\"\"","    spec = get(\"mean_revert_zscore\")","    ","    # Create test features","    zscore = np.array([-1.0, -1.5, -2.0, -2.5, -3.0])","    close = np.array([100.0, 99.0, 98.0, 97.0, 96.0])","    ","    context = {","        \"bar_index\": 2,","        \"order_qty\": 1,","        \"features\": {","            \"zscore\": zscore,","            \"close\": close,","        },","    }","    ","    params = {","        \"zscore_threshold\": -2.0,","    }","    ","    # Run multiple times","    result1 = spec.fn(context, params)","    result2 = spec.fn(context, params)","    ","    # Results should be identical","    assert result1 == result2","",""]}
{"type":"file_footer","path":"tests/test_strategy_contract_purity.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_strategy_registry.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4096,"sha256":"3cb58d0cd55cf2179f11f50b5df95394f7310b0f90066fb4ff76e1acb3eb597c","total_lines":172,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_strategy_registry.py","chunk_index":0,"line_start":1,"line_end":172,"content":["","\"\"\"Test strategy registry.","","Phase 7: Test registry list/get/register behavior is deterministic.","\"\"\"","","from __future__ import annotations","","import pytest","","from strategy.registry import (","    register,","    get,","    list_strategies,","    unregister,","    clear,","    load_builtin_strategies,",")","from strategy.spec import StrategySpec","","","def test_register_and_get() -> None:","    \"\"\"Test register and get operations.\"\"\"","    clear()","    ","    # Create a test strategy","    def test_fn(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {}}","    ","    spec = StrategySpec(","        strategy_id=\"test_strategy\",","        version=\"v1\",","        param_schema={\"type\": \"object\", \"properties\": {}},","        defaults={},","        fn=test_fn,","    )","    ","    # Register","    register(spec)","    ","    # Get","    retrieved = get(\"test_strategy\")","    assert retrieved.strategy_id == \"test_strategy\"","    assert retrieved.version == \"v1\"","    ","    # Cleanup","    unregister(\"test_strategy\")","","","def test_register_duplicate_raises() -> None:","    \"\"\"Test registering duplicate strategy_id raises ValueError.\"\"\"","    clear()","    ","    def test_fn1(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {}}","    ","    def test_fn2(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {\"different\": True}}","    ","    spec1 = StrategySpec(","        strategy_id=\"duplicate\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=test_fn1,","    )","    ","    spec2 = StrategySpec(","        strategy_id=\"duplicate\",","        version=\"v2\",","        param_schema={},","        defaults={},","        fn=test_fn2,","    )","    ","    register(spec1)","    ","    with pytest.raises(ValueError, match=\"already registered\"):","        register(spec2)","    ","    # Cleanup","    unregister(\"duplicate\")","","","def test_get_nonexistent_raises() -> None:","    \"\"\"Test getting nonexistent strategy raises KeyError.\"\"\"","    clear()","    ","    with pytest.raises(KeyError, match=\"not found\"):","        get(\"nonexistent\")","","","def test_list_strategies() -> None:","    \"\"\"Test list_strategies returns sorted list.\"\"\"","    clear()","    ","    def test_fn_a(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {\"fn\": \"a\"}}","    ","    def test_fn_b(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {\"fn\": \"b\"}}","    ","    def test_fn_c(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {\"fn\": \"c\"}}","    ","    # Register multiple strategies with different functions","    spec_b = StrategySpec(","        strategy_id=\"b_strategy\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=test_fn_b,","    )","    ","    spec_a = StrategySpec(","        strategy_id=\"a_strategy\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=test_fn_a,","    )","    ","    spec_c = StrategySpec(","        strategy_id=\"c_strategy\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=test_fn_c,","    )","    ","    register(spec_b)","    register(spec_a)","    register(spec_c)","    ","    # List should be sorted by strategy_id","    strategies = list_strategies()","    assert len(strategies) == 3","    assert strategies[0].strategy_id == \"a_strategy\"","    assert strategies[1].strategy_id == \"b_strategy\"","    assert strategies[2].strategy_id == \"c_strategy\"","    ","    # Cleanup","    clear()","","","def test_load_builtin_strategies() -> None:","    \"\"\"Test load_builtin_strategies registers built-in strategies.\"\"\"","    clear()","    ","    load_builtin_strategies()","    ","    strategies = list_strategies()","    strategy_ids = [s.strategy_id for s in strategies]","    ","    assert \"sma_cross\" in strategy_ids","    assert \"breakout_channel\" in strategy_ids","    assert \"mean_revert_zscore\" in strategy_ids","    ","    # Verify they can be retrieved","    sma_spec = get(\"sma_cross\")","    assert sma_spec.version == \"v1\"","    ","    breakout_spec = get(\"breakout_channel\")","    assert breakout_spec.version == \"v1\"","    ","    zscore_spec = get(\"mean_revert_zscore\")","    assert zscore_spec.version == \"v1\"","    ","    # Cleanup","    clear()","",""]}
{"type":"file_footer","path":"tests/test_strategy_registry.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_strategy_runner_outputs_intents.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3822,"sha256":"061705e5e9d3df47400ffa9a35cb3649fccb18d969f463af7d9b1347ad897dca","total_lines":143,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_strategy_runner_outputs_intents.py","chunk_index":0,"line_start":1,"line_end":143,"content":["","\"\"\"Test strategy runner outputs valid intents.","","Phase 7: Test that runner returns valid OrderIntent schema.","\"\"\"","","from __future__ import annotations","","import numpy as np","import pytest","","from strategy.runner import run_strategy","from strategy.registry import load_builtin_strategies, clear","from engine.types import OrderIntent, OrderRole, OrderKind, Side","","","@pytest.fixture(autouse=True)","def setup_registry() -> None:","    \"\"\"Setup registry before each test.\"\"\"","    clear()","    load_builtin_strategies()","    yield","    clear()","","","def test_runner_outputs_intents_schema() -> None:","    \"\"\"Test runner outputs valid OrderIntent schema.\"\"\"","    # Create test features","    sma_fast = np.array([10.0, 11.0, 12.0, 13.0, 14.0])","    sma_slow = np.array([15.0, 14.0, 13.0, 12.0, 11.0])","    ","    features = {","        \"sma_fast\": sma_fast,","        \"sma_slow\": sma_slow,","    }","    ","    params = {","        \"fast_period\": 10.0,","        \"slow_period\": 20.0,","    }","    ","    context = {","        \"bar_index\": 3,","        \"order_qty\": 1,","    }","    ","    # Run strategy","    intents = run_strategy(\"sma_cross\", features, params, context)","    ","    # Verify intents is a list","    assert isinstance(intents, list)","    ","    # Verify each intent is OrderIntent","    for intent in intents:","        assert isinstance(intent, OrderIntent)","        ","        # Verify required fields","        assert isinstance(intent.order_id, int)","        assert isinstance(intent.created_bar, int)","        assert isinstance(intent.role, OrderRole)","        assert isinstance(intent.kind, OrderKind)","        assert isinstance(intent.side, Side)","        assert isinstance(intent.price, float)","        assert isinstance(intent.qty, int)","        ","        # Verify values are reasonable","        assert intent.order_id > 0","        assert intent.created_bar >= 0","        assert intent.price > 0","        assert intent.qty > 0","","","def test_runner_uses_defaults() -> None:","    \"\"\"Test runner uses default parameters when missing.\"\"\"","    features = {","        \"sma_fast\": np.array([10.0, 11.0]),","        \"sma_slow\": np.array([15.0, 14.0]),","    }","    ","    # Missing params - should use defaults","    params = {}","    ","    context = {","        \"bar_index\": 1,","        \"order_qty\": 1,","    }","    ","    # Should not raise - defaults should be used","    intents = run_strategy(\"sma_cross\", features, params, context)","    assert isinstance(intents, list)","","","def test_runner_allows_extra_params() -> None:","    \"\"\"Test runner allows extra parameters (logs warning but doesn't fail).\"\"\"","    features = {","        \"sma_fast\": np.array([10.0, 11.0]),","        \"sma_slow\": np.array([15.0, 14.0]),","    }","    ","    # Extra param not in schema","    params = {","        \"fast_period\": 10.0,","        \"slow_period\": 20.0,","        \"extra_param\": 999.0,  # Not in schema","    }","    ","    context = {","        \"bar_index\": 1,","        \"order_qty\": 1,","    }","    ","    # Should not raise - extra params allowed","    intents = run_strategy(\"sma_cross\", features, params, context)","    assert isinstance(intents, list)","","","def test_runner_invalid_output_raises() -> None:","    \"\"\"Test runner raises ValueError for invalid strategy output.\"\"\"","    from strategy.registry import register","    from strategy.spec import StrategySpec","    ","    # Create a bad strategy that returns invalid output","    def bad_strategy(context: dict, params: dict) -> dict:","        return {\"invalid\": \"output\"}  # Missing \"intents\" key","    ","    bad_spec = StrategySpec(","        strategy_id=\"bad_strategy\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=bad_strategy,","    )","    ","    register(bad_spec)","    ","    with pytest.raises(ValueError, match=\"must contain 'intents' key\"):","        run_strategy(\"bad_strategy\", {}, {}, {\"bar_index\": 0})","    ","    # Cleanup","    from strategy.registry import unregister","    unregister(\"bad_strategy\")","",""]}
{"type":"file_footer","path":"tests/test_strategy_runner_outputs_intents.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_submit_returns_503_when_worker_missing.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":16245,"sha256":"54c37644c882d760292e890644f35c112c85f541450c13e02db81e5d5b4cc443","total_lines":422,"chunk_count":3}
{"type":"file_chunk","path":"tests/test_submit_returns_503_when_worker_missing.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Test that job submission returns HTTP 503 when worker is unavailable.","","EOOR500  HTTP 503 (WORKER-AWARE) requirement:","- All job submission endpoints must return HTTP 503 Service Unavailable when worker is unavailable","- Never return HTTP 500 for worker unavailability","- Error message must mention worker explicitly","- JSON response must include diagnostic details","\"\"\"","","from __future__ import annotations","","import tempfile","import os","from pathlib import Path","from unittest.mock import patch, MagicMock","","import pytest","from fastapi.testclient import TestClient","","from control.api import app, get_db_path","from control.jobs_db import init_db","","","@pytest.fixture","def test_client_no_worker() -> TestClient:","    \"\"\"Create test client with temporary database and no worker.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        db_path = Path(tmpdir) / \"test.db\"","        init_db(db_path)","        ","        # Save original environment variables","        original_jobs_db_path = os.environ.get(\"JOBS_DB_PATH\")","        original_allow_tmp_db = os.environ.get(\"FISHBRO_ALLOW_TMP_DB\")","        original_allow_spawn = os.environ.get(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\")","        ","        # Override DB path","        os.environ[\"JOBS_DB_PATH\"] = str(db_path)","        # Allow /tmp DB paths (required for temporary DB)","        os.environ[\"FISHBRO_ALLOW_TMP_DB\"] = \"1\"","        # DO NOT allow worker spawn in tests for this fixture (we want to test 503)","        os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = \"0\"","        ","        # Re-import to get new DB path","        from control import api","        ","        # Reinitialize","        api.init_db(db_path)","        ","        # Mock worker status to simulate no worker","        with patch('control.api._check_worker_status') as mock_check, \\","             patch('control.api.load_dataset_index') as mock_load_dataset:","            mock_check.return_value = {","                \"alive\": False,","                \"pid\": None,","                \"last_heartbeat_age_sec\": None,","                \"reason\": \"pidfile missing\",","                \"expected_db\": str(db_path),","            }","            # Mock dataset index to avoid FileNotFoundError","            from data.dataset_registry import DatasetIndex, DatasetRecord","            from datetime import date","            mock_index = DatasetIndex(","                generated_at=\"2024-01-01T00:00:00Z\",","                datasets=[","                    DatasetRecord(","                        id=\"test_dataset\",","                        symbol=\"TEST\",","                        exchange=\"TEST\",","                        timeframe=\"60m\",","                        path=\"TEST/60m/2020-2024.parquet\",","                        start_date=date(2020, 1, 1),","                        end_date=date(2024, 12, 31),","                        fingerprint_sha256_40=\"d\" * 40,","                        tz_provider=\"IANA\",","                        tz_version=\"unknown\",","                    )","                ]","            )","            mock_load_dataset.return_value = mock_index","            try:","                yield TestClient(app)","            finally:","                # Restore original environment variables","                if original_jobs_db_path is not None:","                    os.environ[\"JOBS_DB_PATH\"] = original_jobs_db_path","                else:","                    os.environ.pop(\"JOBS_DB_PATH\", None)","                if original_allow_tmp_db is not None:","                    os.environ[\"FISHBRO_ALLOW_TMP_DB\"] = original_allow_tmp_db","                else:","                    os.environ.pop(\"FISHBRO_ALLOW_TMP_DB\", None)","                if original_allow_spawn is not None:","                    os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = original_allow_spawn","                else:","                    os.environ.pop(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\", None)","","","@pytest.fixture","def test_client_with_worker() -> TestClient:","    \"\"\"Create test client with temporary database and worker running.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        db_path = Path(tmpdir) / \"test.db\"","        init_db(db_path)","        ","        # Save original environment variables","        original_jobs_db_path = os.environ.get(\"JOBS_DB_PATH\")","        original_allow_tmp_db = os.environ.get(\"FISHBRO_ALLOW_TMP_DB\")","        original_allow_spawn = os.environ.get(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\")","        ","        # Override DB path","        os.environ[\"JOBS_DB_PATH\"] = str(db_path)","        os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = \"1\"","        os.environ[\"FISHBRO_ALLOW_TMP_DB\"] = \"1\"","        ","        # Re-import to get new DB path","        from control import api","        ","        # Reinitialize","        api.init_db(db_path)","        ","        # Mock worker status to simulate worker running","        with patch('control.api._check_worker_status') as mock_check, \\","             patch('control.api.load_dataset_index') as mock_load_dataset:","            mock_check.return_value = {","                \"alive\": True,","                \"pid\": 12345,","                \"last_heartbeat_age_sec\": 1.0,","                \"reason\": \"worker alive\",","                \"expected_db\": str(db_path),","            }","            # Mock dataset index to avoid FileNotFoundError","            from data.dataset_registry import DatasetIndex, DatasetRecord","            from datetime import date","            mock_index = DatasetIndex(","                generated_at=\"2024-01-01T00:00:00Z\",","                datasets=[","                    DatasetRecord(","                        id=\"test_dataset\",","                        symbol=\"TEST\",","                        exchange=\"TEST\",","                        timeframe=\"60m\",","                        path=\"TEST/60m/2020-2024.parquet\",","                        start_date=date(2020, 1, 1),","                        end_date=date(2024, 12, 31),","                        fingerprint_sha256_40=\"d\" * 40,","                        tz_provider=\"IANA\",","                        tz_version=\"unknown\",","                    )","                ]","            )","            mock_load_dataset.return_value = mock_index","            try:","                yield TestClient(app)","            finally:","                # Restore original environment variables","                if original_jobs_db_path is not None:","                    os.environ[\"JOBS_DB_PATH\"] = original_jobs_db_path","                else:","                    os.environ.pop(\"JOBS_DB_PATH\", None)","                if original_allow_tmp_db is not None:","                    os.environ[\"FISHBRO_ALLOW_TMP_DB\"] = original_allow_tmp_db","                else:","                    os.environ.pop(\"FISHBRO_ALLOW_TMP_DB\", None)","                if original_allow_spawn is not None:","                    os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = original_allow_spawn","                else:","                    os.environ.pop(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\", None)","","","def test_submit_job_returns_503_when_worker_missing(test_client_no_worker: TestClient) -> None:","    \"\"\"Test POST /jobs returns 503 when worker is unavailable.\"\"\"","    req = {","        \"season\": \"test_season\",","        \"dataset_id\": \"test_dataset\",","        \"outputs_root\": \"outputs\",","        \"config_snapshot\": {\"bars\": 1000, \"params_total\": 100},","        \"config_hash\": \"abc123\",","        \"created_by\": \"b5c\",","    }","    ","    resp = test_client_no_worker.post(\"/jobs\", json=req)","    ","    # Must return HTTP 503, not 500","    assert resp.status_code == 503, f\"Expected 503, got {resp.status_code}\"","    ","    # Must have proper JSON structure","    data = resp.json()","    assert \"detail\" in data","    detail = data[\"detail\"]","    ","    # Error message must mention worker (detail is a dict with \"message\" field)","    assert isinstance(detail, dict)","    assert \"message\" in detail","    assert \"worker\" in detail[\"message\"].lower(), f\"Error message should mention worker: {detail['message']}\"","    ","    # Should not be generic 500 error","    assert \"internal server error\" not in detail[\"message\"].lower()","    ","    # Check response structure matches our error format","    assert \"error\" in detail"]}
{"type":"file_chunk","path":"tests/test_submit_returns_503_when_worker_missing.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    assert detail[\"error\"] == \"WORKER_UNAVAILABLE\"","    assert \"worker\" in detail","    assert \"action\" in detail","","","def test_batch_submit_returns_503_when_worker_missing(test_client_no_worker: TestClient) -> None:","    \"\"\"Test POST /jobs/batch returns 503 when worker is unavailable.\"\"\"","    from datetime import date","    ","    req = {","        \"jobs\": [","            {","                \"season\": \"test_season\",","                \"data1\": {","                    \"dataset_id\": \"test_dataset\",","                    \"start_date\": \"2020-01-01\",","                    \"end_date\": \"2024-12-31\"","                },","                \"data2\": None,","                \"strategy_id\": \"test_strategy\",","                \"params\": {},","                \"wfs\": {","                    \"stage0_subsample\": 1.0,","                    \"top_k\": 100,","                    \"mem_limit_mb\": 4096,","                    \"allow_auto_downsample\": True","                }","            }","        ]","    }","    ","    resp = test_client_no_worker.post(\"/jobs/batch\", json=req)","    ","    # Must return HTTP 503, not 500","    assert resp.status_code == 503, f\"Expected 503, got {resp.status_code}\"","    ","    # Must have proper JSON structure","    data = resp.json()","    assert \"detail\" in data","    detail = data[\"detail\"]","    ","    # Error message must mention worker (detail is a dict with \"message\" field)","    assert isinstance(detail, dict)","    assert \"message\" in detail","    assert \"worker\" in detail[\"message\"].lower(), f\"Error message should mention worker: {detail['message']}\"","    ","    # Should not be generic 500 error","    assert \"internal server error\" not in detail[\"message\"].lower()","    ","    # Check response structure matches our error format","    assert \"error\" in detail","    assert detail[\"error\"] == \"WORKER_UNAVAILABLE\"","","","def test_submit_job_succeeds_when_worker_running(test_client_with_worker: TestClient) -> None:","    \"\"\"Test POST /jobs succeeds when worker is available.\"\"\"","    req = {","        \"season\": \"test_season\",","        \"dataset_id\": \"test_dataset\",","        \"outputs_root\": \"outputs\",","        \"config_snapshot\": {\"bars\": 1000, \"params_total\": 100},","        \"config_hash\": \"abc123\",","        \"created_by\": \"b5c\",","    }","    ","    resp = test_client_with_worker.post(\"/jobs\", json=req)","    ","    # Should succeed (200 or 201)","    assert resp.status_code in (200, 201), f\"Expected success, got {resp.status_code}\"","    ","    data = resp.json()","    assert \"job_id\" in data","    assert isinstance(data[\"job_id\"], str)","","","def test_batch_submit_succeeds_when_worker_running(test_client_with_worker: TestClient) -> None:","    \"\"\"Test POST /jobs/batch succeeds when worker is available.\"\"\"","    from datetime import date","    ","    req = {","        \"jobs\": [","            {","                \"season\": \"test_season\",","                \"data1\": {","                    \"dataset_id\": \"test_dataset\",","                    \"start_date\": \"2020-01-01\",","                    \"end_date\": \"2024-12-31\"","                },","                \"data2\": None,","                \"strategy_id\": \"test_strategy\",","                \"params\": {},","                \"wfs\": {","                    \"stage0_subsample\": 1.0,","                    \"top_k\": 100,","                    \"mem_limit_mb\": 4096,","                    \"allow_auto_downsample\": True","                }","            }","        ]","    }","    ","    resp = test_client_with_worker.post(\"/jobs/batch\", json=req)","    ","    # Should succeed (200 or 201)","    assert resp.status_code in (200, 201), f\"Expected success, got {resp.status_code}\"","    ","    data = resp.json()","    assert \"batch_id\" in data","    assert isinstance(data[\"batch_id\"], str)","","","def test_worker_status_check_integration() -> None:","    \"\"\"Test that _check_worker_status function works correctly.\"\"\"","    from control.api import _check_worker_status","    from pathlib import Path","    ","    # Create a temporary DB path for testing","    db_path = Path(\"/tmp/test.db\")","    ","    # Mock the dependencies","    with patch('control.api.validate_pidfile') as mock_validate, \\","         patch('control.api.time.time') as mock_time, \\","         patch('control.api.Path.exists') as mock_exists, \\","         patch('control.api.Path.read_text') as mock_read_text, \\","         patch('control.api.Path.stat') as mock_stat:","        ","        # Test case 1: pidfile doesn't exist","        mock_exists.return_value = False","        result = _check_worker_status(db_path)","        assert not result[\"alive\"]","        assert result[\"reason\"] == \"pidfile missing\"","        ","        # Test case 2: pidfile exists but corrupted (read_text raises ValueError)","        mock_exists.return_value = True","        mock_validate.return_value = (True, \"\")  # pidfile is valid","        mock_read_text.side_effect = ValueError(\"invalid literal\")","        result = _check_worker_status(db_path)","        assert not result[\"alive\"]","        assert \"corrupted\" in result[\"reason\"]","        ","        # Test case 3: pidfile exists, validate_pidfile returns invalid","        mock_exists.return_value = True","        mock_validate.return_value = (False, \"pidfile stale\")","        # Clear any side effect from previous test","        mock_read_text.side_effect = None","        # read_text won't be called because validate_pidfile returns invalid","        result = _check_worker_status(db_path)","        assert not result[\"alive\"]","        assert result[\"reason\"] == \"pidfile stale\"","        ","        # Test case 4: pidfile exists, process alive, heartbeat stale","        mock_exists.return_value = True","        mock_read_text.side_effect = None  # Clear side effect","        mock_read_text.return_value = \"12345\"","        mock_validate.return_value = (True, \"\")","        # Mock stat for heartbeat file","        mock_stat_obj = MagicMock()","        mock_stat_obj.st_mtime = 1000.0","        mock_stat.return_value = mock_stat_obj","        mock_time.return_value = 2000.0  # Current time (1000 seconds later)","        result = _check_worker_status(db_path)","        assert result[\"alive\"]  # Process is alive","        assert result[\"last_heartbeat_age_sec\"] == 1000.0","        ","        # Test case 5: pidfile exists, process alive, heartbeat fresh","        mock_stat_obj = MagicMock()","        mock_stat_obj.st_mtime = 1995.0  # 5 seconds ago","        mock_stat.return_value = mock_stat_obj","        mock_time.return_value = 2000.0  # Current time","        result = _check_worker_status(db_path)","        assert result[\"alive\"]","        assert result[\"last_heartbeat_age_sec\"] == 5.0","","","def test_error_message_includes_diagnostics() -> None:","    \"\"\"Test that 503 error message includes diagnostic details.\"\"\"","    from control.api import require_worker_or_503","    from pathlib import Path","    import os","    ","    # Create a temporary DB path for testing","    db_path = Path(\"/tmp/test.db\")","    ","    # Mock _check_worker_status to return False with specific reason","    with patch('control.api._check_worker_status') as mock_check:","        mock_check.return_value = {","            \"alive\": False,","            \"pid\": None,","            \"last_heartbeat_age_sec\": None,","            \"reason\": \"pidfile missing\",","            \"expected_db\": str(db_path),","        }","        ","        # Ensure the environment variable does NOT allow spawn in tests","        original = os.environ.get(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\")","        os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = \"0\"","        try:","            # Should raise HTTPException with 503","            import fastapi","            try:"]}
{"type":"file_chunk","path":"tests/test_submit_returns_503_when_worker_missing.py","chunk_index":2,"line_start":401,"line_end":422,"content":["                require_worker_or_503(db_path)","                assert False, \"Should have raised HTTPException\"","            except fastapi.HTTPException as e:","                assert e.status_code == 503","                detail = e.detail","                # Check structure","                assert isinstance(detail, dict)","                assert \"error\" in detail","                assert detail[\"error\"] == \"WORKER_UNAVAILABLE\"","                assert \"worker\" in detail","                assert \"action\" in detail","                assert \"message\" in detail","                assert \"worker\" in detail[\"message\"].lower()","        finally:","            if original is not None:","                os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = original","            else:","                os.environ.pop(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\", None)","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_submit_returns_503_when_worker_missing.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"tests/test_trigger_rate_param_subsample_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":14011,"sha256":"c65f24b8d49cc43ffbf10358f8152582df4105b0250e749d7a800027c5dd7a96","total_lines":347,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_trigger_rate_param_subsample_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Stage P2-3: Contract Tests for Param-subsample Trigger Rate","","Verifies that trigger_rate controls param subsampling:","- selected_params_count scales with trigger_rate","- intents_total scales approximately linearly with trigger_rate","- Workload reduction is effective","\"\"\"","from __future__ import annotations","","import numpy as np","import os","","from pipeline.runner_grid import run_grid","","","def test_selected_params_count_reasonable() -> None:","    \"\"\"","    Test that selected_params_count is reasonable for given trigger_rate.","    ","    With n_params=1000 and trigger_rate=0.05, we expect selected_params_count","    to be approximately 50 (allowing rounding error).","    \"\"\"","    # Ensure clean environment","    old_param_subsample_rate = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","    old_param_subsample_seed = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","    ","    try:","        n_bars = 500","        n_params = 1000","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)","            atr_len = 10 + (i % 5)","            stop_mult = 1.0 + (i % 3) * 0.5","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Set param_subsample_rate=0.05","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"0.05\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"","        ","        result = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify perf dict contains trigger rate info","        assert \"perf\" in result, \"perf must exist in run_grid result\"","        perf = result[\"perf\"]","        assert isinstance(perf, dict), \"perf must be a dict\"","        ","        selected_params_count = perf.get(\"selected_params_count\")","        param_subsample_rate_configured = perf.get(\"param_subsample_rate_configured\")","        selected_params_ratio = perf.get(\"selected_params_ratio\")","        ","        assert selected_params_count is not None, \"selected_params_count must exist\"","        assert param_subsample_rate_configured is not None, \"param_subsample_rate_configured must exist\"","        assert selected_params_ratio is not None, \"selected_params_ratio must exist\"","        ","        assert param_subsample_rate_configured == 0.05, f\"param_subsample_rate_configured should be 0.05, got {param_subsample_rate_configured}\"","        ","        # Contract: selected_params_count should be approximately 5% of n_params","        # Allow rounding error: [45, 55] for n_params=1000, rate=0.05","        assert 45 <= selected_params_count <= 55, (","            f\"selected_params_count ({selected_params_count}) should be approximately 50 \"","            f\"(5% of {n_params}), got {selected_params_count}\"","        )","        ","        # Contract: selected_params_ratio should match trigger_rate approximately","        expected_ratio = 0.05","        assert 0.04 <= selected_params_ratio <= 0.06, (","            f\"selected_params_ratio ({selected_params_ratio}) should be approximately \"","            f\"{expected_ratio}, got {selected_params_ratio}\"","        )","        ","        # Contract: metrics_rows_computed should equal selected_params_count","        metrics_rows_computed = perf.get(\"metrics_rows_computed\")","        assert metrics_rows_computed == selected_params_count, (","            f\"metrics_rows_computed ({metrics_rows_computed}) should equal \"","            f\"selected_params_count ({selected_params_count})\"","        )","        ","    finally:","        # Restore environment","        if old_param_subsample_rate is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = old_param_subsample_rate","        ","        if old_param_subsample_seed is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = old_param_subsample_seed","","","def test_intents_total_linear_scaling() -> None:","    \"\"\"","    Test that intents_total scales approximately linearly with trigger_rate.","    ","    This verifies workload reduction: when we run 5% of params, intents_total","    should be approximately 5% of baseline.","    \"\"\"","    # Ensure clean environment","    old_param_subsample_rate = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","    old_param_subsample_seed = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","    ","    try:","        n_bars = 500","        n_params = 200","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)","            atr_len = 10 + (i % 5)","            stop_mult = 1.0 + (i % 3) * 0.5","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Run A: param_subsample_rate=1.0 (baseline, all params)","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"1.0\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"","        ","        result_a = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Run B: param_subsample_rate=0.05 (5% of params)","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"0.05\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"  # Same seed for deterministic selection","        ","        result_b = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify perf dicts","        perf_a = result_a.get(\"perf\", {})","        perf_b = result_b.get(\"perf\", {})","        ","        assert isinstance(perf_a, dict), \"perf_a must be a dict\"","        assert isinstance(perf_b, dict), \"perf_b must be a dict\"","        ","        intents_total_a = perf_a.get(\"intents_total\")","        intents_total_b = perf_b.get(\"intents_total\")","        ","        assert intents_total_a is not None, \"intents_total_a must exist\"","        assert intents_total_b is not None, \"intents_total_b must exist\"","        ","        # Contract: intents_total_B should be <= intents_total_A * 0.07 (allowing overhead)","        # With 5% params, we expect approximately 5% workload, but allow up to 7% for overhead","        if intents_total_a > 0:"]}
{"type":"file_chunk","path":"tests/test_trigger_rate_param_subsample_contract.py","chunk_index":1,"line_start":201,"line_end":347,"content":["            ratio = intents_total_b / intents_total_a","            assert ratio <= 0.07, (","                f\"intents_total_B ({intents_total_b}) should be <= intents_total_A * 0.07 \"","                f\"({intents_total_a * 0.07}), got ratio {ratio:.4f}\"","            )","        ","        # Verify selected_params_count scaling","        selected_count_a = perf_a.get(\"selected_params_count\", n_params)","        selected_count_b = perf_b.get(\"selected_params_count\")","        ","        assert selected_count_b is not None, \"selected_params_count_B must exist\"","        assert selected_count_b < selected_count_a, (","            f\"selected_params_count_B ({selected_count_b}) should be < \"","            f\"selected_params_count_A ({selected_count_a})\"","        )","        ","    finally:","        # Restore environment","        if old_param_subsample_rate is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = old_param_subsample_rate","        ","        if old_param_subsample_seed is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = old_param_subsample_seed","","","def test_metrics_shape_preserved() -> None:","    \"\"\"","    Test that metrics shape is preserved (n_params, METRICS_N_COLUMNS) even with subsampling.","    ","    Only selected rows should be computed; unselected rows remain zeros.","    Uses metrics_computed_mask to verify which rows were computed.","    \"\"\"","    # Ensure clean environment","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    old_param_subsample_rate = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","    old_param_subsample_seed = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","    ","    try:","        n_bars = 300","        n_params = 100","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)","            atr_len = 10 + (i % 5)","            stop_mult = 1.0","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Fix trigger_rate=1.0 (no intent-level sparsity) to test param subsample only","        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","        # Set param_subsample_rate=0.1 (10% of params)","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"0.1\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"","        ","        result = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify metrics shape is preserved","        metrics = result.get(\"metrics\")","        assert metrics is not None, \"metrics must exist\"","        assert isinstance(metrics, np.ndarray), \"metrics must be np.ndarray\"","        assert metrics.shape == (n_params, 3), (","            f\"metrics shape should be ({n_params}, 3), got {metrics.shape}\"","        )","        ","        # Verify perf dict","        perf = result.get(\"perf\", {})","        metrics_rows_computed = perf.get(\"metrics_rows_computed\")","        selected_params_count = perf.get(\"selected_params_count\")","        metrics_computed_mask = perf.get(\"metrics_computed_mask\")","        ","        assert metrics_rows_computed == selected_params_count, (","            f\"metrics_rows_computed ({metrics_rows_computed}) should equal \"","            f\"selected_params_count ({selected_params_count})\"","        )","        ","        # Verify metrics_computed_mask exists and has correct shape","        assert metrics_computed_mask is not None, \"metrics_computed_mask must exist in perf\"","        assert isinstance(metrics_computed_mask, list), \"metrics_computed_mask must be a list\"","        assert len(metrics_computed_mask) == n_params, (","            f\"metrics_computed_mask length ({len(metrics_computed_mask)}) should equal n_params ({n_params})\"","        )","        ","        # Convert to numpy array for easier manipulation","        mask_array = np.array(metrics_computed_mask, dtype=bool)","        ","        # Verify that mask sum equals selected_params_count","        assert np.sum(mask_array) == selected_params_count, (","            f\"metrics_computed_mask sum ({np.sum(mask_array)}) should equal \"","            f\"selected_params_count ({selected_params_count})\"","        )","        ","        # Verify that uncomputed rows remain all zeros","        uncomputed_non_zero = np.sum(np.any(np.abs(metrics[~mask_array]) > 1e-10, axis=1))","        assert uncomputed_non_zero == 0, (","            f\"Uncomputed rows with non-zero metrics ({uncomputed_non_zero}) should be 0\"","        )","        ","        # NOTE: Do NOT require computed rows to be non-zero.","        # It's valid to have entry fills but no exits (trades=0), producing all-zero metrics.","        # Evidence of computation is provided by metrics_rows_computed == selected_params_count","        # and the metrics_computed_mask bookkeeping above.","        ","    finally:","        # Restore environment","        if old_trigger_rate is None:","            os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","        ","        if old_param_subsample_rate is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = old_param_subsample_rate","        ","        if old_param_subsample_seed is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = old_param_subsample_seed","",""]}
{"type":"file_footer","path":"tests/test_trigger_rate_param_subsample_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_ui_artifact_validation.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":19043,"sha256":"7290feedadbee1e240b8d7ab6b12191d04bd3acf239038d77ef80060a5dddc48","total_lines":541,"chunk_count":3}
{"type":"file_chunk","path":"tests/test_ui_artifact_validation.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Tests for UI artifact validation.","","Tests verify:","1. MISSING status when file does not exist","2. INVALID status when schema validation fails (with readable error messages)","3. DIRTY status when config_hash mismatch","4. OK status when validation passes","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","","import pytest","","from core.artifact_reader import ReadResult, SafeReadResult, try_read_artifact","from core.artifact_status import (","    ArtifactStatus,","    ValidationResult,","    validate_governance_status,","    validate_manifest_status,","    validate_winners_v2_status,",")","from core.schemas.governance import GovernanceReport","from core.schemas.manifest import RunManifest","from core.schemas.winners_v2 import WinnersV2","from gui.viewer.schema import EvidenceLink","","","# Fixtures","@pytest.fixture","def fixtures_dir() -> Path:","    \"\"\"Return path to test fixtures directory.\"\"\"","    return Path(__file__).parent / \"fixtures\" / \"artifacts\"","","","# Note: temp_dir fixture is now defined in conftest.py for all tests","# This local definition is kept for backward compatibility but will be shadowed by conftest.py","","","# Test: MISSING status","def test_manifest_missing_file(temp_dir: Path) -> None:","    \"\"\"Test that missing manifest.json returns MISSING status.\"\"\"","    manifest_path = temp_dir / \"manifest.json\"","    ","    result = validate_manifest_status(str(manifest_path))","    ","    assert result.status == ArtifactStatus.MISSING","    assert \"\" in result.message or \"not found\" in result.message.lower()","","","def test_winners_v2_missing_file(temp_dir: Path) -> None:","    \"\"\"Test that missing winners_v2.json returns MISSING status.\"\"\"","    winners_path = temp_dir / \"winners_v2.json\"","    ","    result = validate_winners_v2_status(str(winners_path))","    ","    assert result.status == ArtifactStatus.MISSING","    assert \"\" in result.message or \"not found\" in result.message.lower()","","","def test_governance_missing_file(temp_dir: Path) -> None:","    \"\"\"Test that missing governance.json returns MISSING status.\"\"\"","    governance_path = temp_dir / \"governance.json\"","    ","    result = validate_governance_status(str(governance_path))","    ","    assert result.status == ArtifactStatus.MISSING","    assert \"\" in result.message or \"not found\" in result.message.lower()","","","# Test: INVALID status (schema validation errors)","def test_manifest_invalid_missing_field(fixtures_dir: Path) -> None:","    \"\"\"Test that manifest with missing required field returns INVALID.\"\"\"","    manifest_path = fixtures_dir / \"manifest_missing_field.json\"","    ","    # Load data","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        manifest_data = json.load(f)","    ","    result = validate_manifest_status(str(manifest_path), manifest_data=manifest_data)","    ","    assert result.status == ArtifactStatus.INVALID","    assert \"\" in result.message or \"missing\" in result.message.lower() or \"required\" in result.message.lower()","    # Should mention config_hash or season (required fields)","    assert \"config_hash\" in result.message or \"season\" in result.message or \"run_id\" in result.message","","","def test_winners_v2_invalid_missing_field(fixtures_dir: Path) -> None:","    \"\"\"Test that winners_v2 with missing required field returns INVALID.\"\"\"","    winners_path = fixtures_dir / \"winners_v2_missing_field.json\"","    ","    # Load data","    with winners_path.open(\"r\", encoding=\"utf-8\") as f:","        winners_data = json.load(f)","    ","    result = validate_winners_v2_status(str(winners_path), winners_data=winners_data)","    ","    assert result.status == ArtifactStatus.INVALID","    assert \"\" in result.message or \"missing\" in result.message.lower() or \"required\" in result.message.lower()","    # Should mention net_profit, max_drawdown, or trades (required in WinnerRow)","    assert any(field in result.message for field in [\"net_profit\", \"max_drawdown\", \"trades\", \"metrics\"])","","","def test_governance_invalid_missing_field(temp_dir: Path) -> None:","    \"\"\"Test that governance with missing required field returns INVALID.\"\"\"","    governance_path = temp_dir / \"governance.json\"","    ","    # Create invalid governance (missing run_id)","    invalid_data = {","        \"items\": [","            {","                \"candidate_id\": \"test:123\",","                \"decision\": \"KEEP\",","            }","        ]","    }","    ","    with governance_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(invalid_data, f)","    ","    result = validate_governance_status(str(governance_path), governance_data=invalid_data)","    ","    assert result.status == ArtifactStatus.INVALID","    assert \"\" in result.message or \"missing\" in result.message.lower() or \"required\" in result.message.lower()","","","# Test: DIRTY status (config_hash mismatch)","def test_manifest_dirty_config_hash(fixtures_dir: Path) -> None:","    \"\"\"Test that manifest with mismatched config_hash returns DIRTY.\"\"\"","    manifest_path = fixtures_dir / \"manifest_valid.json\"","    ","    # Load data","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        manifest_data = json.load(f)","    ","    # Validate with different expected config_hash","    result = validate_manifest_status(","        str(manifest_path),","        manifest_data=manifest_data,","        expected_config_hash=\"different_hash\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"config_hash\" in result.message.lower()","","","def test_winners_v2_dirty_config_hash(temp_dir: Path) -> None:","    \"\"\"Test that winners_v2 with mismatched config_hash returns DIRTY.\"\"\"","    winners_path = temp_dir / \"winners_v2.json\"","    ","    # Create winners with config_hash at top level","    winners_data = {","        \"config_hash\": \"abc123\",","        \"schema\": \"v2\",","        \"stage_name\": \"stage1_topk\",","        \"topk\": [","            {","                \"candidate_id\": \"donchian_atr:123\",","                \"strategy_id\": \"donchian_atr\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe\": \"60m\",","                \"params\": {},","                \"metrics\": {","                    \"net_profit\": 100.0,","                    \"max_dd\": -10.0,","                    \"trades\": 10,","                },","            }","        ],","    }","    ","    with winners_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(winners_data, f)","    ","    result = validate_winners_v2_status(","        str(winners_path),","        winners_data=winners_data,","        expected_config_hash=\"different_hash\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"config_hash\" in result.message.lower()","    assert \"winners_v2.config_hash\" in result.message  # Should reference top-level field","","","def test_governance_dirty_config_hash(temp_dir: Path) -> None:","    \"\"\"Test that governance with mismatched config_hash returns DIRTY.\"\"\"","    governance_path = temp_dir / \"governance.json\"","    ","    # Create governance with config_hash at top level","    governance_data = {","        \"config_hash\": \"abc123\",","        \"run_id\": \"test-run-123\",","        \"items\": [","            {","                \"candidate_id\": \"donchian_atr:123\",","                \"strategy_id\": \"donchian_atr\","]}
{"type":"file_chunk","path":"tests/test_ui_artifact_validation.py","chunk_index":1,"line_start":201,"line_end":400,"content":["                \"decision\": \"KEEP\",","                \"rule_id\": \"R1\",","                \"reason\": \"Test\",","                \"run_id\": \"test-run-123\",","                \"stage\": \"stage1_topk\",","                \"evidence\": [],","                \"key_metrics\": {},","            }","        ],","        \"metadata\": {},","    }","    ","    with governance_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(governance_data, f)","    ","    result = validate_governance_status(","        str(governance_path),","        governance_data=governance_data,","        expected_config_hash=\"different_hash\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"config_hash\" in result.message.lower()","    assert \"governance.config_hash\" in result.message  # Should reference top-level field","","","# Test: OK status (validation passes)","def test_manifest_ok(fixtures_dir: Path) -> None:","    \"\"\"Test that valid manifest returns OK status.\"\"\"","    manifest_path = fixtures_dir / \"manifest_valid.json\"","    ","    # Load data","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        manifest_data = json.load(f)","    ","    result = validate_manifest_status(","        str(manifest_path),","        manifest_data=manifest_data,","        expected_config_hash=\"abc123def456\",","    )","    ","    assert result.status == ArtifactStatus.OK","    assert \"\" in result.message or \"ok\" in result.message.lower()","","","def test_winners_v2_ok(fixtures_dir: Path) -> None:","    \"\"\"Test that valid winners_v2 returns OK status.\"\"\"","    winners_path = fixtures_dir / \"winners_v2_valid.json\"","    ","    # Load data","    with winners_path.open(\"r\", encoding=\"utf-8\") as f:","        winners_data = json.load(f)","    ","    result = validate_winners_v2_status(str(winners_path), winners_data=winners_data)","    ","    assert result.status == ArtifactStatus.OK","    assert \"\" in result.message or \"ok\" in result.message.lower()","","","def test_governance_ok(fixtures_dir: Path) -> None:","    \"\"\"Test that valid governance returns OK status.\"\"\"","    governance_path = fixtures_dir / \"governance_valid.json\"","    ","    # Load data","    with governance_path.open(\"r\", encoding=\"utf-8\") as f:","        governance_data = json.load(f)","    ","    result = validate_governance_status(","        str(governance_path),","        governance_data=governance_data,","        expected_config_hash=\"abc123def456\",","    )","    ","    assert result.status == ArtifactStatus.OK","    assert \"\" in result.message or \"ok\" in result.message.lower()","","","# Test: Phase 6.5 - Missing fingerprint must be DIRTY (Binding Constraint)","def test_manifest_missing_fingerprint_is_dirty(fixtures_dir: Path) -> None:","    \"\"\"Test that manifest without data_fingerprint_sha1 is marked DIRTY.","    ","    Binding Constraint: This test locks down the requirement that","    data_fingerprint_sha1 must be present and non-empty.","    Prevents future changes from making fingerprint optional.","    \"\"\"","    manifest_path = fixtures_dir / \"manifest_valid.json\"","    ","    # Load data and remove fingerprint","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        data = json.load(f)","    data.pop(\"data_fingerprint_sha1\", None)","    ","    result = validate_manifest_status(","        str(manifest_path),","        manifest_data=data,","        expected_config_hash=\"abc123def456\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"fingerprint\" in result.message.lower() or \"untrustworthy\" in result.message.lower()","","","def test_manifest_empty_fingerprint_is_dirty(fixtures_dir: Path) -> None:","    \"\"\"Test that manifest with empty data_fingerprint_sha1 is marked DIRTY.\"\"\"","    manifest_path = fixtures_dir / \"manifest_valid.json\"","    ","    # Load data and set fingerprint to empty string","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        data = json.load(f)","    data[\"data_fingerprint_sha1\"] = \"\"","    ","    result = validate_manifest_status(","        str(manifest_path),","        manifest_data=data,","        expected_config_hash=\"abc123def456\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"fingerprint\" in result.message.lower() or \"untrustworthy\" in result.message.lower()","","","def test_governance_missing_fingerprint_is_dirty(fixtures_dir: Path) -> None:","    \"\"\"Test that governance without data_fingerprint_sha1 in metadata is marked DIRTY.","    ","    Binding Constraint: This test locks down the requirement that","    data_fingerprint_sha1 must be present in governance metadata and non-empty.","    \"\"\"","    governance_path = fixtures_dir / \"governance_valid.json\"","    ","    # Load data and remove fingerprint from metadata","    with governance_path.open(\"r\", encoding=\"utf-8\") as f:","        data = json.load(f)","    ","    if \"metadata\" in data:","        data[\"metadata\"].pop(\"data_fingerprint_sha1\", None)","    else:","        data[\"metadata\"] = {}","    ","    result = validate_governance_status(","        str(governance_path),","        governance_data=data,","        expected_config_hash=\"abc123def456\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"fingerprint\" in result.message.lower() or \"untrustworthy\" in result.message.lower()","","","def test_governance_empty_fingerprint_is_dirty(fixtures_dir: Path) -> None:","    \"\"\"Test that governance with empty data_fingerprint_sha1 in metadata is marked DIRTY.\"\"\"","    governance_path = fixtures_dir / \"governance_valid.json\"","    ","    # Load data and set fingerprint to empty string in metadata","    with governance_path.open(\"r\", encoding=\"utf-8\") as f:","        data = json.load(f)","    ","    if \"metadata\" not in data:","        data[\"metadata\"] = {}","    data[\"metadata\"][\"data_fingerprint_sha1\"] = \"\"","    ","    result = validate_governance_status(","        str(governance_path),","        governance_data=data,","        expected_config_hash=\"abc123def456\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"fingerprint\" in result.message.lower() or \"untrustworthy\" in result.message.lower()","","","# Test: ArtifactReader (safe version)","def test_try_read_artifact_json(fixtures_dir: Path) -> None:","    \"\"\"Test reading JSON artifact with safe version.\"\"\"","    manifest_path = fixtures_dir / \"manifest_valid.json\"","    ","    result = try_read_artifact(manifest_path)","    ","    assert isinstance(result, SafeReadResult)","    assert result.is_ok","    assert result.result is not None","    assert isinstance(result.result.raw, dict)","    assert result.result.meta.source_path == str(manifest_path.resolve())","    assert len(result.result.meta.sha256) == 64  # SHA256 hex length","    assert result.result.meta.mtime_s > 0","","","def test_try_read_artifact_missing_file(temp_dir: Path) -> None:","    \"\"\"Test that reading missing file returns error, never raises.\"\"\"","    missing_path = temp_dir / \"missing.json\"","    ","    result = try_read_artifact(missing_path)","    ","    assert isinstance(result, SafeReadResult)","    assert result.is_error","    assert result.error is not None","    assert result.error.error_code == \"FILE_NOT_FOUND\"","    assert \"not found\" in result.error.message.lower()","","","# Test: EvidenceLink"]}
{"type":"file_chunk","path":"tests/test_ui_artifact_validation.py","chunk_index":2,"line_start":401,"line_end":541,"content":["def test_evidence_link() -> None:","    \"\"\"Test EvidenceLink BaseModel.\"\"\"","    link = EvidenceLink(","        artifact=\"winners_v2\",","        json_pointer=\"/rows/0/net_profit\",","        description=\"Net profit from winners\",","    )","    ","    assert link.artifact == \"winners_v2\"","    assert link.json_pointer == \"/rows/0/net_profit\"","    assert link.description == \"Net profit from winners\"","    ","    # Test with None description","    link2 = EvidenceLink(","        artifact=\"governance\",","        json_pointer=\"/scoring/final_score\",","    )","    assert link2.artifact == \"governance\"","    assert link2.json_pointer == \"/scoring/final_score\"","    assert link2.description is None","","","# Test: Pydantic schemas can parse valid data","def test_manifest_schema_parse(fixtures_dir: Path) -> None:","    \"\"\"Test that RunManifest can parse valid manifest.\"\"\"","    manifest_path = fixtures_dir / \"manifest_valid.json\"","    ","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        manifest_data = json.load(f)","    ","    manifest = RunManifest(**manifest_data)","    ","    assert manifest.run_id == \"test-run-123\"","    assert manifest.season == \"2025Q4\"","    assert manifest.config_hash == \"abc123def456\"","    assert len(manifest.stages) == 1","    assert manifest.stages[0].name == \"stage0\"","","","def test_winners_v2_schema_parse(fixtures_dir: Path) -> None:","    \"\"\"Test that WinnersV2 can parse valid winners.\"\"\"","    winners_path = fixtures_dir / \"winners_v2_valid.json\"","    ","    with winners_path.open(\"r\", encoding=\"utf-8\") as f:","        winners_data = json.load(f)","    ","    winners = WinnersV2(**winners_data)","    ","    assert winners.schema_name == \"v2\"  # schema_name is alias for \"schema\" in JSON","    assert winners.stage_name == \"stage1_topk\"","    assert winners.topk is not None","    assert len(winners.topk) == 1","","","def test_governance_schema_parse(fixtures_dir: Path) -> None:","    \"\"\"Test that GovernanceReport can parse valid governance.\"\"\"","    governance_path = fixtures_dir / \"governance_valid.json\"","    ","    with governance_path.open(\"r\", encoding=\"utf-8\") as f:","        governance_data = json.load(f)","    ","    governance = GovernanceReport(**governance_data)","    ","    assert governance.run_id == \"test-run-123\"","    assert governance.items is not None","    assert len(governance.items) == 1","","","# Test: EvidenceLinkModel render_hint (PR-A)","def test_evidence_link_model_backward_compatibility() -> None:","    \"\"\"Test that EvidenceLinkModel can parse old data without render_hint.\"\"\"","    from core.schemas.governance import EvidenceLinkModel","    ","    # Old data format (without render_hint)","    old_data = {","        \"source_path\": \"winners_v2.json\",","        \"json_pointer\": \"/rows/0/net_profit\",","        \"note\": \"Net profit from winners\",","    }","    ","    # Should parse successfully with default render_hint=\"highlight\"","    link = EvidenceLinkModel(**old_data)","    ","    assert link.source_path == \"winners_v2.json\"","    assert link.json_pointer == \"/rows/0/net_profit\"","    assert link.note == \"Net profit from winners\"","    assert link.render_hint == \"highlight\"  # Default value","    assert link.render_payload == {}  # Default empty dict","","","def test_evidence_link_model_with_render_hint() -> None:","    \"\"\"Test that EvidenceLinkModel can parse new data with render_hint.\"\"\"","    from core.schemas.governance import EvidenceLinkModel","    ","    # New data format (with render_hint) - using allowed value","    new_data = {","        \"source_path\": \"winners_v2.json\",","        \"json_pointer\": \"/rows/0/net_profit\",","        \"note\": \"Net profit from winners\",","        \"render_hint\": \"highlight\",","        \"render_payload\": {\"start_idx\": 0, \"end_idx\": 0},","    }","    ","    link = EvidenceLinkModel(**new_data)","    ","    assert link.source_path == \"winners_v2.json\"","    assert link.json_pointer == \"/rows/0/net_profit\"","    assert link.note == \"Net profit from winners\"","    assert link.render_hint == \"highlight\"","    assert link.render_payload == {\"start_idx\": 0, \"end_idx\": 0}","","","def test_evidence_link_model_roundtrip() -> None:","    \"\"\"Test that EvidenceLinkModel can roundtrip through JSON.\"\"\"","    from core.schemas.governance import EvidenceLinkModel","    ","    # Create model with render_hint - using allowed value","    link = EvidenceLinkModel(","        source_path=\"governance.json\",","        json_pointer=\"/rows/0/decision\",","        note=\"Decision evidence\",","        render_hint=\"diff\",","        render_payload={\"lhs_pointer\": \"/rows/0/decision\", \"rhs_pointer\": \"/rows/0/decision\"},","    )","    ","    # Convert to dict","    link_dict = link.model_dump()","    ","    # Roundtrip: dict -> JSON -> dict -> model","    json_str = json.dumps(link_dict)","    link_dict_roundtrip = json.loads(json_str)","    link_roundtrip = EvidenceLinkModel(**link_dict_roundtrip)","    ","    # Verify all fields preserved","    assert link_roundtrip.source_path == link.source_path","    assert link_roundtrip.json_pointer == link.json_pointer","    assert link_roundtrip.note == link.note","    assert link_roundtrip.render_hint == link.render_hint","    assert link_roundtrip.render_payload == link.render_payload","",""]}
{"type":"file_footer","path":"tests/test_ui_artifact_validation.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"tests/test_ui_race_condition_headless.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":14442,"sha256":"8050434961467dd92b50d14bac87ad8342483b4477cc2889ca1b3130ed3dcaba","total_lines":376,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_ui_race_condition_headless.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Test UI race condition defense for Attack #9  Headless Intent-State Contract.","","Tests that UI cannot cause race conditions because:","1. UI only creates intents (no business logic)","2. All intents go through single ActionQueue","3. StateProcessor is single consumer (sequential execution)","4. UI only reads SystemState snapshots (immutable)","\"\"\"","","import pytest","import asyncio","import threading","import contextlib","from datetime import date","import anyio","from anyio import create_task_group, sleep","","from core.intents import (","    CreateJobIntent, CalculateUnitsIntent, DataSpecIntent,","    IntentStatus",")","from control.action_queue import ActionQueue, reset_action_queue","from core.processor import StateProcessor, reset_processor","from gui.adapters.intent_bridge import IntentBridge, get_intent_bridge","","","@pytest.fixture","def clean_system():","    \"\"\"Clean system state before each test.\"\"\"","    reset_action_queue()","    reset_processor()","    yield","    reset_action_queue()","    reset_processor()","","","@pytest.fixture","def sample_data_spec():","    \"\"\"Create a sample DataSpecIntent for testing.\"\"\"","    return DataSpecIntent(","        dataset_id=\"test_dataset\",","        symbols=[\"MNQ\", \"MXF\"],","        timeframes=[\"60m\", \"120m\"],","        start_date=date(2020, 1, 1),","        end_date=date(2024, 12, 31)","    )","","","@contextlib.asynccontextmanager","async def managed_processor(queue):","    \"\"\"","    Context manager to ensure processor is stopped even if tests fail.","    This prevents hangs caused by unclosed background tasks.","    ","    CRITICAL: Includes a hard timeout shield for stop() to prevent CI hangs.","    \"\"\"","    processor = StateProcessor(queue)","    await processor.start()","    try:","        yield processor","    finally:","        # Force stop with hard timeout to prevent deadlock in teardown.","        # If the processor code is buggy and hangs on stop, we kill it here","        # so pytest can finish and report the results.","        try:","            await asyncio.wait_for(processor.stop(), timeout=2.0)","        except (asyncio.TimeoutError, Exception) as e:","            print(f\"WARNING: Processor stop forced/timed out: {e}\")","","","def test_ui_only_creates_intents():","    \"\"\"Test that UI code only creates intents, doesn't execute logic.\"\"\"","    bridge = IntentBridge()","    ","    # UI creates intents through bridge","    data_spec = bridge.create_data_spec_intent(","        dataset_id=\"test\",","        symbols=[\"MNQ\"],","        timeframes=[\"60m\"]","    )","    ","    intent = bridge.create_job_intent(","        season=\"2024Q1\",","        data1=data_spec,","        data2=None,","        strategy_id=\"test\",","        params={}","    )","    ","    # Intent should be created but not executed","    assert intent.intent_type.value == \"create_job\"","    assert intent.status == IntentStatus.PENDING","    assert intent.result is None","","","@pytest.mark.anyio","async def test_single_consumer_sequential(clean_system, sample_data_spec):","    \"\"\"Test that StateProcessor is single consumer (sequential execution).\"\"\"","    with anyio.fail_after(10):","        queue = ActionQueue()","        ","        async with managed_processor(queue) as processor:","            # Submit multiple intents","            intent_ids = []","            for i in range(5):","                intent = CalculateUnitsIntent(","                    season=f\"2024Q{i}\",","                    data1=sample_data_spec,","                    data2=None,","                    strategy_id=\"sma_cross_v1\",","                    params={\"window_fast\": i}","                )","                intent_id = queue.submit(intent)","                intent_ids.append(intent_id)","            ","            # Wait for all to complete","            for intent_id in intent_ids:","                completed = await queue.wait_for_intent_async(intent_id, timeout=5.0)","                assert completed is not None","                # Even if they fail due to missing data, they are 'processed'","                assert completed.status in (IntentStatus.COMPLETED, IntentStatus.FAILED)","            ","            # Check that they were processed","            metrics = queue.get_metrics()","            assert metrics[\"processed\"] == 5","","","@pytest.mark.anyio","async def test_concurrent_ui_submissions(clean_system, sample_data_spec):","    \"\"\"Test that concurrent UI submissions don't cause race conditions.\"\"\"","    with anyio.fail_after(10):","        queue = ActionQueue()","        ","        async with managed_processor(queue) as processor:","            # Simulate multiple UI threads submitting intents concurrently","            results = []","            errors = []","            ","            async def ui_thread_submit(thread_id: int):","                \"\"\"Simulate a UI thread submitting intents.\"\"\"","                try:","                    # UI creates intent","                    intent = CalculateUnitsIntent(","                        season=f\"2024Q{thread_id}\",","                        data1=sample_data_spec,","                        data2=None,","                        strategy_id=\"sma_cross_v1\",","                        params={\"window_fast\": thread_id}","                    )","                    ","                    # Submit to queue","                    intent_id = queue.submit(intent)","                    ","                    # Wait for result","                    completed = await queue.wait_for_intent_async(intent_id, timeout=5.0)","                    if completed and completed.status in (IntentStatus.COMPLETED, IntentStatus.FAILED):","                        results.append((thread_id, completed.result))","                    else:","                        errors.append((thread_id, \"Failed or Timed out\"))","                        ","                except Exception as e:","                    errors.append((thread_id, str(e)))","            ","            # Launch multiple UI threads","            tasks = [ui_thread_submit(i) for i in range(10)]","            await asyncio.gather(*tasks)","            ","            # All should succeed without race conditions","            assert len(errors) == 0","            assert len(results) == 10","            ","            # Check that queue processed all intents","            metrics = queue.get_metrics()","            assert metrics[\"processed\"] == 10","","","def test_immutable_state_snapshots():","    \"\"\"Test that UI only reads immutable state snapshots.\"\"\"","    from core.state import create_initial_state","    ","    # Create initial state","    state = create_initial_state()","    ","    # UI reads state (this is allowed)","    total_jobs = state.metrics.total_jobs","    is_healthy = state.is_healthy","    ","    # UI cannot modify state (should raise exception or rely on dataclass frozen=True)","    # Note: If models aren't frozen, we rely on convention/architecture","    # But here we verify the read values are correct","    assert state.metrics.total_jobs == total_jobs","    assert state.is_healthy == is_healthy","","","@pytest.mark.anyio","async def test_state_consistency_during_concurrent_reads(clean_system, sample_data_spec):","    \"\"\"Test that concurrent state reads are consistent.\"\"\"","    with anyio.fail_after(10):","        queue = ActionQueue()","        "]}
{"type":"file_chunk","path":"tests/test_ui_race_condition_headless.py","chunk_index":1,"line_start":201,"line_end":376,"content":["        async with managed_processor(queue) as processor:","            # Submit a job to change state","            intent = CreateJobIntent(","                season=\"2024Q1\",","                data1=sample_data_spec,","                data2=None,","                strategy_id=\"sma_cross_v1\",","                params={\"window_fast\": 10}","            )","            ","            intent_id = processor.submit_intent(intent)","            ","            # Multiple UI threads reading state concurrently","            read_values = []","            ","            async def ui_thread_read(thread_id: int):","                \"\"\"UI thread reads state.\"\"\"","                # Get state snapshot","                state = processor.get_state()","                read_values.append((thread_id, state.metrics.total_jobs))","            ","            # Launch concurrent reads","            tasks = [ui_thread_read(i) for i in range(10)]","            await asyncio.gather(*tasks)","            ","            # All reads should see consistent state","            # (either all see initial state or all see updated state, usually initial)","            unique_values = set(value for _, value in read_values)","            assert len(unique_values) == 1","            ","            # Wait for intent to complete","            completed_intent = await processor.wait_for_intent(intent_id, timeout=5.0)","            ","            # Handle timeout case (wait_for_intent returns None on timeout)","            if completed_intent is None:","                # Debug: get current state to understand what happened","                st = processor.get_state()","                print(\"DEBUG: wait_for_intent timed out; state:\", st)","                pytest.fail(\"wait_for_intent timed out (must never hang; must resolve or fail deterministically)\")","            ","            # Now check final state","            final_state = processor.get_state()","            ","            # NOTE: In headless test without real data, job creation will FAIL validation.","            # So total_jobs will remain 0, but failed_count in queue should increase.","            # We check intent status to determine what to expect.","            if completed_intent.status == IntentStatus.FAILED:","                # Expect failure recorded in queue metrics, but not necessarily in business metrics (total_jobs)","                assert final_state.intent_queue.failed_count >= 1","            else:","                # If by some miracle it passed validation (e.g. if mocked)","                assert final_state.metrics.total_jobs == 1","","","def test_intent_bridge_singleton():","    \"\"\"Test that IntentBridge is singleton.\"\"\"","    bridge1 = get_intent_bridge()","    bridge2 = get_intent_bridge()","    ","    assert bridge1 is bridge2","","","@pytest.mark.anyio","async def test_bridge_concurrent_usage(clean_system):","    \"\"\"Test IntentBridge with concurrent UI access.\"\"\"","    with anyio.fail_after(10):","        bridge = IntentBridge()","        ","        # Start processor (bridge manages its own processor lifecycle)","        await bridge.start_processor()","        ","        try:","            # Multiple UI threads using bridge concurrently","            results = []","            ","            async def ui_thread_use_bridge(thread_id: int):","                \"\"\"UI thread uses bridge.\"\"\"","                # Create data spec","                data_spec = bridge.create_data_spec_intent(","                    dataset_id=f\"dataset_{thread_id}\",","                    symbols=[\"MNQ\"],","                    timeframes=[\"60m\"]","                )","                ","                # Create calculation intent","                intent = bridge.calculate_units_intent(","                    season=f\"2024Q{thread_id}\",","                    data1=data_spec,","                    data2=None,","                    strategy_id=\"test\",","                    params={\"param\": thread_id}","                )","                ","                # Submit and wait","                completed = await bridge.submit_and_wait_async(intent, timeout=5.0)","                # Both COMPLETED and FAILED are valid outcomes of \"processing\"","                if completed and completed.status in (IntentStatus.COMPLETED, IntentStatus.FAILED):","                    results.append((thread_id, completed.result))","            ","            # Launch concurrent UI threads","            tasks = [ui_thread_use_bridge(i) for i in range(5)]","            await asyncio.gather(*tasks)","            ","            # All should succeed (in terms of getting a response)","            assert len(results) == 5","            ","        finally:","            await bridge.stop_processor()","","","def test_no_direct_backend_imports():","    \"\"\"Test that UI modules shouldn't import backend logic directly.\"\"\"","    import sys","    ","    # Check that intent_bridge doesn't expose backend logic","    # Note: We need to ensure the module is loaded","    import gui.adapters.intent_bridge","    bridge_module = sys.modules['gui.adapters.intent_bridge']","    ","    # Bridge should not expose backend functions directly","    assert not hasattr(bridge_module, 'create_job_from_wizard_direct')","    assert not hasattr(bridge_module, 'calculate_units_direct')","    ","    # Bridge should expose intent creation methods","    assert hasattr(bridge_module, 'IntentBridge')","    assert hasattr(bridge_module, 'get_intent_bridge')","","","@pytest.mark.anyio","async def test_race_condition_prevention(clean_system, sample_data_spec):","    \"\"\"Test that race conditions are prevented by design.\"\"\"","    with anyio.fail_after(10):","        queue = ActionQueue()","        ","        async with managed_processor(queue) as processor:","            # Simulate race condition scenario:","            # Multiple UI threads trying to create jobs for same season/dataset","            ","            created_job_ids = set()","            lock = threading.Lock()","            ","            async def race_condition_thread(thread_id: int):","                \"\"\"Thread that could cause race condition in traditional system.\"\"\"","                # All threads use same parameters (potential race)","                intent = CreateJobIntent(","                    season=\"2024Q1\",  # Same season","                    data1=sample_data_spec,  # Same data","                    data2=None,","                    strategy_id=\"sma_cross_v1\",","                    params={\"window_fast\": 10}","                )","                ","                # Submit intent","                intent_id = processor.submit_intent(intent)","                completed = await processor.wait_for_intent(intent_id, timeout=5.0)","                ","                if completed and completed.status == IntentStatus.COMPLETED:","                    job_id = completed.result.get(\"job_id\")","                    if job_id:","                        with lock:","                            created_job_ids.add(job_id)","            ","            # Launch many threads simultaneously","            tasks = [race_condition_thread(i) for i in range(20)]","            await asyncio.gather(*tasks)","            ","            # 1. All intents should be processed","            state = processor.get_state()","            # Since these are duplicates, most will be rejected or failed.","            # But total processed (completed + failed) should be 20.","            # Note: Depending on implementation, duplicates might be 'rejected' before processing","            # or 'processed' and failed.","            ","            metrics = queue.get_metrics()","            # Ensure we processed something","            assert metrics[\"processed\"] > 0"]}
{"type":"file_footer","path":"tests/test_ui_race_condition_headless.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_vectorization_parity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2630,"sha256":"3058382f60fc15b786485e67b9d84760e63ecba10a9df1ad2105f15242519b5a","total_lines":77,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_vectorization_parity.py","chunk_index":0,"line_start":1,"line_end":77,"content":["","from __future__ import annotations","","import numpy as np","","from data.layout import normalize_bars","from engine.engine_jit import simulate_arrays","from engine.types import Fill, OrderKind, OrderRole, Side","from strategy.kernel import DonchianAtrParams, run_kernel_arrays, run_kernel_object_mode","","","def _assert_fills_equal(a: list[Fill], b: list[Fill]) -> None:","    assert len(a) == len(b)","    for fa, fb in zip(a, b):","        assert fa.bar_index == fb.bar_index","        assert fa.role == fb.role","        assert fa.kind == fb.kind","        assert fa.side == fb.side","        assert fa.qty == fb.qty","        assert fa.order_id == fb.order_id","        assert abs(fa.price - fb.price) <= 1e-9","","","def test_strategy_object_vs_array_mode_parity() -> None:","    rng = np.random.default_rng(42)","    n = 300","    close = 100.0 + np.cumsum(rng.standard_normal(n)).astype(np.float64)","    high = close + 1.0","    low = close - 1.0","    open_ = (high + low) * 0.5","","    bars = normalize_bars(open_, high, low, close)","    params = DonchianAtrParams(channel_len=20, atr_len=14, stop_mult=2.0)","","    out_obj = run_kernel_object_mode(bars, params, commission=0.0, slip=0.0, order_qty=1)","    out_arr = run_kernel_arrays(bars, params, commission=0.0, slip=0.0, order_qty=1)","","    _assert_fills_equal(out_obj[\"fills\"], out_arr[\"fills\"])  # type: ignore[arg-type]","","","def test_simulate_arrays_same_bar_entry_exit_parity() -> None:","    # Construct a same-bar entry then exit scenario (created_bar=-1 activates on bar0).","    bars = normalize_bars(","        np.array([100.0], dtype=np.float64),","        np.array([120.0], dtype=np.float64),","        np.array([80.0], dtype=np.float64),","        np.array([110.0], dtype=np.float64),","    )","","    # ENTRY BUY STOP 105, EXIT SELL STOP 95, both active on bar0.","    order_id = np.array([1, 2], dtype=np.int64)","    created_bar = np.array([-1, -1], dtype=np.int64)","    role = np.array([1, 0], dtype=np.int8)  # ENTRY then EXIT (order_id tie-break handles)","    kind = np.array([0, 0], dtype=np.int8)  # STOP","    side = np.array([1, -1], dtype=np.int8)  # BUY, SELL","    price = np.array([105.0, 95.0], dtype=np.float64)","    qty = np.array([1, 1], dtype=np.int64)","","    fills = simulate_arrays(","        bars,","        order_id=order_id,","        created_bar=created_bar,","        role=role,","        kind=kind,","        side=side,","        price=price,","        qty=qty,","        ttl_bars=1,","    )","","    assert len(fills) == 2","    assert fills[0].role == OrderRole.ENTRY and fills[0].side == Side.BUY and fills[0].kind == OrderKind.STOP","    assert fills[1].role == OrderRole.EXIT and fills[1].side == Side.SELL and fills[1].kind == OrderKind.STOP","","","",""]}
{"type":"file_footer","path":"tests/test_vectorization_parity.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_winners_schema_v2_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6418,"sha256":"8dedb61cacdcdaf800cfa38d26e7c7746d7789f12129f4ed9e76a63ba3e5e765","total_lines":205,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_winners_schema_v2_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for winners schema v2.","","Tests verify:","1. v2 schema structure (top-level fields)","2. WinnerItemV2 structure (required fields)","3. JSON serialization with sorted keys","4. Schema version detection","\"\"\"","","from __future__ import annotations","","import json","from datetime import datetime, timezone","","from core.winners_schema import (","    WinnerItemV2,","    build_winners_v2_dict,","    is_winners_legacy,","    is_winners_v2,","    WINNERS_SCHEMA_VERSION,",")","","","def test_winners_v2_top_level_schema() -> None:","    \"\"\"Test that v2 winners.json has required top-level fields.\"\"\"","    items = [","        WinnerItemV2(","            candidate_id=\"donchian_atr:123\",","            strategy_id=\"donchian_atr\",","            symbol=\"CME.MNQ\",","            timeframe=\"60m\",","            params={\"LE\": 8, \"LX\": 4, \"Z\": -0.4},","            score=1.234,","            metrics={\"net_profit\": 100.0, \"max_dd\": -10.0, \"trades\": 10, \"param_id\": 123},","            source={\"param_id\": 123, \"run_id\": \"test-123\", \"stage_name\": \"stage1_topk\"},","        ),","    ]","    ","    winners = build_winners_v2_dict(","        stage_name=\"stage1_topk\",","        run_id=\"test-123\",","        topk=items,","    )","    ","    # Verify top-level fields","    assert winners[\"schema\"] == WINNERS_SCHEMA_VERSION","    assert winners[\"stage_name\"] == \"stage1_topk\"","    assert \"generated_at\" in winners","    assert \"topk\" in winners","    assert \"notes\" in winners","    ","    # Verify notes schema","    assert winners[\"notes\"][\"schema\"] == WINNERS_SCHEMA_VERSION","","","def test_winner_item_v2_required_fields() -> None:","    \"\"\"Test that WinnerItemV2 has all required fields.\"\"\"","    item = WinnerItemV2(","        candidate_id=\"donchian_atr:c7bc8b64916c\",","        strategy_id=\"donchian_atr\",","        symbol=\"CME.MNQ\",","        timeframe=\"60m\",","        params={\"LE\": 8, \"LX\": 4, \"Z\": -0.4},","        score=1.234,","        metrics={\"net_profit\": 0.0, \"max_dd\": 0.0, \"trades\": 0, \"param_id\": 9},","        source={\"param_id\": 9, \"run_id\": \"stage1_topk-123\", \"stage_name\": \"stage1_topk\"},","    )","    ","    item_dict = item.to_dict()","    ","    # Verify all required fields exist","    assert \"candidate_id\" in item_dict","    assert \"strategy_id\" in item_dict","    assert \"symbol\" in item_dict","    assert \"timeframe\" in item_dict","    assert \"params\" in item_dict","    assert \"score\" in item_dict","    assert \"metrics\" in item_dict","    assert \"source\" in item_dict","    ","    # Verify field values","    assert item_dict[\"candidate_id\"] == \"donchian_atr:c7bc8b64916c\"","    assert item_dict[\"strategy_id\"] == \"donchian_atr\"","    assert item_dict[\"symbol\"] == \"CME.MNQ\"","    assert item_dict[\"timeframe\"] == \"60m\"","    assert isinstance(item_dict[\"params\"], dict)","    assert isinstance(item_dict[\"score\"], (int, float))","    assert isinstance(item_dict[\"metrics\"], dict)","    assert isinstance(item_dict[\"source\"], dict)","","","def test_winners_v2_json_serializable_sorted_keys() -> None:","    \"\"\"Test that v2 winners.json is JSON-serializable with sorted keys.\"\"\"","    items = [","        WinnerItemV2(","            candidate_id=\"donchian_atr:123\",","            strategy_id=\"donchian_atr\",","            symbol=\"CME.MNQ\",","            timeframe=\"60m\",","            params={\"LE\": 8},","            score=1.234,","            metrics={\"net_profit\": 100.0, \"max_dd\": -10.0, \"trades\": 10, \"param_id\": 123},","            source={\"param_id\": 123, \"run_id\": \"test-123\", \"stage_name\": \"stage1_topk\"},","        ),","    ]","    ","    winners = build_winners_v2_dict(","        stage_name=\"stage1_topk\",","        run_id=\"test-123\",","        topk=items,","    )","    ","    # Serialize to JSON with sorted keys","    json_str = json.dumps(winners, ensure_ascii=False, sort_keys=True, indent=2)","    ","    # Deserialize back","    winners_roundtrip = json.loads(json_str)","    ","    # Verify structure","    assert winners_roundtrip[\"schema\"] == WINNERS_SCHEMA_VERSION","    assert len(winners_roundtrip[\"topk\"]) == 1","    ","    item_dict = winners_roundtrip[\"topk\"][0]","    assert item_dict[\"candidate_id\"] == \"donchian_atr:123\"","    assert item_dict[\"strategy_id\"] == \"donchian_atr\"","    ","    # Verify JSON keys are sorted (check top-level)","    json_lines = json_str.split(\"\\n\")","    # Find line with \"generated_at\" and \"schema\" - should be in sorted order","    # (This is a simple check - full verification would require parsing)","    assert '\"generated_at\"' in json_str","    assert '\"schema\"' in json_str","","","def test_is_winners_v2_detection() -> None:","    \"\"\"Test schema version detection.\"\"\"","    # v2 format","    winners_v2 = {","        \"schema\": \"v2\",","        \"stage_name\": \"stage1_topk\",","        \"generated_at\": \"2025-12-18T00:00:00Z\",","        \"topk\": [],","        \"notes\": {\"schema\": \"v2\"},","    }","    assert is_winners_v2(winners_v2) is True","    assert is_winners_legacy(winners_v2) is False","    ","    # Legacy format","    winners_legacy = {","        \"topk\": [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}],","        \"notes\": {\"schema\": \"v1\"},","    }","    assert is_winners_v2(winners_legacy) is False","    assert is_winners_legacy(winners_legacy) is True","    ","    # Unknown format (no schema)","    winners_unknown = {","        \"topk\": [{\"param_id\": 0}],","    }","    assert is_winners_v2(winners_unknown) is False","    assert is_winners_legacy(winners_unknown) is True  # Falls back to legacy","","","def test_winner_item_v2_metrics_contains_legacy_fields() -> None:","    \"\"\"Test that metrics contains legacy fields for backward compatibility.\"\"\"","    item = WinnerItemV2(","        candidate_id=\"donchian_atr:123\",","        strategy_id=\"donchian_atr\",","        symbol=\"CME.MNQ\",","        timeframe=\"60m\",","        params={},","        score=1.234,","        metrics={","            \"net_profit\": 100.0,","            \"max_dd\": -10.0,","            \"trades\": 10,","            \"param_id\": 123,  # Legacy field","        },","        source={\"param_id\": 123, \"run_id\": \"test-123\", \"stage_name\": \"stage1_topk\"},","    )","    ","    item_dict = item.to_dict()","    metrics = item_dict[\"metrics\"]","    ","    # Verify legacy fields exist","    assert \"net_profit\" in metrics","    assert \"max_dd\" in metrics","    assert \"trades\" in metrics","    assert \"param_id\" in metrics","","","def test_winners_v2_empty_topk() -> None:","    \"\"\"Test that v2 schema handles empty topk correctly.\"\"\"","    winners = build_winners_v2_dict(","        stage_name=\"stage1_topk\",","        run_id=\"test-123\",","        topk=[],","    )","    "]}
{"type":"file_chunk","path":"tests/test_winners_schema_v2_contract.py","chunk_index":1,"line_start":201,"line_end":205,"content":["    assert winners[\"schema\"] == WINNERS_SCHEMA_VERSION","    assert winners[\"topk\"] == []","    assert isinstance(winners[\"topk\"], list)","",""]}
{"type":"file_footer","path":"tests/test_winners_schema_v2_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_worker_writes_traceback_to_log.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2250,"sha256":"f31dd89e83f997deb0014268e492743da3dd030a0a7ee99739a2c1cd917c57fb","total_lines":69,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_worker_writes_traceback_to_log.py","chunk_index":0,"line_start":1,"line_end":69,"content":["","\"\"\"Tests for worker writing full traceback to log.","","Tests that worker writes complete traceback.format_exc() to job_logs table","when job fails, while keeping last_error column short (500 chars).","\"\"\"","","from __future__ import annotations","","from pathlib import Path","from unittest.mock import Mock, patch","","import pytest","","from control.jobs_db import create_job, get_job, get_job_logs, init_db","from control.types import DBJobSpec, JobStatus","from control.worker import run_one_job","","","def test_worker_writes_traceback_to_log(tmp_path: Path) -> None:","    \"\"\"","    Test that worker writes full traceback to job_logs when job fails.","    ","    Verifies:","    - last_error is truncated to 500 chars","    - job_logs contains full traceback with \"Traceback (most recent call last):\"","    \"\"\"","    db = tmp_path / \"jobs.db\"","    init_db(db)","    ","    # Create a job","    spec = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=str(tmp_path / \"outputs\"),","        config_snapshot={\"test\": \"config\"},","        config_hash=\"test_hash\",","    )","    job_id = create_job(db, spec)","    ","    # Mock run_funnel to raise exception with traceback","    with patch(\"control.worker.run_funnel\", side_effect=ValueError(\"Test error with long message \" * 20)):","        # Run job (should catch exception and write traceback)","        run_one_job(db, job_id)","    ","    # Verify job is marked as FAILED","    job = get_job(db, job_id)","    assert job.status == JobStatus.FAILED","    assert job.last_error is not None","    assert len(job.last_error) <= 500  # Truncated","    ","    # Verify traceback is in job_logs","    logs = get_job_logs(db, job_id)","    assert len(logs) > 0, \"Should have at least one log entry\"","    ","    # Find error log entry","    error_logs = [log for log in logs if \"[ERROR]\" in log]","    assert len(error_logs) > 0, \"Should have error log entry\"","    ","    # Verify traceback format","    error_log = error_logs[0]","    assert \"Traceback (most recent call last):\" in error_log, \"Should contain full traceback\"","    assert \"ValueError\" in error_log, \"Should contain exception type\"","    assert \"Test error\" in error_log, \"Should contain error message\"","    ","    # Verify error message is in last_error (truncated)","    assert \"Test error\" in job.last_error","",""]}
{"type":"file_footer","path":"tests/test_worker_writes_traceback_to_log.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/wfs/test_wfs_no_io.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2816,"sha256":"4d3bfd2dd501f0e0484e73d9ed77bfdbc8889773785f467a9a408b51996915cf","total_lines":87,"chunk_count":1}
{"type":"file_chunk","path":"tests/wfs/test_wfs_no_io.py","chunk_index":0,"line_start":1,"line_end":87,"content":["","import builtins","from pathlib import Path","","import numpy as np","import pytest","","from core.feature_bundle import FeatureSeries, FeatureBundle","import wfs.runner as wfs_runner","","","class _DummySpec:","    \"\"\"","    Minimal strategy spec object for tests.","    Must provide:","      - defaults: dict","      - fn(strategy_input: dict, params: dict) -> dict with {\"intents\": [...]}","    \"\"\"","    def __init__(self):","        self.defaults = {}","","        def _fn(strategy_input, params):","            # Must not do IO; return valid structure for run_strategy().","            return {\"intents\": []}","","        self.fn = _fn","","","def test_run_wfs_with_features_disallows_file_io_without_real_strategy(monkeypatch):","    # 1) Hard deny all file IO primitives","    def _deny(*args, **kwargs):","        raise RuntimeError(\"IO is forbidden in run_wfs_with_features\")","","    monkeypatch.setattr(builtins, \"open\", _deny, raising=True)","    monkeypatch.setattr(Path, \"open\", _deny, raising=True)","    monkeypatch.setattr(Path, \"read_text\", _deny, raising=True)","    monkeypatch.setattr(Path, \"exists\", _deny, raising=True)","","    # 2) Inject dummy strategy spec so we don't rely on repo strategy registry/ids","    # Primary patch target: symbol referenced by wfs_runner module","    monkeypatch.setattr(wfs_runner, \"get_strategy_spec\", lambda strategy_id: _DummySpec(), raising=False)","","    # If get_strategy_spec isn't used in this repo layout, add fallback patches:","    # These should be kept harmless by raising=False.","    try:","        import strategy.registry as strat_registry","        monkeypatch.setattr(strat_registry, \"get\", lambda strategy_id: _DummySpec(), raising=False)","    except Exception:","        pass","","    try:","        import strategy.runner as strat_runner","        monkeypatch.setattr(strat_runner, \"get\", lambda strategy_id: _DummySpec(), raising=False)","    except Exception:","        pass","","    # 3) Build a minimal FeatureBundle","    ts = np.array(","        [\"2025-01-01T00:00:00\", \"2025-01-01T00:01:00\", \"2025-01-01T00:02:00\"],","        dtype=\"datetime64[s]\",","    )","    v = np.array([1.0, 2.0, 3.0], dtype=np.float64)","","    s1 = FeatureSeries(ts=ts, values=v, name=\"atr_14\", timeframe_min=60)","    s2 = FeatureSeries(ts=ts, values=v, name=\"ret_z_200\", timeframe_min=60)","    s3 = FeatureSeries(ts=ts, values=v, name=\"session_vwap\", timeframe_min=60)","","    # FeatureBundle requires meta dict with ts_dtype and breaks_policy","    meta = {","        \"ts_dtype\": \"datetime64[s]\",","        \"breaks_policy\": \"drop\",","    }","    bundle = FeatureBundle(","        dataset_id=\"D\",","        season=\"S\",","        series={(s.name, s.timeframe_min): s for s in [s1, s2, s3]},","        meta=meta,","    )","","    out = wfs_runner.run_wfs_with_features(","        strategy_id=\"__dummy__\",","        feature_bundle=bundle,","        config={\"params\": {}},","    )","","    assert isinstance(out, dict)",""]}
{"type":"file_footer","path":"tests/wfs/test_wfs_no_io.py","complete":true,"emitted_chunks":1}
