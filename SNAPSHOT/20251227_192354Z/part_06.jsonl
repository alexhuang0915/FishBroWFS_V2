{"type":"meta","schema_version":2,"run_id":"20251227_192354Z","repo_root":"/home/fishbro/FishBroWFS_V2","snapshot_root":"SNAPSHOT","part":6,"parts":10,"created_at":"2025-12-27T19:23:54Z","generator":{"name":"dump_context","version":"vNext-balanced"},"policies":{"max_parts":10,"target_bytes_per_part":500000,"chunk_max_lines":200,"chunk_max_bytes":120000,"outputs_max_bytes_for_content":2000000,"outputs_policy":"manifest_only_for_big_files","forbid_file_truncated":true,"balance_parts":true,"estimated_total_bytes":3707501,"balance_buffer_ratio":1.08}}
{"type":"file_chunk","path":"tests/test_data_cache_rebuild_fingerprint_stable.py","chunk_index":0,"line_start":1,"line_end":147,"content":["","\"\"\"Test: Delete parquet cache and rebuild - fingerprint must remain stable.","","Binding #4: Parquet is Cache, Not Truth.","Fingerprint is computed from raw TXT + ingest_policy, not from parquet.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from data.cache import CachePaths, cache_paths, read_parquet_cache, write_parquet_cache","from data.fingerprint import compute_txt_fingerprint","from data.raw_ingest import ingest_raw_txt","","","def test_cache_rebuild_fingerprint_stable(temp_dir: Path, sample_raw_txt: Path) -> None:","    \"\"\"Test that deleting parquet and rebuilding produces same fingerprint.","    ","    Flow:","    1. Use sample_raw_txt fixture","    2. Compute fingerprint sha1 A","    3. Ingest → write parquet cache","    4. Delete parquet + meta","    5. Ingest → write parquet cache (same policy)","    6. Compute fingerprint sha1 B","    7. Assert A == B","    8. Assert meta.data_fingerprint_sha1 == A","    \"\"\"","    # Use sample_raw_txt fixture","    txt_path = sample_raw_txt","    ","    # Ingest policy","    ingest_policy = {","        \"normalized_24h\": False,","        \"column_map\": None,","    }","    ","    # Step 1: Compute fingerprint sha1 A","    fingerprint_a = compute_txt_fingerprint(txt_path, ingest_policy=ingest_policy)","    sha1_a = fingerprint_a.sha1","    ","    # Step 2: Ingest → write parquet cache","    result = ingest_raw_txt(txt_path)","    cache_root = temp_dir / \"cache\"","    cache_paths_obj = cache_paths(cache_root, \"TEST_SYMBOL\")","    ","    meta = {","        \"data_fingerprint_sha1\": sha1_a,","        \"source_path\": str(txt_path),","        \"ingest_policy\": ingest_policy,","        \"rows\": result.rows,","        \"first_ts_str\": result.df.iloc[0][\"ts_str\"],","        \"last_ts_str\": result.df.iloc[-1][\"ts_str\"],","    }","    ","    write_parquet_cache(cache_paths_obj, result.df, meta)","    ","    # Verify cache exists","    assert cache_paths_obj.parquet_path.exists()","    assert cache_paths_obj.meta_path.exists()","    ","    # Step 3: Delete parquet + meta","    cache_paths_obj.parquet_path.unlink()","    cache_paths_obj.meta_path.unlink()","    ","    assert not cache_paths_obj.parquet_path.exists()","    assert not cache_paths_obj.meta_path.exists()","    ","    # Step 4: Ingest → write parquet cache (same policy)","    result2 = ingest_raw_txt(txt_path)","    write_parquet_cache(cache_paths_obj, result2.df, meta)","    ","    # Step 5: Compute fingerprint sha1 B","    fingerprint_b = compute_txt_fingerprint(txt_path, ingest_policy=ingest_policy)","    sha1_b = fingerprint_b.sha1","    ","    # Step 6: Assert A == B","    assert sha1_a == sha1_b, f\"Fingerprint changed after cache rebuild: {sha1_a} != {sha1_b}\"","    ","    # Step 7: Assert meta.data_fingerprint_sha1 == A","    df_read, meta_read = read_parquet_cache(cache_paths_obj)","    assert meta_read[\"data_fingerprint_sha1\"] == sha1_a","    assert meta_read[\"data_fingerprint_sha1\"] == sha1_b","","","def test_cache_rebuild_with_24h_normalization(temp_dir: Path) -> None:","    \"\"\"Test fingerprint stability with 24:00 normalization.\"\"\"","    # Create temp raw TXT with 24:00:00 (specific test case, not using fixture)","    txt_path = temp_dir / \"test_data_24h.txt\"","    txt_content = \"\"\"Date,Time,Open,High,Low,Close,TotalVolume","2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000","2013/1/1,24:00:00,104.0,106.0,103.0,105.0,1200","2013/1/2,09:30:00,105.0,107.0,104.0,106.0,1500","\"\"\"","    txt_path.write_text(txt_content, encoding=\"utf-8\")","    ","    # Ingest policy (will normalize 24:00:00)","    ingest_policy = {","        \"normalized_24h\": True,  # Will be set to True after ingest","        \"column_map\": None,","    }","    ","    # Ingest first time","    result1 = ingest_raw_txt(txt_path)","    # Update policy to reflect normalization","    ingest_policy[\"normalized_24h\"] = result1.policy.normalized_24h","    ","    # Compute fingerprint","    fingerprint_a = compute_txt_fingerprint(txt_path, ingest_policy=ingest_policy)","    sha1_a = fingerprint_a.sha1","    ","    # Write cache","    cache_root = temp_dir / \"cache2\"","    cache_paths_obj = cache_paths(cache_root, \"TEST_SYMBOL_24H\")","    ","    meta = {","        \"data_fingerprint_sha1\": sha1_a,","        \"source_path\": str(txt_path),","        \"ingest_policy\": ingest_policy,","        \"rows\": result1.rows,","        \"first_ts_str\": result1.df.iloc[0][\"ts_str\"],","        \"last_ts_str\": result1.df.iloc[-1][\"ts_str\"],","    }","    ","    write_parquet_cache(cache_paths_obj, result1.df, meta)","    ","    # Delete cache","    cache_paths_obj.parquet_path.unlink()","    cache_paths_obj.meta_path.unlink()","    ","    # Rebuild","    result2 = ingest_raw_txt(txt_path)","    write_parquet_cache(cache_paths_obj, result2.df, meta)","    ","    # Compute fingerprint again","    fingerprint_b = compute_txt_fingerprint(txt_path, ingest_policy=ingest_policy)","    sha1_b = fingerprint_b.sha1","    ","    # Assert stability","    assert sha1_a == sha1_b, f\"Fingerprint changed: {sha1_a} != {sha1_b}\"","    assert result1.policy.normalized_24h == True  # Should have normalized 24:00:00","    assert result2.policy.normalized_24h == True","",""]}
{"type":"file_footer","path":"tests/test_data_cache_rebuild_fingerprint_stable.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_data_ingest_e2e.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5903,"sha256":"5238c77aca12c69e48e2ce419a003f295f2b058df77ebf006f9fb6fc7b2b8886","total_lines":171,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_data_ingest_e2e.py","chunk_index":0,"line_start":1,"line_end":171,"content":["","\"\"\"End-to-end test: Ingest → Cache → Rebuild.","","Tests the complete data ingest pipeline:","1. Ingest raw TXT → DataFrame","2. Compute fingerprint","3. Write parquet cache + meta.json","4. Clean cache","5. Rebuild cache","6. Verify fingerprint stability","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from data.cache import cache_paths, read_parquet_cache, write_parquet_cache","from data.fingerprint import compute_txt_fingerprint","from data.raw_ingest import ingest_raw_txt","","# Note: sample_raw_txt fixture is defined in conftest.py for all tests","","","def test_ingest_cache_e2e(tmp_path: Path, sample_raw_txt: Path) -> None:","    \"\"\"End-to-end test: Ingest → Compute fingerprint → Write cache.","    ","    Tests:","    1. ingest_raw_txt() produces DataFrame with correct columns","    2. compute_txt_fingerprint() produces SHA1 hash","    3. write_parquet_cache() creates parquet and meta.json files","    4. meta.json contains data_fingerprint_sha1","    \"\"\"","    # Step 1: Ingest raw TXT","    result = ingest_raw_txt(sample_raw_txt)","    ","    # Verify DataFrame structure","    assert len(result.df) == 3","    assert list(result.df.columns) == [\"ts_str\", \"open\", \"high\", \"low\", \"close\", \"volume\"]","    assert result.df[\"ts_str\"].dtype == \"object\"  # str","    assert result.df[\"open\"].dtype == \"float64\"","    assert result.df[\"volume\"].dtype == \"int64\"","    ","    # Step 2: Compute fingerprint","    ingest_policy = {","        \"normalized_24h\": result.policy.normalized_24h,","        \"column_map\": result.policy.column_map,","    }","    fingerprint = compute_txt_fingerprint(sample_raw_txt, ingest_policy=ingest_policy)","    ","    # Verify fingerprint","    assert len(fingerprint.sha1) == 40  # SHA1 hex length","    assert fingerprint.source_path == str(sample_raw_txt)","    assert fingerprint.rows == 3","    ","    # Step 3: Write cache","    cache_root = tmp_path / \"cache\"","    symbol = \"TEST_SYMBOL\"","    paths = cache_paths(cache_root, symbol)","    ","    meta = {","        \"data_fingerprint_sha1\": fingerprint.sha1,","        \"source_path\": str(sample_raw_txt),","        \"ingest_policy\": ingest_policy,","        \"rows\": result.rows,","        \"first_ts_str\": result.df.iloc[0][\"ts_str\"],","        \"last_ts_str\": result.df.iloc[-1][\"ts_str\"],","    }","    ","    write_parquet_cache(paths, result.df, meta)","    ","    # Step 4: Verify cache files exist","    assert paths.parquet_path.exists(), f\"Parquet file not created: {paths.parquet_path}\"","    assert paths.meta_path.exists(), f\"Meta file not created: {paths.meta_path}\"","    ","    # Step 5: Verify meta.json contains fingerprint","    df_read, meta_read = read_parquet_cache(paths)","    ","    assert \"data_fingerprint_sha1\" in meta_read","    assert meta_read[\"data_fingerprint_sha1\"] == fingerprint.sha1","    assert meta_read[\"data_fingerprint_sha1\"] == meta[\"data_fingerprint_sha1\"]","    ","    # Verify parquet data matches original","    assert len(df_read) == 3","    assert list(df_read.columns) == [\"ts_str\", \"open\", \"high\", \"low\", \"close\", \"volume\"]","    assert df_read.iloc[0][\"ts_str\"] == \"2013/1/1 09:30:00\"","","","def test_clean_rebuild_fingerprint_stable(tmp_path: Path, sample_raw_txt: Path) -> None:","    \"\"\"Test: Clean cache → Rebuild → Fingerprint remains stable.","    ","    Flow:","    1. Ingest → Write cache → Get sha1_before","    2. Clean cache (delete parquet + meta)","    3. Re-ingest → Write cache → Get sha1_after","    4. Assert sha1_before == sha1_after","    ","    ⚠️ No mocks, no hardcoding - real file operations only.","    \"\"\"","    # Step 1: Initial ingest and cache","    result1 = ingest_raw_txt(sample_raw_txt)","    ingest_policy = {","        \"normalized_24h\": result1.policy.normalized_24h,","        \"column_map\": result1.policy.column_map,","    }","    fingerprint1 = compute_txt_fingerprint(sample_raw_txt, ingest_policy=ingest_policy)","    ","    cache_root = tmp_path / \"cache_rebuild\"","    symbol = \"TEST_SYMBOL_REBUILD\"","    paths = cache_paths(cache_root, symbol)","    ","    meta1 = {","        \"data_fingerprint_sha1\": fingerprint1.sha1,","        \"source_path\": str(sample_raw_txt),","        \"ingest_policy\": ingest_policy,","        \"rows\": result1.rows,","        \"first_ts_str\": result1.df.iloc[0][\"ts_str\"],","        \"last_ts_str\": result1.df.iloc[-1][\"ts_str\"],","    }","    ","    write_parquet_cache(paths, result1.df, meta1)","    ","    # Verify cache exists","    assert paths.parquet_path.exists()","    assert paths.meta_path.exists()","    ","    # Read meta to get sha1_before","    _, meta_read_before = read_parquet_cache(paths)","    sha1_before = meta_read_before[\"data_fingerprint_sha1\"]","    assert sha1_before == fingerprint1.sha1","    ","    # Step 2: Clean cache (delete parquet + meta)","    # Directly delete files (real cleanup, no mocks)","    paths.parquet_path.unlink()","    paths.meta_path.unlink()","    ","    # Verify files are deleted","    assert not paths.parquet_path.exists()","    assert not paths.meta_path.exists()","    ","    # Step 3: Re-ingest and rebuild cache","    result2 = ingest_raw_txt(sample_raw_txt)","    fingerprint2 = compute_txt_fingerprint(sample_raw_txt, ingest_policy=ingest_policy)","    ","    meta2 = {","        \"data_fingerprint_sha1\": fingerprint2.sha1,","        \"source_path\": str(sample_raw_txt),","        \"ingest_policy\": ingest_policy,","        \"rows\": result2.rows,","        \"first_ts_str\": result2.df.iloc[0][\"ts_str\"],","        \"last_ts_str\": result2.df.iloc[-1][\"ts_str\"],","    }","    ","    write_parquet_cache(paths, result2.df, meta2)","    ","    # Step 4: Verify fingerprint stability","    _, meta_read_after = read_parquet_cache(paths)","    sha1_after = meta_read_after[\"data_fingerprint_sha1\"]","    ","    assert sha1_before == sha1_after, (","        f\"Fingerprint changed after cache rebuild: \"","        f\"before={sha1_before}, after={sha1_after}\"","    )","    assert sha1_after == fingerprint2.sha1","    assert fingerprint1.sha1 == fingerprint2.sha1, (","        f\"Fingerprint computation changed: \"","        f\"first={fingerprint1.sha1}, second={fingerprint2.sha1}\"","    )","",""]}
{"type":"file_footer","path":"tests/test_data_ingest_e2e.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_data_ingest_monkeypatch_trap.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6003,"sha256":"c2c26bf09ec1ebbd174060519d6794f0d24333603ef7f275dd6b3886adaf0b3e","total_lines":139,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_data_ingest_monkeypatch_trap.py","chunk_index":0,"line_start":1,"line_end":139,"content":["","\"\"\"Monkeypatch trap test: Ensure forbidden pandas methods are never called during raw ingest.","","This test uses monkeypatch to trap any calls to forbidden methods.","If any forbidden method is called, the test immediately fails with a clear error.","","Binding: Raw means RAW (Phase 6.5) - no sort, no dedup, no dropna, no datetime parse.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pandas as pd","import pytest","","from data.raw_ingest import ingest_raw_txt","","","def test_raw_ingest_forbidden_methods_trap(monkeypatch: pytest.MonkeyPatch, sample_raw_txt: Path) -> None:","    \"\"\"Trap test: Any forbidden pandas method call during ingest will immediately fail.","    ","    This test uses monkeypatch to replace forbidden methods with functions that","    raise AssertionError. If ingest_raw_txt() calls any forbidden method, the","    test will fail immediately with a clear error message.","    ","    Forbidden methods:","    - pd.DataFrame.sort_values() - violates row order preservation","    - pd.DataFrame.dropna() - violates empty value preservation","    - pd.DataFrame.drop_duplicates() - violates duplicate preservation","    - pd.to_datetime() - violates naive ts_str contract (Phase 6.5)","    ","    ⚠️ This is a constitutional test, not a debug log.","    The error messages are legal requirements, not debugging hints.","    \"\"\"","    # Arrange: Patch forbidden methods to raise AssertionError if called","    ","    def _boom_sort_values(*args, **kwargs):","        \"\"\"Trap function for sort_values() - violates Raw means RAW.\"\"\"","        raise AssertionError(","            \"FORBIDDEN: sort_values() violates Raw means RAW (Phase 6.5). \"","            \"Row order must be preserved exactly as in TXT file.\"","        )","    ","    def _boom_dropna(*args, **kwargs):","        \"\"\"Trap function for dropna() - violates Raw means RAW.\"\"\"","        raise AssertionError(","            \"FORBIDDEN: dropna() violates Raw means RAW (Phase 6.5). \"","            \"Empty values must be preserved (e.g., volume=0).\"","        )","    ","    def _boom_drop_duplicates(*args, **kwargs):","        \"\"\"Trap function for drop_duplicates() - violates Raw means RAW.\"\"\"","        raise AssertionError(","            \"FORBIDDEN: drop_duplicates() violates Raw means RAW (Phase 6.5). \"","            \"Duplicate rows must be preserved exactly as in TXT file.\"","        )","    ","    def _boom_to_datetime(*args, **kwargs):","        \"\"\"Trap function for pd.to_datetime() - violates naive ts_str contract.\"\"\"","        raise AssertionError(","            \"FORBIDDEN: pd.to_datetime() violates Naive ts_str Contract (Phase 6.5). \"","            \"Timestamp must remain as string literal, no datetime parsing allowed.\"","        )","    ","    # Apply monkeypatches (scope limited to this test function)","    # Note: pd.to_datetime() is only used in _normalize_24h() for date parsing.","    # Since sample_raw_txt doesn't contain 24:00:00, _normalize_24h won't be called,","    # so we can safely trap all pd.to_datetime calls","    monkeypatch.setattr(pd.DataFrame, \"sort_values\", _boom_sort_values)","    monkeypatch.setattr(pd.DataFrame, \"dropna\", _boom_dropna)","    monkeypatch.setattr(pd.DataFrame, \"drop_duplicates\", _boom_drop_duplicates)","    monkeypatch.setattr(pd, \"to_datetime\", _boom_to_datetime)","    ","    # Act: Call ingest_raw_txt() with patched pandas","    # If any forbidden method is called, AssertionError will be raised immediately","    result = ingest_raw_txt(sample_raw_txt)","    ","    # Assert: Ingest completed successfully without triggering any traps","    # If we reach here, no forbidden methods were called","    assert result is not None","    assert len(result.df) > 0","    assert \"ts_str\" in result.df.columns","    assert result.df[\"ts_str\"].dtype == \"object\"  # Must be string, not datetime","","","def test_raw_ingest_forbidden_methods_trap_with_24h_normalization(","    monkeypatch: pytest.MonkeyPatch, temp_dir: Path",") -> None:","    \"\"\"Trap test with 24:00 normalization - ensure no forbidden DataFrame methods called.","    ","    Tests the same traps but with a TXT file containing 24:00:00 time.","    Note: pd.to_datetime() is allowed in _normalize_24h() for date parsing only,","    so we only trap DataFrame methods, not pd.to_datetime().","    \"\"\"","    # Create TXT with 24:00:00 (requires normalization)","    txt_path = temp_dir / \"test_24h.txt\"","    txt_content = \"\"\"Date,Time,Open,High,Low,Close,TotalVolume","2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000","2013/1/1,24:00:00,104.0,106.0,103.0,105.0,1200","2013/1/2,09:30:00,105.0,107.0,104.0,106.0,1500","\"\"\"","    txt_path.write_text(txt_content, encoding=\"utf-8\")","    ","    # Arrange: Patch forbidden DataFrame methods only","    # Note: pd.to_datetime() is allowed for date parsing in _normalize_24h()","    def _boom_sort_values(*args, **kwargs):","        raise AssertionError(","            \"FORBIDDEN: sort_values() violates Raw means RAW (Phase 6.5). \"","            \"Row order must be preserved exactly as in TXT file.\"","        )","    ","    def _boom_dropna(*args, **kwargs):","        raise AssertionError(","            \"FORBIDDEN: dropna() violates Raw means RAW (Phase 6.5). \"","            \"Empty values must be preserved (e.g., volume=0).\"","        )","    ","    def _boom_drop_duplicates(*args, **kwargs):","        raise AssertionError(","            \"FORBIDDEN: drop_duplicates() violates Raw means RAW (Phase 6.5). \"","            \"Duplicate rows must be preserved exactly as in TXT file.\"","        )","    ","    monkeypatch.setattr(pd.DataFrame, \"sort_values\", _boom_sort_values)","    monkeypatch.setattr(pd.DataFrame, \"dropna\", _boom_dropna)","    monkeypatch.setattr(pd.DataFrame, \"drop_duplicates\", _boom_drop_duplicates)","    ","    # Act: Call ingest_raw_txt() - should succeed with 24h normalization","    result = ingest_raw_txt(txt_path)","    ","    # Assert: Ingest completed successfully","    assert result is not None","    assert len(result.df) == 3","    assert result.policy.normalized_24h == True  # Should have normalized 24:00:00","    # Verify 24:00:00 was normalized to next day 00:00:00","    assert \"2013/1/2 00:00:00\" in result.df[\"ts_str\"].values","",""]}
{"type":"file_footer","path":"tests/test_data_ingest_monkeypatch_trap.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_data_ingest_raw_means_raw.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5742,"sha256":"c21e97fafeb30a61d7e5a4148a086df9779e9c75e41a68ceedf7caf1987afaf9","total_lines":160,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_data_ingest_raw_means_raw.py","chunk_index":0,"line_start":1,"line_end":160,"content":["","\"\"\"Test: Raw means RAW - regression prevention.","","RED TEAM #1: Lock down three things:","1. Row order unchanged (no sort)","2. Duplicate ts_str not deduplicated (no drop_duplicates)","3. Empty values not dropped (no dropna) - test with volume=0","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pandas as pd","import pytest","","from data.raw_ingest import ingest_raw_txt","","","def test_row_order_preserved(temp_dir: Path) -> None:","    \"\"\"Test that row order matches TXT file exactly (no sort).\"\"\"","    # Create TXT with intentionally unsorted timestamps","    txt_path = temp_dir / \"test_order.txt\"","    txt_content = \"\"\"Date,Time,Open,High,Low,Close,TotalVolume","2013/1/3,09:30:00,110.0,115.0,109.0,114.0,2000","2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000","2013/1/2,09:30:00,105.0,107.0,104.0,106.0,1500","\"\"\"","    txt_path.write_text(txt_content, encoding=\"utf-8\")","    ","    result = ingest_raw_txt(txt_path)","    ","    # Assert order matches TXT (first row should be 2013/1/3)","    assert result.df.iloc[0][\"ts_str\"] == \"2013/1/3 09:30:00\"","    assert result.df.iloc[1][\"ts_str\"] == \"2013/1/1 09:30:00\"","    assert result.df.iloc[2][\"ts_str\"] == \"2013/1/2 09:30:00\"","    ","    # Verify no sort occurred (should be in TXT order)","    assert len(result.df) == 3","","","def test_duplicate_ts_str_not_deduped(temp_dir: Path) -> None:","    \"\"\"Test that duplicate ts_str rows are preserved (no drop_duplicates).\"\"\"","    # Create TXT with duplicate Date/Time but different Close values","    txt_path = temp_dir / \"test_duplicate.txt\"","    txt_content = \"\"\"Date,Time,Open,High,Low,Close,TotalVolume","2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000","2013/1/1,09:30:00,100.0,105.0,99.0,105.0,1200","2013/1/2,09:30:00,105.0,107.0,104.0,106.0,1500","\"\"\"","    txt_path.write_text(txt_content, encoding=\"utf-8\")","    ","    result = ingest_raw_txt(txt_path)","    ","    # Assert both duplicate rows are present","    assert len(result.df) == 3","    ","    # Assert order matches TXT","    assert result.df.iloc[0][\"ts_str\"] == \"2013/1/1 09:30:00\"","    assert result.df.iloc[0][\"close\"] == 104.0","    ","    assert result.df.iloc[1][\"ts_str\"] == \"2013/1/1 09:30:00\"","    assert result.df.iloc[1][\"close\"] == 105.0  # Different close value","    ","    assert result.df.iloc[2][\"ts_str\"] == \"2013/1/2 09:30:00\"","    ","    # Verify duplicates exist (ts_str column should have duplicates)","    ts_str_counts = result.df[\"ts_str\"].value_counts()","    assert ts_str_counts[\"2013/1/1 09:30:00\"] == 2","","","def test_volume_zero_preserved(temp_dir: Path) -> None:","    \"\"\"Test that volume=0 rows are preserved (no dropna).\"\"\"","    # Create TXT with volume=0","    txt_path = temp_dir / \"test_volume_zero.txt\"","    txt_content = \"\"\"Date,Time,Open,High,Low,Close,TotalVolume","2013/1/1,09:30:00,100.0,105.0,99.0,104.0,0","2013/1/1,10:00:00,104.0,106.0,103.0,105.0,1200","2013/1/2,09:30:00,105.0,107.0,104.0,106.0,0","\"\"\"","    txt_path.write_text(txt_content, encoding=\"utf-8\")","    ","    result = ingest_raw_txt(txt_path)","    ","    # Assert all rows are present (including volume=0)","    assert len(result.df) == 3","    ","    # Assert volume=0 rows are preserved","    assert result.df.iloc[0][\"volume\"] == 0","    assert result.df.iloc[1][\"volume\"] == 1200","    assert result.df.iloc[2][\"volume\"] == 0","    ","    # Verify volume column type is int64","    assert result.df[\"volume\"].dtype == \"int64\"","","","def test_no_sort_values_called(temp_dir: Path) -> None:","    \"\"\"Regression test: Ensure sort_values is never called internally.\"\"\"","    # This is a contract test - if sort is called, order would change","    txt_path = temp_dir / \"test_no_sort.txt\"","    txt_content = \"\"\"Date,Time,Open,High,Low,Close,TotalVolume","2013/1/3,09:30:00,110.0,115.0,109.0,114.0,2000","2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000","2013/1/2,09:30:00,105.0,107.0,104.0,106.0,1500","\"\"\"","    txt_path.write_text(txt_content, encoding=\"utf-8\")","    ","    result = ingest_raw_txt(txt_path)","    ","    # If sort was called, first row would be 2013/1/1 (earliest)","    # But we expect 2013/1/3 (first in TXT)","    first_ts = result.df.iloc[0][\"ts_str\"]","    assert first_ts.startswith(\"2013/1/3\"), f\"Row order changed - first row is {first_ts}, expected 2013/1/3\"","","","def test_no_drop_duplicates_called(temp_dir: Path) -> None:","    \"\"\"Regression test: Ensure drop_duplicates is never called internally.\"\"\"","    txt_path = temp_dir / \"test_no_dedup.txt\"","    txt_content = \"\"\"Date,Time,Open,High,Low,Close,TotalVolume","2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000","2013/1/1,09:30:00,100.0,105.0,99.0,105.0,1200","2013/1/1,09:30:00,100.0,105.0,99.0,106.0,1300","\"\"\"","    txt_path.write_text(txt_content, encoding=\"utf-8\")","    ","    result = ingest_raw_txt(txt_path)","    ","    # If drop_duplicates was called, we'd have only 1 row","    # But we expect 3 rows (all duplicates preserved)","    assert len(result.df) == 3","    ","    # All should have same ts_str","    assert all(result.df[\"ts_str\"] == \"2013/1/1 09:30:00\")","    ","    # But different close values","    assert result.df.iloc[0][\"close\"] == 104.0","    assert result.df.iloc[1][\"close\"] == 105.0","    assert result.df.iloc[2][\"close\"] == 106.0","","","def test_no_dropna_called(temp_dir: Path) -> None:","    \"\"\"Regression test: Ensure dropna is never called internally (volume=0 preserved).\"\"\"","    txt_path = temp_dir / \"test_no_dropna.txt\"","    txt_content = \"\"\"Date,Time,Open,High,Low,Close,TotalVolume","2013/1/1,09:30:00,100.0,105.0,99.0,104.0,0","2013/1/1,10:00:00,104.0,106.0,103.0,105.0,0","2013/1/2,09:30:00,105.0,107.0,104.0,106.0,0","\"\"\"","    txt_path.write_text(txt_content, encoding=\"utf-8\")","    ","    result = ingest_raw_txt(txt_path)","    ","    # If dropna was called on volume, rows with volume=0 might be dropped","    # But we expect all 3 rows preserved","    assert len(result.df) == 3","    ","    # All should have volume=0","    assert all(result.df[\"volume\"] == 0)","",""]}
{"type":"file_footer","path":"tests/test_data_ingest_raw_means_raw.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_data_layout.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":627,"sha256":"f8196b4ec05eecf8a3c5f8b547de6f1322cc8d44f59e975d2864e546bb593dc4","total_lines":30,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_data_layout.py","chunk_index":0,"line_start":1,"line_end":30,"content":["","import numpy as np","import pytest","from data.layout import normalize_bars","","","def test_normalize_bars_dtype_and_contiguous():","    o = np.arange(10, dtype=np.float32)[::2]","    h = o + 1","    l = o - 1","    c = o + 0.5","","    bars = normalize_bars(o, h, l, c)","","    for arr in (bars.open, bars.high, bars.low, bars.close):","        assert arr.dtype == np.float64","        assert arr.flags[\"C_CONTIGUOUS\"]","","","def test_normalize_bars_reject_nan():","    o = np.array([1.0, np.nan])","    h = np.array([1.0, 2.0])","    l = np.array([0.5, 1.5])","    c = np.array([0.8, 1.8])","","    with pytest.raises(ValueError):","        normalize_bars(o, h, l, c)","","",""]}
{"type":"file_footer","path":"tests/test_data_layout.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_day_bar_definition.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3675,"sha256":"f823d9f53da118f92f6d14d718c1c901875ca1466d8b31e21fbcd246d0454853","total_lines":99,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_day_bar_definition.py","chunk_index":0,"line_start":1,"line_end":99,"content":["","\"\"\"Test DAY bar definition: one complete session per bar.\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pandas as pd","import pytest","","from data.session.kbar import aggregate_kbar","from data.session.loader import load_session_profile","","","@pytest.fixture","def mnq_profile(profiles_root: Path) -> Path:","    \"\"\"Load CME.MNQ session profile.\"\"\"","    profile_path = profiles_root / \"CME_MNQ_TPE_v1.yaml\"","    return profile_path","","","def test_day_bar_one_session(mnq_profile: Path) -> None:","    \"\"\"Test DAY bar = one complete DAY session.\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Create bars for one complete DAY session","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/1/1 08:45:00\",  # DAY session start","            \"2013/1/1 09:00:00\",","            \"2013/1/1 10:00:00\",","            \"2013/1/1 11:00:00\",","            \"2013/1/1 12:00:00\",","            \"2013/1/1 13:00:00\",","            \"2013/1/1 13:44:00\",  # Last bar before session end","        ],","        \"open\": [100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0],","        \"high\": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5, 106.5],","        \"low\": [99.5, 100.5, 101.5, 102.5, 103.5, 104.5, 105.5],","        \"close\": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5, 106.5],","        \"volume\": [1000, 1100, 1200, 1300, 1400, 1500, 1600],","    })","    ","    result = aggregate_kbar(df, \"DAY\", profile)","    ","    # Should have exactly one DAY bar","    assert len(result) == 1, f\"Should have 1 DAY bar, got {len(result)}\"","    ","    # Verify the bar contains all DAY session bars","    day_bar = result.iloc[0]","    assert day_bar[\"open\"] == 100.0, \"Open should be first bar's open\"","    assert day_bar[\"high\"] == 106.5, \"High should be max of all bars\"","    assert day_bar[\"low\"] == 99.5, \"Low should be min of all bars\"","    assert day_bar[\"close\"] == 106.5, \"Close should be last bar's close\"","    assert day_bar[\"volume\"] == sum([1000, 1100, 1200, 1300, 1400, 1500, 1600]), \"Volume should be sum\"","    ","    # Verify ts_str is anchored to session start","    ts_str = day_bar[\"ts_str\"]","    time_part = ts_str.split(\" \")[1]","    assert time_part == \"08:45:00\", f\"DAY bar should be anchored to session start, got {time_part}\"","","","def test_day_bar_multiple_sessions(mnq_profile: Path) -> None:","    \"\"\"Test DAY bars for multiple sessions.\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Create bars for DAY and NIGHT sessions on same day","    df = pd.DataFrame({","        \"ts_str\": [","            # DAY session","            \"2013/1/1 08:45:00\",","            \"2013/1/1 10:00:00\",","            \"2013/1/1 13:00:00\",","            # NIGHT session","            \"2013/1/1 21:00:00\",","            \"2013/1/1 23:00:00\",","            \"2013/1/2 02:00:00\",","        ],","        \"open\": [100.0, 101.0, 102.0, 103.0, 104.0, 105.0],","        \"high\": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5],","        \"low\": [99.5, 100.5, 101.5, 102.5, 103.5, 104.5],","        \"close\": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5],","        \"volume\": [1000, 1100, 1200, 1300, 1400, 1500],","    })","    ","    result = aggregate_kbar(df, \"DAY\", profile)","    ","    # Should have 2 DAY bars (one for DAY session, one for NIGHT session)","    assert len(result) == 2, f\"Should have 2 DAY bars (DAY + NIGHT), got {len(result)}\"","    ","    # Verify DAY session bar","    day_bar = result[result[\"ts_str\"].str.contains(\"2013/1/1 08:45:00\")].iloc[0]","    assert day_bar[\"volume\"] == 1000 + 1100 + 1200, \"DAY bar volume should sum DAY session bars\"","    ","    # Verify NIGHT session bar","    night_bar = result[result[\"ts_str\"].str.contains(\"2013/1/1 21:00:00\")].iloc[0]","    assert night_bar[\"volume\"] == 1300 + 1400 + 1500, \"NIGHT bar volume should sum NIGHT session bars\"","",""]}
{"type":"file_footer","path":"tests/test_day_bar_definition.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_dtype_compression_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":16257,"sha256":"881440ca28f66c11bb2c9f4321314690e8e1c1102517951516e784a3ee7b3922","total_lines":419,"chunk_count":3}
{"type":"file_chunk","path":"tests/test_dtype_compression_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for dtype compression (Phase P1).","","These tests ensure:","1. INDEX_DTYPE=int32 safety: order_id, created_bar, qty never exceed 2^31-1","2. UINT8 enum consistency: role/kind/side correctly encode/decode without sentinel issues","\"\"\"","","import numpy as np","import pytest","","from config.dtypes import (","    INDEX_DTYPE,","    INTENT_ENUM_DTYPE,","    INTENT_PRICE_DTYPE,",")","from engine.constants import (","    KIND_LIMIT,","    KIND_STOP,","    ROLE_ENTRY,","    ROLE_EXIT,","    SIDE_BUY,","    SIDE_SELL,",")","from engine.engine_jit import (","    SIDE_BUY_CODE,","    SIDE_SELL_CODE,","    _pack_intents,","    simulate_arrays,",")","from engine.types import BarArrays, OrderIntent, OrderKind, OrderRole, Side","","","class TestIndexDtypeSafety:","    \"\"\"Test that INDEX_DTYPE=int32 is safe for all use cases.\"\"\"","","    def test_order_id_max_value_contract(self):","        \"\"\"","        Contract: order_id must never exceed 2^31-1 (int32 max).","        ","        In strategy/kernel.py, order_id is generated as:","        - Entry: np.arange(1, n_entry + 1)","        - Exit: np.arange(n_entry + 1, n_entry + 1 + exit_intents_count)","        ","        Maximum order_id = n_entry + exit_intents_count","        ","        For 200,000 bars with reasonable intent generation, this should be << 2^31-1.","        \"\"\"","        INT32_MAX = 2**31 - 1","        ","        # Simulate worst-case scenario: 200,000 bars, each bar generates 1 entry + 1 exit","        # This is extremely conservative (realistic scenarios generate far fewer intents)","        n_bars = 200_000","        max_intents_per_bar = 2  # 1 entry + 1 exit per bar (worst case)","        max_total_intents = n_bars * max_intents_per_bar","        ","        # Maximum order_id would be max_total_intents (if all are sequential)","        max_order_id = max_total_intents","        ","        assert max_order_id < INT32_MAX, (","            f\"order_id would exceed int32 max ({INT32_MAX}) \"","            f\"with {n_bars} bars and {max_intents_per_bar} intents per bar. \"","            f\"Max order_id would be {max_order_id}\"","        )","        ","        # More realistic: check that even with 10x safety margin, we're still safe","        safety_margin = 10","        assert max_order_id * safety_margin < INT32_MAX, (","            f\"order_id with {safety_margin}x safety margin would exceed int32 max\"","        )","","    def test_created_bar_max_value_contract(self):","        \"\"\"","        Contract: created_bar must never exceed 2^31-1.","        ","        created_bar is a bar index, so max value = n_bars - 1.","        For 200,000 bars, max created_bar = 199,999 << 2^31-1.","        \"\"\"","        INT32_MAX = 2**31 - 1","        ","        # Worst case: 200,000 bars","        max_bars = 200_000","        max_created_bar = max_bars - 1","        ","        assert max_created_bar < INT32_MAX, (","            f\"created_bar would exceed int32 max ({INT32_MAX}) \"","            f\"with {max_bars} bars. Max created_bar would be {max_created_bar}\"","        )","","    def test_qty_max_value_contract(self):","        \"\"\"","        Contract: qty must never exceed 2^31-1.","        ","        qty is typically small (1, 10, 100, etc.), so this should be safe.","        \"\"\"","        INT32_MAX = 2**31 - 1","        ","        # Realistic qty values are much smaller than int32 max","        realistic_max_qty = 1_000_000  # Even 1M shares is << 2^31-1","        ","        assert realistic_max_qty < INT32_MAX, (","            f\"qty would exceed int32 max ({INT32_MAX}) \"","            f\"with realistic max qty of {realistic_max_qty}\"","        )","","    def test_order_id_generation_in_kernel(self):","        \"\"\"","        Test that actual order_id generation in kernel stays within int32 range.","        ","        This test simulates the order_id generation logic from strategy/kernel.py.","        \"\"\"","        INT32_MAX = 2**31 - 1","        ","        # Simulate realistic scenario: 200,000 bars, ~1000 entry intents, ~500 exit intents","        n_entry = 1000","        n_exit = 500","        ","        # Entry order_ids: np.arange(1, n_entry + 1)","        entry_order_ids = np.arange(1, n_entry + 1, dtype=INDEX_DTYPE)","        assert entry_order_ids.max() < INT32_MAX","        ","        # Exit order_ids: np.arange(n_entry + 1, n_entry + 1 + n_exit)","        exit_order_ids = np.arange(n_entry + 1, n_entry + 1 + n_exit, dtype=INDEX_DTYPE)","        max_order_id = exit_order_ids.max()","        ","        assert max_order_id < INT32_MAX, (","            f\"Generated order_id {max_order_id} exceeds int32 max ({INT32_MAX})\"","        )","","","class TestUint8EnumConsistency:","    \"\"\"Test that uint8 enum encoding/decoding is consistent and safe.\"\"\"","","    def test_role_enum_encoding(self):","        \"\"\"Test that role enum values encode correctly as uint8.\"\"\"","        # ROLE_EXIT = 0, ROLE_ENTRY = 1","        exit_val = INTENT_ENUM_DTYPE(ROLE_EXIT)","        entry_val = INTENT_ENUM_DTYPE(ROLE_ENTRY)","        ","        assert exit_val == 0","        assert entry_val == 1","        assert exit_val.dtype == np.uint8","        assert entry_val.dtype == np.uint8","","    def test_kind_enum_encoding(self):","        \"\"\"Test that kind enum values encode correctly as uint8.\"\"\"","        # KIND_STOP = 0, KIND_LIMIT = 1","        stop_val = INTENT_ENUM_DTYPE(KIND_STOP)","        limit_val = INTENT_ENUM_DTYPE(KIND_LIMIT)","        ","        assert stop_val == 0","        assert limit_val == 1","        assert stop_val.dtype == np.uint8","        assert limit_val.dtype == np.uint8","","    def test_side_enum_encoding(self):","        \"\"\"","        Test that side enum values encode correctly as uint8.","        ","        SIDE_BUY_CODE = 1, SIDE_SELL_CODE = 255 (avoid -1 cast deprecation)","        \"\"\"","        buy_val = INTENT_ENUM_DTYPE(SIDE_BUY_CODE)","        sell_val = INTENT_ENUM_DTYPE(SIDE_SELL_CODE)","        ","        assert buy_val == 1","        assert sell_val == 255","        assert buy_val.dtype == np.uint8","        assert sell_val.dtype == np.uint8","","    def test_side_enum_decoding_consistency(self):","        \"\"\"","        Test that side enum decoding correctly handles uint8 values.","        ","        Critical: uint8 value 255 (SIDE_SELL_CODE) must decode back to SELL.","        \"\"\"","        # Encode SIDE_SELL_CODE (255) as uint8","        sell_encoded = INTENT_ENUM_DTYPE(SIDE_SELL_CODE)","        assert sell_encoded == 255","        ","        # Decode: int(sd[i]) == SIDE_BUY (1) ? BUY : SELL","        # If sd[i] = 255, int(255) != 1, so it should decode to SELL","        decoded_is_buy = int(sell_encoded) == SIDE_BUY","        decoded_is_sell = int(sell_encoded) != SIDE_BUY","        ","        assert not decoded_is_buy, \"uint8 value 255 should not decode to BUY\"","        assert decoded_is_sell, \"uint8 value 255 should decode to SELL\"","        ","        # Also test BUY encoding/decoding","        buy_encoded = INTENT_ENUM_DTYPE(SIDE_BUY_CODE)","        assert buy_encoded == 1","        decoded_is_buy = int(buy_encoded) == SIDE_BUY_CODE","        assert decoded_is_buy, \"uint8 value 1 should decode to BUY\"","","    def test_allowed_enum_values_contract(self):","        \"\"\"","        Contract: enum arrays must only contain explicitly allowed values.","        ","        This test ensures that:","        1. Only valid enum values are used (no uninitialized/invalid values)","        2. Decoding functions will raise ValueError for invalid values (strict mode)"]}
{"type":"file_chunk","path":"tests/test_dtype_compression_contract.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        ","        Allowed values:","        - role: {0 (EXIT), 1 (ENTRY)}","        - kind: {0 (STOP), 1 (LIMIT)}","        - side: {1 (BUY), 255 (SELL as uint8)}","        \"\"\"","        # Define allowed values explicitly","        ALLOWED_ROLE_VALUES = {ROLE_EXIT, ROLE_ENTRY}  # {0, 1}","        ALLOWED_KIND_VALUES = {KIND_STOP, KIND_LIMIT}  # {0, 1}","        ALLOWED_SIDE_VALUES = {SIDE_BUY_CODE, SIDE_SELL_CODE}  # {1, 255} - avoid -1 cast","        ","        # Test that encoding produces only allowed values","        role_encoded = [INTENT_ENUM_DTYPE(ROLE_EXIT), INTENT_ENUM_DTYPE(ROLE_ENTRY)]","        kind_encoded = [INTENT_ENUM_DTYPE(KIND_STOP), INTENT_ENUM_DTYPE(KIND_LIMIT)]","        side_encoded = [INTENT_ENUM_DTYPE(SIDE_BUY_CODE), INTENT_ENUM_DTYPE(SIDE_SELL_CODE)]","        ","        for val in role_encoded:","            assert int(val) in ALLOWED_ROLE_VALUES, f\"Role value {val} not in allowed set {ALLOWED_ROLE_VALUES}\"","        ","        for val in kind_encoded:","            assert int(val) in ALLOWED_KIND_VALUES, f\"Kind value {val} not in allowed set {ALLOWED_KIND_VALUES}\"","        ","        for val in side_encoded:","            assert int(val) in ALLOWED_SIDE_VALUES, f\"Side value {val} not in allowed set {ALLOWED_SIDE_VALUES}\"","        ","        # Test that invalid values raise ValueError (strict decoding)","        from engine.engine_jit import _role_from_int, _kind_from_int, _side_from_int","        ","        # Test invalid role values","        with pytest.raises(ValueError, match=\"Invalid role enum value\"):","            _role_from_int(2)","        with pytest.raises(ValueError, match=\"Invalid role enum value\"):","            _role_from_int(-1)","        ","        # Test invalid kind values","        with pytest.raises(ValueError, match=\"Invalid kind enum value\"):","            _kind_from_int(2)","        with pytest.raises(ValueError, match=\"Invalid kind enum value\"):","            _kind_from_int(-1)","        ","        # Test invalid side values","        with pytest.raises(ValueError, match=\"Invalid side enum value\"):","            _side_from_int(0)","        with pytest.raises(ValueError, match=\"Invalid side enum value\"):","            _side_from_int(2)","        with pytest.raises(ValueError, match=\"Invalid side enum value\"):","            _side_from_int(100)","        ","        # Test valid values don't raise","        assert _role_from_int(0) == OrderRole.EXIT","        assert _role_from_int(1) == OrderRole.ENTRY","        assert _kind_from_int(0) == OrderKind.STOP","        assert _kind_from_int(1) == OrderKind.LIMIT","        assert _side_from_int(SIDE_BUY_CODE) == Side.BUY","        assert _side_from_int(SIDE_SELL_CODE) == Side.SELL","","    def test_pack_intents_roundtrip(self):","        \"\"\"","        Test that packing intents and decoding them preserves enum values correctly.","        ","        This is an integration test to ensure the full encode/decode cycle works.","        \"\"\"","        # Create test intents with all enum combinations","        intents = [","            OrderIntent(","                order_id=1,","                created_bar=0,","                role=OrderRole.EXIT,","                kind=OrderKind.STOP,","                side=Side.SELL,  # -1 -> uint8(255)","                price=100.0,","                qty=1,","            ),","            OrderIntent(","                order_id=2,","                created_bar=0,","                role=OrderRole.ENTRY,","                kind=OrderKind.LIMIT,","                side=Side.BUY,  # 1 -> uint8(1)","                price=101.0,","                qty=1,","            ),","        ]","        ","        # Pack intents","        order_id, created_bar, role, kind, side, price, qty = _pack_intents(intents)","        ","        # Verify dtypes","        assert order_id.dtype == INDEX_DTYPE","        assert created_bar.dtype == INDEX_DTYPE","        assert role.dtype == INTENT_ENUM_DTYPE","        assert kind.dtype == INTENT_ENUM_DTYPE","        assert side.dtype == INTENT_ENUM_DTYPE","        assert price.dtype == INTENT_PRICE_DTYPE","        assert qty.dtype == INDEX_DTYPE","        ","        # Verify enum values","        assert role[0] == ROLE_EXIT  # 0","        assert role[1] == ROLE_ENTRY  # 1","        assert kind[0] == KIND_STOP  # 0","        assert kind[1] == KIND_LIMIT  # 1","        assert side[0] == SIDE_SELL_CODE  # SELL -> uint8(255)","        assert side[1] == SIDE_BUY_CODE  # BUY -> uint8(1)","        ","        # Verify decoding logic (as used in engine_jit.py)","        # Decode role","        decoded_role_0 = OrderRole.EXIT if int(role[0]) == ROLE_EXIT else OrderRole.ENTRY","        assert decoded_role_0 == OrderRole.EXIT","        ","        decoded_role_1 = OrderRole.EXIT if int(role[1]) == ROLE_EXIT else OrderRole.ENTRY","        assert decoded_role_1 == OrderRole.ENTRY","        ","        # Decode kind","        decoded_kind_0 = OrderKind.STOP if int(kind[0]) == KIND_STOP else OrderKind.LIMIT","        assert decoded_kind_0 == OrderKind.STOP","        ","        decoded_kind_1 = OrderKind.STOP if int(kind[1]) == KIND_STOP else OrderKind.LIMIT","        assert decoded_kind_1 == OrderKind.LIMIT","        ","        # Decode side (critical: uint8(255) must decode to SELL)","        decoded_side_0 = Side.BUY if int(side[0]) == SIDE_BUY_CODE else Side.SELL","        assert decoded_side_0 == Side.SELL, f\"uint8(255) should decode to SELL, got {decoded_side_0}\"","        ","        decoded_side_1 = Side.BUY if int(side[1]) == SIDE_BUY_CODE else Side.SELL","        assert decoded_side_1 == Side.BUY, f\"uint8(1) should decode to BUY, got {decoded_side_1}\"","","    def test_simulate_arrays_accepts_uint8_enums(self):","        \"\"\"","        Test that simulate_arrays correctly accepts and processes uint8 enum arrays.","        ","        This ensures the numba kernel can handle uint8 enum values correctly.","        \"\"\"","        # Create minimal test data","        bars = BarArrays(","            open=np.array([100.0, 101.0], dtype=np.float64),","            high=np.array([102.0, 103.0], dtype=np.float64),","            low=np.array([99.0, 100.0], dtype=np.float64),","            close=np.array([101.0, 102.0], dtype=np.float64),","        )","        ","        # Create intent arrays with uint8 enums","        order_id = np.array([1], dtype=INDEX_DTYPE)","        created_bar = np.array([0], dtype=INDEX_DTYPE)","        role = np.array([ROLE_ENTRY], dtype=INTENT_ENUM_DTYPE)","        kind = np.array([KIND_STOP], dtype=INTENT_ENUM_DTYPE)","        side = np.array([SIDE_BUY_CODE], dtype=INTENT_ENUM_DTYPE)  # 1 -> uint8(1)","        price = np.array([102.0], dtype=INTENT_PRICE_DTYPE)","        qty = np.array([1], dtype=INDEX_DTYPE)","        ","        # This should not raise any dtype-related errors","        fills = simulate_arrays(","            bars,","            order_id=order_id,","            created_bar=created_bar,","            role=role,","            kind=kind,","            side=side,","            price=price,","            qty=qty,","            ttl_bars=1,","        )","        ","        # Verify fills were generated (basic sanity check)","        assert isinstance(fills, list)","        ","        # Test with SELL side (uint8 value 255)","        side_sell = np.array([SIDE_SELL_CODE], dtype=INTENT_ENUM_DTYPE)  # 255 (avoid -1 cast)","        fills_sell = simulate_arrays(","            bars,","            order_id=order_id,","            created_bar=created_bar,","            role=role,","            kind=kind,","            side=side_sell,","            price=price,","            qty=qty,","            ttl_bars=1,","        )","        ","        # Should not raise errors","        assert isinstance(fills_sell, list)","        ","        # Verify that fills with SELL side decode correctly","        # Note: numba kernel outputs uint8(255) as 255.0, but _side_from_int correctly decodes it","        if fills_sell:","            # The fill's side should be Side.SELL","            assert fills_sell[0].side == Side.SELL, (","                f\"Fill with uint8(255) side should decode to Side.SELL, got {fills_sell[0].side}\"","            )","","    def test_side_output_value_contract(self):","        \"\"\"","        Contract: numba kernel outputs side as float.","        ","        Note: uint8(255) from SIDE_SELL will output as 255.0, not -1.0.","        This is acceptable as long as _side_from_int correctly decodes it.","        ","        With strict mode, invalid values will raise ValueError instead of silently","        decoding to SELL.","        \"\"\""]}
{"type":"file_chunk","path":"tests/test_dtype_compression_contract.py","chunk_index":2,"line_start":401,"line_end":419,"content":["        from engine.engine_jit import _side_from_int","        ","        # Test that _side_from_int correctly handles allowed values","        assert _side_from_int(SIDE_BUY_CODE) == Side.BUY","        assert _side_from_int(SIDE_SELL_CODE) == Side.SELL, (","            f\"_side_from_int({SIDE_SELL_CODE}) should decode to Side.SELL, not BUY\"","        )","        ","        # Test that invalid values raise ValueError (strict mode)","        with pytest.raises(ValueError, match=\"Invalid side enum value\"):","            _side_from_int(0)","        with pytest.raises(ValueError, match=\"Invalid side enum value\"):","            _side_from_int(-1)","        with pytest.raises(ValueError, match=\"Invalid side enum value\"):","            _side_from_int(2)","        with pytest.raises(ValueError, match=\"Invalid side enum value\"):","            _side_from_int(100)","",""]}
{"type":"file_footer","path":"tests/test_dtype_compression_contract.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"tests/test_engine_constitution.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3417,"sha256":"84e776242a546b30e992f9961c48acb63f21627ad125e2a8f2f9f8693eebca95","total_lines":103,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_engine_constitution.py","chunk_index":0,"line_start":1,"line_end":103,"content":["","import numpy as np","","from data.layout import normalize_bars","from engine.matcher_core import simulate","from engine.types import OrderIntent, OrderKind, OrderRole, Side","","","def _bars1(o, h, l, c):","    return normalize_bars(","        np.array([o], dtype=np.float64),","        np.array([h], dtype=np.float64),","        np.array([l], dtype=np.float64),","        np.array([c], dtype=np.float64),","    )","","","def _bars2(o0, h0, l0, c0, o1, h1, l1, c1):","    return normalize_bars(","        np.array([o0, o1], dtype=np.float64),","        np.array([h0, h1], dtype=np.float64),","        np.array([l0, l1], dtype=np.float64),","        np.array([c0, c1], dtype=np.float64),","    )","","","def test_tc01_buy_stop_normal():","    bars = _bars1(90, 105, 90, 100)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","    ]","    fills = simulate(bars, intents)","    assert len(fills) == 1","    assert fills[0].price == 100.0","","","def test_tc02_buy_stop_gap_up_fill_open():","    bars = _bars1(105, 110, 105, 108)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","    ]","    fills = simulate(bars, intents)","    assert len(fills) == 1","    assert fills[0].price == 105.0","","","def test_tc03_sell_stop_gap_down_fill_open():","    bars = _bars1(90, 95, 80, 85)","    intents = [","        # Exit a long position requires SELL stop; we will enter long first in same bar is not allowed here,","        # so we simulate already-in-position by forcing an entry earlier: created_bar=-2 triggers at -1 (ignored),","        # Instead: use two bars and enter on bar0, exit on bar1.","    ]","    bars2 = _bars2(","        100, 100, 100, 100,   # bar0: enter long at 100 (buy stop gap/normal both ok)","        90, 95, 80, 85        # bar1: exit stop triggers gap down open","    )","    intents2 = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=100.0),","    ]","    fills = simulate(bars2, intents2)","    assert len(fills) == 2","    # second fill is the exit","    assert fills[1].price == 90.0","","","def test_tc08_next_bar_active_not_same_bar():","    # bar0 has high 105 which would hit stop 102, but order created at bar0 must not fill at bar0.","    # bar1 hits again, should fill at bar1.","    bars = _bars2(","        100, 105, 95, 100,","        100, 105, 95, 100,","    )","    intents = [","        OrderIntent(order_id=1, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),","    ]","    fills = simulate(bars, intents)","    assert len(fills) == 1","    assert fills[0].bar_index == 1","    assert fills[0].price == 102.0","","","def test_tc09_open_equals_stop_gap_branch_but_same_price():","    bars = _bars1(100, 100, 90, 95)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","    ]","    fills = simulate(bars, intents)","    assert len(fills) == 1","    assert fills[0].price == 100.0","","","def test_tc10_no_fill_when_not_touched():","    bars = _bars1(90, 95, 90, 92)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","    ]","    fills = simulate(bars, intents)","    assert fills == []","","",""]}
{"type":"file_footer","path":"tests/test_engine_constitution.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_engine_fill_buffer_capacity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2404,"sha256":"67a9b4faac0748013e62688956fbfc42102e9bfad121451abe6c9df70d30ecd9","total_lines":68,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_engine_fill_buffer_capacity.py","chunk_index":0,"line_start":1,"line_end":68,"content":["","\"\"\"Test that engine fill buffer handles extreme intents without crashing.\"\"\"","","from __future__ import annotations","","import numpy as np","import pytest","","from data.layout import normalize_bars","from engine.engine_jit import STATUS_BUFFER_FULL, STATUS_OK, simulate as simulate_jit","from engine.types import OrderIntent, OrderKind, OrderRole, Side","","","def test_engine_fill_buffer_capacity_extreme_intents() -> None:","    \"\"\"","    Test that engine handles extreme intents (many intents, few bars) without crashing.","    ","    Scenario: bars=10, intents=500","    Each intent is designed to fill (STOP BUY that triggers immediately).","    \"\"\"","    n_bars = 10","    n_intents = 500","","    # Create bars with high volatility to ensure fills","    bars = normalize_bars(","        np.array([100.0] * n_bars, dtype=np.float64),","        np.array([120.0] * n_bars, dtype=np.float64),","        np.array([80.0] * n_bars, dtype=np.float64),","        np.array([110.0] * n_bars, dtype=np.float64),","    )","","    # Create many intents that will all fill (STOP BUY at 105, which is below high=120)","    # Distribute across bars to maximize fills","    intents = []","    for i in range(n_intents):","        created_bar = (i % n_bars) - 1  # Distribute across bars","        intents.append(","            OrderIntent(","                order_id=i,","                created_bar=created_bar,","                role=OrderRole.ENTRY,","                kind=OrderKind.STOP,","                side=Side.BUY,","                price=105.0,  # Will trigger on any bar (high=120 > 105)","                qty=1,","            )","        )","","    # Should not crash or segfault","    try:","        fills = simulate_jit(bars, intents)","        # If we get here, no segfault occurred","        ","        # Fills should be bounded by n_intents (each intent can produce at most 1 fill)","        assert len(fills) <= n_intents, f\"fills ({len(fills)}) should not exceed n_intents ({n_intents})\"","        ","        # Should have some fills (most intents should trigger)","        assert len(fills) > 0, \"Should have at least some fills\"","        ","    except RuntimeError as e:","        # If buffer is full, error message should be graceful (not segfault)","        error_msg = str(e)","        assert \"buffer full\" in error_msg.lower() or \"buffer_full\" in error_msg.lower(), (","            f\"Expected buffer full error, got: {error_msg}\"","        )","        # This is acceptable - buffer protection worked correctly","",""]}
{"type":"file_footer","path":"tests/test_engine_fill_buffer_capacity.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_engine_gaps_and_priority.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2858,"sha256":"094e3009d32e5dafd071676087e652ee26a039905ef7956bcb0158e94c94bfed","total_lines":77,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_engine_gaps_and_priority.py","chunk_index":0,"line_start":1,"line_end":77,"content":["","import numpy as np","","from data.layout import normalize_bars","from engine.matcher_core import simulate","from engine.types import OrderIntent, OrderKind, OrderRole, Side","","","def _bars1(o, h, l, c):","    return normalize_bars(","        np.array([o], dtype=np.float64),","        np.array([h], dtype=np.float64),","        np.array([l], dtype=np.float64),","        np.array([c], dtype=np.float64),","    )","","","def test_tc04_buy_limit_gap_down_better_fill_open():","    bars = _bars1(90, 95, 85, 92)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.BUY, price=100.0),","    ]","    fills = simulate(bars, intents)","    assert len(fills) == 1","    assert fills[0].price == 90.0","","","def test_tc05_sell_limit_gap_up_better_fill_open():","    bars = _bars1(105, 110, 100, 108)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.SELL, price=100.0),","    ]","    fills = simulate(bars, intents)","    assert len(fills) == 1","    assert fills[0].price == 105.0","","","def test_tc06_priority_stop_wins_over_limit_on_exit():","    # First enter long on this same bar, then exit on next bar where both stop and limit are triggerable.","    # Bar0: enter long at 100 (buy stop hits)","    # Bar1: both exit stop 90 and exit limit 110 are touchable (high=110, low=80), STOP must win (fill=90)","    bars = normalize_bars(","        np.array([100, 100], dtype=np.float64),","        np.array([110, 110], dtype=np.float64),","        np.array([90, 80], dtype=np.float64),","        np.array([100, 90], dtype=np.float64),","    )","","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=90.0),","        OrderIntent(order_id=3, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.LIMIT, side=Side.SELL, price=110.0),","    ]","    fills = simulate(bars, intents)","    assert len(fills) == 2","    # Second fill is exit; STOP wins -> 90","    assert fills[1].kind == OrderKind.STOP","    assert fills[1].price == 90.0","","","def test_tc07_same_bar_entry_then_exit():","    # Same bar allows Entry then Exit.","    # Bar: O=100 H=120 L=90 C=110","    # Entry: Buy Stop 105 -> fills at 105 (since open 100 < 105 and high 120 >= 105)","    # Exit: Sell Stop 95 -> after entry, low 90 <= 95 -> fills at 95","    bars = _bars1(100, 120, 90, 110)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),","        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),","    ]","    fills = simulate(bars, intents)","    assert len(fills) == 2","    assert fills[0].price == 105.0","    assert fills[1].price == 95.0","","",""]}
{"type":"file_footer","path":"tests/test_engine_gaps_and_priority.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_engine_jit_active_book_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8757,"sha256":"6e7c43a6c223e71806fee46f08b9172399000c1a70fa757d465addf88e4448cc","total_lines":222,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_engine_jit_active_book_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","from __future__ import annotations","","import os","","import numpy as np","import pytest","","from data.layout import normalize_bars","from engine.engine_jit import _simulate_with_ttl, simulate as simulate_jit","from engine.matcher_core import simulate as simulate_py","from engine.types import Fill, OrderIntent, OrderKind, OrderRole, Side","","","def _assert_fills_equal(a: list[Fill], b: list[Fill]) -> None:","    assert len(a) == len(b)","    for fa, fb in zip(a, b):","        assert fa.bar_index == fb.bar_index","        assert fa.role == fb.role","        assert fa.kind == fb.kind","        assert fa.side == fb.side","        assert fa.qty == fb.qty","        assert fa.order_id == fb.order_id","        assert abs(fa.price - fb.price) <= 1e-9","","","def test_jit_sorted_invariance_matches_python() -> None:","    # Bars: 3 bars, deterministic highs/lows for STOP triggers","    bars = normalize_bars(","        np.array([100.0, 100.0, 100.0], dtype=np.float64),","        np.array([110.0, 110.0, 110.0], dtype=np.float64),","        np.array([90.0, 90.0, 90.0], dtype=np.float64),","        np.array([100.0, 100.0, 100.0], dtype=np.float64),","    )","","    # Intents across multiple activate bars (created_bar = t-1)","    intents = [","        # activate on bar0 (created -1)","        OrderIntent(3, -1, OrderRole.EXIT, OrderKind.STOP, Side.SELL, 95.0, 1),","        OrderIntent(2, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 105.0, 1),","        # activate on bar1 (created 0)","        OrderIntent(6, 0, OrderRole.EXIT, OrderKind.LIMIT, Side.SELL, 110.0, 1),","        OrderIntent(5, 0, OrderRole.ENTRY, OrderKind.LIMIT, Side.BUY, 99.0, 1),","        # activate on bar2 (created 1)","        OrderIntent(9, 1, OrderRole.EXIT, OrderKind.STOP, Side.SELL, 90.0, 1),","        OrderIntent(8, 1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),","    ]","","    shuffled = list(intents)","    rng = np.random.default_rng(123)","    rng.shuffle(shuffled)","","    # JIT simulate sorts internally for cursor+book; it must be invariant to input ordering.","    jit_a = simulate_jit(bars, shuffled)","    jit_b = simulate_jit(bars, intents)","    _assert_fills_equal(jit_a, jit_b)","","    # Also must match Python reference semantics.","    py = simulate_py(bars, shuffled)","    _assert_fills_equal(jit_a, py)","","","def test_one_bar_max_one_entry_one_exit_defense() -> None:","    # Single bar is enough: created_bar=-1 activates on bar 0.","    bars = normalize_bars(","        np.array([100.0], dtype=np.float64),","        np.array([120.0], dtype=np.float64),","        np.array([80.0], dtype=np.float64),","        np.array([110.0], dtype=np.float64),","    )","","    # Same activate bar contains Entry1, Exit1, Entry2.","    intents = [","        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 105.0, 1),","        OrderIntent(2, -1, OrderRole.EXIT, OrderKind.STOP, Side.SELL, 95.0, 1),","        OrderIntent(3, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 110.0, 1),","    ]","","    fills = simulate_jit(bars, intents)","    assert len(fills) == 2","    assert fills[0].order_id == 1","    assert fills[1].order_id == 2","","","def test_ttl_one_shot_vs_gtc_extension_point() -> None:","    # Skip if JIT is disabled; ttl=0 is a JIT-only extension behavior.","    import engine.engine_jit as ej","","    if ej.nb is None or os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() == \"1\":","        pytest.skip(\"numba not available or disabled; ttl=0 extension tested only under JIT\")","","    # Bar0: stop not touched, Bar1: stop touched","    bars = normalize_bars(","        np.array([90.0, 90.0], dtype=np.float64),","        np.array([99.0, 110.0], dtype=np.float64),","        np.array([90.0, 90.0], dtype=np.float64),","        np.array([95.0, 100.0], dtype=np.float64),","    )","    intents = [","        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),","    ]","","    # ttl=1 (default semantics): active only on bar0 -> no fill","    fills_ttl1 = simulate_jit(bars, intents)","    assert fills_ttl1 == []","","    # ttl=0 (GTC extension): order stays in book and can fill on bar1","    fills_gtc = _simulate_with_ttl(bars, intents, ttl_bars=0)","    assert len(fills_gtc) == 1","    assert fills_gtc[0].bar_index == 1","    assert abs(fills_gtc[0].price - 100.0) <= 1e-9","","","def test_ttl_one_expires_before_fill_opportunity() -> None:","    \"\"\"","    Case A: ttl=1 is one-shot next-bar-only (does not fill if not triggered on activate bar).","    ","    Scenario:","      - BUY STOP order, created_bar=-1 (activates at bar0)","      - bar0: high < stop (not triggered)","      - bar1: high >= stop (would trigger, but order expired)","      - ttl_bars=1: order should expire after bar0, not fill on bar1","    \"\"\"","    import engine.engine_jit as ej","","    if ej.nb is None or os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() == \"1\":","        pytest.skip(\"numba not available or disabled; ttl semantics tested only under JIT\")","","    # 2 bars: bar0 doesn't trigger, bar1 would trigger","    bars = normalize_bars(","        np.array([90.0, 90.0], dtype=np.float64),  # open","        np.array([99.0, 110.0], dtype=np.float64),  # high: bar0 < 100, bar1 >= 100","        np.array([90.0, 90.0], dtype=np.float64),  # low","        np.array([95.0, 100.0], dtype=np.float64),  # close","    )","    intents = [","        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),","    ]","","    # ttl_bars=1: activate_bar=0, expire_bar=0, so at bar1 (t=1) > expire_bar (0), order expired","    fills_ttl1 = _simulate_with_ttl(bars, intents, ttl_bars=1)","    assert len(fills_ttl1) == 0, \"ttl=1 should expire after activate bar, no fill on bar1\"","","    # Verify JIT matches expected semantics","    # activate_bar = created_bar + 1 = -1 + 1 = 0","    # expire_bar = activate_bar + (ttl_bars - 1) = 0 + (1 - 1) = 0","    # At bar1 (t=1), t > expire_bar (0), so order should be removed before Step B/C","","","def test_ttl_zero_gtc_never_expires() -> None:","    \"\"\"","    Case B: ttl=0 is GTC (Good Till Canceled), order never expires.","    ","    Scenario:","      - BUY STOP order, created_bar=-1 (activates at bar0)","      - bar0: high < stop (not triggered)","      - bar1: high >= stop (triggers)","      - ttl_bars=0: order should remain active and fill on bar1","    \"\"\"","    import engine.engine_jit as ej","","    if ej.nb is None or os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() == \"1\":","        pytest.skip(\"numba not available or disabled; ttl semantics tested only under JIT\")","","    # 2 bars: bar0 doesn't trigger, bar1 triggers","    bars = normalize_bars(","        np.array([90.0, 90.0], dtype=np.float64),  # open","        np.array([99.0, 110.0], dtype=np.float64),  # high: bar0 < 100, bar1 >= 100","        np.array([90.0, 90.0], dtype=np.float64),  # low","        np.array([95.0, 100.0], dtype=np.float64),  # close","    )","    intents = [","        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),","    ]","","    # ttl_bars=0: GTC, order never expires, should fill on bar1","    fills_gtc = _simulate_with_ttl(bars, intents, ttl_bars=0)","    assert len(fills_gtc) == 1, \"ttl=0 (GTC) should allow fill on bar1\"","    assert fills_gtc[0].bar_index == 1, \"Fill should occur on bar1\"","    assert fills_gtc[0].order_id == 1","    assert abs(fills_gtc[0].price - 100.0) <= 1e-9, \"Fill price should be stop price\"","","","def test_ttl_semantics_three_bars() -> None:","    \"\"\"","    Additional test: verify ttl=1 semantics with 3 bars to ensure expiration timing is correct.","    ","    Scenario:","      - BUY STOP order, created_bar=-1 (activates at bar0)","      - bar0: high < stop (not triggered)","      - bar1: high < stop (not triggered)","      - bar2: high >= stop (would trigger, but order expired)","      - ttl_bars=1: order should expire after bar0, not fill on bar2","    \"\"\"","    import engine.engine_jit as ej","","    if ej.nb is None or os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() == \"1\":","        pytest.skip(\"numba not available or disabled; ttl semantics tested only under JIT\")","","    # 3 bars: bar0 and bar1 don't trigger, bar2 would trigger"]}
{"type":"file_chunk","path":"tests/test_engine_jit_active_book_contract.py","chunk_index":1,"line_start":201,"line_end":222,"content":["    bars = normalize_bars(","        np.array([90.0, 90.0, 90.0], dtype=np.float64),  # open","        np.array([99.0, 99.0, 110.0], dtype=np.float64),  # high: bar0,bar1 < 100, bar2 >= 100","        np.array([90.0, 90.0, 90.0], dtype=np.float64),  # low","        np.array([95.0, 95.0, 100.0], dtype=np.float64),  # close","    )","    intents = [","        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),","    ]","","    # ttl_bars=1: activate_bar=0, expire_bar=0, so at bar1 (t=1) > expire_bar (0), order expired","    fills_ttl1 = _simulate_with_ttl(bars, intents, ttl_bars=1)","    assert len(fills_ttl1) == 0, \"ttl=1 should expire after activate bar, no fill on bar2\"","","    # ttl_bars=0: GTC, should fill on bar2","    fills_gtc = _simulate_with_ttl(bars, intents, ttl_bars=0)","    assert len(fills_gtc) == 1, \"ttl=0 (GTC) should allow fill on bar2\"","    assert fills_gtc[0].bar_index == 2, \"Fill should occur on bar2\"","","","",""]}
{"type":"file_footer","path":"tests/test_engine_jit_active_book_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_engine_jit_fill_buffer_capacity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5243,"sha256":"b07cf1f480c32fb9f1ae7257bb27edd7348a19272409c9ea56ea203aa90820e3","total_lines":151,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_engine_jit_fill_buffer_capacity.py","chunk_index":0,"line_start":1,"line_end":151,"content":["","\"\"\"Test that fill buffer scales with n_intents and does not segfault.\"\"\"","","from __future__ import annotations","","import os","","import numpy as np","import pytest","","from data.layout import normalize_bars","from engine.engine_jit import STATUS_BUFFER_FULL, simulate as simulate_jit","from engine.types import OrderIntent, OrderKind, OrderRole, Side","","","def test_fill_buffer_scales_with_intents():","    \"\"\"","    Test that buffer size accommodates n_intents > n_bars*2.","    ","    Scenario: n_bars=10, n_intents=100","    Each intent is designed to fill (market entry with stop that triggers immediately).","    This tests that buffer scales with n_intents, not just n_bars*2.","    \"\"\"","    n_bars = 10","    n_intents = 100","    ","    # Create bars with high volatility to ensure fills","    bars = normalize_bars(","        np.array([100.0] * n_bars, dtype=np.float64),","        np.array([120.0] * n_bars, dtype=np.float64),","        np.array([80.0] * n_bars, dtype=np.float64),","        np.array([110.0] * n_bars, dtype=np.float64),","    )","    ","    # Create many intents that will all fill (STOP BUY at 105, which is below high=120)","    # Each intent activates on a different bar to maximize fills","    intents = []","    for i in range(n_intents):","        created_bar = (i % n_bars) - 1  # Distribute across bars","        intents.append(","            OrderIntent(","                order_id=i,","                created_bar=created_bar,","                role=OrderRole.ENTRY,","                kind=OrderKind.STOP,","                side=Side.BUY,","                price=105.0,  # Will trigger on any bar (high=120 > 105)","                qty=1,","            )","        )","    ","    # Should not crash or segfault","    try:","        fills = simulate_jit(bars, intents)","        # If we get here, no segfault occurred","        ","        # Fills should be bounded by n_intents (each intent can produce at most 1 fill)","        assert len(fills) <= n_intents, f\"fills ({len(fills)}) should not exceed n_intents ({n_intents})\"","        ","        # In this scenario, we expect many fills (most intents should trigger)","        # But exact count depends on bar distribution, so we just check it's reasonable","        assert len(fills) > 0, \"Should have at least some fills\"","        ","    except RuntimeError as e:","        # If buffer is full, error message should be graceful (not segfault)","        error_msg = str(e)","        assert \"buffer full\" in error_msg.lower() or \"buffer_full\" in error_msg.lower(), (","            f\"Expected buffer full error, got: {error_msg}\"","        )","        # This is acceptable - buffer protection worked correctly","","","def test_fill_buffer_protection_prevents_segfault():","    \"\"\"","    Test that buffer protection prevents segfault even with extreme intents.","    ","    This test ensures STATUS_BUFFER_FULL is returned gracefully instead of segfaulting.","    \"\"\"","    import engine.engine_jit as ej","    ","    # Skip if JIT is disabled (buffer protection is in JIT kernel)","    if ej.nb is None or os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() == \"1\":","        pytest.skip(\"numba not available or disabled; buffer protection tested only under JIT\")","    ","    n_bars = 5","    n_intents = 1000  # Extreme: way more intents than bars","    ","    bars = normalize_bars(","        np.array([100.0] * n_bars, dtype=np.float64),","        np.array([120.0] * n_bars, dtype=np.float64),","        np.array([80.0] * n_bars, dtype=np.float64),","        np.array([110.0] * n_bars, dtype=np.float64),","    )","    ","    # Create intents that will all try to fill","    intents = []","    for i in range(n_intents):","        # All activate on bar 0 (created_bar=-1)","        intents.append(","            OrderIntent(","                order_id=i,","                created_bar=-1,","                role=OrderRole.ENTRY,","                kind=OrderKind.STOP,","                side=Side.BUY,","                price=105.0,  # Will trigger","                qty=1,","            )","        )","    ","    # Should not segfault - either succeed or return graceful error","    try:","        fills = simulate_jit(bars, intents)","        # If successful, fills should be bounded","        assert len(fills) <= n_intents","        # With this many intents on one bar, we might hit buffer limit","        # But should not crash","    except RuntimeError as e:","        # Graceful error is acceptable","        assert \"buffer\" in str(e).lower() or \"full\" in str(e).lower(), (","            f\"Expected buffer-related error, got: {e}\"","        )","","","def test_fill_buffer_minimum_size():","    \"\"\"","    Test that buffer is at least n_bars*2 (default heuristic).","    ","    Even with few intents, buffer should accommodate reasonable fill rate.","    \"\"\"","    n_bars = 20","    n_intents = 5  # Few intents","    ","    bars = normalize_bars(","        np.array([100.0] * n_bars, dtype=np.float64),","        np.array([120.0] * n_bars, dtype=np.float64),","        np.array([80.0] * n_bars, dtype=np.float64),","        np.array([110.0] * n_bars, dtype=np.float64),","    )","    ","    intents = [","        OrderIntent(i, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 105.0, 1)","        for i in range(n_intents)","    ]","    ","    # Should work fine (buffer should be at least n_bars*2 = 40, which is > n_intents=5)","    fills = simulate_jit(bars, intents)","    assert len(fills) <= n_intents","    # Should not crash","",""]}
{"type":"file_footer","path":"tests/test_engine_jit_fill_buffer_capacity.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_entry_only_regression.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6994,"sha256":"7dc3a38799914c66767d5d03cf2e5f48cda8c020f1a753c5ee6c5fb72a067afb","total_lines":169,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_entry_only_regression.py","chunk_index":0,"line_start":1,"line_end":169,"content":["","\"\"\"","Regression test for entry-only fills scenario.","","This test ensures that when entry fills occur but exit fills do not,","the metrics behavior is correct:","- trades=0 is valid (no completed round-trips)","- metrics may be all-zero or have non-zero values depending on implementation","- The system should not crash or produce invalid metrics","\"\"\"","from __future__ import annotations","","import numpy as np","import os","","from pipeline.runner_grid import run_grid","","","def test_entry_only_fills_metrics_behavior() -> None:","    \"\"\"","    Test metrics behavior when only entry fills occur (no exit fills).","    ","    Scenario:","    - Entry stop triggers at t=31 (high[31] crosses buy stop=high[30]=120)","    - Exit stop never triggers (all subsequent lows stay above exit stop)","    - Result: entry_fills_total > 0, exit_fills_total == 0, trades == 0","    \"\"\"","    # Ensure clean environment","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    old_param_subsample_rate = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","    old_param_subsample_seed = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","    ","    try:","        # Set required environment variables","        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"1.0\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"","        ","        n = 60","        ","        # Construct OHLC as specified","        # Initial: all flat at 100.0","        close = np.full(n, 100.0, dtype=np.float64)","        open_ = close.copy()","        high = np.full(n, 100.5, dtype=np.float64)","        low = np.full(n, 99.5, dtype=np.float64)","        ","        # At t=30: set high[30]=120.0 (forms Donchian high point)","        high[30] = 120.0","        ","        # At t=31: set high[31]=121.0 and low[31]=110.0","        # This ensures next-bar buy stop=high[30]=120 will be triggered","        high[31] = 121.0","        low[31] = 110.0","        ","        # t>=32: set low[t]=110.1, high[t]=111.0, close[t]=110.5","        # This ensures exit stop will never trigger (low stays above exit stop)","        for t in range(32, n):","            low[t] = 110.1  # Slightly above 110.0 to avoid triggering exit stop","            high[t] = 111.0","            close[t] = 110.5","            open_[t] = 110.5","        ","        # Ensure OHLC consistency","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Single param: channel_len=20, atr_len=10, stop_mult=1.0","        params_matrix = np.array([[20, 10, 1.0]], dtype=np.float64)","        ","        result = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","            force_close_last=False,  # Critical: do not force close","        )","        ","        # Verify metrics shape","        metrics = result.get(\"metrics\")","        assert metrics is not None, \"metrics must exist\"","        assert isinstance(metrics, np.ndarray), \"metrics must be np.ndarray\"","        assert metrics.shape == (1, 3), (","            f\"metrics shape should be (1, 3), got {metrics.shape}\"","        )","        ","        # Verify perf dict","        perf = result.get(\"perf\", {})","        assert isinstance(perf, dict), \"perf must be a dict\"","        ","        # Extract perf fields for entry-only invariants","        fills_total = int(perf.get(\"fills_total\", 0))","        entry_fills_total = int(perf.get(\"entry_fills_total\", 0))","        exit_fills_total = int(perf.get(\"exit_fills_total\", 0))","        entry_intents_total = int(perf.get(\"entry_intents_total\", 0))","        exit_intents_total = int(perf.get(\"exit_intents_total\", 0))","        ","        # Assertions: lock semantics, not performance","        assert fills_total >= 1, (","            f\"fills_total ({fills_total}) should be >= 1 (entry fill should occur)\"","        )","        ","        assert entry_fills_total >= 1, (","            f\"entry_fills_total ({entry_fills_total}) should be >= 1\"","        )","        ","        assert exit_fills_total == 0, (","            f\"exit_fills_total ({exit_fills_total}) should be 0 (exit stop should never trigger)\"","        )","        ","        # If exit intents exist, fine; but they must not fill.","        assert exit_intents_total >= 0, (","            f\"exit_intents_total ({exit_intents_total}) should be >= 0\"","        )","        ","        assert entry_intents_total >= 1, (","            f\"entry_intents_total ({entry_intents_total}) should be >= 1\"","        )","        ","        # Entry-only scenario: no exit fills => no completed trades.","        # Our metrics are trade-based, so metrics may legitimately remain all zeros.","        assert np.all(np.isfinite(metrics[0])), f\"metrics[0] must be finite, got {metrics[0]}\"","        ","        # Verify trades and net_profit from result or perf (compatible with different return locations)","        trades = int(result.get(\"trades\", perf.get(\"trades\", 0)) or 0)","        net_profit = float(result.get(\"net_profit\", perf.get(\"net_profit\", 0.0)) or 0.0)","        ","        assert trades == 0, f\"entry-only must have trades==0, got {trades}\"","        assert abs(net_profit) <= 1e-12, f\"entry-only must have net_profit==0, got {net_profit}\"","        ","        # Verify metrics values match","        assert int(metrics[0, 1]) == 0, f\"metrics[0, 1] (trades) must be 0, got {metrics[0, 1]}\"","        assert abs(float(metrics[0, 0])) <= 1e-12, f\"metrics[0, 0] (net_profit) must be 0, got {metrics[0, 0]}\"","        assert abs(float(metrics[0, 2])) <= 1e-12, f\"metrics[0, 2] (max_dd) must be 0, got {metrics[0, 2]}\"","        ","        # Evidence-chain sanity (optional but recommended)","        if \"metrics_subset_abs_sum\" in perf:","            assert float(perf[\"metrics_subset_abs_sum\"]) >= 0.0","        if \"metrics_subset_nonzero_rows\" in perf:","            assert int(perf[\"metrics_subset_nonzero_rows\"]) == 0","        ","        # Optional: Check if position tracking exists (entry-only should end in open position)","        pos_last = perf.get(\"position_last\", perf.get(\"pos_last\", perf.get(\"last_position\", None)))","        if pos_last is not None:","            assert int(pos_last) != 0, f\"entry-only should end in open position, got {pos_last}\"","        ","    finally:","        # Restore environment","        if old_trigger_rate is None:","            os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","        ","        if old_param_subsample_rate is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = old_param_subsample_rate","        ","        if old_param_subsample_seed is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = old_param_subsample_seed","",""]}
{"type":"file_footer","path":"tests/test_entry_only_regression.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_funnel_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12245,"sha256":"2bff036722cb5eed96b297d1ab3ea16f1afc848f20741e230d4880a97e9718cb","total_lines":340,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_funnel_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for funnel pipeline.","","Tests verify:","1. Funnel plan has three stages","2. Stage2 subsample is 1.0","3. Each stage creates artifacts","4. param_subsample_rate visibility","5. params_effective calculation consistency","6. Funnel result index structure","\"\"\"","","from __future__ import annotations","","import tempfile","from pathlib import Path","","import numpy as np","import pytest","","from core.audit_schema import compute_params_effective","from pipeline.funnel_plan import build_default_funnel_plan","from pipeline.funnel_runner import run_funnel","from pipeline.funnel_schema import StageName","","","def test_funnel_build_default_plan_has_three_stages():","    \"\"\"Test that default funnel plan has exactly three stages.\"\"\"","    cfg = {","        \"param_subsample_rate\": 0.1,","        \"topk_stage0\": 50,","        \"topk_stage1\": 20,","    }","    ","    plan = build_default_funnel_plan(cfg)","    ","    assert len(plan.stages) == 3","    ","    # Verify stage names","    assert plan.stages[0].name == StageName.STAGE0_COARSE","    assert plan.stages[1].name == StageName.STAGE1_TOPK","    assert plan.stages[2].name == StageName.STAGE2_CONFIRM","","","def test_stage2_subsample_is_one():","    \"\"\"Test that Stage2 subsample rate is always 1.0.\"\"\"","    test_cases = [","        {\"param_subsample_rate\": 0.1},","        {\"param_subsample_rate\": 0.5},","        {\"param_subsample_rate\": 0.9},","    ]","    ","    for cfg in test_cases:","        plan = build_default_funnel_plan(cfg)","        stage2 = plan.stages[2]","        ","        assert stage2.name == StageName.STAGE2_CONFIRM","        assert stage2.param_subsample_rate == 1.0, (","            f\"Stage2 subsample must be 1.0, got {stage2.param_subsample_rate}\"","        )","","","def test_subsample_rate_progression():","    \"\"\"Test that subsample rates progress correctly.\"\"\"","    cfg = {\"param_subsample_rate\": 0.1}","    plan = build_default_funnel_plan(cfg)","    ","    s0_rate = plan.stages[0].param_subsample_rate","    s1_rate = plan.stages[1].param_subsample_rate","    s2_rate = plan.stages[2].param_subsample_rate","    ","    # Stage0: config rate","    assert s0_rate == 0.1","    ","    # Stage1: min(1.0, s0 * 2)","    assert s1_rate == min(1.0, 0.1 * 2.0) == 0.2","    ","    # Stage2: must be 1.0","    assert s2_rate == 1.0","    ","    # Verify progression: s0 <= s1 <= s2","    assert s0_rate <= s1_rate <= s2_rate","","","def test_each_stage_creates_run_dir_with_artifacts():","    \"\"\"Test that each stage creates run directory with required artifacts.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        outputs_root = Path(tmpdir) / \"outputs\"","        ","        # Create minimal config","        cfg = {","            \"season\": \"test_season\",","            \"dataset_id\": \"test_dataset\",","            \"bars\": 1000,","            \"params_total\": 100,","            \"param_subsample_rate\": 0.1,","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"commission\": 0.0,","            \"slip\": 0.0,","            \"order_qty\": 1,","        }","        ","        # Run funnel","        result_index = run_funnel(cfg, outputs_root)","        ","        # Verify all stages have run directories","        assert len(result_index.stages) == 3","        ","        artifacts = [","            \"manifest.json\",","            \"config_snapshot.json\",","            \"metrics.json\",","            \"winners.json\",","            \"README.md\",","            \"logs.txt\",","        ]","        ","        for stage_idx in result_index.stages:","            run_dir = outputs_root / stage_idx.run_dir","            ","            # Verify directory exists","            assert run_dir.exists(), f\"Run directory missing for {stage_idx.stage.value}\"","            assert run_dir.is_dir()","            ","            # Verify all artifacts exist","            for artifact_name in artifacts:","                artifact_path = run_dir / artifact_name","                assert artifact_path.exists(), (","                    f\"Missing artifact {artifact_name} for {stage_idx.stage.value}\"","                )","","","def test_param_subsample_rate_visible_in_artifacts():","    \"\"\"Test that param_subsample_rate is visible in manifest/metrics/README.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        outputs_root = Path(tmpdir) / \"outputs\"","        ","        cfg = {","            \"season\": \"test_season\",","            \"dataset_id\": \"test_dataset\",","            \"bars\": 1000,","            \"params_total\": 100,","            \"param_subsample_rate\": 0.25,","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"commission\": 0.0,","            \"slip\": 0.0,","            \"order_qty\": 1,","        }","        ","        result_index = run_funnel(cfg, outputs_root)","        ","        import json","        ","        for stage_idx in result_index.stages:","            run_dir = outputs_root / stage_idx.run_dir","            ","            # Check manifest.json","            manifest_path = run_dir / \"manifest.json\"","            with open(manifest_path, \"r\", encoding=\"utf-8\") as f:","                manifest = json.load(f)","            assert \"param_subsample_rate\" in manifest","            ","            # Check metrics.json","            metrics_path = run_dir / \"metrics.json\"","            with open(metrics_path, \"r\", encoding=\"utf-8\") as f:","                metrics = json.load(f)","            assert \"param_subsample_rate\" in metrics","            ","            # Check README.md","            readme_path = run_dir / \"README.md\"","            with open(readme_path, \"r\", encoding=\"utf-8\") as f:","                readme_content = f.read()","            assert \"param_subsample_rate\" in readme_content","","","def test_params_effective_floor_rule_consistent():","    \"\"\"Test that params_effective uses consistent floor rule across stages.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        outputs_root = Path(tmpdir) / \"outputs\"","        ","        params_total = 1000","        param_subsample_rate = 0.33","        ","        cfg = {","            \"season\": \"test_season\",","            \"dataset_id\": \"test_dataset\",","            \"bars\": 1000,","            \"params_total\": params_total,","            \"param_subsample_rate\": param_subsample_rate,","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),"]}
{"type":"file_chunk","path":"tests/test_funnel_contract.py","chunk_index":1,"line_start":201,"line_end":340,"content":["            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(params_total, 3).astype(np.float64),","            \"commission\": 0.0,","            \"slip\": 0.0,","            \"order_qty\": 1,","        }","        ","        result_index = run_funnel(cfg, outputs_root)","        ","        import json","        ","        plan = result_index.plan","        for i, (spec, stage_idx) in enumerate(zip(plan.stages, result_index.stages)):","            run_dir = outputs_root / stage_idx.run_dir","            ","            # Read manifest","            manifest_path = run_dir / \"manifest.json\"","            with open(manifest_path, \"r\", encoding=\"utf-8\") as f:","                manifest = json.load(f)","            ","            # Verify params_effective matches computed value","            expected_effective = compute_params_effective(","                params_total, spec.param_subsample_rate","            )","            assert manifest[\"params_effective\"] == expected_effective, (","                f\"Stage {i} params_effective mismatch: \"","                f\"expected={expected_effective}, got={manifest['params_effective']}\"","            )","","","def test_funnel_result_index_contains_all_stages():","    \"\"\"Test that funnel result index contains all stages.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        outputs_root = Path(tmpdir) / \"outputs\"","        ","        cfg = {","            \"season\": \"test_season\",","            \"dataset_id\": \"test_dataset\",","            \"bars\": 1000,","            \"params_total\": 100,","            \"param_subsample_rate\": 0.1,","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"commission\": 0.0,","            \"slip\": 0.0,","            \"order_qty\": 1,","        }","        ","        result_index = run_funnel(cfg, outputs_root)","        ","        # Verify index structure","        assert result_index.plan is not None","        assert len(result_index.stages) == 3","        ","        # Verify stage order matches plan","        for spec, stage_idx in zip(result_index.plan.stages, result_index.stages):","            assert spec.name == stage_idx.stage","            assert stage_idx.run_id is not None","            assert stage_idx.run_dir is not None","","","def test_config_snapshot_is_json_serializable_and_small():","    \"\"\"Test that config_snapshot.json excludes ndarrays and is JSON-serializable.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        outputs_root = Path(tmpdir) / \"outputs\"","        ","        cfg = {","            \"season\": \"test_season\",","            \"dataset_id\": \"test_dataset\",","            \"bars\": 1000,","            \"params_total\": 100,","            \"param_subsample_rate\": 0.1,","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"commission\": 0.0,","            \"slip\": 0.0,","            \"order_qty\": 1,","        }","        ","        result_index = run_funnel(cfg, outputs_root)","        ","        import json","        ","        # Keys that should NOT exist in snapshot (raw ndarrays)","        forbidden_keys = {\"open_\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"params_matrix\"}","        ","        # Required keys that MUST exist","        required_keys = {","            \"season\",","            \"dataset_id\",","            \"bars\",","            \"params_total\",","            \"param_subsample_rate\",","            \"stage_name\",","        }","        ","        for stage_idx in result_index.stages:","            run_dir = outputs_root / stage_idx.run_dir","            config_snapshot_path = run_dir / \"config_snapshot.json\"","            ","            assert config_snapshot_path.exists()","            ","            # Verify JSON is valid and loadable","            with open(config_snapshot_path, \"r\", encoding=\"utf-8\") as f:","                snapshot_content = f.read()","                snapshot_data = json.loads(snapshot_content)  # Should not crash","            ","            # Verify no raw ndarray keys exist","            for forbidden_key in forbidden_keys:","                assert forbidden_key not in snapshot_data, (","                    f\"config_snapshot.json should not contain '{forbidden_key}' \"","                    f\"(raw ndarray) for {stage_idx.stage.value}\"","                )","            ","            # Verify required keys exist","            for required_key in required_keys:","                assert required_key in snapshot_data, (","                    f\"config_snapshot.json missing required key '{required_key}' \"","                    f\"for {stage_idx.stage.value}\"","                )","            ","            # Verify param_subsample_rate is present and correct","            assert \"param_subsample_rate\" in snapshot_data","            assert isinstance(snapshot_data[\"param_subsample_rate\"], (int, float))","            ","            # Verify stage_name is present","            assert \"stage_name\" in snapshot_data","            assert isinstance(snapshot_data[\"stage_name\"], str)","            ","            # Optional: verify metadata keys exist if needed","            # (e.g., \"open__meta\", \"params_matrix_meta\")","            # This is optional - metadata may or may not be included","",""]}
{"type":"file_footer","path":"tests/test_funnel_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_funnel_oom_integration.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":11140,"sha256":"c84fc286dada1358a5f153f8b16fa4cdf948aaaadcc6e3737e4c61d337852f45","total_lines":274,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_funnel_oom_integration.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Integration tests for OOM gate in funnel pipeline.","","Tests verify:","1. Funnel metrics include OOM gate fields","2. Auto-downsample updates snapshot and hash consistently","\"\"\"","","from __future__ import annotations","","import json","import tempfile","from pathlib import Path","","import numpy as np","import pytest","","from pipeline.funnel_runner import run_funnel","","","def test_funnel_metrics_include_oom_gate_fields():","    \"\"\"Test that funnel metrics include OOM gate fields.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        outputs_root = Path(tmpdir) / \"outputs\"","        ","        cfg = {","            \"season\": \"test_season\",","            \"dataset_id\": \"test_dataset\",","            \"bars\": 1000,","            \"params_total\": 100,","            \"param_subsample_rate\": 0.1,","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"commission\": 0.0,","            \"slip\": 0.0,","            \"order_qty\": 1,","            \"mem_limit_mb\": 10000.0,  # High limit to ensure PASS","        }","        ","        result_index = run_funnel(cfg, outputs_root)","        ","        # Verify all stages have OOM gate fields in metrics","        for stage_idx in result_index.stages:","            run_dir = outputs_root / stage_idx.run_dir","            metrics_path = run_dir / \"metrics.json\"","            ","            assert metrics_path.exists()","            ","            with open(metrics_path, \"r\", encoding=\"utf-8\") as f:","                metrics = json.load(f)","            ","            # Verify required OOM gate fields","            assert \"oom_gate_action\" in metrics","            assert \"oom_gate_reason\" in metrics","            assert \"mem_est_mb\" in metrics","            assert \"mem_limit_mb\" in metrics","            assert \"ops_est\" in metrics","            assert \"stage_planned_subsample\" in metrics","            ","            # Verify action is valid","            assert metrics[\"oom_gate_action\"] in (\"PASS\", \"BLOCK\", \"AUTO_DOWNSAMPLE\")","            ","            # Verify stage_planned_subsample matches expected planned for this stage","            stage_name = metrics.get(\"stage_name\")","            s0_base = cfg.get(\"param_subsample_rate\", 0.1)","            expected_planned = planned_subsample_for_stage(stage_name, s0_base)","            assert metrics[\"stage_planned_subsample\"] == expected_planned, (","                f\"stage_planned_subsample mismatch for {stage_name}: \"","                f\"expected={expected_planned}, got={metrics['stage_planned_subsample']}\"","            )","","","def planned_subsample_for_stage(stage_name: str, s0: float) -> float:","    \"\"\"","    Get planned subsample rate for a stage based on funnel plan rules.","    ","    Args:","        stage_name: Stage identifier","        s0: Stage0 base subsample rate (from config)","        ","    Returns:","        Planned subsample rate for the stage","    \"\"\"","    if stage_name == \"stage0_coarse\":","        return s0","    if stage_name == \"stage1_topk\":","        return min(1.0, s0 * 2.0)","    if stage_name == \"stage2_confirm\":","        return 1.0","    raise AssertionError(f\"Unknown stage_name: {stage_name}\")","","","def test_auto_downsample_updates_snapshot_and_hash(monkeypatch):","    \"\"\"Test that auto-downsample updates snapshot and hash consistently.\"\"\"","    # Monkeypatch estimate_memory_bytes to trigger auto-downsample","    def mock_estimate_memory_bytes(cfg, work_factor=2.0):","        \"\"\"Mock that makes memory estimate sensitive to subsample.\"\"\"","        bars = int(cfg.get(\"bars\", 0))","        params_total = int(cfg.get(\"params_total\", 0))","        subsample_rate = float(cfg.get(\"param_subsample_rate\", 1.0))","        params_effective = int(params_total * subsample_rate)","        ","        base_mem = bars * 8 * 4  # 4 price arrays","        params_mem = params_effective * 3 * 8  # params_matrix","        total_mem = (base_mem + params_mem) * work_factor","        return int(total_mem)","    ","    monkeypatch.setattr(","        \"core.oom_cost_model.estimate_memory_bytes\",","        mock_estimate_memory_bytes,","    )","    ","    with tempfile.TemporaryDirectory() as tmpdir:","        outputs_root = Path(tmpdir) / \"outputs\"","        ","        # Stage0 base subsample rate (from config)","        s0_base = 0.5","        ","        cfg = {","            \"season\": \"test_season\",","            \"dataset_id\": \"test_dataset\",","            \"bars\": 10000,","            \"params_total\": 1000,","            \"param_subsample_rate\": s0_base,  # Stage0 base rate","            \"open_\": np.random.randn(10000).astype(np.float64),","            \"high\": np.random.randn(10000).astype(np.float64),","            \"low\": np.random.randn(10000).astype(np.float64),","            \"close\": np.random.randn(10000).astype(np.float64),","            \"params_matrix\": np.random.randn(1000, 3).astype(np.float64),","            \"commission\": 0.0,","            \"slip\": 0.0,","            \"order_qty\": 1,","            # Dynamic limit calculation","            \"mem_limit_mb\": 0.65,  # Will trigger auto-downsample for some stages","            \"allow_auto_downsample\": True,","        }","        ","        result_index = run_funnel(cfg, outputs_root)","        ","        # Check each stage","        for stage_idx in result_index.stages:","            run_dir = outputs_root / stage_idx.run_dir","            ","            # Read manifest","            manifest_path = run_dir / \"manifest.json\"","            with open(manifest_path, \"r\", encoding=\"utf-8\") as f:","                manifest = json.load(f)","            ","            # Read config_snapshot","            config_snapshot_path = run_dir / \"config_snapshot.json\"","            with open(config_snapshot_path, \"r\", encoding=\"utf-8\") as f:","                config_snapshot = json.load(f)","            ","            # Read metrics","            metrics_path = run_dir / \"metrics.json\"","            with open(metrics_path, \"r\", encoding=\"utf-8\") as f:","                metrics = json.load(f)","            ","            # Get stage name and planned subsample","            stage_name = metrics.get(\"stage_name\")","            expected_planned = planned_subsample_for_stage(stage_name, s0_base)","            ","            # Verify consistency: if auto-downsample occurred, all must match","            if metrics.get(\"oom_gate_action\") == \"AUTO_DOWNSAMPLE\":","                final_subsample = metrics.get(\"oom_gate_final_subsample\")","                ","                # Manifest must have final subsample","                assert manifest[\"param_subsample_rate\"] == final_subsample, (","                    f\"Manifest subsample mismatch: \"","                    f\"expected={final_subsample}, got={manifest['param_subsample_rate']}\"","                )","                ","                # Config snapshot must have final subsample","                assert config_snapshot[\"param_subsample_rate\"] == final_subsample, (","                    f\"Config snapshot subsample mismatch: \"","                    f\"expected={final_subsample}, got={config_snapshot['param_subsample_rate']}\"","                )","                ","                # Metrics must have final subsample","                assert metrics[\"param_subsample_rate\"] == final_subsample, (","                    f\"Metrics subsample mismatch: \"","                    f\"expected={final_subsample}, got={metrics['param_subsample_rate']}\"","                )","                ","                # Verify original subsample matches planned subsample for this stage","                assert \"oom_gate_original_subsample\" in metrics","                assert metrics[\"oom_gate_original_subsample\"] == expected_planned, (","                    f\"oom_gate_original_subsample mismatch for {stage_name}: \"","                    f\"expected={expected_planned} (planned), \"","                    f\"got={metrics['oom_gate_original_subsample']}\"","                )","                ","                # Verify stage_planned_subsample equals oom_gate_original_subsample","                assert \"stage_planned_subsample\" in metrics","                assert metrics[\"stage_planned_subsample\"] == metrics[\"oom_gate_original_subsample\"], (","                    f\"stage_planned_subsample should equal oom_gate_original_subsample for {stage_name}: \"","                    f\"stage_planned={metrics['stage_planned_subsample']}, \""]}
{"type":"file_chunk","path":"tests/test_funnel_oom_integration.py","chunk_index":1,"line_start":201,"line_end":274,"content":["                    f\"original={metrics['oom_gate_original_subsample']}\"","                )","","","def test_oom_gate_fields_in_readme():","    \"\"\"Test that OOM gate fields are included in README.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        outputs_root = Path(tmpdir) / \"outputs\"","        ","        cfg = {","            \"season\": \"test_season\",","            \"dataset_id\": \"test_dataset\",","            \"bars\": 1000,","            \"params_total\": 100,","            \"param_subsample_rate\": 0.1,","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"commission\": 0.0,","            \"slip\": 0.0,","            \"order_qty\": 1,","            \"mem_limit_mb\": 10000.0,","        }","        ","        result_index = run_funnel(cfg, outputs_root)","        ","        # Check README for at least one stage","        for stage_idx in result_index.stages:","            run_dir = outputs_root / stage_idx.run_dir","            readme_path = run_dir / \"README.md\"","            ","            assert readme_path.exists()","            ","            with open(readme_path, \"r\", encoding=\"utf-8\") as f:","                readme_content = f.read()","            ","            # Verify OOM gate section exists","            assert \"OOM Gate\" in readme_content","            assert \"action\" in readme_content.lower()","            assert \"mem_est_mb\" in readme_content.lower()","            ","            break  # Check at least one stage","","","def test_block_action_raises_error():","    \"\"\"Test that BLOCK action raises RuntimeError.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        outputs_root = Path(tmpdir) / \"outputs\"","        ","        cfg = {","            \"season\": \"test_season\",","            \"dataset_id\": \"test_dataset\",","            \"bars\": 1000000,  # Very large","            \"params_total\": 100000,  # Very large","            \"param_subsample_rate\": 1.0,","            \"open_\": np.random.randn(1000000).astype(np.float64),","            \"high\": np.random.randn(1000000).astype(np.float64),","            \"low\": np.random.randn(1000000).astype(np.float64),","            \"close\": np.random.randn(1000000).astype(np.float64),","            \"params_matrix\": np.random.randn(100000, 3).astype(np.float64),","            \"commission\": 0.0,","            \"slip\": 0.0,","            \"order_qty\": 1,","            \"mem_limit_mb\": 1.0,  # Very low limit","            \"allow_auto_downsample\": False,  # Disable auto-downsample to force BLOCK","        }","        ","        # Should raise RuntimeError","        with pytest.raises(RuntimeError, match=\"OOM Gate BLOCKED\"):","            run_funnel(cfg, outputs_root)","",""]}
{"type":"file_footer","path":"tests/test_funnel_oom_integration.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_funnel_smoke_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5421,"sha256":"3e158da0123af49f7467c3f99d9205d892ce4ac73c18273259407e4c7118f6dc","total_lines":170,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_funnel_smoke_contract.py","chunk_index":0,"line_start":1,"line_end":170,"content":["","\"\"\"Funnel smoke contract tests - Phase 4 Stage D.","","Basic smoke tests to ensure the complete funnel pipeline works end-to-end.","\"\"\"","","import numpy as np","","from pipeline.funnel import FunnelResult, run_funnel","","","def test_funnel_smoke_basic():","    \"\"\"Basic smoke test: run funnel with small parameter grid.\"\"\"","    # Generate deterministic test data","    np.random.seed(42)","    n_bars = 500","    n_params = 20","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    open_ = close + np.random.randn(n_bars) * 2","    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3","    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3","    ","    # Generate parameter grid","    params_matrix = np.column_stack([","        np.random.randint(10, 50, size=n_params),  # channel_len / fast_len","        np.random.randint(5, 30, size=n_params),   # atr_len / slow_len","        np.random.uniform(1.0, 3.0, size=n_params), # stop_mult","    ]).astype(np.float64)","    ","    # Run funnel","    result = run_funnel(","        open_,","        high,","        low,","        close,","        params_matrix,","        k=5,","        commission=0.0,","        slip=0.0,","    )","    ","    # Verify result structure","    assert isinstance(result, FunnelResult)","    assert len(result.stage0_results) == n_params","    assert len(result.topk_param_ids) == 5","    assert len(result.stage2_results) == 5","    ","    # Verify Stage0 results","    for stage0_result in result.stage0_results:","        assert hasattr(stage0_result, \"param_id\")","        assert hasattr(stage0_result, \"proxy_value\")","        assert hasattr(stage0_result, \"warmup_ok\")","        assert isinstance(stage0_result.param_id, int)","        assert isinstance(stage0_result.proxy_value, (int, float))","    ","    # Verify Top-K param_ids are valid","    for param_id in result.topk_param_ids:","        assert 0 <= param_id < n_params","    ","    # Verify Stage2 results match Top-K","    assert len(result.stage2_results) == len(result.topk_param_ids)","    for i, stage2_result in enumerate(result.stage2_results):","        assert stage2_result.param_id == result.topk_param_ids[i]","        assert isinstance(stage2_result.net_profit, (int, float))","        assert isinstance(stage2_result.trades, int)","        assert isinstance(stage2_result.max_dd, (int, float))","","","def test_funnel_smoke_empty_params():","    \"\"\"Test funnel with empty parameter grid.\"\"\"","    np.random.seed(42)","    n_bars = 100","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    open_ = close + np.random.randn(n_bars) * 2","    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3","    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3","    ","    # Empty parameter grid","    params_matrix = np.empty((0, 3), dtype=np.float64)","    ","    result = run_funnel(","        open_,","        high,","        low,","        close,","        params_matrix,","        k=5,","    )","    ","    assert len(result.stage0_results) == 0","    assert len(result.topk_param_ids) == 0","    assert len(result.stage2_results) == 0","","","def test_funnel_smoke_k_larger_than_params():","    \"\"\"Test funnel when k is larger than number of parameters.\"\"\"","    np.random.seed(42)","    n_bars = 100","    n_params = 5","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    open_ = close + np.random.randn(n_bars) * 2","    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3","    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3","    ","    params_matrix = np.column_stack([","        np.random.randint(10, 50, size=n_params),","        np.random.randint(5, 30, size=n_params),","        np.random.uniform(1.0, 3.0, size=n_params),","    ]).astype(np.float64)","    ","    # k=10 but only 5 params","    result = run_funnel(","        open_,","        high,","        low,","        close,","        params_matrix,","        k=10,","    )","    ","    # Should return all 5 params","    assert len(result.topk_param_ids) == 5","    assert len(result.stage2_results) == 5","","","def test_funnel_smoke_pipeline_order():","    \"\"\"Test that pipeline executes in correct order: Stage0 → Top-K → Stage2.\"\"\"","    np.random.seed(42)","    n_bars = 200","    n_params = 10","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    open_ = close + np.random.randn(n_bars) * 2","    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3","    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3","    ","    params_matrix = np.column_stack([","        np.random.randint(10, 30, size=n_params),","        np.random.randint(5, 20, size=n_params),","        np.random.uniform(1.0, 2.0, size=n_params),","    ]).astype(np.float64)","    ","    result = run_funnel(","        open_,","        high,","        low,","        close,","        params_matrix,","        k=3,","    )","    ","    # Verify Stage0 ran on all params","    assert len(result.stage0_results) == n_params","    ","    # Verify Top-K selected from Stage0 results","    assert len(result.topk_param_ids) == 3","    # Top-K should be sorted by proxy_value (descending)","    stage0_by_id = {r.param_id: r for r in result.stage0_results}","    topk_values = [stage0_by_id[pid].proxy_value for pid in result.topk_param_ids]","    assert topk_values == sorted(topk_values, reverse=True)","    ","    # Verify Stage2 ran only on Top-K","    assert len(result.stage2_results) == 3","    stage2_param_ids = [r.param_id for r in result.stage2_results]","    assert set(stage2_param_ids) == set(result.topk_param_ids)","",""]}
{"type":"file_footer","path":"tests/test_funnel_smoke_contract.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_funnel_topk_determinism.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4438,"sha256":"4c582b61e8bfa35d342bb8ee961f8077492337407001cd9fb33b73608f5901e0","total_lines":133,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_funnel_topk_determinism.py","chunk_index":0,"line_start":1,"line_end":133,"content":["","\"\"\"Test Top-K determinism - same input must produce same Top-K selection.\"\"\"","","import numpy as np","","from pipeline.funnel import run_funnel","from pipeline.stage0_runner import Stage0Result, run_stage0","from pipeline.topk import select_topk","","","def test_topk_determinism_same_input():","    \"\"\"Test that Top-K selection is deterministic: same input produces same output.\"\"\"","    # Generate deterministic test data","    np.random.seed(42)","    n_bars = 1000","    n_params = 100","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    open_ = close + np.random.randn(n_bars) * 2","    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3","    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3","    ","    # Generate parameter grid","    params_matrix = np.column_stack([","        np.random.randint(10, 100, size=n_params),  # fast_len / channel_len","        np.random.randint(5, 50, size=n_params),      # slow_len / atr_len","        np.random.uniform(1.0, 5.0, size=n_params),   # stop_mult","    ]).astype(np.float64)","    ","    # Run Stage0 twice with same input","    stage0_results_1 = run_stage0(close, params_matrix)","    stage0_results_2 = run_stage0(close, params_matrix)","    ","    # Verify Stage0 results are identical","    assert len(stage0_results_1) == len(stage0_results_2)","    for r1, r2 in zip(stage0_results_1, stage0_results_2):","        assert r1.param_id == r2.param_id","        assert r1.proxy_value == r2.proxy_value","    ","    # Run Top-K selection twice","    k = 20","    topk_1 = select_topk(stage0_results_1, k=k)","    topk_2 = select_topk(stage0_results_2, k=k)","    ","    # Verify Top-K selection is identical","    assert topk_1 == topk_2, (","        f\"Top-K selection not deterministic:\\n\"","        f\"  First run:  {topk_1}\\n\"","        f\"  Second run: {topk_2}\"","    )","    assert len(topk_1) == k","    assert len(topk_2) == k","","","def test_topk_determinism_tie_break():","    \"\"\"Test that tie-breaking by param_id is deterministic.\"\"\"","    # Create Stage0 results with identical proxy_value","    # Tie-break should use param_id (ascending)","    results = [","        Stage0Result(param_id=5, proxy_value=10.0),","        Stage0Result(param_id=2, proxy_value=10.0),  # Same value, lower param_id","        Stage0Result(param_id=8, proxy_value=10.0),","        Stage0Result(param_id=1, proxy_value=10.0),  # Same value, lowest param_id","        Stage0Result(param_id=3, proxy_value=15.0),  # Higher value","        Stage0Result(param_id=4, proxy_value=12.0),  # Medium value","    ]","    ","    # Select top 3","    topk = select_topk(results, k=3)","    ","    # Expected: param_id=3 (value=15.0), param_id=4 (value=12.0), param_id=1 (value=10.0, lowest param_id)","    assert topk == [3, 4, 1], f\"Tie-break failed: got {topk}, expected [3, 4, 1]\"","    ","    # Run again - should be identical","    topk_2 = select_topk(results, k=3)","    assert topk_2 == topk","","","def test_funnel_determinism():","    \"\"\"Test that complete funnel pipeline is deterministic.\"\"\"","    # Generate deterministic test data","    np.random.seed(123)","    n_bars = 500","    n_params = 50","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    open_ = close + np.random.randn(n_bars) * 2","    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3","    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3","    ","    # Generate parameter grid","    params_matrix = np.column_stack([","        np.random.randint(10, 50, size=n_params),","        np.random.randint(5, 30, size=n_params),","        np.random.uniform(1.0, 3.0, size=n_params),","    ]).astype(np.float64)","    ","    # Run funnel twice","    result_1 = run_funnel(","        open_,","        high,","        low,","        close,","        params_matrix,","        k=10,","        commission=0.0,","        slip=0.0,","    )","    ","    result_2 = run_funnel(","        open_,","        high,","        low,","        close,","        params_matrix,","        k=10,","        commission=0.0,","        slip=0.0,","    )","    ","    # Verify Top-K selection is identical","    assert result_1.topk_param_ids == result_2.topk_param_ids, (","        f\"Funnel Top-K not deterministic:\\n\"","        f\"  First run:  {result_1.topk_param_ids}\\n\"","        f\"  Second run: {result_2.topk_param_ids}\"","    )","    ","    # Verify Stage2 results are for same parameters","    assert len(result_1.stage2_results) == len(result_2.stage2_results)","    for r1, r2 in zip(result_1.stage2_results, result_2.stage2_results):","        assert r1.param_id == r2.param_id","",""]}
{"type":"file_footer","path":"tests/test_funnel_topk_determinism.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_funnel_topk_no_human_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7737,"sha256":"31baa18709898bb24909085f2e945acb79344abbc4d5d341e56b544a12127f2f","total_lines":225,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_funnel_topk_no_human_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Funnel Top-K no-human contract tests - Phase 4 Stage D.","","These tests ensure that Top-K selection is purely automatic based on proxy_value,","with no possibility of human intervention or manual filtering.","\"\"\"","","import numpy as np","","from pipeline.funnel import run_funnel","from pipeline.stage0_runner import Stage0Result, run_stage0","from pipeline.topk import select_topk","","","def test_topk_only_uses_proxy_value():","    \"\"\"Test that Top-K selection uses ONLY proxy_value, not any other field.\"\"\"","    # Create Stage0 results with varying proxy_value and other fields","    results = [","        Stage0Result(param_id=0, proxy_value=5.0, warmup_ok=True, meta={\"custom\": \"data\"}),","        Stage0Result(param_id=1, proxy_value=10.0, warmup_ok=False, meta=None),","        Stage0Result(param_id=2, proxy_value=15.0, warmup_ok=True, meta={\"other\": 123}),","        Stage0Result(param_id=3, proxy_value=8.0, warmup_ok=True, meta=None),","        Stage0Result(param_id=4, proxy_value=12.0, warmup_ok=False, meta={\"test\": True}),","    ]","    ","    # Select top 3","    topk = select_topk(results, k=3)","    ","    # Expected: param_id=2 (value=15.0), param_id=4 (value=12.0), param_id=1 (value=10.0)","    # Should ignore warmup_ok and meta fields","    assert topk == [2, 4, 1], (","        f\"Top-K should only consider proxy_value, got {topk}, expected [2, 4, 1]\"","    )","","","def test_topk_tie_break_param_id():","    \"\"\"Test that tie-breaking uses param_id (ascending) when proxy_value is identical.\"\"\"","    # Create results with identical proxy_value","    results = [","        Stage0Result(param_id=5, proxy_value=10.0),","        Stage0Result(param_id=2, proxy_value=10.0),","        Stage0Result(param_id=8, proxy_value=10.0),","        Stage0Result(param_id=1, proxy_value=10.0),","        Stage0Result(param_id=3, proxy_value=15.0),  # Higher value","        Stage0Result(param_id=4, proxy_value=12.0),   # Medium value","    ]","    ","    # Select top 3","    topk = select_topk(results, k=3)","    ","    # Expected: param_id=3 (value=15.0), param_id=4 (value=12.0), param_id=1 (value=10.0, lowest param_id)","    assert topk == [3, 4, 1], (","        f\"Tie-break should use param_id ascending, got {topk}, expected [3, 4, 1]\"","    )","","","def test_topk_deterministic_same_input():","    \"\"\"Test that Top-K selection is deterministic: same input produces same output.\"\"\"","    np.random.seed(42)","    n_bars = 500","    n_params = 50","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    ","    params_matrix = np.column_stack([","        np.random.randint(10, 50, size=n_params),","        np.random.randint(5, 30, size=n_params),","        np.random.uniform(1.0, 3.0, size=n_params),","    ]).astype(np.float64)","    ","    # Run Stage0 twice","    stage0_results_1 = run_stage0(close, params_matrix)","    stage0_results_2 = run_stage0(close, params_matrix)","    ","    # Select Top-K twice","    topk_1 = select_topk(stage0_results_1, k=10)","    topk_2 = select_topk(stage0_results_2, k=10)","    ","    # Should be identical","    assert topk_1 == topk_2, (","        f\"Top-K selection not deterministic:\\n\"","        f\"  First run:  {topk_1}\\n\"","        f\"  Second run: {topk_2}\"","    )","","","def test_funnel_topk_no_manual_filtering():","    \"\"\"Test that funnel Top-K selection cannot be manually filtered.\"\"\"","    np.random.seed(42)","    n_bars = 300","    n_params = 20","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    open_ = close + np.random.randn(n_bars) * 2","    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3","    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3","    ","    params_matrix = np.column_stack([","        np.random.randint(10, 40, size=n_params),","        np.random.randint(5, 25, size=n_params),","        np.random.uniform(1.0, 2.5, size=n_params),","    ]).astype(np.float64)","    ","    # Run funnel","    result = run_funnel(","        open_,","        high,","        low,","        close,","        params_matrix,","        k=5,","    )","    ","    # Verify Top-K is based solely on proxy_value","    stage0_by_id = {r.param_id: r for r in result.stage0_results}","    ","    # Get proxy_values for Top-K","    topk_values = [stage0_by_id[pid].proxy_value for pid in result.topk_param_ids]","    ","    # Get proxy_values for all params","    all_values = [r.proxy_value for r in result.stage0_results]","    all_values_sorted = sorted(all_values, reverse=True)","    ","    # Top-K values should match top K values from all params","    assert topk_values == all_values_sorted[:5], (","        f\"Top-K should contain top 5 proxy_values:\\n\"","        f\"  Top-K values: {topk_values}\\n\"","        f\"  Top 5 values:  {all_values_sorted[:5]}\"","    )","","","def test_funnel_stage2_only_runs_topk():","    \"\"\"Test that Stage2 only runs on Top-K parameters, not all parameters.\"\"\"","    np.random.seed(42)","    n_bars = 200","    n_params = 15","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    open_ = close + np.random.randn(n_bars) * 2","    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3","    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3","    ","    params_matrix = np.column_stack([","        np.random.randint(10, 30, size=n_params),","        np.random.randint(5, 20, size=n_params),","        np.random.uniform(1.0, 2.0, size=n_params),","    ]).astype(np.float64)","    ","    result = run_funnel(","        open_,","        high,","        low,","        close,","        params_matrix,","        k=3,","    )","    ","    # Verify Stage0 ran on all params","    assert len(result.stage0_results) == n_params","    ","    # Verify Top-K selected","    assert len(result.topk_param_ids) == 3","    ","    # Verify Stage2 ran ONLY on Top-K (not all params)","    assert len(result.stage2_results) == 3, (","        f\"Stage2 should run only on Top-K (3 params), not all params ({n_params})\"","    )","    ","    # Verify Stage2 param_ids match Top-K","    stage2_param_ids = set(r.param_id for r in result.stage2_results)","    topk_param_ids_set = set(result.topk_param_ids)","    assert stage2_param_ids == topk_param_ids_set, (","        f\"Stage2 param_ids should match Top-K:\\n\"","        f\"  Stage2: {stage2_param_ids}\\n\"","        f\"  Top-K:  {topk_param_ids_set}\"","    )","","","def test_funnel_stage0_no_pnl_fields():","    \"\"\"Test that Stage0 results contain NO PnL-related fields.\"\"\"","    np.random.seed(42)","    n_bars = 200","    n_params = 10","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    open_ = close + np.random.randn(n_bars) * 2","    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3","    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3","    ","    params_matrix = np.column_stack([","        np.random.randint(10, 30, size=n_params),","        np.random.randint(5, 20, size=n_params),","        np.random.uniform(1.0, 2.0, size=n_params),","    ]).astype(np.float64)","    ","    result = run_funnel(","        open_,","        high,","        low,","        close,"]}
{"type":"file_chunk","path":"tests/test_funnel_topk_no_human_contract.py","chunk_index":1,"line_start":201,"line_end":225,"content":["        params_matrix,","        k=5,","    )","    ","    # Check all Stage0 results","    forbidden_fields = {\"net\", \"profit\", \"mdd\", \"dd\", \"drawdown\", \"sqn\", \"sharpe\", ","                       \"winrate\", \"equity\", \"pnl\", \"trades\", \"score\"}","    ","    for stage0_result in result.stage0_results:","        # Get field names","        if hasattr(stage0_result, \"__dataclass_fields__\"):","            field_names = set(stage0_result.__dataclass_fields__.keys())","        else:","            field_names = set(getattr(stage0_result, \"__dict__\", {}).keys())","        ","        # Check no forbidden fields","        for field_name in field_names:","            field_lower = field_name.lower()","            for forbidden in forbidden_fields:","                assert forbidden not in field_lower, (","                    f\"Stage0Result contains forbidden PnL field: {field_name} \"","                    f\"(contains '{forbidden}')\"","                )","",""]}
{"type":"file_footer","path":"tests/test_funnel_topk_no_human_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_generate_research_cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4080,"sha256":"560ba869562172f07c7455f5a127b136567a5119045942cb7dd10c6bd2cb9211","total_lines":118,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_generate_research_cli.py","chunk_index":0,"line_start":1,"line_end":118,"content":["\"\"\"Test generate_research.py CLI behavior.","","Ensure that:","1. -h / --help does not execute generate logic","2. --dry-run works without writing files","3. Script does not crash on import errors","\"\"\"","","from __future__ import annotations","","import subprocess","import sys","from pathlib import Path","import pytest","","","def test_generate_research_help_does_not_execute():","    \"\"\"Test that -h/--help does not execute generate logic.\"\"\"","    # Test -h","    result = subprocess.run(","        [sys.executable, \"scripts/generate_research.py\", \"-h\"],","        cwd=Path(__file__).parent.parent,","        capture_output=True,","        text=True,","    )","    ","    assert result.returncode == 0, f\"Help should exit with 0, got {result.returncode}\"","    assert \"usage:\" in result.stdout.lower() or \"help\" in result.stdout.lower()","    assert \"error\" not in result.stdout.lower()","    assert \"error\" not in result.stderr.lower()","    ","    # Test --help","    result = subprocess.run(","        [sys.executable, \"scripts/generate_research.py\", \"--help\"],","        cwd=Path(__file__).parent.parent,","        capture_output=True,","        text=True,","    )","    ","    assert result.returncode == 0, f\"Help should exit with 0, got {result.returncode}\"","    assert \"usage:\" in result.stdout.lower() or \"help\" in result.stdout.lower()","    assert \"error\" not in result.stdout.lower()","    assert \"error\" not in result.stderr.lower()","","","def test_generate_research_dry_run():","    \"\"\"Test that --dry-run works without writing files.\"\"\"","    # Create a temporary outputs directory to test","    import tempfile","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        outputs_root = tmp_path / \"outputs\"","        outputs_root.mkdir()","        ","        result = subprocess.run(","            [","                sys.executable,","                \"scripts/generate_research.py\",","                \"--outputs-root\", str(outputs_root),","                \"--dry-run\",","                \"--verbose\",","            ],","            cwd=Path(__file__).parent.parent,","            capture_output=True,","            text=True,","        )","        ","        assert result.returncode == 0, f\"Dry run should exit with 0, got {result.returncode}\"","        assert \"dry run\" in result.stdout.lower() or \"would generate\" in result.stdout.lower()","        ","        # Ensure no files were actually created","        research_dir = outputs_root / \"research\"","        assert not research_dir.exists() or not list(research_dir.glob(\"*.json\"))","","","def test_generate_research_without_outputs_dir():","    \"\"\"Test that script handles missing outputs directory gracefully.\"\"\"","    import tempfile","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        outputs_root = tmp_path / \"nonexistent\"","        ","        result = subprocess.run(","            [","                sys.executable,","                \"scripts/generate_research.py\",","                \"--outputs-root\", str(outputs_root),","            ],","            cwd=Path(__file__).parent.parent,","            capture_output=True,","            text=True,","        )","        ","        # Should either succeed (creating empty results) or fail gracefully","        # but not crash with import errors","        assert result.returncode in (0, 1), f\"Unexpected exit code: {result.returncode}\"","        assert \"import error\" not in result.stderr.lower(), f\"Import error occurred: {result.stderr}\"","","","def test_generate_research_import_fixed():","    \"\"\"Test that import errors are fixed (no NameError for extract_canonical_metrics).\"\"\"","    # This test imports the module directly to check for import errors","    # Note: conftest.py already adds src/ to sys.path, so no need to modify it here","    ","    try:","        from research.__main__ import generate_canonical_results","        from research.registry import build_research_index","        ","        # If we get here, imports succeeded","        assert True","    except ImportError as e:","        pytest.fail(f\"Import error: {e}\")","    except NameError as e:","        pytest.fail(f\"NameError (missing import): {e}\")","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_generate_research_cli.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_golden_kernel_verification.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2498,"sha256":"1a203a6a9a8ab7c3ed9c3716b9771115e5ae93fe0566e605098720f17c3055fa","total_lines":78,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_golden_kernel_verification.py","chunk_index":0,"line_start":1,"line_end":78,"content":["","import numpy as np","","from strategy.kernel import DonchianAtrParams, run_kernel, _max_drawdown","from engine.types import BarArrays","","","def _bars():","    # Small synthetic OHLC series","    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)","    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)","    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)","    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)","    return BarArrays(open=o, high=h, low=l, close=c)","","","def test_no_trade_case_does_not_crash_and_returns_zero_metrics():","    bars = _bars()","    params = DonchianAtrParams(channel_len=99999, atr_len=3, stop_mult=2.0)","","    out = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)","    pnl = out[\"pnl\"]","    equity = out[\"equity\"]","    metrics = out[\"metrics\"]","","    assert isinstance(pnl, np.ndarray)","    assert pnl.size == 0","    assert isinstance(equity, np.ndarray)","    assert equity.size == 0","    assert metrics[\"net_profit\"] == 0.0","    assert metrics[\"trades\"] == 0","    assert metrics[\"max_dd\"] == 0.0","","","def test_vectorized_metrics_are_self_consistent():","    bars = _bars()","    params = DonchianAtrParams(channel_len=2, atr_len=2, stop_mult=1.0)","","    out = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)","    pnl = out[\"pnl\"]","    equity = out[\"equity\"]","    metrics = out[\"metrics\"]","","    # If zero trades, still must be consistent","    if pnl.size == 0:","        assert metrics[\"net_profit\"] == 0.0","        assert metrics[\"trades\"] == 0","        assert metrics[\"max_dd\"] == 0.0","        return","","    # Vectorized checks","    np.testing.assert_allclose(equity, np.cumsum(pnl), rtol=0.0, atol=0.0)","    assert metrics[\"trades\"] == int(pnl.size)","    assert metrics[\"net_profit\"] == float(np.sum(pnl))","    assert metrics[\"max_dd\"] == _max_drawdown(equity)","","","def test_costs_are_parameterized_not_hardcoded():","    bars = _bars()","    params = DonchianAtrParams(channel_len=2, atr_len=2, stop_mult=1.0)","","    out0 = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)","    out1 = run_kernel(bars, params, commission=1.25, slip=0.75, order_qty=1)","","    pnl0 = out0[\"pnl\"]","    pnl1 = out1[\"pnl\"]","","    # Either both empty or both non-empty; if empty, pass","    if pnl0.size == 0:","        assert pnl1.size == 0","        return","","    # Costs increase => pnl decreases by 2*(commission+slip) per trade","    per_trade_delta = 2.0 * (1.25 + 0.75)","    np.testing.assert_allclose(pnl1, pnl0 - per_trade_delta, rtol=0.0, atol=1e-12)","","",""]}
{"type":"file_footer","path":"tests/test_golden_kernel_verification.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_governance_accepts_winners_v2.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8277,"sha256":"7fcca6f5b67a07de96a7836caecd5e5f5b67f6160e2f84d4d28a379221704fba","total_lines":235,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_governance_accepts_winners_v2.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for governance accepting winners v2.","","Tests verify that governance evaluator can read and process v2 winners.json.","\"\"\"","","from __future__ import annotations","","import json","import tempfile","from datetime import datetime, timezone","from pathlib import Path","","from core.governance_schema import Decision","from pipeline.governance_eval import evaluate_governance","","","def _create_fake_manifest(run_id: str, stage_name: str, season: str = \"test\") -> dict:","    \"\"\"Create fake manifest.json.\"\"\"","    return {","        \"run_id\": run_id,","        \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        \"git_sha\": \"abc123def456\",","        \"dirty_repo\": False,","        \"param_subsample_rate\": 0.1,","        \"config_hash\": \"test_hash\",","        \"season\": season,","        \"dataset_id\": \"test_dataset\",","        \"bars\": 1000,","        \"params_total\": 1000,","        \"params_effective\": 100,","        \"artifact_version\": \"v1\",","    }","","","def _create_fake_metrics(stage_name: str, stage_planned_subsample: float = 0.1) -> dict:","    \"\"\"Create fake metrics.json.\"\"\"","    return {","        \"params_total\": 1000,","        \"params_effective\": 100,","        \"bars\": 1000,","        \"stage_name\": stage_name,","        \"param_subsample_rate\": stage_planned_subsample,","        \"stage_planned_subsample\": stage_planned_subsample,","    }","","","def _create_fake_winners_v2(stage_name: str, topk_items: list[dict]) -> dict:","    \"\"\"Create fake winners.json v2.\"\"\"","    return {","        \"schema\": \"v2\",","        \"stage_name\": stage_name,","        \"generated_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        \"topk\": topk_items,","        \"notes\": {","            \"schema\": \"v2\",","            \"candidate_id_mode\": \"strategy_id:param_id\",","        },","    }","","","def _create_fake_config_snapshot() -> dict:","    \"\"\"Create fake config_snapshot.json.\"\"\"","    return {","        \"dataset_id\": \"test_dataset\",","        \"bars\": 1000,","        \"params_total\": 1000,","    }","","","def _write_artifacts(run_dir: Path, manifest: dict, metrics: dict, winners: dict, config: dict) -> None:","    \"\"\"Write artifacts to run directory.\"\"\"","    run_dir.mkdir(parents=True, exist_ok=True)","    ","    with (run_dir / \"manifest.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f, indent=2)","    ","    with (run_dir / \"metrics.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(metrics, f, indent=2)","    ","    with (run_dir / \"winners.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(winners, f, indent=2)","    ","    with (run_dir / \"config_snapshot.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(config, f, indent=2)","","","def test_governance_reads_winners_v2() -> None:","    \"\"\"Test that governance can read and process v2 winners.json.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Stage0 artifacts","        stage0_dir = tmp_path / \"stage0\"","        _write_artifacts(","            stage0_dir,","            _create_fake_manifest(\"stage0-123\", \"stage0_coarse\"),","            _create_fake_metrics(\"stage0_coarse\"),","            _create_fake_winners_v2(\"stage0_coarse\", [","                {","                    \"candidate_id\": \"donchian_atr:0\",","                    \"strategy_id\": \"donchian_atr\",","                    \"symbol\": \"CME.MNQ\",","                    \"timeframe\": \"60m\",","                    \"params\": {},","                    \"score\": 1.0,","                    \"metrics\": {\"proxy_value\": 1.0, \"param_id\": 0},","                    \"source\": {\"param_id\": 0, \"run_id\": \"stage0-123\", \"stage_name\": \"stage0_coarse\"},","                },","            ]),","            _create_fake_config_snapshot(),","        )","        ","        # Stage1 artifacts (v2 format)","        stage1_dir = tmp_path / \"stage1\"","        stage1_winners = _create_fake_winners_v2(\"stage1_topk\", [","            {","                \"candidate_id\": \"donchian_atr:0\",","                \"strategy_id\": \"donchian_atr\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe\": \"60m\",","                \"params\": {},","                \"score\": 100.0,","                \"metrics\": {\"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0, \"param_id\": 0},","                \"source\": {\"param_id\": 0, \"run_id\": \"stage1-123\", \"stage_name\": \"stage1_topk\"},","            },","        ])","        _write_artifacts(","            stage1_dir,","            _create_fake_manifest(\"stage1-123\", \"stage1_topk\"),","            _create_fake_metrics(\"stage1_topk\"),","            stage1_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Stage2 artifacts (v2 format)","        stage2_dir = tmp_path / \"stage2\"","        stage2_winners = _create_fake_winners_v2(\"stage2_confirm\", [","            {","                \"candidate_id\": \"donchian_atr:0\",","                \"strategy_id\": \"donchian_atr\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe\": \"60m\",","                \"params\": {},","                \"score\": 100.0,","                \"metrics\": {\"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0, \"param_id\": 0},","                \"source\": {\"param_id\": 0, \"run_id\": \"stage2-123\", \"stage_name\": \"stage2_confirm\"},","            },","        ])","        _write_artifacts(","            stage2_dir,","            _create_fake_manifest(\"stage2-123\", \"stage2_confirm\"),","            _create_fake_metrics(\"stage2_confirm\"),","            stage2_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Evaluate governance","        report = evaluate_governance(","            stage0_dir=stage0_dir,","            stage1_dir=stage1_dir,","            stage2_dir=stage2_dir,","        )","        ","        # Verify governance processed v2 format","        assert len(report.items) == 1","        item = report.items[0]","        ","        # Verify candidate_id is preserved","        assert item.candidate_id == \"donchian_atr:0\"","        ","        # Verify decision was made (should be KEEP since all rules pass)","        assert item.decision in (Decision.KEEP, Decision.FREEZE, Decision.DROP)","","","def test_governance_handles_mixed_v2_legacy() -> None:","    \"\"\"Test that governance handles mixed v2/legacy formats gracefully.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Stage0 artifacts (legacy)","        stage0_dir = tmp_path / \"stage0\"","        _write_artifacts(","            stage0_dir,","            _create_fake_manifest(\"stage0-123\", \"stage0_coarse\"),","            _create_fake_metrics(\"stage0_coarse\"),","            {\"topk\": [{\"param_id\": 0, \"proxy_value\": 1.0}], \"notes\": {\"schema\": \"v1\"}},","            _create_fake_config_snapshot(),","        )","        ","        # Stage1 artifacts (v2)","        stage1_dir = tmp_path / \"stage1\"","        stage1_winners = _create_fake_winners_v2(\"stage1_topk\", [","            {","                \"candidate_id\": \"donchian_atr:0\",","                \"strategy_id\": \"donchian_atr\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe\": \"60m\",","                \"params\": {},","                \"score\": 100.0,"]}
{"type":"file_chunk","path":"tests/test_governance_accepts_winners_v2.py","chunk_index":1,"line_start":201,"line_end":235,"content":["                \"metrics\": {\"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0, \"param_id\": 0},","                \"source\": {\"param_id\": 0, \"run_id\": \"stage1-123\", \"stage_name\": \"stage1_topk\"},","            },","        ])","        _write_artifacts(","            stage1_dir,","            _create_fake_manifest(\"stage1-123\", \"stage1_topk\"),","            _create_fake_metrics(\"stage1_topk\"),","            stage1_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Stage2 artifacts (legacy)","        stage2_dir = tmp_path / \"stage2\"","        _write_artifacts(","            stage2_dir,","            _create_fake_manifest(\"stage2-123\", \"stage2_confirm\"),","            _create_fake_metrics(\"stage2_confirm\"),","            {\"topk\": [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}], \"notes\": {\"schema\": \"v1\"}},","            _create_fake_config_snapshot(),","        )","        ","        # Evaluate governance (should handle mixed formats)","        report = evaluate_governance(","            stage0_dir=stage0_dir,","            stage1_dir=stage1_dir,","            stage2_dir=stage2_dir,","        )","        ","        # Verify governance processed successfully","        assert len(report.items) == 1","        item = report.items[0]","        assert item.candidate_id == \"donchian_atr:0\"","",""]}
{"type":"file_footer","path":"tests/test_governance_accepts_winners_v2.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_governance_eval_rules.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":11817,"sha256":"2269afcaf928ddf21acb7714026424aa0735e8dc5b698cce40e46cac30b957f1","total_lines":352,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_governance_eval_rules.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for governance evaluation rules.","","Tests that governance rules (R1/R2/R3) are correctly applied using fixture artifacts.","\"\"\"","","from __future__ import annotations","","import json","import tempfile","from pathlib import Path","from datetime import datetime, timezone","","import pytest","","from core.governance_schema import Decision","from pipeline.governance_eval import evaluate_governance","","","def _create_fake_manifest(run_id: str, stage_name: str, season: str = \"test\") -> dict:","    \"\"\"Create fake manifest.json.\"\"\"","    return {","        \"run_id\": run_id,","        \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        \"git_sha\": \"abc123def456\",","        \"dirty_repo\": False,","        \"param_subsample_rate\": 0.1,","        \"config_hash\": \"test_hash\",","        \"season\": season,","        \"dataset_id\": \"test_dataset\",","        \"bars\": 1000,","        \"params_total\": 1000,","        \"params_effective\": 100,","        \"artifact_version\": \"v1\",","    }","","","def _create_fake_metrics(stage_name: str, stage_planned_subsample: float = 0.1) -> dict:","    \"\"\"Create fake metrics.json.\"\"\"","    return {","        \"params_total\": 1000,","        \"params_effective\": 100,","        \"bars\": 1000,","        \"stage_name\": stage_name,","        \"param_subsample_rate\": stage_planned_subsample,","        \"stage_planned_subsample\": stage_planned_subsample,","    }","","","def _create_fake_winners(stage_name: str, topk_items: list[dict]) -> dict:","    \"\"\"Create fake winners.json.\"\"\"","    return {","        \"topk\": topk_items,","        \"notes\": {","            \"schema\": \"v1\",","            \"stage\": stage_name,","            \"topk_count\": len(topk_items),","        },","    }","","","def _create_fake_config_snapshot() -> dict:","    \"\"\"Create fake config_snapshot.json.\"\"\"","    return {","        \"dataset_id\": \"test_dataset\",","        \"bars\": 1000,","        \"params_total\": 1000,","    }","","","def _write_artifacts(run_dir: Path, manifest: dict, metrics: dict, winners: dict, config: dict) -> None:","    \"\"\"Write artifacts to run directory.\"\"\"","    run_dir.mkdir(parents=True, exist_ok=True)","    ","    with (run_dir / \"manifest.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f, indent=2)","    ","    with (run_dir / \"metrics.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(metrics, f, indent=2)","    ","    with (run_dir / \"winners.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(winners, f, indent=2)","    ","    with (run_dir / \"config_snapshot.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(config, f, indent=2)","","","def test_r1_drop_when_stage2_missing() -> None:","    \"\"\"","    Test R1: DROP when candidate in Stage1 but missing in Stage2.","    ","    Scenario:","    - Stage1 has candidate with param_id=0","    - Stage2 does not have candidate with param_id=0","    - Expected: DROP with reason \"unverified\"","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Stage0 artifacts","        stage0_dir = tmp_path / \"stage0\"","        _write_artifacts(","            stage0_dir,","            _create_fake_manifest(\"stage0-123\", \"stage0_coarse\"),","            _create_fake_metrics(\"stage0_coarse\"),","            _create_fake_winners(\"stage0_coarse\", [{\"param_id\": 0, \"proxy_value\": 1.0}]),","            _create_fake_config_snapshot(),","        )","        ","        # Stage1 artifacts (has candidate)","        stage1_dir = tmp_path / \"stage1\"","        stage1_winners = _create_fake_winners(","            \"stage1_topk\",","            [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}],","        )","        _write_artifacts(","            stage1_dir,","            _create_fake_manifest(\"stage1-123\", \"stage1_topk\"),","            _create_fake_metrics(\"stage1_topk\"),","            stage1_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Stage2 artifacts (missing candidate)","        stage2_dir = tmp_path / \"stage2\"","        stage2_winners = _create_fake_winners(","            \"stage2_confirm\",","            [{\"param_id\": 1, \"net_profit\": 200.0, \"trades\": 20, \"max_dd\": -20.0}],  # Different param_id","        )","        _write_artifacts(","            stage2_dir,","            _create_fake_manifest(\"stage2-123\", \"stage2_confirm\"),","            _create_fake_metrics(\"stage2_confirm\"),","            stage2_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Evaluate governance","        report = evaluate_governance(","            stage0_dir=stage0_dir,","            stage1_dir=stage1_dir,","            stage2_dir=stage2_dir,","        )","        ","        # Verify: candidate should be DROP","        assert len(report.items) == 1","        item = report.items[0]","        assert item.decision == Decision.DROP","        assert any(\"R1\" in reason for reason in item.reasons)","        assert any(\"unverified\" in reason.lower() for reason in item.reasons)","","","def test_r2_drop_when_metric_degrades_over_threshold() -> None:","    \"\"\"","    Test R2: DROP when metrics degrade > 20% from Stage1 to Stage2.","    ","    Scenario:","    - Stage1: net_profit=100, max_dd=-10 -> net_over_mdd = 10.0","    - Stage2: net_profit=70, max_dd=-10 -> net_over_mdd = 7.0","    - Degradation: (10.0 - 7.0) / 10.0 = 0.30 (30% > 20% threshold)","    - Expected: DROP with reason \"degraded\"","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Stage0 artifacts","        stage0_dir = tmp_path / \"stage0\"","        _write_artifacts(","            stage0_dir,","            _create_fake_manifest(\"stage0-123\", \"stage0_coarse\"),","            _create_fake_metrics(\"stage0_coarse\"),","            _create_fake_winners(\"stage0_coarse\", [{\"param_id\": 0, \"proxy_value\": 1.0}]),","            _create_fake_config_snapshot(),","        )","        ","        # Stage1 artifacts","        stage1_dir = tmp_path / \"stage1\"","        stage1_winners = _create_fake_winners(","            \"stage1_topk\",","            [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}],","        )","        _write_artifacts(","            stage1_dir,","            _create_fake_manifest(\"stage1-123\", \"stage1_topk\"),","            _create_fake_metrics(\"stage1_topk\"),","            stage1_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Stage2 artifacts (degraded metrics)","        stage2_dir = tmp_path / \"stage2\"","        stage2_winners = _create_fake_winners(","            \"stage2_confirm\",","            [{\"param_id\": 0, \"net_profit\": 70.0, \"trades\": 10, \"max_dd\": -10.0}],  # 30% degradation","        )","        _write_artifacts(","            stage2_dir,","            _create_fake_manifest(\"stage2-123\", \"stage2_confirm\"),","            _create_fake_metrics(\"stage2_confirm\"),","            stage2_winners,"]}
{"type":"file_chunk","path":"tests/test_governance_eval_rules.py","chunk_index":1,"line_start":201,"line_end":352,"content":["            _create_fake_config_snapshot(),","        )","        ","        # Evaluate governance","        report = evaluate_governance(","            stage0_dir=stage0_dir,","            stage1_dir=stage1_dir,","            stage2_dir=stage2_dir,","        )","        ","        # Verify: candidate should be DROP","        assert len(report.items) == 1","        item = report.items[0]","        assert item.decision == Decision.DROP","        assert any(\"R2\" in reason for reason in item.reasons)","        assert any(\"degraded\" in reason.lower() for reason in item.reasons)","","","def test_r3_freeze_when_density_over_threshold() -> None:","    \"\"\"","    Test R3: FREEZE when same strategy_id appears >= 3 times in Stage1 topk.","    ","    Scenario:","    - Stage1 has 5 candidates with same strategy_id (donchian_atr)","    - Expected: FREEZE with reason \"density\"","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Stage0 artifacts","        stage0_dir = tmp_path / \"stage0\"","        _write_artifacts(","            stage0_dir,","            _create_fake_manifest(\"stage0-123\", \"stage0_coarse\"),","            _create_fake_metrics(\"stage0_coarse\"),","            _create_fake_winners(\"stage0_coarse\", [{\"param_id\": i, \"proxy_value\": 1.0} for i in range(5)]),","            _create_fake_config_snapshot(),","        )","        ","        # Stage1 artifacts (5 candidates)","        stage1_dir = tmp_path / \"stage1\"","        stage1_winners = _create_fake_winners(","            \"stage1_topk\",","            [","                {\"param_id\": i, \"net_profit\": 100.0 + i, \"trades\": 10, \"max_dd\": -10.0}","                for i in range(5)","            ],","        )","        _write_artifacts(","            stage1_dir,","            _create_fake_manifest(\"stage1-123\", \"stage1_topk\"),","            _create_fake_metrics(\"stage1_topk\"),","            stage1_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Stage2 artifacts (all candidates present)","        stage2_dir = tmp_path / \"stage2\"","        stage2_winners = _create_fake_winners(","            \"stage2_confirm\",","            [","                {\"param_id\": i, \"net_profit\": 100.0 + i, \"trades\": 10, \"max_dd\": -10.0}","                for i in range(5)","            ],","        )","        _write_artifacts(","            stage2_dir,","            _create_fake_manifest(\"stage2-123\", \"stage2_confirm\"),","            _create_fake_metrics(\"stage2_confirm\"),","            stage2_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Evaluate governance","        report = evaluate_governance(","            stage0_dir=stage0_dir,","            stage1_dir=stage1_dir,","            stage2_dir=stage2_dir,","        )","        ","        # Verify: all candidates should be FREEZE (density >= 3)","        assert len(report.items) == 5","        for item in report.items:","            assert item.decision == Decision.FREEZE","            assert any(\"R3\" in reason for reason in item.reasons)","            assert any(\"density\" in reason.lower() for reason in item.reasons)","","","def test_keep_when_all_rules_pass() -> None:","    \"\"\"","    Test KEEP when all rules pass.","    ","    Scenario:","    - R1: Stage2 has candidate (pass)","    - R2: Metrics do not degrade (pass)","    - R3: Density < threshold (pass)","    - Expected: KEEP","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Stage0 artifacts","        stage0_dir = tmp_path / \"stage0\"","        _write_artifacts(","            stage0_dir,","            _create_fake_manifest(\"stage0-123\", \"stage0_coarse\"),","            _create_fake_metrics(\"stage0_coarse\"),","            _create_fake_winners(\"stage0_coarse\", [{\"param_id\": 0, \"proxy_value\": 1.0}]),","            _create_fake_config_snapshot(),","        )","        ","        # Stage1 artifacts (single candidate, low density)","        stage1_dir = tmp_path / \"stage1\"","        stage1_winners = _create_fake_winners(","            \"stage1_topk\",","            [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}],","        )","        _write_artifacts(","            stage1_dir,","            _create_fake_manifest(\"stage1-123\", \"stage1_topk\"),","            _create_fake_metrics(\"stage1_topk\"),","            stage1_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Stage2 artifacts (same metrics, no degradation)","        stage2_dir = tmp_path / \"stage2\"","        stage2_winners = _create_fake_winners(","            \"stage2_confirm\",","            [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}],","        )","        _write_artifacts(","            stage2_dir,","            _create_fake_manifest(\"stage2-123\", \"stage2_confirm\"),","            _create_fake_metrics(\"stage2_confirm\"),","            stage2_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Evaluate governance","        report = evaluate_governance(","            stage0_dir=stage0_dir,","            stage1_dir=stage1_dir,","            stage2_dir=stage2_dir,","        )","        ","        # Verify: candidate should be KEEP","        assert len(report.items) == 1","        item = report.items[0]","        assert item.decision == Decision.KEEP","",""]}
{"type":"file_footer","path":"tests/test_governance_eval_rules.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_governance_schema_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3499,"sha256":"b65de1f36411a53f59cdec600187a4fbfc98cba7c79bb6b03a94c7b61585e4bc","total_lines":115,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_governance_schema_contract.py","chunk_index":0,"line_start":1,"line_end":115,"content":["","\"\"\"Contract tests for governance schema.","","Tests that governance schema is JSON-serializable and follows contracts.","\"\"\"","","from __future__ import annotations","","import json","from datetime import datetime, timezone","","from core.governance_schema import (","    Decision,","    EvidenceRef,","    GovernanceItem,","    GovernanceReport,",")","","","def test_governance_report_json_serializable() -> None:","    \"\"\"","    Test that GovernanceReport is JSON-serializable.","    ","    This is a critical contract: governance.json must be machine-readable.","    \"\"\"","    # Create sample evidence","    evidence = [","        EvidenceRef(","            run_id=\"test-run-123\",","            stage_name=\"stage1_topk\",","            artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","            key_metrics={\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10},","        ),","    ]","    ","    # Create sample item","    item = GovernanceItem(","        candidate_id=\"donchian_atr:abc123def456\",","        decision=Decision.KEEP,","        reasons=[\"R3: density_5_over_threshold_3\"],","        evidence=evidence,","        created_at=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        git_sha=\"abc123def456\",","    )","    ","    # Create report","    report = GovernanceReport(","        items=[item],","        metadata={","            \"governance_id\": \"gov-20251218T000000Z-12345678\",","            \"season\": \"test_season\",","            \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","            \"git_sha\": \"abc123def456\",","        },","    )","    ","    # Convert to dict","    report_dict = report.to_dict()","    ","    # Serialize to JSON","    json_str = json.dumps(report_dict, ensure_ascii=False, sort_keys=True, indent=2)","    ","    # Deserialize back","    report_dict_roundtrip = json.loads(json_str)","    ","    # Verify structure","    assert \"items\" in report_dict_roundtrip","    assert \"metadata\" in report_dict_roundtrip","    assert len(report_dict_roundtrip[\"items\"]) == 1","    ","    item_dict = report_dict_roundtrip[\"items\"][0]","    assert item_dict[\"candidate_id\"] == \"donchian_atr:abc123def456\"","    assert item_dict[\"decision\"] == \"KEEP\"","    assert len(item_dict[\"reasons\"]) == 1","    assert len(item_dict[\"evidence\"]) == 1","    ","    evidence_dict = item_dict[\"evidence\"][0]","    assert evidence_dict[\"run_id\"] == \"test-run-123\"","    assert evidence_dict[\"stage_name\"] == \"stage1_topk\"","    assert \"artifact_paths\" in evidence_dict","    assert \"key_metrics\" in evidence_dict","","","def test_decision_enum_values() -> None:","    \"\"\"Test that Decision enum has correct values.\"\"\"","    assert Decision.KEEP.value == \"KEEP\"","    assert Decision.FREEZE.value == \"FREEZE\"","    assert Decision.DROP.value == \"DROP\"","","","def test_evidence_ref_contains_subsample_fields() -> None:","    \"\"\"","    Test that EvidenceRef can contain subsample fields in key_metrics.","    ","    This is a critical requirement: subsample info must be in evidence.","    \"\"\"","    evidence = EvidenceRef(","        run_id=\"test-run-123\",","        stage_name=\"stage1_topk\",","        artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","        key_metrics={","            \"param_id\": 0,","            \"net_profit\": 100.0,","            \"stage_planned_subsample\": 0.1,","            \"param_subsample_rate\": 0.1,","            \"params_effective\": 100,","        },","    )","    ","    # Verify subsample fields are present","    assert \"stage_planned_subsample\" in evidence.key_metrics","    assert \"param_subsample_rate\" in evidence.key_metrics","    assert \"params_effective\" in evidence.key_metrics","",""]}
{"type":"file_footer","path":"tests/test_governance_schema_contract.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_governance_transition.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2738,"sha256":"acab20bb4fb6f9c10a38acc925f5af2f8ae3fdd7320a3c19ce54e6726c439514","total_lines":83,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_governance_transition.py","chunk_index":0,"line_start":1,"line_end":83,"content":["","\"\"\"Contract tests for governance lifecycle state transitions.","","Tests transition matrix: prev_state × decision → next_state","\"\"\"","","from __future__ import annotations","","import pytest","","from core.governance.transition import governance_transition","from core.schemas.governance import Decision, LifecycleState","","","# Transition test matrix: (prev_state, decision, expected_next_state)","TRANSITION_TEST_CASES = [","    # INCUBATION transitions","    (\"INCUBATION\", Decision.KEEP, \"CANDIDATE\"),","    (\"INCUBATION\", Decision.DROP, \"RETIRED\"),","    (\"INCUBATION\", Decision.FREEZE, \"INCUBATION\"),","    ","    # CANDIDATE transitions","    (\"CANDIDATE\", Decision.KEEP, \"LIVE\"),","    (\"CANDIDATE\", Decision.DROP, \"RETIRED\"),","    (\"CANDIDATE\", Decision.FREEZE, \"CANDIDATE\"),","    ","    # LIVE transitions","    (\"LIVE\", Decision.KEEP, \"LIVE\"),","    (\"LIVE\", Decision.DROP, \"RETIRED\"),","    (\"LIVE\", Decision.FREEZE, \"LIVE\"),","    ","    # RETIRED is terminal (no transitions)","    (\"RETIRED\", Decision.KEEP, \"RETIRED\"),","    (\"RETIRED\", Decision.DROP, \"RETIRED\"),","    (\"RETIRED\", Decision.FREEZE, \"RETIRED\"),","]","","","@pytest.mark.parametrize(\"prev_state,decision,expected_next_state\", TRANSITION_TEST_CASES)","def test_governance_transition_matrix(","    prev_state: LifecycleState,","    decision: Decision,","    expected_next_state: LifecycleState,",") -> None:","    \"\"\"","    Test governance transition for all state × decision combinations.","    ","    This is a table-driven test covering the complete transition matrix.","    \"\"\"","    result = governance_transition(prev_state, decision)","    ","    assert result == expected_next_state, (","        f\"Transition failed: {prev_state} + {decision.value} → {result}, \"","        f\"expected {expected_next_state}\"","    )","","","def test_governance_transition_incubation_to_candidate() -> None:","    \"\"\"Test INCUBATION → CANDIDATE transition.\"\"\"","    result = governance_transition(\"INCUBATION\", Decision.KEEP)","    assert result == \"CANDIDATE\"","","","def test_governance_transition_incubation_to_retired() -> None:","    \"\"\"Test INCUBATION → RETIRED transition.\"\"\"","    result = governance_transition(\"INCUBATION\", Decision.DROP)","    assert result == \"RETIRED\"","","","def test_governance_transition_candidate_to_live() -> None:","    \"\"\"Test CANDIDATE → LIVE transition.\"\"\"","    result = governance_transition(\"CANDIDATE\", Decision.KEEP)","    assert result == \"LIVE\"","","","def test_governance_transition_retired_terminal() -> None:","    \"\"\"Test that RETIRED is terminal state (no transitions).\"\"\"","    # RETIRED should remain RETIRED regardless of decision","    assert governance_transition(\"RETIRED\", Decision.KEEP) == \"RETIRED\"","    assert governance_transition(\"RETIRED\", Decision.DROP) == \"RETIRED\"","    assert governance_transition(\"RETIRED\", Decision.FREEZE) == \"RETIRED\"","",""]}
{"type":"file_footer","path":"tests/test_governance_transition.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_governance_writer_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7463,"sha256":"8be6d032d017879c3ad0b91713977950ba97f665546c8c13c64c63352d0edd7e","total_lines":212,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_governance_writer_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for governance writer.","","Tests that governance writer creates expected directory structure and files.","\"\"\"","","from __future__ import annotations","","import json","import tempfile","from pathlib import Path","from datetime import datetime, timezone","","from core.governance_schema import (","    Decision,","    EvidenceRef,","    GovernanceItem,","    GovernanceReport,",")","from core.governance_writer import write_governance_artifacts","","","def test_governance_writer_creates_expected_tree() -> None:","    \"\"\"","    Test that governance writer creates expected directory structure.","    ","    Expected:","    - governance.json (machine-readable)","    - README.md (human-readable)","    - evidence_index.json (optional but recommended)","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        governance_dir = Path(tmpdir) / \"governance\" / \"test-123\"","        ","        # Create sample report","        evidence = [","            EvidenceRef(","                run_id=\"stage1-123\",","                stage_name=\"stage1_topk\",","                artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","                key_metrics={","                    \"param_id\": 0,","                    \"net_profit\": 100.0,","                    \"stage_planned_subsample\": 0.1,","                    \"param_subsample_rate\": 0.1,","                    \"params_effective\": 100,","                },","            ),","        ]","        ","        item = GovernanceItem(","            candidate_id=\"donchian_atr:abc123def456\",","            decision=Decision.KEEP,","            reasons=[],","            evidence=evidence,","            created_at=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","            git_sha=\"abc123def456\",","        )","        ","        report = GovernanceReport(","            items=[item],","            metadata={","                \"governance_id\": \"gov-123\",","                \"season\": \"test_season\",","                \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","                \"git_sha\": \"abc123def456\",","                \"decisions\": {\"KEEP\": 1, \"FREEZE\": 0, \"DROP\": 0},","            },","        )","        ","        # Write artifacts","        write_governance_artifacts(governance_dir, report)","        ","        # Verify files exist","        assert governance_dir.exists()","        assert (governance_dir / \"governance.json\").exists()","        assert (governance_dir / \"README.md\").exists()","        assert (governance_dir / \"evidence_index.json\").exists()","        ","        # Verify governance.json is valid JSON","        with (governance_dir / \"governance.json\").open(\"r\", encoding=\"utf-8\") as f:","            governance_dict = json.load(f)","        ","        assert \"items\" in governance_dict","        assert \"metadata\" in governance_dict","        assert len(governance_dict[\"items\"]) == 1","        ","        # Verify README.md contains key information","        readme_text = (governance_dir / \"README.md\").read_text(encoding=\"utf-8\")","        assert \"Governance Report\" in readme_text","        assert \"governance_id\" in readme_text","        assert \"Decision Summary\" in readme_text","        assert \"KEEP\" in readme_text","        ","        # Verify evidence_index.json is valid JSON","        with (governance_dir / \"evidence_index.json\").open(\"r\", encoding=\"utf-8\") as f:","            evidence_index = json.load(f)","        ","        assert \"governance_id\" in evidence_index","        assert \"evidence_by_candidate\" in evidence_index","","","def test_governance_json_contains_subsample_fields_in_evidence() -> None:","    \"\"\"","    Test that governance.json contains subsample fields in evidence.","    ","    Critical requirement: subsample info must be in evidence chain.","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        governance_dir = Path(tmpdir) / \"governance\" / \"test-123\"","        ","        # Create report with subsample fields in evidence","        evidence = [","            EvidenceRef(","                run_id=\"stage1-123\",","                stage_name=\"stage1_topk\",","                artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","                key_metrics={","                    \"param_id\": 0,","                    \"net_profit\": 100.0,","                    \"stage_planned_subsample\": 0.1,","                    \"param_subsample_rate\": 0.1,","                    \"params_effective\": 100,","                },","            ),","        ]","        ","        item = GovernanceItem(","            candidate_id=\"donchian_atr:abc123def456\",","            decision=Decision.KEEP,","            reasons=[],","            evidence=evidence,","            created_at=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","            git_sha=\"abc123def456\",","        )","        ","        report = GovernanceReport(","            items=[item],","            metadata={","                \"governance_id\": \"gov-123\",","                \"season\": \"test_season\",","                \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","                \"git_sha\": \"abc123def456\",","                \"decisions\": {\"KEEP\": 1, \"FREEZE\": 0, \"DROP\": 0},","            },","        )","        ","        # Write artifacts","        write_governance_artifacts(governance_dir, report)","        ","        # Verify subsample fields are in governance.json","        with (governance_dir / \"governance.json\").open(\"r\", encoding=\"utf-8\") as f:","            governance_dict = json.load(f)","        ","        item_dict = governance_dict[\"items\"][0]","        evidence_dict = item_dict[\"evidence\"][0]","        key_metrics = evidence_dict[\"key_metrics\"]","        ","        assert \"stage_planned_subsample\" in key_metrics","        assert \"param_subsample_rate\" in key_metrics","        assert \"params_effective\" in key_metrics","","","def test_readme_contains_freeze_reasons() -> None:","    \"\"\"","    Test that README.md contains FREEZE reasons.","    ","    Requirement: README must list FREEZE reasons (concise).","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        governance_dir = Path(tmpdir) / \"governance\" / \"test-123\"","        ","        # Create report with FREEZE item","        evidence = [","            EvidenceRef(","                run_id=\"stage1-123\",","                stage_name=\"stage1_topk\",","                artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","                key_metrics={\"param_id\": 0, \"net_profit\": 100.0},","            ),","        ]","        ","        freeze_item = GovernanceItem(","            candidate_id=\"donchian_atr:abc123def456\",","            decision=Decision.FREEZE,","            reasons=[\"R3: density_5_over_threshold_3\"],","            evidence=evidence,","            created_at=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","            git_sha=\"abc123def456\",","        )","        ","        report = GovernanceReport(","            items=[freeze_item],","            metadata={","                \"governance_id\": \"gov-123\",","                \"season\": \"test_season\",","                \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","                \"git_sha\": \"abc123def456\",","                \"decisions\": {\"KEEP\": 0, \"FREEZE\": 1, \"DROP\": 0},","            },"]}
{"type":"file_chunk","path":"tests/test_governance_writer_contract.py","chunk_index":1,"line_start":201,"line_end":212,"content":["        )","        ","        # Write artifacts","        write_governance_artifacts(governance_dir, report)","        ","        # Verify README contains FREEZE reasons","        readme_text = (governance_dir / \"README.md\").read_text(encoding=\"utf-8\")","        assert \"FREEZE Reasons\" in readme_text","        assert \"donchian_atr:abc123def456\" in readme_text","        assert \"density\" in readme_text","",""]}
{"type":"file_footer","path":"tests/test_governance_writer_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_grid_runner_smoke.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2020,"sha256":"af71811716352a4b8192f080bb2bf7a87f3fd9d70bf5323e248e7571bfe3f21c","total_lines":62,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_grid_runner_smoke.py","chunk_index":0,"line_start":1,"line_end":62,"content":["","import numpy as np","","from pipeline.runner_grid import run_grid","","","def _ohlc():","    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)","    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)","    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)","    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)","    return o, h, l, c","","","def test_grid_runner_smoke_shapes_and_no_crash():","    o, h, l, c = _ohlc()","","    # params: [channel_len, atr_len, stop_mult]","    params = np.array(","        [","            [2, 2, 1.0],","            [3, 2, 1.5],","            [99999, 3, 2.0],  # should produce 0 trades","            [2, 99999, 2.0],  # atr_len > n should be safe (atr_wilder returns all-NaN -> kernel => 0 trades)","        ],","        dtype=np.float64,","    )","","    out = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=True)","    m = out[\"metrics\"]","    order = out[\"order\"]","","    assert isinstance(m, np.ndarray)","    assert m.shape == (params.shape[0], 3)","    assert isinstance(order, np.ndarray)","    assert order.shape == (params.shape[0],)","    assert set(order.tolist()) == set(range(params.shape[0]))","    # Optional stronger assertion: at least one row should have 0 trades due to atr_len > n","    assert np.any(m[:, 1] == 0.0)","","","def test_grid_runner_sorting_toggle():","    o, h, l, c = _ohlc()","    params = np.array(","        [","            [3, 2, 1.5],","            [2, 2, 1.0],","            [2, 3, 2.0],","        ],","        dtype=np.float64,","    )","","    out_sorted = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=True)","    out_unsorted = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=False)","","    assert out_sorted[\"metrics\"].shape == out_unsorted[\"metrics\"].shape == (3, 3)","    assert out_sorted[\"order\"].shape == out_unsorted[\"order\"].shape == (3,)","    # unsorted order should be identity","    np.testing.assert_array_equal(out_unsorted[\"order\"], np.array([0, 1, 2], dtype=np.int64))","","",""]}
{"type":"file_footer","path":"tests/test_grid_runner_smoke.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_indicators_consistency.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2635,"sha256":"376717da36d1b11b1382c096814fe3c5a5745ae7cf549d0eae1f2e9f1dcd0afd","total_lines":102,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_indicators_consistency.py","chunk_index":0,"line_start":1,"line_end":102,"content":["","import numpy as np","","from indicators.numba_indicators import (","    rolling_max,","    rolling_min,","    atr_wilder,",")","","","def _py_rolling_max(arr: np.ndarray, window: int) -> np.ndarray:","    n = arr.shape[0]","    out = np.full(n, np.nan, dtype=np.float64)","    if window <= 0:","        return out","    for i in range(n):","        if i < window - 1:","            continue","        start = i - window + 1","        m = arr[start]","        for j in range(start + 1, i + 1):","            v = arr[j]","            if v > m:","                m = v","        out[i] = m","    return out","","","def _py_rolling_min(arr: np.ndarray, window: int) -> np.ndarray:","    n = arr.shape[0]","    out = np.full(n, np.nan, dtype=np.float64)","    if window <= 0:","        return out","    for i in range(n):","        if i < window - 1:","            continue","        start = i - window + 1","        m = arr[start]","        for j in range(start + 1, i + 1):","            v = arr[j]","            if v < m:","                m = v","        out[i] = m","    return out","","","def _py_atr_wilder(high, low, close, window):","    n = len(high)","    out = np.full(n, np.nan, dtype=np.float64)","    if window > n:","        return out","    tr = np.empty(n, dtype=np.float64)","    tr[0] = high[0] - low[0]","    for i in range(1, n):","        tr[i] = max(","            high[i] - low[i],","            abs(high[i] - close[i - 1]),","            abs(low[i] - close[i - 1]),","        )","    end = window","    out[end - 1] = np.mean(tr[:end])","    for i in range(window, n):","        out[i] = (out[i - 1] * (window - 1) + tr[i]) / window","    return out","","","def test_rolling_max_min_consistency():","    arr = np.array([1.0, 3.0, 2.0, 5.0, 4.0], dtype=np.float64)","    w = 3","","    mx_py = _py_rolling_max(arr, w)","    mn_py = _py_rolling_min(arr, w)","","    mx = rolling_max(arr, w)","    mn = rolling_min(arr, w)","","    np.testing.assert_allclose(mx, mx_py, rtol=0.0, atol=0.0)","    np.testing.assert_allclose(mn, mn_py, rtol=0.0, atol=0.0)","","","def test_atr_wilder_consistency():","    high = np.array([10, 11, 12, 11, 13, 14], dtype=np.float64)","    low = np.array([9, 9, 10, 9, 11, 12], dtype=np.float64)","    close = np.array([9.5, 10.5, 11.0, 10.0, 12.0, 13.0], dtype=np.float64)","    w = 3","","    atr_py = _py_atr_wilder(high, low, close, w)","    atr = atr_wilder(high, low, close, w)","","    np.testing.assert_allclose(atr, atr_py, rtol=0.0, atol=1e-12)","","","def test_atr_wilder_window_gt_n_returns_all_nan():","    high = np.array([10, 11], dtype=np.float64)","    low = np.array([9, 10], dtype=np.float64)","    close = np.array([9.5, 10.5], dtype=np.float64)","    atr = atr_wilder(high, low, close, 999)","    assert atr.shape == (2,)","    assert np.all(np.isnan(atr))","","",""]}
{"type":"file_footer","path":"tests/test_indicators_consistency.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_indicators_precompute_bit_exact.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4726,"sha256":"6907bf65c08848e514581b8ac04083727eba78baf51ac76e06d85a3ac6387438","total_lines":139,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_indicators_precompute_bit_exact.py","chunk_index":0,"line_start":1,"line_end":139,"content":["","\"\"\"","Stage P2-2 Step B: Bit-exact test for precomputed indicators.","","Verifies that using precomputed indicators produces identical results","to computing indicators inline in the kernel.","\"\"\"","from __future__ import annotations","","from dataclasses import asdict, is_dataclass","","import numpy as np","","from engine.types import BarArrays, Fill","from strategy.kernel import DonchianAtrParams, PrecomputedIndicators, run_kernel_arrays","from indicators.numba_indicators import rolling_max, rolling_min, atr_wilder","","","def _fill_to_tuple(f: Fill) -> tuple:","    \"\"\"","    Convert Fill to a comparable tuple representation.","    ","    Uses dataclasses.asdict for dataclass instances, falls back to __dict__ or repr.","    Returns sorted tuple to ensure deterministic comparison.","    \"\"\"","    if is_dataclass(f):","        d = asdict(f)","    else:","        # fallback: __dict__ (for normal classes)","        d = dict(getattr(f, \"__dict__\", {}))","        if not d:","            # last resort: repr","            return (repr(f),)","    # Fixed ordering to avoid dict order differences","    return tuple(sorted(d.items()))","","","def test_indicators_precompute_bit_exact() -> None:","    \"\"\"","    Test that precomputed indicators produce bit-exact results.","    ","    Strategy:","    - Generate random bars","    - Choose a channel_len and atr_len","    - Run kernel twice:","      A: Without precomputation (precomp=None)","      B: With precomputation (precomp=PrecomputedIndicators(...))","    - Compare: donch_hi/lo/atr arrays, metrics, fills, equity","    \"\"\"","    # Generate random bars","    rng = np.random.default_rng(42)","    n_bars = 500","    close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","    open_ = (high + low) / 2","    ","    high = np.maximum(high, np.maximum(open_, close))","    low = np.minimum(low, np.minimum(open_, close))","    ","    bars = BarArrays(","        open=open_.astype(np.float64),","        high=high.astype(np.float64),","        low=low.astype(np.float64),","        close=close.astype(np.float64),","    )","    ","    # Choose test parameters","    ch_len = 20","    atr_len = 10","    params = DonchianAtrParams(channel_len=ch_len, atr_len=atr_len, stop_mult=1.0)","    ","    # Pre-compute indicators (same logic as runner_grid)","    donch_hi_precomp = rolling_max(bars.high, ch_len)","    donch_lo_precomp = rolling_min(bars.low, ch_len)","    atr_precomp = atr_wilder(bars.high, bars.low, bars.close, atr_len)","    ","    precomp = PrecomputedIndicators(","        donch_hi=donch_hi_precomp,","        donch_lo=donch_lo_precomp,","        atr=atr_precomp,","    )","    ","    # Run A: Without precomputation","    result_a = run_kernel_arrays(","        bars=bars,","        params=params,","        commission=0.0,","        slip=0.0,","        order_qty=1,","        precomp=None,","    )","    ","    # Run B: With precomputation","    result_b = run_kernel_arrays(","        bars=bars,","        params=params,","        commission=0.0,","        slip=0.0,","        order_qty=1,","        precomp=precomp,","    )","    ","    # Verify indicators are bit-exact (if we could access them)","    # Note: We can't directly access internal arrays, but we verify outputs","    ","    # Verify metrics are identical","    metrics_a = result_a[\"metrics\"]","    metrics_b = result_b[\"metrics\"]","    assert metrics_a[\"net_profit\"] == metrics_b[\"net_profit\"], \"net_profit must be identical\"","    assert metrics_a[\"trades\"] == metrics_b[\"trades\"], \"trades must be identical\"","    assert metrics_a[\"max_dd\"] == metrics_b[\"max_dd\"], \"max_dd must be identical\"","    ","    # Verify fills are identical","    fills_a = result_a[\"fills\"]","    fills_b = result_b[\"fills\"]","    assert len(fills_a) == len(fills_b), \"fills count must be identical\"","    for i, (fill_a, fill_b) in enumerate(zip(fills_a, fills_b)):","        assert _fill_to_tuple(fill_a) == _fill_to_tuple(fill_b), f\"fill[{i}] must be identical\"","    ","    # Verify equity arrays are bit-exact","    equity_a = result_a[\"equity\"]","    equity_b = result_b[\"equity\"]","    assert equity_a.shape == equity_b.shape, \"equity shape must be identical\"","    np.testing.assert_array_equal(equity_a, equity_b, \"equity must be bit-exact\")","    ","    # Verify pnl arrays are bit-exact","    pnl_a = result_a[\"pnl\"]","    pnl_b = result_b[\"pnl\"]","    assert pnl_a.shape == pnl_b.shape, \"pnl shape must be identical\"","    np.testing.assert_array_equal(pnl_a, pnl_b, \"pnl must be bit-exact\")","    ","    # Verify observability counts are identical","    obs_a = result_a.get(\"_obs\", {})","    obs_b = result_b.get(\"_obs\", {})","    assert obs_a.get(\"intents_total\") == obs_b.get(\"intents_total\"), \"intents_total must be identical\"","    assert obs_a.get(\"fills_total\") == obs_b.get(\"fills_total\"), \"fills_total must be identical\"","",""]}
{"type":"file_footer","path":"tests/test_indicators_precompute_bit_exact.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_intent_idempotency.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9866,"sha256":"87681b498bd36cf277badd32a78e2cf838748d840966b2e8c2c038adc9b3802e","total_lines":330,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_intent_idempotency.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Test idempotency enforcement in ActionQueue for Attack #9.","","Tests that duplicate intents are rejected based on idempotency_key.","\"\"\"","","import pytest","import asyncio","from datetime import date","","from core.intents import (","    CreateJobIntent, CalculateUnitsIntent, DataSpecIntent,","    IntentStatus",")","from control.action_queue import ActionQueue, reset_action_queue","from core.processor import StateProcessor","","","@pytest.fixture","def action_queue():","    \"\"\"Create a fresh ActionQueue for each test.\"\"\"","    reset_action_queue()","    queue = ActionQueue(max_size=10)","    yield queue","    queue.clear()","","","@pytest.fixture","def sample_data_spec():","    \"\"\"Create a sample DataSpecIntent for testing.\"\"\"","    return DataSpecIntent(","        dataset_id=\"test_dataset\",","        symbols=[\"MNQ\", \"MXF\"],","        timeframes=[\"60m\", \"120m\"],","        start_date=date(2020, 1, 1),","        end_date=date(2024, 12, 31)","    )","","","def test_idempotency_basic(action_queue, sample_data_spec):","    \"\"\"Test basic idempotency: duplicate intents are rejected.\"\"\"","    # Create first intent","    intent1 = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10, \"window_slow\": 30}","    )","    ","    # Create second intent with same parameters (should have same idempotency_key)","    intent2 = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10, \"window_slow\": 30}","    )","    ","    # Submit first intent","    intent1_id = action_queue.submit(intent1)","    assert intent1_id == intent1.intent_id","    assert action_queue.get_queue_size() == 1","    ","    # Submit second intent (should be marked as duplicate)","    intent2_id = action_queue.submit(intent2)","    assert intent2_id == intent2.intent_id","    assert action_queue.get_queue_size() == 1  # Queue size shouldn't increase","    ","    # Check that second intent is marked as duplicate","    stored_intent2 = action_queue.get_intent(intent2_id)","    assert stored_intent2 is not None","    assert stored_intent2.status == IntentStatus.DUPLICATE","    ","    # Check metrics","    metrics = action_queue.get_metrics()","    assert metrics[\"submitted\"] == 1","    assert metrics[\"duplicate_rejected\"] == 1","","","def test_idempotency_different_params(action_queue, sample_data_spec):","    \"\"\"Test that intents with different parameters are not duplicates.\"\"\"","    # Create first intent","    intent1 = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10, \"window_slow\": 30}","    )","    ","    # Create second intent with different parameters","    intent2 = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 5, \"window_slow\": 20}  # Different params","    )","    ","    # Both should be accepted","    intent1_id = action_queue.submit(intent1)","    intent2_id = action_queue.submit(intent2)","    ","    assert intent1_id != intent2_id","    assert action_queue.get_queue_size() == 2","    ","    # Check metrics","    metrics = action_queue.get_metrics()","    assert metrics[\"submitted\"] == 2","    assert metrics[\"duplicate_rejected\"] == 0","","","def test_idempotency_calculate_units(action_queue, sample_data_spec):","    \"\"\"Test idempotency for CalculateUnitsIntent.\"\"\"","    # Create first calculation intent","    intent1 = CalculateUnitsIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    # Create duplicate calculation intent","    intent2 = CalculateUnitsIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    # Submit both","    action_queue.submit(intent1)","    action_queue.submit(intent2)","    ","    # Only one should be in queue","    assert action_queue.get_queue_size() == 1","    ","    metrics = action_queue.get_metrics()","    assert metrics[\"duplicate_rejected\"] == 1","","","def test_idempotency_manual_key(action_queue, sample_data_spec):","    \"\"\"Test idempotency with manually set idempotency_key.\"\"\"","    # Create intents with same manual idempotency_key","    intent1 = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10},","        idempotency_key=\"manual_key_123\"","    )","    ","    intent2 = CreateJobIntent(","        season=\"2024Q2\",  # Different season","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10},","        idempotency_key=\"manual_key_123\"  # Same key","    )","    ","    # Second should be duplicate despite different parameters","    action_queue.submit(intent1)","    action_queue.submit(intent2)","    ","    assert action_queue.get_queue_size() == 1","    metrics = action_queue.get_metrics()","    assert metrics[\"duplicate_rejected\"] == 1","","","def test_queue_full_rejection(action_queue, sample_data_spec):","    \"\"\"Test that queue rejects intents when full.\"\"\"","    # Fill the queue","    for i in range(10):  # max_size is 10","        intent = CreateJobIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        action_queue.submit(intent)","    ","    assert action_queue.get_queue_size() == 10","    ","    # Try to submit one more (should fail)","    extra_intent = CreateJobIntent(","        season=\"2024Q99\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 99}","    )","    ","    with pytest.raises(ValueError, match=\"ActionQueue is full\"):","        action_queue.submit(extra_intent)","    "]}
{"type":"file_chunk","path":"tests/test_intent_idempotency.py","chunk_index":1,"line_start":201,"line_end":330,"content":["    metrics = action_queue.get_metrics()","    assert metrics[\"queue_full_rejected\"] == 1","","","def test_intent_retrieval(action_queue, sample_data_spec):","    \"\"\"Test retrieving intents by ID.\"\"\"","    intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    intent_id = action_queue.submit(intent)","    ","    # Retrieve intent","    retrieved = action_queue.get_intent(intent_id)","    assert retrieved is not None","    assert retrieved.intent_id == intent_id","    assert retrieved.season == \"2024Q1\"","    assert retrieved.status == IntentStatus.PENDING","    ","    # Try to retrieve non-existent intent","    assert action_queue.get_intent(\"non_existent_id\") is None","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_wait_for_intent(action_queue, sample_data_spec):","    \"\"\"Test waiting for intent completion.\"\"\"","    intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    intent_id = action_queue.submit(intent)","    ","    # Mark as completed in background","    async def mark_completed():","        await asyncio.sleep(0.1)","        action_queue.mark_completed(intent_id, {\"result\": \"success\"})","    ","    # Wait for completion","    task = asyncio.create_task(mark_completed())","    completed = await action_queue.wait_for_intent_async(intent_id, timeout=1.0)","    ","    await task","    ","    assert completed is not None","    assert completed.status == IntentStatus.COMPLETED","    assert completed.result == {\"result\": \"success\"}","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_wait_for_intent_timeout(action_queue, sample_data_spec):","    \"\"\"Test timeout when waiting for intent.\"\"\"","    intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    intent_id = action_queue.submit(intent)","    ","    # Wait with short timeout (intent won't be completed)","    completed = await action_queue.wait_for_intent_async(intent_id, timeout=0.1)","    ","    assert completed is None  # Should timeout","","","def test_queue_state_debugging(action_queue, sample_data_spec):","    \"\"\"Test queue state debugging method.\"\"\"","    # Add some intents","    for i in range(3):","        intent = CreateJobIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        action_queue.submit(intent)","    ","    # Get queue state","    state = action_queue.get_queue_state()","    ","    assert len(state) == 3","    for i, item in enumerate(state):","        assert \"intent_id\" in item","        assert item[\"type\"] == \"create_job\"","        assert item[\"status\"] == \"pending\"","","","def test_clear_queue(action_queue, sample_data_spec):","    \"\"\"Test clearing the queue.\"\"\"","    # Add some intents","    for i in range(5):","        intent = CreateJobIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        action_queue.submit(intent)","    ","    assert action_queue.get_queue_size() == 5","    ","    # Clear queue","    action_queue.clear()","    ","    assert action_queue.get_queue_size() == 0","    assert action_queue.get_metrics()[\"submitted\"] == 0","    ","    # Should be able to submit new intents after clear","    new_intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    action_queue.submit(new_intent)","    assert action_queue.get_queue_size() == 1"]}
{"type":"file_footer","path":"tests/test_intent_idempotency.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_jobs_db_concurrency_smoke.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1777,"sha256":"2829edcb2d733126083d2c8a57e1746662c2c831e2cf4a6861c30b8f5fcaa656","total_lines":67,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_jobs_db_concurrency_smoke.py","chunk_index":0,"line_start":1,"line_end":67,"content":["","\"\"\"Smoke test for jobs_db concurrency (WAL + retry + state machine).\"\"\"","","from __future__ import annotations","","import multiprocessing as mp","from pathlib import Path","","import pytest","","from control.jobs_db import (","    append_log,","    create_job,","    init_db,","    list_jobs,","    mark_done,","    mark_running,",")","from control.types import DBJobSpec","","","def _proc(db_path: str, n: int) -> None:","    \"\"\"Worker process: create n jobs and complete them.\"\"\"","    p = Path(db_path)","    for i in range(n):","        spec = DBJobSpec(","            season=\"test\",","            dataset_id=\"test\",","            outputs_root=\"outputs\",","            config_snapshot={\"test\": i},","            config_hash=f\"hash{i}\",","        )","        job_id = create_job(p, spec)","        mark_running(p, job_id, pid=1000 + i)","        append_log(p, job_id, f\"hi {i}\")","        mark_done(p, job_id, run_id=f\"R{i}\", report_link=f\"/b5?i={i}\")","","","@pytest.mark.parametrize(\"n\", [50])","def test_jobs_db_concurrency_smoke(tmp_path: Path, n: int) -> None:","    \"\"\"","    Test concurrent job creation and completion across multiple processes.","    ","    This test ensures WAL mode, retry logic, and state machine work correctly","    under concurrent access.","    \"\"\"","    db = tmp_path / \"jobs.db\"","    init_db(db)","","    ps = [mp.Process(target=_proc, args=(str(db), n)) for _ in range(2)]","    for p in ps:","        p.start()","    for p in ps:","        p.join()","","    for p in ps:","        assert p.exitcode == 0, f\"Process {p.pid} exited with code {p.exitcode}\"","","    # Verify job count","    jobs = list_jobs(db, limit=1000)","    assert len(jobs) == 2 * n, f\"Expected {2 * n} jobs, got {len(jobs)}\"","","    # Verify all jobs are DONE","    for job in jobs:","        assert job.status.value == \"DONE\", f\"Job {job.job_id} status is {job.status}, expected DONE\"","",""]}
{"type":"file_footer","path":"tests/test_jobs_db_concurrency_smoke.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_jobs_db_concurrency_wal.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1665,"sha256":"52064c6fa42004d248130567ee6102e760787336d73384b14ca0ed963fa59622","total_lines":58,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_jobs_db_concurrency_wal.py","chunk_index":0,"line_start":1,"line_end":58,"content":["","\"\"\"Tests for jobs_db concurrency with WAL mode.","","Tests concurrent writes from multiple processes to ensure no database locked errors.","\"\"\"","","from __future__ import annotations","","import multiprocessing as mp","from pathlib import Path","","import pytest","","import os","","from control.jobs_db import append_log, create_job, init_db, mark_done, update_running","from control.types import DBJobSpec","","","def _worker(db_path: str, n: int) -> None:","    \"\"\"Worker function: create job, append log, mark done.\"\"\"","    p = Path(db_path)","    pid = os.getpid()","    for i in range(n):","        spec = DBJobSpec(","            season=\"2026Q1\",","            dataset_id=\"test_dataset\",","            outputs_root=\"/tmp/outputs\",","            config_snapshot={\"test\": f\"config_{i}\"},","            config_hash=f\"hash_{i}\",","        )","        job_id = create_job(p, spec, tags=[\"test\", f\"worker_{i}\"])","        append_log(p, job_id, f\"hello {i}\")","        update_running(p, job_id, pid=pid)  # ✅ 對齊狀態機：QUEUED → RUNNING","        mark_done(p, job_id, run_id=f\"R_{i}\", report_link=f\"/b5?x=y&i={i}\")","","","@pytest.mark.parametrize(\"n\", [50])","def test_jobs_db_concurrent_writes(tmp_path: Path, n: int) -> None:","    \"\"\"","    Test concurrent writes from multiple processes.","    ","    Two processes each create n jobs, append logs, and mark done.","    Should not raise database locked errors.","    \"\"\"","    db = tmp_path / \"jobs.db\"","    init_db(db)","","    procs = [mp.Process(target=_worker, args=(str(db), n)) for _ in range(2)]","    for pr in procs:","        pr.start()","    for pr in procs:","        pr.join()","","    for pr in procs:","        assert pr.exitcode == 0, f\"Process {pr.pid} exited with code {pr.exitcode}\"","",""]}
{"type":"file_footer","path":"tests/test_jobs_db_concurrency_wal.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_jobs_db_tags.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5508,"sha256":"10f5adee9f14985177b601943a5a7c02c5371679691c7209c58950167037e40a","total_lines":199,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_jobs_db_tags.py","chunk_index":0,"line_start":1,"line_end":199,"content":["","\"\"\"Tests for jobs_db tags functionality.","","Tests:","1. Create job with tags","2. Read job with tags","3. Old rows without tags fallback to []","4. search_by_tag query helper","\"\"\"","","from __future__ import annotations","","import tempfile","from pathlib import Path","","import pytest","","from control.jobs_db import (","    create_job,","    get_job,","    init_db,","    list_jobs,","    search_by_tag,",")","from control.types import DBJobSpec","","","@pytest.fixture","def temp_db(tmp_path: Path) -> Path:","    \"\"\"Create temporary database for testing.\"\"\"","    db_path = tmp_path / \"test_jobs.db\"","    init_db(db_path)","    return db_path","","","def test_create_job_with_tags(temp_db: Path) -> None:","    \"\"\"Test creating a job with tags.\"\"\"","    spec = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config\"},","        config_hash=\"abc123\",","    )","    ","    job_id = create_job(temp_db, spec, tags=[\"production\", \"high-priority\"])","    ","    # Read back and verify tags","    record = get_job(temp_db, job_id)","    assert record.tags == [\"production\", \"high-priority\"]","","","def test_create_job_without_tags(temp_db: Path) -> None:","    \"\"\"Test creating a job without tags (defaults to empty list).\"\"\"","    spec = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config\"},","        config_hash=\"abc123\",","    )","    ","    job_id = create_job(temp_db, spec)","    ","    # Read back and verify tags is empty list","    record = get_job(temp_db, job_id)","    assert record.tags == []","","","def test_read_job_with_tags(temp_db: Path) -> None:","    \"\"\"Test reading a job with tags.\"\"\"","    spec = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config\"},","        config_hash=\"abc123\",","    )","    ","    job_id = create_job(temp_db, spec, tags=[\"test\", \"debug\"])","    ","    # Read back","    record = get_job(temp_db, job_id)","    assert isinstance(record.tags, list)","    assert \"test\" in record.tags","    assert \"debug\" in record.tags","    assert len(record.tags) == 2","","","def test_old_rows_fallback_to_empty_tags(temp_db: Path) -> None:","    \"\"\"","    Test that old rows without tags_json fallback to empty list.","    ","    This tests backward compatibility: existing jobs without tags_json","    should be readable and have tags=[].","    \"\"\"","    import sqlite3","    import json","    ","    # Manually insert a job without tags_json (simulating old schema)","    conn = sqlite3.connect(str(temp_db))","    try:","        # Insert job with old schema (no tags_json)","        conn.execute(\"\"\"","            INSERT INTO jobs (","                job_id, status, created_at, updated_at,","                season, dataset_id, outputs_root, config_hash,","                config_snapshot_json, requested_pause","            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)","        \"\"\", (","            \"old-job-123\",","            \"QUEUED\",","            \"2026-01-01T00:00:00Z\",","            \"2026-01-01T00:00:00Z\",","            \"2026Q1\",","            \"test_dataset\",","            \"/tmp/outputs\",","            \"abc123\",","            json.dumps({\"test\": \"config\"}),","            0,","        ))","        conn.commit()","    finally:","        conn.close()","    ","    # Read back - should have tags=[]","    record = get_job(temp_db, \"old-job-123\")","    assert record.tags == []","","","def test_search_by_tag(temp_db: Path) -> None:","    \"\"\"Test search_by_tag query helper.\"\"\"","    spec1 = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config1\"},","        config_hash=\"abc123\",","    )","    spec2 = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config2\"},","        config_hash=\"def456\",","    )","    spec3 = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config3\"},","        config_hash=\"ghi789\",","    )","    ","    # Create jobs with different tags","    job1 = create_job(temp_db, spec1, tags=[\"production\", \"high-priority\"])","    job2 = create_job(temp_db, spec2, tags=[\"staging\", \"low-priority\"])","    job3 = create_job(temp_db, spec3, tags=[\"production\", \"medium-priority\"])","    ","    # Search for \"production\" tag","    results = search_by_tag(temp_db, \"production\")","    assert len(results) == 2","    job_ids = {r.job_id for r in results}","    assert job1 in job_ids","    assert job3 in job_ids","    assert job2 not in job_ids","    ","    # Search for \"staging\" tag","    results = search_by_tag(temp_db, \"staging\")","    assert len(results) == 1","    assert results[0].job_id == job2","    ","    # Search for non-existent tag","    results = search_by_tag(temp_db, \"non-existent\")","    assert len(results) == 0","","","def test_list_jobs_includes_tags(temp_db: Path) -> None:","    \"\"\"Test that list_jobs includes tags in records.\"\"\"","    spec = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config\"},","        config_hash=\"abc123\",","    )","    ","    job_id = create_job(temp_db, spec, tags=[\"test\", \"debug\"])","    ","    # List jobs","    jobs = list_jobs(temp_db, limit=10)","    assert len(jobs) >= 1","    ","    # Find our job","    our_job = next((j for j in jobs if j.job_id == job_id), None)","    assert our_job is not None","    assert our_job.tags == [\"test\", \"debug\"]","",""]}
{"type":"file_footer","path":"tests/test_jobs_db_tags.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_json_pointer.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5589,"sha256":"99c78ac713b4347a47b2b6df40d5a3f0e710c5092552cbbbb86869351841bfff","total_lines":221,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_json_pointer.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Tests for JSON Pointer resolver.","","Tests normal pointer, list index, missing keys, and never-raise contract.","\"\"\"","","from __future__ import annotations","","import pytest","","from gui.viewer.json_pointer import resolve_json_pointer","","","def test_normal_pointer() -> None:","    \"\"\"Test normal object key pointer.\"\"\"","    data = {","        \"a\": {","            \"b\": {","                \"c\": \"value\"","            }","        }","    }","    ","    found, value = resolve_json_pointer(data, \"/a/b/c\")","    assert found is True","    assert value == \"value\"","    ","    found, value = resolve_json_pointer(data, \"/a/b\")","    assert found is True","    assert value == {\"c\": \"value\"}","","","def test_list_index() -> None:","    \"\"\"Test list index in pointer.\"\"\"","    data = {","        \"items\": [","            {\"name\": \"first\"},","            {\"name\": \"second\"},","        ]","    }","    ","    found, value = resolve_json_pointer(data, \"/items/0/name\")","    assert found is True","    assert value == \"first\"","    ","    found, value = resolve_json_pointer(data, \"/items/1/name\")","    assert found is True","    assert value == \"second\"","    ","    found, value = resolve_json_pointer(data, \"/items/0\")","    assert found is True","    assert value == {\"name\": \"first\"}","","","def test_list_index_out_of_bounds() -> None:","    \"\"\"Test list index out of bounds.\"\"\"","    data = {","        \"items\": [1, 2, 3]","    }","    ","    found, value = resolve_json_pointer(data, \"/items/10\")","    assert found is False","    assert value is None","    ","    found, value = resolve_json_pointer(data, \"/items/-1\")","    assert found is False","    assert value is None","","","def test_missing_key() -> None:","    \"\"\"Test missing key in pointer.\"\"\"","    data = {","        \"a\": {","            \"b\": \"value\"","        }","    }","    ","    found, value = resolve_json_pointer(data, \"/a/c\")","    assert found is False","    assert value is None","    ","    found, value = resolve_json_pointer(data, \"/x/y\")","    assert found is False","    assert value is None","","","def test_root_pointer_disabled() -> None:","    \"\"\"Test root pointer is disabled (by design for Viewer UX).\"\"\"","    data = {\"a\": 1, \"b\": 2}","    ","    # Root pointer \"/\" is intentionally disabled","    found, value = resolve_json_pointer(data, \"/\")","    assert found is False","    assert value is None","    ","    # Empty string is also disabled","    found, value = resolve_json_pointer(data, \"\")","    assert found is False","    assert value is None","","","def test_invalid_pointer_format() -> None:","    \"\"\"Test invalid pointer format.\"\"\"","    data = {\"a\": 1}","    ","    # Missing leading slash","    found, value = resolve_json_pointer(data, \"a/b\")","    assert found is False","    assert value is None","","","def test_nested_list_and_dict() -> None:","    \"\"\"Test nested list and dict combination.\"\"\"","    data = {","        \"results\": [","            {","                \"metrics\": {","                    \"score\": 100","                }","            },","            {","                \"metrics\": {","                    \"score\": 200","                }","            }","        ]","    }","    ","    found, value = resolve_json_pointer(data, \"/results/0/metrics/score\")","    assert found is True","    assert value == 100","    ","    found, value = resolve_json_pointer(data, \"/results/1/metrics/score\")","    assert found is True","    assert value == 200","","","def test_never_raises() -> None:","    \"\"\"Test that resolve_json_pointer never raises exceptions.\"\"\"","    # Test with None data","    found, value = resolve_json_pointer(None, \"/a\")  # type: ignore","    assert found is False","    assert value is None","    ","    # Test with invalid data types","    found, value = resolve_json_pointer(\"string\", \"/a\")  # type: ignore","    assert found is False","    assert value is None","    ","    # Test with empty dict (valid, but key missing)","    found, value = resolve_json_pointer({}, \"/a\")","    assert found is False","    assert value is None","    ","    # Test with invalid pointer type","    found, value = resolve_json_pointer({\"a\": 1}, None)  # type: ignore","    assert found is False","    assert value is None","    ","    # Test with empty string pointer","    found, value = resolve_json_pointer({\"a\": 1}, \"\")","    assert found is False","    assert value is None","    ","    # Test with root pointer (disabled)","    found, value = resolve_json_pointer({\"a\": 1}, \"/\")","    assert found is False","    assert value is None","    ","    # Test with valid pointer","    found, value = resolve_json_pointer({\"a\": 1}, \"/a\")","    assert found is True","    assert value == 1","","","def test_critical_scenarios() -> None:","    \"\"\"Test critical scenarios that must pass.\"\"\"","    data = {\"a\": 1}","    ","    # Scenario 1: None pointer","    found, value = resolve_json_pointer(data, None)  # type: ignore","    assert found is False","    assert value is None","    ","    # Scenario 2: Empty string pointer","    found, value = resolve_json_pointer(data, \"\")","    assert found is False","    assert value is None","    ","    # Scenario 3: Root pointer (disabled by design)","    found, value = resolve_json_pointer(data, \"/\")","    assert found is False","    assert value is None","    ","    # Scenario 4: Valid pointer","    found, value = resolve_json_pointer(data, \"/a\")","    assert found is True","    assert value == 1","",""]}
{"type":"file_chunk","path":"tests/test_json_pointer.py","chunk_index":1,"line_start":201,"line_end":221,"content":["def test_intermediate_type_mismatch() -> None:","    \"\"\"Test intermediate type mismatch.\"\"\"","    data = {","        \"items\": \"not_a_list\"","    }","    ","    # Try to access list index on string","    found, value = resolve_json_pointer(data, \"/items/0\")","    assert found is False","    assert value is None","    ","    data = {","        \"items\": [1, 2, 3]","    }","    ","    # Try to access dict key on list","    found, value = resolve_json_pointer(data, \"/items/key\")","    assert found is False","    assert value is None","",""]}
{"type":"file_footer","path":"tests/test_json_pointer.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_kbar_anchor_alignment.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3158,"sha256":"fd386f23b19272dca49a1ea91a1514055549d9f0a3821fddadcdb3085c96dcbe","total_lines":86,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_kbar_anchor_alignment.py","chunk_index":0,"line_start":1,"line_end":86,"content":["","\"\"\"Test K-bar aggregation: anchor alignment to Session.start.\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pandas as pd","import pytest","","from data.session.kbar import aggregate_kbar","from data.session.loader import load_session_profile","","","@pytest.fixture","def mnq_profile(profiles_root: Path) -> Path:","    \"\"\"Load CME.MNQ session profile.\"\"\"","    profile_path = profiles_root / \"CME_MNQ_TPE_v1.yaml\"","    return profile_path","","","def test_anchor_to_session_start_60m(mnq_profile: Path) -> None:","    \"\"\"Test 60-minute bars are anchored to session start (08:45:00).\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Create bars starting from session start","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/1/1 08:45:00\",  # Session start","            \"2013/1/1 08:50:00\",","            \"2013/1/1 09:00:00\",","            \"2013/1/1 09:30:00\",","            \"2013/1/1 09:45:00\",  # Should be start of next 60m bucket","            \"2013/1/1 10:00:00\",","        ],","        \"open\": [100.0, 101.0, 102.0, 103.0, 104.0, 105.0],","        \"high\": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5],","        \"low\": [99.5, 100.5, 101.5, 102.5, 103.5, 104.5],","        \"close\": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5],","        \"volume\": [1000, 1100, 1200, 1300, 1400, 1500],","    })","    ","    result = aggregate_kbar(df, 60, profile)","    ","    # Verify first bar is anchored to session start","    first_bar_time = result[\"ts_str\"].iloc[0].split(\" \")[1]","    assert first_bar_time == \"08:45:00\", f\"First bar should be anchored to 08:45:00, got {first_bar_time}\"","    ","    # Verify subsequent bars are at 60-minute intervals from start","    if len(result) > 1:","        second_bar_time = result[\"ts_str\"].iloc[1].split(\" \")[1]","        assert second_bar_time == \"09:45:00\", f\"Second bar should be at 09:45:00, got {second_bar_time}\"","","","def test_anchor_to_session_start_30m(mnq_profile: Path) -> None:","    \"\"\"Test 30-minute bars are anchored to session start (08:45:00).\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Create bars starting from session start","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/1/1 08:45:00\",  # Session start","            \"2013/1/1 08:50:00\",","            \"2013/1/1 09:00:00\",","            \"2013/1/1 09:15:00\",  # Should be start of next 30m bucket","            \"2013/1/1 09:30:00\",","        ],","        \"open\": [100.0, 101.0, 102.0, 103.0, 104.0],","        \"high\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"low\": [99.5, 100.5, 101.5, 102.5, 103.5],","        \"close\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"volume\": [1000, 1100, 1200, 1300, 1400],","    })","    ","    result = aggregate_kbar(df, 30, profile)","    ","    # Verify first bar is anchored to session start","    first_bar_time = result[\"ts_str\"].iloc[0].split(\" \")[1]","    assert first_bar_time == \"08:45:00\", f\"First bar should be anchored to 08:45:00, got {first_bar_time}\"","    ","    # Verify subsequent bars are at 30-minute intervals from start","    if len(result) > 1:","        second_bar_time = result[\"ts_str\"].iloc[1].split(\" \")[1]","        assert second_bar_time == \"09:15:00\", f\"Second bar should be at 09:15:00, got {second_bar_time}\"","",""]}
{"type":"file_footer","path":"tests/test_kbar_anchor_alignment.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_kbar_no_cross_session.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3116,"sha256":"89016a7006feeb798b281c7b3a40ed87b3eb0152c1c06b2bf13ea0095c7b1e07","total_lines":88,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_kbar_no_cross_session.py","chunk_index":0,"line_start":1,"line_end":88,"content":["","\"\"\"Test K-bar aggregation: no cross-session aggregation.\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pandas as pd","import pytest","","from data.session.kbar import aggregate_kbar","from data.session.loader import load_session_profile","","","@pytest.fixture","def mnq_profile(profiles_root: Path) -> Path:","    \"\"\"Load CME.MNQ session profile.\"\"\"","    profile_path = profiles_root / \"CME_MNQ_TPE_v1.yaml\"","    return profile_path","","","def test_no_cross_session_60m(mnq_profile: Path) -> None:","    \"\"\"Test 60-minute bars do not cross session boundaries.\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Create bars that span DAY session end and NIGHT session start","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/1/1 13:30:00\",  # DAY session","            \"2013/1/1 13:40:00\",  # DAY session","            \"2013/1/1 13:44:00\",  # DAY session (last bar before end)","            \"2013/1/1 21:00:00\",  # NIGHT session start","            \"2013/1/1 21:10:00\",  # NIGHT session","        ],","        \"open\": [100.0, 101.0, 102.0, 103.0, 104.0],","        \"high\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"low\": [99.5, 100.5, 101.5, 102.5, 103.5],","        \"close\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"volume\": [1000, 1100, 1200, 1300, 1400],","    })","    ","    result = aggregate_kbar(df, 60, profile)","    ","    # Verify no bar contains both DAY and NIGHT session bars","    # Use session column instead of string contains (more robust)","    assert \"session\" in result.columns, \"Result must include session column\"","    ","    # Must have both DAY and NIGHT sessions","    assert set(result[\"session\"].dropna()) == {\"DAY\", \"NIGHT\"}, (","        f\"Should have both DAY and NIGHT sessions, got {set(result['session'].dropna())}\"","    )","    ","    day_bars = result[result[\"session\"] == \"DAY\"]","    night_bars = result[result[\"session\"] == \"NIGHT\"]","    ","    assert len(day_bars) > 0, \"Should have DAY session bars\"","    assert len(night_bars) > 0, \"Should have NIGHT session bars\"","    ","    # Verify no bar mixes sessions (each row has exactly one session)","    assert result[\"session\"].notna().all(), \"All bars must have a session label\"","    assert len(result[result[\"session\"].isna()]) == 0, \"No bar should have session=None\"","","","def test_no_cross_session_30m(mnq_profile: Path) -> None:","    \"\"\"Test 30-minute bars do not cross session boundaries.\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Create bars at DAY session end","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/1/1 13:30:00\",","            \"2013/1/1 13:40:00\",","            \"2013/1/1 13:44:00\",  # Last bar in DAY session","        ],","        \"open\": [100.0, 101.0, 102.0],","        \"high\": [100.5, 101.5, 102.5],","        \"low\": [99.5, 100.5, 101.5],","        \"close\": [100.5, 101.5, 102.5],","        \"volume\": [1000, 1100, 1200],","    })","    ","    result = aggregate_kbar(df, 30, profile)","    ","    # All bars should be in DAY session","    assert \"session\" in result.columns, \"Result must include session column\"","    assert all(result[\"session\"] == \"DAY\"), f\"All bars should be DAY session, got {result['session'].unique()}\"","",""]}
{"type":"file_footer","path":"tests/test_kbar_no_cross_session.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_kernel_parity_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":17243,"sha256":"b767856f5b5f8768cc1ad1713cce3759c68c51ccd2e7392be848a22b23c022c3","total_lines":413,"chunk_count":3}
{"type":"file_chunk","path":"tests/test_kernel_parity_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Kernel parity contract tests - Phase 4 Stage C.","","These tests ensure that Cursor kernel results are bit-level identical to matcher_core.","This is a critical contract: any deviation indicates a semantic bug.","","Tests use simulate_run() unified entry point to ensure we test the actual API used in production.","\"\"\"","","import numpy as np","","from data.layout import normalize_bars","from engine.simulate import simulate_run","from engine.types import OrderIntent, OrderKind, OrderRole, Side","","","def _bars1(o, h, l, c):","    \"\"\"Helper to create single-bar BarArrays.\"\"\"","    return normalize_bars(","        np.array([o], dtype=np.float64),","        np.array([h], dtype=np.float64),","        np.array([l], dtype=np.float64),","        np.array([c], dtype=np.float64),","    )","","","def _bars2(o0, h0, l0, c0, o1, h1, l1, c1):","    \"\"\"Helper to create two-bar BarArrays.\"\"\"","    return normalize_bars(","        np.array([o0, o1], dtype=np.float64),","        np.array([h0, h1], dtype=np.float64),","        np.array([l0, l1], dtype=np.float64),","        np.array([c0, c1], dtype=np.float64),","    )","","","def _bars3(o0, h0, l0, c0, o1, h1, l1, c1, o2, h2, l2, c2):","    \"\"\"Helper to create three-bar BarArrays.\"\"\"","    return normalize_bars(","        np.array([o0, o1, o2], dtype=np.float64),","        np.array([h0, h1, h2], dtype=np.float64),","        np.array([l0, l1, l2], dtype=np.float64),","        np.array([c0, c1, c2], dtype=np.float64),","    )","","","def _compute_position_path(fills):","    \"\"\"","    Compute position path from fills sequence.","    ","    Returns list of (bar_index, position) tuples where position is:","    - 0: flat","    - 1: long","    - -1: short","    \"\"\"","    pos_path = []","    current_pos = 0","    ","    # Group fills by bar_index","    fills_by_bar = {}","    for fill in fills:","        bar_idx = fill.bar_index","        if bar_idx not in fills_by_bar:","            fills_by_bar[bar_idx] = []","        fills_by_bar[bar_idx].append(fill)","    ","    # Process fills chronologically","    for bar_idx in sorted(fills_by_bar.keys()):","        bar_fills = fills_by_bar[bar_idx]","        # Sort by role (ENTRY first), then kind, then order_id","        bar_fills.sort(key=lambda f: (","            0 if f.role == OrderRole.ENTRY else 1,","            0 if f.kind == OrderKind.STOP else 1,","            f.order_id","        ))","        ","        for fill in bar_fills:","            if fill.role == OrderRole.ENTRY:","                if fill.side == Side.BUY:","                    current_pos = 1","                else:","                    current_pos = -1","            elif fill.role == OrderRole.EXIT:","                current_pos = 0","        ","        pos_path.append((bar_idx, current_pos))","    ","    return pos_path","","","def _assert_fills_identical(cursor_fills, reference_fills):","    \"\"\"Assert that two fill sequences are bit-level identical.\"\"\"","    assert len(cursor_fills) == len(reference_fills), (","        f\"Fill count mismatch: cursor={len(cursor_fills)}, reference={len(reference_fills)}\"","    )","    ","    for i, (c_fill, r_fill) in enumerate(zip(cursor_fills, reference_fills)):","        assert c_fill.bar_index == r_fill.bar_index, (","            f\"Fill {i}: bar_index mismatch: cursor={c_fill.bar_index}, reference={r_fill.bar_index}\"","        )","        assert c_fill.role == r_fill.role, (","            f\"Fill {i}: role mismatch: cursor={c_fill.role}, reference={r_fill.role}\"","        )","        assert c_fill.kind == r_fill.kind, (","            f\"Fill {i}: kind mismatch: cursor={c_fill.kind}, reference={r_fill.kind}\"","        )","        assert c_fill.side == r_fill.side, (","            f\"Fill {i}: side mismatch: cursor={c_fill.side}, reference={r_fill.side}\"","        )","        assert c_fill.price == r_fill.price, (","            f\"Fill {i}: price mismatch: cursor={c_fill.price}, reference={r_fill.price}\"","        )","        assert c_fill.qty == r_fill.qty, (","            f\"Fill {i}: qty mismatch: cursor={c_fill.qty}, reference={r_fill.qty}\"","        )","        assert c_fill.order_id == r_fill.order_id, (","            f\"Fill {i}: order_id mismatch: cursor={c_fill.order_id}, reference={r_fill.order_id}\"","        )","","","def _assert_position_path_identical(cursor_fills, reference_fills):","    \"\"\"Assert that position paths are identical.\"\"\"","    cursor_path = _compute_position_path(cursor_fills)","    reference_path = _compute_position_path(reference_fills)","    ","    assert cursor_path == reference_path, (","        f\"Position path mismatch:\\n\"","        f\"  cursor: {cursor_path}\\n\"","        f\"  reference: {reference_path}\"","    )","","","def test_parity_next_bar_activation():","    \"\"\"Test next-bar activation rule: order created at bar N activates at bar N+1.\"\"\"","    # Order created at bar 0, should activate at bar 1","    bars = _bars2(","        100, 105, 95, 100,  # bar 0: high=105 would hit stop 102, but order not active yet","        100, 105, 95, 100,  # bar 1: order activates, should fill","    )","    intents = [","        OrderIntent(order_id=1, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    _assert_position_path_identical(cursor_result.fills, reference_result.fills)","    ","    # Verify: should fill at bar 1, not bar 0","    assert len(cursor_result.fills) == 1","    assert cursor_result.fills[0].bar_index == 1","","","def test_parity_stop_fill_price_exact():","    \"\"\"Test stop fill price = stop_price (not max(open, stop_price)).\"\"\"","    # Buy stop at 100, open=95, high=105 -> should fill at 100 (stop_price), not 105","    bars = _bars1(95, 105, 90, 100)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].price == 100.0  # stop_price, not high","","","def test_parity_stop_fill_price_gap_up():","    \"\"\"Test stop fill price on gap up: fill at open if open >= stop_price.\"\"\"","    # Buy stop at 100, open=105 (gap up) -> should fill at 105 (open), not 100","    bars = _bars1(105, 110, 105, 108)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].price == 105.0  # open (gap branch)","","","def test_parity_stop_fill_price_gap_down():","    \"\"\"Test stop fill price on gap down: fill at open if open <= stop_price.\"\"\"","    # Sell stop at 100, open=90 (gap down) -> should fill at 90 (open), not 100","    bars = _bars2(","        100, 100, 100, 100,  # bar 0: enter long","        90, 95, 80, 85,      # bar 1: exit stop gap down","    )","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point"]}
{"type":"file_chunk","path":"tests/test_kernel_parity_contract.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    _assert_position_path_identical(cursor_result.fills, reference_result.fills)","    # Exit fill should be at open (90) due to gap down","    assert cursor_result.fills[1].price == 90.0","","","def test_parity_same_bar_entry_then_exit():","    \"\"\"Test same-bar entry then exit is allowed.\"\"\"","    # Same bar: entry buy stop 105, exit sell stop 95","    # Bar: O=100 H=120 L=90","    # Entry: Buy Stop 105 -> fills at 105","    # Exit: Sell Stop 95 -> fills at 95 (after entry)","    bars = _bars1(100, 120, 90, 110)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),","        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    _assert_position_path_identical(cursor_result.fills, reference_result.fills)","    ","    assert len(cursor_result.fills) == 2","    assert cursor_result.fills[0].role == OrderRole.ENTRY","    assert cursor_result.fills[0].price == 105.0","    assert cursor_result.fills[1].role == OrderRole.EXIT","    assert cursor_result.fills[1].price == 95.0","    assert cursor_result.fills[0].bar_index == cursor_result.fills[1].bar_index","","","def test_parity_stop_priority_over_limit():","    \"\"\"Test STOP priority over LIMIT (same role, same bar).\"\"\"","    # Entry: Buy Stop 102 and Buy Limit 110 both triggerable","    # STOP must win","    bars = _bars1(100, 115, 95, 105)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),","        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.BUY, price=110.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].kind == OrderKind.STOP","    assert cursor_result.fills[0].order_id == 1","","","def test_parity_stop_priority_exit():","    \"\"\"Test STOP priority over LIMIT on exit.\"\"\"","    # Enter long first, then exit with both stop and limit triggerable","    # STOP must win","    bars = _bars2(","        100, 100, 100, 100,  # bar 0: enter long","        100, 110, 80, 90,    # bar 1: exit stop 90 and exit limit 110 both triggerable","    )","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=90.0),","        OrderIntent(order_id=3, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.LIMIT, side=Side.SELL, price=110.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    _assert_position_path_identical(cursor_result.fills, reference_result.fills)","    ","    # Exit fill should be STOP","    assert cursor_result.fills[1].kind == OrderKind.STOP","    assert cursor_result.fills[1].order_id == 2","","","def test_parity_order_id_tie_break():","    \"\"\"Test order_id tie-break when kind is same.\"\"\"","    # Two STOP orders, lower order_id should win","    bars = _bars1(100, 110, 95, 105)","    intents = [","        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].order_id == 1  # Lower order_id wins","","","def test_parity_limit_gap_down_better_fill():","    \"\"\"Test limit order gap down: fill at open if better.\"\"\"","    # Buy limit at 100, open=90 (gap down) -> should fill at 90 (open), not 100","    bars = _bars1(90, 95, 85, 92)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.BUY, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].price == 90.0  # open (better fill)","","","def test_parity_limit_gap_up_better_fill():","    \"\"\"Test limit order gap up: fill at open if better.\"\"\"","    # Sell limit at 100, open=105 (gap up) -> should fill at 105 (open), not 100","    bars = _bars1(105, 110, 100, 108)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.SELL, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].price == 105.0  # open (better fill)","","","def test_parity_no_fill_when_not_touched():","    \"\"\"Test no fill when price not touched.\"\"\"","    bars = _bars1(90, 95, 90, 92)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert len(cursor_result.fills) == 0","","","def test_parity_open_equals_stop_gap_branch():","    \"\"\"Test open equals stop price: gap branch but same price.\"\"\"","    bars = _bars1(100, 100, 90, 95)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].price == 100.0  # open == stop_price","","","def test_parity_multiple_bars_complex():","    \"\"\"Test complex multi-bar scenario with entry and exit.\"\"\"","    bars = _bars3(","        100, 105, 95, 100,   # bar 0: enter long at 102 (buy stop)","        100, 110, 80, 90,    # bar 1: exit stop 90 triggers","        95, 100, 90, 95,     # bar 2: no fills","    )","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),","        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=90.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    _assert_position_path_identical(cursor_result.fills, reference_result.fills)","    ","    # Verify position path","    pos_path = _compute_position_path(cursor_result.fills)","    assert pos_path == [(0, 1), (1, 0)]  # Enter at bar 0, exit at bar 1","","","def test_parity_entry_skipped_when_position_exists():","    \"\"\"Test that entry is skipped when position already exists.\"\"\"","    # Enter long at bar 0, then at bar 1 try to enter again (should be skipped) and exit","    bars = _bars2(","        100, 100, 100, 100,  # bar 0: enter long","        100, 110, 90, 100,   # bar 1: exit stop 95 triggers, entry stop 105 also triggerable but skipped","    )","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),","        OrderIntent(order_id=3, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)"]}
{"type":"file_chunk","path":"tests/test_kernel_parity_contract.py","chunk_index":2,"line_start":401,"line_end":413,"content":["    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    _assert_position_path_identical(cursor_result.fills, reference_result.fills)","    ","    # Should have entry at bar 0 and exit at bar 1","    # Entry at bar 1 should be skipped (position already exists)","    assert len(cursor_result.fills) == 2","    assert cursor_result.fills[0].bar_index == 0","    assert cursor_result.fills[0].role == OrderRole.ENTRY","    assert cursor_result.fills[1].bar_index == 1","    assert cursor_result.fills[1].role == OrderRole.EXIT","",""]}
{"type":"file_footer","path":"tests/test_kernel_parity_contract.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"tests/test_kpi_drilldown_no_raise.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7910,"sha256":"3b7ddf9b73df30360fa1844525b2a52a245a6a5a9072b38fb8ce95b1a4630522","total_lines":240,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_kpi_drilldown_no_raise.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Tests for KPI drill-down - no raise contract.","","Tests missing artifacts, wrong pointers, empty session_state.","UI functions should never raise exceptions.","","Zero-side-effect imports: All I/O and stateful operations are inside test functions.","","NOTE: This test is skipped because streamlit has been removed from the project.","\"\"\"","","from __future__ import annotations","","import pytest","","pytest.skip(\"Streamlit tests skipped - streamlit removed from project\", allow_module_level=True)","","# Original test code below is not executed","","","def test_kpi_table_missing_name() -> None:","    \"\"\"Test KPI table handles missing name field.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.kpi_table import render_kpi_table","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.columns\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.text\"), \\","         patch(\"streamlit.button\"):","        ","        # Row without name","        kpi_rows = [","            {\"value\": 100}","        ]","        ","        # Should not raise","        render_kpi_table(kpi_rows)","","","def test_kpi_table_missing_value() -> None:","    \"\"\"Test KPI table handles missing value field.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.kpi_table import render_kpi_table","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.columns\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.text\"), \\","         patch(\"streamlit.button\"):","        ","        # Row without value","        kpi_rows = [","            {\"name\": \"net_profit\"}","        ]","        ","        # Should not raise","        render_kpi_table(kpi_rows)","","","def test_kpi_table_empty_rows() -> None:","    \"\"\"Test KPI table handles empty rows list.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.kpi_table import render_kpi_table","    ","    with patch(\"streamlit.info\"):","        # Empty list","        render_kpi_table([])","        ","        # Should not raise","","","def test_kpi_table_unknown_kpi() -> None:","    \"\"\"Test KPI table handles unknown KPI (not in registry).\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.kpi_table import render_kpi_table","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.columns\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.text\"), \\","         patch(\"streamlit.button\"):","        ","        # KPI not in registry","        kpi_rows = [","            {\"name\": \"unknown_kpi\", \"value\": 100}","        ]","        ","        # Should not raise - displays but not clickable","        render_kpi_table(kpi_rows)","","","def test_evidence_panel_missing_artifact() -> None:","    \"\"\"Test evidence panel handles missing artifact.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.evidence_panel import render_evidence_panel","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.warning\"), \\","         patch(\"streamlit.caption\"):","        ","        # Mock session state with missing artifact","        with patch.dict(st.session_state, {","            \"active_evidence\": {","                \"kpi_name\": \"net_profit\",","                \"artifact\": \"winners_v2\",","                \"json_pointer\": \"/summary/net_profit\",","            }","        }):","            # Artifacts dict missing winners_v2","            artifacts = {","                \"manifest\": {},","            }","            ","            # Should not raise - shows warning","            render_evidence_panel(artifacts)","","","def test_evidence_panel_wrong_pointer() -> None:","    \"\"\"Test evidence panel handles wrong JSON pointer.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.evidence_panel import render_evidence_panel","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.warning\"), \\","         patch(\"streamlit.info\"), \\","         patch(\"streamlit.caption\"):","        ","        # Mock session state","        with patch.dict(st.session_state, {","            \"active_evidence\": {","                \"kpi_name\": \"net_profit\",","                \"artifact\": \"winners_v2\",","                \"json_pointer\": \"/nonexistent/pointer\",","            }","        }):","            # Artifact exists but pointer is wrong","            artifacts = {","                \"winners_v2\": {","                    \"summary\": {","                        \"net_profit\": 100","                    }","                }","            }","            ","            # Should not raise - shows warning","            render_evidence_panel(artifacts)","","","def test_evidence_panel_empty_session_state() -> None:","    \"\"\"Test evidence panel handles empty session_state.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.evidence_panel import render_evidence_panel","    ","    with patch(\"streamlit.subheader\"):","        # Empty session state","        with patch.dict(st.session_state, {}, clear=True):","            artifacts = {","                \"winners_v2\": {}","            }","            ","            # Should not raise - returns early","            render_evidence_panel(artifacts)","","","def test_evidence_panel_invalid_session_state() -> None:","    \"\"\"Test evidence panel handles invalid session_state structure.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.evidence_panel import render_evidence_panel","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.warning\"):","        ","        # Invalid session state structure","        with patch.dict(st.session_state, {","            \"active_evidence\": \"not_a_dict\"","        }):","            artifacts = {}","            ","            # Should not raise - handles gracefully","            render_evidence_panel(artifacts)","","","def test_evidence_panel_missing_fields() -> None:","    \"\"\"Test evidence panel handles missing fields in session_state.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.evidence_panel import render_evidence_panel","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.warning\"):","        ","        # Missing fields in active_evidence","        with patch.dict(st.session_state, {","            \"active_evidence\": {","                \"kpi_name\": \"net_profit\",","                # Missing artifact, json_pointer"]}
{"type":"file_chunk","path":"tests/test_kpi_drilldown_no_raise.py","chunk_index":1,"line_start":201,"line_end":240,"content":["            }","        }):","            artifacts = {}","            ","            # Should not raise - handles gracefully","            render_evidence_panel(artifacts)","","","def test_kpi_table_exception_handling() -> None:","    \"\"\"Test KPI table handles exceptions gracefully.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.kpi_table import render_kpi_table","    ","    # Mock streamlit to raise exception","    with patch(\"streamlit.subheader\", side_effect=Exception(\"Streamlit error\")):","        kpi_rows = [","            {\"name\": \"net_profit\", \"value\": 100}","        ]","        ","        # Should catch exception and show error","        with patch(\"streamlit.error\"):","            render_kpi_table(kpi_rows)","            # Should not raise","","","def test_evidence_panel_exception_handling() -> None:","    \"\"\"Test evidence panel handles exceptions gracefully.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.evidence_panel import render_evidence_panel","    ","    # Mock streamlit to raise exception","    with patch(\"streamlit.subheader\", side_effect=Exception(\"Streamlit error\")):","        artifacts = {}","        ","        # Should catch exception and show error","        with patch(\"streamlit.error\"):","            render_evidence_panel(artifacts)","            # Should not raise","",""]}
{"type":"file_footer","path":"tests/test_kpi_drilldown_no_raise.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_kpi_registry.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2796,"sha256":"b7501e63df5f04cbcd2d7c31ddfdbce2115b3980d4cc1bc4c0ce349724f67a37","total_lines":96,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_kpi_registry.py","chunk_index":0,"line_start":1,"line_end":96,"content":["","\"\"\"Tests for KPI Registry.","","Tests registry key → EvidenceLink mapping and defensive behavior.","\"\"\"","","from __future__ import annotations","","import pytest","","from gui.viewer.kpi_registry import (","    KPI_EVIDENCE_REGISTRY,","    get_evidence_link,","    has_evidence,","    EvidenceLink,",")","","","def test_registry_keys_exist() -> None:","    \"\"\"Test that registry keys map to correct EvidenceLink.\"\"\"","    # Test net_profit","    link = get_evidence_link(\"net_profit\")","    assert link is not None","    assert link.artifact == \"winners_v2\"","    assert link.json_pointer == \"/summary/net_profit\"","    assert \"profit\" in link.description.lower()","    ","    # Test max_drawdown","    link = get_evidence_link(\"max_drawdown\")","    assert link is not None","    assert link.artifact == \"winners_v2\"","    assert link.json_pointer == \"/summary/max_drawdown\"","    ","    # Test num_trades","    link = get_evidence_link(\"num_trades\")","    assert link is not None","    assert link.artifact == \"winners_v2\"","    assert link.json_pointer == \"/summary/num_trades\"","    ","    # Test final_score","    link = get_evidence_link(\"final_score\")","    assert link is not None","    assert link.artifact == \"governance\"","    assert link.json_pointer == \"/scoring/final_score\"","","","def test_unknown_kpi_returns_none() -> None:","    \"\"\"Test that unknown KPI names return None without crashing.\"\"\"","    link = get_evidence_link(\"unknown_kpi\")","    assert link is None","    ","    link = get_evidence_link(\"\")","    assert link is None","    ","    link = get_evidence_link(\"nonexistent\")","    assert link is None","","","def test_has_evidence() -> None:","    \"\"\"Test has_evidence function.\"\"\"","    assert has_evidence(\"net_profit\") is True","    assert has_evidence(\"max_drawdown\") is True","    assert has_evidence(\"num_trades\") is True","    assert has_evidence(\"final_score\") is True","    ","    assert has_evidence(\"unknown_kpi\") is False","    assert has_evidence(\"\") is False","","","def test_registry_never_raises() -> None:","    \"\"\"Test that registry functions never raise exceptions.\"\"\"","    # Test with invalid input types","    try:","        get_evidence_link(None)  # type: ignore","    except Exception:","        pytest.fail(\"get_evidence_link should not raise\")","    ","    try:","        has_evidence(None)  # type: ignore","    except Exception:","        pytest.fail(\"has_evidence should not raise\")","","","def test_registry_structure() -> None:","    \"\"\"Test that registry has correct structure.\"\"\"","    assert isinstance(KPI_EVIDENCE_REGISTRY, dict)","    assert len(KPI_EVIDENCE_REGISTRY) > 0","    ","    for kpi_name, link in KPI_EVIDENCE_REGISTRY.items():","        assert isinstance(kpi_name, str)","        assert isinstance(link, EvidenceLink)","        assert link.artifact in (\"manifest\", \"winners_v2\", \"governance\")","        assert link.json_pointer.startswith(\"/\")","        assert isinstance(link.description, str)","",""]}
{"type":"file_footer","path":"tests/test_kpi_registry.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_local_scan_policy.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6885,"sha256":"31dda43f16b0a1898a747ec60caa2165fdea656952e2f002e33a20c92a3ff8e4","total_lines":190,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_local_scan_policy.py","chunk_index":0,"line_start":1,"line_end":190,"content":["#!/usr/bin/env python3","\"\"\"","Test Local-Strict scanner policy and file inclusion logic.","","Contract:","- Build a temp repo-like structure","- src/a.py included","- tests/t.py included","- .venv/x.py excluded","- outputs/jobs.db excluded","- outputs/snapshots/full/REPO_TREE.txt included","- node_modules/pkg/index.js excluded","- root Makefile included","","Assert iter_repo_files_local_strict returns exactly expected list, sorted.","\"\"\"","","import tempfile","import shutil","from pathlib import Path","import pytest","","from control.local_scan import (","    LocalScanPolicy,","    default_local_strict_policy,","    iter_repo_files_local_strict,","    should_include_file,",")","","","def test_default_policy():","    \"\"\"Test that default policy matches spec.\"\"\"","    policy = default_local_strict_policy()","    ","    assert policy.allowed_roots == (\"src\", \"tests\", \"scripts\", \"docs\")","    assert \"Makefile\" in policy.allowed_root_files_glob","    assert \"pyproject.toml\" in policy.allowed_root_files_glob","    assert \".git\" in policy.deny_segments","    assert \".venv\" in policy.deny_segments","    assert \"node_modules\" in policy.deny_segments","    assert \"outputs\" in policy.deny_segments","    assert policy.outputs_allow == (\"outputs/snapshots\",)","    assert policy.max_files == 20000","    assert policy.max_bytes == 2000000","    assert policy.gitignore_respected is False","","","def test_should_include_file():","    \"\"\"Test individual file inclusion decisions.\"\"\"","    policy = default_local_strict_policy()","    ","    # Root files","    assert should_include_file(Path(\"Makefile\"), policy) is True","    assert should_include_file(Path(\"pyproject.toml\"), policy) is True","    assert should_include_file(Path(\"README.md\"), policy) is True","    assert should_include_file(Path(\"random.txt\"), policy) is False  # not in glob","    ","    # Allowed roots","    assert should_include_file(Path(\"src/a.py\"), policy) is True","    assert should_include_file(Path(\"tests/test.py\"), policy) is True","    assert should_include_file(Path(\"scripts/run.py\"), policy) is True","    assert should_include_file(Path(\"docs/index.md\"), policy) is True","    ","    # Denied segments anywhere in path","    assert should_include_file(Path(\"src/.venv/foo.py\"), policy) is False","    assert should_include_file(Path(\"tests/.git/config\"), policy) is False","    assert should_include_file(Path(\"scripts/node_modules/pkg/index.js\"), policy) is False","    assert should_include_file(Path(\"docs/__pycache__/module.cpython-310.pyc\"), policy) is False","    ","    # Outputs exception","    assert should_include_file(Path(\"outputs/jobs.db\"), policy) is False","    assert should_include_file(Path(\"outputs/snapshots/full/REPO_TREE.txt\"), policy) is True","    assert should_include_file(Path(\"outputs/snapshots/full/LOCAL_SCAN_RULES.json\"), policy) is True","    assert should_include_file(Path(\"outputs/snapshots/\"), policy) is True  # exact match","    assert should_include_file(Path(\"outputs/snapshots\"), policy) is True  # exact match","    ","    # Other directories not allowed","    assert should_include_file(Path(\"configs/something.yaml\"), policy) is False","    assert should_include_file(Path(\"data/raw.csv\"), policy) is False","","","def test_iter_repo_files_local_strict_integration(tmp_path: Path):","    \"\"\"Build a temp repo structure and verify scanning.\"\"\"","    repo_root = tmp_path","    ","    # Create expected included files","    (repo_root / \"src\").mkdir()","    (repo_root / \"src\" / \"a.py\").write_text(\"# included\")","    (repo_root / \"src\" / \"subdir\").mkdir()","    (repo_root / \"src\" / \"subdir\" / \"b.py\").write_text(\"# included\")","    ","    (repo_root / \"tests\").mkdir()","    (repo_root / \"tests\" / \"t.py\").write_text(\"# included\")","    ","    (repo_root / \"scripts\").mkdir()","    (repo_root / \"scripts\" / \"run.py\").write_text(\"# included\")","    ","    (repo_root / \"docs\").mkdir()","    (repo_root / \"docs\" / \"index.md\").write_text(\"# included\")","    ","    # Create outputs exception","    (repo_root / \"outputs\").mkdir()","    (repo_root / \"outputs\" / \"jobs.db\").write_text(\"binary\")  # should be excluded","    (repo_root / \"outputs\" / \"snapshots\").mkdir()","    (repo_root / \"outputs\" / \"snapshots\" / \"full\").mkdir()","    (repo_root / \"outputs\" / \"snapshots\" / \"full\" / \"REPO_TREE.txt\").write_text(\"# included\")","    ","    # Create excluded directories","    (repo_root / \".venv\").mkdir()","    (repo_root / \".venv\" / \"x.py\").write_text(\"# excluded\")","    ","    (repo_root / \"node_modules\").mkdir()","    (repo_root / \"node_modules\" / \"pkg\").mkdir()","    (repo_root / \"node_modules\" / \"pkg\" / \"index.js\").write_text(\"// excluded\")","    ","    (repo_root / \"__pycache__\").mkdir()","    (repo_root / \"__pycache__\" / \"module.cpython-310.pyc\").write_bytes(b\"\\x00\\x01\")","    ","    # Root files","    (repo_root / \"Makefile\").write_text(\"# included\")","    (repo_root / \"pyproject.toml\").write_text(\"# included\")","    (repo_root / \"README.md\").write_text(\"# included\")","    (repo_root / \"random.txt\").write_text(\"# excluded - not in glob\")","    ","    # Create a file in disallowed root (configs)","    (repo_root / \"configs\").mkdir()","    (repo_root / \"configs\" / \"profile.yaml\").write_text(\"# excluded\")","    ","    policy = default_local_strict_policy()","    files = iter_repo_files_local_strict(repo_root, policy)","    ","    # files are already relative to repo_root","    rel_files_str = sorted(str(p) for p in files)","    ","    expected = [","        \"Makefile\",","        \"README.md\",","        \"pyproject.toml\",","        \"docs/index.md\",","        \"outputs/snapshots/full/REPO_TREE.txt\",","        \"scripts/run.py\",","        \"src/a.py\",","        \"src/subdir/b.py\",","        \"tests/t.py\",","    ]","    ","    assert rel_files_str == sorted(expected), f\"Got {rel_files_str}, expected {sorted(expected)}\"","    ","    # Verify deterministic ordering","    files2 = iter_repo_files_local_strict(repo_root, policy)","    assert list(files) == list(files2), \"Should be deterministic\"","","","def test_max_files_limit(tmp_path: Path):","    \"\"\"Test max_files limit is respected.\"\"\"","    repo_root = tmp_path","    (repo_root / \"src\").mkdir()","    ","    # Create many files","    for i in range(100):","        (repo_root / \"src\" / f\"file{i}.py\").write_text(\"# content\")","    ","    policy = LocalScanPolicy(","        allowed_roots=(\"src\",),","        allowed_root_files_glob=(),","        deny_segments=(),","        outputs_allow=(),","        max_files=10,  # Low limit","        max_bytes=1000000,","        gitignore_respected=False,","    )","    ","    files = iter_repo_files_local_strict(repo_root, policy)","    assert len(files) == 10, f\"Should be limited to 10 files, got {len(files)}\"","    ","    # Should be sorted deterministically","    file_names = [f.name for f in files]","    assert file_names == sorted(file_names), \"Files should be sorted\"","","","def test_policy_immutability():","    \"\"\"Test that policy dataclass is frozen.\"\"\"","    policy = default_local_strict_policy()","    ","    with pytest.raises(Exception):","        policy.allowed_roots = (\"something\",)  # Should raise because frozen","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_local_scan_policy.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_log_tail_reads_last_n_lines.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1820,"sha256":"e30c41f8faf9bb81714345099d2c150186d29e8bef8306006353ed9a583fe8a2","total_lines":65,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_log_tail_reads_last_n_lines.py","chunk_index":0,"line_start":1,"line_end":65,"content":["","\"\"\"Test that read_tail reads last n lines efficiently without loading entire file.\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from control.api import read_tail","","","def test_read_tail_returns_last_n_lines(tmp_path: Path) -> None:","    \"\"\"Test that read_tail returns exactly the last n lines.\"\"\"","    p = tmp_path / \"big.log\"","    lines = [f\"line {i}\\n\" for i in range(5000)]","    p.write_text(\"\".join(lines), encoding=\"utf-8\")","","    lines_list, truncated = read_tail(p, n=200)","    out = \"\".join(lines_list)","    out_lines = out.splitlines()","","    assert len(out_lines) == 200","    assert out_lines[0] == \"line 4800\"","    assert out_lines[-1] == \"line 4999\"","    assert truncated is True","","","def test_read_tail_handles_small_file(tmp_path: Path) -> None:","    \"\"\"Test that read_tail handles files with fewer lines than requested.\"\"\"","    p = tmp_path / \"small.log\"","    lines = [f\"line {i}\\n\" for i in range(50)]","    p.write_text(\"\".join(lines), encoding=\"utf-8\")","","    lines_list, truncated = read_tail(p, n=200)","    out = \"\".join(lines_list)","    out_lines = out.splitlines()","","    assert len(out_lines) == 50","    assert out_lines[0] == \"line 0\"","    assert out_lines[-1] == \"line 49\"","    assert truncated is False","","","def test_read_tail_handles_empty_file(tmp_path: Path) -> None:","    \"\"\"Test that read_tail handles empty files.\"\"\"","    p = tmp_path / \"empty.log\"","    p.touch()","","    lines_list, truncated = read_tail(p, n=200)","    out = \"\".join(lines_list)","    assert out == \"\"","    assert truncated is False","","","def test_read_tail_handles_missing_file(tmp_path: Path) -> None:","    \"\"\"Test that read_tail handles missing files gracefully.\"\"\"","    p = tmp_path / \"missing.log\"","","    lines_list, truncated = read_tail(p, n=200)","    out = \"\".join(lines_list)","    assert out == \"\"","    assert truncated is False","",""]}
{"type":"file_footer","path":"tests/test_log_tail_reads_last_n_lines.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_mnq_maintenance_break_no_cross.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4644,"sha256":"1ed5d0747ee78baffb45e81cb140e73636e683fe66ff0957483f77e6c9eb505e","total_lines":117,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_mnq_maintenance_break_no_cross.py","chunk_index":0,"line_start":1,"line_end":117,"content":["","\"\"\"Test MNQ maintenance break: no cross-session aggregation.","","Phase 6.6: Verify that MNQ bars before and after maintenance window","are not aggregated into the same K-bar.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pandas as pd","import pytest","","from data.session.kbar import aggregate_kbar","from data.session.loader import load_session_profile","","","@pytest.fixture","def mnq_exchange_profile(profiles_root: Path) -> Path:","    \"\"\"Load CME.MNQ EXCHANGE_RULE profile.\"\"\"","    profile_path = profiles_root / \"CME_MNQ_EXCHANGE_v1.yaml\"","    return profile_path","","","def test_mnq_maintenance_break_no_cross_30m(mnq_exchange_profile: Path) -> None:","    \"\"\"Test 30-minute bars do not cross maintenance boundary.","    ","    MNQ maintenance: 16:00-17:00 CT (approximately 06:00-07:00 TPE, varies with DST).","    Bars just before maintenance (15:59 CT) and just after (17:01 CT)","    should not be in the same 30m bar.","    \"\"\"","    profile = load_session_profile(mnq_exchange_profile)","    ","    # Create bars around maintenance window","    # Using dates that avoid DST transitions for simplicity","    # 2013/3/10 is a Sunday (before DST spring forward on 3/10/2013)","    # Maintenance window: 16:00-17:00 CT = approximately 06:00-07:00 TPE (before DST)","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/3/10 05:55:00\",  # TRADING (before maintenance, ~15:55 CT)","            \"2013/3/10 05:59:00\",  # TRADING (just before maintenance, ~15:59 CT)","            \"2013/3/10 06:30:00\",  # MAINTENANCE (during maintenance, ~16:30 CT)","            \"2013/3/10 07:01:00\",  # TRADING (just after maintenance, ~17:01 CT)","            \"2013/3/10 07:05:00\",  # TRADING (after maintenance, ~17:05 CT)","        ],","        \"open\": [100.0, 101.0, 102.0, 103.0, 104.0],","        \"high\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"low\": [99.5, 100.5, 101.5, 102.5, 103.5],","        \"close\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"volume\": [1000, 1100, 1200, 1300, 1400],","    })","    ","    result = aggregate_kbar(df, 30, profile)","    ","    # Verify result has session column","    assert \"session\" in result.columns, \"Result must include session column\"","    ","    # Verify no bar mixes TRADING and MAINTENANCE","    # Each row must have exactly one session","    assert result[\"session\"].notna().all(), \"All bars must have a session label\"","    ","    # Check that TRADING and MAINTENANCE are separate","    trading_bars = result[result[\"session\"] == \"TRADING\"]","    maintenance_bars = result[result[\"session\"] == \"MAINTENANCE\"]","    ","    # Should have both TRADING and MAINTENANCE bars (if maintenance bars exist)","    if len(maintenance_bars) > 0:","        # Verify no bar contains both sessions","        assert len(result) == len(trading_bars) + len(maintenance_bars), (","            \"Total bars should equal sum of TRADING and MAINTENANCE bars\"","        )","        ","        # Verify bars before maintenance are TRADING","        # Verify bars during maintenance are MAINTENANCE","        # Verify bars after maintenance are TRADING","        # (This is verified by the session column)","","","def test_mnq_maintenance_break_no_cross_60m(mnq_exchange_profile: Path) -> None:","    \"\"\"Test 60-minute bars do not cross maintenance boundary.\"\"\"","    profile = load_session_profile(mnq_exchange_profile)","    ","    # Similar to 30m test, but with 60m interval","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/3/10 05:50:00\",  # TRADING (before maintenance)","            \"2013/3/10 05:59:00\",  # TRADING (just before maintenance)","            \"2013/3/10 06:30:00\",  # MAINTENANCE (during maintenance)","            \"2013/3/10 07:01:00\",  # TRADING (just after maintenance)","            \"2013/3/10 07:10:00\",  # TRADING (after maintenance)","        ],","        \"open\": [100.0, 101.0, 102.0, 103.0, 104.0],","        \"high\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"low\": [99.5, 100.5, 101.5, 102.5, 103.5],","        \"close\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"volume\": [1000, 1100, 1200, 1300, 1400],","    })","    ","    result = aggregate_kbar(df, 60, profile)","    ","    # Verify result has session column","    assert \"session\" in result.columns, \"Result must include session column\"","    ","    # Verify no bar mixes TRADING and MAINTENANCE","    assert result[\"session\"].notna().all(), \"All bars must have a session label\"","    ","    # Verify session separation","    trading_bars = result[result[\"session\"] == \"TRADING\"]","    maintenance_bars = result[result[\"session\"] == \"MAINTENANCE\"]","    ","    if len(maintenance_bars) > 0:","        assert len(result) == len(trading_bars) + len(maintenance_bars), (","            \"Total bars should equal sum of TRADING and MAINTENANCE bars\"","        )","",""]}
{"type":"file_footer","path":"tests/test_mnq_maintenance_break_no_cross.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_no_fog_gate_smoke.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7567,"sha256":"a1ae442c911f99ea7a284535a818d7a4f8bf077b1b1420a655fb11a5f1273b09","total_lines":205,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_no_fog_gate_smoke.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"","Smoke test for No-Fog Gate Automation.","","Tests that the no-fog gate:","1. Can be imported and run","2. Has correct structure and dependencies","3. Can run in check-only mode without side effects","4. Validates core contract tests exist","\"\"\"","","import subprocess","import sys","import tempfile","import json","from pathlib import Path","import pytest","","# Project root","PROJECT_ROOT = Path(__file__).parent.parent.resolve()","NO_FOG_GATE_PY = PROJECT_ROOT / \"scripts\" / \"no_fog\" / \"no_fog_gate.py\"","NO_FOG_GATE_SH = PROJECT_ROOT / \"scripts\" / \"no_fog\" / \"no_fog_gate.sh\"","","","def test_no_fog_gate_py_exists():","    \"\"\"Test that the Python script exists.\"\"\"","    assert NO_FOG_GATE_PY.exists(), f\"No-Fog Gate Python script not found: {NO_FOG_GATE_PY}\"","    assert NO_FOG_GATE_PY.is_file()","","","def test_no_fog_gate_sh_exists():","    \"\"\"Test that the shell script exists.\"\"\"","    assert NO_FOG_GATE_SH.exists(), f\"No-Fog Gate shell script not found: {NO_FOG_GATE_SH}\"","    assert NO_FOG_GATE_SH.is_file()","","","def test_no_fog_gate_py_importable():","    \"\"\"Test that the Python script can be imported (syntax check).\"\"\"","    # Read the file and check for syntax errors","    import ast","    source = NO_FOG_GATE_PY.read_text(encoding=\"utf-8\")","    try:","        ast.parse(source)","    except SyntaxError as e:","        pytest.fail(f\"Syntax error in {NO_FOG_GATE_PY}: {e}\")","","","def test_no_fog_gate_sh_executable():","    \"\"\"Test that the shell script is executable (or can be made executable).\"\"\"","    # Check if it has execute permission, but don't fail if not","    # (it will be made executable by the Makefile)","    pass","","","def test_core_contract_tests_exist():","    \"\"\"Test that all core contract test files exist.\"\"\"","    core_tests = [","        \"tests/strategy/test_ast_identity.py\",","        \"tests/test_ui_race_condition_headless.py\",","        \"tests/features/test_feature_causality.py\",","        \"tests/features/test_feature_lookahead_rejection.py\",","        \"tests/features/test_feature_window_honesty.py\",","    ]","    ","    missing = []","    for test_path in core_tests:","        full_path = PROJECT_ROOT / test_path","        if not full_path.exists():","            missing.append(test_path)","    ","    assert not missing, f\"Core contract test files missing: {missing}\"","","","def test_no_fog_gate_check_only_mode():","    \"\"\"Test that the gate can run in check-only (dry run) mode.\"\"\"","    # Run the Python script with --check-only flag","    cmd = [sys.executable, str(NO_FOG_GATE_PY), \"--check-only\"]","    ","    result = subprocess.run(","        cmd,","        cwd=PROJECT_ROOT,","        capture_output=True,","        text=True,","        timeout=10,","        env={**dict(os.environ), \"PYTHONPATH\": str(PROJECT_ROOT / \"src\")}","    )","    ","    # Should exit with 0 (success) even in check-only mode","    assert result.returncode == 0, f\"Check-only mode failed:\\nStdout: {result.stdout}\\nStderr: {result.stderr}\"","    ","    # Should mention \"Dry run\" or \"check-only\" in output","    output = result.stdout + result.stderr","    assert any(phrase in output.lower() for phrase in [\"dry run\", \"check-only\", \"would run\"]), \\","        f\"Expected dry run message in output:\\n{output}\"","","","def test_no_fog_gate_help():","    \"\"\"Test that the gate shows help when requested.\"\"\"","    # Test Python script help","    cmd_py = [sys.executable, str(NO_FOG_GATE_PY), \"--help\"]","    result_py = subprocess.run(","        cmd_py,","        cwd=PROJECT_ROOT,","        capture_output=True,","        text=True,","        timeout=5","    )","    ","    assert result_py.returncode == 0, f\"Python script help failed: {result_py.stderr}\"","    assert \"usage:\" in result_py.stdout.lower() or \"help\" in result_py.stdout.lower(), \\","        f\"Help not shown in Python script:\\n{result_py.stdout}\"","    ","    # Test shell script help (if executable)","    if NO_FOG_GATE_SH.stat().st_mode & 0o111:  # If executable","        cmd_sh = [\"bash\", str(NO_FOG_GATE_SH), \"--help\"]","        result_sh = subprocess.run(","            cmd_sh,","            cwd=PROJECT_ROOT,","            capture_output=True,","            text=True,","            timeout=5","        )","        ","        # Shell script help should also work","        assert result_sh.returncode in [0, 2], f\"Shell script help failed: {result_sh.stderr}\"","        assert \"usage:\" in result_sh.stdout.lower() or \"help\" in result_sh.stdout.lower() or \"No-Fog Gate\" in result_sh.stdout, \\","            f\"Help not shown in shell script:\\n{result_sh.stdout}\"","","","def test_make_no_fog_target():","    \"\"\"Test that 'make no-fog' target is defined in Makefile.\"\"\"","    makefile_path = PROJECT_ROOT / \"Makefile\"","    assert makefile_path.exists(), \"Makefile not found\"","    ","    makefile_content = makefile_path.read_text(encoding=\"utf-8\")","    ","    # Check for no-fog target definition","    assert \"no-fog:\" in makefile_content, \"'no-fog' target not defined in Makefile\"","    ","    # Check that it's in .PHONY","    assert \"no-fog\" in makefile_content.split(\".PHONY:\")[1].split(\"\\n\")[0], \\","        \"'no-fog' not in .PHONY targets\"","    ","    # Check that it's in help","    assert \"make no-fog\" in makefile_content, \"'make no-fog' not in help section\"","","","def test_pre_commit_config():","    \"\"\"Test that pre-commit configuration includes no-fog gate.\"\"\"","    pre_commit_config = PROJECT_ROOT / \".pre-commit-config.yaml\"","    if pre_commit_config.exists():","        content = pre_commit_config.read_text(encoding=\"utf-8\")","        # Should contain no-fog-gate hook","        assert \"no-fog-gate\" in content, \"No-Fog Gate not in pre-commit config\"","        assert \"scripts/no_fog/no_fog_gate.sh\" in content, \"Shell script path not in pre-commit config\"","","","def test_github_workflow():","    \"\"\"Test that GitHub workflow exists.\"\"\"","    workflow_path = PROJECT_ROOT / \".github\" / \"workflows\" / \"no_fog_gate.yml\"","    if workflow_path.exists():","        content = workflow_path.read_text(encoding=\"utf-8\")","        assert \"No-Fog Gate\" in content, \"Workflow doesn't mention No-Fog Gate\"","        assert \"scripts/no_fog/no_fog_gate.sh\" in content, \"Shell script not in workflow\"","","","def test_snapshot_directory_structure():","    \"\"\"Test that snapshot directory structure is correct.\"\"\"","    snapshot_dir = PROJECT_ROOT / \"SYSTEM_FULL_SNAPSHOT\"","    ","    # Directory might not exist yet, that's OK","    if snapshot_dir.exists():","        # Should have MANIFEST.json if it's a valid snapshot","        manifest_path = snapshot_dir / \"MANIFEST.json\"","        if manifest_path.exists():","            try:","                manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))","                assert \"generated_at\" in manifest, \"Manifest missing generated_at\"","                assert \"chunks\" in manifest, \"Manifest missing chunks\"","                assert \"files\" in manifest, \"Manifest missing files\"","            except json.JSONDecodeError:","                pytest.fail(\"MANIFEST.json is not valid JSON\")","","","def test_gate_timeout_configuration():","    \"\"\"Test that timeout is configurable and defaults to 30 seconds.\"\"\"","    # Read the Python script to check default timeout","    content = NO_FOG_GATE_PY.read_text(encoding=\"utf-8\")","    ","    # Should have GATE_TIMEOUT = 30","    assert \"GATE_TIMEOUT = 30\" in content or \"GATE_TIMEOUT=30\" in content or \"TIMEOUT=30\" in content, \\","        \"Default timeout not set to 30 seconds in Python script\"","    ","    # Check shell script for timeout argument","    sh_content = NO_FOG_GATE_SH.read_text(encoding=\"utf-8\")","    assert \"--timeout\" in sh_content, \"Shell script missing --timeout argument\"","    assert \"30\" in sh_content, \"Shell script missing default timeout value\"","","","# Import os for environment variable","import os"]}
{"type":"file_chunk","path":"tests/test_no_fog_gate_smoke.py","chunk_index":1,"line_start":201,"line_end":205,"content":["","","if __name__ == \"__main__\":","    # Run tests","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_no_fog_gate_smoke.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_no_ui_imports_anywhere.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1427,"sha256":"938bf55f0f6d149ffba03b69d015067e7e7f93a431b02f9f7f561553e05718fa","total_lines":37,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_no_ui_imports_anywhere.py","chunk_index":0,"line_start":1,"line_end":37,"content":["","\"\"\"Contract test: No ui namespace imports anywhere in ","","Ensures the entire FishBroWFS_V2 package does not import from ui namespace.","This is a \"truth test\" to prevent any ui.* imports from being reintroduced.","\"\"\"","","from __future__ import annotations","","import pkgutil","","import pytest","","","def test_no_ui_namespace_anywhere() -> None:","    \"\"\"Test that FishBroWFS_V2 package does not import from ui namespace.\"\"\"","    import FishBroWFS_V2","    ","    # If any module imports ui.*, it will fail during import","    for importer, modname, ispkg in pkgutil.walk_packages(__path__, __name__ + \".\"):","        try:","            # Import module - this will fail if it imports ui.* and ui doesn't exist","            __import__(modname, fromlist=[\"\"])","        except ImportError as e:","            # Check if error is related to ui namespace","            if \"ui\" in str(e) and (\"No module named\" in str(e) or \"cannot import name\" in str(e)):","                pytest.fail(","                    f\"Module {modname} imports from ui namespace (ui module no longer exists): {e}\"","                )","            # 跳過 viewer 模組的 streamlit 導入錯誤","            if \"gui.viewer\" in modname and \"No module named 'streamlit'\" in str(e):","                # viewer 模組依賴 streamlit，但 streamlit 已移除，這是預期的","                continue","            # Re-raise other ImportErrors (might be legitimate missing dependencies)","            raise","",""]}
{"type":"file_footer","path":"tests/test_no_ui_imports_anywhere.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_no_ui_namespace.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6586,"sha256":"a74d7dfc6ffa996bc79b4737dcb29b98c7b0b017c9121bffdc5621941f9eea6b","total_lines":152,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_no_ui_namespace.py","chunk_index":0,"line_start":1,"line_end":152,"content":["","\"\"\"Contract test: No ui namespace imports allowed.","","Ensures the entire FishBroWFS_V2 package does not import from ui namespace.","\"\"\"","","from __future__ import annotations","","import ast","import pkgutil","from pathlib import Path","","import pytest","","","def test_no_ui_namespace_importable() -> None:","    \"\"\"Test that FishBroWFS_V2 package does not import from ui namespace.\"\"\"","    import FishBroWFS_V2 as pkg","    ","    ui_imports: list[tuple[str, str]] = []","    ","    for importer, modname, ispkg in pkgutil.walk_packages(pkg.__path__, pkg.__name__ + \".\"):","        try:","            # Import module to trigger any import errors","            module = __import__(modname, fromlist=[\"\"])","            ","            # Get source file path","            if hasattr(module, \"__file__\") and module.__file__:","                source_path = Path(module.__file__)","                if source_path.exists() and source_path.suffix == \".py\":","                    # Parse AST to find imports","                    try:","                        with source_path.open(\"r\", encoding=\"utf-8\") as f:","                            tree = ast.parse(f.read(), filename=str(source_path))","                        ","                        # Check all imports","                        for node in ast.walk(tree):","                            if isinstance(node, ast.Import):","                                for alias in node.names:","                                    if alias.name.startswith(\"ui.\"):","                                        ui_imports.append((modname, alias.name))","                            elif isinstance(node, ast.ImportFrom):","                                if node.module and node.module.startswith(\"ui.\"):","                                    ui_imports.append((modname, f\"from {node.module}\"))","                    except (SyntaxError, UnicodeDecodeError):","                        # Skip files that can't be parsed (might be binary or invalid)","                        pass","        except Exception as e:","            # Skip modules that fail to import (might be missing dependencies)","            # But log for debugging if it's not an ImportError","            if \"ImportError\" not in str(type(e)) and \"ModuleNotFoundError\" not in str(type(e)):","                pytest.fail(f\"Unexpected error importing {modname}: {e}\")","    ","    # Should have no ui.* imports","    if ui_imports:","        pytest.fail(","            f\"FishBroWFS_V2 package contains ui.* imports:\\n\"","            + \"\\n\".join(f\"  {mod}: {imp}\" for mod, imp in ui_imports)","        )","","","def test_viewer_no_ui_imports() -> None:","    \"\"\"Test that Viewer package specifically does not import from ui namespace.\"\"\"","    import gui.viewer as viewer","    ","    ui_imports: list[tuple[str, str]] = []","    ","    # Walk through all modules in viewer package","    for importer, modname, ispkg in pkgutil.walk_packages(viewer.__path__, viewer.__name__ + \".\"):","        try:","            module = __import__(modname, fromlist=[\"\"])","            ","            if hasattr(module, \"__file__\") and module.__file__:","                source_path = Path(module.__file__)","                if source_path.exists() and source_path.suffix == \".py\":","                    try:","                        with source_path.open(\"r\", encoding=\"utf-8\") as f:","                            tree = ast.parse(f.read(), filename=str(source_path))","                        ","                        for node in ast.walk(tree):","                            if isinstance(node, ast.Import):","                                for alias in node.names:","                                    if alias.name.startswith(\"ui.\"):","                                        ui_imports.append((modname, alias.name))","                            elif isinstance(node, ast.ImportFrom):","                                if node.module and node.module.startswith(\"ui.\"):","                                    ui_imports.append((modname, f\"from {node.module}\"))","                    except (SyntaxError, UnicodeDecodeError):","                        pass","        except Exception as e:","            if \"ImportError\" not in str(type(e)) and \"ModuleNotFoundError\" not in str(type(e)):","                pytest.fail(f\"Unexpected error importing {modname}: {e}\")","    ","    if ui_imports:","        pytest.fail(","            f\"Viewer package contains ui.* imports:\\n\"","            + \"\\n\".join(f\"  {mod}: {imp}\" for mod, imp in ui_imports)","        )","","","def test_no_ui_directory_exists() -> None:","    \"\"\"Test that ui/ directory does not exist in repo root (repo structure contract).\"\"\"","    repo_root = Path(__file__).parent.parent","    ui_dir = repo_root / \"ui\"","    ","    if ui_dir.exists():","        pytest.fail(f\"ui/ directory must not exist in repo root, but found at {ui_dir}\")","","","def test_makefile_no_ui_paths() -> None:","    \"\"\"Test that Makefile does not reference ui/ paths (old namespace).\"\"\"","    repo_root = Path(__file__).parent.parent","    makefile_path = repo_root / \"Makefile\"","    ","    assert makefile_path.exists()","    ","    content = makefile_path.read_text()","    ","    # Check for ui/ references (excluding comments)","    lines = content.split(\"\\n\")","    for i, line in enumerate(lines, 1):","        # Skip comments","        if line.strip().startswith(\"#\"):","            continue","        ","        # Normalize line for checking","        line_lower = line.lower()","        ","        # Prohibited patterns (old ui namespace)","        # 1. Path references containing \"/ui/\" (excluding \"gui/\")","        if \"/ui/\" in line and \"/gui/\" not in line:","            pytest.fail(f\"Makefile line {i} contains prohibited /ui/ path: {line.strip()}\")","        ","        if \"fishbro_wfs_v2.ui.\" in line_lower:","            pytest.fail(f\"Makefile line {i} contains prohibited ui. import: {line.strip()}\")","        ","        # 3. Specific old module \"ui.app_streamlit\"","        if \"ui.app_streamlit\" in line_lower:","            pytest.fail(f\"Makefile line {i} contains prohibited ui.app_streamlit: {line.strip()}\")","        ","        # 4. Standalone \"ui.\" as a module prefix (with word boundary)","        # We'll use a simple check: \"ui.\" preceded by whitespace or start of line","        # but exclude \"gui.\" and \"build\" etc.","        import re","        if re.search(r'(^|\\s)ui\\.', line) and not re.search(r'(^|\\s)gui\\.', line):","            # Allow if it's part of a longer word like \"build\" (but \"ui.\" is likely a module)","            # Additional check: ensure it's not part of a larger word like \"build\"","            if not re.search(r'\\bui\\.', line):  # word boundary check","                continue","            pytest.fail(f\"Makefile line {i} contains prohibited ui. module reference: {line.strip()}\")","",""]}
{"type":"file_footer","path":"tests/test_no_ui_namespace.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_oom_gate.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7813,"sha256":"16927b422047ced80d15af00b616f80dfaf06c922296082623e89e2c6b19eeb6","total_lines":236,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_oom_gate.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Tests for OOM gate decision maker.","","Tests verify:","1. PASS case (estimated <= 60% of budget)","2. BLOCK case (estimated > 90% of budget)","3. AUTO_DOWNSAMPLE case (between 60% and 90%, with recommended_rate in (0,1])","4. Invalid input validation (bars<=0, rate<=0, etc.)","\"\"\"","","from __future__ import annotations","","import pytest","","from core.oom_gate import decide_gate, decide_oom_action, estimate_bytes","from core.schemas.oom_gate import OomGateInput","","","def test_estimate_bytes() -> None:","    \"\"\"Test memory estimation formula.\"\"\"","    inp = OomGateInput(","        bars=1000,","        params=100,","        param_subsample_rate=0.5,","        intents_per_bar=2.0,","        bytes_per_intent_est=64,","    )","    ","    estimated = estimate_bytes(inp)","    ","    # Formula: bars * params * subsample * intents_per_bar * bytes_per_intent_est","    expected = 1000 * 100 * 0.5 * 2.0 * 64","    assert estimated == expected","","","def test_decide_gate_pass() -> None:","    \"\"\"Test PASS decision when estimated <= 60% of budget.\"\"\"","    # Small workload: 1M bytes, budget is 6GB (6_000_000_000)","    inp = OomGateInput(","        bars=100,","        params=10,","        param_subsample_rate=0.1,","        intents_per_bar=2.0,","        bytes_per_intent_est=64,","        ram_budget_bytes=6_000_000_000,","    )","    ","    decision = decide_gate(inp)","    ","    assert decision.decision == \"PASS\"","    assert decision.estimated_bytes <= inp.ram_budget_bytes * 0.6","    assert decision.recommended_subsample_rate is None","    assert \"PASS\" not in decision.notes  # Notes should describe the decision, not repeat it","    assert decision.estimated_bytes > 0","","","def test_decide_gate_block() -> None:","    \"\"\"Test BLOCK decision when estimated > 90% of budget.\"\"\"","    # Large workload: exceed 90% of budget","    # Set budget to 1GB for easier testing","    budget = 1_000_000_000  # 1GB","    # Need estimated > budget * 0.9 = 900MB","    # Let's use: 10000 bars * 10000 params * 1.0 rate * 2.0 intents * 64 bytes = 12.8GB","    inp = OomGateInput(","        bars=10000,","        params=10000,","        param_subsample_rate=1.0,","        intents_per_bar=2.0,","        bytes_per_intent_est=64,","        ram_budget_bytes=budget,","    )","    ","    decision = decide_gate(inp)","    ","    assert decision.decision == \"BLOCK\"","    assert decision.estimated_bytes > budget * 0.9","    assert decision.recommended_subsample_rate is None","    assert \"BLOCKED\" in decision.notes or \"BLOCK\" in decision.notes","","","def test_decide_gate_auto_downsample() -> None:","    \"\"\"Test AUTO_DOWNSAMPLE decision when estimated between 60% and 90%.\"\"\"","    # Medium workload: between 60% and 90% of budget","    # Set budget to 1GB for easier testing","    budget = 1_000_000_000  # 1GB","    # Need: budget * 0.6 < estimated < budget * 0.9","    # 600MB < estimated < 900MB","    # Let's use: 5000 bars * 5000 params * 1.0 rate * 2.0 intents * 64 bytes = 3.2GB","    # That's too high. Let's adjust:","    # For 700MB: 700_000_000 = bars * params * 1.0 * 2.0 * 64","    # bars * params = 700_000_000 / (2.0 * 64) = 5_468_750","    # Let's use: 5000 bars * 1094 params * 1.0 rate * 2.0 * 64 = ~700MB","    inp = OomGateInput(","        bars=5000,","        params=1094,","        param_subsample_rate=1.0,","        intents_per_bar=2.0,","        bytes_per_intent_est=64,","        ram_budget_bytes=budget,","    )","    ","    decision = decide_gate(inp)","    ","    assert decision.decision == \"AUTO_DOWNSAMPLE\"","    assert decision.estimated_bytes > budget * 0.6","    assert decision.estimated_bytes <= budget * 0.9","    assert decision.recommended_subsample_rate is not None","    assert 0.0 < decision.recommended_subsample_rate <= 1.0","    assert \"recommended\" in decision.notes.lower() or \"subsample\" in decision.notes.lower()","","","def test_decide_gate_auto_downsample_recommended_rate_calculation() -> None:","    \"\"\"Test that recommended_rate is calculated correctly for AUTO_DOWNSAMPLE.\"\"\"","    budget = 1_000_000_000  # 1GB","    bars = 1000","    params = 1000","    intents_per_bar = 2.0","    bytes_per_intent = 64","    ","    # Use current rate that puts us in AUTO_DOWNSAMPLE zone","    inp = OomGateInput(","        bars=bars,","        params=params,","        param_subsample_rate=1.0,","        intents_per_bar=intents_per_bar,","        bytes_per_intent_est=bytes_per_intent,","        ram_budget_bytes=budget,","    )","    ","    decision = decide_gate(inp)","    ","    if decision.decision == \"AUTO_DOWNSAMPLE\":","        # Verify recommended_rate formula: (ram_budget * 0.6) / (bars * params * intents_per_bar * bytes_per_intent_est)","        expected_rate = (budget * 0.6) / (bars * params * intents_per_bar * bytes_per_intent)","        expected_rate = max(0.0, min(1.0, expected_rate))","        ","        assert decision.recommended_subsample_rate is not None","        assert abs(decision.recommended_subsample_rate - expected_rate) < 0.0001  # Allow small floating point error","","","def test_invalid_input_bars_zero() -> None:","    \"\"\"Test that bars <= 0 raises validation error.\"\"\"","    with pytest.raises(Exception):  # Pydantic ValidationError","        OomGateInput(","            bars=0,","            params=100,","            param_subsample_rate=0.5,","        )","","","def test_invalid_input_bars_negative() -> None:","    \"\"\"Test that bars < 0 raises validation error.\"\"\"","    with pytest.raises(Exception):  # Pydantic ValidationError","        OomGateInput(","            bars=-1,","            params=100,","            param_subsample_rate=0.5,","        )","","","def test_invalid_input_params_zero() -> None:","    \"\"\"Test that params <= 0 raises validation error.\"\"\"","    with pytest.raises(Exception):  # Pydantic ValidationError","        OomGateInput(","            bars=1000,","            params=0,","            param_subsample_rate=0.5,","        )","","","def test_invalid_input_subsample_rate_zero() -> None:","    \"\"\"Test that param_subsample_rate <= 0 raises validation error.\"\"\"","    with pytest.raises(Exception):  # Pydantic ValidationError","        OomGateInput(","            bars=1000,","            params=100,","            param_subsample_rate=0.0,","        )","","","def test_invalid_input_subsample_rate_negative() -> None:","    \"\"\"Test that param_subsample_rate < 0 raises validation error.\"\"\"","    with pytest.raises(Exception):  # Pydantic ValidationError","        OomGateInput(","            bars=1000,","            params=100,","            param_subsample_rate=-0.1,","        )","","","def test_invalid_input_subsample_rate_over_one() -> None:","    \"\"\"Test that param_subsample_rate > 1.0 raises validation error.\"\"\"","    with pytest.raises(Exception):  # Pydantic ValidationError","        OomGateInput(","            bars=1000,","            params=100,","            param_subsample_rate=1.1,","        )","",""]}
{"type":"file_chunk","path":"tests/test_oom_gate.py","chunk_index":1,"line_start":201,"line_end":236,"content":["def test_default_values() -> None:","    \"\"\"Test that default values work correctly.\"\"\"","    inp = OomGateInput(","        bars=1000,","        params=100,","        param_subsample_rate=0.5,","    )","    ","    assert inp.intents_per_bar == 2.0","    assert inp.bytes_per_intent_est == 64","    assert inp.ram_budget_bytes == 6_000_000_000  # 6GB","    ","    decision = decide_gate(inp)","    assert decision.decision in (\"PASS\", \"BLOCK\", \"AUTO_DOWNSAMPLE\")","    assert decision.estimated_bytes >= 0","    assert decision.ram_budget_bytes == inp.ram_budget_bytes","","","def test_decide_oom_action_returns_dict_schema() -> None:","    \"\"\"Test legacy decide_oom_action() returns dict schema.\"\"\"","    cfg = {\"bars\": 1000, \"params_total\": 100, \"param_subsample_rate\": 0.1}","    res = decide_oom_action(cfg, mem_limit_mb=10_000.0)","    ","    assert isinstance(res, dict)","    assert res[\"action\"] in {\"PASS\", \"BLOCK\", \"AUTO_DOWNSAMPLE\"}","    assert \"estimated_bytes\" in res","    assert \"estimated_mb\" in res","    assert \"mem_limit_mb\" in res","    assert \"mem_limit_bytes\" in res","    assert \"original_subsample\" in res  # Contract key name","    assert \"final_subsample\" in res  # Contract key name","    assert \"params_total\" in res","    assert \"params_effective\" in res","    assert \"reason\" in res","",""]}
{"type":"file_footer","path":"tests/test_oom_gate.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_oom_gate_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7148,"sha256":"c5ed7f99bd6647a75d2c70617be6979781541f08c37790f4480df4a7fa1b2c95","total_lines":204,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_oom_gate_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for OOM gate.","","Tests verify:","1. Gate PASS when under limit","2. Gate BLOCK when over limit and no auto-downsample","3. Gate AUTO_DOWNSAMPLE when allowed","\"\"\"","","from __future__ import annotations","","import numpy as np","import pytest","","from core.oom_gate import decide_oom_action","from core.oom_cost_model import estimate_memory_bytes, summarize_estimates","","","def test_oom_gate_pass_when_under_limit():","    \"\"\"Test that gate PASSes when memory estimate is under limit.\"\"\"","    cfg = {","        \"bars\": 1000,","        \"params_total\": 100,","        \"param_subsample_rate\": 0.1,","        \"open_\": np.random.randn(1000).astype(np.float64),","        \"high\": np.random.randn(1000).astype(np.float64),","        \"low\": np.random.randn(1000).astype(np.float64),","        \"close\": np.random.randn(1000).astype(np.float64),","        \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","    }","    ","    # Use a very high limit to ensure PASS","    mem_limit_mb = 10000.0","    ","    result = decide_oom_action(cfg, mem_limit_mb=mem_limit_mb)","    ","    assert result[\"action\"] == \"PASS\"","    assert result[\"original_subsample\"] == 0.1","    assert result[\"final_subsample\"] == 0.1","    assert \"estimates\" in result","    assert result[\"estimates\"][\"mem_est_mb\"] <= mem_limit_mb","","","def test_oom_gate_block_when_over_limit_and_no_auto():","    \"\"\"Test that gate BLOCKs when over limit and auto-downsample is disabled.\"\"\"","    cfg = {","        \"bars\": 100000,","        \"params_total\": 10000,","        \"param_subsample_rate\": 1.0,","        \"open_\": np.random.randn(100000).astype(np.float64),","        \"high\": np.random.randn(100000).astype(np.float64),","        \"low\": np.random.randn(100000).astype(np.float64),","        \"close\": np.random.randn(100000).astype(np.float64),","        \"params_matrix\": np.random.randn(10000, 3).astype(np.float64),","    }","    ","    # Use a very low limit to ensure BLOCK","    mem_limit_mb = 1.0","    ","    result = decide_oom_action(","        cfg,","        mem_limit_mb=mem_limit_mb,","        allow_auto_downsample=False,","    )","    ","    assert result[\"action\"] == \"BLOCK\"","    assert result[\"original_subsample\"] == 1.0","    assert result[\"final_subsample\"] == 1.0  # Not changed","    assert \"reason\" in result","    assert \"mem_est_mb\" in result[\"reason\"] or \"limit\" in result[\"reason\"]","","","def test_oom_gate_auto_downsample_when_allowed(monkeypatch):","    \"\"\"Test that gate AUTO_DOWNSAMPLEs when allowed and over limit.\"\"\"","    # Monkeypatch estimate_memory_bytes to make it subsample-sensitive for testing","    def mock_estimate_memory_bytes(cfg, work_factor=2.0):","        \"\"\"Mock that makes memory estimate sensitive to subsample.\"\"\"","        bars = int(cfg.get(\"bars\", 0))","        params_total = int(cfg.get(\"params_total\", 0))","        subsample_rate = float(cfg.get(\"param_subsample_rate\", 1.0))","        params_effective = int(params_total * subsample_rate)","        ","        # Simplified: mem scales with bars and effective params","        base_mem = bars * 8 * 4  # 4 price arrays","        params_mem = params_effective * 3 * 8  # params_matrix","        total_mem = (base_mem + params_mem) * work_factor","        return int(total_mem)","    ","    monkeypatch.setattr(","        \"core.oom_cost_model.estimate_memory_bytes\",","        mock_estimate_memory_bytes,","    )","    ","    cfg = {","        \"bars\": 10000,","        \"params_total\": 1000,","        \"param_subsample_rate\": 0.5,  # Start at 50%","        \"open_\": np.random.randn(10000).astype(np.float64),","        \"high\": np.random.randn(10000).astype(np.float64),","        \"low\": np.random.randn(10000).astype(np.float64),","        \"close\": np.random.randn(10000).astype(np.float64),","        \"params_matrix\": np.random.randn(1000, 3).astype(np.float64),","    }","    ","    # Dynamic calculation: compute mem_mb for two subsample rates, use midpoint","    def _mem_mb(cfg_dict):","        b = mock_estimate_memory_bytes(cfg_dict, work_factor=2.0)","        return b / (1024.0 * 1024.0)","    ","    cfg_half = dict(cfg)","    cfg_half[\"param_subsample_rate\"] = 0.5","    cfg_quarter = dict(cfg)","    cfg_quarter[\"param_subsample_rate\"] = 0.25","    ","    mb_half = _mem_mb(cfg_half)  # ~0.633","    mb_quarter = _mem_mb(cfg_quarter)  # ~0.622","    ","    # Set limit between these two values → guaranteed to trigger AUTO_DOWNSAMPLE","    mem_limit_mb = (mb_half + mb_quarter) / 2.0","    ","    result = decide_oom_action(","        cfg,","        mem_limit_mb=mem_limit_mb,","        allow_auto_downsample=True,","        auto_downsample_step=0.5,","        auto_downsample_min=0.02,","    )","    ","    assert result[\"action\"] == \"AUTO_DOWNSAMPLE\"","    assert result[\"original_subsample\"] == 0.5","    assert result[\"final_subsample\"] < result[\"original_subsample\"]","    assert result[\"final_subsample\"] >= 0.02  # Above minimum","    assert \"reason\" in result","    assert \"auto-downsample\" in result[\"reason\"].lower()","    assert result[\"estimates\"][\"mem_est_mb\"] <= mem_limit_mb","","","def test_oom_gate_block_when_min_still_over_limit(monkeypatch):","    \"\"\"Test that gate BLOCKs when even at minimum subsample still over limit.\"\"\"","    def mock_estimate_memory_bytes(cfg, work_factor=2.0):","        \"\"\"Mock that always returns high memory.\"\"\"","        return 100 * 1024 * 1024  # Always 100MB","    ","    monkeypatch.setattr(","        \"core.oom_cost_model.estimate_memory_bytes\",","        mock_estimate_memory_bytes,","    )","    ","    cfg = {","        \"bars\": 1000,","        \"params_total\": 100,","        \"param_subsample_rate\": 0.5,","        \"open_\": np.random.randn(1000).astype(np.float64),","        \"high\": np.random.randn(1000).astype(np.float64),","        \"low\": np.random.randn(1000).astype(np.float64),","        \"close\": np.random.randn(1000).astype(np.float64),","        \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","    }","    ","    mem_limit_mb = 50.0  # Lower than mock estimate","    ","    result = decide_oom_action(","        cfg,","        mem_limit_mb=mem_limit_mb,","        allow_auto_downsample=True,","        auto_downsample_min=0.02,","    )","    ","    assert result[\"action\"] == \"BLOCK\"","    assert \"min_subsample\" in result[\"reason\"].lower() or \"still too large\" in result[\"reason\"].lower()","","","def test_oom_gate_result_schema():","    \"\"\"Test that gate result has correct schema.\"\"\"","    cfg = {","        \"bars\": 1000,","        \"params_total\": 100,","        \"param_subsample_rate\": 0.1,","        \"open_\": np.random.randn(1000).astype(np.float64),","        \"high\": np.random.randn(1000).astype(np.float64),","        \"low\": np.random.randn(1000).astype(np.float64),","        \"close\": np.random.randn(1000).astype(np.float64),","        \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","    }","    ","    result = decide_oom_action(cfg, mem_limit_mb=10000.0)","    ","    # Verify schema","    assert \"action\" in result","    assert result[\"action\"] in (\"PASS\", \"BLOCK\", \"AUTO_DOWNSAMPLE\")","    assert \"reason\" in result","    assert isinstance(result[\"reason\"], str)","    assert \"original_subsample\" in result","    assert \"final_subsample\" in result","    assert \"estimates\" in result","    ","    # Verify estimates structure","    estimates = result[\"estimates\"]","    assert \"mem_est_bytes\" in estimates","    assert \"mem_est_mb\" in estimates"]}
{"type":"file_chunk","path":"tests/test_oom_gate_contract.py","chunk_index":1,"line_start":201,"line_end":204,"content":["    assert \"ops_est\" in estimates","    assert \"time_est_s\" in estimates","",""]}
{"type":"file_footer","path":"tests/test_oom_gate_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_oom_gate_pure_function_hash_consistency.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3240,"sha256":"be3eeef3a13b904e34a116e8c388cfbf9a922625203a75b9bbe23e2ef8ec504c","total_lines":84,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_oom_gate_pure_function_hash_consistency.py","chunk_index":0,"line_start":1,"line_end":84,"content":["","\"\"\"Tests for OOM gate pure function hash consistency.","","Tests that decide_oom_action never mutates input cfg and returns new_cfg SSOT.","\"\"\"","","from __future__ import annotations","","import pytest","","from core.config_hash import stable_config_hash","from core.config_snapshot import make_config_snapshot","from core.oom_gate import decide_oom_action","","","def test_oom_gate_pure_function_hash_consistency(monkeypatch) -> None:","    \"\"\"","    Test that decide_oom_action is pure function (no mutation).","    ","    Uses monkeypatch to ensure subsample-sensitive memory estimation,","    guaranteeing that subsample=1.0 exceeds limit and subsample reduction","    triggers AUTO_DOWNSAMPLE.","    ","    Verifies:","    - Original cfg subsample remains unchanged","    - decision.new_cfg has modified subsample","    - Hash computed from new_cfg differs from original","    - manifest/snapshot records final_subsample correctly","    \"\"\"","    def mock_estimate_memory_bytes(cfg, work_factor=2.0):","        \"\"\"Make mem scale with subsample so AUTO_DOWNSAMPLE is meaningful.\"\"\"","        subsample = float(cfg.get(\"param_subsample_rate\", 1.0))","        # 100MB at subsample=1.0, 50MB at 0.5, etc.","        base = 100 * 1024 * 1024","        return int(base * subsample)","    ","    monkeypatch.setattr(","        \"core.oom_cost_model.estimate_memory_bytes\",","        mock_estimate_memory_bytes,","    )","    ","    cfg = {","        \"bars\": 1,","        \"params_total\": 1,","        \"param_subsample_rate\": 1.0,","    }","    mem_limit_mb = 60.0  # 1.0 => 100MB (over), 0.5 => 50MB (under)","    ","    decision = decide_oom_action(cfg, mem_limit_mb=mem_limit_mb, allow_auto_downsample=True)","    ","    # Verify original cfg unchanged","    assert cfg[\"param_subsample_rate\"] == 1.0, \"Original cfg must not be mutated\"","    ","    # Verify decision has new_cfg","    assert \"new_cfg\" in decision, \"decision must contain new_cfg\"","    new_cfg = decision[\"new_cfg\"]","    ","    # Lock behavior: allow_auto_downsample=True 時不得 PASS，必須 AUTO_DOWNSAMPLE（除非低於 min）","    assert decision[\"action\"] == \"AUTO_DOWNSAMPLE\", \"Should trigger AUTO_DOWNSAMPLE when allow_auto_downsample=True\"","    ","    # Verify new_cfg has modified subsample","    assert new_cfg[\"param_subsample_rate\"] < 1.0, \"new_cfg should have reduced subsample\"","    assert decision[\"final_subsample\"] < 1.0, \"final_subsample should be reduced\"","    assert decision[\"final_subsample\"] < decision[\"original_subsample\"], \"final_subsample must be < original_subsample\"","    assert decision[\"new_cfg\"][\"param_subsample_rate\"] == decision[\"final_subsample\"], \"new_cfg subsample must match final_subsample\"","    ","    # Verify hash consistency","    original_snapshot = make_config_snapshot(cfg)","    original_hash = stable_config_hash(original_snapshot)","    ","    new_snapshot = make_config_snapshot(new_cfg)","    new_hash = stable_config_hash(new_snapshot)","    ","    assert original_hash != new_hash, \"Hash should differ after subsample change\"","    ","    # Verify final_subsample matches new_cfg","    assert decision[\"final_subsample\"] == new_cfg[\"param_subsample_rate\"], (","        \"final_subsample must match new_cfg subsample\"","    )","    ","    # Verify original_subsample preserved","    assert decision[\"original_subsample\"] == 1.0, \"original_subsample must be preserved\"","",""]}
{"type":"file_footer","path":"tests/test_oom_gate_pure_function_hash_consistency.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_perf_breakdown_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":13001,"sha256":"04ef300ef6ef0142f998ddc47b64d6fc8e63d91a967d73098426a7e8fa303d40","total_lines":349,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_perf_breakdown_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Stage P2-1.8: Contract Tests for Granular Breakdown and Extended Observability","","Tests that verify:","- Granular timing keys exist and are non-negative floats","- Extended observability keys exist (entry/exit intents/fills totals)","- Accounting consistency (intents_total == entry + exit, fills_total == entry + exit)","- run_grid output contains timing keys in perf dict","\"\"\"","from __future__ import annotations","","import os","import numpy as np","","from strategy.kernel import run_kernel_arrays, DonchianAtrParams","from engine.types import BarArrays","from pipeline.runner_grid import run_grid","","","def test_perf_breakdown_keys_existence() -> None:","    \"\"\"","    D1: Contract test - Verify granular timing keys exist in _obs and are floats >= 0.0","    Also verify that t_total_kernel_s >= max(stage_times) for sanity check.","    ","    Contract: keys always exist, values always float >= 0.0.","    (When perf harness runs with profiling enabled, these will naturally become >0 real data.)","    \"\"\"","    import os","    # Ensure clean environment for test","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    # Task 2: Kernel profiling is optional - keys will always exist (may be 0.0 if not profiled)","    # We can optionally enable profiling to get real timing data, but it's not required for contract","    old_profile_kernel = os.environ.get(\"FISHBRO_PROFILE_KERNEL\")","    # Optionally enable profiling to get real timing values (not required - keys exist regardless)","    # Uncomment the line below if you want to test with profiling enabled:","    # os.environ[\"FISHBRO_PROFILE_KERNEL\"] = \"1\"","    ","    try:","        n_bars = 200","        warmup = 20","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        bars = BarArrays(","            open=open_.astype(np.float64),","            high=high.astype(np.float64),","            low=low.astype(np.float64),","            close=close.astype(np.float64),","        )","        ","        params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)","        ","        result = run_kernel_arrays(","            bars=bars,","            params=params,","            commission=0.0,","            slip=0.0,","            order_qty=1,","        )","        ","        # Verify _obs exists and contains timing keys","        assert \"_obs\" in result, \"_obs must exist in kernel result\"","        obs = result[\"_obs\"]","        assert isinstance(obs, dict), \"_obs must be a dict\"","        ","        # Required timing keys (now in _obs, not _perf)","        # Task 2: Contract - keys always exist, values always float >= 0.0","        timing_keys = [","            \"t_calc_indicators_s\",","            \"t_build_entry_intents_s\",","            \"t_simulate_entry_s\",","            \"t_calc_exits_s\",","            \"t_simulate_exit_s\",","            \"t_total_kernel_s\",","        ]","        ","        stage_times = []","        for key in timing_keys:","            assert key in obs, f\"{key} must exist in _obs (keys always exist, even if 0.0)\"","            value = obs[key]","            assert isinstance(value, float), f\"{key} must be float, got {type(value)}\"","            assert value >= 0.0, f\"{key} must be >= 0.0, got {value}\"","            if key != \"t_total_kernel_s\":","                stage_times.append(value)","        ","        # Sanity check: total time should be >= max of individual stage times","        # (allowing some overhead for timer calls and other operations)","        # Note: This check only makes sense if profiling was enabled (values > 0)","        t_total = obs[\"t_total_kernel_s\"]","        if stage_times and t_total > 0.0:","            max_stage = max(stage_times)","            # Allow equality or small overhead","            assert t_total >= max_stage, (","                f\"t_total_kernel_s ({t_total}) should be >= max(stage_times) ({max_stage})\"","            )","    finally:","        # Restore environment","        # restore trigger rate","        if old_trigger_rate is None:","            os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","        ","        # restore kernel profiling flag","        if old_profile_kernel is None:","            os.environ.pop(\"FISHBRO_PROFILE_KERNEL\", None)","        else:","            os.environ[\"FISHBRO_PROFILE_KERNEL\"] = old_profile_kernel","","","def test_extended_observability_keys_existence() -> None:","    \"\"\"","    D1: Contract test - Verify extended observability keys exist in _obs","    \"\"\"","    import os","    # Ensure clean environment for test","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    ","    try:","        n_bars = 200","        warmup = 20","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        bars = BarArrays(","            open=open_.astype(np.float64),","            high=high.astype(np.float64),","            low=low.astype(np.float64),","            close=close.astype(np.float64),","        )","        ","        params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)","        ","        result = run_kernel_arrays(","            bars=bars,","            params=params,","            commission=0.0,","            slip=0.0,","            order_qty=1,","        )","        ","        # Verify _obs exists and contains extended keys","        assert \"_obs\" in result, \"_obs must exist in kernel result\"","        obs = result[\"_obs\"]","        assert isinstance(obs, dict), \"_obs must be a dict\"","        ","        # Required observability keys","        obs_keys = [","            \"entry_intents_total\",","            \"entry_fills_total\",","            \"exit_intents_total\",","            \"exit_fills_total\",","        ]","        ","        for key in obs_keys:","            assert key in obs, f\"{key} must exist in _obs\"","            value = obs[key]","            assert isinstance(value, int), f\"{key} must be int, got {type(value)}\"","            assert value >= 0, f\"{key} must be >= 0, got {value}\"","    finally:","        # Restore environment","        if old_trigger_rate is not None:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","","","def test_accounting_consistency() -> None:","    \"\"\"","    D2: Contract test - Verify accounting consistency","    intents_total == entry_intents_total + exit_intents_total","    fills_total == entry_fills_total + exit_fills_total","    Also verify entry_intents_total == valid_mask_sum in arrays mode","    \"\"\"","    import os","    # Ensure clean environment for test","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    ","    try:","        n_bars = 200","        warmup = 20","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))"]}
{"type":"file_chunk","path":"tests/test_perf_breakdown_contract.py","chunk_index":1,"line_start":201,"line_end":349,"content":["        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        bars = BarArrays(","            open=open_.astype(np.float64),","            high=high.astype(np.float64),","            low=low.astype(np.float64),","            close=close.astype(np.float64),","        )","        ","        params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)","        ","        result = run_kernel_arrays(","            bars=bars,","            params=params,","            commission=0.0,","            slip=0.0,","            order_qty=1,","        )","        ","        obs = result[\"_obs\"]","        ","        # Verify intents_total consistency","        intents_total = obs.get(\"intents_total\", 0)","        entry_intents_total = obs.get(\"entry_intents_total\", 0)","        exit_intents_total = obs.get(\"exit_intents_total\", 0)","        ","        assert intents_total == entry_intents_total + exit_intents_total, (","            f\"intents_total ({intents_total}) must equal \"","            f\"entry_intents_total ({entry_intents_total}) + exit_intents_total ({exit_intents_total})\"","        )","        ","        # Verify fills_total consistency","        fills_total = obs.get(\"fills_total\", 0)","        entry_fills_total = obs.get(\"entry_fills_total\", 0)","        exit_fills_total = obs.get(\"exit_fills_total\", 0)","        ","        assert fills_total == entry_fills_total + exit_fills_total, (","            f\"fills_total ({fills_total}) must equal \"","            f\"entry_fills_total ({entry_fills_total}) + exit_fills_total ({exit_fills_total})\"","        )","        ","        # Verify entry_intents_total == valid_mask_sum (arrays mode contract)","        if \"valid_mask_sum\" in obs and \"entry_intents_total\" in obs:","            valid_mask_sum = obs.get(\"valid_mask_sum\", 0)","            entry_intents = obs.get(\"entry_intents_total\", 0)","            assert entry_intents == valid_mask_sum, (","                f\"entry_intents_total ({entry_intents}) must equal valid_mask_sum ({valid_mask_sum})\"","            )","    finally:","        # Restore environment","        if old_trigger_rate is not None:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","","","def test_run_grid_perf_contains_timing_keys(monkeypatch) -> None:","    \"\"\"","    Contract test - Verify run_grid output contains timing keys in perf dict.","    This ensures timing aggregation works correctly at grid level.","    \"\"\"","    # Task 1: Explicitly enable kernel profiling (required for timing collection)","    old_profile_kernel = os.environ.get(\"FISHBRO_PROFILE_KERNEL\")","    os.environ[\"FISHBRO_PROFILE_KERNEL\"] = \"1\"","    ","    # Enable profile mode to ensure timing collection","    monkeypatch.setenv(\"FISHBRO_PROFILE_GRID\", \"1\")","    ","    try:","        n_bars = 200","        n_params = 5","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate minimal params","        params = np.array([","            [20, 10, 1.0],","            [25, 12, 1.5],","            [30, 15, 2.0],","            [35, 18, 1.0],","            [40, 20, 1.5],","        ], dtype=np.float64)","        ","        result = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=False,","        )","        ","        # Verify perf dict exists","        assert \"perf\" in result, \"perf must exist in run_grid result\"","        perf = result[\"perf\"]","        assert isinstance(perf, dict), \"perf must be a dict\"","        ","        # Stage P2-2 Step A: Required micro-profiling timing keys (aggregated across params)","        # Task 2: Since profile is enabled, timing keys must exist","        timing_keys = [","            \"t_ind_donchian_s\",","            \"t_ind_atr_s\",","            \"t_build_entry_intents_s\",","            \"t_simulate_entry_s\",","            \"t_calc_exits_s\",","            \"t_simulate_exit_s\",","            \"t_total_kernel_s\",","        ]","        ","        for key in timing_keys:","            assert key in perf, f\"{key} must exist in perf dict when profile is enabled\"","            value = perf[key]","            assert isinstance(value, float), f\"{key} must be float, got {type(value)}\"","            assert value >= 0.0, f\"{key} must be >= 0.0, got {value}\"","        ","        # Stage P2-2 Step A: Memoization potential assessment keys","        unique_keys = [","            \"unique_channel_len_count\",","            \"unique_atr_len_count\",","            \"unique_ch_atr_pair_count\",","        ]","        ","        for key in unique_keys:","            assert key in perf, f\"{key} must exist in perf dict\"","            value = perf[key]","            assert isinstance(value, int), f\"{key} must be int, got {type(value)}\"","            assert value >= 1, f\"{key} must be >= 1, got {value}\"","    finally:","        # Task 1: Restore FISHBRO_PROFILE_KERNEL environment variable","        if old_profile_kernel is None:","            os.environ.pop(\"FISHBRO_PROFILE_KERNEL\", None)","        else:","            os.environ[\"FISHBRO_PROFILE_KERNEL\"] = old_profile_kernel","",""]}
{"type":"file_footer","path":"tests/test_perf_breakdown_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_perf_env_config_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3257,"sha256":"0cef44b16c3126aa66e42ab08dfa39c3c1cac14231f5e3f6bf24c8659e541f87","total_lines":91,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_perf_env_config_contract.py","chunk_index":0,"line_start":1,"line_end":91,"content":["","\"\"\"Test perf harness environment variable configuration contract.","","Ensures that FISHBRO_PERF_BARS and FISHBRO_PERF_PARAMS env vars are correctly parsed.","\"\"\"","","import os","import sys","from pathlib import Path","from unittest.mock import patch","","","def _get_perf_config():","    \"\"\"","    Helper to get perf config values by reading the script file.","    This avoids import issues with scripts/ module.","    \"\"\"","    script_path = Path(__file__).parent.parent / \"scripts\" / \"perf_grid.py\"","    ","    # Read and parse the constants","    with open(script_path, \"r\", encoding=\"utf-8\") as f:","        content = f.read()","    ","    # Extract default values from the file","    # Look for: TIER_JIT_BARS = int(os.environ.get(\"FISHBRO_PERF_BARS\", \"20000\"))","    import re","    ","    bars_match = re.search(r'TIER_JIT_BARS\\s*=\\s*int\\(os\\.environ\\.get\\(\"FISHBRO_PERF_BARS\",\\s*\"(\\d+)\"\\)\\)', content)","    params_match = re.search(r'TIER_JIT_PARAMS\\s*=\\s*int\\(os\\.environ\\.get\\(\"FISHBRO_PERF_PARAMS\",\\s*\"(\\d+)\"\\)\\)', content)","    ","    default_bars = int(bars_match.group(1)) if bars_match else None","    default_params = int(params_match.group(1)) if params_match else None","    ","    return default_bars, default_params","","","def test_perf_env_bars_parsing():","    \"\"\"Test that FISHBRO_PERF_BARS env var is correctly parsed.\"\"\"","    with patch.dict(os.environ, {\"FISHBRO_PERF_BARS\": \"50000\"}, clear=False):","        # Simulate the parsing logic","        bars = int(os.environ.get(\"FISHBRO_PERF_BARS\", \"20000\"))","        assert bars == 50000","","","def test_perf_env_params_parsing():","    \"\"\"Test that FISHBRO_PERF_PARAMS env var is correctly parsed.\"\"\"","    with patch.dict(os.environ, {\"FISHBRO_PERF_PARAMS\": \"5000\"}, clear=False):","        # Simulate the parsing logic","        params = int(os.environ.get(\"FISHBRO_PERF_PARAMS\", \"1000\"))","        assert params == 5000","","","def test_perf_env_both_parsing():","    \"\"\"Test that both env vars can be set simultaneously.\"\"\"","    with patch.dict(os.environ, {","        \"FISHBRO_PERF_BARS\": \"30000\",","        \"FISHBRO_PERF_PARAMS\": \"3000\",","    }, clear=False):","        bars = int(os.environ.get(\"FISHBRO_PERF_BARS\", \"20000\"))","        params = int(os.environ.get(\"FISHBRO_PERF_PARAMS\", \"1000\"))","        ","        assert bars == 30000","        assert params == 3000","","","def test_perf_env_defaults():","    \"\"\"Test that defaults are baseline (20000×1000) when env vars are not set.\"\"\"","    # Ensure env vars are not set for this test","    env_backup = {}","    for key in [\"FISHBRO_PERF_BARS\", \"FISHBRO_PERF_PARAMS\"]:","        if key in os.environ:","            env_backup[key] = os.environ[key]","            del os.environ[key]","    ","    try:","        # Check defaults match baseline","        default_bars, default_params = _get_perf_config()","        assert default_bars == 20000, f\"Expected default bars=20000, got {default_bars}\"","        assert default_params == 1000, f\"Expected default params=1000, got {default_params}\"","        ","        # Verify parsing logic uses defaults","        bars = int(os.environ.get(\"FISHBRO_PERF_BARS\", \"20000\"))","        params = int(os.environ.get(\"FISHBRO_PERF_PARAMS\", \"1000\"))","        assert bars == 20000","        assert params == 1000","    finally:","        # Restore env vars","        for key, value in env_backup.items():","            os.environ[key] = value","",""]}
{"type":"file_footer","path":"tests/test_perf_env_config_contract.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_perf_evidence_chain.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2927,"sha256":"ff79bead60c0d830feeadeebdeb8a55ce716ed756542dbb8fce97f7a6cd89702","total_lines":85,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_perf_evidence_chain.py","chunk_index":0,"line_start":1,"line_end":85,"content":["","from __future__ import annotations","","import numpy as np","","from pipeline.runner_grid import run_grid","","","def test_perf_evidence_chain_exists() -> None:","    \"\"\"","    Phase 3.0-D: Contract Test - Evidence Chain Existence","    ","    Purpose: Lock down that evidence fields always exist and are non-null.","    This test only verifies evidence existence, not timing or strategy quality.","    \"\"\"","    # Use minimal data: bars=50, params=3","    n_bars = 50","    n_params = 3","    ","    # Generate synthetic OHLC data","    rng = np.random.default_rng(42)","    close = 100.0 + np.cumsum(rng.standard_normal(n_bars)).astype(np.float64)","    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","    open_ = (high + low) / 2 + rng.standard_normal(n_bars) * 0.5","    ","    # Ensure high >= max(open, close) and low <= min(open, close)","    high = np.maximum(high, np.maximum(open_, close))","    low = np.minimum(low, np.minimum(open_, close))","    ","    # Generate minimal params: [channel_len, atr_len, stop_mult]","    params = np.array(","        [","            [10, 5, 1.0],","            [15, 7, 1.5],","            [20, 10, 2.0],","        ],","        dtype=np.float64,","    )","    ","    # Run grid runner (array path)","    # Note: perf field is always present in runner output (Phase 3.0-B)","    out = run_grid(","        open_=open_,","        high=high,","        low=low,","        close=close,","        params_matrix=params,","        commission=0.0,","        slip=0.0,","        order_qty=1,","        sort_params=False,","    )","    ","    # Verify perf field exists","    assert \"perf\" in out, \"perf field must exist in runner output\"","    perf = out[\"perf\"]","    assert isinstance(perf, dict), \"perf must be a dict\"","    ","    # Phase 3.0-D: Assert evidence fields exist and are non-null","    # 1. intent_mode must be \"arrays\"","    assert \"intent_mode\" in perf, \"intent_mode must exist in perf\"","    assert perf[\"intent_mode\"] == \"arrays\", (","        f\"intent_mode expected 'arrays' but got '{perf['intent_mode']}'\"","    )","    ","    # 2. intents_total must exist, be non-null, and > 0","    assert \"intents_total\" in perf, \"intents_total must exist in perf\"","    assert perf[\"intents_total\"] is not None, \"intents_total must not be None\"","    assert isinstance(perf[\"intents_total\"], (int, np.integer)), (","        f\"intents_total must be an integer, got {type(perf['intents_total'])}\"","    )","    assert int(perf[\"intents_total\"]) > 0, (","        f\"intents_total must be > 0, got {perf['intents_total']}\"","    )","    ","    # 3. fills_total must exist and be non-null (can be 0, but not None)","    assert \"fills_total\" in perf, \"fills_total must exist in perf\"","    assert perf[\"fills_total\"] is not None, \"fills_total must not be None\"","    assert isinstance(perf[\"fills_total\"], (int, np.integer)), (","        f\"fills_total must be an integer, got {type(perf['fills_total'])}\"","    )","    # fills_total can be 0 (no trades), but must not be None","",""]}
{"type":"file_footer","path":"tests/test_perf_evidence_chain.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_perf_grid_profile_report.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":619,"sha256":"d04f3674e1625e436dda8058520c945b739a3872961db35df796befbe0fd07da","total_lines":30,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_perf_grid_profile_report.py","chunk_index":0,"line_start":1,"line_end":30,"content":["","from __future__ import annotations","","import cProfile","","from perf.profile_report import _format_profile_report","","","def test_profile_report_markers_present() -> None:","    pr = cProfile.Profile()","    pr.enable()","    _ = sum(range(10_000))  # tiny workload, deterministic","    pr.disable()","    report = _format_profile_report(","        lane_id=\"3\",","        n_bars=2000,","        n_params=100,","        jit_enabled=True,","        sort_params=False,","        topn=10,","        mode=\"\",","        pr=pr,","    )","    assert \"__PROFILE_START__\" in report","    assert \"pstats sort: cumtime\" in report","    assert \"__PROFILE_END__\" in report","","","",""]}
{"type":"file_footer","path":"tests/test_perf_grid_profile_report.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_perf_obs_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6538,"sha256":"fddec6aa753e3a538f4844c59f7b4706ee5d89590050e4d8870f752835875a38","total_lines":171,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_perf_obs_contract.py","chunk_index":0,"line_start":1,"line_end":171,"content":["","\"\"\"Contract tests for perf observability (Stage P2-1.5).","","These tests ensure that entry sparse observability fields are correctly","propagated from kernel to perf JSON output.","\"\"\"","","import numpy as np","import pytest","","from pipeline.runner_grid import run_grid","","","def test_perf_obs_entry_sparse_fields():","    \"\"\"","    Contract: perf dict must contain entry sparse observability fields.","    ","    This test directly calls run_grid (no subprocess) to verify that:","    1. entry_valid_mask_sum is present in perf dict","    2. entry_intents_total is present in perf dict","    3. entry_valid_mask_sum == entry_intents_total (contract)","    4. entry_intents_per_bar_avg is correctly calculated","    \"\"\"","    # Generate small synthetic data (fast test)","    n_bars = 2000","    n_params = 50","    ","    rng = np.random.default_rng(42)","    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10","    high = close + np.abs(rng.standard_normal(n_bars)) * 5","    low = close - np.abs(rng.standard_normal(n_bars)) * 5","    open_ = (high + low) / 2 + rng.standard_normal(n_bars)","    ","    high = np.maximum(high, np.maximum(open_, close))","    low = np.minimum(low, np.minimum(open_, close))","    ","    # Generate params matrix (channel_len, atr_len, stop_mult)","    params_matrix = np.column_stack([","        np.random.randint(10, 30, size=n_params),  # channel_len","        np.random.randint(5, 20, size=n_params),   # atr_len","        np.random.uniform(1.0, 2.0, size=n_params),  # stop_mult","    ]).astype(np.float64)","    ","    # Call run_grid (will use arrays mode by default)","    result = run_grid(","        open_=open_,","        high=high,","        low=low,","        close=close,","        params_matrix=params_matrix,","        commission=0.0,","        slip=0.0,","        order_qty=1,","        sort_params=False,","    )","    ","    # Verify result structure","    assert \"perf\" in result, \"result must contain 'perf' dict\"","    perf = result[\"perf\"]","    assert isinstance(perf, dict), \"perf must be a dict\"","    ","    # Verify entry sparse observability fields exist","    assert \"entry_valid_mask_sum\" in perf, (","        \"perf must contain 'entry_valid_mask_sum' field\"","    )","    assert \"entry_intents_total\" in perf, (","        \"perf must contain 'entry_intents_total' field\"","    )","    ","    entry_valid_mask_sum = perf[\"entry_valid_mask_sum\"]","    entry_intents_total = perf[\"entry_intents_total\"]","    ","    # Verify types","    assert isinstance(entry_valid_mask_sum, (int, np.integer)), (","        f\"entry_valid_mask_sum must be int, got {type(entry_valid_mask_sum)}\"","    )","    assert isinstance(entry_intents_total, (int, np.integer)), (","        f\"entry_intents_total must be int, got {type(entry_intents_total)}\"","    )","    ","    # Contract: entry_valid_mask_sum == entry_intents_total","    assert entry_valid_mask_sum == entry_intents_total, (","        f\"entry_valid_mask_sum ({entry_valid_mask_sum}) must equal \"","        f\"entry_intents_total ({entry_intents_total})\"","    )","    ","    # Verify entry_intents_per_bar_avg if present","    if \"entry_intents_per_bar_avg\" in perf:","        entry_intents_per_bar_avg = perf[\"entry_intents_per_bar_avg\"]","        assert isinstance(entry_intents_per_bar_avg, (float, np.floating)), (","            f\"entry_intents_per_bar_avg must be float, got {type(entry_intents_per_bar_avg)}\"","        )","        ","        # Verify calculation: entry_intents_per_bar_avg == entry_intents_total / n_bars","        expected_avg = entry_intents_total / n_bars","        assert abs(entry_intents_per_bar_avg - expected_avg) <= 1e-12, (","            f\"entry_intents_per_bar_avg ({entry_intents_per_bar_avg}) must equal \"","            f\"entry_intents_total / n_bars ({expected_avg})\"","        )","    ","    # Verify intents_total_reported is present (preserves original)","    if \"intents_total_reported\" in perf:","        intents_total_reported = perf[\"intents_total_reported\"]","        assert isinstance(intents_total_reported, (int, np.integer)), (","            f\"intents_total_reported must be int, got {type(intents_total_reported)}\"","        )","        # intents_total_reported should equal original intents_total","        if \"intents_total\" in perf:","            assert intents_total_reported == perf[\"intents_total\"], (","                f\"intents_total_reported ({intents_total_reported}) should equal \"","                f\"intents_total ({perf['intents_total']})\"","            )","","","def test_perf_obs_entry_sparse_non_zero():","    \"\"\"","    Contract: With valid data, entry sparse fields should be non-zero.","    ","    This ensures that sparse masking is actually working and producing","    observable results.","    \"\"\"","    # Generate data that should produce some valid intents","    n_bars = 1000","    n_params = 20","    ","    rng = np.random.default_rng(42)","    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10","    high = close + np.abs(rng.standard_normal(n_bars)) * 5","    low = close - np.abs(rng.standard_normal(n_bars)) * 5","    open_ = (high + low) / 2 + rng.standard_normal(n_bars)","    ","    high = np.maximum(high, np.maximum(open_, close))","    low = np.minimum(low, np.minimum(open_, close))","    ","    # Use reasonable params (should produce valid donch_hi)","    params_matrix = np.column_stack([","        np.full(n_params, 20, dtype=np.float64),  # channel_len = 20","        np.full(n_params, 14, dtype=np.float64),  # atr_len = 14","        np.full(n_params, 2.0, dtype=np.float64),  # stop_mult = 2.0","    ])","    ","    result = run_grid(","        open_=open_,","        high=high,","        low=low,","        close=close,","        params_matrix=params_matrix,","        commission=0.0,","        slip=0.0,","        order_qty=1,","        sort_params=False,","    )","    ","    perf = result.get(\"perf\", {})","    if \"entry_valid_mask_sum\" in perf and \"entry_intents_total\" in perf:","        entry_valid_mask_sum = perf[\"entry_valid_mask_sum\"]","        entry_intents_total = perf[\"entry_intents_total\"]","        ","        # With valid data and reasonable params, we should have some intents","        # (but allow for edge cases where all are filtered)","        assert entry_valid_mask_sum >= 0, \"entry_valid_mask_sum must be non-negative\"","        assert entry_intents_total >= 0, \"entry_intents_total must be non-negative\"","        ","        # With n_bars=1000 and channel_len=20, we should have some valid intents","        # after warmup (at least a few)","        if n_bars > 100:  # Only check if we have enough bars","            # Conservative: allow for edge cases but expect some intents","            # In practice, with valid data, we should have >> 0","            pass  # Just verify non-negative, don't enforce minimum","",""]}
{"type":"file_footer","path":"tests/test_perf_obs_contract.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_perf_trigger_rate_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9899,"sha256":"db87bda10984645f730e091005977713c33470ccec2948cf406c29e0d9e67a85","total_lines":315,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_perf_trigger_rate_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Stage P2-1.6: Contract Tests for Trigger Rate Masking","","Tests that verify trigger_rate control works correctly:","- entry_intents_total scales linearly with trigger_rate","- entry_valid_mask_sum == entry_intents_total","- Deterministic behavior (same seed → same result)","\"\"\"","from __future__ import annotations","","import numpy as np","import os","","from perf.scenario_control import apply_trigger_rate_mask","","","def test_trigger_rate_mask_rate_1_0_no_change() -> None:","    \"\"\"","    Test that trigger_rate=1.0 preserves all valid triggers unchanged.","    \"\"\"","    n_bars = 2000","    warmup = 100","    ","    # Create trigger array: warmup period NaN, rest are valid positive values","    trigger = np.full(n_bars, np.nan, dtype=np.float64)","    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    # Apply mask with rate=1.0","    masked = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=1.0,","        warmup=warmup,","        seed=42,","    )","    ","    # Should be unchanged","    assert np.array_equal(trigger, masked, equal_nan=True), (","        \"trigger_rate=1.0 should not change trigger array\"","    )","","","def test_trigger_rate_mask_rate_0_05_approximately_5_percent() -> None:","    \"\"\"","    Test that trigger_rate=0.05 results in approximately 5% of valid triggers.","    Allows ±20% relative error to account for random fluctuations.","    \"\"\"","    n_bars = 2000","    warmup = 100","    n_valid_expected = n_bars - warmup  # Valid positions after warmup","    ","    # Create trigger array: warmup period NaN, rest are valid positive values","    trigger = np.full(n_bars, np.nan, dtype=np.float64)","    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    # Apply mask with rate=0.05","    masked = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=0.05,","        warmup=warmup,","        seed=42,","    )","    ","    # Count valid (finite) positions after warmup","    valid_after_warmup = np.isfinite(masked[warmup:])","    n_valid_actual = int(np.sum(valid_after_warmup))","    ","    # Expected: approximately 5% of valid positions","    expected_min = int(n_valid_expected * 0.05 * 0.8)  # 80% of 5% (lower bound)","    expected_max = int(n_valid_expected * 0.05 * 1.2)  # 120% of 5% (upper bound)","    ","    assert expected_min <= n_valid_actual <= expected_max, (","        f\"Expected ~5% valid triggers ({expected_min}-{expected_max}), \"","        f\"got {n_valid_actual} ({n_valid_actual/n_valid_expected*100:.2f}%)\"","    )","","","def test_trigger_rate_mask_deterministic() -> None:","    \"\"\"","    Test that same seed and same input produce identical mask results.","    \"\"\"","    n_bars = 2000","    warmup = 100","    ","    # Create trigger array","    trigger = np.full(n_bars, np.nan, dtype=np.float64)","    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    # Apply mask twice with same parameters","    masked1 = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=0.05,","        warmup=warmup,","        seed=42,","    )","    ","    masked2 = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=0.05,","        warmup=warmup,","        seed=42,","    )","    ","    # Should be identical","    assert np.array_equal(masked1, masked2, equal_nan=True), (","        \"Same seed and input should produce identical mask results\"","    )","","","def test_trigger_rate_mask_different_seeds_different_results() -> None:","    \"\"\"","    Test that different seeds produce different mask results (when rate < 1.0).","    \"\"\"","    n_bars = 2000","    warmup = 100","    ","    # Create trigger array","    trigger = np.full(n_bars, np.nan, dtype=np.float64)","    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    # Apply mask with different seeds","    masked1 = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=0.05,","        warmup=warmup,","        seed=42,","    )","    ","    masked2 = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=0.05,","        warmup=warmup,","        seed=999,","    )","    ","    # Should be different (very unlikely to be identical with different seeds)","    assert not np.array_equal(masked1, masked2, equal_nan=True), (","        \"Different seeds should produce different mask results\"","    )","","","def test_trigger_rate_mask_preserves_warmup_nan() -> None:","    \"\"\"","    Test that warmup period NaN positions are preserved (not masked).","    \"\"\"","    n_bars = 2000","    warmup = 100","    ","    # Create trigger array: warmup period NaN, rest are valid","    trigger = np.full(n_bars, np.nan, dtype=np.float64)","    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    # Apply mask","    masked = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=0.05,","        warmup=warmup,","        seed=42,","    )","    ","    # Warmup period should remain NaN","    assert np.all(np.isnan(masked[:warmup])), (","        \"Warmup period should remain NaN after masking\"","    )","","","def test_trigger_rate_mask_linear_scaling() -> None:","    \"\"\"","    Test that valid trigger count scales approximately linearly with trigger_rate.","    \"\"\"","    n_bars = 2000","    warmup = 100","    n_valid_expected = n_bars - warmup","    ","    # Create trigger array","    trigger = np.full(n_bars, np.nan, dtype=np.float64)","    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    rates = [0.1, 0.3, 0.5, 0.7, 0.9]","    valid_counts = []","    ","    for rate in rates:","        masked = apply_trigger_rate_mask(","            trigger=trigger,","            trigger_rate=rate,","            warmup=warmup,","            seed=42,","        )","        n_valid = int(np.sum(np.isfinite(masked[warmup:])))","        valid_counts.append(n_valid)","    ","    # Check approximate linearity: valid_counts[i] / valid_counts[j] ≈ rates[i] / rates[j]","    # Use first and last as reference","    ratio_expected = rates[-1] / rates[0]  # 0.9 / 0.1 = 9.0","    ratio_actual = valid_counts[-1] / valid_counts[0] if valid_counts[0] > 0 else 0","    ","    # Allow ±30% error for random fluctuations","    assert 0.7 * ratio_expected <= ratio_actual <= 1.3 * ratio_expected, (","        f\"Valid counts should scale linearly with rate. \"","        f\"Expected ratio ~{ratio_expected:.2f}, got {ratio_actual:.2f}. \""]}
{"type":"file_chunk","path":"tests/test_perf_trigger_rate_contract.py","chunk_index":1,"line_start":201,"line_end":315,"content":["        f\"Counts: {valid_counts}\"","    )","","","def test_trigger_rate_mask_preserves_dtype() -> None:","    \"\"\"","    Test that masking preserves the input dtype.","    \"\"\"","    n_bars = 200","    warmup = 20","    ","    # Test with float64","    trigger_f64 = np.full(n_bars, np.nan, dtype=np.float64)","    trigger_f64[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    masked_f64 = apply_trigger_rate_mask(","        trigger=trigger_f64,","        trigger_rate=0.5,","        warmup=warmup,","        seed=42,","    )","    ","    assert masked_f64.dtype == np.float64, (","        f\"Expected float64, got {masked_f64.dtype}\"","    )","    ","    # Test with float32","    trigger_f32 = np.full(n_bars, np.nan, dtype=np.float32)","    trigger_f32[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float32)","    ","    masked_f32 = apply_trigger_rate_mask(","        trigger=trigger_f32,","        trigger_rate=0.5,","        warmup=warmup,","        seed=42,","    )","    ","    assert masked_f32.dtype == np.float32, (","        f\"Expected float32, got {masked_f32.dtype}\"","    )","","","def test_trigger_rate_mask_integration_with_kernel() -> None:","    \"\"\"","    Integration test: verify that trigger_rate affects entry_intents_total in run_kernel_arrays.","    This test uses run_kernel_arrays directly (no subprocess) to verify the integration.","    \"\"\"","    from strategy.kernel import run_kernel_arrays, DonchianAtrParams","    from engine.types import BarArrays","    ","    n_bars = 200","    warmup = 20","    ","    # Generate simple OHLC data","    rng = np.random.default_rng(42)","    close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","    open_ = (high + low) / 2","    ","    high = np.maximum(high, np.maximum(open_, close))","    low = np.minimum(low, np.minimum(open_, close))","    ","    bars = BarArrays(","        open=open_.astype(np.float64),","        high=high.astype(np.float64),","        low=low.astype(np.float64),","        close=close.astype(np.float64),","    )","    ","    params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)","    ","    # Test with trigger_rate=1.0 (baseline) - explicitly set to avoid env interference","    os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","    result_1_0 = run_kernel_arrays(","        bars=bars,","        params=params,","        commission=0.0,","        slip=0.0,","        order_qty=1,","    )","    ","    # Contract test: fail fast if keys missing (no .get() with defaults)","    entry_intents_1_0 = result_1_0[\"_obs\"][\"entry_intents_total\"]","    valid_mask_sum_1_0 = result_1_0[\"_obs\"][\"entry_valid_mask_sum\"]","    assert entry_intents_1_0 == valid_mask_sum_1_0","    ","    # Test with trigger_rate=0.5","    os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"0.5\"","    result_0_5 = run_kernel_arrays(","        bars=bars,","        params=params,","        commission=0.0,","        slip=0.0,","        order_qty=1,","    )","    ","    # Contract test: fail fast if keys missing (no .get() with defaults)","    entry_intents_0_5 = result_0_5[\"_obs\"][\"entry_intents_total\"]","    valid_mask_sum_0_5 = result_0_5[\"_obs\"][\"entry_valid_mask_sum\"]","    assert entry_intents_0_5 == valid_mask_sum_0_5","    ","    # Cleanup","    os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    ","    # Verify that entry_intents_0_5 is approximately 50% of entry_intents_1_0","    # Allow ±30% error for random fluctuations and warmup/NaN deterministic effects","    if entry_intents_1_0 > 0:","        ratio = entry_intents_0_5 / entry_intents_1_0","        assert 0.35 <= ratio <= 0.65, (","            f\"With trigger_rate=0.5, expected entry_intents ~50% of baseline, \"","            f\"got {ratio*100:.1f}% (baseline={entry_intents_1_0}, actual={entry_intents_0_5})\"","        )","",""]}
{"type":"file_footer","path":"tests/test_perf_trigger_rate_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_phase13_batch_submit.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6391,"sha256":"ef0f03299b63d60f0549962c7a467b4cd0c03fa61f113ef80fef76e2ef1d0059","total_lines":193,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase13_batch_submit.py","chunk_index":0,"line_start":1,"line_end":193,"content":["","\"\"\"Unit tests for batch_submit module (Phase 13).\"\"\"","","import pytest","from control.batch_submit import (","    BatchSubmitRequest,","    BatchSubmitResponse,","    compute_batch_id,","    wizard_to_db_jobspec,","    submit_batch,",")","from control.job_spec import WizardJobSpec, DataSpec, WFSSpec","from control.types import DBJobSpec","from datetime import date","","","def test_batch_submit_request():","    \"\"\"BatchSubmitRequest creation.\"\"\"","    jobs = [","        WizardJobSpec(","            season=\"2024Q1\",","            data1=DataSpec(dataset_id=\"test\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","            strategy_id=\"s1\",","            params={\"p\": 1},","            wfs=WFSSpec()","        ),","        WizardJobSpec(","            season=\"2024Q1\",","            data1=DataSpec(dataset_id=\"test\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","            strategy_id=\"s1\",","            params={\"p\": 2},","            wfs=WFSSpec()","        ),","    ]","    req = BatchSubmitRequest(jobs=jobs)","    assert len(req.jobs) == 2","    assert req.jobs[0].params[\"p\"] == 1","    assert req.jobs[1].params[\"p\"] == 2","","","def test_batch_submit_response():","    \"\"\"BatchSubmitResponse creation.\"\"\"","    resp = BatchSubmitResponse(","        batch_id=\"batch-123\",","        total_jobs=5,","        job_ids=[\"job1\", \"job2\", \"job3\", \"job4\", \"job5\"]","    )","    assert resp.batch_id == \"batch-123\"","    assert resp.total_jobs == 5","    assert len(resp.job_ids) == 5","","","def test_compute_batch_id_deterministic():","    \"\"\"Batch ID is deterministic based on sorted JobSpec JSON.\"\"\"","    jobs = [","        WizardJobSpec(","            season=\"2024Q1\",","            data1=DataSpec(dataset_id=\"test\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","            strategy_id=\"s1\",","            params={\"a\": 1, \"b\": 2},","            wfs=WFSSpec()","        ),","        WizardJobSpec(","            season=\"2024Q1\",","            data1=DataSpec(dataset_id=\"test\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","            strategy_id=\"s1\",","            params={\"a\": 3, \"b\": 4},","            wfs=WFSSpec()","        ),","    ]","    batch_id1 = compute_batch_id(jobs)","    # Same jobs, different order should produce same batch ID","    jobs_reversed = list(reversed(jobs))","    batch_id2 = compute_batch_id(jobs_reversed)","    assert batch_id1 == batch_id2","    # Different jobs produce different ID","    jobs2 = [jobs[0]]","    batch_id3 = compute_batch_id(jobs2)","    assert batch_id1 != batch_id3","","","def test_wizard_to_db_jobspec():","    \"\"\"Convert Wizard JobSpec to DB JobSpec.\"\"\"","    wizard_spec = WizardJobSpec(","        season=\"2024Q1\",","        data1=DataSpec(dataset_id=\"CME_MNQ_v2\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","        strategy_id=\"my_strategy\",","        params={\"param1\": 42},","        wfs=WFSSpec(stage0_subsample=0.5, top_k=100, mem_limit_mb=2048, allow_auto_downsample=True)","    )","    # Mock dataset record with fingerprint","    dataset_record = {","        \"fingerprint_sha256_40\": \"abc123def456ghi789jkl012mno345pqr678stu901\",","        \"normalized_sha256_40\": \"abc123def456ghi789jkl012mno345pqr678stu901\"","    }","    db_spec = wizard_to_db_jobspec(wizard_spec, dataset_record)","    assert isinstance(db_spec, DBJobSpec)","    assert db_spec.season == \"2024Q1\"","    assert db_spec.dataset_id == \"CME_MNQ_v2\"","    assert db_spec.outputs_root == \"outputs/seasons/2024Q1/runs\"","    # config_snapshot should contain params and wfs","    config = db_spec.config_snapshot","    assert config[\"params\"][\"param1\"] == 42","    assert config[\"wfs\"][\"stage0_subsample\"] == 0.5","    assert config[\"wfs\"][\"top_k\"] == 100","    # config_hash should be non-empty","    assert db_spec.config_hash","    assert db_spec.created_by == \"wizard_batch\"","    # fingerprint should be set","    assert db_spec.data_fingerprint_sha256_40 == \"abc123def456ghi789jkl012mno345pqr678stu901\"","","","def test_submit_batch_mocked(monkeypatch):","    \"\"\"Test submit_batch with mocked DB calls.\"\"\"","    # Mock create_job to return predictable job IDs","    job_ids = [\"job-a\", \"job-b\", \"job-c\"]","    call_count = 0","    def mock_create_job(db_path, spec):","        nonlocal call_count","        # Ensure spec is DBJobSpec","        assert isinstance(spec, DBJobSpec)","        # Return sequential ID","        result = job_ids[call_count]","        call_count += 1","        return result","    ","    import control.batch_submit as batch_module","    monkeypatch.setattr(batch_module, \"create_job\", mock_create_job)","    ","    # Prepare request","    jobs = [","        WizardJobSpec(","            season=\"2024Q1\",","            data1=DataSpec(dataset_id=\"test\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","            strategy_id=\"s1\",","            params={\"p\": i},","            wfs=WFSSpec()","        ) for i in range(3)","    ]","    req = BatchSubmitRequest(jobs=jobs)","    ","    # Mock dataset index","    dataset_index = {","        \"test\": {","            \"fingerprint_sha256_40\": \"abc123def456ghi789jkl012mno345pqr678stu901\",","            \"normalized_sha256_40\": \"abc123def456ghi789jkl012mno345pqr678stu901\"","        }","    }","    ","    # Call submit_batch with dummy db_path","    from pathlib import Path","    db_path = Path(\"/tmp/test.db\")","    resp = submit_batch(db_path, req, dataset_index)","    ","    assert resp.batch_id.startswith(\"batch-\")","    assert resp.total_jobs == 3","    assert resp.job_ids == job_ids","    assert call_count == 3","","","def test_submit_batch_empty_jobs():","    \"\"\"Empty jobs list raises.\"\"\"","    req = BatchSubmitRequest(jobs=[])","    from pathlib import Path","    db_path = Path(\"/tmp/test.db\")","    dataset_index = {\"test\": {\"fingerprint_sha256_40\": \"abc123\"}}","    with pytest.raises(ValueError, match=\"jobs list cannot be empty\"):","        submit_batch(db_path, req, dataset_index)","","","def test_submit_batch_too_many_jobs():","    \"\"\"Jobs exceed cap raises.\"\"\"","    jobs = [","        WizardJobSpec(","            season=\"2024Q1\",","            data1=DataSpec(dataset_id=\"test\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","            strategy_id=\"s1\",","            params={\"p\": i},","            wfs=WFSSpec()","        ) for i in range(1001)  # exceed default cap of 1000","    ]","    req = BatchSubmitRequest(jobs=jobs)","    from pathlib import Path","    db_path = Path(\"/tmp/test.db\")","    dataset_index = {\"test\": {\"fingerprint_sha256_40\": \"abc123\"}}","    with pytest.raises(ValueError, match=\"exceeds maximum\"):","        submit_batch(db_path, req, dataset_index)","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])","",""]}
{"type":"file_footer","path":"tests/test_phase13_batch_submit.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase13_job_expand.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6884,"sha256":"d1c964fc59f3ded9cd3f61f9e178170d704faf3a1e811b6bbc8016a55a232b51","total_lines":227,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_phase13_job_expand.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Unit tests for job_expand module (Phase 13).\"\"\"","","import pytest","from control.param_grid import GridMode, ParamGridSpec","from control.job_expand import JobTemplate, expand_job_template, estimate_total_jobs, validate_template","from control.job_spec import WFSSpec","","","def test_job_template_creation():","    \"\"\"JobTemplate creation and serialization.\"\"\"","    param_grid = {","        \"param1\": ParamGridSpec(mode=GridMode.SINGLE, single_value=10),","        \"param2\": ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=2, range_step=1),","    }","    wfs = WFSSpec(stage0_subsample=0.5, top_k=100, mem_limit_mb=2048, allow_auto_downsample=True)","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"CME_MNQ_v2\",","        strategy_id=\"my_strategy\",","        param_grid=param_grid,","        wfs=wfs","    )","    assert template.season == \"2024Q1\"","    assert template.dataset_id == \"CME_MNQ_v2\"","    assert template.strategy_id == \"my_strategy\"","    assert len(template.param_grid) == 2","    assert template.wfs == wfs","","","def test_expand_job_template_single():","    \"\"\"Expand single parameter.\"\"\"","    param_grid = {","        \"p\": ParamGridSpec(mode=GridMode.SINGLE, single_value=42),","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    jobs = list(expand_job_template(template))","    assert len(jobs) == 1","    job = jobs[0]","    assert job.season == \"2024Q1\"","    assert job.dataset_id == \"test\"","    assert job.strategy_id == \"s\"","    assert job.params == {\"p\": 42}","","","def test_expand_job_template_range():","    \"\"\"Expand range parameter.\"\"\"","    param_grid = {","        \"p\": ParamGridSpec(mode=GridMode.RANGE, range_start=1, range_end=3, range_step=1),","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    jobs = list(expand_job_template(template))","    assert len(jobs) == 3","    values = [job.params[\"p\"] for job in jobs]","    assert values == [1, 2, 3]","    # Order should be deterministic (sorted by param name, then values)","    assert jobs[0].params[\"p\"] == 1","    assert jobs[1].params[\"p\"] == 2","    assert jobs[2].params[\"p\"] == 3","","","def test_expand_job_template_multi():","    \"\"\"Expand multi values parameter.\"\"\"","    param_grid = {","        \"p\": ParamGridSpec(mode=GridMode.MULTI, multi_values=[\"a\", \"b\", \"c\"]),","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    jobs = list(expand_job_template(template))","    assert len(jobs) == 3","    values = [job.params[\"p\"] for job in jobs]","    assert values == [\"a\", \"b\", \"c\"]","","","def test_expand_job_template_two_params():","    \"\"\"Expand two parameters (cartesian product).\"\"\"","    param_grid = {","        \"p1\": ParamGridSpec(mode=GridMode.RANGE, range_start=1, range_end=2, range_step=1),","        \"p2\": ParamGridSpec(mode=GridMode.MULTI, multi_values=[\"x\", \"y\"]),","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    jobs = list(expand_job_template(template))","    assert len(jobs) == 4  # 2 * 2","    # Order: param names sorted alphabetically, then values","    # p1 values: 1,2 ; p2 values: x,y","    # Expected order: (p1=1, p2=x), (p1=1, p2=y), (p1=2, p2=x), (p1=2, p2=y)","    expected = [","        {\"p1\": 1, \"p2\": \"x\"},","        {\"p1\": 1, \"p2\": \"y\"},","        {\"p1\": 2, \"p2\": \"x\"},","        {\"p1\": 2, \"p2\": \"y\"},","    ]","    for i, job in enumerate(jobs):","        assert job.params == expected[i]","","","def test_estimate_total_jobs():","    \"\"\"Estimate total jobs count.\"\"\"","    param_grid = {","        \"p1\": ParamGridSpec(mode=GridMode.RANGE, range_start=1, range_end=10, range_step=1),  # 10 values","        \"p2\": ParamGridSpec(mode=GridMode.MULTI, multi_values=[\"a\", \"b\", \"c\"]),  # 3 values","        \"p3\": ParamGridSpec(mode=GridMode.SINGLE, single_value=99),  # 1 value","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    total = estimate_total_jobs(template)","    assert total == 10 * 3 * 1  # 30","","","def test_validate_template_ok():","    \"\"\"Valid template passes.\"\"\"","    param_grid = {","        \"p\": ParamGridSpec(mode=GridMode.SINGLE, single_value=5),","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    validate_template(template)  # no exception","","","def test_validate_template_empty_param_grid():","    \"\"\"Empty param grid raises.\"\"\"","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid={},","        wfs=WFSSpec()","    )","    with pytest.raises(ValueError, match=\"param_grid cannot be empty\"):","        validate_template(template)","","","def test_validate_template_missing_season():","    \"\"\"Missing season raises.\"\"\"","    param_grid = {\"p\": ParamGridSpec(mode=GridMode.SINGLE, single_value=1)}","    template = JobTemplate(","        season=\"\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    with pytest.raises(ValueError, match=\"season must be non-empty\"):","        validate_template(template)","","","def test_validate_template_missing_dataset_id():","    \"\"\"Missing dataset_id raises.\"\"\"","    param_grid = {\"p\": ParamGridSpec(mode=GridMode.SINGLE, single_value=1)}","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    with pytest.raises(ValueError, match=\"dataset_id must be non-empty\"):","        validate_template(template)","","","def test_validate_template_missing_strategy_id():","    \"\"\"Missing strategy_id raises.\"\"\"","    param_grid = {\"p\": ParamGridSpec(mode=GridMode.SINGLE, single_value=1)}","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"\","]}
{"type":"file_chunk","path":"tests/test_phase13_job_expand.py","chunk_index":1,"line_start":201,"line_end":227,"content":["        param_grid=param_grid,","        wfs=WFSSpec()","    )","    with pytest.raises(ValueError, match=\"strategy_id must be non-empty\"):","        validate_template(template)","","","def test_validate_template_param_grid_invalid():","    \"\"\"ParamGrid validation errors propagate.\"\"\"","    param_grid = {","        \"p\": ParamGridSpec(mode=GridMode.RANGE, range_start=10, range_end=0, range_step=1),  # invalid","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    with pytest.raises(ValueError, match=\"start <= end\"):","        validate_template(template)","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])","",""]}
{"type":"file_footer","path":"tests/test_phase13_job_expand.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_phase13_param_grid.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5128,"sha256":"ac31fc7b55401f4806325c144e926ef0c53796ab5d2f85a103cc073a62027fac","total_lines":146,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase13_param_grid.py","chunk_index":0,"line_start":1,"line_end":146,"content":["","\"\"\"Unit tests for param_grid module (Phase 13).\"\"\"","","import pytest","from control.param_grid import GridMode, ParamGridSpec, values_for_param, count_for_param, validate_grid_for_param","","","def test_grid_mode_enum():","    \"\"\"GridMode enum values.\"\"\"","    assert GridMode.SINGLE.value == \"single\"","    assert GridMode.RANGE.value == \"range\"","    assert GridMode.MULTI.value == \"multi\"","","","def test_param_grid_spec_single():","    \"\"\"Single mode spec.\"\"\"","    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=42)","    assert spec.mode == GridMode.SINGLE","    assert spec.single_value == 42","    assert spec.range_start is None","    assert spec.range_end is None","    assert spec.range_step is None","    assert spec.multi_values is None","","","def test_param_grid_spec_range():","    \"\"\"Range mode spec.\"\"\"","    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=10, range_step=2)","    assert spec.mode == GridMode.RANGE","    assert spec.range_start == 0","    assert spec.range_end == 10","    assert spec.range_step == 2","    assert spec.single_value is None","    assert spec.multi_values is None","","","def test_param_grid_spec_multi():","    \"\"\"Multi mode spec.\"\"\"","    spec = ParamGridSpec(mode=GridMode.MULTI, multi_values=[1, 2, 3])","    assert spec.mode == GridMode.MULTI","    assert spec.multi_values == [1, 2, 3]","    assert spec.single_value is None","    assert spec.range_start is None","","","def test_values_for_param_single():","    \"\"\"Single mode yields single value.\"\"\"","    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=5.5)","    vals = list(values_for_param(spec))","    assert vals == [5.5]","","","def test_values_for_param_range_int():","    \"\"\"Range mode with integer step.\"\"\"","    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=5, range_step=1)","    vals = list(values_for_param(spec))","    assert vals == [0, 1, 2, 3, 4, 5]","","","def test_values_for_param_range_float():","    \"\"\"Range mode with float step.\"\"\"","    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=0.0, range_end=1.0, range_step=0.5)","    vals = list(values_for_param(spec))","    assert vals == [0.0, 0.5, 1.0]","","","def test_values_for_param_multi():","    \"\"\"Multi mode yields list of values.\"\"\"","    spec = ParamGridSpec(mode=GridMode.MULTI, multi_values=[\"a\", \"b\", \"c\"])","    vals = list(values_for_param(spec))","    assert vals == [\"a\", \"b\", \"c\"]","","","def test_count_for_param():","    \"\"\"Count of values.\"\"\"","    spec_single = ParamGridSpec(mode=GridMode.SINGLE, single_value=1)","    assert count_for_param(spec_single) == 1","    ","    spec_range = ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=10, range_step=2)","    # 0,2,4,6,8,10 => 6 values","    assert count_for_param(spec_range) == 6","    ","    spec_multi = ParamGridSpec(mode=GridMode.MULTI, multi_values=[1, 2, 3, 4])","    assert count_for_param(spec_multi) == 4","","","def test_validate_grid_for_param_single_ok():","    \"\"\"Single mode validation passes.\"\"\"","    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=100)","    validate_grid_for_param(spec, \"int\", min=0, max=200)","    # No exception","","","def test_validate_grid_for_param_single_out_of_range():","    \"\"\"Single mode value out of range raises.\"\"\"","    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=300)","    with pytest.raises(ValueError, match=\"out of range\"):","        validate_grid_for_param(spec, \"int\", min=0, max=200)","","","def test_validate_grid_for_param_range_invalid_step():","    \"\"\"Range mode with zero step raises.\"\"\"","    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=10, range_step=0)","    with pytest.raises(ValueError, match=\"step must be positive\"):","        validate_grid_for_param(spec, \"int\", min=0, max=100)","","","def test_validate_grid_for_param_range_start_gt_end():","    \"\"\"Range start > end raises.\"\"\"","    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=10, range_end=0, range_step=1)","    with pytest.raises(ValueError, match=\"start <= end\"):","        validate_grid_for_param(spec, \"int\", min=0, max=100)","","","def test_validate_grid_for_param_multi_empty():","    \"\"\"Multi mode with empty list raises.\"\"\"","    spec = ParamGridSpec(mode=GridMode.MULTI, multi_values=[])","    with pytest.raises(ValueError, match=\"at least one value\"):","        validate_grid_for_param(spec, \"int\", min=0, max=100)","","","def test_validate_grid_for_param_multi_duplicates():","    \"\"\"Multi mode with duplicates raises.\"\"\"","    spec = ParamGridSpec(mode=GridMode.MULTI, multi_values=[1, 2, 2, 3])","    with pytest.raises(ValueError, match=\"duplicate values\"):","        validate_grid_for_param(spec, \"int\", min=0, max=100)","","","def test_validate_grid_for_param_enum():","    \"\"\"Enum type validation passes if value in choices.\"\"\"","    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=\"buy\")","    validate_grid_for_param(spec, \"enum\", choices=[\"buy\", \"sell\", \"hold\"])","    # No exception","","","def test_validate_grid_for_param_enum_invalid():","    \"\"\"Enum value not in choices raises.\"\"\"","    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=\"invalid\")","    with pytest.raises(ValueError, match=\"not in choices\"):","        validate_grid_for_param(spec, \"enum\", choices=[\"buy\", \"sell\"])","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])","",""]}
{"type":"file_footer","path":"tests/test_phase13_param_grid.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase141_batch_status_summary.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4224,"sha256":"6bd14a2e209af2335351359162cd2335e646a957a3b2d2685384ce1f7ccd18a8","total_lines":124,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase141_batch_status_summary.py","chunk_index":0,"line_start":1,"line_end":124,"content":["","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","","","@pytest.fixture","def client():","    return TestClient(app)","","","def _write_json(p: Path, obj):","    p.parent.mkdir(parents=True, exist_ok=True)","    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")","","","def test_batch_status_reads_execution_json(client):","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        batch_id = \"batch1\"","","        # execution schema: jobs mapping","        _write_json(","            root / batch_id / \"execution.json\",","            {","                \"batch_state\": \"RUNNING\",","                \"jobs\": {","                    \"jobA\": {\"state\": \"SUCCESS\"},","                    \"jobB\": {\"state\": \"FAILED\"},","                    \"jobC\": {\"state\": \"RUNNING\"},","                },","            },","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            r = client.get(f\"/batches/{batch_id}/status\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"batch_id\"] == batch_id","            assert data[\"state\"] == \"RUNNING\"","            assert data[\"jobs_total\"] == 3","            assert data[\"jobs_done\"] == 1","            assert data[\"jobs_failed\"] == 1","","","def test_batch_status_missing_execution_json(client):","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            r = client.get(\"/batches/batchX/status\")","            assert r.status_code == 404","","","def test_batch_summary_reads_summary_json(client):","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        batch_id = \"batch1\"","        _write_json(","            root / batch_id / \"summary.json\",","            {\"topk\": [{\"job_id\": \"jobA\", \"score\": 1.23}], \"metrics\": {\"n\": 10}},","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            r = client.get(f\"/batches/{batch_id}/summary\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"batch_id\"] == batch_id","            assert isinstance(data[\"topk\"], list)","            assert data[\"topk\"][0][\"job_id\"] == \"jobA\"","            assert data[\"metrics\"][\"n\"] == 10","","","def test_batch_summary_missing_summary_json(client):","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            r = client.get(\"/batches/batchX/summary\")","            assert r.status_code == 404","","","def test_batch_index_endpoint(client):","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        batch_id = \"batch1\"","        _write_json(root / batch_id / \"index.json\", {\"batch_id\": batch_id, \"jobs\": [\"jobA\", \"jobB\"]})","","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            r = client.get(f\"/batches/{batch_id}/index\")","            assert r.status_code == 200","            assert r.json()[\"batch_id\"] == batch_id","","","def test_batch_artifacts_listing(client):","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        batch_id = \"batch1\"","","        # artifacts tree","        _write_json(","            root / batch_id / \"jobA\" / \"attempt_1\" / \"manifest.json\",","            {\"job_id\": \"jobA\", \"score\": 2.0},","        )","        _write_json(","            root / batch_id / \"jobA\" / \"attempt_2\" / \"manifest.json\",","            {\"job_id\": \"jobA\", \"metrics\": {\"score\": 3.0}},","        )","        (root / batch_id / \"jobB\" / \"attempt_1\").mkdir(parents=True, exist_ok=True)  # no manifest ok","","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            r = client.get(f\"/batches/{batch_id}/artifacts\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"batch_id\"] == batch_id","            assert [j[\"job_id\"] for j in data[\"jobs\"]] == [\"jobA\", \"jobB\"]","            jobA = data[\"jobs\"][0]","            assert [a[\"attempt\"] for a in jobA[\"attempts\"]] == [1, 2]","",""]}
{"type":"file_footer","path":"tests/test_phase141_batch_status_summary.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase14_api_batches.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7351,"sha256":"64637eaa919d9ba45f1dfe0563f451bd06eafc2ecdca501830aef17466ddc1ce","total_lines":215,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_phase14_api_batches.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Phase 14: API batch endpoints tests.\"\"\"","","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","","","@pytest.fixture","def client():","    \"\"\"FastAPI test client.\"\"\"","    return TestClient(app)","","","@pytest.fixture","def mock_governance_store():","    \"\"\"Mock governance store.","","    NOTE:","    Governance store now uses artifacts root and stores metadata at:","      artifacts/{batch_id}/metadata.json","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        artifacts_root = Path(tmpdir) / \"artifacts\"","        artifacts_root.mkdir(parents=True, exist_ok=True)","","        with patch(\"control.api._get_artifacts_root\") as mock_root, \\","             patch(\"control.api._get_governance_store\") as mock_store:","            from control.governance import BatchGovernanceStore","            real_store = BatchGovernanceStore(artifacts_root)","            mock_root.return_value = artifacts_root","            mock_store.return_value = real_store","            yield real_store","","","def test_get_batch_metadata(client, mock_governance_store):","    \"\"\"GET /batches/{batch_id}/metadata returns metadata.\"\"\"","    # Create metadata","    from control.governance import BatchMetadata","    meta = BatchMetadata(","        batch_id=\"batch1\",","        season=\"2026Q1\",","        tags=[\"test\"],","        note=\"hello\",","        frozen=False,","        created_at=\"2025-01-01T00:00:00Z\",","        updated_at=\"2025-01-01T00:00:00Z\",","        created_by=\"system\",","    )","    mock_governance_store.set_metadata(\"batch1\", meta)","","    response = client.get(\"/batches/batch1/metadata\")","    assert response.status_code == 200","    data = response.json()","    assert data[\"batch_id\"] == \"batch1\"","    assert data[\"season\"] == \"2026Q1\"","    assert data[\"tags\"] == [\"test\"]","    assert data[\"note\"] == \"hello\"","    assert data[\"frozen\"] is False","","","def test_get_batch_metadata_not_found(client, mock_governance_store):","    \"\"\"GET /batches/{batch_id}/metadata returns 404 if not found.\"\"\"","    response = client.get(\"/batches/nonexistent/metadata\")","    assert response.status_code == 404","    assert \"not found\" in response.json()[\"detail\"].lower()","","","def test_update_batch_metadata(client, mock_governance_store):","    \"\"\"PATCH /batches/{batch_id}/metadata updates metadata.\"\"\"","    # First create","    from control.governance import BatchMetadata","    meta = BatchMetadata(","        batch_id=\"batch1\",","        season=\"2026Q1\",","        tags=[],","        note=\"\",","        frozen=False,","        created_at=\"2025-01-01T00:00:00Z\",","        updated_at=\"2025-01-01T00:00:00Z\",","        created_by=\"system\",","    )","    mock_governance_store.set_metadata(\"batch1\", meta)","","    # Update","    update = {\"season\": \"2026Q2\", \"tags\": [\"newtag\"], \"note\": \"updated\"}","    response = client.patch(\"/batches/batch1/metadata\", json=update)","    assert response.status_code == 200","    data = response.json()","    assert data[\"season\"] == \"2026Q2\"","    assert data[\"tags\"] == [\"newtag\"]","    assert data[\"note\"] == \"updated\"","    assert data[\"frozen\"] is False","    assert data[\"updated_at\"] != \"2025-01-01T00:00:00Z\"  # timestamp updated","","","def test_update_batch_metadata_frozen_restrictions(client, mock_governance_store):","    \"\"\"PATCH respects frozen rules.\"\"\"","    # Create frozen batch","    from control.governance import BatchMetadata","    meta = BatchMetadata(","        batch_id=\"frozenbatch\",","        season=\"2026Q1\",","        tags=[],","        note=\"\",","        frozen=True,","        created_at=\"2025-01-01T00:00:00Z\",","        updated_at=\"2025-01-01T00:00:00Z\",","        created_by=\"system\",","    )","    mock_governance_store.set_metadata(\"frozenbatch\", meta)","","    # Attempt to change season -> 400","    response = client.patch(\"/batches/frozenbatch/metadata\", json={\"season\": \"2026Q2\"})","    assert response.status_code == 400","    assert \"Cannot change season\" in response.json()[\"detail\"]","","    # Attempt to unfreeze -> 400","    response = client.patch(\"/batches/frozenbatch/metadata\", json={\"frozen\": False})","    assert response.status_code == 400","    assert \"Cannot unfreeze\" in response.json()[\"detail\"]","","    # Append tags should work","    response = client.patch(\"/batches/frozenbatch/metadata\", json={\"tags\": [\"newtag\"]})","    assert response.status_code == 200","    data = response.json()","    assert \"newtag\" in data[\"tags\"]","","    # Update note should work","    response = client.patch(\"/batches/frozenbatch/metadata\", json={\"note\": \"updated\"})","    assert response.status_code == 200","    assert response.json()[\"note\"] == \"updated\"","","","def test_freeze_batch(client, mock_governance_store):","    \"\"\"POST /batches/{batch_id}/freeze freezes batch.\"\"\"","    # Create unfrozen batch","    from control.governance import BatchMetadata","    meta = BatchMetadata(","        batch_id=\"batch1\",","        season=\"2026Q1\",","        tags=[],","        note=\"\",","        frozen=False,","        created_at=\"2025-01-01T00:00:00Z\",","        updated_at=\"2025-01-01T00:00:00Z\",","        created_by=\"system\",","    )","    mock_governance_store.set_metadata(\"batch1\", meta)","","    response = client.post(\"/batches/batch1/freeze\")","    assert response.status_code == 200","    data = response.json()","    assert data[\"status\"] == \"frozen\"","    assert data[\"batch_id\"] == \"batch1\"","","    # Verify frozen","    assert mock_governance_store.is_frozen(\"batch1\") is True","","","def test_freeze_batch_not_found(client, mock_governance_store):","    \"\"\"POST /batches/{batch_id}/freeze returns 404 if batch not found.\"\"\"","    response = client.post(\"/batches/nonexistent/freeze\")","    assert response.status_code == 404","","","def test_retry_batch_frozen(client, mock_governance_store):","    \"\"\"POST /batches/{batch_id}/retry rejects frozen batch.\"\"\"","    # Create frozen batch","    from control.governance import BatchMetadata","    meta = BatchMetadata(","        batch_id=\"frozenbatch\",","        season=\"2026Q1\",","        tags=[],","        note=\"\",","        frozen=True,","        created_at=\"2025-01-01T00:00:00Z\",","        updated_at=\"2025-01-01T00:00:00Z\",","        created_by=\"system\",","    )","    mock_governance_store.set_metadata(\"frozenbatch\", meta)","","    response = client.post(\"/batches/frozenbatch/retry\", json={\"force\": False})","    assert response.status_code == 403","    assert \"frozen\" in response.json()[\"detail\"].lower()","","","def test_batch_status_not_implemented(client):","    \"\"\"GET /batches/{batch_id}/status returns 404 when execution.json missing.\"\"\"","    # Mock artifacts root to return a path that doesn't have execution.json","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        root.mkdir(parents=True, exist_ok=True)","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            response = client.get(\"/batches/batch1/status\")"]}
{"type":"file_chunk","path":"tests/test_phase14_api_batches.py","chunk_index":1,"line_start":201,"line_end":215,"content":["            assert response.status_code == 404","            assert \"execution.json not found\" in response.json()[\"detail\"]","","","def test_batch_summary_not_implemented(client):","    \"\"\"GET /batches/{batch_id}/summary returns 404 when summary.json missing.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        root.mkdir(parents=True, exist_ok=True)","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            response = client.get(\"/batches/batch1/summary\")","            assert response.status_code == 404","            assert \"summary.json not found\" in response.json()[\"detail\"]","",""]}
{"type":"file_footer","path":"tests/test_phase14_api_batches.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_phase14_artifacts.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2569,"sha256":"9cd4b79f7ffbf55042fc6326d3ec3cb43b9e7deb3f69196355cd7ce322e0eb9b","total_lines":88,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase14_artifacts.py","chunk_index":0,"line_start":1,"line_end":88,"content":["","\"\"\"Phase 14: Artifacts module tests.\"\"\"","","import json","import tempfile","from pathlib import Path","","from control.artifacts import (","    canonical_json_bytes,","    compute_sha256,","    write_atomic_json,","    build_job_manifest,",")","","","def test_canonical_json_bytes_deterministic():","    \"\"\"Canonical JSON must be deterministic regardless of dict order.\"\"\"","    obj1 = {\"a\": 1, \"b\": 2, \"c\": [3, 4]}","    obj2 = {\"c\": [3, 4], \"b\": 2, \"a\": 1}","    ","    bytes1 = canonical_json_bytes(obj1)","    bytes2 = canonical_json_bytes(obj2)","    ","    assert bytes1 == bytes2","    # Ensure no extra whitespace","    decoded = json.loads(bytes1.decode(\"utf-8\"))","    assert decoded == obj1","","","def test_canonical_json_bytes_unicode():","    \"\"\"Canonical JSON handles Unicode characters.\"\"\"","    obj = {\"name\": \"測試\", \"value\": \"🎯\"}","    bytes_out = canonical_json_bytes(obj)","    decoded = json.loads(bytes_out.decode(\"utf-8\"))","    assert decoded == obj","","","def test_compute_sha256():","    \"\"\"SHA256 hash matches known value.\"\"\"","    data = b\"hello world\"","    hash_hex = compute_sha256(data)","    # Expected SHA256 of \"hello world\"","    expected = \"b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9\"","    assert hash_hex == expected","","","def test_write_atomic_json():","    \"\"\"Atomic write creates file with correct content.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        path = Path(tmpdir) / \"test.json\"","        obj = {\"x\": 42, \"y\": \"text\"}","        ","        write_atomic_json(path, obj)","        ","        assert path.exists()","        content = json.loads(path.read_text(encoding=\"utf-8\"))","        assert content == obj","","","def test_build_job_manifest():","    \"\"\"Job manifest includes required fields.\"\"\"","    job_spec = {","        \"season\": \"2026Q1\",","        \"dataset_id\": \"CME_MNQ_v2\",","        \"outputs_root\": \"/tmp/outputs\",","        \"config_snapshot\": {\"param\": 1.0},","        \"config_hash\": \"abc123\",","        \"created_by\": \"test\",","    }","    job_id = \"job-123\"","    ","    manifest = build_job_manifest(job_spec, job_id)","    ","    assert manifest[\"job_id\"] == job_id","    assert manifest[\"season\"] == job_spec[\"season\"]","    assert manifest[\"dataset_id\"] == job_spec[\"dataset_id\"]","    assert manifest[\"config_hash\"] == job_spec[\"config_hash\"]","    assert \"created_at\" in manifest","    assert \"manifest_hash\" in manifest","    ","    # Verify manifest_hash is SHA256 of canonical JSON","    import copy","    manifest_copy = copy.deepcopy(manifest)","    expected_hash = manifest_copy.pop(\"manifest_hash\")","    computed = compute_sha256(canonical_json_bytes(manifest_copy))","    assert expected_hash == computed","",""]}
{"type":"file_footer","path":"tests/test_phase14_artifacts.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase14_batch_aggregate.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3502,"sha256":"4c7c8d41b3fe65fec48f4b85ca434d13bcda331f3cff070329019bd1d5cceedc","total_lines":110,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase14_batch_aggregate.py","chunk_index":0,"line_start":1,"line_end":110,"content":["","\"\"\"Phase 14: Batch aggregation tests.\"\"\"","","import tempfile","from pathlib import Path","","from control.batch_aggregate import compute_batch_summary","from control.artifacts import canonical_json_bytes, compute_sha256","","","def test_compute_batch_summary_topk():","    \"\"\"Batch summary selects top K jobs by score.\"\"\"","    job_entries = [","        {\"job_id\": \"job1\", \"score\": 0.1},","        {\"job_id\": \"job2\", \"score\": 0.9},","        {\"job_id\": \"job3\", \"score\": 0.5},","        {\"job_id\": \"job4\", \"score\": 0.7},","        {\"job_id\": \"job5\", \"score\": 0.3},","    ]","    ","    summary = compute_batch_summary(job_entries, top_k=3)","    ","    assert summary[\"total_jobs\"] == 5","    assert len(summary[\"top_k\"]) == 3","    # Should be sorted descending by score","    assert [e[\"job_id\"] for e in summary[\"top_k\"]] == [\"job2\", \"job4\", \"job3\"]","    assert [e[\"score\"] for e in summary[\"top_k\"]] == [0.9, 0.7, 0.5]","    ","    # Stats should contain counts","    stats = summary[\"stats\"]","    assert stats[\"count\"] == 5","    assert \"mean_score\" in stats","    assert \"median_score\" in stats","    assert \"std_score\" in stats","    ","    # summary_hash should be SHA256 of canonical JSON of summary without hash","    import copy","    summary_copy = copy.deepcopy(summary)","    expected_hash = summary_copy.pop(\"summary_hash\")","    computed = compute_sha256(canonical_json_bytes(summary_copy))","    assert expected_hash == computed","","","def test_compute_batch_summary_no_score():","    \"\"\"Batch summary uses job_id ordering when score missing.\"\"\"","    job_entries = [","        {\"job_id\": \"jobC\", \"config\": {\"x\": 1}},","        {\"job_id\": \"jobA\", \"config\": {\"x\": 2}},","        {\"job_id\": \"jobB\", \"config\": {\"x\": 3}},","    ]","    ","    summary = compute_batch_summary(job_entries, top_k=2)","    ","    # Top K by job_id alphabetical","    assert [e[\"job_id\"] for e in summary[\"top_k\"]] == [\"jobA\", \"jobB\"]","    ","    # Stats should not contain score statistics","    stats = summary[\"stats\"]","    assert stats[\"count\"] == 3","    assert \"mean_score\" not in stats","    assert \"median_score\" not in stats","    assert \"std_score\" not in stats","","","def test_compute_batch_summary_empty():","    \"\"\"Batch summary handles empty job list.\"\"\"","    summary = compute_batch_summary([], top_k=5)","    ","    assert summary[\"total_jobs\"] == 0","    assert summary[\"top_k\"] == []","    stats = summary[\"stats\"]","    assert stats[\"count\"] == 0","    assert \"mean_score\" not in stats","","","def test_compute_batch_summary_k_larger_than_total():","    \"\"\"Top K larger than total jobs returns all jobs.\"\"\"","    job_entries = [","        {\"job_id\": \"job1\", \"score\": 0.5},","        {\"job_id\": \"job2\", \"score\": 0.8},","    ]","    ","    summary = compute_batch_summary(job_entries, top_k=10)","    ","    assert len(summary[\"top_k\"]) == 2","    assert [e[\"job_id\"] for e in summary[\"top_k\"]] == [\"job2\", \"job1\"]","","","def test_compute_batch_summary_deterministic():","    \"\"\"Summary is deterministic regardless of input order.\"\"\"","    job_entries1 = [","        {\"job_id\": \"job1\", \"score\": 0.5},","        {\"job_id\": \"job2\", \"score\": 0.8},","    ]","    job_entries2 = [","        {\"job_id\": \"job2\", \"score\": 0.8},","        {\"job_id\": \"job1\", \"score\": 0.5},","    ]","    ","    summary1 = compute_batch_summary(job_entries1, top_k=5)","    summary2 = compute_batch_summary(job_entries2, top_k=5)","    ","    # Top K order should be same (descending score)","    assert summary1[\"top_k\"] == summary2[\"top_k\"]","    # Stats should be identical","    assert summary1[\"stats\"] == summary2[\"stats\"]","    # Hash should match","    assert summary1[\"summary_hash\"] == summary2[\"summary_hash\"]","",""]}
{"type":"file_footer","path":"tests/test_phase14_batch_aggregate.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase14_batch_execute.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4513,"sha256":"469be8b4387b47282f2829b81463b287dfce9afb0f4e8023432cf6ff36a1198d","total_lines":132,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase14_batch_execute.py","chunk_index":0,"line_start":1,"line_end":132,"content":["","\"\"\"Phase 14: Batch execution tests.\"\"\"","","import tempfile","from pathlib import Path","from unittest.mock import Mock, patch","","from control.batch_execute import (","    BatchExecutor,","    BatchExecutionState,","    JobExecutionState,","    run_batch,","    retry_failed,",")","","","def test_batch_execution_state_enum():","    \"\"\"Batch execution state enum values.\"\"\"","    assert BatchExecutionState.PENDING.value == \"PENDING\"","    assert BatchExecutionState.RUNNING.value == \"RUNNING\"","    assert BatchExecutionState.DONE.value == \"DONE\"","    assert BatchExecutionState.FAILED.value == \"FAILED\"","    assert BatchExecutionState.PARTIAL_FAILED.value == \"PARTIAL_FAILED\"","","","def test_job_execution_state_enum():","    \"\"\"Job execution state enum values.\"\"\"","    assert JobExecutionState.PENDING.value == \"PENDING\"","    assert JobExecutionState.RUNNING.value == \"RUNNING\"","    assert JobExecutionState.SUCCESS.value == \"SUCCESS\"","    assert JobExecutionState.FAILED.value == \"FAILED\"","    assert JobExecutionState.SKIPPED.value == \"SKIPPED\"","","","def test_batch_executor_initial_state():","    \"\"\"BatchExecutor initializes with correct state.\"\"\"","    batch_id = \"batch-123\"","    job_ids = [\"job1\", \"job2\", \"job3\"]","    ","    executor = BatchExecutor(batch_id, job_ids)","    ","    assert executor.batch_id == batch_id","    assert executor.job_ids == job_ids","    assert executor.state == BatchExecutionState.PENDING","    assert executor.job_states == {","        \"job1\": JobExecutionState.PENDING,","        \"job2\": JobExecutionState.PENDING,","        \"job3\": JobExecutionState.PENDING,","    }","    assert executor.created_at is not None","    assert executor.updated_at is not None","","","def test_batch_executor_transition():","    \"\"\"BatchExecutor transitions state based on job states.\"\"\"","    executor = BatchExecutor(\"batch\", [\"job1\", \"job2\"])","    ","    # Initially PENDING","    assert executor.state == BatchExecutionState.PENDING","    ","    # Start first job -> RUNNING","    executor._set_job_state(\"job1\", JobExecutionState.RUNNING)","    assert executor.state == BatchExecutionState.RUNNING","    ","    # Finish first job successfully, second still pending -> RUNNING","    executor._set_job_state(\"job1\", JobExecutionState.SUCCESS)","    assert executor.state == BatchExecutionState.RUNNING","    ","    # Start second job -> RUNNING","    executor._set_job_state(\"job2\", JobExecutionState.RUNNING)","    assert executor.state == BatchExecutionState.RUNNING","    ","    # Finish second job successfully -> DONE","    executor._set_job_state(\"job2\", JobExecutionState.SUCCESS)","    assert executor.state == BatchExecutionState.DONE","    ","    # If one job fails -> PARTIAL_FAILED","    executor._set_job_state(\"job1\", JobExecutionState.FAILED)","    executor._set_job_state(\"job2\", JobExecutionState.SUCCESS)","    executor._recompute_state()","    assert executor.state == BatchExecutionState.PARTIAL_FAILED","    ","    # If all jobs fail -> FAILED","    executor._set_job_state(\"job2\", JobExecutionState.FAILED)","    executor._recompute_state()","    assert executor.state == BatchExecutionState.FAILED","","","def test_batch_executor_skipped_jobs():","    \"\"\"SKIPPED jobs count as completed for state computation.\"\"\"","    executor = BatchExecutor(\"batch\", [\"job1\", \"job2\"])","    ","    executor._set_job_state(\"job1\", JobExecutionState.SUCCESS)","    executor._set_job_state(\"job2\", JobExecutionState.SKIPPED)","    ","    # Both jobs are completed (SUCCESS + SKIPPED) -> DONE","    assert executor.state == BatchExecutionState.DONE","","","@patch(\"control.batch_execute.BatchExecutor\")","def test_run_batch_mock(mock_executor_cls):","    \"\"\"run_batch creates executor and runs jobs.\"\"\"","    mock_executor = Mock()","    mock_executor_cls.return_value = mock_executor","    ","    batch_id = \"batch-test\"","    job_ids = [\"job1\", \"job2\"]","    artifacts_root = Path(\"/tmp/artifacts\")","    ","    result = run_batch(batch_id, job_ids, artifacts_root)","    ","    mock_executor_cls.assert_called_once_with(batch_id, job_ids)","    mock_executor.run.assert_called_once_with(artifacts_root)","    assert result == mock_executor","","","@patch(\"control.batch_execute.BatchExecutor\")","def test_retry_failed_mock(mock_executor_cls):","    \"\"\"retry_failed creates executor and retries failed jobs.\"\"\"","    mock_executor = Mock()","    mock_executor_cls.return_value = mock_executor","    ","    batch_id = \"batch-retry\"","    artifacts_root = Path(\"/tmp/artifacts\")","    ","    result = retry_failed(batch_id, artifacts_root)","    ","    mock_executor_cls.assert_called_once_with(batch_id, [])","    mock_executor.retry_failed.assert_called_once_with(artifacts_root)","    assert result == mock_executor","",""]}
{"type":"file_footer","path":"tests/test_phase14_batch_execute.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase14_batch_index.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3631,"sha256":"d7ded17228f43b758c9430482492e0935bcfb7bf9599cea1c40425330db32a9a","total_lines":87,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase14_batch_index.py","chunk_index":0,"line_start":1,"line_end":87,"content":["","\"\"\"Phase 14: Batch index tests.\"\"\"","","import json","import tempfile","from pathlib import Path","","from control.batch_index import build_batch_index","from control.artifacts import canonical_json_bytes, compute_sha256","","","def test_build_batch_index_deterministic():","    \"\"\"Batch index is deterministic regardless of job entry order.\"\"\"","    job_entries = [","        {\"job_id\": \"job1\", \"score\": 0.5, \"manifest_hash\": \"abc123\", \"manifest_path\": \"batch-123/job1/manifest.json\"},","        {\"job_id\": \"job2\", \"score\": 0.3, \"manifest_hash\": \"def456\", \"manifest_path\": \"batch-123/job2/manifest.json\"},","        {\"job_id\": \"job3\", \"score\": 0.8, \"manifest_hash\": \"ghi789\", \"manifest_path\": \"batch-123/job3/manifest.json\"},","    ]","    job_entries_shuffled = [","        {\"job_id\": \"job3\", \"score\": 0.8, \"manifest_hash\": \"ghi789\", \"manifest_path\": \"batch-123/job3/manifest.json\"},","        {\"job_id\": \"job1\", \"score\": 0.5, \"manifest_hash\": \"abc123\", \"manifest_path\": \"batch-123/job1/manifest.json\"},","        {\"job_id\": \"job2\", \"score\": 0.3, \"manifest_hash\": \"def456\", \"manifest_path\": \"batch-123/job2/manifest.json\"},","    ]","    ","    with tempfile.TemporaryDirectory() as tmpdir:","        artifacts_root = Path(tmpdir)","        batch_id = \"batch-123\"","        ","        index1 = build_batch_index(artifacts_root, batch_id, job_entries)","        index2 = build_batch_index(artifacts_root, batch_id, job_entries_shuffled)","        ","        # Index should be identical (entries sorted by job_id)","        assert index1 == index2","        ","        # Verify structure","        assert index1[\"batch_id\"] == batch_id","        assert index1[\"job_count\"] == 3","        assert len(index1[\"jobs\"]) == 3","        # Entries should be sorted by job_id","        assert [e[\"job_id\"] for e in index1[\"jobs\"]] == [\"job1\", \"job2\", \"job3\"]","        ","        # Verify index_hash is SHA256 of canonical JSON of index without hash","        import copy","        index_copy = copy.deepcopy(index1)","        expected_hash = index_copy.pop(\"index_hash\")","        computed = compute_sha256(canonical_json_bytes(index_copy))","        assert expected_hash == computed","","","def test_build_batch_index_without_score():","    \"\"\"Batch index works when jobs have no score field.\"\"\"","    job_entries = [","        {\"job_id\": \"jobA\", \"config\": {\"x\": 1}, \"manifest_hash\": \"hashA\", \"manifest_path\": \"batch-no-score/jobA/manifest.json\"},","        {\"job_id\": \"jobB\", \"config\": {\"x\": 2}, \"manifest_hash\": \"hashB\", \"manifest_path\": \"batch-no-score/jobB/manifest.json\"},","    ]","    ","    with tempfile.TemporaryDirectory() as tmpdir:","        artifacts_root = Path(tmpdir)","        batch_id = \"batch-no-score\"","        ","        index = build_batch_index(artifacts_root, batch_id, job_entries)","        ","        assert index[\"batch_id\"] == batch_id","        assert index[\"job_count\"] == 2","        # Entries sorted by job_id","        assert [e[\"job_id\"] for e in index[\"jobs\"]] == [\"jobA\", \"jobB\"]","","","def test_build_batch_index_writes_file():","    \"\"\"Batch index writes index.json to artifacts directory.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        artifacts_root = Path(tmpdir)","        batch_id = \"batch-write\"","        job_entries = [{\"job_id\": \"job1\", \"manifest_hash\": \"hash1\", \"manifest_path\": \"batch-write/job1/manifest.json\"}]","        ","        index = build_batch_index(artifacts_root, batch_id, job_entries)","        ","        # Check file exists","        batch_dir = artifacts_root / batch_id","        index_file = batch_dir / \"index.json\"","        assert index_file.exists()","        ","        # Content matches returned index","        loaded = json.loads(index_file.read_text(encoding=\"utf-8\"))","        assert loaded == index","",""]}
{"type":"file_footer","path":"tests/test_phase14_batch_index.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase14_governance.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5716,"sha256":"489d6dd71f475c385815b0d02689f92ecd1514f008196f233325fd448af59f9c","total_lines":166,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase14_governance.py","chunk_index":0,"line_start":1,"line_end":166,"content":["","\"\"\"Phase 14: Governance tests.\"\"\"","","import tempfile","from pathlib import Path","","from control.governance import (","    BatchGovernanceStore,","    BatchMetadata,",")","","","def test_batch_metadata_creation():","    \"\"\"BatchMetadata can be created with defaults.\"\"\"","    meta = BatchMetadata(batch_id=\"batch1\", season=\"2026Q1\", tags=[\"test\"], note=\"hello\")","    assert meta.batch_id == \"batch1\"","    assert meta.season == \"2026Q1\"","    assert meta.tags == [\"test\"]","    assert meta.note == \"hello\"","    assert meta.frozen is False","    assert meta.created_at == \"\"","    assert meta.updated_at == \"\"","","","def test_batch_governance_store_init():","    \"\"\"Store creates directory if not exists.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        store_root = Path(tmpdir) / \"artifacts\"","        store = BatchGovernanceStore(store_root)","        assert store.artifacts_root.exists()","        assert store.artifacts_root.is_dir()","","","def test_batch_governance_store_set_get():","    \"\"\"Store can set and retrieve metadata.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        store_root = Path(tmpdir) / \"artifacts\"","        store = BatchGovernanceStore(store_root)","","        meta = BatchMetadata(","            batch_id=\"batch1\",","            season=\"2026Q1\",","            tags=[\"tag1\", \"tag2\"],","            note=\"test note\",","            frozen=False,","            created_at=\"2025-01-01T00:00:00Z\",","            updated_at=\"2025-01-01T00:00:00Z\",","            created_by=\"user\",","        )","","        store.set_metadata(\"batch1\", meta)","","        retrieved = store.get_metadata(\"batch1\")","        assert retrieved is not None","        assert retrieved.batch_id == meta.batch_id","        assert retrieved.season == meta.season","        assert retrieved.tags == meta.tags","        assert retrieved.note == meta.note","        assert retrieved.frozen == meta.frozen","        assert retrieved.created_at == meta.created_at","        assert retrieved.updated_at == meta.updated_at","        assert retrieved.created_by == meta.created_by","","","def test_batch_governance_store_update_metadata_new():","    \"\"\"Update metadata creates new metadata if not exists.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        store_root = Path(tmpdir) / \"artifacts\"","        store = BatchGovernanceStore(store_root)","","        meta = store.update_metadata(","            \"newbatch\",","            season=\"2026Q2\",","            tags=[\"new\"],","            note=\"created\",","        )","","        assert meta.batch_id == \"newbatch\"","        assert meta.season == \"2026Q2\"","        assert meta.tags == [\"new\"]","        assert meta.note == \"created\"","        assert meta.frozen is False","        assert meta.created_at != \"\"","        assert meta.updated_at != \"\"","","","def test_batch_governance_store_update_metadata_frozen_rules():","    \"\"\"Frozen batch restricts updates.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        store_root = Path(tmpdir) / \"artifacts\"","        store = BatchGovernanceStore(store_root)","","        # Create a frozen batch","        meta = store.update_metadata(\"frozenbatch\", season=\"2026Q1\", frozen=True)","        assert meta.frozen is True","","        import pytest","        # Attempt to change season -> should raise","        with pytest.raises(ValueError, match=\"Cannot change season of frozen batch\"):","            store.update_metadata(\"frozenbatch\", season=\"2026Q2\")","","        # Attempt to unfreeze -> should raise","        with pytest.raises(ValueError, match=\"Cannot unfreeze a frozen batch\"):","            store.update_metadata(\"frozenbatch\", frozen=False)","","        # Append tags should work","        meta2 = store.update_metadata(\"frozenbatch\", tags=[\"newtag\"])","        assert \"newtag\" in meta2.tags","        assert meta2.season == \"2026Q1\"  # unchanged","","        # Update note should work","        meta3 = store.update_metadata(\"frozenbatch\", note=\"updated note\")","        assert meta3.note == \"updated note\"","","        # Setting frozen=True again is no-op","        meta4 = store.update_metadata(\"frozenbatch\", frozen=True)","        assert meta4.frozen is True","","","def test_batch_governance_store_freeze():","    \"\"\"Freeze method sets frozen flag.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        store_root = Path(tmpdir) / \"artifacts\"","        store = BatchGovernanceStore(store_root)","","        store.update_metadata(\"batch1\", season=\"2026Q1\")","        assert store.is_frozen(\"batch1\") is False","","        store.freeze(\"batch1\")","        assert store.is_frozen(\"batch1\") is True","","        # Freeze again is idempotent","        store.freeze(\"batch1\")","        assert store.is_frozen(\"batch1\") is True","","","def test_batch_governance_store_list_batches():","    \"\"\"List batches with filters.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        store_root = Path(tmpdir) / \"artifacts\"","        store = BatchGovernanceStore(store_root)","","        store.update_metadata(\"batch1\", season=\"2026Q1\", tags=[\"a\", \"b\"])","        store.update_metadata(\"batch2\", season=\"2026Q1\", tags=[\"b\", \"c\"], frozen=True)","        store.update_metadata(\"batch3\", season=\"2026Q2\", tags=[\"a\"])","","        # All batches","        all_batches = store.list_batches()","        assert len(all_batches) == 3","        ids = [m.batch_id for m in all_batches]","        assert sorted(ids) == [\"batch1\", \"batch2\", \"batch3\"]","","        # Filter by season","        season_batches = store.list_batches(season=\"2026Q1\")","        assert len(season_batches) == 2","        assert {m.batch_id for m in season_batches} == {\"batch1\", \"batch2\"}","","        # Filter by tag","        tag_batches = store.list_batches(tag=\"a\")","        assert {m.batch_id for m in tag_batches} == {\"batch1\", \"batch3\"}","","        # Filter by frozen","        frozen_batches = store.list_batches(frozen=True)","        assert {m.batch_id for m in frozen_batches} == {\"batch2\"}","",""]}
{"type":"file_footer","path":"tests/test_phase14_governance.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase150_season_index.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4832,"sha256":"ea4dc10939e7ae8ab52f088f7370b59ade187585de4807357a6413f104398283","total_lines":131,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase150_season_index.py","chunk_index":0,"line_start":1,"line_end":131,"content":["","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","","","@pytest.fixture","def client():","    return TestClient(app)","","","def _wjson(p: Path, obj):","    p.parent.mkdir(parents=True, exist_ok=True)","    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")","","","def test_rebuild_season_index_collects_batches_and_is_deterministic(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        # batch2 (lexicographically after batch1) — write first to verify sorting","        _wjson(","            artifacts_root / \"batch2\" / \"metadata.json\",","            {\"batch_id\": \"batch2\", \"season\": season, \"tags\": [\"b\", \"a\"], \"note\": \"n2\", \"frozen\": False},","        )","        _wjson(artifacts_root / \"batch2\" / \"index.json\", {\"x\": 1})","        _wjson(artifacts_root / \"batch2\" / \"summary.json\", {\"topk\": [], \"metrics\": {}})","","        # batch1","        _wjson(","            artifacts_root / \"batch1\" / \"metadata.json\",","            {\"batch_id\": \"batch1\", \"season\": season, \"tags\": [\"z\"], \"note\": \"n1\", \"frozen\": True},","        )","        _wjson(artifacts_root / \"batch1\" / \"index.json\", {\"y\": 2})","        _wjson(artifacts_root / \"batch1\" / \"summary.json\", {\"topk\": [{\"job_id\": \"j\", \"score\": 1.0}], \"metrics\": {\"n\": 1}})","","        # different season should be ignored","        _wjson(","            artifacts_root / \"batchX\" / \"metadata.json\",","            {\"batch_id\": \"batchX\", \"season\": \"2026Q2\", \"tags\": [\"ignore\"], \"note\": \"\", \"frozen\": False},","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.post(f\"/seasons/{season}/rebuild_index\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","            assert len(data[\"batches\"]) == 2","","            # deterministic order by batch_id","            assert [b[\"batch_id\"] for b in data[\"batches\"]] == [\"batch1\", \"batch2\"]","","            # tags dedupe+sort in index entries","            b2 = data[\"batches\"][1]","            assert b2[\"tags\"] == [\"a\", \"b\"]","","            # index file exists","            idx_path = season_root / season / \"season_index.json\"","            assert idx_path.exists()","","","def test_season_metadata_lifecycle_and_freeze_rules(client):","    with tempfile.TemporaryDirectory() as tmp:","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        with patch(\"control.api._get_season_index_root\", return_value=season_root):","            # metadata not exist -> 404","            r = client.get(f\"/seasons/{season}/metadata\")","            assert r.status_code == 404","","            # create/update metadata","            r = client.patch(f\"/seasons/{season}/metadata\", json={\"tags\": [\"core\", \"core\"], \"note\": \"hello\"})","            assert r.status_code == 200","            meta = r.json()","            assert meta[\"season\"] == season","            assert meta[\"tags\"] == [\"core\"]","            assert meta[\"note\"] == \"hello\"","            assert meta[\"frozen\"] is False","","            # freeze","            r = client.post(f\"/seasons/{season}/freeze\")","            assert r.status_code == 200","            assert r.json()[\"status\"] == \"frozen\"","","            # cannot unfreeze","            r = client.patch(f\"/seasons/{season}/metadata\", json={\"frozen\": False})","            assert r.status_code == 400","","            # tags/note still allowed","            r = client.patch(f\"/seasons/{season}/metadata\", json={\"tags\": [\"z\"], \"note\": \"n2\"})","            assert r.status_code == 200","            meta2 = r.json()","            assert meta2[\"tags\"] == [\"core\", \"z\"]","            assert meta2[\"note\"] == \"n2\"","            assert meta2[\"frozen\"] is True","","","def test_rebuild_index_forbidden_when_season_frozen(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        # prepare one batch","        _wjson(","            artifacts_root / \"batch1\" / \"metadata.json\",","            {\"batch_id\": \"batch1\", \"season\": season, \"tags\": [], \"note\": \"\", \"frozen\": False},","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","","            # freeze season first","            r = client.post(f\"/seasons/{season}/freeze\")","            assert r.status_code == 200","","            # rebuild should be forbidden","            r = client.post(f\"/seasons/{season}/rebuild_index\")","            assert r.status_code == 403","",""]}
{"type":"file_footer","path":"tests/test_phase150_season_index.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase151_season_compare_topk.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4664,"sha256":"3fc5ba4441c684f8289835633c0c4a3aad3524106ac4936f4093500b6e0b1f88","total_lines":134,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase151_season_compare_topk.py","chunk_index":0,"line_start":1,"line_end":134,"content":["","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","","","@pytest.fixture","def client():","    return TestClient(app)","","","def _wjson(p: Path, obj):","    p.parent.mkdir(parents=True, exist_ok=True)","    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")","","","def test_season_compare_topk_merge_and_tiebreak(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        # season index lists two batches","        _wjson(","            season_root / season / \"season_index.json\",","            {","                \"season\": season,","                \"generated_at\": \"2025-12-21T00:00:00Z\",","                \"batches\": [{\"batch_id\": \"batchA\"}, {\"batch_id\": \"batchB\"}],","            },","        )","","        # batchA summary","        _wjson(","            artifacts_root / \"batchA\" / \"summary.json\",","            {","                \"topk\": [","                    {\"job_id\": \"j2\", \"score\": 2.0},","                    {\"job_id\": \"j1\", \"score\": 2.0},  # tie on score, job_id decides inside same batch later","                    {\"job_id\": \"j0\", \"score\": 1.0},","                ],","                \"metrics\": {\"n\": 3},","            },","        )","","        # batchB summary (tie score with batchA to test tie-break by batch_id then job_id)","        _wjson(","            artifacts_root / \"batchB\" / \"summary.json\",","            {","                \"topk\": [","                    {\"job_id\": \"j9\", \"score\": 2.0},","                    {\"job_id\": \"j8\", \"score\": None},  # None goes last","                ],","                \"metrics\": {},","            },","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.get(f\"/seasons/{season}/compare/topk?k=10\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","            items = data[\"items\"]","","            # score desc, tie-break batch_id asc, tie-break job_id asc","            # score=2.0 items are: batchA j1/j2, batchB j9","            # batchA < batchB => all batchA first; within batchA j1 < j2","            assert [(x[\"batch_id\"], x[\"job_id\"], x[\"score\"]) for x in items[:3]] == [","                (\"batchA\", \"j1\", 2.0),","                (\"batchA\", \"j2\", 2.0),","                (\"batchB\", \"j9\", 2.0),","            ]","","            # None score should be at the end","            assert items[-1][\"score\"] is None","","","def test_season_compare_skips_missing_or_corrupt_summaries(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        _wjson(","            season_root / season / \"season_index.json\",","            {","                \"season\": season,","                \"generated_at\": \"2025-12-21T00:00:00Z\",","                \"batches\": [{\"batch_id\": \"batchOK\"}, {\"batch_id\": \"batchMissing\"}, {\"batch_id\": \"batchBad\"}],","            },","        )","","        _wjson(","            artifacts_root / \"batchOK\" / \"summary.json\",","            {\"topk\": [{\"job_id\": \"j1\", \"score\": 1.0}], \"metrics\": {}},","        )","","        # batchMissing -> no summary.json","","        # batchBad -> corrupt json","        bad_path = artifacts_root / \"batchBad\" / \"summary.json\"","        bad_path.parent.mkdir(parents=True, exist_ok=True)","        bad_path.write_text(\"{not-json\", encoding=\"utf-8\")","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.get(f\"/seasons/{season}/compare/topk?k=20\")","            assert r.status_code == 200","            data = r.json()","            assert [(x[\"batch_id\"], x[\"job_id\"]) for x in data[\"items\"]] == [(\"batchOK\", \"j1\")]","","            skipped = set(data[\"skipped_batches\"])","            assert \"batchMissing\" in skipped","            assert \"batchBad\" in skipped","","","def test_season_compare_404_when_season_index_missing(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.get(\"/seasons/NOPE/compare/topk?k=20\")","            assert r.status_code == 404","",""]}
{"type":"file_footer","path":"tests/test_phase151_season_compare_topk.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase152_season_compare_batches.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5637,"sha256":"4cc12233ac76daf6d4d7b7c369ca17172b3b35504b1474cf41ede9041b340ea1","total_lines":151,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase152_season_compare_batches.py","chunk_index":0,"line_start":1,"line_end":151,"content":["","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","","","@pytest.fixture","def client():","    return TestClient(app)","","","def _wjson(p: Path, obj):","    p.parent.mkdir(parents=True, exist_ok=True)","    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")","","","def test_compare_batches_cards_and_robust_summary(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        # season index includes 3 batches; ensure order is batchA, batchB, batchC","        _wjson(","            season_root / season / \"season_index.json\",","            {","                \"season\": season,","                \"generated_at\": \"2025-12-21T00:00:00Z\",","                \"batches\": [","                    {\"batch_id\": \"batchB\", \"frozen\": False, \"tags\": [\"b\"], \"note\": \"nB\", \"index_hash\": \"iB\", \"summary_hash\": \"sB\"},","                    {\"batch_id\": \"batchA\", \"frozen\": True, \"tags\": [\"a\"], \"note\": \"nA\", \"index_hash\": \"iA\", \"summary_hash\": \"sA\"},","                    {\"batch_id\": \"batchC\", \"frozen\": False, \"tags\": [], \"note\": \"\", \"index_hash\": None, \"summary_hash\": None},","                ],","            },","        )","","        # batchA: ok summary","        _wjson(","            artifacts_root / \"batchA\" / \"summary.json\",","            {\"topk\": [{\"job_id\": \"j1\", \"score\": 1.23}], \"metrics\": {\"n\": 1}},","        )","","        # batchB: corrupt summary","        p_bad = artifacts_root / \"batchB\" / \"summary.json\"","        p_bad.parent.mkdir(parents=True, exist_ok=True)","        p_bad.write_text(\"{not-json\", encoding=\"utf-8\")","","        # batchC: missing summary","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.get(f\"/seasons/{season}/compare/batches\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","","            batches = data[\"batches\"]","            assert [b[\"batch_id\"] for b in batches] == [\"batchA\", \"batchB\", \"batchC\"]","","            bA = batches[0]","            assert bA[\"summary_ok\"] is True","            assert bA[\"top_job_id\"] == \"j1\"","            assert bA[\"top_score\"] == 1.23","            assert bA[\"topk_size\"] == 1","","            bB = batches[1]","            assert bB[\"summary_ok\"] is False","","            bC = batches[2]","            assert bC[\"summary_ok\"] is False","            assert bC[\"topk_size\"] == 0","","            skipped = set(data[\"skipped_summaries\"])","            assert \"batchB\" in skipped","            assert \"batchC\" in skipped","","","def test_compare_leaderboard_grouping_and_determinism(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        _wjson(","            season_root / season / \"season_index.json\",","            {","                \"season\": season,","                \"generated_at\": \"2025-12-21T00:00:00Z\",","                \"batches\": [{\"batch_id\": \"batchA\"}, {\"batch_id\": \"batchB\"}],","            },","        )","","        # Include strategy_id and dataset_id in rows for grouping","        _wjson(","            artifacts_root / \"batchA\" / \"summary.json\",","            {","                \"topk\": [","                    {\"job_id\": \"a2\", \"score\": 2.0, \"strategy_id\": \"S1\"},","                    {\"job_id\": \"a1\", \"score\": 2.0, \"strategy_id\": \"S1\"},  # tie within same group","                    {\"job_id\": \"a0\", \"score\": 1.0, \"strategy_id\": \"S2\"},","                ]","            },","        )","        _wjson(","            artifacts_root / \"batchB\" / \"summary.json\",","            {","                \"topk\": [","                    {\"job_id\": \"b9\", \"score\": 2.0, \"strategy_id\": \"S1\"},","                    {\"job_id\": \"b8\", \"score\": None, \"strategy_id\": \"S1\"},","                ]","            },","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.get(f\"/seasons/{season}/compare/leaderboard?group_by=strategy_id&per_group=3\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","            assert data[\"group_by\"] == \"strategy_id\"","            assert data[\"per_group\"] == 3","","            groups = {g[\"key\"]: g[\"items\"] for g in data[\"groups\"]}","            assert \"S1\" in groups","            # Deterministic ordering inside group S1 by score desc, tie-break batch_id asc, job_id asc","            # score=2.0: batchA a1/a2, batchB b9 => batchA first; within batchA a1 < a2","            assert [(x[\"batch_id\"], x[\"job_id\"], x[\"score\"]) for x in groups[\"S1\"][:3]] == [","                (\"batchA\", \"a1\", 2.0),","                (\"batchA\", \"a2\", 2.0),","                (\"batchB\", \"b9\", 2.0),","            ]","","","def test_compare_endpoints_404_when_season_index_missing(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.get(\"/seasons/NOPE/compare/batches\")","            assert r.status_code == 404","            r = client.get(\"/seasons/NOPE/compare/leaderboard\")","            assert r.status_code == 404","",""]}
{"type":"file_footer","path":"tests/test_phase152_season_compare_batches.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase153_season_export.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4325,"sha256":"882959ebae5f1030db7c756a7d52d94f187a8cb8f547f4e2bf6b4efeee20feee","total_lines":109,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase153_season_export.py","chunk_index":0,"line_start":1,"line_end":109,"content":["","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","from control.artifacts import compute_sha256","","","@pytest.fixture","def client():","    return TestClient(app)","","","def _wjson(p: Path, obj):","    p.parent.mkdir(parents=True, exist_ok=True)","    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")","","","def test_export_requires_frozen_season(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","","        # season index exists","        _wjson(","            season_root / season / \"season_index.json\",","            {\"season\": season, \"generated_at\": \"Z\", \"batches\": []},","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root), \\","             patch(\"control.season_export.get_exports_root\", return_value=exports_root):","            r = client.post(f\"/seasons/{season}/export\")","            assert r.status_code == 403","","","def test_export_builds_package_and_manifest_sha_matches(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","","        # create season index with 2 batches","        _wjson(","            season_root / season / \"season_index.json\",","            {","                \"season\": season,","                \"generated_at\": \"2025-12-21T00:00:00Z\",","                \"batches\": [{\"batch_id\": \"batchB\"}, {\"batch_id\": \"batchA\"}],","            },","        )","","        # create season metadata and freeze it","        # (use API to freeze for realism)","        with patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.post(f\"/seasons/{season}/freeze\")","            assert r.status_code == 200","","        # artifacts files","        _wjson(artifacts_root / \"batchA\" / \"metadata.json\", {\"season\": season, \"frozen\": True, \"tags\": [\"a\"], \"note\": \"\"})","        _wjson(artifacts_root / \"batchA\" / \"index.json\", {\"x\": 1})","        _wjson(artifacts_root / \"batchA\" / \"summary.json\", {\"topk\": [{\"job_id\": \"j1\", \"score\": 1.0}], \"metrics\": {}})","","        _wjson(artifacts_root / \"batchB\" / \"metadata.json\", {\"season\": season, \"frozen\": False, \"tags\": [\"b\"], \"note\": \"n\"})","        _wjson(artifacts_root / \"batchB\" / \"index.json\", {\"y\": 2})","        # omit batchB summary.json to test missing files recorded","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root), \\","             patch(\"control.season_export.get_exports_root\", return_value=exports_root):","            r = client.post(f\"/seasons/{season}/export\")","            assert r.status_code == 200","            out = r.json()","","            export_dir = Path(out[\"export_dir\"])","            manifest_path = Path(out[\"manifest_path\"])","            assert export_dir.exists()","            assert manifest_path.exists()","","            # verify manifest sha matches actual bytes","            actual_sha = compute_sha256(manifest_path.read_bytes())","            assert out[\"manifest_sha256\"] == actual_sha","","            # verify key files copied","            assert (export_dir / \"season_index.json\").exists()","            # metadata may exist (freeze created it)","            assert (export_dir / \"season_metadata.json\").exists()","            assert (export_dir / \"batches\" / \"batchA\" / \"metadata.json\").exists()","            assert (export_dir / \"batches\" / \"batchA\" / \"index.json\").exists()","            assert (export_dir / \"batches\" / \"batchA\" / \"summary.json\").exists()","","            # batchB summary missing -> recorded","            assert \"batches/batchB/summary.json\" in out[\"missing_files\"]","","            # manifest contains file hashes","            man = json.loads(manifest_path.read_text(encoding=\"utf-8\"))","            assert man[\"season\"] == season","            assert \"files\" in man and isinstance(man[\"files\"], list)","            assert \"manifest_sha256\" in man","",""]}
{"type":"file_footer","path":"tests/test_phase153_season_export.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase16_export_replay.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":15844,"sha256":"96a2906ae9202c6631f1ad6d200cc069884f74b874b10b1335405ebd2aa55bb0","total_lines":450,"chunk_count":3}
{"type":"file_chunk","path":"tests/test_phase16_export_replay.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 16: Export Pack Replay Mode regression tests.","","Tests that exported season packages can be replayed without artifacts.","\"\"\"","","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","from control.season_export_replay import (","    load_replay_index,","    replay_season_topk,","    replay_season_batch_cards,","    replay_season_leaderboard,",")","","","@pytest.fixture","def client():","    return TestClient(app)","","","def _wjson(p: Path, obj):","    p.parent.mkdir(parents=True, exist_ok=True)","    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")","","","def test_load_replay_index():","    \"\"\"Test loading replay_index.json.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"job1\", \"score\": 1.5, \"strategy_id\": \"S1\"}],","                        \"metrics\": {\"n\": 10},","                    },","                    \"index\": {\"jobs\": [\"job1\"]},","                }","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        loaded = load_replay_index(exports_root, season)","        assert loaded[\"season\"] == season","        assert len(loaded[\"batches\"]) == 1","        assert loaded[\"batches\"][0][\"batch_id\"] == \"batchA\"","","","def test_load_replay_index_missing():","    \"\"\"Test FileNotFoundError when replay_index.json missing.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        with pytest.raises(FileNotFoundError):","            load_replay_index(exports_root, season)","","","def test_replay_season_topk():","    \"\"\"Test replay season topk.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [","                            {\"job_id\": \"job1\", \"score\": 1.5, \"strategy_id\": \"S1\"},","                            {\"job_id\": \"job2\", \"score\": 1.2, \"strategy_id\": \"S2\"},","                        ],","                        \"metrics\": {},","                    },","                },","                {","                    \"batch_id\": \"batchB\",","                    \"summary\": {","                        \"topk\": [","                            {\"job_id\": \"job3\", \"score\": 1.8, \"strategy_id\": \"S1\"},","                        ],","                        \"metrics\": {},","                    },","                },","                {","                    \"batch_id\": \"batchC\",","                    \"summary\": None,  # missing summary","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        res = replay_season_topk(exports_root, season, k=5)","        assert res.season == season","        assert res.k == 5","        assert len(res.items) == 3  # all topk items merged","        assert res.skipped_batches == [\"batchC\"]","        ","        # Verify ordering by score descending","        scores = [item[\"score\"] for item in res.items]","        assert scores == [1.8, 1.5, 1.2]","        ","        # Verify batch_id added","        assert all(\"_batch_id\" in item for item in res.items)","","","def test_replay_season_batch_cards():","    \"\"\"Test replay season batch cards.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"job1\", \"score\": 1.5}],","                        \"metrics\": {\"n\": 10},","                    },","                    \"index\": {\"jobs\": [\"job1\"]},","                },","                {","                    \"batch_id\": \"batchB\",","                    \"summary\": None,  # missing summary","                    \"index\": {\"jobs\": [\"job2\"]},","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        res = replay_season_batch_cards(exports_root, season)","        assert res.season == season","        assert len(res.batches) == 1","        assert res.batches[0][\"batch_id\"] == \"batchA\"","        assert res.skipped_summaries == [\"batchB\"]","","","def test_replay_season_leaderboard():","    \"\"\"Test replay season leaderboard.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [","                            {\"job_id\": \"job1\", \"score\": 1.5, \"strategy_id\": \"S1\", \"dataset_id\": \"D1\"},","                            {\"job_id\": \"job2\", \"score\": 1.2, \"strategy_id\": \"S2\", \"dataset_id\": \"D1\"},","                        ],","                        \"metrics\": {},","                    },","                },","                {","                    \"batch_id\": \"batchB\",","                    \"summary\": {","                        \"topk\": [","                            {\"job_id\": \"job3\", \"score\": 1.8, \"strategy_id\": \"S1\", \"dataset_id\": \"D2\"},","                            {\"job_id\": \"job4\", \"score\": 0.9, \"strategy_id\": \"S2\", \"dataset_id\": \"D2\"},","                        ],","                        \"metrics\": {},","                    },","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        # Test group_by strategy_id","        res = replay_season_leaderboard(exports_root, season, group_by=\"strategy_id\", per_group=2)","        assert res.season == season","        assert res.group_by == \"strategy_id\"","        assert res.per_group == 2"]}
{"type":"file_chunk","path":"tests/test_phase16_export_replay.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        assert len(res.groups) == 2  # S1 and S2","        ","        # Find S1 group","        s1_group = next(g for g in res.groups if g[\"key\"] == \"S1\")","        assert s1_group[\"total\"] == 2","        assert len(s1_group[\"items\"]) == 2","        assert s1_group[\"items\"][0][\"score\"] == 1.8  # top score first","        ","        # Test group_by dataset_id","        res2 = replay_season_leaderboard(exports_root, season, group_by=\"dataset_id\", per_group=1)","        assert len(res2.groups) == 2  # D1 and D2","        d1_group = next(g for g in res2.groups if g[\"key\"] == \"D1\")","        assert len(d1_group[\"items\"]) == 1  # per_group=1","","","def test_export_season_compare_topk_endpoint(client):","    \"\"\"Test /exports/seasons/{season}/compare/topk endpoint.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"job1\", \"score\": 1.5}],","                        \"metrics\": {},","                    },","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        with patch(\"control.api.get_exports_root\", return_value=exports_root):","            r = client.get(f\"/exports/seasons/{season}/compare/topk?k=5\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","            assert data[\"k\"] == 5","            assert len(data[\"items\"]) == 1","            assert data[\"items\"][0][\"job_id\"] == \"job1\"","","","def test_export_season_compare_batches_endpoint(client):","    \"\"\"Test /exports/seasons/{season}/compare/batches endpoint.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"job1\", \"score\": 1.5}],","                        \"metrics\": {\"n\": 10},","                    },","                    \"index\": {\"jobs\": [\"job1\"]},","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        with patch(\"control.api.get_exports_root\", return_value=exports_root):","            r = client.get(f\"/exports/seasons/{season}/compare/batches\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","            assert len(data[\"batches\"]) == 1","            assert data[\"batches\"][0][\"batch_id\"] == \"batchA\"","","","def test_export_season_compare_leaderboard_endpoint(client):","    \"\"\"Test /exports/seasons/{season}/compare/leaderboard endpoint.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [","                            {\"job_id\": \"job1\", \"score\": 1.5, \"strategy_id\": \"S1\"},","                        ],","                        \"metrics\": {},","                    },","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        with patch(\"control.api.get_exports_root\", return_value=exports_root):","            r = client.get(f\"/exports/seasons/{season}/compare/leaderboard?group_by=strategy_id\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","            assert data[\"group_by\"] == \"strategy_id\"","            assert len(data[\"groups\"]) == 1","            assert data[\"groups\"][0][\"key\"] == \"S1\"","","","def test_export_endpoints_missing_replay_index(client):","    \"\"\"Test 404 when replay_index.json missing.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        with patch(\"control.api.get_exports_root\", return_value=exports_root):","            r = client.get(f\"/exports/seasons/{season}/compare/topk\")","            assert r.status_code == 404","            assert \"replay_index.json\" in r.json()[\"detail\"]","","","def test_deterministic_ordering():","    \"\"\"Test deterministic ordering in replay functions.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        # Create replay index with batches in non-alphabetical order","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchZ\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"jobZ\", \"score\": 1.0}],","                        \"metrics\": {},","                    },","                },","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"jobA\", \"score\": 2.0}],","                        \"metrics\": {},","                    },","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        # Test that batches are processed in sorted order (batchA before batchZ)","        res = replay_season_topk(exports_root, season, k=10)","        # The items should be sorted by score, not batch order","        scores = [item[\"score\"] for item in res.items]","        assert scores == [2.0, 1.0]  # score ordering, not batch ordering","        ","        # Test batch cards ordering","        res2 = replay_season_batch_cards(exports_root, season)","        batch_ids = [b[\"batch_id\"] for b in res2.batches]","        assert batch_ids == [\"batchA\", \"batchZ\"]  # sorted by batch_id","","","def test_replay_with_empty_topk():","    \"\"\"Test replay with empty topk lists.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [],","                        \"metrics\": {},","                    },","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        res = replay_season_topk(exports_root, season, k=5)","        assert res.season == season","        assert len(res.items) == 0","        assert res.skipped_batches == []  # not skipped because summary exists","","","def test_replay_endpoint_zero_write_guarantee(client):","    \"\"\"Ensure replay endpoints do NOT write to exports tree.\"\"\"","    import os","    import time","    "]}
{"type":"file_chunk","path":"tests/test_phase16_export_replay.py","chunk_index":2,"line_start":401,"line_end":450,"content":["    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"job1\", \"score\": 1.5}],","                        \"metrics\": {},","                    },","                    \"index\": {\"jobs\": [\"job1\"]},","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        # Record initial state","        def get_file_state():","            files = []","            for root, dirs, filenames in os.walk(exports_root):","                for f in filenames:","                    path = Path(root) / f","                    files.append((str(path.relative_to(exports_root)), path.stat().st_mtime))","            return sorted(files)","        ","        initial_state = get_file_state()","        ","        with patch(\"control.api.get_exports_root\", return_value=exports_root):","            # Call each replay endpoint","            r1 = client.get(f\"/exports/seasons/{season}/compare/topk?k=5\")","            assert r1.status_code == 200","            r2 = client.get(f\"/exports/seasons/{season}/compare/batches\")","            assert r2.status_code == 200","            r3 = client.get(f\"/exports/seasons/{season}/compare/leaderboard?group_by=strategy_id\")","            assert r3.status_code == 200","        ","        # Wait a tiny bit to ensure mtime could change if write occurred","        time.sleep(0.01)","        ","        final_state = get_file_state()","        ","        # No new files should appear, no mtime changes","        assert initial_state == final_state, \"Replay endpoints must not write to exports tree\"","",""]}
{"type":"file_footer","path":"tests/test_phase16_export_replay.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"tests/test_portfolio_artifacts_hash_stable.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5939,"sha256":"62431b47ac72a5211408cacf3f3d77e9240aecb31e490cb34cd4de4536f0f4c1","total_lines":222,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_portfolio_artifacts_hash_stable.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Test portfolio artifacts hash stability.","","Phase 8: Test hash is deterministic and changes with spec changes.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from portfolio.artifacts import compute_portfolio_hash, write_portfolio_artifacts","from portfolio.compiler import compile_portfolio","from portfolio.loader import load_portfolio_spec","from strategy.registry import load_builtin_strategies, clear","","","@pytest.fixture(autouse=True)","def setup_registry() -> None:","    \"\"\"Setup strategy registry before each test.\"\"\"","    clear()","    load_builtin_strategies()","    yield","    clear()","","","def test_hash_same_spec_consistent(tmp_path: Path) -> None:","    \"\"\"Test hash is consistent for same spec.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    ","    # Compute hash multiple times","    hash1 = compute_portfolio_hash(spec)","    hash2 = compute_portfolio_hash(spec)","    hash3 = compute_portfolio_hash(spec)","    ","    # All hashes should be identical","    assert hash1 == hash2 == hash3","    assert len(hash1) == 40  # SHA1 hex string length","","","def test_hash_different_order_consistent(tmp_path: Path) -> None:","    \"\"\"Test hash is consistent even if legs are in different order.\"\"\"","    yaml_content1 = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","  - leg_id: \"leg2\"","    symbol: \"TWF.MXF\"","    timeframe_min: 60","    session_profile: \"configs/profiles/TWF_MXF_v2.yaml\"","    strategy_id: \"mean_revert_zscore\"","    strategy_version: \"v1\"","    params:","      zscore_threshold: -2.0","    enabled: true","\"\"\"","    ","    yaml_content2 = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg2\"  # Different order","    symbol: \"TWF.MXF\"","    timeframe_min: 60","    session_profile: \"configs/profiles/TWF_MXF_v2.yaml\"","    strategy_id: \"mean_revert_zscore\"","    strategy_version: \"v1\"","    params:","      zscore_threshold: -2.0","    enabled: true","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","\"\"\"","    ","    spec_path1 = tmp_path / \"test1.yaml\"","    spec_path1.write_text(yaml_content1, encoding=\"utf-8\")","    ","    spec_path2 = tmp_path / \"test2.yaml\"","    spec_path2.write_text(yaml_content2, encoding=\"utf-8\")","    ","    spec1 = load_portfolio_spec(spec_path1)","    spec2 = load_portfolio_spec(spec_path2)","    ","    hash1 = compute_portfolio_hash(spec1)","    hash2 = compute_portfolio_hash(spec2)","    ","    # Hashes should be identical (legs are sorted by leg_id before hashing)","    assert hash1 == hash2","","","def test_hash_changes_with_param_change(tmp_path: Path) -> None:","    \"\"\"Test hash changes when params change.\"\"\"","    yaml_content1 = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","\"\"\"","    ","    yaml_content2 = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 15.0  # Changed","      slow_period: 20.0","    enabled: true","\"\"\"","    ","    spec_path1 = tmp_path / \"test1.yaml\"","    spec_path1.write_text(yaml_content1, encoding=\"utf-8\")","    ","    spec_path2 = tmp_path / \"test2.yaml\"","    spec_path2.write_text(yaml_content2, encoding=\"utf-8\")","    ","    spec1 = load_portfolio_spec(spec_path1)","    spec2 = load_portfolio_spec(spec_path2)","    ","    hash1 = compute_portfolio_hash(spec1)","    hash2 = compute_portfolio_hash(spec2)","    ","    # Hashes should be different","    assert hash1 != hash2","","","def test_write_artifacts_creates_files(tmp_path: Path) -> None:","    \"\"\"Test write_portfolio_artifacts creates all required files.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)"]}
{"type":"file_chunk","path":"tests/test_portfolio_artifacts_hash_stable.py","chunk_index":1,"line_start":201,"line_end":222,"content":["    jobs = compile_portfolio(spec)","    ","    out_dir = tmp_path / \"artifacts\"","    artifact_paths = write_portfolio_artifacts(spec, jobs, out_dir)","    ","    # Check all files exist","    assert (out_dir / \"portfolio_spec_snapshot.yaml\").exists()","    assert (out_dir / \"compiled_jobs.json\").exists()","    assert (out_dir / \"portfolio_index.json\").exists()","    assert (out_dir / \"portfolio_hash.txt\").exists()","    ","    # Check hash file content","    hash_content = (out_dir / \"portfolio_hash.txt\").read_text(encoding=\"utf-8\").strip()","    computed_hash = compute_portfolio_hash(spec)","    assert hash_content == computed_hash","    ","    # Check index contains hash","    import json","    index_content = json.loads((out_dir / \"portfolio_index.json\").read_text(encoding=\"utf-8\"))","    assert index_content[\"portfolio_hash\"] == computed_hash","",""]}
{"type":"file_footer","path":"tests/test_portfolio_artifacts_hash_stable.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_portfolio_compile_jobs.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2938,"sha256":"b602a42dcf0e71653e9e96c215bb5a7dc098bce419132dd79ec207b97814dcf5","total_lines":119,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_portfolio_compile_jobs.py","chunk_index":0,"line_start":1,"line_end":119,"content":["","\"\"\"Test portfolio compiler.","","Phase 8: Test compilation produces correct job configs.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from portfolio.compiler import compile_portfolio","from portfolio.loader import load_portfolio_spec","from strategy.registry import load_builtin_strategies, clear","","","@pytest.fixture(autouse=True)","def setup_registry() -> None:","    \"\"\"Setup strategy registry before each test.\"\"\"","    clear()","    load_builtin_strategies()","    yield","    clear()","","","def test_compile_enabled_legs_only(tmp_path: Path) -> None:","    \"\"\"Test compilation only includes enabled legs.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","  - leg_id: \"leg2\"","    symbol: \"TWF.MXF\"","    timeframe_min: 60","    session_profile: \"configs/profiles/TWF_MXF_v2.yaml\"","    strategy_id: \"mean_revert_zscore\"","    strategy_version: \"v1\"","    params:","      zscore_threshold: -2.0","    enabled: false  # Disabled","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    jobs = compile_portfolio(spec)","    ","    # Should only have 1 job (leg1 enabled, leg2 disabled)","    assert len(jobs) == 1","    assert jobs[0][\"leg_id\"] == \"leg1\"","","","def test_compile_job_has_required_keys(tmp_path: Path) -> None:","    \"\"\"Test compiled jobs have all required keys.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","    tags: [\"test\"]","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    jobs = compile_portfolio(spec)","    ","    assert len(jobs) == 1","    job = jobs[0]","    ","    # Check required keys","    required_keys = {","        \"portfolio_id\",","        \"portfolio_version\",","        \"leg_id\",","        \"symbol\",","        \"timeframe_min\",","        \"session_profile\",","        \"strategy_id\",","        \"strategy_version\",","        \"params\",","    }","    ","    assert required_keys.issubset(job.keys())","    ","    # Check values","    assert job[\"portfolio_id\"] == \"test\"","    assert job[\"portfolio_version\"] == \"v1\"","    assert job[\"leg_id\"] == \"leg1\"","    assert job[\"symbol\"] == \"CME.MNQ\"","    assert job[\"timeframe_min\"] == 60","    assert job[\"strategy_id\"] == \"sma_cross\"","    assert job[\"strategy_version\"] == \"v1\"","    assert job[\"params\"] == {\"fast_period\": 10.0, \"slow_period\": 20.0}","    assert job[\"tags\"] == [\"test\"]","",""]}
{"type":"file_footer","path":"tests/test_portfolio_compile_jobs.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_portfolio_spec_loader.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3573,"sha256":"094a70166bb42b7a8ee0edc27812a5695913deb22c344918a17bdc1d6b424d8e","total_lines":133,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_portfolio_spec_loader.py","chunk_index":0,"line_start":1,"line_end":133,"content":["","\"\"\"Test portfolio spec loader.","","Phase 8: Test YAML/JSON loader can load and type is correct.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from portfolio.loader import load_portfolio_spec","from portfolio.spec import PortfolioLeg, PortfolioSpec","","","def test_load_yaml_spec(tmp_path: Path) -> None:","    \"\"\"Test loading YAML portfolio spec.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","data_tz: \"Asia/Taipei\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","    tags: [\"test\"]","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    ","    assert isinstance(spec, PortfolioSpec)","    assert spec.portfolio_id == \"test\"","    assert spec.version == \"v1\"","    assert spec.data_tz == \"Asia/Taipei\"","    assert len(spec.legs) == 1","    ","    leg = spec.legs[0]","    assert isinstance(leg, PortfolioLeg)","    assert leg.leg_id == \"leg1\"","    assert leg.symbol == \"CME.MNQ\"","    assert leg.timeframe_min == 60","    assert leg.strategy_id == \"sma_cross\"","    assert leg.strategy_version == \"v1\"","    assert leg.params == {\"fast_period\": 10.0, \"slow_period\": 20.0}","    assert leg.enabled is True","    assert leg.tags == [\"test\"]","","","def test_load_json_spec(tmp_path: Path) -> None:","    \"\"\"Test loading JSON portfolio spec.\"\"\"","    import json","    ","    json_content = {","        \"portfolio_id\": \"test\",","        \"version\": \"v1\",","        \"data_tz\": \"Asia/Taipei\",","        \"legs\": [","            {","                \"leg_id\": \"leg1\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe_min\": 60,","                \"session_profile\": \"configs/profiles/CME_MNQ_v2.yaml\",","                \"strategy_id\": \"sma_cross\",","                \"strategy_version\": \"v1\",","                \"params\": {","                    \"fast_period\": 10.0,","                    \"slow_period\": 20.0,","                },","                \"enabled\": True,","                \"tags\": [\"test\"],","            }","        ],","    }","    ","    spec_path = tmp_path / \"test.json\"","    with spec_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(json_content, f)","    ","    spec = load_portfolio_spec(spec_path)","    ","    assert isinstance(spec, PortfolioSpec)","    assert spec.portfolio_id == \"test\"","    assert len(spec.legs) == 1","","","def test_load_missing_fields_raises(tmp_path: Path) -> None:","    \"\"\"Test loading spec with missing required fields raises ValueError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","# Missing version","legs: []","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    with pytest.raises(ValueError, match=\"missing 'version' field\"):","        load_portfolio_spec(spec_path)","","","def test_load_invalid_params_type_raises(tmp_path: Path) -> None:","    \"\"\"Test loading spec with invalid params type raises ValueError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params: \"invalid\"  # Should be dict","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    with pytest.raises(ValueError, match=\"params must be dict\"):","        load_portfolio_spec(spec_path)","",""]}
{"type":"file_footer","path":"tests/test_portfolio_spec_loader.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_portfolio_validate.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3845,"sha256":"98e708497f46b3592ad6a69e9d4245c21fdc1bfb7b29546125a6ae52abddbd57","total_lines":144,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_portfolio_validate.py","chunk_index":0,"line_start":1,"line_end":144,"content":["","\"\"\"Test portfolio validator.","","Phase 8: Test validation raises errors for invalid specs.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from portfolio.loader import load_portfolio_spec","from portfolio.validate import validate_portfolio_spec","from strategy.registry import load_builtin_strategies, clear","","","@pytest.fixture(autouse=True)","def setup_registry() -> None:","    \"\"\"Setup strategy registry before each test.\"\"\"","    clear()","    load_builtin_strategies()","    yield","    clear()","","","def test_validate_empty_legs_raises(tmp_path: Path) -> None:","    \"\"\"Test validating spec with empty legs raises ValueError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs: []","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    ","    with pytest.raises(ValueError, match=\"at least one leg\"):","        validate_portfolio_spec(spec)","","","def test_validate_duplicate_leg_id_raises(tmp_path: Path) -> None:","    \"\"\"Test validating spec with duplicate leg_id raises ValueError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params: {}","  - leg_id: \"leg1\"  # Duplicate","    symbol: \"TWF.MXF\"","    timeframe_min: 60","    session_profile: \"configs/profiles/TWF_MXF_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params: {}","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    with pytest.raises(ValueError, match=\"Duplicate leg_id\"):","        load_portfolio_spec(spec_path)","","","def test_validate_nonexistent_strategy_raises(tmp_path: Path) -> None:","    \"\"\"Test validating spec with nonexistent strategy raises KeyError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"nonexistent_strategy\"  # Not in registry","    strategy_version: \"v1\"","    params: {}","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    ","    with pytest.raises(KeyError, match=\"not found in registry\"):","        validate_portfolio_spec(spec)","","","def test_validate_strategy_version_mismatch_raises(tmp_path: Path) -> None:","    \"\"\"Test validating spec with strategy version mismatch raises ValueError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v2\"  # Mismatch (registry has v1)","    params: {}","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    ","    with pytest.raises(ValueError, match=\"strategy_version mismatch\"):","        validate_portfolio_spec(spec)","","","def test_validate_nonexistent_session_profile_raises(tmp_path: Path) -> None:","    \"\"\"Test validating spec with nonexistent session profile raises FileNotFoundError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"nonexistent_profile.yaml\"  # Not found","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params: {}","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    ","    with pytest.raises(FileNotFoundError):","        validate_portfolio_spec(spec)","",""]}
{"type":"file_footer","path":"tests/test_portfolio_validate.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_report_link_allows_minimal_artifacts.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6109,"sha256":"223c823e15d03612c3f7d7fc9252abdaed391624c48cb57ca69f609f83ac58ce","total_lines":170,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_report_link_allows_minimal_artifacts.py","chunk_index":0,"line_start":1,"line_end":170,"content":["","\"\"\"Tests for report link allowing minimal artifacts.","","Tests that report readiness only checks file existence,","and build_report_link always returns Viewer URL.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","","import pytest","","from control.report_links import (","    build_report_link,","    get_outputs_root,","    is_report_ready,",")","","","def test_is_report_ready_with_minimal_artifacts(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that is_report_ready returns True with only three files.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    run_dir = tmp_path / run_id","    run_dir.mkdir(parents=True)","    ","    # Create only the three required files","    (run_dir / \"manifest.json\").write_text(json.dumps({\"run_id\": run_id}))","    # Use winners_v2.json (preferred) or winners.json (fallback)","    (run_dir / \"winners_v2.json\").write_text(json.dumps({\"summary\": {}}))","    (run_dir / \"governance.json\").write_text(json.dumps({\"scoring\": {}}))","    ","    # Should return True","    assert is_report_ready(run_id) is True","","","def test_is_report_ready_missing_file(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that is_report_ready returns False if any file is missing.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    run_dir = tmp_path / run_id","    run_dir.mkdir(parents=True)","    ","    # Create only two files (missing governance.json)","    (run_dir / \"manifest.json\").write_text(json.dumps({\"run_id\": run_id}))","    (run_dir / \"winners.json\").write_text(json.dumps({\"summary\": {}}))","    ","    # Should return False","    assert is_report_ready(run_id) is False","","","def test_build_report_link_always_returns_url(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that build_report_link always returns Viewer URL.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    ","    # Should return URL even if artifacts don't exist","    report_link = build_report_link(run_id)","    ","    assert report_link is not None","    assert report_link.startswith(\"/?\")","    assert run_id in report_link","    assert \"season\" in report_link","","","def test_build_report_link_no_error_string(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that build_report_link never returns error string.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    ","    # Should never return error string","    report_link = build_report_link(run_id)","    ","    assert report_link is not None","    assert isinstance(report_link, str)","    assert \"error\" not in report_link.lower()","    assert \"not ready\" not in report_link.lower()","    assert \"missing\" not in report_link.lower()","","","def test_is_report_ready_never_raises(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that is_report_ready never raises exceptions.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    # Should not raise even with invalid run_id","    result = is_report_ready(\"nonexistent_run\")","    assert isinstance(result, bool)","    ","    # Should not raise even with None","    result = is_report_ready(None)  # type: ignore","    assert isinstance(result, bool)","","","def test_build_report_link_never_raises(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that build_report_link never raises exceptions.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    # Should not raise even with invalid run_id","    report_link = build_report_link(\"nonexistent_run\")","    assert report_link is not None","    assert isinstance(report_link, str)","    ","    # Should not raise even with empty string","    report_link = build_report_link(\"\")","    assert report_link is not None","    assert isinstance(report_link, str)","","","def test_minimal_artifacts_content_not_checked(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that is_report_ready does not check content validity.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    run_dir = tmp_path / run_id","    run_dir.mkdir(parents=True)","    ","    # Create files with invalid JSON content","    (run_dir / \"manifest.json\").write_text(\"invalid json\")","    (run_dir / \"winners_v2.json\").write_text(\"not json\")","    (run_dir / \"governance.json\").write_text(\"{}\")","    ","    # Should still return True (only checks existence)","    assert is_report_ready(run_id) is True","","","def test_is_report_ready_accepts_winners_json_fallback(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that is_report_ready accepts winners.json as fallback.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    run_dir = tmp_path / run_id","    run_dir.mkdir(parents=True)","    ","    # Create files with winners.json (not winners_v2.json)","    (run_dir / \"manifest.json\").write_text(json.dumps({\"run_id\": run_id}))","    (run_dir / \"winners.json\").write_text(json.dumps({\"summary\": {}}))","    (run_dir / \"governance.json\").write_text(json.dumps({\"scoring\": {}}))","    ","    # Should still return True (only checks existence)","    assert is_report_ready(run_id) is True","","","def test_ui_does_not_block_with_minimal_artifacts(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that UI flow does not block with minimal artifacts.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    run_dir = tmp_path / run_id","    run_dir.mkdir(parents=True)","    ","    # Create minimal artifacts","    (run_dir / \"manifest.json\").write_text(json.dumps({\"run_id\": run_id}))","    (run_dir / \"winners_v2.json\").write_text(json.dumps({\"summary\": {}}))","    (run_dir / \"governance.json\").write_text(json.dumps({\"scoring\": {}}))","    ","    # build_report_link should work","    report_link = build_report_link(run_id)","    assert report_link is not None","    assert \"error\" not in report_link.lower()","    ","    # is_report_ready should return True","    assert is_report_ready(run_id) is True","",""]}
{"type":"file_footer","path":"tests/test_report_link_allows_minimal_artifacts.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_research_console_filters.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":13879,"sha256":"809cf41bfe4b5ba2cdefa50faf4ff7de7a1d353c91186dd2b63b39567ff54d75","total_lines":346,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_research_console_filters.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Test research_console filters.","","Phase 10: Test apply_filters() deterministic behavior.","\"\"\"","","import pytest","from gui.research_console import apply_filters, _norm_optional_text, _norm_optional_choice","","","def test_norm_optional_text():","    \"\"\"Test _norm_optional_text helper.\"\"\"","    # None -> None","    assert _norm_optional_text(None) is None","    ","    # Empty string -> None","    assert _norm_optional_text(\"\") is None","    assert _norm_optional_text(\" \") is None","    assert _norm_optional_text(\"\\n\\t\") is None","    ","    # Non-string -> string","    assert _norm_optional_text(123) == \"123\"","    assert _norm_optional_text(True) == \"True\"","    ","    # String with whitespace -> trimmed","    assert _norm_optional_text(\"  hello  \") == \"hello\"","    assert _norm_optional_text(\"hello\\n\") == \"hello\"","    assert _norm_optional_text(\"\\thello\\t\") == \"hello\"","","","def test_norm_optional_choice():","    \"\"\"Test _norm_optional_choice helper.\"\"\"","    # None -> None","    assert _norm_optional_choice(None) is None","    assert _norm_optional_choice(None, all_tokens=(\"ALL\", \"UNDECIDED\")) is None","    ","    # Empty/whitespace -> None","    assert _norm_optional_choice(\"\") is None","    assert _norm_optional_choice(\" \") is None","    assert _norm_optional_choice(\"\\n\\t\") is None","    ","    # ALL tokens -> None (case-insensitive)","    assert _norm_optional_choice(\"ALL\") is None","    assert _norm_optional_choice(\"all\") is None","    assert _norm_optional_choice(\" All \") is None","    assert _norm_optional_choice(\"UNDECIDED\", all_tokens=(\"ALL\", \"UNDECIDED\")) is None","    assert _norm_optional_choice(\"undecided\", all_tokens=(\"ALL\", \"UNDECIDED\")) is None","    ","    # Other values -> trimmed original","    assert _norm_optional_choice(\"AAPL\") == \"AAPL\"","    assert _norm_optional_choice(\"  AAPL  \") == \"AAPL\"","    assert _norm_optional_choice(\"keep\") == \"keep\"  # NOT uppercased","    assert _norm_optional_choice(\"KEEP\") == \"KEEP\"","","","def test_apply_filters_empty_rows():","    \"\"\"Test with empty rows.\"\"\"","    rows = []","    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=None)","    assert result == []","","","def test_apply_filters_no_filters():","    \"\"\"Test with no filters applied.\"\"\"","    rows = [","        {\"run_id\": \"run1\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run2\", \"symbol\": \"GOOG\", \"strategy_id\": \"strategy2\", \"decision\": \"DROP\"},","    ]","    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=None)","    assert result == rows","","","def test_apply_filters_text_normalize():","    \"\"\"Test text filter normalization.\"\"\"","    rows = [","        {\"run_id\": \"run1\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run2\", \"symbol\": \"GOOG\", \"strategy_id\": \"strategy2\", \"decision\": \"DROP\"},","    ]","    ","    # Empty string should not filter","    result = apply_filters(rows, text=\"\", symbol=None, strategy_id=None, decision=None)","    assert len(result) == 2","    ","    # Whitespace-only should not filter","    result = apply_filters(rows, text=\" \", symbol=None, strategy_id=None, decision=None)","    assert len(result) == 2","    ","    result = apply_filters(rows, text=\"\\n\\t\", symbol=None, strategy_id=None, decision=None)","    assert len(result) == 2","    ","    # Actual text should filter","    result = apply_filters(rows, text=\"run1\", symbol=None, strategy_id=None, decision=None)","    assert len(result) == 1","    assert result[0][\"run_id\"] == \"run1\"","","","def test_apply_filters_choice_normalize():","    \"\"\"Test choice filter normalization.\"\"\"","    rows = [","        {\"run_id\": \"run1\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run2\", \"symbol\": \"GOOG\", \"strategy_id\": \"strategy2\", \"decision\": \"DROP\"},","    ]","    ","    # ALL should not filter (case-insensitive)","    result = apply_filters(rows, text=None, symbol=\"ALL\", strategy_id=None, decision=None)","    assert len(result) == 2","    ","    result = apply_filters(rows, text=None, symbol=\"all\", strategy_id=None, decision=None)","    assert len(result) == 2","    ","    result = apply_filters(rows, text=None, symbol=\" All \", strategy_id=None, decision=None)","    assert len(result) == 2","    ","    # Same for strategy_id","    result = apply_filters(rows, text=None, symbol=None, strategy_id=\"ALL\", decision=None)","    assert len(result) == 2","    ","    # Same for decision","    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=\"ALL\")","    assert len(result) == 2","","","def test_apply_filters_undecided_semantics():","    \"\"\"Test UNDECIDED decision filter semantics.\"\"\"","    rows = [","        {\"run_id\": \"run1\", \"symbol\": \"AAPL\", \"strategy_id\": \"s1\", \"decision\": None},","        {\"run_id\": \"run2\", \"symbol\": \"GOOG\", \"strategy_id\": \"s2\", \"decision\": \"\"},","        {\"run_id\": \"run3\", \"symbol\": \"MSFT\", \"strategy_id\": \"s3\", \"decision\": \" \"},","        {\"run_id\": \"run4\", \"symbol\": \"TSLA\", \"strategy_id\": \"s4\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run5\", \"symbol\": \"NVDA\", \"strategy_id\": \"s5\", \"decision\": \"DROP\"},","    ]","    ","    # UNDECIDED should match None, empty string, and whitespace-only","    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=\"UNDECIDED\")","    assert len(result) == 3","    run_ids = {r[\"run_id\"] for r in result}","    assert run_ids == {\"run1\", \"run2\", \"run3\"}","    ","    # Case-insensitive","    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=\"undecided\")","    assert len(result) == 3","    ","    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=\" Undecided \")","    assert len(result) == 3","","","def test_apply_filters_case_insensitive():","    \"\"\"Test case-insensitive filtering.\"\"\"","    rows = [","        {\"run_id\": \"RUN1\", \"symbol\": \"AAPL\", \"strategy_id\": \"STRATEGY1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run2\", \"symbol\": \"goog\", \"strategy_id\": \"strategy2\", \"decision\": \"drop\"},","    ]","    ","    # Symbol filter case-insensitive","    result = apply_filters(rows, text=None, symbol=\"aapl\", strategy_id=None, decision=None)","    assert len(result) == 1","    assert result[0][\"symbol\"] == \"AAPL\"","    ","    result = apply_filters(rows, text=None, symbol=\"AAPL\", strategy_id=None, decision=None)","    assert len(result) == 1","    assert result[0][\"symbol\"] == \"AAPL\"","    ","    # Strategy filter case-insensitive","    result = apply_filters(rows, text=None, symbol=None, strategy_id=\"strategy1\", decision=None)","    assert len(result) == 1","    assert result[0][\"strategy_id\"] == \"STRATEGY1\"","    ","    # Decision filter case-insensitive","    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=\"keep\")","    assert len(result) == 1","    assert result[0][\"decision\"] == \"KEEP\"","    ","    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=\"KEEP\")","    assert len(result) == 1","    assert result[0][\"decision\"] == \"KEEP\"","","","def test_apply_filters_text_search():","    \"\"\"Test text filter.\"\"\"","    rows = [","        {\"run_id\": \"run_aapl_001\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy1\", \"decision\": \"KEEP\"},","        {\"run_id\": \"run_goog_002\", \"symbol\": \"GOOG\", \"strategy_id\": \"strategy2\", \"decision\": \"DROP\"},","        {\"run_id\": \"run_aapl_003\", \"symbol\": \"AAPL\", \"strategy_id\": \"strategy3\", \"decision\": \"ARCHIVE\"},","    ]","    ","    # Search in run_id","    result = apply_filters(rows, text=\"aapl\", symbol=None, strategy_id=None, decision=None)","    assert len(result) == 2","    assert all(\"aapl\" in row[\"run_id\"].lower() for row in result)","    ","    # Search in symbol","    result = apply_filters(rows, text=\"goog\", symbol=None, strategy_id=None, decision=None)","    assert len(result) == 1","    assert result[0][\"symbol\"] == \"GOOG\"","    ","    # Search in strategy_id","    result = apply_filters(rows, text=\"strategy2\", symbol=None, strategy_id=None, decision=None)","    assert len(result) == 1","    assert result[0][\"strategy_id\"] == \"strategy2\"","    "]}
