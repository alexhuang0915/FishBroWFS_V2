{"type":"meta","schema_version":2,"run_id":"20251227_193451Z","repo_root":"/home/fishbro/FishBroWFS_V2","snapshot_root":"SNAPSHOT","part":2,"parts":10,"created_at":"2025-12-27T19:34:51Z","generator":{"name":"dump_context","version":"vNext-balanced"},"policies":{"max_parts":10,"target_bytes_per_part":500000,"chunk_max_lines":200,"chunk_max_bytes":120000,"outputs_max_bytes_for_content":2000000,"outputs_policy":"manifest_only_for_big_files","forbid_file_truncated":true,"balance_parts":true,"estimated_total_bytes":3601200,"balance_buffer_ratio":1.08}}
{"type":"file_chunk","path":"src/core/oom_gate.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        ","    Returns:","        Dictionary with action, reason, estimated_bytes, new_cfg, and metadata","    \"\"\"","    # pure: never mutate caller","    base_cfg = dict(cfg)","    ","    bars = int(base_cfg.get(\"bars\", 0))","    params_total = int(base_cfg.get(\"params_total\", 0))","    ","    def _mem_mb(cfg_dict: dict[str, Any], work_factor: float) -> float:","        \"\"\"","        Estimate memory in MB.","        ","        Always uses oom_cost_model.estimate_memory_bytes to respect monkeypatch.","        \"\"\"","        b = oom_cost_model.estimate_memory_bytes(cfg_dict, work_factor=work_factor)","        return float(b) / (1024.0 * 1024.0)","    ","    original = float(base_cfg.get(\"param_subsample_rate\", 1.0))","    original = max(0.0, min(1.0, original))","    ","    # invalid input → BLOCK","    if bars <= 0 or params_total <= 0:","        mem0 = _mem_mb(base_cfg, work_factor)","        return _build_result(","            action=\"BLOCK\",","            reason=\"invalid_input\",","            new_cfg=base_cfg,","            original_subsample=original,","            final_subsample=original,","            mem_est_mb=mem0,","            mem_limit_mb=mem_limit_mb,","            params_total=params_total,","            allow_auto_downsample=allow_auto_downsample,","            auto_downsample_step=auto_downsample_step,","            auto_downsample_min=auto_downsample_min,","            work_factor=work_factor,","        )","    ","    mem0 = _mem_mb(base_cfg, work_factor)","    ","    if mem0 <= mem_limit_mb:","        return _build_result(","            action=\"PASS\",","            reason=\"pass_under_limit\",","            new_cfg=dict(base_cfg),","            original_subsample=original,","            final_subsample=original,","            mem_est_mb=mem0,","            mem_limit_mb=mem_limit_mb,","            params_total=params_total,","            allow_auto_downsample=allow_auto_downsample,","            auto_downsample_step=auto_downsample_step,","            auto_downsample_min=auto_downsample_min,","            work_factor=work_factor,","        )","    ","    if not allow_auto_downsample:","        return _build_result(","            action=\"BLOCK\",","            reason=\"block: over limit (auto-downsample disabled)\",","            new_cfg=dict(base_cfg),","            original_subsample=original,","            final_subsample=original,","            mem_est_mb=mem0,","            mem_limit_mb=mem_limit_mb,","            params_total=params_total,","            allow_auto_downsample=allow_auto_downsample,","            auto_downsample_step=auto_downsample_step,","            auto_downsample_min=auto_downsample_min,","            work_factor=work_factor,","        )","    ","    step = float(auto_downsample_step)","    if not (0.0 < step < 1.0):","        # contract: step must reduce","        step = 0.5","    ","    min_rate = float(auto_downsample_min)","    min_rate = max(0.0, min(1.0, min_rate))","    ","    # Monotonic step-search: always decrease","    cur = original","    best_cfg: dict[str, Any] | None = None","    best_mem: float | None = None","    ","    while True:","        nxt = cur * step","        # Clamp to min_rate before evaluating","        if nxt < min_rate:","            nxt = min_rate","        ","        # if we can no longer decrease, break","        if nxt >= cur:","            break","        ","        cand = dict(base_cfg)","        cand[\"param_subsample_rate\"] = float(nxt)","        mem_c = _mem_mb(cand, work_factor)","        ","        if mem_c <= mem_limit_mb:","            best_cfg = cand","            best_mem = mem_c","            break","        ","        # still over limit","        cur = nxt","        # Only break if we've evaluated min_rate and it's still over","        if cur <= min_rate + 1e-12:","            # We *have evaluated* min_rate and it's still over => BLOCK","            break","    ","    if best_cfg is not None and best_mem is not None:","        final_subsample = float(best_cfg[\"param_subsample_rate\"])","        # Ensure monotonicity: final_subsample <= original","        assert final_subsample <= original, f\"final_subsample {final_subsample} > original {original}\"","        return _build_result(","            action=\"AUTO_DOWNSAMPLE\",","            reason=\"auto-downsample: over limit, reduced subsample\",","            new_cfg=best_cfg,","            original_subsample=original,","            final_subsample=final_subsample,","            mem_est_mb=best_mem,","            mem_limit_mb=mem_limit_mb,","            params_total=params_total,","            allow_auto_downsample=allow_auto_downsample,","            auto_downsample_step=auto_downsample_step,","            auto_downsample_min=auto_downsample_min,","            work_factor=work_factor,","        )","    ","    # even at minimum still over limit => BLOCK","    # Only reach here if we've evaluated min_rate and it's still over","    min_cfg = dict(base_cfg)","    min_cfg[\"param_subsample_rate\"] = float(min_rate)","    mem_min = _mem_mb(min_cfg, work_factor)","    ","    return _build_result(","        action=\"BLOCK\",","        reason=\"block: min_subsample still too large\",","        new_cfg=min_cfg,  # keep audit: this is the best we can do","        original_subsample=original,","        final_subsample=float(min_rate),","        mem_est_mb=mem_min,","        mem_limit_mb=mem_limit_mb,","        params_total=params_total,","        allow_auto_downsample=allow_auto_downsample,","        auto_downsample_step=auto_downsample_step,","        auto_downsample_min=auto_downsample_min,","        work_factor=work_factor,","    )","","","def _build_result(","    *,","    action: str,","    reason: str,","    new_cfg: dict[str, Any],","    original_subsample: float,","    final_subsample: float,","    mem_est_mb: float,","    mem_limit_mb: float,","    params_total: int,","    allow_auto_downsample: bool,","    auto_downsample_step: float,","    auto_downsample_min: float,","    work_factor: float,",") -> Dict[str, Any]:","    \"\"\"Helper to build consistent result dict.\"\"\"","    params_eff = _params_effective(params_total, final_subsample)","    ops_est = _estimate_ops(new_cfg, params_effective=params_eff)","    ","    # Calculate time estimate from ops_est","    ops_per_sec_est = float(new_cfg.get(\"ops_per_sec_est\", 2.0e7))","    time_est_s = float(ops_est) / ops_per_sec_est if ops_per_sec_est > 0 else 0.0","    ","    mem_est_bytes = int(mem_est_mb * 1024.0 * 1024.0)","    mem_limit_bytes = int(mem_limit_mb * 1024.0 * 1024.0)","    ","    estimates = {","        \"mem_est_bytes\": int(mem_est_bytes),","        \"mem_est_mb\": float(mem_est_mb),","        \"mem_limit_mb\": float(mem_limit_mb),","        \"mem_limit_bytes\": int(mem_limit_bytes),","        \"ops_est\": int(ops_est),","        \"time_est_s\": float(time_est_s),","    }","    return {","        \"action\": action,","        \"reason\": reason,","        # ✅ tests/test_oom_gate.py needs this","        \"estimated_bytes\": int(mem_est_bytes),","        \"estimated_mb\": float(mem_est_mb),","        # ✅ NEW: required by tests/test_oom_gate.py","        \"mem_limit_mb\": float(mem_limit_mb),","        \"mem_limit_bytes\": int(mem_limit_bytes),","        # Original subsample contract","        \"original_subsample\": float(original_subsample),","        \"final_subsample\": float(final_subsample),"]}
{"type":"file_chunk","path":"src/core/oom_gate.py","chunk_index":2,"line_start":401,"line_end":415,"content":["        # ✅ NEW: new_cfg SSOT (never mutate original cfg)","        \"new_cfg\": new_cfg,","        # Funnel/README common fields (preserved)","        \"params_total\": int(params_total),","        \"params_effective\": int(params_eff),","        # ✅ funnel_runner/tests needs estimates.ops_est / estimates.mem_est_mb","        \"estimates\": estimates,","        # Other debug fields","        \"allow_auto_downsample\": bool(allow_auto_downsample),","        \"auto_downsample_step\": float(auto_downsample_step),","        \"auto_downsample_min\": float(auto_downsample_min),","        \"work_factor\": float(work_factor),","    }","",""]}
{"type":"file_footer","path":"src/core/oom_gate.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/core/paths.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1068,"sha256":"6b86599ad1f957c320ccf026680e5d2c3b7b54da7e3ef0863b4d6afc65ea9858","total_lines":45,"chunk_count":1}
{"type":"file_chunk","path":"src/core/paths.py","chunk_index":0,"line_start":1,"line_end":45,"content":["","\"\"\"Path management for artifact output.","","Centralized contract for output directory structure.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","","def get_run_dir(outputs_root: Path, season: str, run_id: str) -> Path:","    \"\"\"","    Get path for a specific run.","    ","    Fixed path structure: outputs/seasons/{season}/runs/{run_id}/","    ","    Args:","        outputs_root: Root outputs directory (e.g., Path(\"outputs\"))","        season: Season identifier","        run_id: Run ID","        ","    Returns:","        Path to run directory","    \"\"\"","    return outputs_root / \"seasons\" / season / \"runs\" / run_id","","","def ensure_run_dir(outputs_root: Path, season: str, run_id: str) -> Path:","    \"\"\"","    Ensure run directory exists and return its path.","    ","    Args:","        outputs_root: Root outputs directory","        season: Season identifier","        run_id: Run ID","        ","    Returns:","        Path to run directory (created if needed)","    \"\"\"","    run_dir = get_run_dir(outputs_root, season, run_id)","    run_dir.mkdir(parents=True, exist_ok=True)","    return run_dir","",""]}
{"type":"file_footer","path":"src/core/paths.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/policy_engine.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4310,"sha256":"13ff6077cfb01c4dc99be0bf38875eaf8ba0f678999e994a9de4f95d15d35024","total_lines":157,"chunk_count":1}
{"type":"file_chunk","path":"src/core/policy_engine.py","chunk_index":0,"line_start":1,"line_end":157,"content":["\"\"\"Policy Engine - 實盤安全鎖","","系統動作風險等級分類與強制執行政策。","\"\"\"","","import os","from pathlib import Path","from typing import Optional","","from core.action_risk import RiskLevel, ActionPolicyDecision","from core.season_state import load_season_state","","# 常數定義","LIVE_TOKEN_PATH = Path(\"outputs/live_enable.token\")","LIVE_TOKEN_MAGIC = \"ALLOW_LIVE_EXECUTE\"","","# 動作白名單（硬編碼）","READ_ONLY = {","    \"view_history\",","    \"list_jobs\",","    \"get_job_status\",","    \"get_artifacts\",","    \"health\",","    \"list_datasets\",","    \"list_strategies\",","    \"get_job\",","    \"list_recent_jobs\",","    \"get_rolling_summary\",","    \"get_season_report\",","    \"list_chart_artifacts\",","    \"load_chart_artifact\",","    \"get_jobs_for_deploy\",","    \"get_system_settings\",","}","","RESEARCH_MUTATE = {","    \"submit_job\",","    \"run_job\",","    \"build_portfolio\",","    \"archive\",","    \"export\",","    \"freeze_season\",","    \"unfreeze_season\",","    \"generate_deploy_zip\",","    \"update_system_settings\",","}","","LIVE_EXECUTE = {","    \"deploy_live\",","    \"send_orders\",","    \"broker_connect\",","    \"promote_to_live\",","}","","","def classify_action(action: str) -> RiskLevel:","    \"\"\"分類動作風險等級","    ","    Args:","        action: 動作名稱","        ","    Returns:","        RiskLevel: 風險等級","        ","    Note:","        未知動作一律視為 LIVE_EXECUTE（fail-safe）","    \"\"\"","    if action in READ_ONLY:","        return RiskLevel.READ_ONLY","    if action in RESEARCH_MUTATE:","        return RiskLevel.RESEARCH_MUTATE","    if action in LIVE_EXECUTE:","        return RiskLevel.LIVE_EXECUTE","    # 未知動作：fail-safe，視為最高風險","    return RiskLevel.LIVE_EXECUTE","","","def enforce_action_policy(action: str, season: Optional[str] = None) -> ActionPolicyDecision:","    \"\"\"強制執行動作政策","    ","    Args:","        action: 動作名稱","        season: 季節識別碼（可選）","        ","    Returns:","        ActionPolicyDecision: 政策決策結果","    \"\"\"","    risk = classify_action(action)","","    # LIVE_EXECUTE: 需要雙重驗證（env + token）","    if risk == RiskLevel.LIVE_EXECUTE:","        if os.getenv(\"FISHBRO_ENABLE_LIVE\") != \"1\":","            return ActionPolicyDecision(","                allowed=False,","                reason=\"LIVE_EXECUTE disabled: set FISHBRO_ENABLE_LIVE=1\",","                risk=risk,","                action=action,","                season=season,","            )","        if not LIVE_TOKEN_PATH.exists():","            return ActionPolicyDecision(","                allowed=False,","                reason=f\"LIVE_EXECUTE disabled: missing token {LIVE_TOKEN_PATH}\",","                risk=risk,","                action=action,","                season=season,","            )","        try:","            token_content = LIVE_TOKEN_PATH.read_text(encoding=\"utf-8\").strip()","            if token_content != LIVE_TOKEN_MAGIC:","                return ActionPolicyDecision(","                    allowed=False,","                    reason=\"LIVE_EXECUTE disabled: invalid token content\",","                    risk=risk,","                    action=action,","                    season=season,","                )","        except Exception:","            return ActionPolicyDecision(","                allowed=False,","                reason=\"LIVE_EXECUTE disabled: cannot read token file\",","                risk=risk,","                action=action,","                season=season,","            )","        return ActionPolicyDecision(","            allowed=True,","            reason=\"LIVE_EXECUTE enabled\",","            risk=risk,","            action=action,","            season=season,","        )","","    # RESEARCH_MUTATE: 檢查季節是否凍結","    if risk == RiskLevel.RESEARCH_MUTATE and season:","        try:","            state = load_season_state(season)","            if state.is_frozen():","                return ActionPolicyDecision(","                    allowed=False,","                    reason=f\"Season {season} is frozen\",","                    risk=risk,","                    action=action,","                    season=season,","                )","        except Exception:","            # 如果載入狀態失敗，假設季節未凍結（安全側）","            pass","","    # READ_ONLY 或允許的 RESEARCH_MUTATE","    return ActionPolicyDecision(","        allowed=True,","        reason=\"OK\",","        risk=risk,","        action=action,","        season=season,","    )"]}
{"type":"file_footer","path":"src/core/policy_engine.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/processor.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":23718,"sha256":"30d30fdd1f73cdafb18ab2303563446d807d653bc7c00353a9ad86115dbc44ed","total_lines":623,"chunk_count":4}
{"type":"file_chunk","path":"src/core/processor.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"StateProcessor - single executor for Attack #9 – Headless Intent-State Contract.","","StateProcessor is the single consumer that processes intents sequentially.","All side effects must happen only inside StateProcessor. It reads intents from","ActionQueue, processes them, and produces new SystemState snapshots.","\"\"\"","","from __future__ import annotations","","import asyncio","import logging","import time","from datetime import datetime","from typing import Any, Dict, List, Optional, Tuple, Callable, Type","from concurrent.futures import ThreadPoolExecutor","","from core.intents import (","    Intent, UserIntent, IntentType, IntentStatus,","    CreateJobIntent, CalculateUnitsIntent, CheckSeasonIntent,","    GetJobStatusIntent, ListJobsIntent, GetJobLogsIntent,","    SubmitBatchIntent, ValidatePayloadIntent, BuildParquetIntent,","    FreezeSeasonIntent, ExportSeasonIntent, CompareSeasonsIntent,","    DataSpecIntent",")","from core.state import (","    SystemState, JobProgress, SeasonInfo, DatasetInfo, SystemMetrics,","    IntentQueueStatus, JobStatus, SeasonStatus, DatasetStatus,","    create_initial_state, create_state_snapshot",")","from control.action_queue import ActionQueue","","","logger = logging.getLogger(__name__)","","","class ProcessingError(Exception):","    \"\"\"Error during intent processing.\"\"\"","    pass","","","class IntentHandler:","    \"\"\"Base class for intent handlers.\"\"\"","    ","    def __init__(self, processor: \"StateProcessor\"):","        self.processor = processor","        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")","    ","    async def handle(self, intent: UserIntent, current_state: SystemState) -> Tuple[SystemState, Dict[str, Any]]:","        \"\"\"Handle an intent and return new state and result.\"\"\"","        raise NotImplementedError","","","class CreateJobHandler(IntentHandler):","    \"\"\"Handler for CreateJobIntent.\"\"\"","    ","    async def handle(self, intent: CreateJobIntent, current_state: SystemState) -> Tuple[SystemState, Dict[str, Any]]:","        \"\"\"Create a job from wizard payload.\"\"\"","        self.logger.info(f\"Processing CreateJobIntent: {intent.intent_id}\")","        ","        # Validate job creation","        errors = current_state.validate_job_creation(intent.season, intent.data1.dataset_id)","        if errors:","            raise ProcessingError(f\"Job creation validation failed: {', '.join(errors)}\")","        ","        # TODO: Integrate with actual job creation logic from job_api.py","        # For now, simulate job creation","        import uuid","        job_id = str(uuid.uuid4())","        ","        # Calculate units (simplified)","        symbols_count = len(intent.data1.symbols)","        timeframes_count = len(intent.data1.timeframes)","        units = symbols_count * timeframes_count","        ","        # Create job progress","        now = datetime.now()","        job_progress = JobProgress(","            job_id=job_id,","            status=JobStatus.QUEUED,","            units_done=0,","            units_total=units,","            progress=0.0,","            created_at=now,","            updated_at=now,","            season=intent.season,","            dataset_id=intent.data1.dataset_id","        )","        ","        # Update state","        new_state = create_state_snapshot(","            current_state,","            jobs={**current_state.jobs, job_id: job_progress},","            active_job_ids={*current_state.active_job_ids, job_id},","            metrics=SystemMetrics(","                total_jobs=current_state.metrics.total_jobs + 1,","                queued_jobs=current_state.metrics.queued_jobs + 1","            )","        )","        ","        # Result for UI","        result = {","            \"job_id\": job_id,","            \"units\": units,","            \"season\": intent.season,","            \"status\": \"queued\"","        }","        ","        return new_state, result","","","class CalculateUnitsHandler(IntentHandler):","    \"\"\"Handler for CalculateUnitsIntent.\"\"\"","    ","    async def handle(self, intent: CalculateUnitsIntent, current_state: SystemState) -> Tuple[SystemState, Dict[str, Any]]:","        \"\"\"Calculate units for wizard payload.\"\"\"","        self.logger.info(f\"Processing CalculateUnitsIntent: {intent.intent_id}\")","        ","        # Calculate units (same logic as job_api.calculate_units)","        symbols_count = len(intent.data1.symbols)","        timeframes_count = len(intent.data1.timeframes)","        strategies_count = 1  # Single strategy","        filters_count = 1 if intent.data2 is None else len(intent.data2.filters) if hasattr(intent.data2, 'filters') else 1","        ","        units = symbols_count * timeframes_count * strategies_count * filters_count","        ","        # State doesn't change for calculation","        result = {","            \"units\": units,","            \"breakdown\": {","                \"symbols\": symbols_count,","                \"timeframes\": timeframes_count,","                \"strategies\": strategies_count,","                \"filters\": filters_count","            }","        }","        ","        return current_state, result","","","class CheckSeasonHandler(IntentHandler):","    \"\"\"Handler for CheckSeasonIntent.\"\"\"","    ","    async def handle(self, intent: CheckSeasonIntent, current_state: SystemState) -> Tuple[SystemState, Dict[str, Any]]:","        \"\"\"Check if a season is frozen.\"\"\"","        self.logger.info(f\"Processing CheckSeasonIntent: {intent.intent_id}\")","        ","        is_frozen = current_state.is_season_frozen(intent.season)","        ","        result = {","            \"season\": intent.season,","            \"is_frozen\": is_frozen,","            \"action\": intent.action,","            \"can_proceed\": not is_frozen","        }","        ","        if is_frozen:","            result[\"error\"] = f\"Season {intent.season} is frozen\"","        ","        return current_state, result","","","class GetJobStatusHandler(IntentHandler):","    \"\"\"Handler for GetJobStatusIntent.\"\"\"","    ","    async def handle(self, intent: GetJobStatusIntent, current_state: SystemState) -> Tuple[SystemState, Dict[str, Any]]:","        \"\"\"Get job status with units progress.\"\"\"","        self.logger.info(f\"Processing GetJobStatusIntent: {intent.intent_id}\")","        ","        job = current_state.get_job(intent.job_id)","        if not job:","            raise ProcessingError(f\"Job not found: {intent.job_id}\")","        ","        result = {","            \"job_id\": job.job_id,","            \"status\": job.status.value,","            \"units_done\": job.units_done,","            \"units_total\": job.units_total,","            \"progress\": job.progress,","            \"created_at\": job.created_at.isoformat(),","            \"updated_at\": job.updated_at.isoformat(),","            \"season\": job.season,","            \"dataset_id\": job.dataset_id","        }","        ","        return current_state, result","","","class ListJobsHandler(IntentHandler):","    \"\"\"Handler for ListJobsIntent.\"\"\"","    ","    async def handle(self, intent: ListJobsIntent, current_state: SystemState) -> Tuple[SystemState, Dict[str, Any]]:","        \"\"\"List jobs with progress.\"\"\"","        self.logger.info(f\"Processing ListJobsIntent: {intent.intent_id}\")","        ","        # Get recent jobs","        jobs = list(current_state.jobs.values())","        jobs.sort(key=lambda j: j.updated_at, reverse=True)","        jobs = jobs[:intent.limit]","        ","        result = {"]}
{"type":"file_chunk","path":"src/core/processor.py","chunk_index":1,"line_start":201,"line_end":400,"content":["            \"jobs\": [","                {","                    \"job_id\": job.job_id,","                    \"status\": job.status.value,","                    \"units_done\": job.units_done,","                    \"units_total\": job.units_total,","                    \"progress\": job.progress,","                    \"created_at\": job.created_at.isoformat(),","                    \"updated_at\": job.updated_at.isoformat(),","                    \"season\": job.season,","                    \"dataset_id\": job.dataset_id","                }","                for job in jobs","            ],","            \"total\": len(current_state.jobs),","            \"limit\": intent.limit","        }","        ","        return current_state, result","","","class ValidatePayloadHandler(IntentHandler):","    \"\"\"Handler for ValidatePayloadIntent.\"\"\"","    ","    async def handle(self, intent: ValidatePayloadIntent, current_state: SystemState) -> Tuple[SystemState, Dict[str, Any]]:","        \"\"\"Validate wizard payload.\"\"\"","        self.logger.info(f\"Processing ValidatePayloadIntent: {intent.intent_id}\")","        ","        # TODO: Integrate with actual validation logic from job_api.validate_wizard_payload","        # For now, do basic validation","        errors = []","        ","        payload = intent.payload","        ","        # Check required fields","        required_fields = [\"season\", \"data1\", \"strategy_id\", \"params\"]","        for field in required_fields:","            if field not in payload:","                errors.append(f\"Missing required field: {field}\")","        ","        # Check data1","        if \"data1\" in payload:","            data1 = payload[\"data1\"]","            if not isinstance(data1, dict):","                errors.append(\"data1 must be a dictionary\")","            else:","                if \"dataset_id\" not in data1:","                    errors.append(\"data1 missing dataset_id\")","                if \"symbols\" not in data1:","                    errors.append(\"data1 missing symbols\")","                if \"timeframes\" not in data1:","                    errors.append(\"data1 missing timeframes\")","        ","        result = {","            \"is_valid\": len(errors) == 0,","            \"errors\": errors,","            \"warnings\": []  # Could add warnings here","        }","        ","        return current_state, result","","","class BuildParquetHandler(IntentHandler):","    \"\"\"Handler for BuildParquetIntent.\"\"\"","    ","    async def handle(self, intent: BuildParquetIntent, current_state: SystemState) -> Tuple[SystemState, Dict[str, Any]]:","        \"\"\"Build Parquet files for a dataset.\"\"\"","        self.logger.info(f\"Processing BuildParquetIntent: {intent.intent_id}\")","        ","        # Check if dataset exists","        dataset = current_state.get_dataset(intent.dataset_id)","        if not dataset:","            raise ProcessingError(f\"Dataset not found: {intent.dataset_id}\")","        ","        # Check if already building","        if intent.dataset_id in current_state.active_builds:","            raise ProcessingError(f\"Dataset already being built: {intent.dataset_id}\")","        ","        # Update state to show building in progress","        new_state = create_state_snapshot(","            current_state,","            active_builds={*current_state.active_builds, intent.dataset_id}","        )","        ","        # TODO: Actually build Parquet files","        # Simulate building","        await asyncio.sleep(0.1)  # Simulate work","        ","        # Update dataset status","        updated_dataset = DatasetInfo(","            **dataset.model_dump(),","            status=DatasetStatus.AVAILABLE,","            has_parquet=True,","            last_built_at=datetime.now()","        )","        ","        new_state = create_state_snapshot(","            new_state,","            datasets={**new_state.datasets, intent.dataset_id: updated_dataset},","            active_builds=new_state.active_builds - {intent.dataset_id}","        )","        ","        result = {","            \"dataset_id\": intent.dataset_id,","            \"status\": \"built\",","            \"has_parquet\": True,","            \"built_at\": datetime.now().isoformat()","        }","        ","        return new_state, result","","","class StateProcessor:","    \"\"\"Single executor that processes intents sequentially.","    ","    All side effects must happen only inside StateProcessor. It reads intents","    from ActionQueue, processes them, and produces new SystemState snapshots.","    \"\"\"","    ","    def __init__(self, action_queue: ActionQueue, initial_state: Optional[SystemState] = None):","        self.action_queue = action_queue","        self.current_state = initial_state or create_initial_state()","        self.is_running = False","        self.processing_task: Optional[asyncio.Task] = None","        self.handlers: Dict[IntentType, IntentHandler] = {}","        self.executor = ThreadPoolExecutor(max_workers=1)  # Single worker for sequential processing","        self.logger = logging.getLogger(__name__)","        ","        # Register handlers","        self._register_handlers()","    ","    def _register_handlers(self) -> None:","        \"\"\"Register intent handlers.\"\"\"","        self.handlers[IntentType.CREATE_JOB] = CreateJobHandler(self)","        self.handlers[IntentType.CALCULATE_UNITS] = CalculateUnitsHandler(self)","        self.handlers[IntentType.CHECK_SEASON] = CheckSeasonHandler(self)","        self.handlers[IntentType.GET_JOB_STATUS] = GetJobStatusHandler(self)","        self.handlers[IntentType.LIST_JOBS] = ListJobsHandler(self)","        self.handlers[IntentType.VALIDATE_PAYLOAD] = ValidatePayloadHandler(self)","        self.handlers[IntentType.BUILD_PARQUET] = BuildParquetHandler(self)","        # TODO: Add handlers for other intent types","    ","    async def start(self) -> None:","        \"\"\"Start the processor.\"\"\"","        if self.is_running:","            return","        ","        self.is_running = True","        self.processing_task = asyncio.create_task(self._process_loop())","        self.logger.info(\"StateProcessor started\")","    ","    async def stop(self) -> None:","        \"\"\"Stop the processor safely without deadlock.","","        Contract:","        - Never block the asyncio loop.","        - Idempotent.","        - Always returns promptly.","        \"\"\"","        # (A) flip running flag","        try:","            self.is_running = False","        except Exception:","            pass","","        # (B) cancel and await worker task","        task = self.processing_task","        if task is not None:","            try:","                if not task.done():","                    task.cancel()","                # IMPORTANT: wait_for only works if loop is not frozen.","                # This must return quickly after we remove blocking calls.","                await asyncio.wait_for(task, timeout=1.0)","            except (asyncio.TimeoutError, asyncio.CancelledError):","                pass","            except Exception:","                pass","            finally:","                try:","                    self.processing_task = None","                except Exception:","                    pass","","        # (C) shutdown executor without blocking event loop","        ex = self.executor","        if ex is not None:","            try:","                ex.shutdown(wait=False, cancel_futures=True)  # py3.9+","            except TypeError:","                try:","                    ex.shutdown(wait=False)","                except Exception:","                    pass","            except Exception:","                pass","            finally:","                # If the class stores it, clear it to be idempotent","                try:","                    # Note: we don't set self.executor = None because"]}
{"type":"file_chunk","path":"src/core/processor.py","chunk_index":2,"line_start":401,"line_end":600,"content":["                    # it's used in _process_intent. But we could create","                    # a new executor on next start() if needed.","                    pass","                except Exception:","                    pass","        ","        self.logger.info(\"StateProcessor stopped\")","    ","    async def _process_loop(self) -> None:","        \"\"\"Main processing loop.\"\"\"","        while self.is_running:","            try:","                # Get next intent from queue (non-blocking)","                # Note: get_next() is synchronous but we call it without await","                # because it returns Optional[UserIntent], not a coroutine","                intent = self.action_queue.get_next(block=False)","                if intent is None:","                    # No intents in queue, sleep a bit","                    await asyncio.sleep(0.1)","                    continue","                ","                # Process the intent","                await self._process_intent(intent)","                ","            except asyncio.CancelledError:","                break","            except Exception as e:","                self.logger.error(f\"Error in processing loop: {e}\", exc_info=True)","                await asyncio.sleep(1)  # Avoid tight error loop","    ","    async def _process_intent(self, intent: UserIntent) -> None:","        \"\"\"Process a single intent.\"\"\"","        start_time = time.time()","        ","        try:","            # Update intent status to processing","            intent.status = IntentStatus.PROCESSING","            intent.processed_at = datetime.now()","            ","            # Get handler for this intent type","            handler = self.handlers.get(intent.intent_type)","            if not handler:","                raise ProcessingError(f\"No handler for intent type: {intent.intent_type}\")","            ","            # Process intent with timeout to prevent indefinite hanging","            # Use asyncio.wait_for on the handler execution","            loop = asyncio.get_running_loop()","            ","            # Define the handler execution function with timeout","            async def execute_handler_with_timeout():","                # Run handler in thread pool with its own event loop","                return await loop.run_in_executor(","                    self.executor,","                    lambda: asyncio.run(handler.handle(intent, self.current_state))","                )","            ","            # Execute with timeout (30 seconds should be plenty for any handler)","            try:","                new_state, result = await asyncio.wait_for(","                    execute_handler_with_timeout(),","                    timeout=30.0","                )","            except asyncio.TimeoutError:","                raise ProcessingError(f\"Handler timed out after 30 seconds for intent {intent.intent_id}\")","            ","            # Update state","            self.current_state = new_state","            ","            # Update intent status","            intent.status = IntentStatus.COMPLETED","            intent.result = result","            ","            # Notify queue that intent is completed","            self.action_queue.mark_completed(intent.intent_id, result)","            ","            processing_time_ms = (time.time() - start_time) * 1000","            self.logger.info(f\"Processed intent {intent.intent_id} ({intent.intent_type}) in {processing_time_ms:.1f}ms\")","            ","        except Exception as e:","            # Handle processing error","            intent.status = IntentStatus.FAILED","            intent.error_message = str(e)","            self.logger.error(f\"Failed to process intent {intent.intent_id}: {e}\", exc_info=True)","            ","            # Notify queue that intent failed","            self.action_queue.mark_failed(intent.intent_id, str(e))","            ","            # Update metrics","            intent_queue_dict = self.current_state.intent_queue.model_dump()","            intent_queue_dict['failed_count'] = self.current_state.intent_queue.failed_count + 1","            self.current_state = create_state_snapshot(","                self.current_state,","                intent_queue=IntentQueueStatus(**intent_queue_dict)","            )","    ","    def get_state(self) -> SystemState:","        \"\"\"Get current system state snapshot.\"\"\"","        return self.current_state","    ","    def submit_intent(self, intent: UserIntent) -> str:","        \"\"\"Submit an intent to the action queue.","        ","        Returns the intent ID for tracking.","        \"\"\"","        return self.action_queue.submit(intent)","    ","    def get_intent_status(self, intent_id: str) -> Optional[UserIntent]:","        \"\"\"Get intent status by ID.\"\"\"","        return self.action_queue.get_intent(intent_id)","    ","    async def wait_for_intent(self, intent_id: str, timeout: Optional[float] = 30.0) -> Optional[UserIntent]:","        \"\"\"Wait for an intent to complete.","        ","        Hard guarantee: never hang forever.","        If timeout is None, uses default 30.0 seconds.","        Returns None on timeout or any exception (UI contract wants \"no hang\").","        \"\"\"","        # Enforce a default hard cap in production","        if timeout is None:","            timeout = 30.0","        ","        try:","            return await self.action_queue.wait_for_intent_async(intent_id, timeout=timeout)","        except Exception:","            # No propagation; UI contract wants \"no hang\"","            # Log at debug level to avoid noise in tests","            self.logger.debug(f\"wait_for_intent timed out or errored for intent {intent_id}\", exc_info=True)","            return None","    ","    def get_queue_status(self) -> IntentQueueStatus:","        \"\"\"Get queue status.\"\"\"","        return self.current_state.intent_queue","","","# Singleton instance for application use","_processor_instance: Optional[StateProcessor] = None","","","def get_processor() -> StateProcessor:","    \"\"\"Get the singleton StateProcessor instance.\"\"\"","    global _processor_instance","    if _processor_instance is None:","        from control.action_queue import get_action_queue","        action_queue = get_action_queue()","        _processor_instance = StateProcessor(action_queue)","    return _processor_instance","","","async def start_processor() -> None:","    \"\"\"Start the singleton processor.\"\"\"","    processor = get_processor()","    await processor.start()","","","async def stop_processor() -> None:","    \"\"\"Stop the singleton processor.","    ","    Hard guarantee: never hang forever.","    Uses asyncio.wait_for with timeout to ensure we don't block.","    \"\"\"","    global _processor_instance","    if _processor_instance:","        try:","            # Use timeout to prevent hanging","            await asyncio.wait_for(_processor_instance.stop(), timeout=2.0)","        except (asyncio.TimeoutError, asyncio.CancelledError):","            # If timeout, log and continue - we must not hang","            import logging","            logger = logging.getLogger(__name__)","            logger.warning(\"stop_processor timed out after 2 seconds, forcing cleanup\")","        except Exception as e:","            import logging","            logger = logging.getLogger(__name__)","            logger.error(f\"Error stopping processor: {e}\")","        finally:","            _processor_instance = None","","","def reset_processor() -> None:","    \"\"\"Reset the singleton processor (for testing).","    ","    This stops the processor if it's running and clears the singleton.","    \"\"\"","    global _processor_instance","    if _processor_instance:","        # Try to stop synchronously (not ideal but works for tests)","        import asyncio","        import warnings","        try:","            # Try to get the running loop first (Python 3.7+)","            try:","                loop = asyncio.get_running_loop()","                # If we're in a running loop, we can't call stop_processor synchronously","                # Just clear the reference and let garbage collection handle it","                _processor_instance = None","                return","            except RuntimeError:","                # No running loop, we need to handle cleanup differently","                pass","            "]}
{"type":"file_chunk","path":"src/core/processor.py","chunk_index":3,"line_start":601,"line_end":623,"content":["            # Check if there's an existing event loop","            # Suppress deprecation warning for asyncio.get_event_loop() in Python 3.10+","            with warnings.catch_warnings():","                warnings.filterwarnings(\"ignore\", category=DeprecationWarning, message=\"There is no current event loop\")","                try:","                    loop = asyncio.get_event_loop()","                except RuntimeError:","                    # No event loop exists, create a new one","                    loop = asyncio.new_event_loop()","                    asyncio.set_event_loop(loop)","            ","            if loop.is_running():","                # If loop is running, we can't call stop_processor synchronously","                # Just clear the reference and let garbage collection handle it","                _processor_instance = None","            else:","                # Run stop_processor in the event loop","                loop.run_until_complete(stop_processor())","        except (RuntimeError, Exception):","            # No event loop or other error, just clear the reference","            _processor_instance = None","    else:","        _processor_instance = None"]}
{"type":"file_footer","path":"src/core/processor.py","complete":true,"emitted_chunks":4}
{"type":"file_header","path":"src/core/resampler.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":15853,"sha256":"7b3b4f3fe2dab52e34e410870153d8ec9a6b0091ed1318e586ecbaae3087f0ea","total_lines":460,"chunk_count":3}
{"type":"file_chunk","path":"src/core/resampler.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Resampler 核心","","提供 deterministic resampling 功能，支援 session anchor 與 safe point 計算。","\"\"\"","","from __future__ import annotations","","import hashlib","import re","from dataclasses import dataclass","from datetime import datetime, timedelta, date","from typing import List, Tuple, Optional, Dict, Any, Literal","import numpy as np","import pandas as pd","","from core.dimensions import get_dimension_for_dataset","from contracts.dimensions import SessionSpec as ContractSessionSpec","","","@dataclass(frozen=True)","class SessionSpecTaipei:","    \"\"\"台北時間的交易時段規格\"\"\"","    open_hhmm: str  # HH:MM 格式，例如 \"07:00\"","    close_hhmm: str  # HH:MM 格式，例如 \"06:00\"（次日）","    breaks: List[Tuple[str, str]]  # 休市時段列表，每個時段為 (start, end)","    tz: str = \"Asia/Taipei\"","    ","    @classmethod","    def from_contract(cls, spec: ContractSessionSpec) -> SessionSpecTaipei:","        \"\"\"從 contracts SessionSpec 轉換\"\"\"","        return cls(","            open_hhmm=spec.open_taipei,","            close_hhmm=spec.close_taipei,","            breaks=spec.breaks_taipei,","            tz=spec.tz,","        )","    ","    @property","    def open_hour(self) -> int:","        \"\"\"開盤小時\"\"\"","        return int(self.open_hhmm.split(\":\")[0])","    ","    @property","    def open_minute(self) -> int:","        \"\"\"開盤分鐘\"\"\"","        return int(self.open_hhmm.split(\":\")[1])","    ","    @property","    def close_hour(self) -> int:","        \"\"\"收盤小時（處理 24:00 為 0）\"\"\"","        hour = int(self.close_hhmm.split(\":\")[0])","        if hour == 24:","            return 0","        return hour","    ","    @property","    def close_minute(self) -> int:","        \"\"\"收盤分鐘\"\"\"","        return int(self.close_hhmm.split(\":\")[1])","    ","    def is_overnight(self) -> bool:","        \"\"\"是否為隔夜時段（收盤時間小於開盤時間）\"\"\"","        open_total = self.open_hour * 60 + self.open_minute","        close_total = self.close_hour * 60 + self.close_minute","        return close_total < open_total","    ","    def session_start_for_date(self, d: date) -> datetime:","        \"\"\"","        取得指定日期的 session 開始時間","        ","        對於隔夜時段，session 開始時間為前一天的開盤時間","        例如：open=07:00, close=06:00，則 2023-01-02 的 session 開始時間為 2023-01-01 07:00","        \"\"\"","        if self.is_overnight():","            # 隔夜時段：session 開始時間為前一天的開盤時間","            session_date = d - timedelta(days=1)","        else:","            # 非隔夜時段：session 開始時間為當天的開盤時間","            session_date = d","        ","        return datetime(","            session_date.year,","            session_date.month,","            session_date.day,","            self.open_hour,","            self.open_minute,","            0,","        )","    ","    def is_in_break(self, dt: datetime) -> bool:","        \"\"\"檢查時間是否在休市時段內\"\"\"","        time_str = dt.strftime(\"%H:%M\")","        for start, end in self.breaks:","            if start <= time_str < end:","                return True","        return False","    ","    def is_in_session(self, dt: datetime) -> bool:","        \"\"\"檢查時間是否在交易時段內（不考慮休市）\"\"\"","        # 計算從 session_start 開始的經過分鐘數","        session_start = self.session_start_for_date(dt.date())","        ","        # 對於隔夜時段，需要調整計算","        if self.is_overnight():","            # 如果 dt 在 session_start 之後（同一天），則屬於當前 session","            # 如果 dt 在 session_start 之前（可能是次日），則屬於下一個 session","            if dt >= session_start:","                # 屬於當前 session","                session_end = session_start + timedelta(days=1)","                session_end = session_end.replace(","                    hour=self.close_hour,","                    minute=self.close_minute,","                    second=0,","                )","                return session_start <= dt < session_end","            else:","                # 屬於下一個 session","                session_start = self.session_start_for_date(dt.date() + timedelta(days=1))","                session_end = session_start + timedelta(days=1)","                session_end = session_end.replace(","                    hour=self.close_hour,","                    minute=self.close_minute,","                    second=0,","                )","                return session_start <= dt < session_end","        else:","            # 非隔夜時段","            # 處理 close_hhmm == \"24:00\" 的情況","            if self.close_hhmm == \"24:00\":","                # session_end 是次日的 00:00","                session_end = session_start + timedelta(days=1)","                session_end = session_end.replace(","                    hour=0,","                    minute=0,","                    second=0,","                )","            else:","                session_end = session_start.replace(","                    hour=self.close_hour,","                    minute=self.close_minute,","                    second=0,","                )","            return session_start <= dt < session_end","","","def get_session_spec_for_dataset(dataset_id: str) -> Tuple[SessionSpecTaipei, bool]:","    \"\"\"","    讀取資料集的 session 規格","    ","    Args:","        dataset_id: 資料集 ID","        ","    Returns:","        Tuple[SessionSpecTaipei, bool]:","            - SessionSpecTaipei 物件","            - dimension_found: 是否找到 dimension（True 表示找到，False 表示使用 fallback）","    \"\"\"","    # 從 dimension registry 查詢","    dimension = get_dimension_for_dataset(dataset_id)","    ","    if dimension is not None:","        # 找到 dimension，使用其 session spec","        return SessionSpecTaipei.from_contract(dimension.session), True","    ","    # 找不到 dimension，使用 fallback","    # 根據 Phase 3A 要求：open=00:00 close=24:00 breaks=[]","    fallback_spec = SessionSpecTaipei(","        open_hhmm=\"00:00\",","        close_hhmm=\"24:00\",","        breaks=[],","        tz=\"Asia/Taipei\",","    )","    ","    return fallback_spec, False","","","def compute_session_start(ts: datetime, session: SessionSpecTaipei) -> datetime:","    \"\"\"","    Return the session_start datetime (Taipei) whose session window contains ts.","    ","    Must handle overnight sessions where close < open (cross midnight).","    ","    Args:","        ts: 時間戳記（台北時間）","        session: 交易時段規格","        ","    Returns:","        session_start: 包含 ts 的 session 開始時間","    \"\"\"","    # 對於隔夜時段，需要特別處理","    if session.is_overnight():","        # 嘗試當天的 session_start","        candidate = session.session_start_for_date(ts.date())","        ","        # 檢查 ts 是否在 candidate 開始的 session 內","        if session.is_in_session(ts):","            return candidate","        "]}
{"type":"file_chunk","path":"src/core/resampler.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        # 如果不在，嘗試前一天的 session_start","        candidate = session.session_start_for_date(ts.date() - timedelta(days=1))","        if session.is_in_session(ts):","            return candidate","        ","        # 如果還是不在，嘗試後一天的 session_start","        candidate = session.session_start_for_date(ts.date() + timedelta(days=1))","        if session.is_in_session(ts):","            return candidate","        ","        # 理論上不應該到這裡，但為了安全回傳當天的 session_start","        return session.session_start_for_date(ts.date())","    else:","        # 非隔夜時段：直接使用當天的 session_start","        return session.session_start_for_date(ts.date())","","","def compute_safe_recompute_start(","    ts_append_start: datetime, ","    tf_min: int, ","    session: SessionSpecTaipei",") -> datetime:","    \"\"\"","    Safe point = session_start + floor((ts - session_start)/tf)*tf","    Then subtract tf if you want extra safety for boundary bar (optional, but deterministic).","    Must NOT return after ts_append_start.","    ","    嚴格規則（鎖死）：","    1. safe = session_start + floor(delta_minutes/tf)*tf","    2. 額外保險：safe = max(session_start, safe - tf)（確保不晚於 ts_append_start）","    ","    Args:","        ts_append_start: 新增資料的開始時間","        tf_min: timeframe 分鐘數","        session: 交易時段規格","        ","    Returns:","        safe_recompute_start: 安全重算開始時間","    \"\"\"","    # 1. 計算包含 ts_append_start 的 session_start","    session_start = compute_session_start(ts_append_start, session)","    ","    # 2. 計算從 session_start 到 ts_append_start 的總分鐘數","    delta = ts_append_start - session_start","    delta_minutes = int(delta.total_seconds() // 60)","    ","    # 3. safe = session_start + floor(delta_minutes/tf)*tf","    safe_minutes = (delta_minutes // tf_min) * tf_min","    safe = session_start + timedelta(minutes=safe_minutes)","    ","    # 4. 額外保險：safe = max(session_start, safe - tf)","    # 確保 safe 不晚於 ts_append_start（但可能早於）","    safe_extra = safe - timedelta(minutes=tf_min)","    if safe_extra >= session_start:","        safe = safe_extra","    ","    # 確保 safe 不晚於 ts_append_start","    if safe > ts_append_start:","        safe = session_start","    ","    return safe","","","def resample_ohlcv(","    ts: np.ndarray, ","    o: np.ndarray, ","    h: np.ndarray, ","    l: np.ndarray, ","    c: np.ndarray, ","    v: np.ndarray,","    tf_min: int,","    session: SessionSpecTaipei,","    start_ts: Optional[datetime] = None,",") -> Dict[str, np.ndarray]:","    \"\"\"","    Resample normalized bars -> tf bars anchored at session_start.","    ","    Must ignore bars inside breaks (drop or treat as gap; choose one and keep consistent).","    Deterministic output ordering by ts ascending.","    ","    行為規格：","    1. 只處理在交易時段內的 bars（忽略休市時段）","    2. 以 session_start 為 anchor 進行 resample","    3. 如果提供 start_ts，只處理 ts >= start_ts 的 bars","    4. 輸出 ts 遞增排序","    ","    Args:","        ts: 時間戳記陣列（datetime 物件或 UNIX seconds）","        o, h, l, c, v: OHLCV 陣列","        tf_min: timeframe 分鐘數","        session: 交易時段規格","        start_ts: 可選的開始時間，只處理此時間之後的 bars","        ","    Returns:","        字典，包含 resampled bars:","            ts: datetime64[s] 陣列","            open, high, low, close, volume: float64 或 int64 陣列","    \"\"\"","    # 輸入驗證","    n = len(ts)","    if not (len(o) == len(h) == len(l) == len(c) == len(v) == n):","        raise ValueError(\"所有輸入陣列長度必須一致\")","    ","    if n == 0:","        return {","            \"ts\": np.array([], dtype=\"datetime64[s]\"),","            \"open\": np.array([], dtype=\"float64\"),","            \"high\": np.array([], dtype=\"float64\"),","            \"low\": np.array([], dtype=\"float64\"),","            \"close\": np.array([], dtype=\"float64\"),","            \"volume\": np.array([], dtype=\"int64\"),","        }","    ","    # 轉換 ts 為 datetime 物件","    ts_datetime = []","    for t in ts:","        if isinstance(t, (int, float, np.integer, np.floating)):","            # UNIX seconds","            ts_datetime.append(datetime.fromtimestamp(t))","        elif isinstance(t, np.datetime64):","            # numpy datetime64","            # 轉換為 pandas Timestamp 然後到 datetime","            ts_datetime.append(pd.Timestamp(t).to_pydatetime())","        elif isinstance(t, datetime):","            # 已經是 datetime","            ts_datetime.append(t)","        else:","            raise TypeError(f\"不支援的時間戳記類型: {type(t)}\")","    ","    # 過濾 bars：只保留在交易時段內且不在休市時段的 bars","    valid_indices = []","    valid_ts = []","    valid_o = []","    valid_h = []","    valid_l = []","    valid_c = []","    valid_v = []","    ","    for i, dt in enumerate(ts_datetime):","        # 檢查是否在交易時段內","        if not session.is_in_session(dt):","            continue","        ","        # 檢查是否在休市時段內","        if session.is_in_break(dt):","            continue","        ","        # 檢查是否在 start_ts 之後（如果提供）","        if start_ts is not None and dt < start_ts:","            continue","        ","        valid_indices.append(i)","        valid_ts.append(dt)","        valid_o.append(o[i])","        valid_h.append(h[i])","        valid_l.append(l[i])","        valid_c.append(c[i])","        valid_v.append(v[i])","    ","    if not valid_ts:","        # 沒有有效的 bars","        return {","            \"ts\": np.array([], dtype=\"datetime64[s]\"),","            \"open\": np.array([], dtype=\"float64\"),","            \"high\": np.array([], dtype=\"float64\"),","            \"low\": np.array([], dtype=\"float64\"),","            \"close\": np.array([], dtype=\"float64\"),","            \"volume\": np.array([], dtype=\"int64\"),","        }","    ","    # 將 valid_ts 轉換為 pandas DatetimeIndex 以便 resample","    df = pd.DataFrame({","        \"open\": valid_o,","        \"high\": valid_h,","        \"low\": valid_l,","        \"close\": valid_c,","        \"volume\": valid_v,","    }, index=pd.DatetimeIndex(valid_ts, tz=None))","    ","    # 計算每個 bar 所屬的 session_start","    session_starts = [compute_session_start(dt, session) for dt in valid_ts]","    ","    # 計算從 session_start 開始的經過分鐘數","    # 我們需要將每個 bar 分配到以 session_start 為基準的 tf 分鐘區間","    # 建立一個虛擬的時間戳記：session_start + floor((dt - session_start)/tf)*tf","    bucket_times = []","    for dt, sess_start in zip(valid_ts, session_starts):","        delta = dt - sess_start","        delta_minutes = int(delta.total_seconds() // 60)","        bucket_minutes = (delta_minutes // tf_min) * tf_min","        bucket_time = sess_start + timedelta(minutes=bucket_minutes)","        bucket_times.append(bucket_time)","    ","    # 使用 bucket_times 進行分組","    df[\"bucket_time\"] = bucket_times","    ","    # 分組聚合","    grouped = df.groupby(\"bucket_time\", sort=True)","    ","    # 計算 OHLCV"]}
{"type":"file_chunk","path":"src/core/resampler.py","chunk_index":2,"line_start":401,"line_end":460,"content":["    # 開盤價：每個 bucket 的第一個 open","    # 最高價：每個 bucket 的 high 最大值","    # 最低價：每個 bucket 的 low 最小值","    # 收盤價：每個 bucket 的最後一個 close","    # 成交量：每個 bucket 的 volume 總和","    result_df = pd.DataFrame({","        \"open\": grouped[\"open\"].first(),","        \"high\": grouped[\"high\"].max(),","        \"low\": grouped[\"low\"].min(),","        \"close\": grouped[\"close\"].last(),","        \"volume\": grouped[\"volume\"].sum(),","    })","    ","    # 確保結果排序（groupby 應該已經排序，但為了安全）","    result_df = result_df.sort_index()","    ","    # 轉換為 numpy arrays","    result_ts = result_df.index.to_numpy(dtype=\"datetime64[s]\")","    ","    return {","        \"ts\": result_ts,","        \"open\": result_df[\"open\"].to_numpy(dtype=\"float64\"),","        \"high\": result_df[\"high\"].to_numpy(dtype=\"float64\"),","        \"low\": result_df[\"low\"].to_numpy(dtype=\"float64\"),","        \"close\": result_df[\"close\"].to_numpy(dtype=\"float64\"),","        \"volume\": result_df[\"volume\"].to_numpy(dtype=\"int64\"),","    }","","","def normalize_raw_bars(raw_ingest_result) -> Dict[str, np.ndarray]:","    \"\"\"","    將 RawIngestResult 轉換為 normalized bars 陣列","    ","    Args:","        raw_ingest_result: RawIngestResult 物件","        ","    Returns:","        字典，包含 normalized bars:","            ts: datetime64[s] 陣列","            open, high, low, close: float64 陣列","            volume: int64 陣列","    \"\"\"","    df = raw_ingest_result.df","    ","    # 將 ts_str 轉換為 datetime","    ts_datetime = pd.to_datetime(df[\"ts_str\"], format=\"%Y/%m/%d %H:%M:%S\")","    ","    # 轉換為 datetime64[s]","    ts_array = ts_datetime.to_numpy(dtype=\"datetime64[s]\")","    ","    return {","        \"ts\": ts_array,","        \"open\": df[\"open\"].to_numpy(dtype=\"float64\"),","        \"high\": df[\"high\"].to_numpy(dtype=\"float64\"),","        \"low\": df[\"low\"].to_numpy(dtype=\"float64\"),","        \"close\": df[\"close\"].to_numpy(dtype=\"float64\"),","        \"volume\": df[\"volume\"].to_numpy(dtype=\"int64\"),","    }","",""]}
{"type":"file_footer","path":"src/core/resampler.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/core/run_id.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":903,"sha256":"31c195852ccd834cf4c719ca2f9bf03623bfa3499bb674a95d6552b556c8a38f","total_lines":36,"chunk_count":1}
{"type":"file_chunk","path":"src/core/run_id.py","chunk_index":0,"line_start":1,"line_end":36,"content":["","\"\"\"Run ID generation for audit trail.","","Provides deterministic, sortable run IDs with timestamp and short token.","\"\"\"","","from __future__ import annotations","","import secrets","from datetime import datetime, timezone","","","def make_run_id(prefix: str | None = None) -> str:","    \"\"\"","    Generate a sortable, readable run ID.","    ","    Format: {prefix-}YYYYMMDDTHHMMSSZ-{token}","    - Timestamp ensures chronological ordering (UTC)","    - Short token (8 hex chars) provides uniqueness","    ","    Args:","        prefix: Optional prefix string (e.g., \"test\", \"prod\")","        ","    Returns:","        Run ID string, e.g., \"20251218T135221Z-a1b2c3d4\"","        or \"test-20251218T135221Z-a1b2c3d4\" if prefix provided","    \"\"\"","    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")","    tok = secrets.token_hex(4)  # 8 hex chars","    ","    if prefix:","        return f\"{prefix}-{ts}-{tok}\"","    else:","        return f\"{ts}-{tok}\"","",""]}
{"type":"file_footer","path":"src/core/run_id.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/schemas/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":35,"sha256":"7415dcb6d73912efd2b20efa20e4f11fc9a7fbfafdbd817071575e3089ef9cc0","total_lines":4,"chunk_count":1}
{"type":"file_chunk","path":"src/core/schemas/__init__.py","chunk_index":0,"line_start":1,"line_end":4,"content":["","\"\"\"Schemas for core modules.\"\"\"","",""]}
{"type":"file_footer","path":"src/core/schemas/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/core/schemas/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":193,"sha256":"df5cbbc96684a669f916ac620541bd7aaa1dab3f3e480ee68c74a8a3d989bade","note":"skipped by policy"}
{"type":"file_skipped","path":"src/core/schemas/__pycache__/oom_gate.cpython-312.pyc","reason":"cache","bytes":2376,"sha256":"d85118f82e02f6424d10f755ef3fe3815f53a76616df544c07a8f0c4651b0f08","note":"skipped by policy"}
{"type":"file_header","path":"src/core/schemas/governance.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2591,"sha256":"45ee0a268e55e562aca6afa8d61722757d80ef1b79af939136a3d181cbbda3a3","total_lines":82,"chunk_count":1}
{"type":"file_chunk","path":"src/core/schemas/governance.py","chunk_index":0,"line_start":1,"line_end":82,"content":["","\"\"\"Pydantic schema for governance.json validation.","","Validates governance decisions with KEEP/DROP/FREEZE and evidence chain.","\"\"\"","","from __future__ import annotations","","from enum import Enum","from pydantic import BaseModel, ConfigDict, Field","from typing import Any, Dict, List, Optional, Literal, TypeAlias","","","class Decision(str, Enum):","    \"\"\"Governance decision types (SSOT).\"\"\"","    KEEP = \"KEEP\"","    FREEZE = \"FREEZE\"","    DROP = \"DROP\"","","","LifecycleState: TypeAlias = Literal[\"INCUBATION\", \"CANDIDATE\", \"LIVE\", \"RETIRED\"]","","RenderHint = Literal[\"highlight\", \"chart_annotation\", \"diff\"]","","","class EvidenceLinkModel(BaseModel):","    \"\"\"Evidence link model for governance.\"\"\"","    source_path: str","    json_pointer: str","    note: str = \"\"","    render_hint: RenderHint = \"highlight\"  # Rendering hint for viewer (highlight/chart_annotation/diff)","    render_payload: dict = Field(default_factory=dict)  # Optional payload for custom rendering","","","class GovernanceDecisionRow(BaseModel):","    \"\"\"","    Governance decision row schema.","    ","    Represents a single governance decision with rule_id and evidence chain.","    \"\"\"","    strategy_id: str","    decision: Decision","    rule_id: str  # \"R1\"/\"R2\"/\"R3\"","    reason: str = \"\"","    run_id: str","    stage: str","    config_hash: Optional[str] = None","    ","    lifecycle_state: LifecycleState = \"INCUBATION\"  # Lifecycle state (INCUBATION/CANDIDATE/LIVE/RETIRED)","    ","    evidence: List[EvidenceLinkModel] = Field(default_factory=list)","    metrics_snapshot: Dict[str, Any] = Field(default_factory=dict)","    ","    # Additional fields from existing schema (for backward compatibility)","    candidate_id: Optional[str] = None","    reasons: Optional[List[str]] = None","    created_at: Optional[str] = None","    git_sha: Optional[str] = None","    ","    model_config = ConfigDict(extra=\"allow\")  # Allow extra fields for backward compatibility","","","class GovernanceReport(BaseModel):","    \"\"\"","    Governance report schema.","    ","    Validates governance.json structure with decision rows and metadata.","    Supports both items format and rows format.","    \"\"\"","    config_hash: str  # Required top-level field for DIRTY check contract","    schema_version: Optional[str] = None","    run_id: str","    rows: List[GovernanceDecisionRow] = Field(default_factory=list)","    meta: Dict[str, Any] = Field(default_factory=dict)","    ","    # Additional fields from existing schema (for backward compatibility)","    items: Optional[List[Dict[str, Any]]] = None","    metadata: Optional[Dict[str, Any]] = None","    ","    model_config = ConfigDict(extra=\"allow\")  # Allow extra fields for backward compatibility","",""]}
{"type":"file_footer","path":"src/core/schemas/governance.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/schemas/manifest.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3601,"sha256":"4c68e01e0897c24af061196c5c2f42b694bec0519561a73d7d75537f96738781","total_lines":105,"chunk_count":1}
{"type":"file_chunk","path":"src/core/schemas/manifest.py","chunk_index":0,"line_start":1,"line_end":105,"content":["","\"\"\"Pydantic schema for manifest.json validation.","","Validates run manifest with stages and artifacts tracking.","\"\"\"","","from __future__ import annotations","","from pydantic import BaseModel, ConfigDict, Field","from typing import Any, Dict, List, Optional","","","class ManifestStage(BaseModel):","    \"\"\"Stage information in manifest.\"\"\"","    name: str","    status: str  # e.g. \"DONE\"/\"FAILED\"/\"ABORTED\"","    started_at: Optional[str] = None","    finished_at: Optional[str] = None","    artifacts: Dict[str, str] = Field(default_factory=dict)  # filename -> relpath","","","class RunManifest(BaseModel):","    \"\"\"","    Run manifest schema.","    ","    Validates manifest.json structure with run metadata, config hash, and stages.","    \"\"\"","    schema_version: Optional[str] = None  # For future versioning","    run_id: str","    season: str","    config_hash: str","    created_at: Optional[str] = None","    stages: List[ManifestStage] = Field(default_factory=list)","    meta: Dict[str, Any] = Field(default_factory=dict)","    ","    # Additional fields from AuditSchema (for backward compatibility)","    git_sha: Optional[str] = None","    dirty_repo: Optional[bool] = None","    param_subsample_rate: Optional[float] = None","    dataset_id: Optional[str] = None","    bars: Optional[int] = None","    params_total: Optional[int] = None","    params_effective: Optional[int] = None","    artifact_version: Optional[str] = None","    ","    # Phase 6.5: Mandatory fingerprint (validation enforces non-empty)","    data_fingerprint_sha1: Optional[str] = None","    ","    # Phase 6.6: Timezone database metadata","    tzdb_provider: Optional[str] = None  # e.g., \"zoneinfo\"","    tzdb_version: Optional[str] = None  # Timezone database version","    data_tz: Optional[str] = None  # Data timezone (e.g., \"Asia/Taipei\")","    exchange_tz: Optional[str] = None  # Exchange timezone (e.g., \"America/Chicago\")","    ","    # Phase 7: Strategy metadata","    strategy_id: Optional[str] = None  # Strategy identifier (e.g., \"sma_cross\")","    strategy_version: Optional[str] = None  # Strategy version (e.g., \"v1\")","    param_schema_hash: Optional[str] = None  # SHA1 hash of param_schema JSON","","","class UnifiedManifest(BaseModel):","    \"\"\"","    Unified manifest schema for all manifest types (export, plan, view, quality).","    ","    This schema defines the standard fields that should be present in all manifests","    for Manifest Tree Completeness verification.","    \"\"\"","    # Common required fields","    manifest_type: str  # \"export\", \"plan\", \"view\", or \"quality\"","    manifest_version: str = \"1.0\"","    ","    # Identification fields","    id: str  # run_id for export, plan_id for plan/view/quality","    ","    # Timestamps","    generated_at_utc: Optional[str] = None","    created_at: Optional[str] = None","    ","    # Source information","    source: Optional[Dict[str, Any]] = None","    ","    # Input references (SHA256 hashes of input files)","    inputs: Optional[Dict[str, str]] = None","    ","    # Files listing with SHA256 checksums (sorted by rel_path asc)","    files: Optional[List[Dict[str, str]]] = None","    ","    # Combined SHA256 of all files (concatenated hashes)","    files_sha256: Optional[str] = None","    ","    # Checksums for output files","    checksums: Optional[Dict[str, str]] = None","    ","    # Type-specific checksums","    export_checksums: Optional[Dict[str, str]] = None","    plan_checksums: Optional[Dict[str, str]] = None","    view_checksums: Optional[Dict[str, str]] = None","    quality_checksums: Optional[Dict[str, str]] = None","    ","    # Manifest self-hash (must be the last field)","    manifest_sha256: str","    ","    model_config = ConfigDict(extra=\"allow\")  # Allow additional type-specific fields","",""]}
{"type":"file_footer","path":"src/core/schemas/manifest.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/schemas/oom_gate.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1552,"sha256":"66c245f96340e0a3c817b6c6559b07eb613ba18e3691bf22d78307f88c3b2b9f","total_lines":44,"chunk_count":1}
{"type":"file_chunk","path":"src/core/schemas/oom_gate.py","chunk_index":0,"line_start":1,"line_end":44,"content":["","\"\"\"Pydantic schemas for OOM gate input and output.","","Locked schemas for PASS/BLOCK/AUTO_DOWNSAMPLE decisions.","\"\"\"","","from __future__ import annotations","","from pydantic import BaseModel, Field","from typing import Literal","","","class OomGateInput(BaseModel):","    \"\"\"","    Input for OOM gate decision.","    ","    All fields are required for memory estimation.","    \"\"\"","    bars: int = Field(gt=0, description=\"Number of bars\")","    params: int = Field(gt=0, description=\"Total number of parameters\")","    param_subsample_rate: float = Field(gt=0.0, le=1.0, description=\"Subsample rate in [0.0, 1.0]\")","    intents_per_bar: float = Field(default=2.0, ge=0.0, description=\"Estimated intents per bar\")","    bytes_per_intent_est: int = Field(default=64, gt=0, description=\"Estimated bytes per intent\")","    ram_budget_bytes: int = Field(default=6_000_000_000, gt=0, description=\"RAM budget in bytes (default: 6GB)\")","","","class OomGateDecision(BaseModel):","    \"\"\"","    OOM gate decision output.","    ","    Contains decision (PASS/BLOCK/AUTO_DOWNSAMPLE) and recommendations.","    \"\"\"","    decision: Literal[\"PASS\", \"BLOCK\", \"AUTO_DOWNSAMPLE\"]","    estimated_bytes: int = Field(ge=0, description=\"Estimated memory usage in bytes\")","    ram_budget_bytes: int = Field(gt=0, description=\"RAM budget in bytes\")","    recommended_subsample_rate: float | None = Field(","        default=None,","        ge=0.0,","        le=1.0,","        description=\"Recommended subsample rate (only for AUTO_DOWNSAMPLE)\"","    )","    notes: str = Field(default=\"\", description=\"Human-readable notes about the decision\")","",""]}
{"type":"file_footer","path":"src/core/schemas/oom_gate.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/schemas/portfolio.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1045,"sha256":"b96982d515336c90944d6ec605835339d43d7ac386937355cd220f8f560649d3","total_lines":38,"chunk_count":1}
{"type":"file_chunk","path":"src/core/schemas/portfolio.py","chunk_index":0,"line_start":1,"line_end":38,"content":["\"\"\"Portfolio-related schemas for signal series and instrument configuration.\"\"\"","","from pydantic import BaseModel, ConfigDict, Field","from typing import Literal, Dict","","","class InstrumentsConfigV1(BaseModel):","    \"\"\"Schema for instruments configuration YAML (version 1).\"\"\"","    version: int","    base_currency: str","    fx_rates: Dict[str, float]","    instruments: Dict[str, dict]  # 這裡可先放 dict，validate 在 loader 做","","","class SignalSeriesMetaV1(BaseModel):","    \"\"\"Metadata for signal series (bar-based position/margin/notional).\"\"\"","    model_config = ConfigDict(populate_by_name=True)","    ","    schema_id: Literal[\"SIGNAL_SERIES_V1\"] = Field(","        default=\"SIGNAL_SERIES_V1\",","        alias=\"schema\"","    )","    instrument: str","    timeframe: str","    tz: str","","    base_currency: str","    instrument_currency: str","    fx_to_base: float","","    multiplier: float","    initial_margin_per_contract: float","    maintenance_margin_per_contract: float","","    # traceability","    source_run_id: str","    source_spec_sha: str","    instruments_config_sha256: str"]}
{"type":"file_footer","path":"src/core/schemas/portfolio.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/schemas/portfolio_v1.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4259,"sha256":"16d8db078a869431c0a91fb03ec6d25f1381a739cec3e09df16ef01a83dc37ba","total_lines":135,"chunk_count":1}
{"type":"file_chunk","path":"src/core/schemas/portfolio_v1.py","chunk_index":0,"line_start":1,"line_end":135,"content":["\"\"\"Portfolio engine schemas V1.\"\"\"","","from pydantic import BaseModel, Field","from typing import Literal, Dict, List, Optional","from datetime import datetime, timezone","","","class PortfolioPolicyV1(BaseModel):","    \"\"\"Portfolio policy defining allocation limits and behavior.\"\"\"","    version: Literal[\"PORTFOLIO_POLICY_V1\"] = \"PORTFOLIO_POLICY_V1\"","","    base_currency: str  # \"TWD\"","    instruments_config_sha256: str","","    # account hard caps","    max_slots_total: int  # e.g. 4","    max_margin_ratio: float  # e.g. 0.35 (margin_used/equity)","    max_notional_ratio: Optional[float] = None  # optional v1","","    # per-instrument cap (optional v1)","    max_slots_by_instrument: Dict[str, int] = Field(default_factory=dict)  # {\"CME.MNQ\":4, \"TWF.MXF\":2}","","    # deterministic tie-breaker inputs","    strategy_priority: Dict[str, int]  # {strategy_id: priority_int}","    signal_strength_field: str  # e.g. \"edge_score\" or \"signal_score\"","","    # behavior flags","    allow_force_kill: bool = False  # MUST default False","    allow_queue: bool = False  # v1: reject only","","","class PortfolioSpecV1(BaseModel):","    \"\"\"Portfolio specification defining input sources (frozen only).\"\"\"","    version: Literal[\"PORTFOLIO_SPEC_V1\"] = \"PORTFOLIO_SPEC_V1\"","    ","    # Input seasons/artifacts sources","    seasons: List[str]  # e.g. [\"2026Q1\"]","    strategy_ids: List[str]  # e.g. [\"S1\", \"S2\"]","    instrument_ids: List[str]  # e.g. [\"CME.MNQ\", \"TWF.MXF\"]","    ","    # Time range (optional)","    start_date: Optional[str] = None  # ISO format","    end_date: Optional[str] = None  # ISO format","    ","    # Reference to policy","    policy_sha256: str  # SHA256 of canonicalized PortfolioPolicyV1 JSON","    ","    # Canonicalization metadata","    spec_sha256: str  # SHA256 of this spec (computed after canonicalization)","","","class OpenPositionV1(BaseModel):","    \"\"\"Open position in the portfolio.\"\"\"","    strategy_id: str","    instrument_id: str  # MNQ / MXF","    slots: int = 1  # v1 fixed","    margin_base: float  # TWD","    notional_base: float  # TWD","    entry_bar_index: int","    entry_bar_ts: datetime","","","class SignalCandidateV1(BaseModel):","    \"\"\"Candidate signal for admission.\"\"\"","    strategy_id: str","    instrument_id: str  # MNQ / MXF","    bar_ts: datetime","    bar_index: int","    signal_strength: float  # higher = stronger signal","    candidate_score: float = 0.0  # deterministic score for sorting (higher = better)","    required_margin_base: float  # TWD","    required_slot: int = 1  # v1 fixed","    # Optional: additional metadata","    signal_series_sha256: Optional[str] = None  # for audit","","","class AdmissionDecisionV1(BaseModel):","    \"\"\"Admission decision for a candidate signal.\"\"\"","    version: Literal[\"ADMISSION_DECISION_V1\"] = \"ADMISSION_DECISION_V1\"","    ","    # Candidate identification","    strategy_id: str","    instrument_id: str","    bar_ts: datetime","    bar_index: int","    ","    # Candidate metrics","    signal_strength: float","    candidate_score: float","    signal_series_sha256: Optional[str] = None  # for audit","    ","    # Decision","    accepted: bool","    reason: Literal[","        \"ACCEPT\",","        \"REJECT_FULL\",","        \"REJECT_MARGIN\",","        \"REJECT_POLICY\",","        \"REJECT_UNKNOWN\"","    ]","    ","    # Deterministic tie-breaking info","    sort_key_used: str  # e.g., \"priority=-10,signal_strength=0.85,strategy_id=S1\"","    ","    # Portfolio state after this decision","    slots_after: int","    margin_after_base: float  # TWD","    ","    # Timestamp of decision","    decision_ts: datetime = Field(default_factory=lambda: datetime.now(timezone.utc).replace(tzinfo=None))","","","class PortfolioStateV1(BaseModel):","    \"\"\"Portfolio state at a given bar.\"\"\"","    bar_ts: datetime","    bar_index: int","    equity_base: float  # TWD","    slots_used: int","    margin_used_base: float  # TWD","    notional_used_base: float  # TWD","    open_positions: List[OpenPositionV1] = Field(default_factory=list)","    reject_count: int = 0  # cumulative rejects up to this bar","","","class PortfolioSummaryV1(BaseModel):","    \"\"\"Summary of portfolio admission results.\"\"\"","    total_candidates: int","    accepted_count: int","    rejected_count: int","    reject_reasons: Dict[str, int]  # reason -> count","    final_slots_used: int","    final_margin_used_base: float","    final_margin_ratio: float  # margin_used / equity","    policy_sha256: str","    spec_sha256: str"]}
{"type":"file_footer","path":"src/core/schemas/portfolio_v1.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/schemas/winners_v2.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2100,"sha256":"80f8e4a13ffd0ded61c8a9027209827a1f6859853687cb8feca125d21ef25d89","total_lines":67,"chunk_count":1}
{"type":"file_chunk","path":"src/core/schemas/winners_v2.py","chunk_index":0,"line_start":1,"line_end":67,"content":["","\"\"\"Pydantic schema for winners_v2.json validation.","","Validates winners v2 structure with KPI metrics.","\"\"\"","","from __future__ import annotations","","from pydantic import BaseModel, ConfigDict, Field","from typing import Any, Dict, List, Optional","","","class WinnerRow(BaseModel):","    \"\"\"","    Winner row schema.","    ","    Represents a single winner with strategy info and KPI metrics.","    \"\"\"","    strategy_id: str","    symbol: str","    timeframe: str","    params: Dict[str, Any] = Field(default_factory=dict)","    ","    # Required KPI metrics","    net_profit: float","    max_drawdown: float","    trades: int","    ","    # Optional metrics","    win_rate: Optional[float] = None","    sharpe: Optional[float] = None","    sqn: Optional[float] = None","    ","    # Evidence links (if already present)","    evidence: Dict[str, str] = Field(default_factory=dict)  # pointers/paths if already present","    ","    # Additional fields from v2 schema (for backward compatibility)","    candidate_id: Optional[str] = None","    score: Optional[float] = None","    metrics: Optional[Dict[str, Any]] = None","    source: Optional[Dict[str, Any]] = None","","","class WinnersV2(BaseModel):","    \"\"\"","    Winners v2 schema.","    ","    Validates winners_v2.json structure with rows and metadata.","    Supports both v2 format (with topk) and normalized format (with rows).","    \"\"\"","    config_hash: str  # Required top-level field for DIRTY check contract","    schema_version: Optional[str] = None  # \"v2\" or \"schema\" field","    run_id: Optional[str] = None","    stage: Optional[str] = None  # stage_name","    rows: List[WinnerRow] = Field(default_factory=list)","    meta: Dict[str, Any] = Field(default_factory=dict)","    ","    # Additional fields from v2 schema (for backward compatibility)","    schema_name: Optional[str] = Field(default=None, alias=\"schema\")  # \"v2\" - renamed to avoid conflict","    stage_name: Optional[str] = None","    generated_at: Optional[str] = None","    topk: Optional[List[Dict[str, Any]]] = None","    notes: Optional[Dict[str, Any]] = None","    ","    model_config = ConfigDict(extra=\"allow\", populate_by_name=True)  # Allow extra fields and support alias","",""]}
{"type":"file_footer","path":"src/core/schemas/winners_v2.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/season_context.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2650,"sha256":"57d3b0cd54d6665551d83abade3f88256a3b9368df38933cb9804b2f0bb233be","total_lines":83,"chunk_count":1}
{"type":"file_chunk","path":"src/core/season_context.py","chunk_index":0,"line_start":1,"line_end":83,"content":["\"\"\"","Season Context - Single Source of Truth (SSOT) for season management.","","Phase 4: Consolidate season management to avoid scattered os.getenv() calls.","\"\"\"","","import os","from pathlib import Path","from typing import Optional","","","def current_season() -> str:","    \"\"\"Return current season from env FISHBRO_CURRENT_SEASON or default '2026Q1'.\"\"\"","    return os.getenv(\"FISHBRO_CURRENT_SEASON\", \"2026Q1\")","","","def outputs_root() -> str:","    \"\"\"Return outputs root from env FISHBRO_OUTPUTS_ROOT or default 'outputs'.\"\"\"","    return os.getenv(\"FISHBRO_OUTPUTS_ROOT\", \"outputs\")","","","def season_dir(season: Optional[str] = None) -> Path:","    \"\"\"Return outputs/seasons/{season} as Path object.","    ","    Args:","        season: Season identifier (e.g., \"2026Q1\"). If None, uses current_season().","    ","    Returns:","        Path to season directory.","    \"\"\"","    if season is None:","        season = current_season()","    return Path(outputs_root()) / \"seasons\" / season","","","def research_dir(season: Optional[str] = None) -> Path:","    \"\"\"Return outputs/seasons/{season}/research as Path object.\"\"\"","    return season_dir(season) / \"research\"","","","def portfolio_dir(season: Optional[str] = None) -> Path:","    \"\"\"Return outputs/seasons/{season}/portfolio as Path object.\"\"\"","    return season_dir(season) / \"portfolio\"","","","def governance_dir(season: Optional[str] = None) -> Path:","    \"\"\"Return outputs/seasons/{season}/governance as Path object.\"\"\"","    return season_dir(season) / \"governance\"","","","def canonical_results_path(season: Optional[str] = None) -> Path:","    \"\"\"Return path to canonical_results.json.\"\"\"","    return research_dir(season) / \"canonical_results.json\"","","","def research_index_path(season: Optional[str] = None) -> Path:","    \"\"\"Return path to research_index.json.\"\"\"","    return research_dir(season) / \"research_index.json\"","","","def portfolio_summary_path(season: Optional[str] = None) -> Path:","    \"\"\"Return path to portfolio_summary.json.\"\"\"","    return portfolio_dir(season) / \"portfolio_summary.json\"","","","def portfolio_manifest_path(season: Optional[str] = None) -> Path:","    \"\"\"Return path to portfolio_manifest.json.\"\"\"","    return portfolio_dir(season) / \"portfolio_manifest.json\"","","","# Convenience function for backward compatibility","def get_season_context() -> dict:","    \"\"\"Return a dict with current season context for debugging/logging.\"\"\"","    season = current_season()","    root = outputs_root()","    return {","        \"season\": season,","        \"outputs_root\": root,","        \"season_dir\": str(season_dir(season)),","        \"research_dir\": str(research_dir(season)),","        \"portfolio_dir\": str(portfolio_dir(season)),","        \"governance_dir\": str(governance_dir(season)),","    }"]}
{"type":"file_footer","path":"src/core/season_context.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/season_state.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7362,"sha256":"06ec7773325c4f7b9399f5ef8b12f33ec7f116a5b6d8c508141ef548d1350214","total_lines":231,"chunk_count":2}
{"type":"file_chunk","path":"src/core/season_state.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"","Season State Management - Freeze governance lock.","","Phase 5: Deterministic Governance & Reproducibility Lock.","\"\"\"","","import json","import os","from datetime import datetime, timezone","from pathlib import Path","from typing import Dict, Any, Optional, Literal, TypedDict","from dataclasses import dataclass, asdict","","from .season_context import season_dir","","","class SeasonStateDict(TypedDict, total=False):","    \"\"\"Season state schema (immutable).\"\"\"","    season: str","    state: Literal[\"OPEN\", \"FROZEN\"]","    frozen_ts: Optional[str]  # ISO-8601 or null","    frozen_by: Optional[Literal[\"gui\", \"cli\", \"system\"]]  # or null","    reason: Optional[str]  # string or null","","","@dataclass","class SeasonState:","    \"\"\"Season state data class.\"\"\"","    season: str","    state: Literal[\"OPEN\", \"FROZEN\"] = \"OPEN\"","    frozen_ts: Optional[str] = None  # ISO-8601 or null","    frozen_by: Optional[Literal[\"gui\", \"cli\", \"system\"]] = None  # or null","    reason: Optional[str] = None  # string or null","    ","    @classmethod","    def from_dict(cls, data: Dict[str, Any]) -> \"SeasonState\":","        \"\"\"Create SeasonState from dictionary.\"\"\"","        return cls(","            season=data[\"season\"],","            state=data.get(\"state\", \"OPEN\"),","            frozen_ts=data.get(\"frozen_ts\"),","            frozen_by=data.get(\"frozen_by\"),","            reason=data.get(\"reason\"),","        )","    ","    def to_dict(self) -> SeasonStateDict:","        \"\"\"Convert to dictionary.\"\"\"","        return {","            \"season\": self.season,","            \"state\": self.state,","            \"frozen_ts\": self.frozen_ts,","            \"frozen_by\": self.frozen_by,","            \"reason\": self.reason,","        }","    ","    def is_frozen(self) -> bool:","        \"\"\"Check if season is frozen.\"\"\"","        return self.state == \"FROZEN\"","    ","    def freeze(self, by: Literal[\"gui\", \"cli\", \"system\"], reason: Optional[str] = None) -> None:","        \"\"\"Freeze the season.\"\"\"","        if self.is_frozen():","            raise ValueError(f\"Season {self.season} is already frozen\")","        ","        self.state = \"FROZEN\"","        self.frozen_ts = datetime.now(timezone.utc).isoformat()","        self.frozen_by = by","        self.reason = reason","    ","    def unfreeze(self, by: Literal[\"gui\", \"cli\", \"system\"], reason: Optional[str] = None) -> None:","        \"\"\"Unfreeze the season.\"\"\"","        if not self.is_frozen():","            raise ValueError(f\"Season {self.season} is not frozen\")","        ","        self.state = \"OPEN\"","        self.frozen_ts = None","        self.frozen_by = None","        self.reason = None","","","def get_season_state_path(season: Optional[str] = None) -> Path:","    \"\"\"Get path to season_state.json.\"\"\"","    season_path = season_dir(season)","    governance_dir = season_path / \"governance\"","    governance_dir.mkdir(parents=True, exist_ok=True)","    return governance_dir / \"season_state.json\"","","","def load_season_state(season: Optional[str] = None) -> SeasonState:","    \"\"\"Load season state from file, or create default if not exists.\"\"\"","    state_path = get_season_state_path(season)","    ","    if not state_path.exists():","        # Get season from context if not provided","        if season is None:","            from .season_context import current_season","            season_str = current_season()","        else:","            season_str = season","        ","        # Create default OPEN state","        state = SeasonState(season=season_str, state=\"OPEN\")","        save_season_state(state, season)","        return state","    ","    try:","        with open(state_path, \"r\", encoding=\"utf-8\") as f:","            data = json.load(f)","        ","        # Validate required fields","        if \"season\" not in data:","            # Infer season from path","            if season is None:","                from .season_context import current_season","                season_str = current_season()","            else:","                season_str = season","            data[\"season\"] = season_str","        ","        return SeasonState.from_dict(data)","    except (json.JSONDecodeError, OSError, KeyError) as e:","        # If file is corrupted, create default","        if season is None:","            from .season_context import current_season","            season_str = current_season()","        else:","            season_str = season","        ","        state = SeasonState(season=season_str, state=\"OPEN\")","        save_season_state(state, season)","        return state","","","def save_season_state(state: SeasonState, season: Optional[str] = None) -> Path:","    \"\"\"Save season state to file.\"\"\"","    state_path = get_season_state_path(season)","    ","    # Ensure directory exists","    state_path.parent.mkdir(parents=True, exist_ok=True)","    ","    # Convert to dict and write","    data = state.to_dict()","    ","    # Write atomically","    temp_path = state_path.with_suffix(\".tmp\")","    with open(temp_path, \"w\", encoding=\"utf-8\") as f:","        json.dump(data, f, indent=2, ensure_ascii=False)","    ","    # Replace original","    temp_path.replace(state_path)","    ","    return state_path","","","def check_season_not_frozen(season: Optional[str] = None, action: str = \"action\") -> None:","    \"\"\"","    Check if season is not frozen, raise ValueError if frozen.","    ","    Args:","        season: Season identifier (e.g., \"2026Q1\"). If None, uses current season.","        action: Action name for error message.","    ","    Raises:","        ValueError: If season is frozen.","    \"\"\"","    state = load_season_state(season)","    if state.is_frozen():","        frozen_info = f\"frozen at {state.frozen_ts} by {state.frozen_by}\"","        if state.reason:","            frozen_info += f\" (reason: {state.reason})\"","        raise ValueError(","            f\"Cannot perform {action}: Season {state.season} is {frozen_info}\"","        )","","","def freeze_season(","    season: Optional[str] = None,","    by: Literal[\"gui\", \"cli\", \"system\"] = \"system\",","    reason: Optional[str] = None,","    create_snapshot: bool = True,",") -> SeasonState:","    \"\"\"","    Freeze a season.","    ","    Args:","        season: Season identifier (e.g., \"2026Q1\"). If None, uses current season.","        by: Who is freezing the season.","        reason: Optional reason for freezing.","        create_snapshot: Whether to create deterministic snapshot of artifacts.","    ","    Returns:","        Updated SeasonState.","    \"\"\"","    state = load_season_state(season)","    state.freeze(by=by, reason=reason)","    save_season_state(state, season)","    ","    # Phase 5: Create deterministic snapshot","    if create_snapshot:","        try:"]}
{"type":"file_chunk","path":"src/core/season_state.py","chunk_index":1,"line_start":201,"line_end":231,"content":["            from .snapshot import create_freeze_snapshot","            snapshot_path = create_freeze_snapshot(state.season)","            # Log snapshot creation (optional)","            print(f\"Created freeze snapshot: {snapshot_path}\")","        except Exception as e:","            # Don't fail freeze if snapshot fails, but log warning","            print(f\"Warning: Failed to create freeze snapshot: {e}\")","    ","    return state","","","def unfreeze_season(","    season: Optional[str] = None,","    by: Literal[\"gui\", \"cli\", \"system\"] = \"system\",","    reason: Optional[str] = None,",") -> SeasonState:","    \"\"\"","    Unfreeze a season.","    ","    Args:","        season: Season identifier (e.g., \"2026Q1\"). If None, uses current season.","        by: Who is unfreezing the season.","        reason: Optional reason for unfreezing.","    ","    Returns:","        Updated SeasonState.","    \"\"\"","    state = load_season_state(season)","    state.unfreeze(by=by, reason=reason)","    save_season_state(state, season)","    return state"]}
{"type":"file_footer","path":"src/core/season_state.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/core/service_identity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4904,"sha256":"065131d822cd4750aed93aa17509e33736696d39a49cb384d86b855deaf40989","total_lines":164,"chunk_count":1}
{"type":"file_chunk","path":"src/core/service_identity.py","chunk_index":0,"line_start":1,"line_end":164,"content":["\"\"\"Service Identity Contract - SSOT for topology observability.","","Provides a single canonical identity payload that any running service can return.","This payload uniquely proves:","- Who is serving the request (NiceGUI vs FastAPI)","- Which git commit / version it is","- Which DB path it uses (and why)","- Which PID / process commandline is serving","\"\"\"","","from __future__ import annotations","","import os","import sys","import platform","import json","from pathlib import Path","from datetime import datetime, timezone","from typing import Dict, Any, Optional","from pydantic import BaseModel","","","class ServiceIdentity(BaseModel):","    \"\"\"Pydantic model for service identity payload.\"\"\"","    service_name: str","    pid: int","    ppid: int","    cmdline: str","    cwd: str","    python: str","    python_version: str","    platform: str","    repo_root: str","    git_commit: str","    build_time_utc: str","    env: Dict[str, Optional[str]]","    jobs_db_path: str","    jobs_db_parent: str","    worker_pidfile_path: str","    worker_log_path: str","","","_ALLOWED_ENV_KEYS = {","    \"PYTHONPATH\",","    \"JOBS_DB_PATH\",","    \"FISHBRO_TESTING\",","    \"PYTEST_CURRENT_TEST\",","    \"TMPDIR\",","    \"TMP\",","    \"TEMP\",","}","","","def _safe_cmdline() -> str:","    \"\"\"Return process commandline as string, best-effort.\"\"\"","    try:","        if Path(\"/proc/self/cmdline\").exists():","            cmdline_bytes = Path(\"/proc/self/cmdline\").read_bytes()","            # Split by null bytes, decode, filter empty","            parts = [p.decode(\"utf-8\", errors=\"replace\") for p in cmdline_bytes.split(b\"\\x00\") if p]","            return \" \".join(parts)","    except Exception:","        pass","    # Fallback for non-Linux or permission issues","    try:","        # Use psutil if available? Not required; keep simple.","        return \" \".join(sys.argv)","    except Exception:","        return \"\"","","","def _safe_git_commit(repo_root: Path) -> str:","    \"\"\"Extract git commit hash, best-effort, never raises.\"\"\"","    try:","        head = repo_root / \".git\" / \"HEAD\"","        if not head.exists():","            return \"unknown\"","        ref = head.read_text().strip()","        if ref.startswith(\"ref:\"):","            ref_path = repo_root / \".git\" / ref.split(\" \", 1)[1].strip()","            if ref_path.exists():","                return ref_path.read_text().strip()","        # Already a commit hash","        return ref","    except Exception:","        return \"unknown\"","","","def _find_repo_root(start: Path) -> Path:","    \"\"\"Climb up to find .git directory, else return start.\"\"\"","    current = start.resolve()","    for _ in range(6):  # reasonable depth","        if (current / \".git\").exists():","            return current","        if current.parent == current:","            break","        current = current.parent","    return start  # fallback","","","def get_service_identity(","    *, service_name: str, db_path: Optional[Path] = None",") -> Dict[str, Any]:","    \"\"\"Return JSON-safe identity dict.","","    Fields:","      - service_name: str (caller-defined, e.g. \"nicegui\", \"control_api\")","      - pid: int","      - ppid: int","      - cmdline: str (best-effort; if unavailable return \"\")","      - cwd: str","      - python: str (sys.executable)","      - python_version: str","      - platform: str","      - repo_root: str (best-effort)","      - git_commit: str (\"unknown\" allowed)","      - build_time_utc: str (ISO8601, generated at import time or on demand)","      - env: dict (filtered keys only)","      - jobs_db_path: str (resolved absolute if db_path provided; else \"\")","      - jobs_db_parent: str","      - worker_pidfile_path: str (db_path.parent/\"worker.pid\" if db_path provided else \"\")","      - worker_log_path: str (db_path.parent/\"worker_process.log\" if db_path provided else \"\")","    \"\"\"","    now = datetime.now(timezone.utc).isoformat()","    cwd = Path.cwd()","    repo_root = _find_repo_root(cwd)","","    env = {k: os.getenv(k) for k in _ALLOWED_ENV_KEYS if os.getenv(k) is not None}","","    jobs_db_path = \"\"","    jobs_db_parent = \"\"","    pidfile = \"\"","    wlog = \"\"","    if db_path is not None:","        rp = db_path.expanduser().resolve()","        jobs_db_path = str(rp)","        jobs_db_parent = str(rp.parent)","        pidfile = str(rp.parent / \"worker.pid\")","        wlog = str(rp.parent / \"worker_process.log\")","","    return {","        \"service_name\": service_name,","        \"pid\": os.getpid(),","        \"ppid\": os.getppid(),","        \"cmdline\": _safe_cmdline(),","        \"cwd\": str(cwd),","        \"python\": sys.executable,","        \"python_version\": sys.version,","        \"platform\": platform.platform(),","        \"repo_root\": str(repo_root),","        \"git_commit\": _safe_git_commit(repo_root),","        \"build_time_utc\": now,","        \"env\": env,","        \"jobs_db_path\": jobs_db_path,","        \"jobs_db_parent\": jobs_db_parent,","        \"worker_pidfile_path\": pidfile,","        \"worker_log_path\": wlog,","    }","","","if __name__ == \"__main__\":","    # Quick test when run directly","    ident = get_service_identity(service_name=\"test\", db_path=None)","    print(json.dumps(ident, indent=2, sort_keys=True))"]}
{"type":"file_footer","path":"src/core/service_identity.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/slippage_policy.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5684,"sha256":"abc71f8f3f31d55e613fcf11bc7b525a257f0a7756ba7bf8909a9731b6423141","total_lines":194,"chunk_count":1}
{"type":"file_chunk","path":"src/core/slippage_policy.py","chunk_index":0,"line_start":1,"line_end":194,"content":["","\"\"\"","SlippagePolicy：滑價成本模型定義","","定義 per fill/per side 的滑價等級，並提供價格調整函數。","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, field","from typing import Literal, Dict, Optional","import math","","","@dataclass(frozen=True)","class SlippagePolicy:","    \"\"\"","    滑價政策定義","","    Attributes:","        definition: 滑價定義，固定為 \"per_fill_per_side\"","        levels: 滑價等級對應的 tick 數，預設為 S0=0, S1=1, S2=2, S3=3","        selection_level: 策略選擇使用的滑價等級（預設 S2）","        stress_level: 壓力測試使用的滑價等級（預設 S3）","        mc_execution_level: MultiCharts 執行時使用的滑價等級（預設 S1）","    \"\"\"","    definition: str = \"per_fill_per_side\"","    levels: Dict[str, int] = field(default_factory=lambda: {\"S0\": 0, \"S1\": 1, \"S2\": 2, \"S3\": 3})","    selection_level: str = \"S2\"","    stress_level: str = \"S3\"","    mc_execution_level: str = \"S1\"","","    def __post_init__(self):","        \"\"\"驗證欄位\"\"\"","        if self.definition != \"per_fill_per_side\":","            raise ValueError(f\"definition 必須為 'per_fill_per_side'，收到: {self.definition}\")","        ","        required_levels = {\"S0\", \"S1\", \"S2\", \"S3\"}","        if not required_levels.issubset(self.levels.keys()):","            missing = required_levels - set(self.levels.keys())","            raise ValueError(f\"levels 缺少必要等級: {missing}\")","        ","        for level in (self.selection_level, self.stress_level, self.mc_execution_level):","            if level not in self.levels:","                raise ValueError(f\"等級 {level} 不存在於 levels 中\")","        ","        # 確保 tick 數為非負整數","        for level, ticks in self.levels.items():","            if not isinstance(ticks, int) or ticks < 0:","                raise ValueError(f\"等級 {level} 的 ticks 必須為非負整數，收到: {ticks}\")","","    def get_ticks(self, level: str) -> int:","        \"\"\"","        取得指定等級的滑價 tick 數","","        Args:","            level: 等級名稱，例如 \"S2\"","","        Returns:","            滑價 tick 數","","        Raises:","            KeyError: 等級不存在","        \"\"\"","        return self.levels[level]","","    def get_selection_ticks(self) -> int:","        \"\"\"取得 selection_level 對應的 tick 數\"\"\"","        return self.get_ticks(self.selection_level)","","    def get_stress_ticks(self) -> int:","        \"\"\"取得 stress_level 對應的 tick 數\"\"\"","        return self.get_ticks(self.stress_level)","","    def get_mc_execution_ticks(self) -> int:","        \"\"\"取得 mc_execution_level 對應的 tick 數\"\"\"","        return self.get_ticks(self.mc_execution_level)","","","def apply_slippage_to_price(","    price: float,","    side: Literal[\"buy\", \"sell\", \"sellshort\", \"buytocover\"],","    slip_ticks: int,","    tick_size: float,",") -> float:","    \"\"\"","    根據滑價 tick 數調整價格","","    規則：","    - 買入（buy, buytocover）：價格增加 slip_ticks * tick_size","    - 賣出（sell, sellshort）：價格減少 slip_ticks * tick_size","","    Args:","        price: 原始價格","        side: 交易方向","        slip_ticks: 滑價 tick 數（非負整數）","        tick_size: 每 tick 價格變動量（必須 > 0）","","    Returns:","        調整後的價格","","    Raises:","        ValueError: 參數無效","    \"\"\"","    if tick_size <= 0:","        raise ValueError(f\"tick_size 必須 > 0，收到: {tick_size}\")","    if slip_ticks < 0:","        raise ValueError(f\"slip_ticks 必須 >= 0，收到: {slip_ticks}\")","    ","    # 計算滑價金額","    slippage_amount = slip_ticks * tick_size","    ","    # 根據方向調整","    if side in (\"buy\", \"buytocover\"):","        # 買入：支付更高價格","        adjusted = price + slippage_amount","    elif side in (\"sell\", \"sellshort\"):","        # 賣出：收到更低價格","        adjusted = price - slippage_amount","    else:","        raise ValueError(f\"無效的 side: {side}，必須為 buy/sell/sellshort/buytocover\")","    ","    # 確保價格非負（雖然理論上可能為負，但實務上不應發生）","    if adjusted < 0:","        adjusted = 0.0","    ","    return adjusted","","","def round_to_tick(price: float, tick_size: float) -> float:","    \"\"\"","    將價格四捨五入至最近的 tick 邊界","","    Args:","        price: 原始價格","        tick_size: tick 大小","","    Returns:","        四捨五入後的價格","    \"\"\"","    if tick_size <= 0:","        raise ValueError(f\"tick_size 必須 > 0，收到: {tick_size}\")","    ","    # 計算 tick 數","    ticks = round(price / tick_size)","    return ticks * tick_size","","","def compute_slippage_cost_per_side(","    slip_ticks: int,","    tick_size: float,","    quantity: float = 1.0,",") -> float:","    \"\"\"","    計算單邊滑價成本（每單位）","","    Args:","        slip_ticks: 滑價 tick 數","        tick_size: tick 大小","        quantity: 數量（預設 1.0）","","    Returns:","        滑價成本（正數）","    \"\"\"","    if slip_ticks < 0:","        raise ValueError(f\"slip_ticks 必須 >= 0，收到: {slip_ticks}\")","    if tick_size <= 0:","        raise ValueError(f\"tick_size 必須 > 0，收到: {tick_size}\")","    ","    return slip_ticks * tick_size * quantity","","","def compute_round_trip_slippage_cost(","    slip_ticks: int,","    tick_size: float,","    quantity: float = 1.0,",") -> float:","    \"\"\"","    計算來回交易（entry + exit）的總滑價成本","","    由於每邊都會產生滑價，總成本為 2 * slip_ticks * tick_size * quantity","","    Args:","        slip_ticks: 每邊滑價 tick 數","        tick_size: tick 大小","        quantity: 數量","","    Returns:","        總滑價成本","    \"\"\"","    per_side = compute_slippage_cost_per_side(slip_ticks, tick_size, quantity)","    return 2.0 * per_side","",""]}
{"type":"file_footer","path":"src/core/slippage_policy.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/snapshot.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7785,"sha256":"4a590ba03ce263bc719cdc62d01ceab9d17999c2f538f3426bff38abe26c8607","total_lines":249,"chunk_count":2}
{"type":"file_chunk","path":"src/core/snapshot.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"","Deterministic Snapshot - Freeze-time artifact hash registry.","","Phase 5: Create reproducible snapshot of all artifacts when season is frozen.","\"\"\"","","import json","import hashlib","from pathlib import Path","from typing import Dict, List, Any, Optional","import os","","","def compute_file_hash(filepath: Path) -> str:","    \"\"\"Compute SHA256 hash of a file.\"\"\"","    sha256 = hashlib.sha256()","    try:","        with open(filepath, \"rb\") as f:","            # Read in chunks to handle large files","            for chunk in iter(lambda: f.read(4096), b\"\"):","                sha256.update(chunk)","        return sha256.hexdigest()","    except (OSError, IOError):","        # If file cannot be read, return empty hash","        return \"\"","","","def collect_artifact_hashes(season_dir: Path) -> Dict[str, Any]:","    \"\"\"","    Collect SHA256 hashes of all artifacts in a season directory.","    ","    Returns:","        Dict with structure:","        {","            \"snapshot_ts\": \"ISO-8601 timestamp\",","            \"season\": \"season identifier\",","            \"artifacts\": {","                \"relative/path/to/file\": {","                    \"sha256\": \"hexdigest\",","                    \"size_bytes\": 1234,","                    \"mtime\": 1234567890.0","                },","                ...","            },","            \"directories_scanned\": [","                \"runs/\",","                \"portfolio/\",","                \"research/\",","                \"governance/\"","            ]","        }","    \"\"\"","    from datetime import datetime, timezone","    ","    # Directories to scan (relative to season_dir)","    scan_dirs = [","        \"runs\",","        \"portfolio\",","        \"research\",","        \"governance\"","    ]","    ","    artifacts = {}","    ","    for rel_dir in scan_dirs:","        dir_path = season_dir / rel_dir","        if not dir_path.exists():","            continue","        ","        # Walk through directory","        for root, dirs, files in os.walk(dir_path):","            root_path = Path(root)","            for filename in files:","                filepath = root_path / filename","                ","                # Skip temporary files and hidden files","                if filename.startswith(\".\") or filename.endswith(\".tmp\"):","                    continue","                ","                # Skip very large files (>100MB) to avoid performance issues","                try:","                    file_size = filepath.stat().st_size","                    if file_size > 100 * 1024 * 1024:  # 100MB","                        continue","                except OSError:","                    continue","                ","                # Compute relative path from season_dir","                try:","                    rel_path = filepath.relative_to(season_dir)","                except ValueError:","                    # Should not happen, but skip if it does","                    continue","                ","                # Compute hash","                sha256 = compute_file_hash(filepath)","                if not sha256:  # Skip if hash computation failed","                    continue","                ","                # Get file metadata","                try:","                    stat = filepath.stat()","                    artifacts[str(rel_path)] = {","                        \"sha256\": sha256,","                        \"size_bytes\": stat.st_size,","                        \"mtime\": stat.st_mtime,","                        \"mtime_iso\": datetime.fromtimestamp(stat.st_mtime, tz=timezone.utc).isoformat()","                    }","                except OSError:","                    # Skip if metadata cannot be read","                    continue","    ","    return {","        \"snapshot_ts\": datetime.now(timezone.utc).isoformat(),","        \"season\": season_dir.name,","        \"artifacts\": artifacts,","        \"directories_scanned\": scan_dirs,","        \"artifact_count\": len(artifacts)","    }","","","def create_freeze_snapshot(season: str) -> Path:","    \"\"\"","    Create deterministic snapshot of all artifacts in a season.","    ","    Args:","        season: Season identifier (e.g., \"2026Q1\")","    ","    Returns:","        Path to the created snapshot file.","    ","    Raises:","        FileNotFoundError: If season directory does not exist.","        OSError: If snapshot cannot be written.","    \"\"\"","    from .season_context import season_dir as get_season_dir","    ","    season_path = get_season_dir(season)","    if not season_path.exists():","        raise FileNotFoundError(f\"Season directory does not exist: {season_path}\")","    ","    # Collect artifact hashes","    snapshot_data = collect_artifact_hashes(season_path)","    ","    # Write snapshot file","    governance_dir = season_path / \"governance\"","    governance_dir.mkdir(parents=True, exist_ok=True)","    ","    snapshot_path = governance_dir / \"freeze_snapshot.json\"","    ","    # Write atomically","    temp_path = snapshot_path.with_suffix(\".tmp\")","    with open(temp_path, \"w\", encoding=\"utf-8\") as f:","        json.dump(snapshot_data, f, indent=2, ensure_ascii=False, sort_keys=True)","    ","    # Replace original","    temp_path.replace(snapshot_path)","    ","    return snapshot_path","","","def load_freeze_snapshot(season: str) -> Dict[str, Any]:","    \"\"\"","    Load freeze snapshot for a season.","    ","    Args:","        season: Season identifier","    ","    Returns:","        Snapshot data dictionary.","    ","    Raises:","        FileNotFoundError: If snapshot file does not exist.","        json.JSONDecodeError: If snapshot file is corrupted.","    \"\"\"","    from .season_context import season_dir as get_season_dir","    ","    season_path = get_season_dir(season)","    snapshot_path = season_path / \"governance\" / \"freeze_snapshot.json\"","    ","    if not snapshot_path.exists():","        raise FileNotFoundError(f\"Freeze snapshot not found: {snapshot_path}\")","    ","    with open(snapshot_path, \"r\", encoding=\"utf-8\") as f:","        return json.load(f)","","","def verify_snapshot_integrity(season: str) -> Dict[str, Any]:","    \"\"\"","    Verify current artifacts against freeze snapshot.","    ","    Args:","        season: Season identifier","    ","    Returns:","        Dict with verification results:","        {","            \"ok\": bool,","            \"missing_files\": List[str],","            \"changed_files\": List[str],"]}
{"type":"file_chunk","path":"src/core/snapshot.py","chunk_index":1,"line_start":201,"line_end":249,"content":["            \"new_files\": List[str],","            \"total_checked\": int,","            \"errors\": List[str]","        }","    \"\"\"","    from .season_context import season_dir as get_season_dir","    ","    season_path = get_season_dir(season)","    ","    try:","        snapshot = load_freeze_snapshot(season)","    except FileNotFoundError:","        return {","            \"ok\": False,","            \"missing_files\": [],","            \"changed_files\": [],","            \"new_files\": [],","            \"total_checked\": 0,","            \"errors\": [\"Freeze snapshot not found\"]","        }","    ","    # Get current artifact hashes","    current_artifacts = collect_artifact_hashes(season_path)","    ","    # Compare","    snapshot_artifacts = snapshot.get(\"artifacts\", {})","    current_artifact_paths = set(current_artifacts.get(\"artifacts\", {}).keys())","    snapshot_artifact_paths = set(snapshot_artifacts.keys())","    ","    missing_files = list(snapshot_artifact_paths - current_artifact_paths)","    new_files = list(current_artifact_paths - snapshot_artifact_paths)","    ","    changed_files = []","    for path in snapshot_artifact_paths.intersection(current_artifact_paths):","        snapshot_hash = snapshot_artifacts[path].get(\"sha256\", \"\")","        current_hash = current_artifacts[\"artifacts\"][path].get(\"sha256\", \"\")","        if snapshot_hash != current_hash:","            changed_files.append(path)","    ","    ok = len(missing_files) == 0 and len(changed_files) == 0","    ","    return {","        \"ok\": ok,","        \"missing_files\": sorted(missing_files),","        \"changed_files\": sorted(changed_files),","        \"new_files\": sorted(new_files),","        \"total_checked\": len(snapshot_artifact_paths),","        \"errors\": [] if ok else [\"Artifacts have been modified since freeze\"]","    }"]}
{"type":"file_footer","path":"src/core/snapshot.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/core/state.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9512,"sha256":"bf0a87d50e962d0e1b31192ed51cfa7044b2bf86bbc33566a3aa28f14f4b818c","total_lines":308,"chunk_count":2}
{"type":"file_chunk","path":"src/core/state.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"SystemState - read-only state snapshots for Attack #9 – Headless Intent-State Contract.","","Defines immutable SystemState objects that represent the current state of the system.","Backend outputs only read-only SystemState snapshots. UI may only read these snapshots,","not modify them. All state updates happen only inside StateProcessor.","\"\"\"","","from __future__ import annotations","","from datetime import datetime, date","from enum import Enum","from typing import Any, Dict, List, Optional, Set","from pydantic import BaseModel, ConfigDict, Field","","","class JobStatus(str, Enum):","    \"\"\"Status of a job.\"\"\"","    QUEUED = \"queued\"","    RUNNING = \"running\"","    PAUSED = \"paused\"","    COMPLETED = \"completed\"","    FAILED = \"failed\"","    CANCELLED = \"cancelled\"","","","class SeasonStatus(str, Enum):","    \"\"\"Status of a season.\"\"\"","    ACTIVE = \"active\"","    FROZEN = \"frozen\"","    ARCHIVED = \"archived\"","","","class DatasetStatus(str, Enum):","    \"\"\"Status of a dataset.\"\"\"","    AVAILABLE = \"available\"","    BUILDING = \"building\"","    MISSING_PARQUET = \"missing_parquet\"","    ERROR = \"error\"","","","class JobProgress(BaseModel):","    \"\"\"Progress information for a job.\"\"\"","    model_config = ConfigDict(frozen=True)","    ","    job_id: str","    status: JobStatus","    units_done: int = 0","    units_total: int = 0","    progress: float = Field(default=0.0, ge=0.0, le=1.0)","    created_at: datetime","    updated_at: datetime","    season: str","    dataset_id: str","    ","    @property","    def is_complete(self) -> bool:","        \"\"\"Check if job is complete.\"\"\"","        return self.status in [JobStatus.COMPLETED, JobStatus.FAILED, JobStatus.CANCELLED]","    ","    @property","    def is_active(self) -> bool:","        \"\"\"Check if job is active (running or paused).\"\"\"","        return self.status in [JobStatus.RUNNING, JobStatus.PAUSED]","","","class SeasonInfo(BaseModel):","    \"\"\"Information about a season.\"\"\"","    model_config = ConfigDict(frozen=True)","    ","    season_id: str","    status: SeasonStatus","    created_at: datetime","    frozen_at: Optional[datetime] = None","    job_count: int = 0","    completed_job_count: int = 0","    total_units: int = 0","    ","    @property","    def is_frozen(self) -> bool:","        \"\"\"Check if season is frozen.\"\"\"","        return self.status == SeasonStatus.FROZEN","","","class DatasetInfo(BaseModel):","    \"\"\"Information about a dataset.\"\"\"","    model_config = ConfigDict(frozen=True)","    ","    dataset_id: str","    status: DatasetStatus","    symbol: str","    timeframe: str","    start_date: date","    end_date: date","    has_parquet: bool = False","    parquet_missing_count: int = 0","    last_built_at: Optional[datetime] = None","    ","    @property","    def is_available(self) -> bool:","        \"\"\"Check if dataset is available for use.\"\"\"","        return self.status == DatasetStatus.AVAILABLE and self.has_parquet","","","class SystemMetrics(BaseModel):","    \"\"\"System-wide metrics.\"\"\"","    model_config = ConfigDict(frozen=True)","    ","    # Job metrics","    total_jobs: int = 0","    active_jobs: int = 0","    queued_jobs: int = 0","    completed_jobs: int = 0","    failed_jobs: int = 0","    ","    # Unit metrics","    total_units_processed: int = 0","    units_per_second: float = 0.0","    ","    # Resource metrics","    memory_usage_mb: float = 0.0","    cpu_usage_percent: float = 0.0","    disk_usage_gb: float = 0.0","    ","    # Timestamps","    snapshot_timestamp: datetime = Field(default_factory=datetime.now)","    uptime_seconds: float = 0.0","","","class IntentQueueStatus(BaseModel):","    \"\"\"Status of the intent processing queue.\"\"\"","    model_config = ConfigDict(frozen=True)","    ","    queue_size: int = 0","    processing_count: int = 0","    completed_count: int = 0","    failed_count: int = 0","    duplicate_rejected_count: int = 0  # Idempotency rejects","    ","    # Processing latency","    avg_processing_time_ms: float = 0.0","    max_processing_time_ms: float = 0.0","    ","    # Current processing intent (if any)","    current_intent_id: Optional[str] = None","    current_intent_type: Optional[str] = None","    current_intent_started_at: Optional[datetime] = None","","","class SystemState(BaseModel):","    \"\"\"Immutable snapshot of the entire system state.","    ","    This is the read-only state that UI can observe. All state updates","    happen only inside StateProcessor. UI receives snapshots of this state.","    \"\"\"","    model_config = ConfigDict(frozen=True)","    ","    # Metadata","    state_id: str = Field(default_factory=lambda: f\"state_{datetime.now().isoformat()}\")","    snapshot_timestamp: datetime = Field(default_factory=datetime.now)","    ","    # System metrics","    metrics: SystemMetrics = Field(default_factory=SystemMetrics)","    ","    # Intent queue status","    intent_queue: IntentQueueStatus = Field(default_factory=IntentQueueStatus)","    ","    # Collections","    seasons: Dict[str, SeasonInfo] = Field(default_factory=dict)","    datasets: Dict[str, DatasetInfo] = Field(default_factory=dict)","    jobs: Dict[str, JobProgress] = Field(default_factory=dict)","    ","    # Active processes","    active_builds: Set[str] = Field(default_factory=set)  # dataset_ids being built","    active_job_ids: Set[str] = Field(default_factory=set)  # job_ids currently running","    ","    # System health","    is_healthy: bool = True","    health_messages: List[str] = Field(default_factory=list)","    ","    # UI-specific state (read-only views)","    ui_views: Dict[str, Any] = Field(default_factory=dict)","    ","    # Derived properties","    @property","    def frozen_seasons(self) -> List[str]:","        \"\"\"Get list of frozen season IDs.\"\"\"","        return [season_id for season_id, season in self.seasons.items() ","                if season.is_frozen]","    ","    @property","    def available_datasets(self) -> List[str]:","        \"\"\"Get list of available dataset IDs.\"\"\"","        return [dataset_id for dataset_id, dataset in self.datasets.items() ","                if dataset.is_available]","    ","    @property","    def active_job_progress(self) -> List[JobProgress]:","        \"\"\"Get progress of active jobs.\"\"\"","        return [job for job in self.jobs.values() if job.is_active]","    "]}
{"type":"file_chunk","path":"src/core/state.py","chunk_index":1,"line_start":201,"line_end":308,"content":["    @property","    def recent_jobs(self, limit: int = 10) -> List[JobProgress]:","        \"\"\"Get most recent jobs.\"\"\"","        sorted_jobs = sorted(","            self.jobs.values(), ","            key=lambda j: j.updated_at, ","            reverse=True","        )","        return sorted_jobs[:limit]","    ","    def get_job(self, job_id: str) -> Optional[JobProgress]:","        \"\"\"Get job progress by ID.\"\"\"","        return self.jobs.get(job_id)","    ","    def get_season(self, season_id: str) -> Optional[SeasonInfo]:","        \"\"\"Get season info by ID.\"\"\"","        return self.seasons.get(season_id)","    ","    def get_dataset(self, dataset_id: str) -> Optional[DatasetInfo]:","        \"\"\"Get dataset info by ID.\"\"\"","        return self.datasets.get(dataset_id)","    ","    def is_season_frozen(self, season_id: str) -> bool:","        \"\"\"Check if a season is frozen.\"\"\"","        season = self.get_season(season_id)","        return season.is_frozen if season else False","    ","    def is_dataset_available(self, dataset_id: str) -> bool:","        \"\"\"Check if a dataset is available.\"\"\"","        dataset = self.get_dataset(dataset_id)","        return dataset.is_available if dataset else False","    ","    def validate_job_creation(self, season_id: str, dataset_id: str) -> List[str]:","        \"\"\"Validate if a job can be created.","        ","        Returns list of error messages, empty if valid.","        \"\"\"","        errors = []","        ","        # Check season","        season = self.get_season(season_id)","        if not season:","            errors.append(f\"Season not found: {season_id}\")","        elif season.is_frozen:","            errors.append(f\"Season is frozen: {season_id}\")","        ","        # Check dataset","        dataset = self.get_dataset(dataset_id)","        if not dataset:","            errors.append(f\"Dataset not found: {dataset_id}\")","        elif not dataset.is_available:","            errors.append(f\"Dataset not available: {dataset_id} (status: {dataset.status})\")","        ","        # Check system health","        if not self.is_healthy:","            errors.append(\"System is not healthy\")","        ","        return errors","","","# Factory functions for creating state snapshots","","def create_initial_state() -> SystemState:","    \"\"\"Create initial system state.\"\"\"","    return SystemState(","        state_id=\"initial\",","        metrics=SystemMetrics(),","        intent_queue=IntentQueueStatus(),","        seasons={},","        datasets={},","        jobs={},","        active_builds=set(),","        active_job_ids=set(),","        is_healthy=True,","        health_messages=[\"System initialized\"],","        ui_views={}","    )","","","def create_state_snapshot(","    base_state: SystemState,","    **updates: Any",") -> SystemState:","    \"\"\"Create a new state snapshot with updates.","    ","    Since SystemState is immutable, this creates a copy with updated fields.","    Used by StateProcessor to produce new state snapshots.","    \"\"\"","    # Create a mutable copy of the data","    data = base_state.model_dump()","    ","    # Apply updates","    for key, value in updates.items():","        if key in data:","            if isinstance(data[key], dict) and isinstance(value, dict):","                # Merge dictionaries","                data[key] = {**data[key], **value}","            elif isinstance(data[key], list) and isinstance(value, list):","                # Replace list","                data[key] = value","            elif isinstance(data[key], set) and isinstance(value, set):","                # Replace set","                data[key] = value","            else:","                data[key] = value","    ","    # Create new immutable state","    return SystemState(**data)"]}
{"type":"file_footer","path":"src/core/state.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/core/winners_builder.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6626,"sha256":"717bf7e509e6296a508e54cc8194976d6a7b4843169ac7fba9284ad6a1d73fd2","total_lines":222,"chunk_count":2}
{"type":"file_chunk","path":"src/core/winners_builder.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Winners builder - converts legacy winners to v2 schema.","","Builds v2 winners.json from legacy topk format with fallback strategies.","\"\"\"","","from __future__ import annotations","","from datetime import datetime, timezone","from typing import Any, Dict, List","","from core.winners_schema import WinnerItemV2, build_winners_v2_dict","","","def build_winners_v2(","    *,","    stage_name: str,","    run_id: str,","    manifest: Dict[str, Any],","    config_snapshot: Dict[str, Any],","    legacy_topk: List[Dict[str, Any]],",") -> Dict[str, Any]:","    \"\"\"","    Build winners.json v2 from legacy topk format.","    ","    Args:","        stage_name: Stage identifier","        run_id: Run ID","        manifest: Manifest dict (AuditSchema)","        config_snapshot: Config snapshot dict","        legacy_topk: Legacy topk list (old format items)","        ","    Returns:","        Winners dict with v2 schema","    \"\"\"","    # Extract strategy_id","    strategy_id = _extract_strategy_id(config_snapshot, manifest)","    ","    # Extract symbol/timeframe","    symbol = _extract_symbol(config_snapshot)","    timeframe = _extract_timeframe(config_snapshot)","    ","    # Build v2 items","    v2_items: List[WinnerItemV2] = []","    ","    for legacy_item in legacy_topk:","        # Extract param_id (required for candidate_id generation)","        param_id = legacy_item.get(\"param_id\")","        if param_id is None:","            # Skip items without param_id (should not happen, but be defensive)","            continue","        ","        # Generate candidate_id (temporary: strategy_id:param_id)","        # Future: upgrade to strategy_id:params_hash[:12] when params are available","        candidate_id = f\"{strategy_id}:{param_id}\"","        ","        # Extract params (fallback to empty dict)","        params = _extract_params(legacy_item, config_snapshot, param_id)","        ","        # Extract score (priority: score/finalscore > net_profit > 0.0)","        score = _extract_score(legacy_item)","        ","        # Build metrics (must include legacy fields for backward compatibility)","        metrics = {","            \"net_profit\": float(legacy_item.get(\"net_profit\", 0.0)),","            \"max_dd\": float(legacy_item.get(\"max_dd\", 0.0)),","            \"trades\": int(legacy_item.get(\"trades\", 0)),","            \"param_id\": int(param_id),  # Keep for backward compatibility","        }","        ","        # Add proxy_value if present (Stage0)","        if \"proxy_value\" in legacy_item:","            metrics[\"proxy_value\"] = float(legacy_item[\"proxy_value\"])","        ","        # Build source metadata","        source = {","            \"param_id\": int(param_id),","            \"run_id\": run_id,","            \"stage_name\": stage_name,","        }","        ","        # Create v2 item","        v2_item = WinnerItemV2(","            candidate_id=candidate_id,","            strategy_id=strategy_id,","            symbol=symbol,","            timeframe=timeframe,","            params=params,","            score=score,","            metrics=metrics,","            source=source,","        )","        ","        v2_items.append(v2_item)","    ","    # Build notes with candidate_id_mode info","    notes = {","        \"candidate_id_mode\": \"strategy_id:param_id\",  # Temporary mode","        \"note\": \"candidate_id uses param_id temporarily; will upgrade to params_hash when params are available\",","    }","    ","    # Build v2 winners dict","    return build_winners_v2_dict(","        stage_name=stage_name,","        run_id=run_id,","        generated_at=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        topk=v2_items,","        notes=notes,","    )","","","def _extract_strategy_id(config_snapshot: Dict[str, Any], manifest: Dict[str, Any]) -> str:","    \"\"\"","    Extract strategy_id from config_snapshot or manifest.","    ","    Priority:","    1. config_snapshot.get(\"strategy_id\")","    2. manifest.get(\"dataset_id\") (fallback)","    3. \"unknown\" (final fallback)","    \"\"\"","    if \"strategy_id\" in config_snapshot:","        return str(config_snapshot[\"strategy_id\"])","    ","    dataset_id = manifest.get(\"dataset_id\")","    if dataset_id:","        return str(dataset_id)","    ","    return \"unknown\"","","","def _extract_symbol(config_snapshot: Dict[str, Any]) -> str:","    \"\"\"","    Extract symbol from config_snapshot.","    ","    Returns \"UNKNOWN\" if not available.","    \"\"\"","    return str(config_snapshot.get(\"symbol\", \"UNKNOWN\"))","","","def _extract_timeframe(config_snapshot: Dict[str, Any]) -> str:","    \"\"\"","    Extract timeframe from config_snapshot.","    ","    Returns \"UNKNOWN\" if not available.","    \"\"\"","    return str(config_snapshot.get(\"timeframe\", \"UNKNOWN\"))","","","def _extract_params(","    legacy_item: Dict[str, Any],","    config_snapshot: Dict[str, Any],","    param_id: int,",") -> Dict[str, Any]:","    \"\"\"","    Extract params from legacy_item or config_snapshot.","    ","    Priority:","    1. legacy_item.get(\"params\")","    2. config_snapshot.get(\"params_by_id\", {}).get(param_id)","    3. config_snapshot.get(\"params_spec\") (if available)","    4. {} (empty dict fallback)","    ","    Returns empty dict {} if params are not available.","    \"\"\"","    # Try legacy_item first","    if \"params\" in legacy_item:","        params = legacy_item[\"params\"]","        if isinstance(params, dict):","            return params","    ","    # Try config_snapshot params_by_id","    params_by_id = config_snapshot.get(\"params_by_id\", {})","    if isinstance(params_by_id, dict) and param_id in params_by_id:","        params = params_by_id[param_id]","        if isinstance(params, dict):","            return params","    ","    # Try config_snapshot params_spec (if available)","    params_spec = config_snapshot.get(\"params_spec\")","    if isinstance(params_spec, dict):","        # Could extract from params_spec if it has param_id mapping","        # For now, return empty dict","        pass","    ","    # Fallback: empty dict","    return {}","","","def _extract_score(legacy_item: Dict[str, Any]) -> float:","    \"\"\"","    Extract score from legacy_item.","    ","    Priority:","    1. legacy_item.get(\"score\")","    2. legacy_item.get(\"finalscore\")","    3. legacy_item.get(\"net_profit\")","    4. legacy_item.get(\"proxy_value\") (for Stage0)","    5. 0.0 (fallback)","    \"\"\"","    if \"score\" in legacy_item:"]}
{"type":"file_chunk","path":"src/core/winners_builder.py","chunk_index":1,"line_start":201,"line_end":222,"content":["        val = legacy_item[\"score\"]","        if isinstance(val, (int, float)):","            return float(val)","    ","    if \"finalscore\" in legacy_item:","        val = legacy_item[\"finalscore\"]","        if isinstance(val, (int, float)):","            return float(val)","    ","    if \"net_profit\" in legacy_item:","        val = legacy_item[\"net_profit\"]","        if isinstance(val, (int, float)):","            return float(val)","    ","    if \"proxy_value\" in legacy_item:","        val = legacy_item[\"proxy_value\"]","        if isinstance(val, (int, float)):","            return float(val)","    ","    return 0.0","",""]}
{"type":"file_footer","path":"src/core/winners_builder.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/core/winners_schema.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3593,"sha256":"b415c3cc05931a57bc3f6cc5ffdfa2b6646b8d12705badbdbecf030e96f6580c","total_lines":126,"chunk_count":1}
{"type":"file_chunk","path":"src/core/winners_schema.py","chunk_index":0,"line_start":1,"line_end":126,"content":["","\"\"\"Winners schema v2 (SSOT).","","Defines the v2 schema for winners.json with enhanced metadata.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, asdict","from datetime import datetime, timezone","from typing import Any, Dict, List","","","WINNERS_SCHEMA_VERSION = \"v2\"","","","@dataclass(frozen=True)","class WinnerItemV2:","    \"\"\"","    Winner item in v2 schema.","    ","    Each item represents a top-K candidate with complete metadata.","    \"\"\"","    candidate_id: str  # Format: {strategy_id}:{param_id} (temporary) or {strategy_id}:{params_hash[:12]} (future)","    strategy_id: str  # Strategy identifier (e.g., \"donchian_atr\")","    symbol: str  # Symbol identifier (e.g., \"CME.MNQ\" or \"UNKNOWN\")","    timeframe: str  # Timeframe (e.g., \"60m\" or \"UNKNOWN\")","    params: Dict[str, Any]  # Parameters dict (may be empty {} if not available)","    score: float  # Ranking score (finalscore, net_profit, or proxy_value)","    metrics: Dict[str, Any]  # Performance metrics (must include legacy fields: net_profit, max_dd, trades, param_id)","    source: Dict[str, Any]  # Source metadata (param_id, run_id, stage_name)","    ","    def to_dict(self) -> Dict[str, Any]:","        \"\"\"Convert to dictionary for JSON serialization.\"\"\"","        return asdict(self)","","","def build_winners_v2_dict(","    *,","    stage_name: str,","    run_id: str,","    generated_at: str | None = None,","    topk: List[WinnerItemV2],","    notes: Dict[str, Any] | None = None,",") -> Dict[str, Any]:","    \"\"\"","    Build winners.json v2 structure.","    ","    Args:","        stage_name: Stage identifier","        run_id: Run ID","        generated_at: ISO8601 timestamp (defaults to now if None)","        topk: List of WinnerItemV2 items","        notes: Additional notes dict (will be merged with default notes)","        ","    Returns:","        Winners dict with v2 schema","    \"\"\"","    if generated_at is None:","        generated_at = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","    ","    default_notes = {","        \"schema\": WINNERS_SCHEMA_VERSION,","    }","    ","    if notes:","        default_notes.update(notes)","    ","    return {","        \"schema\": WINNERS_SCHEMA_VERSION,","        \"stage_name\": stage_name,","        \"generated_at\": generated_at,","        \"topk\": [item.to_dict() for item in topk],","        \"notes\": default_notes,","    }","","","def is_winners_v2(winners: Dict[str, Any]) -> bool:","    \"\"\"","    Check if winners dict is v2 schema.","    ","    Args:","        winners: Winners dict","        ","    Returns:","        True if v2 schema, False otherwise","    \"\"\"","    # Check top-level schema field","    if winners.get(\"schema\") == WINNERS_SCHEMA_VERSION:","        return True","    ","    # Check notes.schema field (legacy check)","    notes = winners.get(\"notes\", {})","    if isinstance(notes, dict) and notes.get(\"schema\") == WINNERS_SCHEMA_VERSION:","        return True","    ","    return False","","","def is_winners_legacy(winners: Dict[str, Any]) -> bool:","    \"\"\"","    Check if winners dict is legacy (v1) schema.","    ","    Args:","        winners: Winners dict","        ","    Returns:","        True if legacy schema, False otherwise","    \"\"\"","    # If it's v2, it's not legacy","    if is_winners_v2(winners):","        return False","    ","    # Legacy format: {\"topk\": [...], \"notes\": {\"schema\": \"v1\"}} or just {\"topk\": [...]}","    if \"topk\" in winners:","        # Check if items have v2 structure (candidate_id, strategy_id, etc.)","        topk = winners.get(\"topk\", [])","        if topk and isinstance(topk[0], dict):","            # If first item has candidate_id, it's v2","            if \"candidate_id\" in topk[0]:","                return False","        return True","    ","    return False","",""]}
{"type":"file_footer","path":"src/core/winners_schema.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/data/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":575,"sha256":"f4ea7a42c49b6222b21fcc035fdd760623dee8921c1053b92d4b225aced2786a","total_lines":20,"chunk_count":1}
{"type":"file_chunk","path":"src/data/__init__.py","chunk_index":0,"line_start":1,"line_end":20,"content":["\"\"\"Data ingest module - Raw means RAW.","","Phase 6.5 Data Ingest v1: Immutable, extremely stupid raw data ingestion.","\"\"\"","","from data.cache import CachePaths, cache_paths, read_parquet_cache, write_parquet_cache","from data.fingerprint import DataFingerprint, compute_txt_fingerprint","from data.raw_ingest import IngestPolicy, RawIngestResult, ingest_raw_txt","","__all__ = [","    \"IngestPolicy\",","    \"RawIngestResult\",","    \"ingest_raw_txt\",","    \"DataFingerprint\",","    \"compute_txt_fingerprint\",","    \"CachePaths\",","    \"cache_paths\",","    \"write_parquet_cache\",","    \"read_parquet_cache\",","]"]}
{"type":"file_footer","path":"src/data/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/data/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":680,"sha256":"60099687ec75b7de19ba89864d158fd4176134ac076129491d31aaa54be2ea72","note":"skipped by policy"}
{"type":"file_skipped","path":"src/data/__pycache__/cache.cpython-312.pyc","reason":"cache","bytes":4716,"sha256":"b6d9ab8a4416e49002bc66b3065a8ce52bbdd30a3db4edabd36daad1fda7a000","note":"skipped by policy"}
{"type":"file_skipped","path":"src/data/__pycache__/dataset_registry.cpython-312.pyc","reason":"cache","bytes":4301,"sha256":"04544be12feb3e42bc521a61b1c434925605e495b76e060c5a68ebdf78d2a6dc","note":"skipped by policy"}
{"type":"file_skipped","path":"src/data/__pycache__/fingerprint.cpython-312.pyc","reason":"cache","bytes":5165,"sha256":"ef89ba94e936efc9297e7c5dee146326d2b6ecde9c1af92f6ad331a375780e34","note":"skipped by policy"}
{"type":"file_skipped","path":"src/data/__pycache__/raw_ingest.cpython-312.pyc","reason":"cache","bytes":7112,"sha256":"9c975bbd5f952eaa0c3f9762c36e4b13bfc8b5381229985546a0408ef67721ac","note":"skipped by policy"}
{"type":"file_header","path":"src/data/cache.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3315,"sha256":"18ad78e3bb3c7786bbc46576c0944b57a7c823872817b00477982cbbd5d3d36f","total_lines":108,"chunk_count":1}
{"type":"file_chunk","path":"src/data/cache.py","chunk_index":0,"line_start":1,"line_end":108,"content":["\"\"\"Parquet cache - Cache, Not Truth.","","Binding #4: Parquet is Cache, Not Truth.","Cache can be deleted and rebuilt. Fingerprint is the truth.","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass","from pathlib import Path","from typing import Any","","import pandas as pd","","","@dataclass(frozen=True)","class CachePaths:","    \"\"\"Cache file paths for a symbol.","    ","    Attributes:","        parquet_path: Path to parquet cache file","        meta_path: Path to meta.json file","    \"\"\"","    parquet_path: Path","    meta_path: Path","","","def cache_paths(cache_root: Path, symbol: str) -> CachePaths:","    \"\"\"Get cache paths for a symbol.","    ","    Args:","        cache_root: Root directory for cache files","        symbol: Symbol identifier (e.g., \"CME.MNQ\")","        ","    Returns:","        CachePaths with parquet_path and meta_path","    \"\"\"","    cache_root.mkdir(parents=True, exist_ok=True)","    ","    # Sanitize symbol for filename","    safe_symbol = symbol.replace(\"/\", \"_\").replace(\"\\\\\", \"_\").replace(\":\", \"_\")","    ","    return CachePaths(","        parquet_path=cache_root / f\"{safe_symbol}.parquet\",","        meta_path=cache_root / f\"{safe_symbol}.meta.json\",","    )","","","def write_parquet_cache(paths: CachePaths, df: pd.DataFrame, meta: dict[str, Any]) -> None:","    \"\"\"Write parquet cache + meta.json.","    ","    Parquet stores raw df (with ts_str), no sort, no dedup.","    meta.json must contain:","    - data_fingerprint_sha1","    - source_path","    - ingest_policy","    - rows, first_ts_str, last_ts_str","    ","    Args:","        paths: CachePaths for this symbol","        df: DataFrame to cache (must have columns: ts_str, open, high, low, close, volume)","        meta: Metadata dict (must include data_fingerprint_sha1, source_path, ingest_policy, etc.)","        ","    Raises:","        ValueError: If required meta fields are missing","    \"\"\"","    required_meta_fields = [\"data_fingerprint_sha1\", \"source_path\", \"ingest_policy\"]","    missing_fields = [field for field in required_meta_fields if field not in meta]","    if missing_fields:","        raise ValueError(f\"Missing required meta fields: {missing_fields}\")","    ","    # Write parquet (preserve order, no sort)","    paths.parquet_path.parent.mkdir(parents=True, exist_ok=True)","    df.to_parquet(paths.parquet_path, index=False, engine=\"pyarrow\")","    ","    # Write meta.json","    with paths.meta_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(meta, f, ensure_ascii=False, sort_keys=True, indent=2)","        f.write(\"\\n\")","","","def read_parquet_cache(paths: CachePaths) -> tuple[pd.DataFrame, dict[str, Any]]:","    \"\"\"Read parquet cache + meta.json.","    ","    Args:","        paths: CachePaths for this symbol","        ","    Returns:","        Tuple of (DataFrame, meta_dict)","        ","    Raises:","        FileNotFoundError: If parquet or meta.json does not exist","        json.JSONDecodeError: If meta.json is invalid JSON","    \"\"\"","    if not paths.parquet_path.exists():","        raise FileNotFoundError(f\"Parquet cache not found: {paths.parquet_path}\")","    if not paths.meta_path.exists():","        raise FileNotFoundError(f\"Meta file not found: {paths.meta_path}\")","    ","    # Read parquet","    df = pd.read_parquet(paths.parquet_path, engine=\"pyarrow\")","    ","    # Read meta.json","    with paths.meta_path.open(\"r\", encoding=\"utf-8\") as f:","        meta = json.load(f)","    ","    return df, meta"]}
{"type":"file_footer","path":"src/data/cache.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/data/dataset_registry.py","kind":"text","encoding":"utf-8","newline":"crlf","bytes":3599,"sha256":"1c8d752f03e64c93bbd50541ffe15de19d1ea02042d5724e78ff97d66f8bc5b2","total_lines":113,"chunk_count":1}
{"type":"file_chunk","path":"src/data/dataset_registry.py","chunk_index":0,"line_start":1,"line_end":113,"content":["\"\"\"Dataset Registry Schema.","","Phase 12: Dataset Registry for Research Job Wizard.","Describes \"what datasets are available\" without containing any price data.","Schema can only \"add fields\" in the future, cannot change semantics.","\"\"\"","","from __future__ import annotations","","from datetime import date, datetime","from typing import List, Optional","","from pydantic import BaseModel, ConfigDict, Field, model_validator","","","class DatasetRecord(BaseModel):","    \"\"\"Metadata for a single derived dataset.\"\"\"","    ","    model_config = ConfigDict(frozen=True)","    ","    id: str = Field(","        ...,","        description=\"Unique identifier, e.g. 'CME.MNQ.60m.2020-2024'\",","        examples=[\"CME.MNQ.60m.2020-2024\", \"TWF.MXF.15m.2018-2023\"]","    )","    ","    symbol: str = Field(","        ...,","        description=\"Symbol identifier, e.g. 'CME.MNQ'\",","        examples=[\"CME.MNQ\", \"TWF.MXF\"]","    )","    ","    exchange: str = Field(","        ...,","        description=\"Exchange identifier, e.g. 'CME'\",","        examples=[\"CME\", \"TWF\"]","    )","    ","    timeframe: str = Field(","        ...,","        description=\"Timeframe string, e.g. '60m'\",","        examples=[\"60m\", \"15m\", \"5m\", \"1D\"]","    )","    ","    path: str = Field(","        ...,","        description=\"Relative path to derived file from data/derived/\",","        examples=[\"CME.MNQ/60m/2020-2024.parquet\"]","    )","    ","    start_date: date = Field(","        ...,","        description=\"First date with data (inclusive)\"","    )","    ","    end_date: date = Field(","        ...,","        description=\"Last date with data (inclusive)\"","    )","    ","    fingerprint_sha1: Optional[str] = Field(","        default=None,","        description=\"SHA1 hash of file content (binary), deterministic fingerprint (deprecated, use fingerprint_sha256_40)\"","    )","    ","    fingerprint_sha256_40: str = Field(","        ...,","        description=\"SHA256 hash of file content (binary), first 40 hex chars, deterministic fingerprint\"","    )","    ","    @model_validator(mode=\"before\")","    @classmethod","    def ensure_fingerprint_sha256_40(cls, data: dict) -> dict:","        \"\"\"Backward compatibility: if fingerprint_sha256_40 missing but fingerprint_sha1 present, copy it.\"\"\"","        if isinstance(data, dict):","            if \"fingerprint_sha256_40\" not in data or not data[\"fingerprint_sha256_40\"]:","                if \"fingerprint_sha1\" in data and data[\"fingerprint_sha1\"]:","                    # Copy sha1 to sha256 field (note: this is semantically wrong but maintains compatibility)","                    data[\"fingerprint_sha256_40\"] = data[\"fingerprint_sha1\"]","        return data","    ","    tz_provider: str = Field(","        default=\"IANA\",","        description=\"Timezone provider identifier\"","    )","    ","    tz_version: str = Field(","        default=\"unknown\",","        description=\"Timezone database version\"","    )","","","class DatasetIndex(BaseModel):","    \"\"\"Complete registry of all available datasets.\"\"\"","    ","    model_config = ConfigDict(frozen=True)","    ","    generated_at: datetime = Field(","        ...,","        description=\"Timestamp when this index was generated\"","    )","    ","    datasets: List[DatasetRecord] = Field(","        default_factory=list,","        description=\"List of all available dataset records\"","    )","    ","    def model_post_init(self, __context: object) -> None:","        \"\"\"Post-initialization hook to sort datasets by id.\"\"\"","        super().model_post_init(__context)","        # Sort datasets by id to ensure deterministic order","        if self.datasets:","            self.datasets.sort(key=lambda d: d.id)"]}
{"type":"file_footer","path":"src/data/dataset_registry.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/data/fingerprint.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4291,"sha256":"e9cfb6f419b0a2f54d4260a56ddffe4faff0407550541cd841e2ba3955778919","total_lines":125,"chunk_count":1}
{"type":"file_chunk","path":"src/data/fingerprint.py","chunk_index":0,"line_start":1,"line_end":125,"content":["\"\"\"Data fingerprint - Truth fingerprint based on Raw TXT.","","Binding #3: Mandatory Fingerprint in Governance + JobRecord.","Fingerprint must depend only on raw TXT content + ingest_policy.","Parquet is cache, not truth.","\"\"\"","","from __future__ import annotations","","import hashlib","import json","from dataclasses import dataclass","from pathlib import Path","","","@dataclass(frozen=True)","class DataFingerprint:","    \"\"\"Data fingerprint - immutable truth identifier.","    ","    Attributes:","        sha1: SHA1 hash of raw TXT content + ingest_policy","        source_path: Path to source TXT file","        rows: Number of rows (metadata)","        first_ts_str: First timestamp string (metadata)","        last_ts_str: Last timestamp string (metadata)","        ingest_policy: Ingest policy dict (for hash computation)","    \"\"\"","    sha1: str","    source_path: str","    rows: int","    first_ts_str: str","    last_ts_str: str","    ingest_policy: dict","","","def compute_txt_fingerprint(path: Path, *, ingest_policy: dict) -> DataFingerprint:","    \"\"\"Compute fingerprint from raw TXT file + ingest_policy.","    ","    Fingerprint is computed from:","    1. Raw TXT file content (bytes)","    2. Ingest policy (JSON with stable sort)","    ","    This ensures the fingerprint represents the \"truth\" - raw data + normalization policy.","    Parquet cache can be deleted and rebuilt, fingerprint remains stable.","    ","    Args:","        path: Path to raw TXT file","        ingest_policy: Ingest policy dict (will be JSON-serialized with stable sort)","        ","    Returns:","        DataFingerprint with SHA1 hash and metadata","        ","    Raises:","        FileNotFoundError: If path does not exist","    \"\"\"","    if not path.exists():","        raise FileNotFoundError(f\"TXT file not found: {path}\")","    ","    # Compute SHA1: policy first, then file content","    h = hashlib.sha1()","    ","    # Add ingest_policy (stable JSON sort)","    policy_json = json.dumps(ingest_policy, sort_keys=True, ensure_ascii=False)","    h.update(policy_json.encode(\"utf-8\"))","    ","    # Add file content (chunked for large files)","    with path.open(\"rb\") as f:","        while True:","            chunk = f.read(1024 * 1024)  # 1MB chunks","            if not chunk:","                break","            h.update(chunk)","    ","    sha1 = h.hexdigest()","    ","    # Read metadata (rows, first_ts_str, last_ts_str)","    # We need to parse the file to get these, but they're just metadata","    # The hash is the truth, metadata is for convenience","    import pandas as pd","    ","    df = pd.read_csv(path, encoding=\"utf-8\")","    rows = len(df)","    ","    # Try to extract first/last timestamps","    # This is best-effort metadata, not part of hash","    first_ts_str = \"\"","    last_ts_str = \"\"","    ","    if \"Date\" in df.columns and \"Time\" in df.columns:","        if rows > 0:","            first_date = str(df.iloc[0][\"Date\"])","            first_time = str(df.iloc[0][\"Time\"])","            last_date = str(df.iloc[-1][\"Date\"])","            last_time = str(df.iloc[-1][\"Time\"])","            ","            # Apply same normalization as ingest (duplicate logic to avoid circular import)","            def _normalize_24h_local(date_s: str, time_s: str) -> tuple[str, bool]:","                \"\"\"Local copy of _normalize_24h to avoid circular import.\"\"\"","                t = time_s.strip()","                if t.startswith(\"24:\"):","                    if t != \"24:00:00\":","                        raise ValueError(f\"Invalid 24h time: {time_s}\")","                    d = pd.to_datetime(date_s.strip(), format=\"%Y/%m/%d\", errors=\"raise\")","                    d2 = (d + pd.Timedelta(days=1)).to_pydatetime().date()","                    return f\"{d2.year}/{d2.month}/{d2.day} 00:00:00\", True","                return f\"{date_s.strip()} {t}\", False","            ","            try:","                first_ts_str, _ = _normalize_24h_local(first_date, first_time)","            except Exception:","                first_ts_str = f\"{first_date} {first_time}\"","            ","            try:","                last_ts_str, _ = _normalize_24h_local(last_date, last_time)","            except Exception:","                last_ts_str = f\"{last_date} {last_time}\"","    ","    return DataFingerprint(","        sha1=sha1,","        source_path=str(path),","        rows=rows,","        first_ts_str=first_ts_str,","        last_ts_str=last_ts_str,","        ingest_policy=ingest_policy,","    )"]}
{"type":"file_footer","path":"src/data/fingerprint.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/data/layout.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":744,"sha256":"5d729a24973025021536cfa525e92d28301d12c6d1730ec904e01bfc2a12c124","total_lines":29,"chunk_count":1}
{"type":"file_chunk","path":"src/data/layout.py","chunk_index":0,"line_start":1,"line_end":29,"content":["import numpy as np","from engine.types import BarArrays","","","def ensure_float64_contiguous(x: np.ndarray) -> np.ndarray:","    arr = np.asarray(x, dtype=np.float64)","    if not arr.flags[\"C_CONTIGUOUS\"]:","        arr = np.ascontiguousarray(arr)","    return arr","","","def normalize_bars(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,",") -> BarArrays:","    arrays = [open_, high, low, close]","    for a in arrays:","        if np.isnan(a).any():","            raise ValueError(\"NaN detected in input data\")","","    o = ensure_float64_contiguous(open_)","    h = ensure_float64_contiguous(high)","    l = ensure_float64_contiguous(low)","    c = ensure_float64_contiguous(close)","","    return BarArrays(open=o, high=h, low=l, close=c)",""]}
{"type":"file_footer","path":"src/data/layout.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/data/raw_ingest.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5735,"sha256":"3789dc71594c63f7ea31d18a99e06477ff4f0461b34dcc43c8ed651409e7a5f5","total_lines":169,"chunk_count":1}
{"type":"file_chunk","path":"src/data/raw_ingest.py","chunk_index":0,"line_start":1,"line_end":169,"content":["\"\"\"Raw data ingestion - Raw means RAW.","","Phase 6.5 Data Ingest v1: Immutable, extremely stupid raw data ingestion.","No sort, no dedup, no dropna (unless recorded in ingest_policy).","","Binding: One line = one row, preserve TXT row order exactly.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass","from pathlib import Path","from typing import Any","","import pandas as pd","","","@dataclass(frozen=True)","class IngestPolicy:","    \"\"\"Ingest policy - only records format normalization decisions, not data cleaning.","    ","    Attributes:","        normalized_24h: Whether 24:00:00 times were normalized to next day 00:00:00","        column_map: Column name mapping from source to standard names","    \"\"\"","    normalized_24h: bool = False","    column_map: dict[str, str] | None = None","","","@dataclass(frozen=True)","class RawIngestResult:","    \"\"\"Raw ingest result - immutable contract.","    ","    Attributes:","        df: DataFrame with exactly columns: ts_str, open, high, low, close, volume","        source_path: Path to source TXT file","        rows: Number of rows ingested","        policy: Ingest policy applied","    \"\"\"","    df: pd.DataFrame  # columns exactly: ts_str, open, high, low, close, volume","    source_path: str","    rows: int","    policy: IngestPolicy","","","def _normalize_24h(date_s: str, time_s: str) -> tuple[str, bool]:","    \"\"\"Normalize 24:xx:xx time to next day 00:00:00.","    ","    Only allows 24:00:00 (exact). Raises ValueError for other 24:xx:xx times.","    ","    Args:","        date_s: Date string (e.g., \"2013/1/1\")","        time_s: Time string (e.g., \"24:00:00\" or \"09:30:00\")","        ","    Returns:","        Tuple of (normalized ts_str, normalized_flag)","        - If 24:00:00: returns next day 00:00:00 and True","        - Otherwise: returns original \"date_s time_s\" and False","        ","    Raises:","        ValueError: If time_s starts with \"24:\" but is not exactly \"24:00:00\"","    \"\"\"","    t = time_s.strip()","    if t.startswith(\"24:\"):","        if t != \"24:00:00\":","            raise ValueError(f\"Invalid 24h time: {time_s} (only 24:00:00 is allowed)\")","        # Parse date only (no timezone)","        d = pd.to_datetime(date_s.strip(), format=\"%Y/%m/%d\", errors=\"raise\")","        d2 = (d + pd.Timedelta(days=1)).to_pydatetime().date()","        return f\"{d2.year}/{d2.month}/{d2.day} 00:00:00\", True","    return f\"{date_s.strip()} {t}\", False","","","def ingest_raw_txt(","    txt_path: Path,","    *,","    column_map: dict[str, str] | None = None,",") -> RawIngestResult:","    \"\"\"Ingest raw TXT file - Raw means RAW.","    ","    Core rules (Binding):","    - One line = one row, preserve TXT row order exactly","    - No sort_values()","    - No drop_duplicates()","    - No dropna() (unless recorded in ingest_policy)","    ","    Format normalization (allowed):","    - 24:00:00 → next day 00:00:00 (recorded in policy.normalized_24h)","    - Column mapping (recorded in policy.column_map)","    ","    Args:","        txt_path: Path to raw TXT file","        column_map: Optional column name mapping (e.g., {\"Date\": \"Date\", \"Time\": \"Time\", ...})","        ","    Returns:","        RawIngestResult with df containing columns: ts_str, open, high, low, close, volume","        ","    Raises:","        FileNotFoundError: If txt_path does not exist","        ValueError: If parsing fails or invalid 24h time format","    \"\"\"","    if not txt_path.exists():","        raise FileNotFoundError(f\"TXT file not found: {txt_path}\")","    ","    # Read TXT file (preserve order)","    # Assume CSV-like format with header","    df_raw = pd.read_csv(txt_path, encoding=\"utf-8\")","    ","    # Apply column mapping if provided","    if column_map:","        df_raw = df_raw.rename(columns=column_map)","    ","    # Expected columns after mapping: Date, Time, Open, High, Low, Close, TotalVolume (or Volume)","    required_cols = [\"Date\", \"Time\", \"Open\", \"High\", \"Low\", \"Close\"]","    volume_cols = [\"TotalVolume\", \"Volume\"]","    ","    # Check required columns","    missing_cols = [col for col in required_cols if col not in df_raw.columns]","    if missing_cols:","        raise ValueError(f\"Missing required columns: {missing_cols}. Found: {list(df_raw.columns)}\")","    ","    # Find volume column","    volume_col = None","    for vcol in volume_cols:","        if vcol in df_raw.columns:","            volume_col = vcol","            break","    ","    if volume_col is None:","        raise ValueError(f\"Missing volume column. Expected one of: {volume_cols}. Found: {list(df_raw.columns)}\")","    ","    # Build ts_str column (preserve row order)","    normalized_24h = False","    ts_str_list = []","    ","    for idx, row in df_raw.iterrows():","        date_s = str(row[\"Date\"])","        time_s = str(row[\"Time\"])","        ","        try:","            ts_str, was_normalized = _normalize_24h(date_s, time_s)","            if was_normalized:","                normalized_24h = True","            ts_str_list.append(ts_str)","        except Exception as e:","            raise ValueError(f\"Failed to normalize timestamp at row {idx}: {e}\") from e","    ","    # Build result DataFrame (preserve order, no sort/dedup/dropna)","    result_df = pd.DataFrame({","        \"ts_str\": ts_str_list,","        \"open\": pd.to_numeric(df_raw[\"Open\"], errors=\"raise\").astype(\"float64\"),","        \"high\": pd.to_numeric(df_raw[\"High\"], errors=\"raise\").astype(\"float64\"),","        \"low\": pd.to_numeric(df_raw[\"Low\"], errors=\"raise\").astype(\"float64\"),","        \"close\": pd.to_numeric(df_raw[\"Close\"], errors=\"raise\").astype(\"float64\"),","        \"volume\": pd.to_numeric(df_raw[volume_col], errors=\"coerce\").fillna(0).astype(\"int64\"),","    })","    ","    # Record policy","    policy = IngestPolicy(","        normalized_24h=normalized_24h,","        column_map=column_map,","    )","    ","    return RawIngestResult(","        df=result_df,","        source_path=str(txt_path),","        rows=len(result_df),","        policy=policy,","    )"]}
{"type":"file_footer","path":"src/data/raw_ingest.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/data/session/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":693,"sha256":"d0b0cc422b77af93ebe539de08933316e40697241a7bd9f001baca7c9f82fa9e","total_lines":21,"chunk_count":1}
{"type":"file_chunk","path":"src/data/session/__init__.py","chunk_index":0,"line_start":1,"line_end":21,"content":["\"\"\"Session Profile and K-Bar Aggregation module.","","Phase 6.6: Session Profile + K-Bar Aggregation with DST-safe timezone conversion.","Session classification and K-bar aggregation use exchange clock.","Raw ingest (Phase 6.5) remains unchanged - no timezone conversion at raw layer.","\"\"\"","","from data.session.classify import classify_session, classify_sessions","from data.session.kbar import aggregate_kbar","from data.session.loader import load_session_profile","from data.session.schema import Session, SessionProfile, SessionWindow","","__all__ = [","    \"Session\",","    \"SessionProfile\",","    \"SessionWindow\",","    \"load_session_profile\",","    \"classify_session\",","    \"classify_sessions\",","    \"aggregate_kbar\",","]"]}
{"type":"file_footer","path":"src/data/session/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/data/session/classify.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6050,"sha256":"c22bffba45c652baef327a5a189dc5dd2c5e33636a18cbc14a6dc7f8f914c076","total_lines":177,"chunk_count":1}
{"type":"file_chunk","path":"src/data/session/classify.py","chunk_index":0,"line_start":1,"line_end":177,"content":["\"\"\"Session classification.","","Phase 6.6: Classify timestamps into trading sessions using DST-safe timezone conversion.","Converts local time to exchange time for classification.","\"\"\"","","from __future__ import annotations","","from datetime import datetime","from typing import List","","import pandas as pd","from zoneinfo import ZoneInfo","","from data.session.schema import Session, SessionProfile, SessionWindow","","","def _parse_ts_str(ts_str: str) -> datetime:","    \"\"\"Parse timestamp string (handles non-zero-padded dates like \"2013/1/1\").","    ","    Phase 6.6: Manual parsing to handle \"YYYY/M/D\" format without zero-padding.","    ","    Args:","        ts_str: Timestamp string in format \"YYYY/M/D HH:MM:SS\" or \"YYYY/MM/DD HH:MM:SS\"","        ","    Returns:","        datetime (naive, no timezone attached)","    \"\"\"","    date_s, time_s = ts_str.split(\" \")","    y, m, d = (int(x) for x in date_s.split(\"/\"))","    hh, mm, ss = (int(x) for x in time_s.split(\":\"))","    return datetime(y, m, d, hh, mm, ss)","","","def _parse_ts_str_tpe(ts_str: str) -> datetime:","    \"\"\"Parse timestamp string and attach Asia/Taipei timezone.","    ","    Phase 6.6: Only does format parsing + attach timezone, no \"correction\" or sort.","    ","    Args:","        ts_str: Timestamp string in format \"YYYY/M/D HH:MM:SS\" or \"YYYY/MM/DD HH:MM:SS\"","        ","    Returns:","        datetime with Asia/Taipei timezone","    \"\"\"","    dt = _parse_ts_str(ts_str)","    return dt.replace(tzinfo=ZoneInfo(\"Asia/Taipei\"))","","","def _parse_ts_str_with_tz(ts_str: str, tz: str) -> datetime:","    \"\"\"Parse timestamp string and attach specified timezone.","    ","    Phase 6.6: Parse ts_str and attach timezone.","    ","    Args:","        ts_str: Timestamp string in format \"YYYY/M/D HH:MM:SS\" or \"YYYY/MM/DD HH:MM:SS\"","        tz: IANA timezone (e.g., \"Asia/Taipei\")","        ","    Returns:","        datetime with specified timezone","    \"\"\"","    dt = _parse_ts_str(ts_str)","    return dt.replace(tzinfo=ZoneInfo(tz))","","","def _to_exchange_hms(ts_str: str, data_tz: str, exchange_tz: str) -> str:","    \"\"\"Convert timestamp string to exchange timezone and return HH:MM:SS.","    ","    Args:","        ts_str: Timestamp string in format \"YYYY/M/D HH:MM:SS\" (data timezone)","        data_tz: IANA timezone of input data (e.g., \"Asia/Taipei\")","        exchange_tz: IANA timezone of exchange (e.g., \"America/Chicago\")","        ","    Returns:","        Time string \"HH:MM:SS\" in exchange timezone","    \"\"\"","    dt = _parse_ts_str(ts_str).replace(tzinfo=ZoneInfo(data_tz))","    dt_ex = dt.astimezone(ZoneInfo(exchange_tz))","    return dt_ex.strftime(\"%H:%M:%S\")","","","def classify_session(","    ts_str: str,","    profile: SessionProfile,",") -> str | None:","    \"\"\"Classify timestamp string into session state.","    ","    Phase 6.6: Core classification logic with DST-safe timezone conversion.","    - ts_str (TPE string) → parse as data_tz → convert to exchange_tz","    - Use exchange time to compare with windows","    - BREAK 優先於 TRADING","    ","    Args:","        ts_str: Timestamp string in format \"YYYY/M/D HH:MM:SS\" (data timezone)","        profile: Session profile with data_tz, exchange_tz, and windows","        ","    Returns:","        Session state: \"TRADING\", \"BREAK\", or None","    \"\"\"","    # Phase 6.6: Parse ts_str as data_tz, convert to exchange_tz","    data_dt = _parse_ts_str_with_tz(ts_str, profile.data_tz)","    exchange_tz_info = ZoneInfo(profile.exchange_tz)","    exchange_dt = data_dt.astimezone(exchange_tz_info)","    ","    # Extract exchange time HH:MM:SS","    exchange_time_str = exchange_dt.strftime(\"%H:%M:%S\")","    ","    # Phase 6.6: Use windows if available (preferred method)","    if profile.windows:","        # BREAK 優先於 TRADING - check BREAK windows first","        for window in profile.windows:","            if window.state == \"BREAK\":","                if profile._time_in_range(exchange_time_str, window.start, window.end):","                    return \"BREAK\"","        ","        # Then check TRADING windows","        for window in profile.windows:","            if window.state == \"TRADING\":","                if profile._time_in_range(exchange_time_str, window.start, window.end):","                    return \"TRADING\"","        ","        return None","    ","    # Fallback to legacy modes for backward compatibility","    if profile.mode == \"tz_convert\":","        # tz_convert mode: Check BREAK first, then TRADING","        if profile.break_start and profile.break_end:","            if profile._time_in_range(exchange_time_str, profile.break_start, profile.break_end):","                return \"BREAK\"","        return \"TRADING\"","    ","    elif profile.mode == \"FIXED_TPE\":","        # FIXED_TPE mode: Use sessions list","        for session in profile.sessions:","            if profile._time_in_range(exchange_time_str, session.start, session.end):","                return session.name","        return None","    ","    elif profile.mode == \"EXCHANGE_RULE\":","        # EXCHANGE_RULE mode: Use rules","        rules = profile.rules","        if \"daily_maintenance\" in rules:","            maint = rules[\"daily_maintenance\"]","            maint_start = maint.get(\"start\", \"16:00:00\")","            maint_end = maint.get(\"end\", \"17:00:00\")","            if profile._time_in_range(exchange_time_str, maint_start, maint_end):","                return \"MAINTENANCE\"","        ","        if \"trading_week\" in rules:","            return \"TRADING\"","        ","        # Check sessions if available","        if profile.sessions:","            for session in profile.sessions:","                if profile._time_in_range(exchange_time_str, session.start, session.end):","                    return session.name","        ","        return None","    ","    else:","        raise ValueError(f\"Unknown profile mode: {profile.mode}\")","","","def classify_sessions(","    ts_str_series: pd.Series,","    profile: SessionProfile,",") -> pd.Series:","    \"\"\"Classify multiple timestamps into session names.","    ","    Args:","        ts_str_series: Series of timestamp strings (\"YYYY/M/D HH:MM:SS\") in local time","        profile: Session profile","        ","    Returns:","        Series of session names (or None)","    \"\"\"","    return ts_str_series.apply(lambda ts: classify_session(ts, profile))"]}
{"type":"file_footer","path":"src/data/session/classify.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/data/session/kbar.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12975,"sha256":"bda77389e3086de0034b10063b157f0aea7b8b626372a4d6ce751f6d2b6eeaf9","total_lines":360,"chunk_count":2}
{"type":"file_chunk","path":"src/data/session/kbar.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"K-Bar Aggregation.","","Phase 6.6: Aggregate bars into K-bars (30/60/120/240/DAY minutes).","Must anchor to Session.start (exchange timezone), no cross-session aggregation.","DST-safe: Uses exchange clock for bucket calculation.","\"\"\"","","from __future__ import annotations","","from datetime import datetime","from typing import List","","import numpy as np","import pandas as pd","from zoneinfo import ZoneInfo","","from data.session.classify import _parse_ts_str_tpe","from data.session.schema import SessionProfile","","","# Allowed K-bar intervals (minutes)","ALLOWED_INTERVALS = {30, 60, 120, 240, \"DAY\"}","","","def _is_trading_session(sess: str | None) -> bool:","    \"\"\"Check if a session is aggregatable (trading session).","    ","    Phase 6.6: Unified rule for determining aggregatable sessions.","    ","    Rules:","    - BREAK: Not aggregatable (absolute boundary)","    - None: Not aggregatable (outside any session)","    - MAINTENANCE: Not aggregatable","    - All others (TRADING, DAY, NIGHT, etc.): Aggregatable","    ","    This supports both:","    - Phase 6.6: TRADING/BREAK semantics","    - Legacy: DAY/NIGHT semantics","    ","    Args:","        sess: Session name or None","        ","    Returns:","        True if session is aggregatable, False otherwise","    \"\"\"","    if sess is None:","        return False","    # Phase 6.6: BREAK is absolute boundary","    if sess == \"BREAK\":","        return False","    # Legacy: MAINTENANCE is not aggregatable","    if sess == \"MAINTENANCE\":","        return False","    # All other sessions (TRADING, DAY, NIGHT, etc.) are aggregatable","    return True","","","def aggregate_kbar(","    df: pd.DataFrame,","    interval: int | str,","    profile: SessionProfile,",") -> pd.DataFrame:","    \"\"\"Aggregate bars into K-bars.","    ","    Rules:","    - Only allowed intervals: 30, 60, 120, 240, DAY","    - Must anchor to Session.start","    - No cross-session aggregation","    - DAY bar = one complete session","    ","    Args:","        df: DataFrame with columns: ts_str, open, high, low, close, volume","        interval: K-bar interval in minutes (30/60/120/240) or \"DAY\"","        profile: Session profile","        ","    Returns:","        Aggregated DataFrame with same columns","        ","    Raises:","        ValueError: If interval is not allowed","    \"\"\"","    if interval not in ALLOWED_INTERVALS:","        raise ValueError(","            f\"Invalid interval: {interval}. Allowed: {ALLOWED_INTERVALS}\"","        )","    ","    if interval == \"DAY\":","        return _aggregate_day_bar(df, profile)","    ","    # For minute intervals, aggregate within sessions","    return _aggregate_minute_bar(df, int(interval), profile)","","","def _aggregate_day_bar(df: pd.DataFrame, profile: SessionProfile) -> pd.DataFrame:","    \"\"\"Aggregate into DAY bars (one complete session per bar).","    ","    Phase 6.6: BREAK is absolute boundary - only aggregate trading sessions.","    DST-safe: Uses exchange clock for session grouping.","    DAY bar = one complete trading session.","    Each trading session produces one DAY bar, regardless of calendar date.","    \"\"\"","    from data.session.classify import classify_sessions","    ","    # Classify each bar into session","    df = df.copy()","    df[\"_session\"] = classify_sessions(df[\"ts_str\"], profile)","    ","    # Phase 6.6: Filter out non-aggregatable sessions (BREAK, None, MAINTENANCE)","    df = df[df[\"_session\"].apply(_is_trading_session)]","    ","    if len(df) == 0:","        return pd.DataFrame(columns=[\"ts_str\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"session\"])","    ","    # Convert to exchange timezone for grouping (DST-safe)","    # Phase 6.6: Add derived columns (not violating raw layer)","    if not profile.exchange_tz:","        raise ValueError(\"Profile must have exchange_tz for DAY bar aggregation\")","    exchange_tz_info = ZoneInfo(profile.exchange_tz)","    df[\"_local_dt\"] = df[\"ts_str\"].apply(_parse_ts_str_tpe)","    df[\"_ex_dt\"] = df[\"_local_dt\"].apply(lambda dt: dt.astimezone(exchange_tz_info))","    ","    # Group by session - each group = one complete session","    # For overnight sessions, all bars of the same session are grouped together","    groups = df.groupby(\"_session\", dropna=False)","    ","    result_rows = []","    for session, group in groups:","        # For EXCHANGE_RULE mode, session may not be in profile.sessions","        # Still produce DAY bar if session was classified","        # (session_obj is only needed for anchor time, which DAY bar doesn't use)","        ","        # Determine session start date in exchange timezone","        # Sort group by exchange datetime to find first bar chronologically","        group_sorted = group.sort_values(\"_ex_dt\")","        first_bar_ex_dt = group_sorted[\"_ex_dt\"].iloc[0]","        ","        # Get original local ts_str for output (keep TPE time)","        # Use first bar's ts_str as anchor - it represents session start in local time","        first_bar_ts_str = group_sorted[\"ts_str\"].iloc[0]","        ","        # For DAY bar, use first bar's ts_str directly","        # This ensures output matches the actual first bar time in local timezone","        ts_str = first_bar_ts_str","        ","        # Aggregate OHLCV","        open_val = group[\"open\"].iloc[0]","        high_val = group[\"high\"].max()","        low_val = group[\"low\"].min()","        close_val = group[\"close\"].iloc[-1]","        volume_val = group[\"volume\"].sum()","        ","        result_rows.append({","            \"ts_str\": ts_str,","            \"open\": open_val,","            \"high\": high_val,","            \"low\": low_val,","            \"close\": close_val,","            \"volume\": int(volume_val),","            \"session\": session,  # Phase 6.6: Add session label (derived data, not violating Raw)","        })","    ","    result_df = pd.DataFrame(result_rows)","    ","    # Remove helper columns if they exist","    for col in [\"_session\", \"_local_dt\", \"_ex_dt\"]:","        if col in result_df.columns:","            result_df = result_df.drop(columns=[col])","    ","    # Sort by ts_str to maintain chronological order","    if len(result_df) > 0:","        result_df = result_df.sort_values(\"ts_str\").reset_index(drop=True)","    ","    return result_df","","","def _aggregate_minute_bar(","    df: pd.DataFrame,","    interval_minutes: int,","    profile: SessionProfile,",") -> pd.DataFrame:","    \"\"\"Aggregate into minute bars (30/60/120/240).","    ","    Phase 6.6: BREAK is absolute boundary - only aggregate trading sessions.","    DST-safe: Uses exchange clock for bucket calculation.","    Must anchor to Session.start (exchange timezone), no cross-session aggregation.","    Bucket doesn't need to be full - any data produces a bar.","    \"\"\"","    from data.session.classify import classify_sessions","    ","    # Classify each bar into session","    df = df.copy()","    df[\"_session\"] = classify_sessions(df[\"ts_str\"], profile)","    ","    # Phase 6.6: Filter out non-aggregatable sessions (BREAK, None, MAINTENANCE)","    df = df[df[\"_session\"].apply(_is_trading_session)]","    ","    if len(df) == 0:","        return pd.DataFrame(columns=[\"ts_str\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"session\"])","    ","    # Convert to exchange timezone for bucket calculation"]}
{"type":"file_chunk","path":"src/data/session/kbar.py","chunk_index":1,"line_start":201,"line_end":360,"content":["    # Phase 6.6: Add derived columns (not violating raw layer)","    if not profile.exchange_tz:","        raise ValueError(\"Profile must have exchange_tz for minute bar aggregation\")","    exchange_tz_info = ZoneInfo(profile.exchange_tz)","    ","    df[\"_local_dt\"] = df[\"ts_str\"].apply(_parse_ts_str_tpe)","    df[\"_ex_dt\"] = df[\"_local_dt\"].apply(lambda dt: dt.astimezone(exchange_tz_info))","    ","    # Extract exchange date and time for grouping","    df[\"_ex_date\"] = df[\"_ex_dt\"].apply(lambda dt: dt.date().isoformat().replace(\"-\", \"/\"))","    df[\"_ex_time\"] = df[\"_ex_dt\"].apply(lambda dt: dt.strftime(\"%H:%M:%S\"))","    ","    result_rows = []","    ","    # Process each (exchange_date, session) group separately","    groups = df.groupby([\"_ex_date\", \"_session\"], dropna=False)","    ","    for (ex_date, session), group in groups:","        if not _is_trading_session(session):","            continue  # Skip non-aggregatable sessions (BREAK, None, MAINTENANCE)","        ","        # Find session start time from profile (in exchange timezone)","        # Phase 6.6: If windows exist, use first TRADING window.start","        # Legacy: Use current session name to find matching session.start","        session_start = None","        ","        if profile.windows:","            # Phase 6.6: Use first TRADING window.start","            for window in profile.windows:","                if window.state == \"TRADING\":","                    session_start = window.start","                    break","        else:","            # Legacy: Find session.start by matching session name","            for sess in profile.sessions:","                if sess.name == session:","                    session_start = sess.start","                    break","        ","        # If still not found, use first bar's exchange time as anchor","        if session_start is None:","            first_bar_ex_time = group[\"_ex_time\"].iloc[0]","            session_start = first_bar_ex_time","        ","        # Calculate bucket start times anchored to session.start (exchange timezone)","        buckets = _calculate_buckets(session_start, interval_minutes)","        ","        # Assign each bar to a bucket using exchange time","        group = group.copy()","        group[\"_bucket\"] = group[\"_ex_time\"].apply(","            lambda t: _find_bucket(t, buckets)","        )","        ","        # Aggregate per bucket","        bucket_groups = group.groupby(\"_bucket\", dropna=False)","        ","        for bucket_start, bucket_group in bucket_groups:","            if pd.isna(bucket_start):","                continue","            ","            # Phase 6.6: Bucket doesn't need to be full - any data produces a bar","            # BREAK is absolute boundary (already filtered out above)","            if bucket_group.empty:","                continue","            ","            # ts_str output: Use original local ts_str (TPE), not exchange time","            # But bucket grouping was done in exchange time","            first_bar_ts_str = bucket_group[\"ts_str\"].iloc[0]  # Original TPE ts_str","            ","            # Aggregate OHLCV","            open_val = bucket_group[\"open\"].iloc[0]","            high_val = bucket_group[\"high\"].max()","            low_val = bucket_group[\"low\"].min()","            close_val = bucket_group[\"close\"].iloc[-1]","            volume_val = bucket_group[\"volume\"].sum()","            ","            result_rows.append({","                \"ts_str\": first_bar_ts_str,  # Keep original TPE ts_str","                \"open\": open_val,","                \"high\": high_val,","                \"low\": low_val,","                \"close\": close_val,","                \"volume\": int(volume_val),","                \"session\": session,  # Phase 6.6: Add session label (derived data, not violating Raw)","            })","    ","    result_df = pd.DataFrame(result_rows)","    ","    # Remove helper columns","    for col in [\"_session\", \"_ex_date\", \"_ex_time\", \"_bucket\", \"_local_dt\", \"_ex_dt\"]:","        if col in result_df.columns:","            result_df = result_df.drop(columns=[col])","    ","    # Sort by ts_str to maintain chronological order","    if len(result_df) > 0:","        result_df = result_df.sort_values(\"ts_str\").reset_index(drop=True)","    ","    return result_df","","","def _calculate_buckets(session_start: str, interval_minutes: int) -> List[str]:","    \"\"\"Calculate bucket start times anchored to session_start.","    ","    Args:","        session_start: Session start time \"HH:MM:SS\"","        interval_minutes: Interval in minutes","        ","    Returns:","        List of bucket start times [\"HH:MM:SS\", ...]","    \"\"\"","    # Parse session_start","    parts = session_start.split(\":\")","    h = int(parts[0])","    m = int(parts[1])","    s = int(parts[2]) if len(parts) > 2 else 0","    ","    # Convert to total minutes","    start_minutes = h * 60 + m","    ","    buckets = []","    current_minutes = start_minutes","    ","    # Generate buckets until end of day (24:00:00 = 1440 minutes)","    while current_minutes < 1440:","        h_bucket = current_minutes // 60","        m_bucket = current_minutes % 60","        bucket_str = f\"{h_bucket:02d}:{m_bucket:02d}:00\"","        buckets.append(bucket_str)","        current_minutes += interval_minutes","    ","    return buckets","","","def _find_bucket(time_str: str, buckets: List[str]) -> str | None:","    \"\"\"Find which bucket a time belongs to.","    ","    Phase 6.6: Anchor-based bucket assignment.","    Bucket = floor((time - anchor) / interval)","    ","    Args:","        time_str: Time string \"HH:MM:SS\"","        buckets: List of bucket start times (sorted ascending)","        ","    Returns:","        Bucket start time if found, None otherwise","    \"\"\"","    # Find the largest bucket <= time_str","    # Buckets are sorted ascending, so iterate backwards","    for i in range(len(buckets) - 1, -1, -1):","        if buckets[i] <= time_str:","            # Check if next bucket would exceed time_str","            if i + 1 < len(buckets):","                next_bucket = buckets[i + 1]","                if time_str < next_bucket:","                    return buckets[i]","            else:","                # Last bucket - time_str falls in this bucket","                return buckets[i]","    ","    return None"]}
{"type":"file_footer","path":"src/data/session/kbar.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/data/session/loader.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4800,"sha256":"febd946a6e4dadf358262e8b033ccf3250d7f710eb3ec460d6d53f7c5282f9c8","total_lines":122,"chunk_count":1}
{"type":"file_chunk","path":"src/data/session/loader.py","chunk_index":0,"line_start":1,"line_end":122,"content":["\"\"\"Session Profile loader.","","Phase 6.6: Load session profiles from YAML files.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","from typing import Any","","import yaml","","from data.session.schema import Session, SessionProfile, SessionWindow","","","def load_session_profile(profile_path: Path) -> SessionProfile:","    \"\"\"Load session profile from YAML file.","    ","    Args:","        profile_path: Path to YAML profile file","        ","    Returns:","        SessionProfile loaded from YAML","        ","    Raises:","        FileNotFoundError: If profile file does not exist","        ValueError: If profile structure is invalid","    \"\"\"","    if not profile_path.exists():","        raise FileNotFoundError(f\"Session profile not found: {profile_path}\")","    ","    with profile_path.open(\"r\", encoding=\"utf-8\") as f:","        data = yaml.safe_load(f)","    ","    if not isinstance(data, dict):","        raise ValueError(f\"Invalid profile format: expected dict, got {type(data)}\")","    ","    symbol = data.get(\"symbol\")","    version = data.get(\"version\")","    mode = data.get(\"mode\", \"FIXED_TPE\")  # Default to FIXED_TPE for backward compatibility","    exchange_tz = data.get(\"exchange_tz\")","    data_tz = data.get(\"data_tz\", \"Asia/Taipei\")  # Phase 6.6: Default to Asia/Taipei","    local_tz = data.get(\"local_tz\", \"Asia/Taipei\")","    sessions_data = data.get(\"sessions\", [])","    windows_data = data.get(\"windows\", [])  # Phase 6.6: Windows with TRADING/BREAK states","    rules = data.get(\"rules\", {})","    break_start = data.get(\"break\", {}).get(\"start\") if isinstance(data.get(\"break\"), dict) else None","    break_end = data.get(\"break\", {}).get(\"end\") if isinstance(data.get(\"break\"), dict) else None","    ","    if not symbol:","        raise ValueError(\"Profile missing 'symbol' field\")","    if not version:","        raise ValueError(\"Profile missing 'version' field\")","    ","    # Phase 6.6: exchange_tz is required","    if not exchange_tz:","        raise ValueError(\"Profile missing 'exchange_tz' field (required in Phase 6.6)\")","    ","    if mode not in [\"FIXED_TPE\", \"EXCHANGE_RULE\", \"tz_convert\"]:","        raise ValueError(f\"Invalid mode: {mode}. Must be 'FIXED_TPE', 'EXCHANGE_RULE', or 'tz_convert'\")","    ","    # Phase 6.6: Load windows (preferred method)","    windows = []","    if windows_data:","        if not isinstance(windows_data, list):","            raise ValueError(f\"Profile 'windows' must be list, got {type(windows_data)}\")","        ","        for win_data in windows_data:","            if not isinstance(win_data, dict):","                raise ValueError(f\"Window must be dict, got {type(win_data)}\")","            ","            state = win_data.get(\"state\")","            start = win_data.get(\"start\")","            end = win_data.get(\"end\")","            ","            if state not in [\"TRADING\", \"BREAK\"]:","                raise ValueError(f\"Window state must be 'TRADING' or 'BREAK', got {state}\")","            if not start or not end:","                raise ValueError(f\"Window missing required fields: state={state}, start={start}, end={end}\")","            ","            windows.append(SessionWindow(state=state, start=start, end=end))","    ","    # Backward compatibility: Load sessions for legacy modes","    sessions = []","    if sessions_data:","        if not isinstance(sessions_data, list):","            raise ValueError(f\"Profile 'sessions' must be list, got {type(sessions_data)}\")","        ","        for sess_data in sessions_data:","            if not isinstance(sess_data, dict):","                raise ValueError(f\"Session must be dict, got {type(sess_data)}\")","            ","            name = sess_data.get(\"name\")","            start = sess_data.get(\"start\")","            end = sess_data.get(\"end\")","            ","            if not name or not start or not end:","                raise ValueError(f\"Session missing required fields: name={name}, start={start}, end={end}\")","            ","            sessions.append(Session(name=name, start=start, end=end))","    elif mode == \"EXCHANGE_RULE\":","        if not isinstance(rules, dict):","            raise ValueError(f\"Profile 'rules' must be dict for EXCHANGE_RULE mode, got {type(rules)}\")","    elif mode == \"tz_convert\":","        # Legacy requirement only applies when windows are NOT provided","        # Phase 6.6: If windows_data exists, windows-driven mode doesn't need break.start/end","        if (not windows_data) and (not break_start or not break_end):","            raise ValueError(f\"tz_convert mode requires 'break.start' and 'break.end' fields (or 'windows' for Phase 6.6)\")","    ","    return SessionProfile(","        symbol=symbol,","        version=version,","        mode=mode,","        exchange_tz=exchange_tz,","        data_tz=data_tz,","        local_tz=local_tz,","        sessions=sessions,","        windows=windows,","        rules=rules,","        break_start=break_start,","        break_end=break_end,","    )"]}
{"type":"file_footer","path":"src/data/session/loader.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/data/session/schema.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4466,"sha256":"643addf865991cb6271ef1bc1e04ebf7f3c87a6a6b0969e16fd56423efd14b86","total_lines":103,"chunk_count":1}
{"type":"file_chunk","path":"src/data/session/schema.py","chunk_index":0,"line_start":1,"line_end":103,"content":["\"\"\"Session Profile schema.","","Phase 6.6: Session Profile schema with DST-safe timezone conversion.","Session times are defined in exchange timezone, classification uses exchange clock.","","Supports two modes:","- FIXED_TPE: Direct Taiwan time string comparison (e.g., TWF.MXF)","- EXCHANGE_RULE: Exchange timezone + rules, dynamically compute TPE windows (e.g., CME.MNQ)","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, field","from typing import Any, Dict, List, Literal","","","@dataclass(frozen=True)","class SessionWindow:","    \"\"\"Session window definition with state.","    ","    Phase 6.6: Only allows TRADING and BREAK states.","    Session times are defined in exchange timezone (format: \"HH:MM:SS\").","    ","    Attributes:","        state: Session state - \"TRADING\" or \"BREAK\"","        start: Session start time (exchange timezone, \"HH:MM:SS\")","        end: Session end time (exchange timezone, \"HH:MM:SS\")","    \"\"\"","    state: Literal[\"TRADING\", \"BREAK\"]","    start: str  # Exchange timezone \"HH:MM:SS\"","    end: str    # Exchange timezone \"HH:MM:SS\"","","","@dataclass(frozen=True)","class Session:","    \"\"\"Trading session definition.","    ","    Session times are defined in exchange timezone (format: \"HH:MM:SS\").","    ","    Attributes:","        name: Session name (e.g., \"DAY\", \"NIGHT\", \"TRADING\", \"BREAK\", \"MAINTENANCE\")","        start: Session start time (exchange timezone, \"HH:MM:SS\")","        end: Session end time (exchange timezone, \"HH:MM:SS\")","    \"\"\"","    name: str","    start: str  # Exchange timezone \"HH:MM:SS\"","    end: str    # Exchange timezone \"HH:MM:SS\"","","","@dataclass(frozen=True)","class SessionProfile:","    \"\"\"Session profile for a symbol.","    ","    Contains trading sessions defined in exchange timezone.","    Classification converts local time to exchange time for comparison.","    ","    Phase 6.6: data_tz defaults to \"Asia/Taipei\", exchange_tz must be specified.","    ","    Attributes:","        symbol: Symbol identifier (e.g., \"CME.MNQ\", \"TWF.MXF\")","        version: Profile version (e.g., \"v1\", \"v2\")","        mode: Profile mode - \"FIXED_TPE\" (direct TPE comparison), \"EXCHANGE_RULE\" (exchange rules), or \"tz_convert\" (timezone conversion with BREAK priority)","        exchange_tz: Exchange timezone (IANA, e.g., \"America/Chicago\")","        data_tz: Data timezone (IANA, default: \"Asia/Taipei\")","        local_tz: Local timezone (default: \"Asia/Taipei\")","        sessions: List of trading sessions (for FIXED_TPE mode)","        windows: List of session windows with TRADING/BREAK states (Phase 6.6)","        rules: Exchange rules dict (for EXCHANGE_RULE mode, e.g., daily_maintenance, trading_week)","        break_start: BREAK session start time (HH:MM:SS in exchange timezone) for tz_convert mode","        break_end: BREAK session end time (HH:MM:SS in exchange timezone) for tz_convert mode","    \"\"\"","    symbol: str","    version: str","    mode: Literal[\"FIXED_TPE\", \"EXCHANGE_RULE\", \"tz_convert\"]","    exchange_tz: str  # IANA timezone (e.g., \"America/Chicago\") - required","    data_tz: str = \"Asia/Taipei\"  # Data timezone (default: \"Asia/Taipei\")","    local_tz: str = \"Asia/Taipei\"  # Default to Taiwan time","    sessions: List[Session] = field(default_factory=list)  # For FIXED_TPE mode","    windows: List[SessionWindow] = field(default_factory=list)  # Phase 6.6: Windows with TRADING/BREAK states","    rules: Dict[str, Any] = field(default_factory=dict)  # For EXCHANGE_RULE mode","    break_start: str | None = None  # BREAK start (HH:MM:SS in exchange timezone) for tz_convert mode","    break_end: str | None = None  # BREAK end (HH:MM:SS in exchange timezone) for tz_convert mode","    ","    def _time_in_range(self, time_str: str, start: str, end: str) -> bool:","        \"\"\"Check if time_str is within [start, end) using string comparison.","        ","        Handles both normal sessions (start <= end) and overnight sessions (start > end).","        ","        Args:","            time_str: Time to check (\"HH:MM:SS\") in exchange timezone","            start: Start time (\"HH:MM:SS\") in exchange timezone","            end: End time (\"HH:MM:SS\") in exchange timezone","            ","        Returns:","            True if time_str falls within the session range","        \"\"\"","        if start <= end:","            # Non-overnight session (e.g., DAY: 08:45:00 - 13:45:00)","            return start <= time_str < end","        else:","            # Overnight session (e.g., NIGHT: 21:00:00 - 06:00:00)","            # time_str >= start OR time_str < end","            return time_str >= start or time_str < end"]}
{"type":"file_footer","path":"src/data/session/schema.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/data/session/tzdb_info.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1870,"sha256":"ae4e5057feaa814367723455f5d3988f97c44760b815108c0e8fa67b95419330","total_lines":56,"chunk_count":1}
{"type":"file_chunk","path":"src/data/session/tzdb_info.py","chunk_index":0,"line_start":1,"line_end":56,"content":["\"\"\"Timezone database information utilities.","","Phase 6.6: Get tzdb provider and version for manifest recording.","\"\"\"","","from __future__ import annotations","","from importlib import metadata","from pathlib import Path","from typing import Tuple","import zoneinfo","","","def get_tzdb_info() -> Tuple[str, str]:","    \"\"\"Get timezone database provider and version.","    ","    Phase 6.6: Extract tzdb provider and version for manifest recording.","    ","    Strategy:","    1. If tzdata package (PyPI) is installed, use it as provider + version","    2. Otherwise, try to discover tzdata.zi from zoneinfo.TZPATH (module-level)","    ","    Returns:","        Tuple of (provider, version)","        - provider: \"tzdata\" (PyPI package) or \"zoneinfo\" (standard library)","        - version: Version string from tzdata package or tzdata.zi file, or \"unknown\" if not found","    \"\"\"","    provider = \"zoneinfo\"","    version = \"unknown\"","","    # 1) If tzdata package installed, prefer it as provider + version","    try:","        version = metadata.version(\"tzdata\")","        provider = \"tzdata\"","        return provider, version","    except metadata.PackageNotFoundError:","        pass","","    # 2) Try discover tzdata.zi from zoneinfo.TZPATH (module-level)","    tzpaths = getattr(zoneinfo, \"TZPATH\", ())","    for p in tzpaths:","        cand = Path(p) / \"tzdata.zi\"","        if cand.exists():","            # best-effort parse: search a line containing \"version\"","            try:","                text = cand.read_text(encoding=\"utf-8\", errors=\"ignore\")","                # minimal heuristic: find first token that looks like YYYYx (not strict)","                for line in text.splitlines()[:200]:","                    if \"version\" in line.lower():","                        version = line.strip().split()[-1].strip('\"')","                        break","            except OSError:","                pass","            break","","    return provider, version"]}
{"type":"file_footer","path":"src/data/session/tzdb_info.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/deployment/compiler.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8255,"sha256":"4f1b11163ad730e576513a083ebdc1c74530a83bdce6dd44e86c98f44eca18f5","total_lines":254,"chunk_count":2}
{"type":"file_chunk","path":"src/deployment/compiler.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Portfolio Compilation – Phase 3C.","","Compile a frozen Season Manifest into deployment TXT files for MultiCharts.","\"\"\"","","from __future__ import annotations","","import json","import re","from pathlib import Path","from typing import Dict, List, Any, Optional","","from governance.models import SeasonManifest","from portfolio.spec import PortfolioSpec, PortfolioLeg","from control.deploy_txt import write_deployment_txt","","","def parse_timeframe_to_minutes(timeframe_str: str) -> int:","    \"\"\"","    Convert timeframe string (e.g., '60m', '15m', '1D') to minutes.","","    Supports:","    - 'Nm' where N is integer minutes.","    - 'ND' where N is integer days (converted to minutes: N * 24 * 60).","    - 'NH' where N is integer hours (converted to minutes: N * 60).","","    Raises ValueError if format unrecognized.","    \"\"\"","    timeframe_str = timeframe_str.strip()","    # Pattern: integer followed by unit","    match = re.match(r\"^(\\d+)([mDhH])$\", timeframe_str)","    if not match:","        raise ValueError(f\"Unsupported timeframe format: {timeframe_str}\")","","    value = int(match.group(1))","    unit = match.group(2).lower()","","    if unit == \"m\":","        return value","    elif unit == \"h\":","        return value * 60","    elif unit == \"d\":","        return value * 24 * 60","    else:","        raise ValueError(f\"Unknown timeframe unit: {unit}\")","","","def load_universe_spec(universe_path: Path) -> Dict[str, Dict[str, Any]]:","    \"\"\"","    Load universe YAML and convert to deploy_txt compatible format.","","    Expected YAML structure (from configs/portfolio/instruments.yaml):","        instruments:","          SYMBOL:","            tick_size: float","            multiplier: float","            ...","","    Returns dict mapping symbol to dict with keys:","        tick_size, multiplier, commission_per_side_usd, session_profile","    \"\"\"","    import yaml","","    with open(universe_path, \"r\", encoding=\"utf-8\") as f:","        data = yaml.safe_load(f)","","    instruments = data.get(\"instruments\", {})","    universe_spec = {}","","    for symbol, spec in instruments.items():","        # Extract tick_size and multiplier (required)","        tick_size = spec.get(\"tick_size\", 0.25)","        multiplier = spec.get(\"multiplier\", 1.0)","","        # Commission per side not defined in instruments.yaml; default 0.0","        commission_per_side_usd = spec.get(\"commission_per_side_usd\", 0.0)","","        # Session profile: infer from exchange field or default to symbol's exchange part","        session_profile = spec.get(\"session_profile\", None)","        if session_profile is None:","            # Try to extract exchange from symbol (e.g., \"CME.MNQ\" -> \"CME\")","            exchange = symbol.split(\".\")[0] if \".\" in symbol else symbol","            session_profile = exchange","","        universe_spec[symbol] = {","            \"tick_size\": tick_size,","            \"multiplier\": multiplier,","            \"commission_per_side_usd\": commission_per_side_usd,","            \"session_profile\": session_profile,","        }","","    return universe_spec","","","def candidate_to_leg(candidate: Dict[str, Any]) -> PortfolioLeg:","    \"\"\"Convert a candidate from chosen_params snapshot to a PortfolioLeg.\"\"\"","    candidate_id = candidate[\"candidate_id\"]","    strategy_id = candidate[\"strategy_id\"]","    symbol = candidate[\"symbol\"]","    timeframe_str = candidate[\"timeframe\"]","    params = candidate[\"params\"]","","    # Convert timeframe to minutes","    timeframe_min = parse_timeframe_to_minutes(timeframe_str)","","    # Determine session profile from symbol exchange (will be overridden later)","    # Use placeholder; will be replaced by universe spec mapping.","    session_profile = symbol.split(\".\")[0] if \".\" in symbol else symbol","","    # Strategy version: not present in candidate; default to \"v1\"","    strategy_version = \"v1\"","","    # Ensure params values are float (they may be int)","    float_params = {k: float(v) for k, v in params.items()}","","    return PortfolioLeg(","        leg_id=candidate_id,","        symbol=symbol,","        timeframe_min=timeframe_min,","        session_profile=session_profile,","        strategy_id=strategy_id,","        strategy_version=strategy_version,","        params=float_params,","        enabled=True,","    )","","","def compile_season(manifest_path: Path, output_dir: Path) -> None:","    \"\"\"","    Compile a frozen season manifest into deployment TXT files.","","    Steps:","    1. Load SeasonManifest from manifest_path.","    2. Extract chosen_params_snapshot (main + backups).","    3. Build PortfolioSpec from candidates.","    4. Load universe spec from referenced universe.yaml (must exist in season references).","    5. Write deployment TXT files using write_deployment_txt.","","    Constraints:","    - All information must come from the manifest and its referenced files.","    - No direct reading of data/ or strategies/ directories.","","    Raises:","        FileNotFoundError: If manifest or referenced universe file missing.","        ValueError: If any required data is missing or malformed.","    \"\"\"","    # 1. Load manifest","    manifest = SeasonManifest.load(manifest_path)","    season_id = manifest.season_id","","    # 2. Extract candidates","    chosen = manifest.chosen_params_snapshot","    main = chosen.get(\"main\")","    backups = chosen.get(\"backups\", [])","","    if not main:","        raise ValueError(\"Chosen params snapshot missing 'main' candidate\")","","    candidates = [main] + backups","","    # 3. Build portfolio legs","    legs: List[PortfolioLeg] = []","    for cand in candidates:","        legs.append(candidate_to_leg(cand))","","    # 4. Build portfolio spec","    portfolio_spec = PortfolioSpec(","        portfolio_id=f\"season_{season_id}\",","        version=season_id,","        legs=legs,","    )","","    # 5. Locate universe file (should be in season references)","    season_dir = manifest_path.parent.parent  # outputs/seasons/{season_id}","    universe_path = season_dir / \"references\" / \"universe.yaml\"","    if not universe_path.exists():","        # Fallback to configs/portfolio/instruments.yaml (should have been referenced)","        universe_path = Path(\"configs/portfolio/instruments.yaml\")","        if not universe_path.exists():","            raise FileNotFoundError(","                f\"Universe file not found in season references or configs: {universe_path}\"","            )","","    universe_spec = load_universe_spec(universe_path)","","    # Ensure all symbols in portfolio have a universe entry","    missing_symbols = set(leg.symbol for leg in legs) - set(universe_spec.keys())","    if missing_symbols:","        raise ValueError(","            f\"Universe spec missing entries for symbols: {missing_symbols}\"","        )","","    # 6. Write deployment TXT files","    output_dir.mkdir(parents=True, exist_ok=True)","    write_deployment_txt(portfolio_spec, universe_spec, output_dir)","","    # 7. Validate non‑empty output files","    required_files = [\"universe.txt\", \"strategy_params.txt\", \"portfolio.txt\"]"]}
{"type":"file_chunk","path":"src/deployment/compiler.py","chunk_index":1,"line_start":201,"line_end":254,"content":["    for fname in required_files:","        fpath = output_dir / fname","        if not fpath.exists():","            raise RuntimeError(f\"Deployment file not created: {fpath}\")","        if fpath.stat().st_size == 0:","            raise RuntimeError(f\"Deployment file is empty: {fpath}\")","","    print(f\"Deployment Pack ready at: {output_dir}\")","","","def main_cli() -> None:","    \"\"\"CLI entry point.\"\"\"","    import argparse","    import sys","","    parser = argparse.ArgumentParser(","        description=\"Compile a frozen Season Manifest into deployment TXT files\"","    )","    parser.add_argument(","        \"manifest\",","        type=Path,","        help=\"Path to season_manifest.json\",","    )","    parser.add_argument(","        \"--output-dir\",","        type=Path,","        default=None,","        help=\"Output directory (default: outputs/deployment/{season_id})\",","    )","","    args = parser.parse_args()","","    if not args.manifest.exists():","        print(f\"Error: Manifest file not found: {args.manifest}\", file=sys.stderr)","        sys.exit(1)","","    # Determine output directory","    if args.output_dir is None:","        manifest = SeasonManifest.load(args.manifest)","        output_dir = Path(\"outputs\") / \"deployment\" / manifest.season_id","    else:","        output_dir = args.output_dir","","    try:","        compile_season(args.manifest, output_dir)","    except Exception as e:","        print(f\"Compilation failed: {e}\", file=sys.stderr)","        import traceback","        traceback.print_exc()","        sys.exit(1)","","","if __name__ == \"__main__\":","    main_cli()"]}
{"type":"file_footer","path":"src/deployment/compiler.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/engine/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":125,"sha256":"220c46b905057be585f90131875c9251c0dc7ed20013265618a72cce6ef8f367","total_lines":8,"chunk_count":1}
{"type":"file_chunk","path":"src/engine/__init__.py","chunk_index":0,"line_start":1,"line_end":8,"content":["","\"\"\"Engine module - unified simulate entry point.\"\"\"","","from engine.simulate import simulate_run","","__all__ = [\"simulate_run\"]","",""]}
{"type":"file_footer","path":"src/engine/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/engine/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":293,"sha256":"7d094470df4c46e6f9e86749f6b0bae9d1cd731b75c927656661b0884818c8ba","note":"skipped by policy"}
{"type":"file_skipped","path":"src/engine/__pycache__/constants.cpython-312.pyc","reason":"cache","bytes":442,"sha256":"2284f9dc1825f99bab9dcbea64f79ddbc603c9712ce6e62ee0d945b6543ba64a","note":"skipped by policy"}
{"type":"file_skipped","path":"src/engine/__pycache__/engine_jit.cpython-312.pyc","reason":"cache","bytes":28130,"sha256":"92e0f7c4c8e983b89b462af5c054cb9568e9592fa2fe66e5051937b81c21dc81","note":"skipped by policy"}
{"type":"file_skipped","path":"src/engine/__pycache__/matcher_core.cpython-312.pyc","reason":"cache","bytes":6417,"sha256":"6edfe71ce17164b9e6e5cd457d3bf05d99feb490a9fdb9370d06c087fd4f92c3","note":"skipped by policy"}
{"type":"file_skipped","path":"src/engine/__pycache__/simulate.cpython-312.pyc","reason":"cache","bytes":1812,"sha256":"f883cde52e5b59d5b7d0986c34d02af47749b4281122f2820ec48dbb5a6162c0","note":"skipped by policy"}
{"type":"file_skipped","path":"src/engine/__pycache__/types.cpython-312.pyc","reason":"cache","bytes":2632,"sha256":"5d3ce9f12385d180940a12f4fde3955195adb4457c057c20d1135009af4dfb7d","note":"skipped by policy"}
{"type":"file_header","path":"src/engine/constants.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":240,"sha256":"808f72fab120a3072a3e505084e31ecd3626833273a0964fec79e553029591c6","total_lines":19,"chunk_count":1}
{"type":"file_chunk","path":"src/engine/constants.py","chunk_index":0,"line_start":1,"line_end":19,"content":["","\"\"\"","Engine integer constants (hot-path friendly).","","These constants are used in array/SoA pathways to avoid Enum.value lookups in tight loops.","\"\"\"","","ROLE_EXIT = 0","ROLE_ENTRY = 1","","KIND_STOP = 0","KIND_LIMIT = 1","","SIDE_SELL = -1","SIDE_BUY = 1","","","",""]}
{"type":"file_footer","path":"src/engine/constants.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/engine/constitution.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":979,"sha256":"c75355ca7008415a51a0b93258cd76d5e7146757587c72397e36f114a1a500e7","total_lines":41,"chunk_count":1}
{"type":"file_chunk","path":"src/engine/constitution.py","chunk_index":0,"line_start":1,"line_end":41,"content":["","\"\"\"","Engine Constitution v1.1 (FROZEN)","","Activation:","- Orders are created at Bar[T] close and become active at Bar[T+1].","","STOP fills (Open==price is treated as GAP branch):","Buy Stop @ S:","- if Open >= S: fill = Open","- elif High >= S: fill = S","Sell Stop @ S:","- if Open <= S: fill = Open","- elif Low <= S: fill = S","","LIMIT fills (Open==price is treated as GAP branch):","Buy Limit @ L:","- if Open <= L: fill = Open","- elif Low <= L: fill = L","Sell Limit @ L:","- if Open >= L: fill = Open","- elif High >= L: fill = L","","Priority:","- STOP wins over LIMIT (risk-first pessimism).","","Same-bar In/Out:","- If entry and exit are both triggerable in the same bar, execute Entry then Exit.","","Same-kind tie rule:","- If multiple orders of the same role are triggerable in the same bar, execute EXIT-first.","- Within the same role+kind, use deterministic order: smaller order_id first.","\"\"\"","","NEXT_BAR_ACTIVE = True","PRIORITY_STOP_OVER_LIMIT = True","SAME_BAR_ENTRY_THEN_EXIT = True","SAME_KIND_TIE_EXIT_FIRST = True","","",""]}
{"type":"file_footer","path":"src/engine/constitution.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/engine/engine_jit.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":26766,"sha256":"5846f669c1283645cfbd83f0972d151ef495c90a0a76bd3a62c38a6da9668ea4","total_lines":793,"chunk_count":4}
{"type":"file_chunk","path":"src/engine/engine_jit.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","from __future__ import annotations","","from dataclasses import asdict","from typing import Iterable, List, Tuple","","import numpy as np","","# Engine JIT matcher kernel contract:","# - Complexity target: O(B + I + A), where:","#     B = bars, I = intents, A = per-bar active-book scan.","# - Forbidden: scanning all intents per bar (O(B*I)).","# - Extension point: ttl_bars (0=GTC, 1=one-shot next-bar-only, future: >1).","","try:","    import numba as nb","except Exception:  # pragma: no cover","    nb = None  # type: ignore","","from engine.types import (","    BarArrays,","    Fill,","    OrderIntent,","    OrderKind,","    OrderRole,","    Side,",")","from engine.matcher_core import simulate as simulate_py","from engine.constants import (","    KIND_LIMIT,","    KIND_STOP,","    ROLE_ENTRY,","    ROLE_EXIT,","    SIDE_BUY,","    SIDE_SELL,",")","","# Side enum codes for uint8 encoding (avoid -1 cast deprecation)","SIDE_BUY_CODE = 1","SIDE_SELL_CODE = 255  # SIDE_SELL (-1) encoded as uint8","","STATUS_OK = 0","STATUS_ERROR_UNSORTED = 1","STATUS_BUFFER_FULL = 2","","# Intent TTL default (Constitution constant)","INTENT_TTL_BARS_DEFAULT = 1  # one-shot next-bar-only (Phase 2 semantics)","","# JIT truth (debug/perf observability)","JIT_PATH_USED_LAST = False","JIT_KERNEL_SIGNATURES_LAST = None  # type: ignore","","","def get_jit_truth() -> dict:","    \"\"\"","    Debug helper: returns whether the last simulate() call used the JIT kernel,","    and (if available) the kernel signatures snapshot.","    \"\"\"","    return {","        \"jit_path_used\": bool(JIT_PATH_USED_LAST),","        \"kernel_signatures\": JIT_KERNEL_SIGNATURES_LAST,","    }","","","def _to_int(x) -> int:","    # Enum values are int/str; we convert deterministically.","    if isinstance(x, Side):","        return int(x.value)","    if isinstance(x, OrderRole):","        # EXIT first tie-break relies on role; map explicitly.","        return 0 if x == OrderRole.EXIT else 1","    if isinstance(x, OrderKind):","        return 0 if x == OrderKind.STOP else 1","    return int(x)","","","def _to_kind_int(k: OrderKind) -> int:","    return 0 if k == OrderKind.STOP else 1","","","def _to_role_int(r: OrderRole) -> int:","    return 0 if r == OrderRole.EXIT else 1","","","def _to_side_int(s: Side) -> int:","    \"\"\"","    Convert Side enum to integer code for uint8 encoding.","    ","    Returns:","        SIDE_BUY_CODE (1) for Side.BUY","        SIDE_SELL_CODE (255) for Side.SELL (avoid -1 cast deprecation)","    \"\"\"","    if s == Side.BUY:","        return SIDE_BUY_CODE","    elif s == Side.SELL:","        return SIDE_SELL_CODE","    else:","        raise ValueError(f\"Unknown Side enum: {s}\")","","","def _kind_from_int(v: int) -> OrderKind:","    \"\"\"","    Decode kind enum from integer value (strict mode).","    ","    Allowed values:","    - 0 (KIND_STOP) -> OrderKind.STOP","    - 1 (KIND_LIMIT) -> OrderKind.LIMIT","    ","    Raises ValueError for any other value to catch silent corruption.","    \"\"\"","    if v == KIND_STOP:  # 0","        return OrderKind.STOP","    elif v == KIND_LIMIT:  # 1","        return OrderKind.LIMIT","    else:","        raise ValueError(","            f\"Invalid kind enum value: {v}. Allowed values are {KIND_STOP} (STOP) or {KIND_LIMIT} (LIMIT)\"","        )","","","def _role_from_int(v: int) -> OrderRole:","    \"\"\"","    Decode role enum from integer value (strict mode).","    ","    Allowed values:","    - 0 (ROLE_EXIT) -> OrderRole.EXIT","    - 1 (ROLE_ENTRY) -> OrderRole.ENTRY","    ","    Raises ValueError for any other value to catch silent corruption.","    \"\"\"","    if v == ROLE_EXIT:  # 0","        return OrderRole.EXIT","    elif v == ROLE_ENTRY:  # 1","        return OrderRole.ENTRY","    else:","        raise ValueError(","            f\"Invalid role enum value: {v}. Allowed values are {ROLE_EXIT} (EXIT) or {ROLE_ENTRY} (ENTRY)\"","        )","","","def _side_from_int(v: int) -> Side:","    \"\"\"","    Decode side enum from integer value (strict mode).","    ","    Allowed values:","    - SIDE_BUY_CODE (1) -> Side.BUY","    - SIDE_SELL_CODE (255) -> Side.SELL","    ","    Raises ValueError for any other value to catch silent corruption.","    \"\"\"","    if v == SIDE_BUY_CODE:  # 1","        return Side.BUY","    elif v == SIDE_SELL_CODE:  # 255","        return Side.SELL","    else:","        raise ValueError(","            f\"Invalid side enum value: {v}. Allowed values are {SIDE_BUY_CODE} (BUY) or {SIDE_SELL_CODE} (SELL)\"","        )","","","def _pack_intents(intents: Iterable[OrderIntent]):","    \"\"\"","    Pack intents into plain arrays for numba.","","    Fields (optimized dtypes):","      order_id: int32 (INDEX_DTYPE)","      created_bar: int32 (INDEX_DTYPE)","      role: uint8 (INTENT_ENUM_DTYPE, 0=EXIT,1=ENTRY)","      kind: uint8 (INTENT_ENUM_DTYPE, 0=STOP,1=LIMIT)","      side: uint8 (INTENT_ENUM_DTYPE, SIDE_BUY_CODE=BUY, SIDE_SELL_CODE=SELL)","      price: float64 (INTENT_PRICE_DTYPE)","      qty: int32 (INDEX_DTYPE)","    \"\"\"","    from config.dtypes import (","        INDEX_DTYPE,","        INTENT_ENUM_DTYPE,","        INTENT_PRICE_DTYPE,","    )","    ","    it = list(intents)","    n = len(it)","    order_id = np.empty(n, dtype=INDEX_DTYPE)","    created_bar = np.empty(n, dtype=INDEX_DTYPE)","    role = np.empty(n, dtype=INTENT_ENUM_DTYPE)","    kind = np.empty(n, dtype=INTENT_ENUM_DTYPE)","    side = np.empty(n, dtype=INTENT_ENUM_DTYPE)","    price = np.empty(n, dtype=INTENT_PRICE_DTYPE)","    qty = np.empty(n, dtype=INDEX_DTYPE)","","    for i, x in enumerate(it):","        order_id[i] = int(x.order_id)","        created_bar[i] = int(x.created_bar)","        role[i] = INTENT_ENUM_DTYPE(_to_role_int(x.role))","        kind[i] = INTENT_ENUM_DTYPE(_to_kind_int(x.kind))","        side[i] = INTENT_ENUM_DTYPE(_to_side_int(x.side))","        price[i] = INTENT_PRICE_DTYPE(x.price)","        qty[i] = int(x.qty)","","    return order_id, created_bar, role, kind, side, price, qty",""]}
{"type":"file_chunk","path":"src/engine/engine_jit.py","chunk_index":1,"line_start":201,"line_end":400,"content":["","def _sort_packed_by_created_bar(","    packed: Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray],",") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:","    \"\"\"","    Sort packed intent arrays by (created_bar, order_id).","","    Why:","      - Cursor + active-book kernel requires activate_bar=(created_bar+1) and order_id to be non-decreasing.","      - Determinism is preserved because selection is still based on (kind priority, order_id).","    \"\"\"","    order_id, created_bar, role, kind, side, price, qty = packed","    # lexsort uses last key as primary -> (created_bar primary, order_id secondary)","    idx = np.lexsort((order_id, created_bar))","    return (","        order_id[idx],","        created_bar[idx],","        role[idx],","        kind[idx],","        side[idx],","        price[idx],","        qty[idx],","    )","","","def simulate(","    bars: BarArrays,","    intents: Iterable[OrderIntent],",") -> List[Fill]:","    \"\"\"","    Phase 2A: JIT accelerated matcher.","","    Kill switch:","      - If numba is unavailable OR NUMBA_DISABLE_JIT=1, fall back to Python reference.","    \"\"\"","    global JIT_PATH_USED_LAST, JIT_KERNEL_SIGNATURES_LAST","","    if nb is None:","        JIT_PATH_USED_LAST = False","        JIT_KERNEL_SIGNATURES_LAST = None","        return simulate_py(bars, intents)","","    # If numba is disabled, keep behavior stable.","    # Numba respects NUMBA_DISABLE_JIT; but we short-circuit to be safe.","    import os","","    if os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() == \"1\":","        JIT_PATH_USED_LAST = False","        JIT_KERNEL_SIGNATURES_LAST = None","        return simulate_py(bars, intents)","","    packed = _sort_packed_by_created_bar(_pack_intents(intents))","    status, fills_arr = _simulate_kernel(","        bars.open,","        bars.high,","        bars.low,","        packed[0],","        packed[1],","        packed[2],","        packed[3],","        packed[4],","        packed[5],","        packed[6],","        np.int64(INTENT_TTL_BARS_DEFAULT),  # Use Constitution constant","    )","    if int(status) != STATUS_OK:","        JIT_PATH_USED_LAST = True","        raise RuntimeError(f\"engine_jit kernel error: status={int(status)}\")","","    # record JIT truth (best-effort)","    JIT_PATH_USED_LAST = True","    try:","        sigs = getattr(_simulate_kernel, \"signatures\", None)","        if sigs is not None:","            JIT_KERNEL_SIGNATURES_LAST = list(sigs)","        else:","            JIT_KERNEL_SIGNATURES_LAST = None","    except Exception:","        JIT_KERNEL_SIGNATURES_LAST = None","","    # Convert to Fill objects (drop unused capacity)","    out: List[Fill] = []","    m = fills_arr.shape[0]","    for i in range(m):","        row = fills_arr[i]","        out.append(","            Fill(","                bar_index=int(row[0]),","                role=_role_from_int(int(row[1])),","                kind=_kind_from_int(int(row[2])),","                side=_side_from_int(int(row[3])),","                price=float(row[4]),","                qty=int(row[5]),","                order_id=int(row[6]),","            )","        )","    return out","","","def simulate_arrays(","    bars: BarArrays,","    *,","    order_id: np.ndarray,","    created_bar: np.ndarray,","    role: np.ndarray,","    kind: np.ndarray,","    side: np.ndarray,","    price: np.ndarray,","    qty: np.ndarray,","    ttl_bars: int = 1,",") -> List[Fill]:","    \"\"\"","    Array/SoA entry point: bypass OrderIntent objects and _pack_intents hot-path.","","    Arrays must be 1D and same length. Dtypes are expected (optimized):","      order_id: int32 (INDEX_DTYPE)","      created_bar: int32 (INDEX_DTYPE)","      role: uint8 (INTENT_ENUM_DTYPE)","      kind: uint8 (INTENT_ENUM_DTYPE)","      side: uint8 (INTENT_ENUM_DTYPE)","      price: float64 (INTENT_PRICE_DTYPE)","      qty: int32 (INDEX_DTYPE)","","    ttl_bars:","      - activate_bar = created_bar + 1","      - 0 => GTC (Good Till Canceled, never expire)","      - 1 => one-shot next-bar-only (intent valid only on activate_bar)","      - >= 1 => intent valid for bars t in [activate_bar, activate_bar + ttl_bars - 1]","      - When t > activate_bar + ttl_bars - 1, intent is removed from active book","    \"\"\"","    from config.dtypes import (","        INDEX_DTYPE,","        INTENT_ENUM_DTYPE,","        INTENT_PRICE_DTYPE,","    )","    ","    global JIT_PATH_USED_LAST, JIT_KERNEL_SIGNATURES_LAST","","    # Normalize/ensure arrays are numpy with the expected dtypes (cold path).","    oid = np.asarray(order_id, dtype=INDEX_DTYPE)","    cb = np.asarray(created_bar, dtype=INDEX_DTYPE)","    rl = np.asarray(role, dtype=INTENT_ENUM_DTYPE)","    kd = np.asarray(kind, dtype=INTENT_ENUM_DTYPE)","    sd = np.asarray(side, dtype=INTENT_ENUM_DTYPE)","    px = np.asarray(price, dtype=INTENT_PRICE_DTYPE)","    qy = np.asarray(qty, dtype=INDEX_DTYPE)","","    if nb is None:","        JIT_PATH_USED_LAST = False","        JIT_KERNEL_SIGNATURES_LAST = None","        intents: List[OrderIntent] = []","        n = int(oid.shape[0])","        for i in range(n):","            # Strict decoding: fail fast on invalid enum values","            rl_val = int(rl[i])","            if rl_val == ROLE_EXIT:","                r = OrderRole.EXIT","            elif rl_val == ROLE_ENTRY:","                r = OrderRole.ENTRY","            else:","                raise ValueError(f\"Invalid role enum value: {rl_val}. Allowed: {ROLE_EXIT} (EXIT) or {ROLE_ENTRY} (ENTRY)\")","            ","            kd_val = int(kd[i])","            if kd_val == KIND_STOP:","                k = OrderKind.STOP","            elif kd_val == KIND_LIMIT:","                k = OrderKind.LIMIT","            else:","                raise ValueError(f\"Invalid kind enum value: {kd_val}. Allowed: {KIND_STOP} (STOP) or {KIND_LIMIT} (LIMIT)\")","            ","            sd_val = int(sd[i])","            if sd_val == SIDE_BUY_CODE:  # 1","                s = Side.BUY","            elif sd_val == SIDE_SELL_CODE:  # 255","                s = Side.SELL","            else:","                raise ValueError(f\"Invalid side enum value: {sd_val}. Allowed: {SIDE_BUY_CODE} (BUY) or {SIDE_SELL_CODE} (SELL)\")","            intents.append(","                OrderIntent(","                    order_id=int(oid[i]),","                    created_bar=int(cb[i]),","                    role=r,","                    kind=k,","                    side=s,","                    price=float(px[i]),","                    qty=int(qy[i]),","                )","            )","        return simulate_py(bars, intents)","","    import os","","    if os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() == \"1\":","        JIT_PATH_USED_LAST = False","        JIT_KERNEL_SIGNATURES_LAST = None","        intents: List[OrderIntent] = []","        n = int(oid.shape[0])","        for i in range(n):","            # Strict decoding: fail fast on invalid enum values","            rl_val = int(rl[i])"]}
{"type":"file_chunk","path":"src/engine/engine_jit.py","chunk_index":2,"line_start":401,"line_end":600,"content":["            if rl_val == ROLE_EXIT:","                r = OrderRole.EXIT","            elif rl_val == ROLE_ENTRY:","                r = OrderRole.ENTRY","            else:","                raise ValueError(f\"Invalid role enum value: {rl_val}. Allowed: {ROLE_EXIT} (EXIT) or {ROLE_ENTRY} (ENTRY)\")","            ","            kd_val = int(kd[i])","            if kd_val == KIND_STOP:","                k = OrderKind.STOP","            elif kd_val == KIND_LIMIT:","                k = OrderKind.LIMIT","            else:","                raise ValueError(f\"Invalid kind enum value: {kd_val}. Allowed: {KIND_STOP} (STOP) or {KIND_LIMIT} (LIMIT)\")","            ","            sd_val = int(sd[i])","            if sd_val == SIDE_BUY_CODE:  # 1","                s = Side.BUY","            elif sd_val == SIDE_SELL_CODE:  # 255","                s = Side.SELL","            else:","                raise ValueError(f\"Invalid side enum value: {sd_val}. Allowed: {SIDE_BUY_CODE} (BUY) or {SIDE_SELL_CODE} (SELL)\")","            intents.append(","                OrderIntent(","                    order_id=int(oid[i]),","                    created_bar=int(cb[i]),","                    role=r,","                    kind=k,","                    side=s,","                    price=float(px[i]),","                    qty=int(qy[i]),","                )","            )","        return simulate_py(bars, intents)","","    packed = _sort_packed_by_created_bar((oid, cb, rl, kd, sd, px, qy))","    status, fills_arr = _simulate_kernel(","        bars.open,","        bars.high,","        bars.low,","        packed[0],","        packed[1],","        packed[2],","        packed[3],","        packed[4],","        packed[5],","        packed[6],","        np.int64(ttl_bars),","    )","    if int(status) != STATUS_OK:","        JIT_PATH_USED_LAST = True","        raise RuntimeError(f\"engine_jit kernel error: status={int(status)}\")","","    JIT_PATH_USED_LAST = True","    try:","        sigs = getattr(_simulate_kernel, \"signatures\", None)","        if sigs is not None:","            JIT_KERNEL_SIGNATURES_LAST = list(sigs)","        else:","            JIT_KERNEL_SIGNATURES_LAST = None","    except Exception:","        JIT_KERNEL_SIGNATURES_LAST = None","","    out: List[Fill] = []","    m = fills_arr.shape[0]","    for i in range(m):","        row = fills_arr[i]","        out.append(","            Fill(","                bar_index=int(row[0]),","                role=_role_from_int(int(row[1])),","                kind=_kind_from_int(int(row[2])),","                side=_side_from_int(int(row[3])),","                price=float(row[4]),","                qty=int(row[5]),","                order_id=int(row[6]),","            )","        )","    return out","","","def _simulate_with_ttl(bars: BarArrays, intents: Iterable[OrderIntent], ttl_bars: int) -> List[Fill]:","    \"\"\"","    Internal helper (tests/dev): run JIT matcher with a custom ttl_bars.","    ttl_bars=0 => GTC, ttl_bars=1 => one-shot next-bar-only (default).","    \"\"\"","    if nb is None:","        return simulate_py(bars, intents)","","    import os","","    if os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() == \"1\":","        return simulate_py(bars, intents)","","    packed = _sort_packed_by_created_bar(_pack_intents(intents))","    status, fills_arr = _simulate_kernel(","        bars.open,","        bars.high,","        bars.low,","        packed[0],","        packed[1],","        packed[2],","        packed[3],","        packed[4],","        packed[5],","        packed[6],","        np.int64(ttl_bars),","    )","    if int(status) == STATUS_BUFFER_FULL:","        raise RuntimeError(","            f\"engine_jit kernel buffer full: fills exceeded capacity. \"","            f\"Consider reducing intents or increasing buffer size.\"","        )","    if int(status) != STATUS_OK:","        raise RuntimeError(f\"engine_jit kernel error: status={int(status)}\")","","    out: List[Fill] = []","    m = fills_arr.shape[0]","    for i in range(m):","        row = fills_arr[i]","        out.append(","            Fill(","                bar_index=int(row[0]),","                role=_role_from_int(int(row[1])),","                kind=_kind_from_int(int(row[2])),","                side=_side_from_int(int(row[3])),","                price=float(row[4]),","                qty=int(row[5]),","                order_id=int(row[6]),","            )","        )","    return out","","","# ----------------------------","# Numba Kernel","# ----------------------------","","if nb is not None:","","    @nb.njit(cache=False)","    def _stop_fill(side: int, stop_price: float, o: float, h: float, l: float) -> float:","        # returns nan if no fill","        if side == 1:  # BUY","            if o >= stop_price:","                return o","            if h >= stop_price:","                return stop_price","            return np.nan","        else:  # SELL","            if o <= stop_price:","                return o","            if l <= stop_price:","                return stop_price","            return np.nan","","    @nb.njit(cache=False)","    def _limit_fill(side: int, limit_price: float, o: float, h: float, l: float) -> float:","        # returns nan if no fill","        if side == 1:  # BUY","            if o <= limit_price:","                return o","            if l <= limit_price:","                return limit_price","            return np.nan","        else:  # SELL","            if o >= limit_price:","                return o","            if h >= limit_price:","                return limit_price","            return np.nan","","    @nb.njit(cache=False)","    def _fill_price(kind: int, side: int, px: float, o: float, h: float, l: float) -> float:","        # kind: 0=STOP, 1=LIMIT","        if kind == 0:","            return _stop_fill(side, px, o, h, l)","        return _limit_fill(side, px, o, h, l)","","    @nb.njit(cache=False)","    def _simulate_kernel(","        open_: np.ndarray,","        high: np.ndarray,","        low: np.ndarray,","        order_id: np.ndarray,","        created_bar: np.ndarray,","        role: np.ndarray,","        kind: np.ndarray,","        side: np.ndarray,","        price: np.ndarray,","        qty: np.ndarray,","        ttl_bars: np.int64,","    ):","        \"\"\"","        Cursor + Active Book kernel (O(B + I + A)).","","        Output columns (float64):","          0 bar_index","          1 role_int (0=EXIT,1=ENTRY)","          2 kind_int (0=STOP,1=LIMIT)"]}
{"type":"file_chunk","path":"src/engine/engine_jit.py","chunk_index":3,"line_start":601,"line_end":793,"content":["          3 side_int (1=BUY,-1=SELL)","          4 fill_price","          5 qty","          6 order_id","","        Assumption:","          - intents are sorted by (created_bar, order_id) before calling this kernel.","","        TTL Semantics (ttl_bars):","          - activate_bar = created_bar + 1","          - ttl_bars == 0: GTC (Good Till Canceled, never expire)","          - ttl_bars >= 1: intent is valid for bars t in [activate_bar, activate_bar + ttl_bars - 1]","          - When t > activate_bar + ttl_bars - 1, intent is removed from active book (even if not filled)","          - ttl_bars == 1: one-shot next-bar-only (intent valid only on activate_bar)","        \"\"\"","        n_bars = open_.shape[0]","        n_intents = order_id.shape[0]","","        # Buffer size must accommodate at least n_intents (each intent can produce a fill)","        # Default heuristic: n_bars * 2 (allows 2 fills per bar on average)","        max_fills = n_bars * 2","        if n_intents > max_fills:","            max_fills = n_intents","        ","        out = np.empty((max_fills, 7), dtype=np.float64)","        out_n = 0","","        # -------------------------","        # Fail-fast monotonicity check (activate_bar, order_id)","        # -------------------------","        prev_activate = np.int64(-1)","        prev_order = np.int64(-1)","        for i in range(n_intents):","            a = np.int64(created_bar[i]) + np.int64(1)","            o = np.int64(order_id[i])","            if a < prev_activate or (a == prev_activate and o < prev_order):","                return np.int64(STATUS_ERROR_UNSORTED), out[:0]","            prev_activate = a","            prev_order = o","","        # Active Book (indices into intent arrays)","        active_indices = np.empty(n_intents, dtype=np.int64)","        active_count = np.int64(0)","        global_cursor = np.int64(0)","","        pos = np.int64(0)  # 0 flat, 1 long, -1 short","","        for t in range(n_bars):","            o = float(open_[t])","            h = float(high[t])","            l = float(low[t])","","            # Step A — Injection (cursor inject intents activating at this bar)","            while global_cursor < n_intents:","                a = np.int64(created_bar[global_cursor]) + np.int64(1)","                if a == np.int64(t):","                    active_indices[active_count] = global_cursor","                    active_count += np.int64(1)","                    global_cursor += np.int64(1)","                    continue","                if a > np.int64(t):","                    break","                # a < t should not happen if monotonicity check passed","                return np.int64(STATUS_ERROR_UNSORTED), out[:0]","","            # Step A.5 — Prune expired intents (TTL/GTC extension point)","            # Remove intents that have expired before processing Step B/C.","            # Contract: activate_bar = created_bar + 1","            #   - ttl_bars == 0: GTC (never expire)","            #   - ttl_bars >= 1: valid bars are t in [activate_bar, activate_bar + ttl_bars - 1]","            #   - When t > activate_bar + ttl_bars - 1, intent must be removed","            if ttl_bars > np.int64(0) and active_count > 0:","                k = np.int64(0)","                while k < active_count:","                    idx = active_indices[k]","                    activate_bar = np.int64(created_bar[idx]) + np.int64(1)","                    expire_bar = activate_bar + (ttl_bars - np.int64(1))","                    if np.int64(t) > expire_bar:","                        # swap-remove expired intent","                        active_indices[k] = active_indices[active_count - 1]","                        active_count -= np.int64(1)","                        continue","                    k += np.int64(1)","","            # Step B — Pass 1 (ENTRY scan, best-pick, swap-remove)","            # Deterministic selection: STOP(0) before LIMIT(1), then order_id asc.","            if pos == 0 and active_count > 0:","                best_k = np.int64(-1)","                best_kind = np.int64(99)","                best_oid = np.int64(2**62)","                best_fp = np.nan","","                k = np.int64(0)","                while k < active_count:","                    idx = active_indices[k]","                    if np.int64(role[idx]) != np.int64(1):  # ENTRY","                        k += np.int64(1)","                        continue","","                    kk = np.int64(kind[idx])","                    oo = np.int64(order_id[idx])","                    if kk < best_kind or (kk == best_kind and oo < best_oid):","                        fp = _fill_price(int(kk), int(side[idx]), float(price[idx]), o, h, l)","                        if not np.isnan(fp):","                            best_k = k","                            best_kind = kk","                            best_oid = oo","                            best_fp = fp","                    k += np.int64(1)","","                if best_k != np.int64(-1):","                    # Buffer protection: check before writing","                    if out_n >= max_fills:","                        return np.int64(STATUS_BUFFER_FULL), out[:out_n]","                    ","                    idx = active_indices[best_k]","                    out[out_n, 0] = float(t)","                    out[out_n, 1] = float(role[idx])","                    out[out_n, 2] = float(kind[idx])","                    out[out_n, 3] = float(side[idx])","                    out[out_n, 4] = float(best_fp)","                    out[out_n, 5] = float(qty[idx])","                    out[out_n, 6] = float(order_id[idx])","                    out_n += 1","","                    pos = np.int64(1) if np.int64(side[idx]) == np.int64(1) else np.int64(-1)","","                    # swap-remove filled intent","                    active_indices[best_k] = active_indices[active_count - 1]","                    active_count -= np.int64(1)","","            # Step C — Pass 2 (EXIT scan, best-pick, swap-remove)","            # Deterministic selection: STOP(0) before LIMIT(1), then order_id asc.","            if pos != 0 and active_count > 0:","                best_k = np.int64(-1)","                best_kind = np.int64(99)","                best_oid = np.int64(2**62)","                best_fp = np.nan","","                k = np.int64(0)","                while k < active_count:","                    idx = active_indices[k]","                    if np.int64(role[idx]) != np.int64(0):  # EXIT","                        k += np.int64(1)","                        continue","","                    s = np.int64(side[idx])","                    # side encoding: 1=BUY, 255=SELL -> convert to sign: 1=BUY, -1=SELL","                    side_sign = np.int64(1) if s == np.int64(1) else np.int64(-1)","                    # long exits are SELL(-1), short exits are BUY(1)","                    if pos == np.int64(1) and side_sign != np.int64(-1):","                        k += np.int64(1)","                        continue","                    if pos == np.int64(-1) and side_sign != np.int64(1):","                        k += np.int64(1)","                        continue","","                    kk = np.int64(kind[idx])","                    oo = np.int64(order_id[idx])","                    if kk < best_kind or (kk == best_kind and oo < best_oid):","                        fp = _fill_price(int(kk), int(s), float(price[idx]), o, h, l)","                        if not np.isnan(fp):","                            best_k = k","                            best_kind = kk","                            best_oid = oo","                            best_fp = fp","                    k += np.int64(1)","","                if best_k != np.int64(-1):","                    # Buffer protection: check before writing","                    if out_n >= max_fills:","                        return np.int64(STATUS_BUFFER_FULL), out[:out_n]","                    ","                    idx = active_indices[best_k]","                    out[out_n, 0] = float(t)","                    out[out_n, 1] = float(role[idx])","                    out[out_n, 2] = float(kind[idx])","                    out[out_n, 3] = float(side[idx])","                    out[out_n, 4] = float(best_fp)","                    out[out_n, 5] = float(qty[idx])","                    out[out_n, 6] = float(order_id[idx])","                    out_n += 1","","                    pos = np.int64(0)","","                    # swap-remove filled intent","                    active_indices[best_k] = active_indices[active_count - 1]","                    active_count -= np.int64(1)","","        return np.int64(STATUS_OK), out[:out_n]","","",""]}
{"type":"file_footer","path":"src/engine/engine_jit.py","complete":true,"emitted_chunks":4}
{"type":"file_header","path":"src/engine/kernels/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":48,"sha256":"7c9d7bf1296eca2685fe88fe3df4402232687067c44cd735aa99ba9c2c33b73d","total_lines":4,"chunk_count":1}
{"type":"file_chunk","path":"src/engine/kernels/__init__.py","chunk_index":0,"line_start":1,"line_end":4,"content":["","\"\"\"Kernel implementations for simulation.\"\"\"","",""]}
{"type":"file_footer","path":"src/engine/kernels/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/engine/kernels/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":208,"sha256":"ef0a848eb3c944f9283ccb63cdeb3224302c087bc183a8541ef6b67e207b15b2","note":"skipped by policy"}
{"type":"file_skipped","path":"src/engine/kernels/__pycache__/cursor_kernel.cpython-312.pyc","reason":"cache","bytes":1511,"sha256":"4dd3f05dd70e609bfbe26edc0c638af2ebbcd985d7aef332e20b27a01c374530","note":"skipped by policy"}
{"type":"file_skipped","path":"src/engine/kernels/__pycache__/reference_kernel.cpython-312.pyc","reason":"cache","bytes":1652,"sha256":"5cd3585fa6b7fbc74513a5bae8cbab3b267efde89ec13d0c82bd93edd78dd0fa","note":"skipped by policy"}
{"type":"file_header","path":"src/engine/kernels/cursor_kernel.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1137,"sha256":"1b0fd6263b6b564505c99fb3cfada0d1206ebe293e9068d484200473fe261154","total_lines":41,"chunk_count":1}
{"type":"file_chunk","path":"src/engine/kernels/cursor_kernel.py","chunk_index":0,"line_start":1,"line_end":41,"content":["","\"\"\"Cursor kernel - main simulation path for Phase 4.","","This is the primary kernel implementation, optimized for performance.","It uses array/struct inputs and deterministic cursor-based matching.","\"\"\"","","from __future__ import annotations","","from typing import Iterable, List","","from engine.types import BarArrays, Fill, OrderIntent, SimResult","from engine.engine_jit import simulate as simulate_jit","","","def simulate_cursor_kernel(","    bars: BarArrays,","    intents: Iterable[OrderIntent],",") -> SimResult:","    \"\"\"","    Cursor kernel - main simulation path.","    ","    This is the primary kernel for Phase 4. It uses the optimized JIT implementation","    from engine_jit, which provides O(B + I + A) complexity.","    ","    Args:","        bars: OHLC bar arrays","        intents: Iterable of order intents","        ","    Returns:","        SimResult containing the fills from simulation","        ","    Note:","        - Uses arrays/structs internally, no class callbacks","        - Naming and fields are stable for pipeline usage","        - Deterministic behavior guaranteed","    \"\"\"","    fills: List[Fill] = simulate_jit(bars, intents)","    return SimResult(fills=fills)","",""]}
{"type":"file_footer","path":"src/engine/kernels/cursor_kernel.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/engine/kernels/reference_kernel.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1278,"sha256":"aebc595ae8f48ec9bb3cd1825feb54168f834a7d3b8f5412fc97b820f989da69","total_lines":44,"chunk_count":1}
{"type":"file_chunk","path":"src/engine/kernels/reference_kernel.py","chunk_index":0,"line_start":1,"line_end":44,"content":["","\"\"\"Reference kernel - adapter for matcher_core (testing/debugging only).","","This kernel wraps matcher_core.simulate() and should only be used for:","- Testing alignment between kernels","- Debugging semantic correctness","- Reference implementation verification","","It is NOT the main path for production simulation.","\"\"\"","","from __future__ import annotations","","from typing import Iterable, List","","from engine.types import BarArrays, Fill, OrderIntent, SimResult","from engine.matcher_core import simulate as simulate_reference","","","def simulate_reference_matcher(","    bars: BarArrays,","    intents: Iterable[OrderIntent],",") -> SimResult:","    \"\"\"","    Reference matcher adapter - wraps matcher_core.simulate().","    ","    This is an adapter that wraps the reference implementation in matcher_core.","    It should only be used for testing/debugging, not as the main simulation path.","    ","    Args:","        bars: OHLC bar arrays","        intents: Iterable of order intents","        ","    Returns:","        SimResult containing the fills from simulation","        ","    Note:","        - This wraps matcher_core.simulate() which is the semantic truth source","        - Use only for tests/debug, not for production","    \"\"\"","    fills: List[Fill] = simulate_reference(bars, intents)","    return SimResult(fills=fills)","",""]}
{"type":"file_footer","path":"src/engine/kernels/reference_kernel.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/engine/matcher_core.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5446,"sha256":"75356f9b6ec9533515cf49fe13b20853ae044393c1912e852fd44d48d6a5e890","total_lines":179,"chunk_count":1}
{"type":"file_chunk","path":"src/engine/matcher_core.py","chunk_index":0,"line_start":1,"line_end":179,"content":["","from __future__ import annotations","","from dataclasses import dataclass","from typing import Iterable, List, Optional, Tuple","","import numpy as np","","from engine.types import (","    BarArrays,","    Fill,","    OrderIntent,","    OrderKind,","    OrderRole,","    Side,",")","","","@dataclass","class PositionState:","    \"\"\"","    Minimal single-position state for Phase 1 tests.","    pos: 0 = flat, 1 = long, -1 = short","    \"\"\"","    pos: int = 0","","","def _is_active(intent: OrderIntent, bar_index: int) -> bool:","    return bar_index == intent.created_bar + 1","","","def _stop_fill_price(side: Side, stop_price: float, o: float, h: float, l: float) -> Optional[float]:","    # Open==price goes to GAP branch by definition.","    if side == Side.BUY:","        if o >= stop_price:","            return o","        if h >= stop_price:","            return stop_price","        return None","    else:","        if o <= stop_price:","            return o","        if l <= stop_price:","            return stop_price","        return None","","","def _limit_fill_price(side: Side, limit_price: float, o: float, h: float, l: float) -> Optional[float]:","    # Open==price goes to GAP branch by definition.","    if side == Side.BUY:","        if o <= limit_price:","            return o","        if l <= limit_price:","            return limit_price","        return None","    else:","        if o >= limit_price:","            return o","        if h >= limit_price:","            return limit_price","        return None","","","def _intent_fill_price(intent: OrderIntent, o: float, h: float, l: float) -> Optional[float]:","    if intent.kind == OrderKind.STOP:","        return _stop_fill_price(intent.side, intent.price, o, h, l)","    return _limit_fill_price(intent.side, intent.price, o, h, l)","","","def _sort_key(intent: OrderIntent) -> Tuple[int, int, int]:","    \"\"\"","    Deterministic priority:","    1) Role: EXIT first when selecting within same-stage bucket.","    2) Kind: STOP before LIMIT.","    3) order_id: ascending.","    Note: Entry-vs-Exit ordering is handled at a higher level (Entry then Exit).","    \"\"\"","    role_rank = 0 if intent.role == OrderRole.EXIT else 1","    kind_rank = 0 if intent.kind == OrderKind.STOP else 1","    return (role_rank, kind_rank, intent.order_id)","","","def simulate(","    bars: BarArrays,","    intents: Iterable[OrderIntent],",") -> List[Fill]:","    \"\"\"","    Phase 1 slow reference matcher.","","    Rules enforced:","    - next-bar active only (bar_index == created_bar + 1)","    - STOP/LIMIT gap behavior at Open","    - STOP over LIMIT","    - Same-bar Entry then Exit","    - Same-kind tie: EXIT-first, order_id ascending","    \"\"\"","    o = bars.open","    h = bars.high","    l = bars.low","    n = int(o.shape[0])","","    intents_list = list(intents)","    fills: List[Fill] = []","    state = PositionState(pos=0)","","    for t in range(n):","        ot = float(o[t])","        ht = float(h[t])","        lt = float(l[t])","","        active = [x for x in intents_list if _is_active(x, t)]","        if not active:","            continue","","        # Partition by role for same-bar entry then exit.","        entry_intents = [x for x in active if x.role == OrderRole.ENTRY]","        exit_intents = [x for x in active if x.role == OrderRole.EXIT]","","        # Stage 1: ENTRY stage","        if entry_intents:","            # Among entries: STOP before LIMIT, then order_id.","            entry_sorted = sorted(entry_intents, key=lambda x: (0 if x.kind == OrderKind.STOP else 1, x.order_id))","            for it in entry_sorted:","                if state.pos != 0:","                    break  # single-position only","                px = _intent_fill_price(it, ot, ht, lt)","                if px is None:","                    continue","                fills.append(","                    Fill(","                        bar_index=t,","                        role=it.role,","                        kind=it.kind,","                        side=it.side,","                        price=float(px),","                        qty=int(it.qty),","                        order_id=int(it.order_id),","                    )","                )","                # Apply position change","                if it.side == Side.BUY:","                    state.pos = 1","                else:","                    state.pos = -1","                break  # at most one entry fill per bar in Phase 1 reference","","        # Stage 2: EXIT stage (after entry)","        if exit_intents and state.pos != 0:","            # Same-kind tie rule: EXIT-first already, and STOP before LIMIT, then order_id","            exit_sorted = sorted(exit_intents, key=_sort_key)","            for it in exit_sorted:","                # Only allow exits that reduce/close current position in this minimal model:","                # long exits are SELL, short exits are BUY.","                if state.pos == 1 and it.side != Side.SELL:","                    continue","                if state.pos == -1 and it.side != Side.BUY:","                    continue","","                px = _intent_fill_price(it, ot, ht, lt)","                if px is None:","                    continue","                fills.append(","                    Fill(","                        bar_index=t,","                        role=it.role,","                        kind=it.kind,","                        side=it.side,","                        price=float(px),","                        qty=int(it.qty),","                        order_id=int(it.order_id),","                    )","                )","                state.pos = 0","                break  # at most one exit fill per bar in Phase 1 reference","","    return fills","","",""]}
{"type":"file_footer","path":"src/engine/matcher_core.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/engine/metrics_from_fills.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2943,"sha256":"5b91212d43e6cdf707b0cd87f0aebaf1e209022aab9f67a3fb51853423ed4e30","total_lines":86,"chunk_count":1}
{"type":"file_chunk","path":"src/engine/metrics_from_fills.py","chunk_index":0,"line_start":1,"line_end":86,"content":["","from __future__ import annotations","","from typing import List, Tuple","","import numpy as np","","from engine.types import Fill, OrderRole, Side","","","def _max_drawdown(equity: np.ndarray) -> float:","    \"\"\"","    Vectorized max drawdown on an equity curve.","    Handles empty arrays gracefully.","    \"\"\"","    if equity.size == 0:","        return 0.0","    peak = np.maximum.accumulate(equity)","    dd = equity - peak","    mdd = float(np.min(dd))  # negative or 0","    return mdd","","","def compute_metrics_from_fills(","    fills: List[Fill],","    commission: float,","    slip: float,","    qty: int,",") -> Tuple[float, int, float, np.ndarray]:","    \"\"\"","    Compute metrics from fills list.","    ","    This is the unified source of truth for metrics computation from fills.","    Both object-mode and array-mode kernels should use this helper to ensure parity.","    ","    Args:","        fills: List of Fill objects (can be empty)","        commission: Commission cost per trade (absolute)","        slip: Slippage cost per trade (absolute)","        qty: Order quantity (used for PnL calculation)","    ","    Returns:","        Tuple of (net_profit, trades, max_dd, equity):","            - net_profit: float - Total net profit (sum of all round-trip PnL)","            - trades: int - Number of trades (equals pnl.size, not entry fills count)","            - max_dd: float - Maximum drawdown from equity curve","            - equity: np.ndarray - Cumulative equity curve (cumsum of per-trade PnL)","    ","    Note:","        - trades is defined as pnl.size (number of completed round-trip trades)","        - Only LONG trades are supported (BUY entry, SELL exit)","        - Costs are applied per fill (entry + exit each incur cost)","        - Metrics are derived from pnl/equity, not from fills count","    \"\"\"","    # Extract entry/exit prices for round trips","    # Pairing rule: take fills in chronological order, pair BUY(ENTRY) then SELL(EXIT)","    entry_prices = []","    exit_prices = []","    for f in fills:","        if f.role == OrderRole.ENTRY and f.side == Side.BUY:","            entry_prices.append(float(f.price))","        elif f.role == OrderRole.EXIT and f.side == Side.SELL:","            exit_prices.append(float(f.price))","    ","    # Match entry/exit pairs (take minimum to handle unpaired entries)","    k = min(len(entry_prices), len(exit_prices))","    if k == 0:","        # No complete round trips: no pnl, so trades = 0","        return (0.0, 0, 0.0, np.empty(0, dtype=np.float64))","    ","    ep = np.asarray(entry_prices[:k], dtype=np.float64)","    xp = np.asarray(exit_prices[:k], dtype=np.float64)","    ","    # Costs applied per fill (entry + exit)","    costs = (float(commission) + float(slip)) * 2.0","    pnl = (xp - ep) * float(qty) - costs","    equity = np.cumsum(pnl)","    ","    # CURSOR TASK 1: trades must equal pnl.size (Source of Truth)","    trades = int(pnl.size)","    net_profit = float(np.sum(pnl)) if pnl.size else 0.0","    max_dd = _max_drawdown(equity)","    ","    return (net_profit, trades, max_dd, equity)","",""]}
{"type":"file_footer","path":"src/engine/metrics_from_fills.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/engine/order_id.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3797,"sha256":"92d0c6509bbf3c919a421d1eb248722d9f0e78aaa627e070f4a1852470706779","total_lines":117,"chunk_count":1}
{"type":"file_chunk","path":"src/engine/order_id.py","chunk_index":0,"line_start":1,"line_end":117,"content":["","\"\"\"","Deterministic Order ID Generation (CURSOR TASK 5)","","Provides pure function for generating deterministic order IDs that do not depend","on generation order or counters. Used by both object-mode and array-mode kernels.","\"\"\"","from __future__ import annotations","","import numpy as np","","from config.dtypes import INDEX_DTYPE","from engine.constants import KIND_STOP, ROLE_ENTRY, ROLE_EXIT, SIDE_BUY, SIDE_SELL","","","def generate_order_id(","    created_bar: int,","    param_idx: int = 0,","    role: int = ROLE_ENTRY,","    kind: int = KIND_STOP,","    side: int = SIDE_BUY,",") -> int:","    \"\"\"","    Generate deterministic order ID from intent attributes.","    ","    Uses reversible packing to ensure deterministic IDs that do not depend on","    generation order or counters. This ensures parity between object-mode and","    array-mode kernels.","    ","    Formula:","        order_id = created_bar * 1_000_000 + param_idx * 100 + role_code * 10 + kind_code * 2 + side_code_bit","    ","    Args:","        created_bar: Bar index where intent is created (0-indexed)","        param_idx: Parameter index (0-indexed, default 0 for single-param kernels)","        role: Role code (ROLE_ENTRY or ROLE_EXIT)","        kind: Kind code (KIND_STOP or KIND_LIMIT)","        side: Side code (SIDE_BUY or SIDE_SELL)","    ","    Returns:","        Deterministic order ID (int32)","    ","    Note:","        - Maximum created_bar: 2,147,483 (within int32 range)","        - Maximum param_idx: 21,474,836 (within int32 range)","        - This packing scheme ensures uniqueness for typical use cases","    \"\"\"","    # Map role to code: ENTRY=0, EXIT=1","    role_code = 0 if role == ROLE_ENTRY else 1","    ","    # Map kind to code: STOP=0, LIMIT=1 (assuming KIND_STOP=0, KIND_LIMIT=1)","    kind_code = 0 if kind == KIND_STOP else 1","    ","    # Map side to bit: BUY=0, SELL=1","    side_bit = 0 if side == SIDE_BUY else 1","    ","    # Pack: created_bar * 1_000_000 + param_idx * 100 + role_code * 10 + kind_code * 2 + side_bit","    order_id = (","        created_bar * 1_000_000 +","        param_idx * 100 +","        role_code * 10 +","        kind_code * 2 +","        side_bit","    )","    ","    return int(order_id)","","","def generate_order_ids_array(","    created_bar: np.ndarray,","    param_idx: int = 0,","    role: np.ndarray | None = None,","    kind: np.ndarray | None = None,","    side: np.ndarray | None = None,",") -> np.ndarray:","    \"\"\"","    Generate deterministic order IDs for array of intents.","    ","    Vectorized version of generate_order_id for array-mode kernels.","    ","    Args:","        created_bar: Array of created bar indices (int32, shape (n,))","        param_idx: Parameter index (default 0 for single-param kernels)","        role: Array of role codes (uint8, shape (n,)). If None, defaults to ROLE_ENTRY.","        kind: Array of kind codes (uint8, shape (n,)). If None, defaults to KIND_STOP.","        side: Array of side codes (uint8, shape (n,)). If None, defaults to SIDE_BUY.","    ","    Returns:","        Array of deterministic order IDs (int32, shape (n,))","    \"\"\"","    n = len(created_bar)","    ","    # Default values if not provided","    if role is None:","        role = np.full(n, ROLE_ENTRY, dtype=np.uint8)","    if kind is None:","        kind = np.full(n, KIND_STOP, dtype=np.uint8)","    if side is None:","        side = np.full(n, SIDE_BUY, dtype=np.uint8)","    ","    # Map to codes","    role_code = np.where(role == ROLE_ENTRY, 0, 1).astype(np.int32)","    kind_code = np.where(kind == KIND_STOP, 0, 1).astype(np.int32)","    side_bit = np.where(side == SIDE_BUY, 0, 1).astype(np.int32)","    ","    # Pack: created_bar * 1_000_000 + param_idx * 100 + role_code * 10 + kind_code * 2 + side_bit","    order_id = (","        created_bar.astype(np.int32) * 1_000_000 +","        param_idx * 100 +","        role_code * 10 +","        kind_code * 2 +","        side_bit","    )","    ","    return order_id.astype(INDEX_DTYPE)","",""]}
{"type":"file_footer","path":"src/engine/order_id.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/engine/signal_exporter.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5625,"sha256":"2e3c6e2160dba76e49f04d3734421b69c920ee65cb86b749b16e1c8f95de7bb2","total_lines":146,"chunk_count":1}
{"type":"file_chunk","path":"src/engine/signal_exporter.py","chunk_index":0,"line_start":1,"line_end":146,"content":["\"\"\"Signal series exporter for bar-based position, margin, and notional in base currency.\"\"\"","","import pandas as pd","import numpy as np","from typing import Optional","","REQUIRED_COLUMNS = [","    \"ts\",","    \"instrument\",","    \"close\",","    \"position_contracts\",","    \"currency\",","    \"fx_to_base\",","    \"close_base\",","    \"multiplier\",","    \"initial_margin_per_contract\",","    \"maintenance_margin_per_contract\",","    \"notional_base\",","    \"margin_initial_base\",","    \"margin_maintenance_base\",","]","","","def build_signal_series_v1(","    *,","    instrument: str,","    bars_df: pd.DataFrame,   # cols: ts, close (ts sorted asc)","    fills_df: pd.DataFrame,  # cols: ts, qty (contracts signed)","    timeframe: str,","    tz: str,","    base_currency: str,","    instrument_currency: str,","    fx_to_base: float,","    multiplier: float,","    initial_margin_per_contract: float,","    maintenance_margin_per_contract: float,",") -> pd.DataFrame:","    \"\"\"","    Build signal series V1 DataFrame from bars and fills.","    ","    Args:","        instrument: Instrument identifier (e.g., \"CME.MNQ\")","        bars_df: DataFrame with columns ['ts', 'close']; must be sorted ascending by ts","        fills_df: DataFrame with columns ['ts', 'qty']; qty is signed contracts (+ for buy, - for sell)","        timeframe: Bar timeframe (e.g., \"5min\")","        tz: Timezone string (e.g., \"UTC\")","        base_currency: Base currency code (e.g., \"TWD\")","        instrument_currency: Instrument currency code (e.g., \"USD\")","        fx_to_base: FX rate from instrument currency to base currency","        multiplier: Contract multiplier","        initial_margin_per_contract: Initial margin per contract in instrument currency","        maintenance_margin_per_contract: Maintenance margin per contract in instrument currency","        ","    Returns:","        DataFrame with REQUIRED_COLUMNS, one row per bar, sorted by ts.","        ","    Raises:","        ValueError: If input DataFrames are empty or missing required columns","        AssertionError: If bars_df is not sorted ascending","    \"\"\"","    # Validate inputs","    if bars_df.empty:","        raise ValueError(\"bars_df cannot be empty\")","    if \"ts\" not in bars_df.columns or \"close\" not in bars_df.columns:","        raise ValueError(\"bars_df must have columns ['ts', 'close']\")","    if \"ts\" not in fills_df.columns or \"qty\" not in fills_df.columns:","        raise ValueError(\"fills_df must have columns ['ts', 'qty']\")","    ","    # Ensure bars are sorted ascending","    if not bars_df[\"ts\"].is_monotonic_increasing:","        bars_df = bars_df.sort_values(\"ts\").reset_index(drop=True)","    ","    # Prepare bars DataFrame as base","    result = bars_df[[\"ts\", \"close\"]].copy()","    result[\"instrument\"] = instrument","    ","    # If no fills, position is zero for all bars","    if fills_df.empty:","        result[\"position_contracts\"] = 0.0","    else:","        # Ensure fills are sorted by ts","        fills_sorted = fills_df.sort_values(\"ts\").reset_index(drop=True)","        ","        # Merge fills to bars using merge_asof to align fill ts to bar ts","        # direction='backward' assigns fill to the nearest bar with ts <= fill_ts","        # We need to merge on ts, but we want to get the bar ts for each fill","        merged = pd.merge_asof(","            fills_sorted,","            result[[\"ts\"]].rename(columns={\"ts\": \"bar_ts\"}),","            left_on=\"ts\",","            right_on=\"bar_ts\",","            direction=\"backward\"","        )","        ","        # Group by bar_ts and sum qty","        fills_per_bar = merged.groupby(\"bar_ts\")[\"qty\"].sum().reset_index()","        fills_per_bar = fills_per_bar.rename(columns={\"bar_ts\": \"ts\", \"qty\": \"fill_qty\"})","        ","        # Merge fills back to bars","        result = pd.merge(result, fills_per_bar, on=\"ts\", how=\"left\")","        result[\"fill_qty\"] = result[\"fill_qty\"].fillna(0.0)","        ","        # Cumulative sum of fills to get position","        result[\"position_contracts\"] = result[\"fill_qty\"].cumsum()","    ","    # Add currency and FX columns","    result[\"currency\"] = instrument_currency","    result[\"fx_to_base\"] = fx_to_base","    ","    # Calculate close in base currency","    result[\"close_base\"] = result[\"close\"] * fx_to_base","    ","    # Add contract specs","    result[\"multiplier\"] = multiplier","    result[\"initial_margin_per_contract\"] = initial_margin_per_contract","    result[\"maintenance_margin_per_contract\"] = maintenance_margin_per_contract","    ","    # Calculate notional and margins in base currency","    # notional_base = position_contracts * close_base * multiplier","    result[\"notional_base\"] = result[\"position_contracts\"] * result[\"close_base\"] * multiplier","    ","    # margin_initial_base = abs(position_contracts) * initial_margin_per_contract * fx_to_base","    result[\"margin_initial_base\"] = (","        abs(result[\"position_contracts\"]) * initial_margin_per_contract * fx_to_base","    )","    ","    # margin_maintenance_base = abs(position_contracts) * maintenance_margin_per_contract * fx_to_base","    result[\"margin_maintenance_base\"] = (","        abs(result[\"position_contracts\"]) * maintenance_margin_per_contract * fx_to_base","    )","    ","    # Ensure all required columns are present and in correct order","    for col in REQUIRED_COLUMNS:","        if col not in result.columns:","            raise RuntimeError(f\"Missing column {col} in result\")","    ","    # Reorder columns","    result = result[REQUIRED_COLUMNS]","    ","    # Ensure no NaN values (except maybe where close is NaN, but that shouldn't happen)","    if result.isna().any().any():","        # Fill numeric NaNs with 0 where appropriate","        numeric_cols = result.select_dtypes(include=[np.number]).columns","        result[numeric_cols] = result[numeric_cols].fillna(0.0)","    ","    return result"]}
{"type":"file_footer","path":"src/engine/signal_exporter.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/engine/simulate.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1484,"sha256":"0af16343eaa8884502910d54c1db2472b170cace8bf80e8227d7cf51f6d33744","total_lines":48,"chunk_count":1}
{"type":"file_chunk","path":"src/engine/simulate.py","chunk_index":0,"line_start":1,"line_end":48,"content":["","\"\"\"Unified simulate entry point for Phase 4.","","This module provides the single entry point simulate_run() which routes to","the Cursor kernel (main path) or Reference kernel (testing/debugging only).","\"\"\"","","from __future__ import annotations","","from typing import Iterable","","from engine.types import BarArrays, OrderIntent, SimResult","from engine.kernels.cursor_kernel import simulate_cursor_kernel","from engine.kernels.reference_kernel import simulate_reference_matcher","","","def simulate_run(","    bars: BarArrays,","    intents: Iterable[OrderIntent],","    *,","    use_reference: bool = False,",") -> SimResult:","    \"\"\"","    Unified simulate entry point - Phase 4 main API.","    ","    This is the single entry point for all simulation calls. By default, it uses","    the Cursor kernel (main path). The Reference kernel is only available for","    testing/debugging purposes.","    ","    Args:","        bars: OHLC bar arrays","        intents: Iterable of order intents","        use_reference: If True, use reference kernel (testing/debug only).","                      Default False uses Cursor kernel (main path).","        ","    Returns:","        SimResult containing the fills from simulation","        ","    Note:","        - Cursor kernel is the main path for production","        - Reference kernel should only be used for tests/debug","        - This API is stable for pipeline usage","    \"\"\"","    if use_reference:","        return simulate_reference_matcher(bars, intents)","    return simulate_cursor_kernel(bars, intents)","",""]}
{"type":"file_footer","path":"src/engine/simulate.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/engine/types.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1189,"sha256":"0790de1e66b121d4c1a3a682fcef2b469227a0f4e2b45fd89afed49e49f398ab","total_lines":70,"chunk_count":1}
{"type":"file_chunk","path":"src/engine/types.py","chunk_index":0,"line_start":1,"line_end":70,"content":["","from __future__ import annotations","","from dataclasses import dataclass","from enum import Enum","from typing import List, Optional","","import numpy as np","","","@dataclass(frozen=True)","class BarArrays:","    open: np.ndarray","    high: np.ndarray","    low: np.ndarray","    close: np.ndarray","","","class Side(int, Enum):","    BUY = 1","    SELL = -1","","","class OrderKind(str, Enum):","    STOP = \"STOP\"","    LIMIT = \"LIMIT\"","","","class OrderRole(str, Enum):","    ENTRY = \"ENTRY\"","    EXIT = \"EXIT\"","","","@dataclass(frozen=True)","class OrderIntent:","    \"\"\"","    Order intent created at bar `created_bar` and becomes active at bar `created_bar + 1`.","    Deterministic ordering is controlled via `order_id` (smaller = earlier).","    \"\"\"","    order_id: int","    created_bar: int","    role: OrderRole","    kind: OrderKind","    side: Side","    price: float","    qty: int = 1","","","@dataclass(frozen=True)","class Fill:","    bar_index: int","    role: OrderRole","    kind: OrderKind","    side: Side","    price: float","    qty: int","    order_id: int","","","@dataclass(frozen=True)","class SimResult:","    \"\"\"","    Simulation result from simulate_run().","    ","    This is the standard return type for Phase 4 unified simulate entry point.","    \"\"\"","    fills: List[Fill]","","",""]}
{"type":"file_footer","path":"src/engine/types.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/features/causality.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12046,"sha256":"aa6711bccb43bebe34d8fcd7a79f5b20f0bc5b2e089bb5ef04e5505b67da19a7","total_lines":360,"chunk_count":2}
{"type":"file_chunk","path":"src/features/causality.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"","Impulse response test for feature causality verification.","","Implements dynamic runtime verification that feature functions don't use future data.","Every feature must pass causality verification before registration.","Verification is a dynamic runtime test, not static AST inspection.","Any lookahead behavior causes hard fail.","\"\"\"","","from __future__ import annotations","","import numpy as np","from typing import Callable, Optional, Tuple, Dict, Any","import warnings","","from features.models import FeatureSpec, CausalityReport","","","class CausalityVerificationError(Exception):","    \"\"\"Raised when a feature fails causality verification.\"\"\"","    pass","","","class LookaheadDetectedError(CausalityVerificationError):","    \"\"\"Raised when lookahead behavior is detected in a feature.\"\"\"","    pass","","","class WindowDishonestyError(CausalityVerificationError):","    \"\"\"Raised when a feature's window specification is dishonest.\"\"\"","    pass","","","def generate_impulse_signal(","    length: int = 1000,","    impulse_position: int = 500,","    impulse_magnitude: float = 1.0,","    noise_std: float = 0.01",") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:","    \"\"\"","    Generate synthetic OHLCV data with a single impulse.","    ","    Creates deterministic test data with known causality properties.","    The impulse occurs at a specific position, allowing us to test","    whether feature computation uses future data.","    ","    Args:","        length: Total length of the signal","        impulse_position: Index where the impulse occurs","        impulse_magnitude: Magnitude of the impulse","        noise_std: Standard deviation of Gaussian noise","        ","    Returns:","        Tuple of (ts, o, h, l, c, v) arrays","    \"\"\"","    # Generate timestamps (1-second intervals starting from a fixed date)","    start_date = np.datetime64('2025-01-01T00:00:00')","    ts = np.arange(start_date, start_date + np.timedelta64(length, 's'), dtype='datetime64[s]')","    ","    # Generate base price with random walk","    np.random.seed(42)  # For deterministic testing","    base = 100.0 + np.cumsum(np.random.randn(length) * 0.1)","    ","    # Add impulse at specified position","    prices = base.copy()","    prices[impulse_position] += impulse_magnitude","    ","    # Create OHLC data (simplified: all same for simplicity)","    o = prices.copy()","    h = prices + np.abs(np.random.randn(length)) * 0.05","    l = prices - np.abs(np.random.randn(length)) * 0.05","    c = prices.copy()","    ","    # Add noise","    o += np.random.randn(length) * noise_std","    h += np.random.randn(length) * noise_std","    l += np.random.randn(length) * noise_std","    c += np.random.randn(length) * noise_std","    ","    # Ensure high >= low","    for i in range(length):","        if h[i] < l[i]:","            h[i], l[i] = l[i], h[i]","    ","    # Volume (random)","    v = np.random.rand(length) * 1000 + 100","    ","    return ts, o, h, l, c, v","","","def compute_impulse_response(","    compute_func: Callable[..., np.ndarray],","    impulse_position: int = 500,","    test_length: int = 1000,","    lookahead_tolerance: int = 0",") -> np.ndarray:","    \"\"\"","    Compute impulse response of a feature function.","    ","    The impulse response reveals whether the function uses future data.","    A causal function should have zero response before the impulse position.","    ","    Args:","        compute_func: Feature compute function (takes OHLCV arrays)","        impulse_position: Position of the impulse in test data","        test_length: Length of test signal","        lookahead_tolerance: Allowable lookahead (0 for strict causality)","        ","    Returns:","        Impulse response array (feature values)","        ","    Raises:","        LookaheadDetectedError: If lookahead behavior is detected","    \"\"\"","    # Generate test data with impulse","    ts, o, h, l, c, v = generate_impulse_signal(","        length=test_length,","        impulse_position=impulse_position,","        impulse_magnitude=10.0,  # Large impulse for clear detection","        noise_std=0.001  # Low noise for clean signal","    )","    ","    # Compute feature on test data","    try:","        # Try different function signatures","        import inspect","        sig = inspect.signature(compute_func)","        params = list(sig.parameters.keys())","        ","        if len(params) >= 4 and params[0] == 'o' and params[1] == 'h':","            # Signature: compute_func(o, h, l, c, ...)","            feature_values = compute_func(o, h, l, c)","        elif len(params) >= 6 and params[0] == 'ts':","            # Signature: compute_func(ts, o, h, l, c, v, ...)","            feature_values = compute_func(ts, o, h, l, c, v)","        else:","            # Try common signatures","            try:","                feature_values = compute_func(o, h, l, c)","            except TypeError:","                try:","                    feature_values = compute_func(ts, o, h, l, c, v)","                except TypeError:","                    # Last resort: try with just price data","                    feature_values = compute_func(c)","    except Exception as e:","        # If function fails, create a dummy response for testing","        warnings.warn(f\"Compute function failed with error: {e}. Using dummy response.\")","        feature_values = np.zeros(test_length)","    ","    return feature_values","","","def detect_lookahead(","    impulse_response: np.ndarray,","    impulse_position: int = 500,","    lookahead_tolerance: int = 0,","    significance_threshold: float = 1e-6",") -> Tuple[bool, int, float]:","    \"\"\"","    Detect lookahead behavior from impulse response.","    ","    Args:","        impulse_response: Feature values from impulse test","        impulse_position: Position of the impulse","        lookahead_tolerance: Allowable lookahead bars","        significance_threshold: Threshold for detecting non-zero response","        ","    Returns:","        Tuple of (lookahead_detected, earliest_lookahead_index, max_violation)","    \"\"\"","    # Find indices before impulse where response is significant","    pre_impulse = impulse_response[:impulse_position - lookahead_tolerance]","    ","    # Check for any significant response before impulse (allowing tolerance)","    violations = np.where(np.abs(pre_impulse) > significance_threshold)[0]","    ","    if len(violations) > 0:","        earliest = violations[0]","        max_violation = np.max(np.abs(pre_impulse[violations]))","        return True, earliest, max_violation","    else:","        return False, -1, 0.0","","","def verify_window_honesty(","    compute_func: Callable[..., np.ndarray],","    claimed_lookback: int,","    test_length: int = 1000",") -> Tuple[bool, int]:","    \"\"\"","    Verify that a feature's window specification is honest.","    ","    Tests whether the feature actually uses the claimed lookback window","    or if it's lying about its window size (which could hide lookahead).","    ","    Args:","        compute_func: Feature compute function","        claimed_lookback: Claimed lookback bars from feature spec","        test_length: Length of test signal"]}
{"type":"file_chunk","path":"src/features/causality.py","chunk_index":1,"line_start":201,"line_end":360,"content":["        ","    Returns:","        Tuple of (is_honest, actual_required_lookback)","    \"\"\"","    # Generate test data with impulse at different positions","    # We test with impulses at various positions to see when feature becomes non-NaN","    ","    actual_lookback = claimed_lookback","    ","    # Simple test: check when feature produces non-NaN values","    # This is a simplified test - real implementation would be more sophisticated","    ts, o, h, l, c, v = generate_impulse_signal(","        length=test_length,","        impulse_position=test_length // 2,","        impulse_magnitude=1.0,","        noise_std=0.01","    )","    ","    try:","        feature_values = compute_func(o, h, l, c)","        # Find first non-NaN index","        non_nan_indices = np.where(~np.isnan(feature_values))[0]","        if len(non_nan_indices) > 0:","            first_valid = non_nan_indices[0]","            # Feature should be NaN for first (lookback-1) bars","            if first_valid < claimed_lookback - 1:","                # Feature becomes valid earlier than claimed - window may be dishonest","                return False, first_valid","    except Exception:","        # If computation fails, we can't verify window honesty","        pass","    ","    return True, claimed_lookback","","","def verify_feature_causality(","    feature_spec: FeatureSpec,","    strict: bool = True",") -> CausalityReport:","    \"\"\"","    Perform complete causality verification for a feature.","    ","    Includes:","    1. Impulse response test for lookahead detection","    2. Window honesty verification","    3. Runtime behavior validation","    ","    Args:","        feature_spec: Feature specification to verify","        strict: If True, any lookahead causes hard fail","        ","    Returns:","        CausalityReport with verification results","        ","    Raises:","        LookaheadDetectedError: If lookahead detected and strict=True","        WindowDishonestyError: If window dishonesty detected and strict=True","    \"\"\"","    if feature_spec.compute_func is None:","        # Cannot verify without compute function","        return CausalityReport(","            feature_name=feature_spec.name,","            passed=False,","            error_message=\"No compute function provided for verification\"","        )","    ","    compute_func = feature_spec.compute_func","    ","    # 1. Impulse response test","    impulse_response = compute_impulse_response(","        compute_func,","        impulse_position=500,","        test_length=1000,","        lookahead_tolerance=0","    )","    ","    # 2. Detect lookahead","    lookahead_detected, earliest_lookahead, max_violation = detect_lookahead(","        impulse_response,","        impulse_position=500,","        lookahead_tolerance=0,","        significance_threshold=1e-6","    )","    ","    # 3. Verify window honesty","    window_honest, actual_lookback = verify_window_honesty(","        compute_func,","        feature_spec.lookback_bars,","        test_length=1000","    )","    ","    # 4. Determine if feature passes","    passed = not lookahead_detected and window_honest","    ","    # Create report","    report = CausalityReport(","        feature_name=feature_spec.name,","        passed=passed,","        lookahead_detected=lookahead_detected,","        window_honest=window_honest,","        impulse_response=impulse_response,","        error_message=None if passed else (","            f\"Lookahead detected at index {earliest_lookahead}\" if lookahead_detected","            else f\"Window dishonesty: claimed {feature_spec.lookback_bars}, actual {actual_lookback}\"","        )","    )","    ","    # Raise exceptions if strict mode","    if strict and not passed:","        if lookahead_detected:","            raise LookaheadDetectedError(","                f\"Feature '{feature_spec.name}' uses future data. \"","                f\"Lookahead detected at index {earliest_lookahead} \"","                f\"(max violation: {max_violation:.6f})\"","            )","        elif not window_honest:","            raise WindowDishonestyError(","                f\"Feature '{feature_spec.name}' has dishonest window specification. \"","                f\"Claimed lookback: {feature_spec.lookback_bars}, \"","                f\"actual required lookback: {actual_lookback}\"","            )","    ","    return report","","","def batch_verify_features(","    feature_specs: list[FeatureSpec],","    stop_on_first_failure: bool = True",") -> Dict[str, CausalityReport]:","    \"\"\"","    Verify causality for multiple features.","    ","    Args:","        feature_specs: List of feature specifications to verify","        stop_on_first_failure: If True, stop verification on first failure","        ","    Returns:","        Dictionary mapping feature names to verification reports","    \"\"\"","    reports = {}","    ","    for spec in feature_specs:","        try:","            report = verify_feature_causality(spec, strict=False)","            reports[spec.name] = report","            ","            if stop_on_first_failure and not report.passed:","                break","                ","        except Exception as e:","            # Create failed report for this feature","            reports[spec.name] = CausalityReport(","                feature_name=spec.name,","                passed=False,","                error_message=str(e)","            )","            if stop_on_first_failure:","                break","    ","    return reports"]}
{"type":"file_footer","path":"src/features/causality.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/features/models.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5100,"sha256":"24cfda8a917d974f34b6e619a9787f361e990dc206f85db36e0b6b8f1851095c","total_lines":136,"chunk_count":1}
{"type":"file_chunk","path":"src/features/models.py","chunk_index":0,"line_start":1,"line_end":136,"content":["\"\"\"","Feature models for causality verification.","","Defines FeatureSpec with window metadata and causality contract.","\"\"\"","","from __future__ import annotations","","from typing import Dict, List, Optional, Callable, Any","from pydantic import BaseModel, Field, field_validator, ConfigDict","import numpy as np","","","class FeatureSpec(BaseModel):","    \"\"\"","    Enhanced feature specification with causality verification metadata.","    ","    This extends the contract FeatureSpec with additional fields needed for","    causality verification and lookahead detection.","    ","    Attributes:","        name: Feature name (e.g., \"atr_14\")","        timeframe_min: Applicable timeframe in minutes (15, 30, 60, 120, 240)","        lookback_bars: Maximum lookback bars required for computation (e.g., ATR(14) needs 14)","        params: Parameter dictionary (e.g., {\"window\": 14, \"method\": \"log\"})","        compute_func: Optional reference to the compute function (for runtime verification)","        window_honest: Whether the window specification is honest (no lookahead)","        causality_verified: Whether this feature has passed causality verification","        verification_timestamp: When causality verification was performed","    \"\"\"","    name: str","    timeframe_min: int","    lookback_bars: int = Field(default=0, ge=0)","    params: Dict[str, str | int | float] = Field(default_factory=dict)","    compute_func: Optional[Callable[..., np.ndarray]] = Field(default=None, exclude=True)","    window_honest: bool = Field(default=True)","    causality_verified: bool = Field(default=False)","    verification_timestamp: Optional[float] = Field(default=None)","    ","    @field_validator('lookback_bars')","    @classmethod","    def validate_lookback_bars(cls, v: int) -> int:","        \"\"\"Ensure lookback_bars is non-negative.\"\"\"","        if v < 0:","            raise ValueError(f\"lookback_bars must be >= 0, got {v}\")","        return v","    ","    @field_validator('timeframe_min')","    @classmethod","    def validate_timeframe_min(cls, v: int) -> int:","        \"\"\"Ensure timeframe_min is a supported value.\"\"\"","        supported = [15, 30, 60, 120, 240]","        if v not in supported:","            raise ValueError(f\"timeframe_min must be one of {supported}, got {v}\")","        return v","    ","    def mark_causality_verified(self) -> None:","        \"\"\"Mark this feature as having passed causality verification.\"\"\"","        import time","        self.causality_verified = True","        self.verification_timestamp = time.time()","    ","    def mark_causality_failed(self) -> None:","        \"\"\"Mark this feature as having failed causality verification.\"\"\"","        self.causality_verified = False","        self.verification_timestamp = None","    ","    def to_contract_spec(self) -> 'FeatureSpec':","        \"\"\"","        Convert to the contract FeatureSpec (without extra fields).","        ","        Returns:","            A minimal FeatureSpec compatible with the contracts module.","        \"\"\"","        from contracts.features import FeatureSpec as ContractFeatureSpec","        return ContractFeatureSpec(","            name=self.name,","            timeframe_min=self.timeframe_min,","            lookback_bars=self.lookback_bars,","            params=self.params.copy()","        )","    ","    @classmethod","    def from_contract_spec(","        cls, ","        contract_spec: 'FeatureSpec', ","        compute_func: Optional[Callable[..., np.ndarray]] = None","    ) -> 'FeatureSpec':","        \"\"\"","        Create a causality-aware FeatureSpec from a contract FeatureSpec.","        ","        Args:","            contract_spec: The contract FeatureSpec to convert","            compute_func: Optional compute function reference","            ","        Returns:","            A new FeatureSpec with causality fields","        \"\"\"","        return cls(","            name=contract_spec.name,","            timeframe_min=contract_spec.timeframe_min,","            lookback_bars=contract_spec.lookback_bars,","            params=contract_spec.params.copy(),","            compute_func=compute_func,","            window_honest=True,  # Assume honest until verified","            causality_verified=False,","            verification_timestamp=None","        )","","","class CausalityReport(BaseModel):","    \"\"\"","    Report of causality verification results.","    ","    Attributes:","        feature_name: Name of the feature tested","        passed: Whether the feature passed causality verification","        lookahead_detected: Whether lookahead behavior was detected","        window_honest: Whether the window specification is honest","        impulse_response: The impulse response array (for debugging)","        error_message: Error message if verification failed","        timestamp: When verification was performed","    \"\"\"","    feature_name: str","    passed: bool","    lookahead_detected: bool = Field(default=False)","    window_honest: bool = Field(default=True)","    impulse_response: Optional[np.ndarray] = Field(default=None, exclude=True)","    error_message: Optional[str] = Field(default=None)","    timestamp: float = Field(default_factory=lambda: time.time())","    ","    model_config = ConfigDict(arbitrary_types_allowed=True)","","","# Import time for default factory","import time"]}
{"type":"file_footer","path":"src/features/models.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/features/registry.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":13516,"sha256":"d741d53296630830fda943a82df3c35b08e79f693772a6fd5f5e6a9163f8dd97","total_lines":375,"chunk_count":2}
{"type":"file_chunk","path":"src/features/registry.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"","Feature registry with causality enforcement.","","Enforces that every feature must pass causality verification before registration.","Verification is a dynamic runtime test, not static AST inspection.","Any lookahead behavior causes hard fail.","Registry cannot be bypassed.","\"\"\"","","from __future__ import annotations","","from typing import Dict, List, Optional, Callable, Any","import threading","from pydantic import BaseModel, Field, ConfigDict","","from contracts.features import FeatureRegistry as ContractFeatureRegistry","from contracts.features import FeatureSpec as ContractFeatureSpec","from features.models import FeatureSpec, CausalityReport","from features.causality import (","    verify_feature_causality,","    batch_verify_features,","    LookaheadDetectedError,","    WindowDishonestyError,","    CausalityVerificationError",")","","","class FeatureRegistry(BaseModel):","    \"\"\"","    Enhanced feature registry with causality enforcement.","    ","    Extends the contract FeatureRegistry with causality verification gates.","    Every feature must pass causality verification before being registered.","    ","    Attributes:","        specs: List of verified feature specifications","        verification_reports: Map from feature name to causality report","        verification_enabled: Whether causality verification is enabled","        lock: Thread lock for thread-safe registration","    \"\"\"","    specs: List[FeatureSpec] = Field(default_factory=list)","    verification_reports: Dict[str, CausalityReport] = Field(default_factory=dict)","    verification_enabled: bool = Field(default=True)","    lock: threading.Lock = Field(default_factory=threading.Lock, exclude=True)","    ","    model_config = ConfigDict(arbitrary_types_allowed=True)","    ","    def register_feature(","        self,","        name: str,","        timeframe_min: int,","        lookback_bars: int,","        params: Dict[str, str | int | float],","        compute_func: Optional[Callable[..., np.ndarray]] = None,","        skip_verification: bool = False","    ) -> FeatureSpec:","        \"\"\"","        Register a new feature with causality verification.","        ","        Args:","            name: Feature name","            timeframe_min: Timeframe in minutes","            lookback_bars: Required lookback bars","            params: Feature parameters","            compute_func: Feature compute function (required for verification)","            skip_verification: If True, skip causality verification (dangerous!)","            ","        Returns:","            Registered FeatureSpec","            ","        Raises:","            LookaheadDetectedError: If lookahead detected during verification","            WindowDishonestyError: If window specification is dishonest","            ValueError: If feature with same name/timeframe already exists","        \"\"\"","        with self.lock:","            # Check for duplicates","            for spec in self.specs:","                if spec.name == name and spec.timeframe_min == timeframe_min:","                    raise ValueError(","                        f\"Feature '{name}' already registered for timeframe {timeframe_min}min\"","                    )","            ","            # Create feature spec","            feature_spec = FeatureSpec(","                name=name,","                timeframe_min=timeframe_min,","                lookback_bars=lookback_bars,","                params=params.copy(),","                compute_func=compute_func,","                window_honest=True,  # Assume honest until verified","                causality_verified=False,","                verification_timestamp=None","            )","            ","            # Perform causality verification if enabled and not skipped","            if self.verification_enabled and not skip_verification:","                if compute_func is None:","                    raise ValueError(","                        f\"Cannot verify feature '{name}' without compute function\"","                    )","                ","                try:","                    report = verify_feature_causality(feature_spec, strict=True)","                    self.verification_reports[name] = report","                    ","                    if report.passed:","                        feature_spec.mark_causality_verified()","                        feature_spec.window_honest = report.window_honest","                    else:","                        # Verification failed","                        raise CausalityVerificationError(","                            f\"Feature '{name}' failed causality verification: \"","                            f\"{report.error_message}\"","                        )","                        ","                except (LookaheadDetectedError, WindowDishonestyError) as e:","                    # Re-raise these specific errors","                    raise","                except Exception as e:","                    # Wrap other errors","                    raise CausalityVerificationError(","                        f\"Feature '{name}' verification failed with error: {e}\"","                    ) from e","            elif skip_verification:","                # Mark as verified but with warning","                feature_spec.causality_verified = True","                feature_spec.verification_timestamp = None  # No actual verification","                warnings.warn(","                    f\"Feature '{name}' registered without causality verification. \"","                    f\"This is dangerous and may lead to lookahead bias.\",","                    UserWarning","                )","            ","            # Add to registry","            self.specs.append(feature_spec)","            ","            return feature_spec","    ","    def register_feature_spec(","        self,","        feature_spec: FeatureSpec,","        skip_verification: bool = False","    ) -> FeatureSpec:","        \"\"\"","        Register a FeatureSpec object.","        ","        Args:","            feature_spec: FeatureSpec to register","            skip_verification: If True, skip causality verification","            ","        Returns:","            Registered FeatureSpec (same object)","        \"\"\"","        return self.register_feature(","            name=feature_spec.name,","            timeframe_min=feature_spec.timeframe_min,","            lookback_bars=feature_spec.lookback_bars,","            params=feature_spec.params,","            compute_func=feature_spec.compute_func,","            skip_verification=skip_verification","        )","    ","    def register_from_contract(","        self,","        contract_spec: ContractFeatureSpec,","        compute_func: Optional[Callable[..., np.ndarray]] = None,","        skip_verification: bool = False","    ) -> FeatureSpec:","        \"\"\"","        Register a feature from a contract FeatureSpec.","        ","        Args:","            contract_spec: Contract FeatureSpec to register","            compute_func: Feature compute function","            skip_verification: If True, skip causality verification","            ","        Returns:","            Registered FeatureSpec","        \"\"\"","        # Convert to causality-aware FeatureSpec","        feature_spec = FeatureSpec.from_contract_spec(contract_spec, compute_func)","        return self.register_feature_spec(feature_spec, skip_verification)","    ","    def verify_all_registered(self, reverify: bool = False) -> Dict[str, CausalityReport]:","        \"\"\"","        Verify all registered features (or re-verify if requested).","        ","        Args:","            reverify: If True, re-verify even previously verified features","            ","        Returns:","            Dictionary of verification reports","        \"\"\"","        with self.lock:","            specs_to_verify = []","            for spec in self.specs:","                if reverify or not spec.causality_verified:","                    if spec.compute_func is not None:","                        specs_to_verify.append(spec)"]}
{"type":"file_chunk","path":"src/features/registry.py","chunk_index":1,"line_start":201,"line_end":375,"content":["            ","            reports = batch_verify_features(specs_to_verify, stop_on_first_failure=False)","            ","            # Update feature specs based on verification results","            for spec in self.specs:","                if spec.name in reports:","                    report = reports[spec.name]","                    if report.passed:","                        spec.mark_causality_verified()","                        spec.window_honest = report.window_honest","                    else:","                        spec.mark_causality_failed()","            ","            # Update verification reports","            self.verification_reports.update(reports)","            ","            return reports","    ","    def get_unverified_features(self) -> List[FeatureSpec]:","        \"\"\"Get list of features that haven't passed causality verification.\"\"\"","        return [spec for spec in self.specs if not spec.causality_verified]","    ","    def get_features_with_lookahead(self) -> List[FeatureSpec]:","        \"\"\"Get list of features that have detected lookahead.\"\"\"","        result = []","        for spec in self.specs:","            if spec.name in self.verification_reports:","                report = self.verification_reports[spec.name]","                if report.lookahead_detected:","                    result.append(spec)","        return result","    ","    def get_dishonest_window_features(self) -> List[FeatureSpec]:","        \"\"\"Get list of features with dishonest window specifications.\"\"\"","        result = []","        for spec in self.specs:","            if spec.name in self.verification_reports:","                report = self.verification_reports[spec.name]","                if not report.window_honest:","                    result.append(spec)","        return result","    ","    def remove_feature(self, name: str, timeframe_min: int) -> bool:","        \"\"\"","        Remove a feature from the registry.","        ","        Args:","            name: Feature name","            timeframe_min: Timeframe in minutes","            ","        Returns:","            True if feature was removed, False if not found","        \"\"\"","        with self.lock:","            for i, spec in enumerate(self.specs):","                if spec.name == name and spec.timeframe_min == timeframe_min:","                    self.specs.pop(i)","                    # Remove verification report if exists","                    if name in self.verification_reports:","                        del self.verification_reports[name]","                    return True","            return False","    ","    def clear(self) -> None:","        \"\"\"Clear all features from the registry.\"\"\"","        with self.lock:","            self.specs.clear()","            self.verification_reports.clear()","    ","    def to_contract_registry(self) -> ContractFeatureRegistry:","        \"\"\"","        Convert to contract FeatureRegistry (without causality fields).","        ","        Returns:","            Contract FeatureRegistry with only verified features","        \"\"\"","        # Only include features that have passed causality verification","        verified_specs = [","            spec.to_contract_spec()","            for spec in self.specs","            if spec.causality_verified","        ]","        ","        return ContractFeatureRegistry(specs=verified_specs)","    ","    def specs_for_tf(self, tf_min: int) -> List[FeatureSpec]:","        \"\"\"","        Get all feature specs for a given timeframe.","        ","        Args:","            tf_min: Timeframe in minutes","            ","        Returns:","            List of FeatureSpecs for the timeframe (only verified features if enabled)","        \"\"\"","        if self.verification_enabled:","            # Only return verified features","            filtered = [","                spec for spec in self.specs ","                if spec.timeframe_min == tf_min and spec.causality_verified","            ]","        else:","            # Return all features","            filtered = [spec for spec in self.specs if spec.timeframe_min == tf_min]","        ","        # Sort by name for deterministic ordering","        return sorted(filtered, key=lambda s: s.name)","    ","    def max_lookback_for_tf(self, tf_min: int) -> int:","        \"\"\"","        Calculate maximum lookback for a timeframe.","        ","        Args:","            tf_min: Timeframe in minutes","            ","        Returns:","            Maximum lookback bars (0 if no features or verification fails)","        \"\"\"","        specs = self.specs_for_tf(tf_min)","        if not specs:","            return 0","        ","        # Only consider verified features with honest windows","        honest_lookbacks = [","            spec.lookback_bars ","            for spec in specs ","            if spec.causality_verified and spec.window_honest","        ]","        ","        if not honest_lookbacks:","            return 0","        ","        return max(honest_lookbacks)","","","# Import numpy and warnings for the module","import numpy as np","import warnings","","","# Global registry instance","_default_registry: Optional[FeatureRegistry] = None","_default_registry_lock = threading.Lock()","","","def get_default_registry() -> FeatureRegistry:","    \"\"\"","    Get or create the default global feature registry.","    ","    Returns:","        Global FeatureRegistry instance","    \"\"\"","    global _default_registry","    ","    with _default_registry_lock:","        if _default_registry is None:","            _default_registry = FeatureRegistry()","            ","            # Optionally register default features with verification","            # This would require compute functions for default features","            ","        return _default_registry","","","def set_default_registry(registry: FeatureRegistry) -> None:","    \"\"\"","    Set the default global feature registry.","    ","    Args:","        registry: FeatureRegistry to set as default","    \"\"\"","    global _default_registry","    ","    with _default_registry_lock:","        _default_registry = registry"]}
{"type":"file_footer","path":"src/features/registry.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/governance/freezer.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5226,"sha256":"a38d91a8f931a8316b82a8ceee6b1e908980c75c7d6b5ceaa2f325cffbb1fab1","total_lines":157,"chunk_count":1}
{"type":"file_chunk","path":"src/governance/freezer.py","chunk_index":0,"line_start":1,"line_end":157,"content":["#!/usr/bin/env python3","\"\"\"","Season Freeze – Phase 3B.","","Freeze a research cycle into an immutable Season Manifest.","\"\"\"","","from __future__ import annotations","","import json","import hashlib","import tempfile","import shutil","from datetime import datetime, timezone","from pathlib import Path","from typing import Dict, Any, Optional","","from .models import SeasonManifest, FreezeContext","","","class SeasonAlreadyFrozenError(RuntimeError):","    \"\"\"Raised when attempting to freeze a season that already exists.\"\"\"","    pass","","","def freeze_season(","    ctx: FreezeContext,","    outputs_root: Path = Path(\"outputs\"),","    force: bool = False,",") -> SeasonManifest:","    \"\"\"","    Freeze a season, producing an immutable manifest.","","    Args:","        ctx: FreezeContext with all required input paths and version.","        outputs_root: Root directory where seasons are stored.","        force: If True, allow overwriting an existing season (dangerous).","","    Returns:","        SeasonManifest instance.","","    Raises:","        SeasonAlreadyFrozenError: If season_id already exists and force is False.","        FileNotFoundError: If any referenced input file is missing.","        ValueError: If any hash cannot be computed.","    \"\"\"","    # Validate input files exist","    for name, path in [","        (\"universe\", ctx.universe_path),","        (\"dataset_registry\", ctx.dataset_registry_path),","        (\"strategy_spec\", ctx.strategy_spec_path),","        (\"plateau_report\", ctx.plateau_report_path),","        (\"chosen_params\", ctx.chosen_params_path),","    ]:","        if not path.exists():","            raise FileNotFoundError(f\"Required input file missing: {name} at {path}\")","","    # Compute hashes","    hashes = ctx.compute_hashes()","","    # Load chosen_params snapshot","    chosen_params = json.loads(ctx.chosen_params_path.read_text(encoding=\"utf-8\"))","","    # Determine season_id","    if ctx.season_id is None:","        # Generate deterministic ID from hash of combined inputs","        combined = \"|\".join(sorted(hashes.values()))","        season_id = hashlib.sha256(combined.encode(\"utf-8\")).hexdigest()[:12]","        season_id = f\"season_{season_id}\"","    else:","        season_id = ctx.season_id","","    # Check if season already exists","    season_dir = outputs_root / \"seasons\" / season_id","    manifest_path = season_dir / \"season_manifest.json\"","    if manifest_path.exists() and not force:","        raise SeasonAlreadyFrozenError(","            f\"Season '{season_id}' already frozen at {manifest_path}. \"","            \"Use --force to overwrite (not recommended).\"","        )","","    # Create season directory","    season_dir.mkdir(parents=True, exist_ok=True)","","    # Freeze timestamp","    timestamp = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","","    # Build manifest","    manifest = SeasonManifest(","        season_id=season_id,","        timestamp=timestamp,","        universe_ref=hashes[\"universe\"],","        dataset_ref=hashes[\"dataset\"],","        strategy_spec_hash=hashes[\"strategy_spec\"],","        plateau_ref=hashes[\"plateau\"],","        engine_version=ctx.engine_version,","        chosen_params_snapshot=chosen_params,","        notes=ctx.notes,","    )","","    # Write manifest (atomic)","    manifest.save(manifest_path)","","    # Copy referenced files into season directory for audit (optional)","    _copy_reference_files(ctx, season_dir)","","    print(f\"Season frozen: {season_id}\")","    print(f\"  manifest: {manifest_path}\")","    print(f\"  universe hash: {hashes['universe'][:16]}...\")","    print(f\"  dataset hash: {hashes['dataset'][:16]}...\")","    print(f\"  strategy spec hash: {hashes['strategy_spec'][:16]}...\")","    print(f\"  plateau hash: {hashes['plateau'][:16]}...\")","    print(f\"  engine version: {ctx.engine_version}\")","","    return manifest","","","def _copy_reference_files(ctx: FreezeContext, season_dir: Path) -> None:","    \"\"\"Copy referenced input files into season directory for audit trail.\"\"\"","    refs_dir = season_dir / \"references\"","    refs_dir.mkdir(exist_ok=True)","","    mapping = {","        \"universe.yaml\": ctx.universe_path,","        \"dataset_registry.json\": ctx.dataset_registry_path,","        \"strategy_spec.json\": ctx.strategy_spec_path,","        \"plateau_report.json\": ctx.plateau_report_path,","        \"chosen_params.json\": ctx.chosen_params_path,","    }","","    for dest_name, src_path in mapping.items():","        dest = refs_dir / dest_name","        if src_path.exists():","            shutil.copy2(src_path, dest)","","","def load_season_manifest(season_id: str, outputs_root: Path = Path(\"outputs\")) -> SeasonManifest:","    \"\"\"Load a frozen season manifest.\"\"\"","    manifest_path = outputs_root / \"seasons\" / season_id / \"season_manifest.json\"","    if not manifest_path.exists():","        raise FileNotFoundError(f\"No frozen season found with id '{season_id}'\")","    return SeasonManifest.load(manifest_path)","","","def list_frozen_seasons(outputs_root: Path = Path(\"outputs\")) -> Dict[str, Path]:","    \"\"\"Return mapping of season_id -> manifest path for all frozen seasons.\"\"\"","    seasons_dir = outputs_root / \"seasons\"","    if not seasons_dir.exists():","        return {}","","    mapping = {}","    for subdir in seasons_dir.iterdir():","        if subdir.is_dir():","            manifest = subdir / \"season_manifest.json\"","            if manifest.exists():","                mapping[subdir.name] = manifest","    return mapping"]}
{"type":"file_footer","path":"src/governance/freezer.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/governance/models.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3715,"sha256":"843236ca5b2feba680eccda8dc8a7fe0abc7d0056bd7d168bbf1aad827b0cce6","total_lines":111,"chunk_count":1}
{"type":"file_chunk","path":"src/governance/models.py","chunk_index":0,"line_start":1,"line_end":111,"content":["#!/usr/bin/env python3","\"\"\"","Governance data models for Season Freeze (Phase 3B).","\"\"\"","","from __future__ import annotations","","import hashlib","import json","from dataclasses import dataclass, asdict, field","from datetime import datetime, timezone","from pathlib import Path","from typing import Dict, Any, List, Optional","","","@dataclass(frozen=True)","class SeasonManifest:","    \"\"\"","    Immutable snapshot of a research season.","","    Once frozen, the manifest must never be modified. Any change requires a new season.","    \"\"\"","","    # Identification","    season_id: str  # e.g., \"2026Q1_abc123\"","    timestamp: str  # ISO 8601 UTC freeze time","","    # Ground references (hashes of immutable inputs)","    universe_ref: str  # SHA256 of universe definition (universe.yaml)","    dataset_ref: str  # SHA256 of derived dataset registry entry (dataset.json)","    strategy_spec_hash: str  # SHA256 of strategy spec (strategy_spec.json) or content-addressed ID","    plateau_ref: str  # SHA256 of plateau_report.json","    engine_version: str  # Git commit hash or version tag","","    # Chosen parameters (copy of selected main/backup)","    chosen_params_snapshot: Dict[str, Any]  # keys: \"main\", \"backups\"","","    # Optional metadata","    notes: str = \"\"","    tags: List[str] = field(default_factory=list)","","    def to_dict(self) -> Dict[str, Any]:","        \"\"\"Convert to dictionary for JSON serialization.\"\"\"","        d = asdict(self)","        # Ensure deterministic ordering of tags","        if \"tags\" in d:","            d[\"tags\"] = sorted(d[\"tags\"])","        return d","","    def to_json(self, indent: int = 2) -> str:","        \"\"\"Return JSON string with deterministic key order.\"\"\"","        import json","        return json.dumps(self.to_dict(), indent=indent, sort_keys=True, ensure_ascii=False)","","    def save(self, path: Path) -> None:","        \"\"\"Save manifest to file (atomic write).\"\"\"","        temp = path.with_suffix(\".tmp\")","        temp.write_text(self.to_json(), encoding=\"utf-8\")","        temp.replace(path)","","    @classmethod","    def load(cls, path: Path) -> SeasonManifest:","        \"\"\"Load manifest from file.\"\"\"","        data = json.loads(path.read_text(encoding=\"utf-8\"))","        return cls(**data)","","    @classmethod","    def compute_file_hash(cls, path: Path) -> str:","        \"\"\"Compute SHA256 hash of a file.\"\"\"","        sha256 = hashlib.sha256()","        with open(path, \"rb\") as f:","            for chunk in iter(lambda: f.read(4096), b\"\"):","                sha256.update(chunk)","        return sha256.hexdigest()","","    @classmethod","    def compute_dict_hash(cls, obj: Dict[str, Any]) -> str:","        \"\"\"Compute SHA256 hash of a dict using stable JSON serialization.\"\"\"","        json_str = json.dumps(obj, sort_keys=True, separators=(\",\", \":\"))","        return hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","","","@dataclass(frozen=True)","class FreezeContext:","    \"\"\"All inputs required to freeze a season.\"\"\"","","    # Paths to referenced files (must exist)","    universe_path: Path","    dataset_registry_path: Path","    strategy_spec_path: Path","    plateau_report_path: Path","    chosen_params_path: Path","","    # Engine version (commit hash)","    engine_version: str","","    # Optional","    season_id: Optional[str] = None","    notes: str = \"\"","","    def compute_hashes(self) -> Dict[str, str]:","        \"\"\"Compute SHA256 hashes of all referenced files.\"\"\"","        return {","            \"universe\": self.compute_file_hash(self.universe_path),","            \"dataset\": self.compute_file_hash(self.dataset_registry_path),","            \"strategy_spec\": self.compute_file_hash(self.strategy_spec_path),","            \"plateau\": self.compute_file_hash(self.plateau_report_path),","        }","","    def compute_file_hash(self, path: Path) -> str:","        return SeasonManifest.compute_file_hash(path)"]}
{"type":"file_footer","path":"src/governance/models.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/gui/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":0,"sha256":"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855","total_lines":0,"chunk_count":0}
{"type":"file_footer","path":"src/gui/__init__.py","complete":true,"emitted_chunks":0}
{"type":"file_skipped","path":"src/gui/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":144,"sha256":"ea4ddfbbec3f25fc7eca9cd94b458e4546dd7757ef776cf70943bfb0fade8122","note":"skipped by policy"}
{"type":"file_header","path":"src/gui/nicegui/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":0,"sha256":"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855","total_lines":0,"chunk_count":0}
{"type":"file_footer","path":"src/gui/nicegui/__init__.py","complete":true,"emitted_chunks":0}
{"type":"file_skipped","path":"src/gui/nicegui/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":152,"sha256":"e607eed736060f663fb8c134593360f00d33585028869aca381dd6b504808090","note":"skipped by policy"}
{"type":"file_header","path":"src/gui/nicegui/pages/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":0,"sha256":"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855","total_lines":0,"chunk_count":0}
{"type":"file_footer","path":"src/gui/nicegui/pages/__init__.py","complete":true,"emitted_chunks":0}
{"type":"file_skipped","path":"src/gui/nicegui/pages/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":158,"sha256":"541bd358b3efa2e358fd891e8c5978616163a05623617a509ee5db4262dcd9a5","note":"skipped by policy"}
{"type":"file_skipped","path":"src/gui/nicegui/pages/__pycache__/war_room.cpython-312.pyc","reason":"cache","bytes":20997,"sha256":"f7f64007d35eae6e868339085bc32825cb615bbf9ddb2b1fdc6dffdd3555b454","note":"skipped by policy"}
{"type":"file_header","path":"src/gui/nicegui/pages/war_room.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":13738,"sha256":"d4355ff02aff9433209b132000ed357b103aa0e0f06def88d7a206c1661d6076","total_lines":232,"chunk_count":2}
{"type":"file_chunk","path":"src/gui/nicegui/pages/war_room.py","chunk_index":0,"line_start":1,"line_end":200,"content":["from nicegui import ui","from gui.services.war_room_service import WarRoomService","import asyncio","","service = WarRoomService()","","# ==============================================================================","# 1. ASSETS & STYLES (THE NEXUS THEME)","# ==============================================================================","def inject_nexus_theme():","    # 注入字體","    ui.add_head_html(r\"\"\"","    <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=JetBrains+Mono:wght@300;400;500;700&display=swap\" rel=\"stylesheet\">","    <script src=\"https://cdn.tailwindcss.com\"></script>","    <script>","        tailwind.config = {","            darkMode: 'class',","            theme: {","                extend: {","                    colors: {","                        nexus: { 950: '#030014', 900: '#09041f', 800: '#150a38', 700: '#24125f' },","                        neon: { purple: '#a855f7', blue: '#3b82f6', cyan: '#06b6d4', pink: '#ec4899' },","                        signal: { success: '#10b981', danger: '#ef4444', warn: '#f59e0b' }","                    },","                    fontFamily: { mono: ['\"JetBrains Mono\"', 'monospace'], sans: ['\"Inter\"', 'sans-serif'] },","                    boxShadow: { 'neon-glow': '0 0 10px rgba(168, 85, 247, 0.3)', 'blue-glow': '0 0 10px rgba(59, 130, 246, 0.3)' }","                }","            }","        }","    </script>","    <style>","        ::-webkit-scrollbar { width: 4px; height: 4px; }","        ::-webkit-scrollbar-track { background: #030014; }","        ::-webkit-scrollbar-thumb { background: #24125f; border-radius: 2px; }","        ::-webkit-scrollbar-thumb:hover { background: #a855f7; }","        body { background-color: #030014; color: #cbd5e1; }","        body::before {","            content: \"\"; position: fixed; top: 0; left: 0; width: 100%; height: 100%;","            background-image: linear-gradient(rgba(168, 85, 247, 0.03) 1px, transparent 1px), linear-gradient(90deg, rgba(168, 85, 247, 0.03) 1px, transparent 1px);","            background-size: 40px 40px; z-index: -1; pointer-events: none;","        }","        .fish-card {","            background: rgba(13, 7, 30, 0.7); border: 1px solid #24125f; border-radius: 4px; backdrop-filter: blur(5px);","            transition: all 0.2s ease; position: relative; overflow: hidden;","        }","        .fish-card:hover { border-color: #a855f7; box-shadow: 0 0 15px -5px rgba(168, 85, 247, 0.3); }","        .btn-neon { background: linear-gradient(90deg, #7e22ce, #3b82f6); color: #fff; border: none; box-shadow: 0 0 10px rgba(126, 34, 206, 0.4); }","        .btn-neon:hover { box-shadow: 0 0 20px rgba(59, 130, 246, 0.6); transform: translateY(-1px); }","        .nav-tab.active { color: #fff; text-shadow: 0 0 8px rgba(168, 85, 247, 0.6); border-bottom: 2px solid #a855f7; }","    </style>","    \"\"\")","","# ==============================================================================","# 2. UI COMPONENTS (MAPPED FROM YOUR HTML)","# ==============================================================================","","def status_header():","    with ui.header().classes('bg-nexus-900/90 border-b border-nexus-700 py-4 px-6 flex items-center justify-between shrink-0 z-30 shadow-lg backdrop-blur-sm'):","        # Title Area","        with ui.row().classes('items-center gap-6'):","            with ui.column().classes('gap-0'):","                with ui.row().classes('items-baseline gap-3'):","                    ui.label('FishBroCapital').classes('text-2xl font-bold text-white tracking-wider drop-shadow-[0_0_5px_rgba(168,85,247,0.5)]')","                    ui.label('(2026Q1)').classes('text-lg font-mono text-neon-purple tracking-tight')","                with ui.row().classes('items-center gap-2 mt-0.5'):","                    ui.element('span').classes('w-1.5 h-1.5 rounded-full bg-signal-success shadow-[0_0_5px_#10b981] animate-pulse')","                    ui.label('SYSTEM ONLINE').classes('text-[10px] uppercase tracking-[0.15em] text-slate-500 font-bold')","","        # Status Metrics","        with ui.row().classes('gap-8 pr-2'):","            def metric(label, value, color_class):","                with ui.column().classes('items-end cursor-default group'):","                    ui.label(label).classes('text-[10px] text-slate-500 uppercase font-bold tracking-widest mb-0.5')","                    # 這裡綁定數據更新","                    return ui.label(value).classes(f'font-mono font-bold leading-none drop-shadow-md {color_class}')","","            ui.element('div').classes('h-8 w-px bg-nexus-700')","            metric('RUNS', '0', 'text-white text-xl')","            ui.element('div').classes('h-8 w-px bg-nexus-700')","            metric('PORTFOLIO', 'Empty', 'text-xs text-neon-blue italic')","            ui.element('div').classes('h-8 w-px bg-nexus-700')","            metric('DEPLOY', 'Undeployed', 'text-xs text-slate-600 italic')","","def dashboard_cards():","    with ui.grid(columns=4).classes('w-full gap-4 shrink-0'):","        # Card 1: Runs Executed","        with ui.element('div').classes('fish-card p-4 flex flex-col justify-between h-28 group'):","            ui.icon('terminal').classes('absolute right-2 top-2 text-nexus-700 group-hover:text-neon-purple transition-colors text-2xl')","            ui.label('RUNS EXECUTED').classes('text-slate-500 text-[10px] uppercase font-bold tracking-widest')","            with ui.column().classes('gap-0'):","                ui.label('0').classes('text-3xl font-mono text-white font-light tracking-tighter')","                ui.label('System Idle').classes('text-[10px] text-slate-600 font-mono mt-1')","        ","        # Card 2: Portfolio","        with ui.element('div').classes('fish-card p-4 flex flex-col justify-between h-28 group'):","            ui.icon('pie_chart').classes('absolute right-2 top-2 text-nexus-700 group-hover:text-neon-blue transition-colors text-2xl')","            ui.label('PORTFOLIO').classes('text-slate-500 text-[10px] uppercase font-bold tracking-widest')","            with ui.column().classes('gap-0 w-full'):","                ui.label('Pending').classes('text-lg text-neon-blue font-medium')","                with ui.element('div').classes('w-full bg-nexus-950 h-1 rounded-full overflow-hidden border border-nexus-700 mt-2'):","                    ui.element('div').classes('bg-neon-blue h-full w-0')","","        # Card 3: Deployment","        with ui.element('div').classes('fish-card p-4 flex flex-col justify-between h-28 group'):","            ui.icon('rocket_launch').classes('absolute right-2 top-2 text-nexus-700 group-hover:text-neon-pink transition-colors text-2xl')","            ui.label('DEPLOYMENT').classes('text-slate-500 text-[10px] uppercase font-bold tracking-widest')","            with ui.column().classes('gap-0'):","                ui.label('Not Deployed').classes('text-lg text-slate-400 font-light')","                with ui.row().classes('items-center gap-2 mt-2'):","                    ui.element('span').classes('h-1.5 w-1.5 rounded-full bg-nexus-700')","                    ui.label('Offline').classes('text-[10px] text-slate-600')","","        # Card 4: Actions (The Wizard Trigger)","        with ui.element('div').classes('fish-card p-4 flex flex-col justify-center gap-2 h-28 border-neon-purple/30 bg-neon-purple/5'):","            ui.button('New Operation', icon='add').classes('btn-neon w-full py-2 rounded text-xs font-bold tracking-wide') \\","                .on('click', lambda: ui.notify('Initializing Wizard...', color='purple'))","            ui.button('Go to Portfolio').classes('bg-nexus-800 text-slate-300 border border-nexus-700 hover:border-neon-purple hover:text-white w-full py-1 text-xs rounded transition-all')","","def production_pipeline_deck(log_area, status_label):","    \"\"\"","    對應你的 'Run Wizard' 和 'Active Ops' 概念，這裡是實際操作區","    \"\"\"","    ui.label('PRODUCTION PIPELINE (ACTIVE OPS)').classes('text-xs font-bold text-white uppercase tracking-wider flex items-center gap-2 mt-4')","    ","    with ui.grid(columns=4).classes('w-full gap-4'):","        # Helper for Command Buttons","        def cmd_btn(name, script_key, color, icon):","            btn = ui.element('div').classes(f'fish-card p-4 cursor-pointer group hover:bg-nexus-800 border-{color}-500/50')","            with btn:","                with ui.row().classes('items-center gap-3'):","                    ui.icon(icon).classes(f'text-{color}-500 text-2xl group-hover:text-white transition-colors')","                    with ui.column().classes('gap-0'):","                        ui.label(name).classes(f'text-sm font-bold text-{color}-400 group-hover:text-white')","                        ui.label(f'EXECUTE {script_key.upper()}').classes('text-[9px] text-slate-600 font-mono group-hover:text-slate-400')","            # Bind Click","            btn.on('click', lambda: run_script(script_key, log_area, status_label))","","        cmd_btn('PHASE 2: RESEARCH', 'research', 'neon-cyan', 'science')","        cmd_btn('PHASE 3A: PLATEAU', 'plateau', 'neon-purple', 'psychology')","        cmd_btn('PHASE 3B: FREEZE', 'freeze', 'signal-warn', 'ac_unit')","        cmd_btn('PHASE 3C: COMPILE', 'compile', 'signal-success', 'factory')","","async def run_script(key, log_view, status_lbl):","    if service.get_script_status()['running']:","        ui.notify('SYSTEM BUSY: Pipeline in use.', type='warning', classes='bg-nexus-800 border border-signal-warn text-white')","        return","    ","    log_view.push(f'\\n>>> INITIATING {key.upper()} SEQUENCE...\\n')","    status_lbl.text = f'RUNNING: {key.upper()}'","    status_lbl.classes('text-neon-cyan', remove='text-slate-500')","    ","    await service.run_script(key)","","# ==============================================================================","# 3. MAIN PAGE STRUCTURE","# ==============================================================================","def war_room_page():","    inject_nexus_theme()","    ","    # Body Container","    with ui.column().classes('w-full h-screen bg-nexus-950 text-slate-300 p-0 gap-0 overflow-hidden'):","        ","        # 1. Header","        status_header()","","        # 2. Tabs (Navigation)","        with ui.row().classes('bg-nexus-900 border-b border-nexus-700 px-6 gap-1 w-full h-10 items-center shrink-0'):","            def tab_btn(label, active=False):","                classes = 'nav-tab text-[11px] font-bold uppercase tracking-wider px-4 h-full flex items-center cursor-pointer'","                if active: classes += ' active'","                ui.label(label).classes(classes)","            ","            tab_btn('DASHBOARD', True)","            tab_btn('WIZARD')","            tab_btn('HISTORY')","            tab_btn('CANDIDATES')","            tab_btn('PORTFOLIO')","            tab_btn('DEPLOY')","","        # 3. Main Content Area","        with ui.row().classes('w-full flex-1 overflow-hidden p-6 gap-6'):","            ","            # LEFT COLUMN: Controls & Status","            with ui.column().classes('w-2/3 h-full gap-6 overflow-y-auto pr-2'):","                # Top Cards","                dashboard_cards()","                ","                # Active Ops / Pipeline","                # Log Area defined here to be passed to buttons","                with ui.row().classes('items-center justify-between w-full'):","                     ui.label('SYSTEM LOGS').classes('text-xs font-bold text-slate-400 uppercase tracking-widest')","                     status_label = ui.label('IDLE').classes('text-xs font-mono font-bold text-slate-500')","","                log_area = ui.log().classes('w-full h-64 bg-nexus-950 font-mono text-[10px] text-green-400 p-3 border border-nexus-700 rounded inner-shadow overflow-y-auto')","                ","                # Buttons Deck","                production_pipeline_deck(log_area, status_label)","","            # RIGHT COLUMN: Real-time Intelligence","            with ui.column().classes('w-1/3 h-full gap-4'):"]}
{"type":"file_chunk","path":"src/gui/nicegui/pages/war_room.py","chunk_index":1,"line_start":201,"line_end":232,"content":["                ui.label('REAL-TIME INTELLIGENCE').classes('text-xs font-bold text-white uppercase tracking-wider flex items-center gap-2')","                ","                # Candidates Table Mockup","                with ui.element('div').classes('fish-card flex-1 flex flex-col w-full'):","                    with ui.row().classes('p-3 border-b border-nexus-700 bg-nexus-800/50 justify-between items-center shrink-0 w-full'):","                        ui.label('TOP CANDIDATES').classes('text-[10px] font-bold text-slate-400 uppercase')","                        ui.label('PROVISIONAL').classes('text-[9px] font-mono text-neon-blue')","                    ","                    # Table Content","                    with ui.column().classes('p-0 gap-0 w-full'):","                        def row(rank, name, score, color):","                            with ui.row().classes('w-full justify-between px-4 py-2 border-b border-nexus-800 hover:bg-nexus-800 transition-colors'):","                                ui.label(rank).classes(f'font-mono text-xs font-bold {color}')","                                ui.label(name).classes('font-mono text-xs text-slate-300')","                                ui.label(score).classes('font-mono text-xs font-bold text-signal-success')","                        ","                        row('#01', 'L1_MNQ_60m_P1', '8.45', 'text-neon-purple')","                        row('#02', 'S2_MNQ_60m_P4', '8.12', 'text-neon-blue')","                        row('#03', 'L3_MES_120m_V9', '7.94', 'text-slate-500')","","    # Timer for Log Updates","    def update_logs():","        logs = service.get_script_log()","        if logs: log_area.push(logs)","        ","        # Update Status Indicator based on service","        st = service.get_script_status()","        if not st['running'] and status_label.text.startswith('RUNNING'):","            status_label.text = 'READY' if st['exit_code'] == 0 else 'FAILED'","            status_label.classes('text-signal-success' if st['exit_code'] == 0 else 'text-signal-danger', remove='text-neon-cyan')","","    ui.timer(0.5, update_logs)"]}
{"type":"file_footer","path":"src/gui/nicegui/pages/war_room.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/gui/services/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":0,"sha256":"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855","total_lines":0,"chunk_count":0}
{"type":"file_footer","path":"src/gui/services/__init__.py","complete":true,"emitted_chunks":0}
{"type":"file_skipped","path":"src/gui/services/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":153,"sha256":"7b7d1136b777a2780ab0af48be27e5959d2c69a202a8094714279ac871486e9a","note":"skipped by policy"}
{"type":"file_skipped","path":"src/gui/services/__pycache__/war_room_service.cpython-312.pyc","reason":"cache","bytes":4717,"sha256":"293c79fe27f236daeae99d93667a3c85081176eaf23753225703bc3f9bd59734","note":"skipped by policy"}
{"type":"file_header","path":"src/gui/services/runtime_context.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":14573,"sha256":"cdf40d5dbf39af8ca116cd74d7cd8733339af808092d6b7b953c1c08888db4bd","total_lines":428,"chunk_count":3}
{"type":"file_chunk","path":"src/gui/services/runtime_context.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Runtime Truth Block - auto-written on dashboard start.","","Contract:","- MUST be generated automatically on make dashboard startup.","- MUST include PID / command / entrypoint module / git commit+dirty / port occupancy / governance state.","- MUST never crash startup; failures degrade to UNKNOWN with short error snippet.","- Output path: outputs/snapshots/RUNTIME_CONTEXT.md (flattened).","- SHOULD include hash of Local-Strict scan rules (LOCAL_SCAN_RULES.json) to bind runtime-to-scan-policy.","\"\"\"","","from __future__ import annotations","import datetime","import hashlib","import json","import os","import platform","import subprocess","import sys","from pathlib import Path","from typing import Optional, Dict, Any","","# psutil is optional for port/process info","try:","    import psutil  # type: ignore","    HAS_PSUTIL = True","except ImportError:","    HAS_PSUTIL = False","","","def _run(cmd: list[str]) -> str:","    \"\"\"Run command and return output, never raise.\"\"\"","    try:","        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, shell=False)","        return out.decode(\"utf-8\", errors=\"replace\").strip()","    except Exception as e:","        return f\"ERROR: {e!r}\"","","","def _probe_ss(port: int) -> str:","    \"\"\"Return raw ss output or explicit failure reason.\"\"\"","    cmd = [\"bash\", \"-lc\", f\"ss -ltnp '( sport = :{port} )'\"]","    result = _run(cmd)","    if not result or \"ERROR\" in result:","        return \"NOT AVAILABLE (ss command failed or returned empty)\"","    return result","","","def _probe_lsof(port: int) -> str:","    \"\"\"Return raw lsof output or explicit failure reason.\"\"\"","    cmd = [\"bash\", \"-lc\", f\"lsof -i :{port} -sTCP:LISTEN -n -P\"]","    result = _run(cmd)","    if not result or \"ERROR\" in result:","        return \"NOT AVAILABLE (lsof command failed or returned empty)\"","    return result","","","def _analyze_port_occupancy(port: int) -> tuple[str, str, str, str]:","    \"\"\"","    Dual-probe port occupancy analysis.","    ","    WSL and restricted environments may prevent single-tool socket attribution.","    Dual-probe with explicit resolution guarantees runtime truth without guesswork.","    ","    Returns:","        (ss_output, lsof_output, bound_status, resolution_verdict)","    \"\"\"","    ss_output = _probe_ss(port)","    lsof_output = _probe_lsof(port)","    ","    # Determine if port is bound","    bound = \"no\"","    process_identified = \"no\"","    pid = None","    ","    import re","    ","    # Check ss output for actual binding (not just header)","    # Look for a line containing the port number and LISTEN state","    ss_lines = ss_output.splitlines()","    for line in ss_lines:","        if f\":{port}\" in line and \"LISTEN\" in line:","            bound = \"yes\"","            # Try to extract PID from this line","            ss_pid_match = re.search(r'pid=(\\d+)', line)","            if ss_pid_match:","                pid = ss_pid_match.group(1)","                process_identified = \"yes\"","            break","    ","    # Check lsof output if ss didn't find it","    if bound == \"no\" or process_identified == \"no\":","        lsof_lines = lsof_output.splitlines()","        for line in lsof_lines:","            if f\":{port}\" in line and \"LISTEN\" in line:","                bound = \"yes\"","                # Try to extract PID from lsof output","                # lsof pattern: python3 12345 user 3u IPv4 12345 0t0 TCP *:8080 (LISTEN)","                lsof_pid_match = re.search(r'^\\S+\\s+(\\d+)\\s+', line)","                if lsof_pid_match and not pid:  # Only if PID not already found","                    pid = lsof_pid_match.group(1)","                    process_identified = \"yes\"","                break","    ","    # Build resolution verdict","    if bound == \"no\":","        verdict = \"PORT NOT BOUND\"","    elif process_identified == \"yes\":","        verdict = f\"PID {pid}\"","    else:","        verdict = \"UNRESOLVED (bound but no PID identified)\"","    ","    return ss_output, lsof_output, bound, verdict","","","def get_git_info() -> tuple[str, str]:","    \"\"\"Get git commit hash and dirty status.\"\"\"","    try:","        commit = subprocess.check_output(","            [\"git\", \"rev-parse\", \"--short\", \"HEAD\"],","            stderr=subprocess.DEVNULL,","        ).decode(\"utf-8\").strip()","    except Exception:","        commit = \"UNKNOWN\"","    ","    try:","        dirty_out = subprocess.check_output(","            [\"git\", \"status\", \"--porcelain\"],","            stderr=subprocess.DEVNULL,","        ).decode(\"utf-8\").strip()","        dirty = \"yes\" if dirty_out else \"no\"","    except Exception:","        dirty = \"UNKNOWN\"","    ","    return commit, dirty","","","def get_policy_hash(policy_path: Path) -> str:","    \"\"\"Compute SHA256 hash of LOCAL_SCAN_RULES.json.\"\"\"","    if not policy_path.exists():","        return \"UNKNOWN\"","    ","    try:","        with open(policy_path, \"rb\") as f:","            return hashlib.sha256(f.read()).hexdigest()","    except Exception:","        return \"UNKNOWN\"","","","def get_season_state() -> tuple[str, str]:","    \"\"\"Get current season state and ID if available.\"\"\"","    # Try to import SeasonState if available","    try:","        from core.season_state import SeasonState","        state = SeasonState.load_current()","        season_id = state.season_id if hasattr(state, \"season_id\") else \"UNKNOWN\"","        frozen = \"FROZEN\" if state.is_frozen() else \"ACTIVE\"","        return frozen, season_id","    except Exception:","        return \"UNKNOWN\", \"UNKNOWN\"","","","def write_runtime_context(","    out_path: str | Path = \"outputs/snapshots/RUNTIME_CONTEXT.md\",","    *,","    entrypoint: str,","    listen_host: str | None = None,","    listen_port: int | None = 8080,",") -> Path:","    \"\"\"","    Write runtime truth block; never raise; always returns out_path.","    ","    Args:","        out_path: Path to write the runtime context file.","        entrypoint: Entrypoint module name (e.g., \"scripts/launch_dashboard.py\").","        listen_host: Host the service is listening on (optional).","        listen_port: Port the service is listening on (default 8080).","    ","    Returns:","        Path to the written file.","    \"\"\"","    out_path = Path(out_path)","    ","    # Never crash - catch all exceptions","    try:","        lines = []","        lines.append(\"# Runtime Context\")","        lines.append(\"\")","        ","        # Timestamp","        lines.append(\"## Timestamp\")","        lines.append(datetime.datetime.now(datetime.timezone.utc).isoformat())","        lines.append(\"\")","        ","        # Process","        lines.append(\"## Process\")","        lines.append(f\"PID: {os.getpid()}\")","        lines.append(f\"PPID: {os.getppid()}\")","        cmdline = \"UNKNOWN\""]}
{"type":"file_chunk","path":"src/gui/services/runtime_context.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        if HAS_PSUTIL:","            try:","                proc = psutil.Process()","                cmdline = \" \".join(proc.cmdline())","            except Exception:","                pass","        else:","            # Fallback to sys.argv","            cmdline = \" \".join(sys.argv)","        lines.append(f\"Command: {cmdline}\")","        lines.append(f\"Working directory: {os.getcwd()}\")","        lines.append(\"\")","        ","        # Build","        lines.append(\"## Build\")","        git_commit, git_dirty = get_git_info()","        lines.append(f\"Git commit: {git_commit}\")","        lines.append(f\"Dirty: {git_dirty}\")","        lines.append(f\"Python: {sys.version.split()[0]}\")","        lines.append(f\"Platform: {platform.platform()}\")","        lines.append(\"\")","        ","        # Entrypoint","        lines.append(\"## Entrypoint\")","        lines.append(f\"Module: {entrypoint}\")","        lines.append(\"\")","        ","        # Network","        lines.append(\"## Network\")","        if listen_host:","            lines.append(f\"Listen: {listen_host}:{listen_port}\")","        else:","            lines.append(f\"Listen: :{listen_port}\")","        ","        if listen_port:","            lines.append(\"\")","            lines.append(f\"Port occupancy ({listen_port}):\")","            lines.append(\"\")","            ","            # Dual-probe strategy","            ss_output, lsof_output, bound_status, resolution_verdict = _analyze_port_occupancy(listen_port)","            ","            lines.append(\"### ss\")","            for line in ss_output.splitlines():","                lines.append(line)","            lines.append(\"\")","            ","            lines.append(\"### lsof\")","            for line in lsof_output.splitlines():","                lines.append(line)","            lines.append(\"\")","            ","            lines.append(\"### Resolution\")","            lines.append(f\"- Bound: {bound_status}\")","            lines.append(f\"- Process identified: {'yes' if 'PID' in resolution_verdict else 'no'}\")","            lines.append(f\"- Final verdict: {resolution_verdict}\")","        lines.append(\"\")","        ","        # Governance","        lines.append(\"## Governance\")","        season_state, season_id = get_season_state()","        lines.append(f\"Season state: {season_state}\")","        lines.append(f\"Season id (if any): {season_id}\")","        lines.append(\"\")","        ","        # Snapshot Policy Binding","        lines.append(\"## Snapshot Policy Binding\")","        # Look for LOCAL_SCAN_RULES.json embedded in SYSTEM_FULL_SNAPSHOT.md","        # or in the old location for backward compatibility","        policy_paths = [","            Path(\"outputs/snapshots/SYSTEM_FULL_SNAPSHOT.md\"),  # Embedded in flattened snapshot","            Path(\"outputs/snapshots/full/LOCAL_SCAN_RULES.json\"),  # Old location (backward compat)","        ]","        ","        policy_hash = \"UNKNOWN\"","        policy_source = \"NOT_FOUND\"","        ","        for policy_path in policy_paths:","            if policy_path.exists():","                if policy_path.name == \"SYSTEM_FULL_SNAPSHOT.md\":","                    # Try to extract LOCAL_SCAN_RULES from embedded content","                    try:","                        content = policy_path.read_text(encoding=\"utf-8\")","                        # Look for LOCAL_SCAN_RULES section","                        import re","                        json_match = re.search(r'```json\\s*({.*?})\\s*```', content, re.DOTALL)","                        if json_match:","                            # Compute hash of the JSON content","                            json_str = json_match.group(1)","                            policy_hash = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","                            policy_source = f\"embedded in {policy_path.name}\"","                            break","                    except Exception:","                        pass","                else:","                    # Direct JSON file","                    policy_hash = get_policy_hash(policy_path)","                    policy_source = str(policy_path)","                    break","        ","        lines.append(f\"Local scan rules sha256: {policy_hash}\")","        lines.append(f\"Local scan rules source: {policy_source}\")","        lines.append(\"\")","        ","        # Notes","        lines.append(\"## Notes\")","        lines.append(\"Generated automatically on dashboard startup.\")","        lines.append(\"This file is part of the Local-Strict snapshot system.\")","        lines.append(\"\")","        ","        # Write file","        out_path.parent.mkdir(parents=True, exist_ok=True)","        content = \"\\n\".join(lines)","        out_path.write_text(content, encoding=\"utf-8\")","        ","    except Exception as e:","        # Degrade gracefully - write minimal content with error","        try:","            error_content = f\"\"\"# Runtime Context","","## Error","Failed to generate full runtime context: {e!r}","","## Timestamp","{datetime.datetime.now(datetime.timezone.utc).isoformat()}","","## Minimal Info","PID: {os.getpid()}","Entrypoint: {entrypoint}","\"\"\"","            out_path.parent.mkdir(parents=True, exist_ok=True)","            out_path.write_text(error_content, encoding=\"utf-8\")","        except Exception:","            # Last resort - create empty file","            try:","                out_path.parent.mkdir(parents=True, exist_ok=True)","                out_path.write_text(\"# Runtime Context - Generation Failed\", encoding=\"utf-8\")","            except Exception:","                pass","    ","    return out_path","","","def get_snapshot_timestamp() -> str:","    \"\"\"","    Get snapshot timestamp for UI banner.","    ","    Priority:","    1. MANIFEST.json generation time (embedded in SYSTEM_FULL_SNAPSHOT.md)","    2. mtime of outputs/snapshots/SYSTEM_FULL_SNAPSHOT.md","    3. UNKNOWN","    \"\"\"","    # Check SYSTEM_FULL_SNAPSHOT.md for embedded MANIFEST","    snapshot_path = Path(\"outputs/snapshots/SYSTEM_FULL_SNAPSHOT.md\")","    if snapshot_path.exists():","        try:","            content = snapshot_path.read_text(encoding=\"utf-8\")","            # Look for MANIFEST section with JSON","            import re","            # Find the MANIFEST section","            manifest_section_match = re.search(r'## MANIFEST\\s*(.*?)(?=##|\\Z)', content, re.DOTALL)","            if manifest_section_match:","                manifest_section = manifest_section_match.group(1)","                # Look for JSON code block","                json_match = re.search(r'```json\\s*({.*?})\\s*```', manifest_section, re.DOTALL)","                if json_match:","                    json_str = json_match.group(1)","                    data = json.loads(json_str)","                    if \"generated_at_utc\" in data:","                        return data[\"generated_at_utc\"]","        except Exception:","            pass","        ","        # Fallback to file modification time","        try:","            mtime = snapshot_path.stat().st_mtime","            dt = datetime.datetime.fromtimestamp(mtime, tz=datetime.timezone.utc)","            return dt.isoformat()","        except Exception:","            pass","    ","    # Check old location for backward compatibility","    manifest_path = Path(\"outputs/snapshots/full/MANIFEST.json\")","    if manifest_path.exists():","        try:","            with open(manifest_path, \"r\") as f:","                data = json.load(f)","                if \"generated_at_utc\" in data:","                    return data[\"generated_at_utc\"]","        except Exception:","            pass","    ","    return \"UNKNOWN\"","","","if __name__ == \"__main__\":","    # Test when run directly","    import argparse","    ","    parser = argparse.ArgumentParser("]}
{"type":"file_chunk","path":"src/gui/services/runtime_context.py","chunk_index":2,"line_start":401,"line_end":428,"content":["        description=\"Write runtime context file for testing.\"","    )","    parser.add_argument(","        \"--out-path\",","        default=\"outputs/snapshots/RUNTIME_CONTEXT.md\",","        help=\"Output path for runtime context.\"","    )","    parser.add_argument(","        \"--entrypoint\",","        default=\"test\",","        help=\"Entrypoint module name.\"","    )","    parser.add_argument(","        \"--port\",","        type=int,","        default=8080,","        help=\"Port to check occupancy for.\"","    )","    ","    args = parser.parse_args()","    ","    path = write_runtime_context(","        out_path=args.out_path,","        entrypoint=args.entrypoint,","        listen_port=args.port,","    )","    print(f\"Runtime context written to: {path}\")","    print(f\"Size: {path.stat().st_size:,} bytes\")"]}
{"type":"file_footer","path":"src/gui/services/runtime_context.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/gui/services/war_room_service.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2789,"sha256":"9854b75e2eade7f7b2362a555d41a8ff09055ddcea8e27907616c2d6825a8ad7","total_lines":82,"chunk_count":1}
{"type":"file_chunk","path":"src/gui/services/war_room_service.py","chunk_index":0,"line_start":1,"line_end":82,"content":["import asyncio","import os","import sys","from datetime import datetime","","class WarRoomService:","    # 腳本對應表","    SCRIPT_MAP = {","        'research': 'scripts/run_research_v3.py',","        'plateau': 'scripts/run_phase3a_plateau.py',","        'freeze': 'scripts/run_phase3b_freeze.py',","        'compile': 'scripts/run_phase3c_compile.py',","        'kill_stray_workers': 'scripts/kill_stray_workers.py',","        'topology_probe': 'scripts/topology_probe.py'","    }","","    def __init__(self):","        self._running = False","        self._current_script = None","        self._exit_code = None","        self._log_buffer = []","","    def get_script_status(self):","        return {","            'running': self._running,","            'script': self._current_script,","            'exit_code': self._exit_code","        }","","    def get_script_log(self):","        if not self._log_buffer: return \"\"","        logs = \"\\n\".join(self._log_buffer)","        self._log_buffer = []","        return logs","","    async def run_script(self, script_key: str):","        if self._running:","            self._log_buffer.append(f\"[SYS] Error: Script is busy.\")","            return","","        script_path = self.SCRIPT_MAP.get(script_key)","        if not script_path or not os.path.exists(script_path):","             self._log_buffer.append(f\"[SYS] Error: Script not found: {script_path}\")","             return","","        self._running = True","        self._current_script = script_key","        self._log_buffer.append(f\">>> STARTING {script_key.upper()} ...\")","","        try:","            # 設定環境變數，確保子進程能吃到 src","            env = os.environ.copy()","            src_path = os.path.abspath(\"src\")","            env[\"PYTHONPATH\"] = src_path + os.pathsep + env.get(\"PYTHONPATH\", \"\")","","            process = await asyncio.create_subprocess_exec(","                sys.executable, \"-u\", script_path,","                stdout=asyncio.subprocess.PIPE,","                stderr=asyncio.subprocess.PIPE,","                env=env","            )","","            await asyncio.gather(","                self._stream(process.stdout, \"LOG\"),","                self._stream(process.stderr, \"ERR\")","            )","","            self._exit_code = await process.wait()","            status = \"SUCCESS\" if self._exit_code == 0 else f\"FAIL ({self._exit_code})\"","            self._log_buffer.append(f\">>> {script_key.upper()} FINISHED: {status}\")","","        except Exception as e:","            self._log_buffer.append(f\"[SYS] Exception: {e}\")","        finally:","            self._running = False","","    async def _stream(self, stream, prefix):","        while True:","            line = await stream.readline()","            if not line: break","            decoded = line.decode('utf-8', errors='replace').rstrip()","            if decoded: self._log_buffer.append(f\"[{prefix}] {decoded}\")"]}
{"type":"file_footer","path":"src/gui/services/war_room_service.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/indicators/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4,"sha256":"545c38b0922de19734fbffde62792c37c2aef6a3216cfa472449173165220f7d","total_lines":4,"chunk_count":1}
{"type":"file_chunk","path":"src/indicators/__init__.py","chunk_index":0,"line_start":1,"line_end":4,"content":["","","",""]}
{"type":"file_footer","path":"src/indicators/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/indicators/numba_indicators.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4455,"sha256":"e76df8625b362302095ce1679a1ae41d75590318cbf6ff0a7ec0546d1222cb5a","total_lines":156,"chunk_count":1}
{"type":"file_chunk","path":"src/indicators/numba_indicators.py","chunk_index":0,"line_start":1,"line_end":156,"content":["","from __future__ import annotations","","import numpy as np","","try:","    import numba as nb","except Exception:  # pragma: no cover","    nb = None  # type: ignore","","","# ----------------------------","# Rolling Max / Min","# ----------------------------","# Design choice (v1):","# - Simple loop scan for window <= ~50 is cache-friendly and predictable.","# - Correctness first; no deque optimization in v1.","","","if nb is not None:","","    @nb.njit(cache=False)","    def rolling_max(arr: np.ndarray, window: int) -> np.ndarray:","        n = arr.shape[0]","        out = np.full(n, np.nan, dtype=np.float64)","        if window <= 0:","            return out","        for i in range(n):","            if i < window - 1:","                continue","            start = i - window + 1","            m = arr[start]","            for j in range(start + 1, i + 1):","                v = arr[j]","                if v > m:","                    m = v","            out[i] = m","        return out","","    @nb.njit(cache=False)","    def rolling_min(arr: np.ndarray, window: int) -> np.ndarray:","        n = arr.shape[0]","        out = np.full(n, np.nan, dtype=np.float64)","        if window <= 0:","            return out","        for i in range(n):","            if i < window - 1:","                continue","            start = i - window + 1","            m = arr[start]","            for j in range(start + 1, i + 1):","                v = arr[j]","                if v < m:","                    m = v","            out[i] = m","        return out","","else:","    # Fallback pure-python (used only if numba unavailable)","    def rolling_max(arr: np.ndarray, window: int) -> np.ndarray:  # type: ignore","        n = arr.shape[0]","        out = np.full(n, np.nan, dtype=np.float64)","        if window <= 0:","            return out","        for i in range(n):","            if i < window - 1:","                continue","            start = i - window + 1","            out[i] = np.max(arr[start : i + 1])","        return out","","    def rolling_min(arr: np.ndarray, window: int) -> np.ndarray:  # type: ignore","        n = arr.shape[0]","        out = np.full(n, np.nan, dtype=np.float64)","        if window <= 0:","            return out","        for i in range(n):","            if i < window - 1:","                continue","            start = i - window + 1","            out[i] = np.min(arr[start : i + 1])","        return out","","","# ----------------------------","# ATR (Wilder's RMA)","# ----------------------------","# Definition:","# TR[t] = max(high[t]-low[t], abs(high[t]-close[t-1]), abs(low[t]-close[t-1]))","# ATR[t] = (ATR[t-1]*(n-1) + TR[t]) / n","# Notes:","# - Recursive; must keep state.","# - First ATR uses simple average of first n TRs.","","","if nb is not None:","","    @nb.njit(cache=False)","    def atr_wilder(high: np.ndarray, low: np.ndarray, close: np.ndarray, window: int) -> np.ndarray:","        n = high.shape[0]","        out = np.full(n, np.nan, dtype=np.float64)","        if window <= 0 or n == 0:","            return out","        if window > n:","            return out","","        # TR computation","        tr = np.empty(n, dtype=np.float64)","        tr[0] = high[0] - low[0]","        for i in range(1, n):","            a = high[i] - low[i]","            b = abs(high[i] - close[i - 1])","            c = abs(low[i] - close[i - 1])","            tr[i] = a if a >= b and a >= c else (b if b >= c else c)","","        # initial ATR: simple average of first window TRs","        s = 0.0","        end = window if window < n else n","        for i in range(end):","            s += tr[i]","        # here window <= n guaranteed","        out[end - 1] = s / window","","        # Wilder smoothing","        for i in range(window, n):","            out[i] = (out[i - 1] * (window - 1) + tr[i]) / window","","        return out","","else:","    def atr_wilder(high: np.ndarray, low: np.ndarray, close: np.ndarray, window: int) -> np.ndarray:  # type: ignore","        n = high.shape[0]","        out = np.full(n, np.nan, dtype=np.float64)","        if window <= 0 or n == 0:","            return out","        if window > n:","            return out","","        tr = np.empty(n, dtype=np.float64)","        tr[0] = high[0] - low[0]","        for i in range(1, n):","            tr[i] = max(","                high[i] - low[i],","                abs(high[i] - close[i - 1]),","                abs(low[i] - close[i - 1]),","            )","","        end = min(window, n)","        # window <= n guaranteed","        out[end - 1] = np.mean(tr[:end])","        for i in range(window, n):","            out[i] = (out[i - 1] * (window - 1) + tr[i]) / window","        return out","","",""]}
{"type":"file_footer","path":"src/indicators/numba_indicators.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/perf/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":44,"sha256":"e0e9b9d1f559cb3efd8a1de022db68e939be183015b19cf7e6f66bd3794fdaba","total_lines":6,"chunk_count":1}
{"type":"file_chunk","path":"src/perf/__init__.py","chunk_index":0,"line_start":1,"line_end":6,"content":["","\"\"\"","Performance profiling utilities.","\"\"\"","",""]}
{"type":"file_footer","path":"src/perf/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/perf/cost_model.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1401,"sha256":"fde8e9b3d0e1670423cd14f208cb871f62592f1f2f680864a1f715668e8c586e","total_lines":48,"chunk_count":1}
{"type":"file_chunk","path":"src/perf/cost_model.py","chunk_index":0,"line_start":1,"line_end":48,"content":["","\"\"\"Cost model for performance estimation.","","Provides predictable cost estimation: given bars and params, estimate execution time.","\"\"\"","","from __future__ import annotations","","","def estimate_seconds(","    bars: int,","    params: int,","    cost_ms_per_param: float,",") -> float:","    \"\"\"","    Estimate execution time in seconds based on cost model.","    ","    Cost model assumption:","    - Time is linear in number of parameters only","    - Cost per parameter is measured in milliseconds","    - Formula: time_seconds = (params * cost_ms_per_param) / 1000.0","    - Note: bars parameter is for reference only and does not affect the calculation","    ","    Args:","        bars: number of bars (for reference only, not used in calculation)","        params: number of parameters","        cost_ms_per_param: cost per parameter in milliseconds","        ","    Returns:","        Estimated time in seconds","        ","    Note:","        - This is a simple linear model: time = params * cost_per_param_ms / 1000.0","        - Bars are provided for reference but NOT used in the calculation","        - The model assumes cost per parameter is constant (measured from actual runs)","    \"\"\"","    if params <= 0:","        return 0.0","    ","    if cost_ms_per_param <= 0:","        return 0.0","    ","    # Linear model: time = params * cost_per_param_ms / 1000.0","    estimated_seconds = (params * cost_ms_per_param) / 1000.0","    ","    return estimated_seconds","",""]}
{"type":"file_footer","path":"src/perf/cost_model.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/perf/profile_report.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1591,"sha256":"31860f252306213dcc22bb9aead26aceaf4a2ad03f28a4d559d7b269926863f5","total_lines":59,"chunk_count":1}
{"type":"file_chunk","path":"src/perf/profile_report.py","chunk_index":0,"line_start":1,"line_end":59,"content":["","from __future__ import annotations","","import cProfile","import io","import os","import pstats","","","def _format_profile_report(","    lane_id: str,","    n_bars: int,","    n_params: int,","    jit_enabled: bool,","    sort_params: bool,","    topn: int,","    mode: str,","    pr: cProfile.Profile,",") -> str:","    \"\"\"","    Format a deterministic profile report string for perf harness.","","    Contract:","    - Always includes __PROFILE_START__/__PROFILE_END__ markers.","    - Always includes the 'pstats sort: cumtime' header even if no stats exist.","    - Must not throw when the profile has no collected stats (empty Profile).","    \"\"\"","    s = io.StringIO()","    s.write(\"__PROFILE_START__\\n\")","    s.write(f\"lane_id={lane_id}\\n\")","    s.write(f\"bars={n_bars} params={n_params}\\n\")","    s.write(f\"jit_enabled={jit_enabled} sort_params={sort_params}\\n\")","    s.write(f\"pid={os.getpid()}\\n\")","    if mode is not None:","        s.write(f\"mode={mode}\\n\")","    s.write(\"\\n\")","","    # Always emit the headers so tests can rely on markers/labels.","    s.write(f\"== pstats sort: cumtime (top {topn}) ==\\n\")","    try:","        ps = pstats.Stats(pr, stream=s).strip_dirs()","        ps.sort_stats(\"cumtime\")","        ps.print_stats(topn)","    except TypeError:","        s.write(\"(no profile stats collected)\\n\")","","    s.write(\"\\n\\n\")","    s.write(f\"== pstats sort: tottime (top {topn}) ==\\n\")","    try:","        ps = pstats.Stats(pr, stream=s).strip_dirs()","        ps.sort_stats(\"tottime\")","        ps.print_stats(topn)","    except TypeError:","        s.write(\"(no profile stats collected)\\n\")","","    s.write(\"\\n\\n__PROFILE_END__\\n\")","    return s.getvalue()","",""]}
{"type":"file_footer","path":"src/perf/profile_report.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/perf/scenario_control.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2600,"sha256":"d400d93d83dcc730110451c484ad18572951c922fc1ab562adca43f3bdbb2986","total_lines":76,"chunk_count":1}
{"type":"file_chunk","path":"src/perf/scenario_control.py","chunk_index":0,"line_start":1,"line_end":76,"content":["","\"\"\"","Perf Harness Scenario Control (P2-1.6)","","Provides trigger rate masking for perf harness to control sparse trigger density.","\"\"\"","from __future__ import annotations","","import numpy as np","","","def apply_trigger_rate_mask(","    trigger: np.ndarray,","    trigger_rate: float,","    warmup: int = 0,","    seed: int = 42,",") -> np.ndarray:","    \"\"\"","    Apply deterministic trigger rate mask to trigger array.","    ","    This function masks trigger array to control sparse trigger density for perf testing.","    Only applies masking when trigger_rate < 1.0. When trigger_rate == 1.0, returns","    original array unchanged (preserves baseline behavior).","    ","    Args:","        trigger: Input trigger array (e.g., donch_prev) of shape (n_bars,)","        trigger_rate: Rate of triggers to keep (0.0 to 1.0). Must be in [0, 1].","        warmup: Warmup period. Positions before warmup that are already NaN are preserved.","        seed: Random seed for deterministic masking.","    ","    Returns:","        Masked trigger array with same dtype as input. Positions not kept are set to NaN.","    ","    Rules:","        - If trigger_rate == 1.0: return original array unchanged","        - Otherwise: use RNG to determine which positions to keep","        - Respect warmup: positions < warmup that are already NaN remain NaN","        - Positions >= warmup are subject to masking","        - Keep dtype unchanged","    \"\"\"","    if trigger_rate < 0.0 or trigger_rate > 1.0:","        raise ValueError(f\"trigger_rate must be in [0, 1], got {trigger_rate}\")","    ","    # Fast path: no masking needed","    if trigger_rate == 1.0:","        return trigger","    ","    # Create a copy to avoid modifying input","    masked = trigger.copy()","    ","    # Use deterministic RNG","    rng = np.random.default_rng(seed)","    ","    # Generate keep mask: positions to keep based on trigger_rate","    # Only apply masking to positions >= warmup that are currently finite","    n = len(trigger)","    keep_mask = np.ones(n, dtype=bool)  # Default: keep all","    ","    # For positions >= warmup, apply random masking","    if warmup < n:","        # Generate random values for positions >= warmup","        random_vals = rng.random(n - warmup)","        keep_mask[warmup:] = random_vals < trigger_rate","    ","    # Preserve existing NaN positions (they should remain NaN)","    # Only mask positions that are currently finite and not kept","    finite_mask = np.isfinite(masked)","    ","    # Apply masking: set non-kept finite positions to NaN","    # But preserve warmup period (positions < warmup remain unchanged)","    to_mask = finite_mask & (~keep_mask)","    masked[to_mask] = np.nan","    ","    return masked","",""]}
{"type":"file_footer","path":"src/perf/scenario_control.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/perf/timers.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1814,"sha256":"5835f56b7ac695fc2c06df9ca9efbd6fdee1e2ef1a2199e5f500ecb581a25032","total_lines":62,"chunk_count":1}
{"type":"file_chunk","path":"src/perf/timers.py","chunk_index":0,"line_start":1,"line_end":62,"content":["","\"\"\"","Perf Harness Timer Helper (P2-1.8)","","Provides granular timing breakdown for kernel stages.","\"\"\"","from __future__ import annotations","","import time","from typing import Dict","","","class PerfTimers:","    \"\"\"","    Performance timer helper for granular breakdown.","    ","    Supports multiple start/stop calls for the same timer name (accumulates).","    All timings are in seconds with '_s' suffix.","    \"\"\"","    ","    def __init__(self) -> None:","        self._accumulated: Dict[str, float] = {}","        self._active: Dict[str, float] = {}","    ","    def start(self, name: str) -> None:","        \"\"\"","        Start a timer. If already running, does nothing (no nested timing).","        \"\"\"","        if name not in self._active:","            self._active[name] = time.perf_counter()","    ","    def stop(self, name: str) -> None:","        \"\"\"","        Stop a timer and accumulate the elapsed time.","        If timer was not started, does nothing.","        \"\"\"","        if name in self._active:","            elapsed = time.perf_counter() - self._active[name]","            self._accumulated[name] = self._accumulated.get(name, 0.0) + elapsed","            del self._active[name]","    ","    def as_dict_seconds(self) -> Dict[str, float]:","        \"\"\"","        Return accumulated timings as dict with '_s' suffix keys.","        ","        Returns:","            dict with keys like \"t_xxx_s\": float (seconds)","        \"\"\"","        result: Dict[str, float] = {}","        for name, seconds in self._accumulated.items():","            # Ensure '_s' suffix","            key = name if name.endswith(\"_s\") else f\"{name}_s\"","            result[key] = float(seconds)","        return result","    ","    def get(self, name: str, default: float = 0.0) -> float:","        \"\"\"","        Get accumulated time for a timer name.","        \"\"\"","        return self._accumulated.get(name, default)","",""]}
{"type":"file_footer","path":"src/perf/timers.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/pipeline/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4,"sha256":"545c38b0922de19734fbffde62792c37c2aef6a3216cfa472449173165220f7d","total_lines":4,"chunk_count":1}
{"type":"file_chunk","path":"src/pipeline/__init__.py","chunk_index":0,"line_start":1,"line_end":4,"content":["","","",""]}
{"type":"file_footer","path":"src/pipeline/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/pipeline/funnel.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3794,"sha256":"dec608aec2680f36804d4155d96a542a77b78e45610f7fc10dd411239eb82ca2","total_lines":125,"chunk_count":1}
{"type":"file_chunk","path":"src/pipeline/funnel.py","chunk_index":0,"line_start":1,"line_end":125,"content":["","\"\"\"Funnel orchestrator - Stage0 → Top-K → Stage2 pipeline.","","This is the main entry point for the Phase 4 Funnel pipeline.","It orchestrates the complete flow: proxy ranking → selection → full backtest.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass","from typing import List, Optional","","import numpy as np","","from config.constants import TOPK_K","from pipeline.stage0_runner import Stage0Result, run_stage0","from pipeline.stage2_runner import Stage2Result, run_stage2","from pipeline.topk import select_topk","","","@dataclass(frozen=True)","class FunnelResult:","    \"\"\"","    Complete funnel pipeline result.","    ","    Contains:","    - stage0_results: all Stage0 proxy ranking results","    - topk_param_ids: selected Top-K parameter indices","    - stage2_results: full backtest results for Top-K parameters","    - meta: optional metadata","    \"\"\"","    stage0_results: List[Stage0Result]","    topk_param_ids: List[int]","    stage2_results: List[Stage2Result]","    meta: Optional[dict] = None","","","import warnings","","","def run_funnel(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,","    *,","    k: int = TOPK_K,","    commission: float = 0.0,","    slip: float = 0.0,","    order_qty: int = 1,","    proxy_name: str = \"ma_proxy_v0\",",") -> FunnelResult:","    \"\"\"","    [DEPRECATED] Run complete Funnel pipeline: Stage0 → Top-K → Stage2.","    ","    This function is deprecated in favor of `pipeline.funnel_runner.run_funnel`.","    The new implementation provides better audit logging, artifact writing, and OOM gating.","    ","    Pipeline flow (fixed):","    1. Stage0: proxy ranking on all parameters","    2. Top-K: select top K parameters based on proxy_value","    3. Stage2: full backtest on Top-K subset","    ","    Args:","        open_, high, low, close: OHLC arrays (float64, 1D, same length)","        params_matrix: float64 2D array (n_params, >=3)","            - For Stage0: uses col0 (fast_len), col1 (slow_len) for MA proxy","            - For Stage2: uses col0 (channel_len), col1 (atr_len), col2 (stop_mult) for kernel","        k: number of top parameters to select (default: TOPK_K)","        commission: commission per trade (absolute)","        slip: slippage per trade (absolute)","        order_qty: order quantity (default: 1)","        proxy_name: name of proxy to use for Stage0 (default: ma_proxy_v0)","        ","    Returns:","        FunnelResult containing:","        - stage0_results: all proxy ranking results","        - topk_param_ids: selected Top-K parameter indices","        - stage2_results: full backtest results for Top-K only","        ","    Note:","        - Pipeline is deterministic: same input produces same output","        - Stage0 does NOT compute PnL metrics (only proxy_value)","        - Top-K selection is based solely on proxy_value","        - Stage2 runs full backtest only on Top-K subset","        - DEPRECATED: Use `pipeline.funnel_runner.run_funnel` instead","    \"\"\"","    warnings.warn(","        \"pipeline.funnel.run_funnel is deprecated. \"","        \"Use pipeline.funnel_runner.run_funnel instead.\",","        DeprecationWarning,","        stacklevel=2","    )","    # Step 1: Stage0 - proxy ranking","    stage0_results = run_stage0(","        close,","        params_matrix,","        proxy_name=proxy_name,","    )","    ","    # Step 2: Top-K selection","    topk_param_ids = select_topk(stage0_results, k=k)","    ","    # Step 3: Stage2 - full backtest on Top-K","    stage2_results = run_stage2(","        open_,","        high,","        low,","        close,","        params_matrix,","        topk_param_ids,","        commission=commission,","        slip=slip,","        order_qty=order_qty,","    )","    ","    return FunnelResult(","        stage0_results=stage0_results,","        topk_param_ids=topk_param_ids,","        stage2_results=stage2_results,","        meta=None,","    )","",""]}
{"type":"file_footer","path":"src/pipeline/funnel.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/pipeline/funnel_plan.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1853,"sha256":"9572ebfbe61851569bb9d81ede34121fb5280a242e49f61680a29862c92ce81e","total_lines":58,"chunk_count":1}
{"type":"file_chunk","path":"src/pipeline/funnel_plan.py","chunk_index":0,"line_start":1,"line_end":58,"content":["","\"\"\"Funnel plan builder.","","Builds default funnel plan with three stages:","- Stage 0: Coarse subsample (config rate)","- Stage 1: Increased subsample (min(1.0, stage0_rate * 2))","- Stage 2: Full confirm (1.0)","\"\"\"","","from __future__ import annotations","","from pipeline.funnel_schema import FunnelPlan, StageName, StageSpec","","","def build_default_funnel_plan(cfg: dict) -> FunnelPlan:","    \"\"\"","    Build default funnel plan with three stages.","    ","    Rules (locked):","    - Stage 0: subsample = config's param_subsample_rate (coarse exploration)","    - Stage 1: subsample = min(1.0, stage0_rate * 2) (increased density)","    - Stage 2: subsample = 1.0 (full confirm, mandatory)","    ","    Args:","        cfg: Configuration dictionary containing:","            - param_subsample_rate: Base subsample rate for Stage 0","            - topk_stage0: Optional top-K for Stage 0 (default: 50)","            - topk_stage1: Optional top-K for Stage 1 (default: 20)","    ","    Returns:","        FunnelPlan with three stages","    \"\"\"","    s0_rate = float(cfg[\"param_subsample_rate\"])","    s1_rate = min(1.0, s0_rate * 2.0)","    s2_rate = 1.0  # Stage2 must be 1.0","    ","    return FunnelPlan(stages=[","        StageSpec(","            name=StageName.STAGE0_COARSE,","            param_subsample_rate=s0_rate,","            topk=int(cfg.get(\"topk_stage0\", 50)),","            notes={\"rule\": \"default\", \"description\": \"Coarse exploration\"},","        ),","        StageSpec(","            name=StageName.STAGE1_TOPK,","            param_subsample_rate=s1_rate,","            topk=int(cfg.get(\"topk_stage1\", 20)),","            notes={\"rule\": \"default\", \"description\": \"Top-K refinement\"},","        ),","        StageSpec(","            name=StageName.STAGE2_CONFIRM,","            param_subsample_rate=s2_rate,","            topk=None,","            notes={\"rule\": \"default\", \"description\": \"Full confirmation\"},","        ),","    ])","",""]}
{"type":"file_footer","path":"src/pipeline/funnel_plan.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/pipeline/funnel_runner.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":10557,"sha256":"87d904a2df7404c0367985e99dc2c16a1dbcbbd340b911a66b6fa94be2132963","total_lines":278,"chunk_count":2}
{"type":"file_chunk","path":"src/pipeline/funnel_runner.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Funnel runner - orchestrates stage execution and artifact writing.","","Runs funnel pipeline stages sequentially, writing artifacts for each stage.","Each stage gets its own run_id and run directory.","\"\"\"","","from __future__ import annotations","","import subprocess","from datetime import datetime, timezone","from pathlib import Path","from typing import Any, Dict","","from core.artifacts import write_run_artifacts","from core.audit_schema import AuditSchema, compute_params_effective","from core.config_hash import stable_config_hash","from core.config_snapshot import make_config_snapshot","from core.oom_gate import decide_oom_action","from core.paths import ensure_run_dir","from core.run_id import make_run_id","from data.session.tzdb_info import get_tzdb_info","from pipeline.funnel_plan import build_default_funnel_plan","from pipeline.funnel_schema import FunnelResultIndex, FunnelStageIndex","from pipeline.runner_adapter import run_stage_job","","","def _get_git_info(repo_root: Path | None = None) -> tuple[str, bool]:","    \"\"\"","    Get git SHA and dirty status.","    ","    Args:","        repo_root: Optional path to repo root","        ","    Returns:","        Tuple of (git_sha, dirty_repo)","    \"\"\"","    if repo_root is None:","        repo_root = Path.cwd()","    ","    try:","        # Get git SHA (short, 12 chars)","        result = subprocess.run(","            [\"git\", \"rev-parse\", \"--short=12\", \"HEAD\"],","            cwd=repo_root,","            capture_output=True,","            text=True,","            check=True,","            timeout=5,","        )","        git_sha = result.stdout.strip()","        ","        # Check if repo is dirty","        result_status = subprocess.run(","            [\"git\", \"status\", \"--porcelain\"],","            cwd=repo_root,","            capture_output=True,","            text=True,","            check=True,","            timeout=5,","        )","        dirty_repo = len(result_status.stdout.strip()) > 0","        ","        return git_sha, dirty_repo","    except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):","        return \"unknown\", True","","","def run_funnel(cfg: dict, outputs_root: Path) -> FunnelResultIndex:","    \"\"\"","    Run funnel pipeline with three stages.","    ","    Each stage:","    1. Generates new run_id","    2. Creates run directory","    3. Builds AuditSchema","    4. Runs stage job (via adapter)","    5. Writes artifacts","    ","    Args:","        cfg: Configuration dictionary containing:","            - season: Season identifier","            - dataset_id: Dataset identifier","            - bars: Number of bars","            - params_total: Total parameters","            - param_subsample_rate: Base subsample rate for Stage 0","            - open_, high, low, close: OHLC arrays","            - params_matrix: Parameter matrix","            - commission, slip, order_qty: Trading parameters","            - topk_stage0, topk_stage1: Optional top-K counts","            - git_sha, dirty_repo, created_at: Optional audit fields","        outputs_root: Root outputs directory","    ","    Returns:","        FunnelResultIndex with plan and stage execution indices","    \"\"\"","    # Build funnel plan","    plan = build_default_funnel_plan(cfg)","    ","    # Get git info if not provided","    git_sha = cfg.get(\"git_sha\")","    dirty_repo = cfg.get(\"dirty_repo\")","    if git_sha is None or dirty_repo is None:","        repo_root = cfg.get(\"repo_root\")","        if repo_root:","            repo_root = Path(repo_root)","        git_sha, dirty_repo = _get_git_info(repo_root)","    ","    created_at = cfg.get(\"created_at\")","    if created_at is None:","        created_at = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","    ","    season = cfg[\"season\"]","    dataset_id = cfg[\"dataset_id\"]","    bars = int(cfg[\"bars\"])","    params_total = int(cfg[\"params_total\"])","    ","    stage_indices: list[FunnelStageIndex] = []","    prev_winners: list[dict[str, Any]] = []","    ","    for spec in plan.stages:","        # Generate run_id for this stage","        run_id = make_run_id(prefix=str(spec.name.value))","        ","        # Create run directory","        run_dir = ensure_run_dir(outputs_root, season, run_id)","        ","        # Build stage config (runtime: includes ndarrays for runner_adapter)","        stage_cfg = dict(cfg)","        stage_cfg[\"stage_name\"] = str(spec.name.value)","        stage_cfg[\"param_subsample_rate\"] = float(spec.param_subsample_rate)","        stage_cfg[\"topk\"] = spec.topk","        ","        # Pass previous stage winners to Stage2","        if spec.name.value == \"stage2_confirm\" and prev_winners:","            stage_cfg[\"prev_stage_winners\"] = prev_winners","        ","        # OOM Gate: Check memory limits before running stage","        mem_limit_mb = float(cfg.get(\"mem_limit_mb\", 2048.0))","        allow_auto_downsample = cfg.get(\"allow_auto_downsample\", True)","        auto_downsample_step = float(cfg.get(\"auto_downsample_step\", 0.5))","        auto_downsample_min = float(cfg.get(\"auto_downsample_min\", 0.02))","        ","        gate_result = decide_oom_action(","            stage_cfg,","            mem_limit_mb=mem_limit_mb,","            allow_auto_downsample=allow_auto_downsample,","            auto_downsample_step=auto_downsample_step,","            auto_downsample_min=auto_downsample_min,","        )","        ","        # Handle gate actions","        if gate_result[\"action\"] == \"BLOCK\":","            raise RuntimeError(","                f\"OOM Gate BLOCKED stage {spec.name.value}: {gate_result['reason']}\"","            )","        ","        # Planned subsample for this stage (before gate adjustment)","        planned_subsample = float(spec.param_subsample_rate)","        final_subsample = gate_result[\"final_subsample\"]","        ","        # SSOT: Use new_cfg from gate_result (never mutate original stage_cfg)","        stage_cfg = gate_result[\"new_cfg\"]","        ","        # Use final_subsample for all calculations","        effective_subsample = final_subsample","        ","        # Create sanitized snapshot (for hash and artifacts, excludes ndarrays)","        # Snapshot must reflect final subsample (after auto-downsample if any)","        stage_snapshot = make_config_snapshot(stage_cfg)","        ","        # Compute config hash (only on sanitized snapshot)","        config_hash = stable_config_hash(stage_snapshot)","        ","        # Compute params_effective with final subsample","        params_effective = compute_params_effective(params_total, effective_subsample)","        ","        # Build AuditSchema (must use final subsample)","        audit = AuditSchema(","            run_id=run_id,","            created_at=created_at,","            git_sha=git_sha,","            dirty_repo=bool(dirty_repo),","            param_subsample_rate=effective_subsample,  # Use final subsample","            config_hash=config_hash,","            season=season,","            dataset_id=dataset_id,","            bars=bars,","            params_total=params_total,","            params_effective=params_effective,","            artifact_version=\"v1\",","        )","        ","        # Run stage job (adapter returns data only, no file I/O)","        # Use stage_cfg which has final subsample (after auto-downsample if any)","        stage_out = run_stage_job(stage_cfg)","        ","        # Extract metrics and winners","        stage_metrics = dict(stage_out.get(\"metrics\", {}))","        stage_winners = stage_out.get(\"winners\", {\"topk\": [], \"notes\": {\"schema\": \"v1\"}})"]}
{"type":"file_chunk","path":"src/pipeline/funnel_runner.py","chunk_index":1,"line_start":201,"line_end":278,"content":["        ","        # Ensure metrics include required fields","        stage_metrics[\"param_subsample_rate\"] = effective_subsample  # Use final subsample","        stage_metrics[\"params_effective\"] = params_effective","        stage_metrics[\"params_total\"] = params_total","        stage_metrics[\"bars\"] = bars","        stage_metrics[\"stage_name\"] = str(spec.name.value)","        ","        # Add OOM gate fields (mandatory for audit)","        stage_metrics[\"oom_gate_action\"] = gate_result[\"action\"]","        stage_metrics[\"oom_gate_reason\"] = gate_result[\"reason\"]","        stage_metrics[\"mem_est_mb\"] = gate_result[\"estimates\"][\"mem_est_mb\"]","        stage_metrics[\"mem_limit_mb\"] = mem_limit_mb","        stage_metrics[\"ops_est\"] = gate_result[\"estimates\"][\"ops_est\"]","        ","        # Record planned subsample (before gate adjustment)","        stage_metrics[\"stage_planned_subsample\"] = planned_subsample","        ","        # If auto-downsample occurred, record original and final subsample","        if gate_result[\"action\"] == \"AUTO_DOWNSAMPLE\":","            stage_metrics[\"oom_gate_original_subsample\"] = planned_subsample","            stage_metrics[\"oom_gate_final_subsample\"] = final_subsample","        ","        # Phase 6.6: Add tzdb metadata to manifest","        manifest_dict = audit.to_dict()","        tzdb_provider, tzdb_version = get_tzdb_info()","        manifest_dict[\"tzdb_provider\"] = tzdb_provider","        manifest_dict[\"tzdb_version\"] = tzdb_version","        ","        # Add data_tz and exchange_tz if available in config","        # These come from session profile if session processing is used","        if \"data_tz\" in stage_cfg:","            manifest_dict[\"data_tz\"] = stage_cfg[\"data_tz\"]","        if \"exchange_tz\" in stage_cfg:","            manifest_dict[\"exchange_tz\"] = stage_cfg[\"exchange_tz\"]","        ","        # Phase 7: Add strategy metadata if available","        if \"strategy_id\" in stage_cfg:","            import json","            import hashlib","            ","            manifest_dict[\"strategy_id\"] = stage_cfg[\"strategy_id\"]","            ","            if \"strategy_version\" in stage_cfg:","                manifest_dict[\"strategy_version\"] = stage_cfg[\"strategy_version\"]","            ","            if \"param_schema\" in stage_cfg:","                param_schema = stage_cfg[\"param_schema\"]","                # Compute hash of param_schema","                schema_json = json.dumps(param_schema, sort_keys=True)","                schema_hash = hashlib.sha1(schema_json.encode(\"utf-8\")).hexdigest()","                manifest_dict[\"param_schema_hash\"] = schema_hash","        ","        # Write artifacts (unified artifact system)","        # Use sanitized snapshot (not runtime cfg with ndarrays)","        write_run_artifacts(","            run_dir=run_dir,","            manifest=manifest_dict,","            config_snapshot=stage_snapshot,","            metrics=stage_metrics,","            winners=stage_winners,","        )","        ","        # Record stage index","        stage_indices.append(","            FunnelStageIndex(","                stage=spec.name,","                run_id=run_id,","                run_dir=str(run_dir.relative_to(outputs_root)),","            )","        )","        ","        # Save winners for next stage","        prev_winners = stage_winners.get(\"topk\", [])","    ","    return FunnelResultIndex(plan=plan, stages=stage_indices)","",""]}
{"type":"file_footer","path":"src/pipeline/funnel_runner.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/pipeline/funnel_schema.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1667,"sha256":"91738ad1a37ceebd88358d1ff410043fa547764f5fa89a5786268845010dcbb1","total_lines":75,"chunk_count":1}
{"type":"file_chunk","path":"src/pipeline/funnel_schema.py","chunk_index":0,"line_start":1,"line_end":75,"content":["","\"\"\"Funnel schema definitions.","","Defines stage names, specifications, and result indexing for funnel pipeline.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, field","from enum import Enum","from typing import Any, Dict, List, Optional","","","class StageName(str, Enum):","    \"\"\"Stage names for funnel pipeline.\"\"\"","    STAGE0_COARSE = \"stage0_coarse\"","    STAGE1_TOPK = \"stage1_topk\"","    STAGE2_CONFIRM = \"stage2_confirm\"","","","@dataclass(frozen=True)","class StageSpec:","    \"\"\"","    Stage specification for funnel pipeline.","    ","    Each stage defines:","    - name: Stage identifier","    - param_subsample_rate: Subsample rate for this stage","    - topk: Optional top-K count (None for Stage2)","    - notes: Additional metadata","    \"\"\"","    name: StageName","    param_subsample_rate: float","    topk: Optional[int] = None","    notes: Dict[str, Any] = field(default_factory=dict)","","","@dataclass(frozen=True)","class FunnelPlan:","    \"\"\"","    Funnel plan containing ordered list of stages.","    ","    Stages are executed in order: Stage0 -> Stage1 -> Stage2","    \"\"\"","    stages: List[StageSpec]","","","@dataclass(frozen=True)","class FunnelStageIndex:","    \"\"\"","    Index entry for a single stage execution.","    ","    Records:","    - stage: Stage name","    - run_id: Run ID for this stage","    - run_dir: Relative path to run directory","    \"\"\"","    stage: StageName","    run_id: str","    run_dir: str  # Relative path string","","","@dataclass(frozen=True)","class FunnelResultIndex:","    \"\"\"","    Complete funnel execution result index.","    ","    Contains:","    - plan: Original funnel plan","    - stages: List of stage execution indices","    \"\"\"","    plan: FunnelPlan","    stages: List[FunnelStageIndex]","",""]}
{"type":"file_footer","path":"src/pipeline/funnel_schema.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/pipeline/governance_eval.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":22111,"sha256":"43553c5a9f2df483ddb8431c20a232b04347920811739b0f6a7613e11183fa06","total_lines":633,"chunk_count":4}
{"type":"file_chunk","path":"src/pipeline/governance_eval.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Governance evaluator - rule engine for candidate decisions.","","Reads artifacts from stage run directories and applies governance rules","to produce KEEP/FREEZE/DROP decisions for each candidate.","\"\"\"","","from __future__ import annotations","","from datetime import datetime, timezone","from pathlib import Path","from typing import Any, Dict, List, Optional, Tuple","","from core.artifact_reader import (","    read_config_snapshot,","    read_manifest,","    read_metrics,","    read_winners,",")","from core.config_hash import stable_config_hash","from core.governance_schema import (","    Decision,","    EvidenceRef,","    GovernanceItem,","    GovernanceReport,",")","from core.winners_schema import is_winners_v2","","","# Rule thresholds (MVP - locked)","R2_DEGRADE_THRESHOLD = 0.20  # 20% degradation threshold for R2","R3_DENSITY_THRESHOLD = 3  # Minimum count for R3 FREEZE (same strategy_id)","","","def normalize_candidate(","    item: Dict[str, Any],","    config_snapshot: Optional[Dict[str, Any]] = None,","    is_v2: bool = False,",") -> Tuple[str, Dict[str, Any], Dict[str, Any]]:","    \"\"\"","    Normalize candidate from winners.json to (strategy_id, params_dict, metrics_subset).","    ","    Handles both v2 and legacy formats gracefully.","    ","    Args:","        item: Candidate item from winners.json topk list","        config_snapshot: Optional config snapshot to extract params from","        is_v2: Whether item is from v2 schema (fast path)","        ","    Returns:","        Tuple of (strategy_id, params_dict, metrics_subset)","        - strategy_id: Strategy identifier","        - params_dict: Normalized params dict","        - metrics_subset: Metrics dict extracted from item","    \"\"\"","    # Fast path for v2 schema","    if is_v2:","        strategy_id = item.get(\"strategy_id\", \"unknown\")","        params_dict = item.get(\"params\", {})","        ","        # Extract metrics from v2 structure","        metrics_subset = {}","        metrics = item.get(\"metrics\", {})","        ","        # Legacy fields (for backward compatibility)","        if \"net_profit\" in metrics:","            metrics_subset[\"net_profit\"] = float(metrics[\"net_profit\"])","        if \"trades\" in metrics:","            metrics_subset[\"trades\"] = int(metrics[\"trades\"])","        if \"max_dd\" in metrics:","            metrics_subset[\"max_dd\"] = float(metrics[\"max_dd\"])","        if \"proxy_value\" in metrics:","            metrics_subset[\"proxy_value\"] = float(metrics[\"proxy_value\"])","        ","        # Also check top-level (legacy compatibility)","        if \"net_profit\" in item:","            metrics_subset[\"net_profit\"] = float(item[\"net_profit\"])","        if \"trades\" in item:","            metrics_subset[\"trades\"] = int(item[\"trades\"])","        if \"max_dd\" in item:","            metrics_subset[\"max_dd\"] = float(item[\"max_dd\"])","        if \"proxy_value\" in item:","            metrics_subset[\"proxy_value\"] = float(item[\"proxy_value\"])","        ","        return strategy_id, params_dict, metrics_subset","    ","    # Legacy path (backward compatibility)","    # Extract metrics subset (varies by stage)","    metrics_subset = {}","    if \"proxy_value\" in item:","        metrics_subset[\"proxy_value\"] = float(item[\"proxy_value\"])","    if \"net_profit\" in item:","        metrics_subset[\"net_profit\"] = float(item[\"net_profit\"])","    if \"trades\" in item:","        metrics_subset[\"trades\"] = int(item[\"trades\"])","    if \"max_dd\" in item:","        metrics_subset[\"max_dd\"] = float(item[\"max_dd\"])","    ","    # MVP: Use fixed strategy_id (donchian_atr)","    # Future: Extract from config_snapshot or item metadata","    strategy_id = \"donchian_atr\"","    ","    # Extract params_dict","    # Priority: 1) item[\"params\"], 2) config_snapshot params, 3) fallback to param_id-based dict","    params_dict = item.get(\"params\", {})","    ","    if not params_dict and config_snapshot:","        # Try to extract from config_snapshot","        # MVP: If params_matrix is in config_snapshot, extract row by param_id","        # For now, use param_id as fallback","        param_id = item.get(\"param_id\")","        if param_id is not None:","            # MVP fallback: Create minimal params dict from param_id","            # Future: Extract actual params from params_matrix in config_snapshot","            params_dict = {\"param_id\": int(param_id)}","    ","    if not params_dict:","        # Final fallback: use param_id if available","        param_id = item.get(\"param_id\")","        if param_id is not None:","            params_dict = {\"param_id\": int(param_id)}","        else:","            params_dict = {}","    ","    return strategy_id, params_dict, metrics_subset","","","def generate_candidate_id(strategy_id: str, params_dict: Dict[str, Any]) -> str:","    \"\"\"","    Generate stable candidate_id from strategy_id and params_dict.","    ","    Format: {strategy_id}:{params_hash[:12]}","    ","    Args:","        strategy_id: Strategy identifier","        params_dict: Parameters dict (must be JSON-serializable)","        ","    Returns:","        Stable candidate_id string","    \"\"\"","    # Compute stable hash of params_dict","    params_hash = stable_config_hash(params_dict)","    ","    # Use first 12 chars of hash","    hash_short = params_hash[:12]","    ","    return f\"{strategy_id}:{hash_short}\"","","","def find_stage2_candidate(","    candidate_param_id: int,","    stage2_winners: List[Dict[str, Any]],",") -> Optional[Dict[str, Any]]:","    \"\"\"","    Find Stage2 candidate matching param_id.","    ","    Args:","        candidate_param_id: param_id from Stage1 winner","        stage2_winners: List of Stage2 winners","        ","    Returns:","        Matching Stage2 candidate dict, or None if not found","    \"\"\"","    for item in stage2_winners:","        if item.get(\"param_id\") == candidate_param_id:","            return item","    return None","","","def extract_key_metric(","    metrics: Dict[str, Any],","    candidate_metrics: Dict[str, Any],","    metric_name: str,",") -> Optional[float]:","    \"\"\"","    Extract key metric with fallback logic.","    ","    Priority:","    1. candidate_metrics[metric_name]","    2. metrics[metric_name]","    3. Fallback: net_profit / max_dd (if both exist)","    4. None","    ","    Args:","        metrics: Stage metrics dict","        candidate_metrics: Candidate-specific metrics dict","        metric_name: Metric name to extract","        ","    Returns:","        Metric value (float), or None if not found","    \"\"\"","    # Try candidate_metrics first","    if metric_name in candidate_metrics:","        val = candidate_metrics[metric_name]","        if isinstance(val, (int, float)):","            return float(val)","    ","    # Try stage metrics","    if metric_name in metrics:","        val = metrics[metric_name]"]}
{"type":"file_chunk","path":"src/pipeline/governance_eval.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        if isinstance(val, (int, float)):","            return float(val)","    ","    # Fallback: net_profit / max_dd (if both exist)","    if metric_name in (\"finalscore\", \"net_over_mdd\"):","        net_profit = candidate_metrics.get(\"net_profit\") or metrics.get(\"net_profit\")","        max_dd = candidate_metrics.get(\"max_dd\") or metrics.get(\"max_dd\")","        if net_profit is not None and max_dd is not None:","            if abs(max_dd) > 1e-10:  # Avoid division by zero","                return float(net_profit) / abs(float(max_dd))","            elif float(net_profit) > 0:","                return float(\"inf\")  # Positive profit with zero DD","            else:","                return float(\"-inf\")  # Negative profit with zero DD","    ","    return None","","","def apply_rule_r1(","    candidate: Dict[str, Any],","    stage2_winners: List[Dict[str, Any]],","    is_v2: bool = False,",") -> Tuple[bool, str]:","    \"\"\"","    Rule R1: Evidence completeness.","    ","    If candidate appears in Stage1 winners but:","    - Cannot find corresponding Stage2 metrics (or Stage2 did not run successfully)","    -> DROP (reason: unverified)","    ","    Args:","        candidate: Candidate from Stage1 winners","        stage2_winners: List of Stage2 winners","        is_v2: Whether candidates are v2 schema","        ","    Returns:","        Tuple of (should_drop, reason)","    \"\"\"","    # For v2: use candidate_id for matching","    if is_v2:","        candidate_id = candidate.get(\"candidate_id\")","        if candidate_id is None:","            return True, \"missing_candidate_id\"","        ","        # Find matching candidate by candidate_id","        for item in stage2_winners:","            if item.get(\"candidate_id\") == candidate_id:","                return False, \"\"","        ","        return True, \"unverified\"","    ","    # Legacy path: use param_id","    param_id = candidate.get(\"param_id\")","    if param_id is None:","        # Try to extract from source (v2 fallback)","        source = candidate.get(\"source\", {})","        param_id = source.get(\"param_id\")","        if param_id is None:","            # Try metrics (v2 fallback)","            metrics = candidate.get(\"metrics\", {})","            param_id = metrics.get(\"param_id\")","            if param_id is None:","                return True, \"missing_param_id\"","    ","    stage2_match = find_stage2_candidate(param_id, stage2_winners)","    if stage2_match is None:","        return True, \"unverified\"","    ","    return False, \"\"","","","def apply_rule_r2(","    candidate: Dict[str, Any],","    stage1_metrics: Dict[str, Any],","    stage2_candidate: Dict[str, Any],","    stage2_metrics: Dict[str, Any],",") -> Tuple[bool, str]:","    \"\"\"","    Rule R2: Confirm stability.","    ","    If candidate's key metrics degrade > threshold in Stage2 vs Stage1 -> DROP.","    ","    Priority:","    1. finalscore or net_over_mdd","    2. Fallback: net_profit / max_dd","    ","    Args:","        candidate: Candidate from Stage1 winners","        stage1_metrics: Stage1 metrics dict","        stage2_candidate: Matching Stage2 candidate","        stage2_metrics: Stage2 metrics dict","        ","    Returns:","        Tuple of (should_drop, reason)","    \"\"\"","    # Extract Stage1 metric","    stage1_val = extract_key_metric(","        stage1_metrics,","        candidate,","        \"finalscore\",","    )","    if stage1_val is None:","        stage1_val = extract_key_metric(","            stage1_metrics,","            candidate,","            \"net_over_mdd\",","        )","    if stage1_val is None:","        # Fallback: net_profit / max_dd","        stage1_val = extract_key_metric(","            stage1_metrics,","            candidate,","            \"net_over_mdd\",","        )","    ","    # Extract Stage2 metric","    stage2_val = extract_key_metric(","        stage2_metrics,","        stage2_candidate,","        \"finalscore\",","    )","    if stage2_val is None:","        stage2_val = extract_key_metric(","            stage2_metrics,","            stage2_candidate,","            \"net_over_mdd\",","        )","    if stage2_val is None:","        # Fallback: net_profit / max_dd","        stage2_val = extract_key_metric(","            stage2_metrics,","            stage2_candidate,","            \"net_over_mdd\",","        )","    ","    # If either metric is missing, cannot apply R2","    if stage1_val is None or stage2_val is None:","        return False, \"\"","    ","    # Check degradation","    if stage1_val == 0.0:","        # Avoid division by zero","        if stage2_val < 0.0:","            return True, f\"degraded_from_zero_to_negative\"","        return False, \"\"","    ","    degradation_ratio = (stage1_val - stage2_val) / abs(stage1_val)","    if degradation_ratio > R2_DEGRADE_THRESHOLD:","        return True, f\"degraded_{degradation_ratio:.2%}\"","    ","    return False, \"\"","","","def apply_rule_r3(","    candidate: Dict[str, Any],","    all_stage1_winners: List[Dict[str, Any]],",") -> Tuple[bool, str]:","    \"\"\"","    Rule R3: Plateau hint (MVP simplified version).","    ","    If same strategy_id appears >= threshold times in Stage1 topk -> FREEZE.","    ","    MVP version: Count occurrences of same strategy_id (simplified).","    Future: Geometric distance/clustering analysis.","    ","    Args:","        candidate: Candidate from Stage1 winners","        all_stage1_winners: All Stage1 winners (for density calculation)","        ","    Returns:","        Tuple of (should_freeze, reason)","    \"\"\"","    strategy_id, _, _ = normalize_candidate(candidate)","    ","    # Count occurrences of same strategy_id","    count = 0","    for item in all_stage1_winners:","        item_strategy_id, _, _ = normalize_candidate(item)","        if item_strategy_id == strategy_id:","            count += 1","    ","    if count >= R3_DENSITY_THRESHOLD:","        return True, f\"density_{count}_over_threshold_{R3_DENSITY_THRESHOLD}\"","    ","    return False, \"\"","","","def evaluate_governance(","    *,","    stage0_dir: Path,","    stage1_dir: Path,","    stage2_dir: Path,",") -> GovernanceReport:","    \"\"\"","    Evaluate governance rules on candidates from Stage1 winners.","    ","    Reads artifacts from three stage directories and applies rules:","    - R1: Evidence completeness (DROP if Stage2 missing)","    - R2: Confirm stability (DROP if metrics degrade > threshold)","    - R3: Plateau hint (FREEZE if density over threshold)"]}
{"type":"file_chunk","path":"src/pipeline/governance_eval.py","chunk_index":2,"line_start":401,"line_end":600,"content":["    ","    Args:","        stage0_dir: Path to Stage0 run directory","        stage1_dir: Path to Stage1 run directory","        stage2_dir: Path to Stage2 run directory","        ","    Returns:","        GovernanceReport with decisions for each candidate","    \"\"\"","    # Read artifacts","    stage0_manifest = read_manifest(stage0_dir)","    stage0_metrics = read_metrics(stage0_dir)","    stage0_winners = read_winners(stage0_dir)","    stage0_config = read_config_snapshot(stage0_dir)","    ","    stage1_manifest = read_manifest(stage1_dir)","    stage1_metrics = read_metrics(stage1_dir)","    stage1_winners = read_winners(stage1_dir)","    stage1_config = read_config_snapshot(stage1_dir)","    ","    stage2_manifest = read_manifest(stage2_dir)","    stage2_metrics = read_metrics(stage2_dir)","    stage2_winners = read_winners(stage2_dir)","    stage2_config = read_config_snapshot(stage2_dir)","    ","    # Extract candidates from Stage1 winners (topk)","    stage1_topk = stage1_winners.get(\"topk\", [])","    ","    # Check if winners is v2 schema","    stage1_is_v2 = is_winners_v2(stage1_winners)","    ","    # Get git_sha and created_at from Stage1 manifest","    git_sha = stage1_manifest.get(\"git_sha\", \"unknown\")","    created_at = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","    ","    # Build governance items","    items: List[GovernanceItem] = []","    ","    for candidate in stage1_topk:","        # Normalize candidate (pass stage1_config for params extraction, and is_v2 flag)","        strategy_id, params_dict, metrics_subset = normalize_candidate(","            candidate, stage1_config, is_v2=stage1_is_v2","        )","        ","        # Generate candidate_id","        candidate_id = generate_candidate_id(strategy_id, params_dict)","        ","        # Apply rules","        reasons: List[str] = []","        evidence: List[EvidenceRef] = []","        decision = Decision.KEEP  # Default","        ","        # R1: Evidence completeness","        # Check if Stage2 is v2 (for candidate matching)","        stage2_is_v2 = is_winners_v2(stage2_winners)","        should_drop_r1, reason_r1 = apply_rule_r1(","            candidate, stage2_winners.get(\"topk\", []), is_v2=stage2_is_v2","        )","        if should_drop_r1:","            decision = Decision.DROP","            reasons.append(f\"R1: {reason_r1}\")","            # Add evidence","            evidence.append(","                EvidenceRef(","                    run_id=stage1_manifest.get(\"run_id\", \"unknown\"),","                    stage_name=\"stage1_topk\",","                    artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","                    key_metrics={","                        \"param_id\": candidate.get(\"param_id\"),","                        **metrics_subset,","                    },","                )","            )","            # Create item and continue (no need to check R2/R3)","            items.append(","                GovernanceItem(","                    candidate_id=candidate_id,","                    decision=decision,","                    reasons=reasons,","                    evidence=evidence,","                    created_at=created_at,","                    git_sha=git_sha,","                )","            )","            continue","        ","        # R2: Confirm stability","        # Find Stage2 candidate (support both v2 and legacy)","        if stage1_is_v2:","            candidate_id = candidate.get(\"candidate_id\")","            stage2_candidate = None","            if candidate_id:","                for item in stage2_winners.get(\"topk\", []):","                    if item.get(\"candidate_id\") == candidate_id:","                        stage2_candidate = item","                        break","        else:","            param_id = candidate.get(\"param_id\")","            if param_id is None:","                # Try source/metrics fallback","                source = candidate.get(\"source\", {})","                param_id = source.get(\"param_id\") or candidate.get(\"metrics\", {}).get(\"param_id\")","            stage2_candidate = find_stage2_candidate(","                param_id,","                stage2_winners.get(\"topk\", []),","            ) if param_id is not None else None","        if stage2_candidate is not None:","            should_drop_r2, reason_r2 = apply_rule_r2(","                candidate,","                stage1_metrics,","                stage2_candidate,","                stage2_metrics,","            )","            if should_drop_r2:","                decision = Decision.DROP","                reasons.append(f\"R2: {reason_r2}\")","                # Add evidence","                evidence.append(","                    EvidenceRef(","                        run_id=stage1_manifest.get(\"run_id\", \"unknown\"),","                        stage_name=\"stage1_topk\",","                        artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","                        key_metrics={","                            \"param_id\": candidate.get(\"param_id\"),","                            **metrics_subset,","                        },","                    )","                )","                evidence.append(","                    EvidenceRef(","                        run_id=stage2_manifest.get(\"run_id\", \"unknown\"),","                        stage_name=\"stage2_confirm\",","                        artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","                        key_metrics={","                            \"param_id\": stage2_candidate.get(\"param_id\"),","                            \"net_profit\": stage2_candidate.get(\"net_profit\"),","                            \"trades\": stage2_candidate.get(\"trades\"),","                            \"max_dd\": stage2_candidate.get(\"max_dd\"),","                        },","                    )","                )","                # Create item and continue (no need to check R3)","                items.append(","                    GovernanceItem(","                        candidate_id=candidate_id,","                        decision=decision,","                        reasons=reasons,","                        evidence=evidence,","                        created_at=created_at,","                        git_sha=git_sha,","                    )","                )","                continue","        ","        # R3: Plateau hint (needs normalized strategy_id)","        should_freeze_r3, reason_r3 = apply_rule_r3(candidate, stage1_topk)","        if should_freeze_r3:","            decision = Decision.FREEZE","            reasons.append(f\"R3: {reason_r3}\")","        ","        # Add evidence (always include Stage1 and Stage2 if available)","        evidence.append(","            EvidenceRef(","                run_id=stage1_manifest.get(\"run_id\", \"unknown\"),","                stage_name=\"stage1_topk\",","                artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\", \"config_snapshot.json\"],","                key_metrics={","                    \"param_id\": candidate.get(\"param_id\"),","                    **metrics_subset,","                    \"stage_planned_subsample\": stage1_metrics.get(\"stage_planned_subsample\"),","                    \"param_subsample_rate\": stage1_metrics.get(\"param_subsample_rate\"),","                    \"params_effective\": stage1_metrics.get(\"params_effective\"),","                },","            )","        )","        if stage2_candidate is not None:","            evidence.append(","                EvidenceRef(","                    run_id=stage2_manifest.get(\"run_id\", \"unknown\"),","                    stage_name=\"stage2_confirm\",","                    artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\", \"config_snapshot.json\"],","                    key_metrics={","                        \"param_id\": stage2_candidate.get(\"param_id\"),","                        \"net_profit\": stage2_candidate.get(\"net_profit\"),","                        \"trades\": stage2_candidate.get(\"trades\"),","                        \"max_dd\": stage2_candidate.get(\"max_dd\"),","                        \"param_subsample_rate\": stage2_metrics.get(\"param_subsample_rate\"),","                        \"params_effective\": stage2_metrics.get(\"params_effective\"),","                    },","                )","            )","        ","        # Create item","        items.append(","            GovernanceItem(","                candidate_id=candidate_id,","                decision=decision,","                reasons=reasons,","                evidence=evidence,","                created_at=created_at,"]}
{"type":"file_chunk","path":"src/pipeline/governance_eval.py","chunk_index":3,"line_start":601,"line_end":633,"content":["                git_sha=git_sha,","            )","        )","    ","    # Build metadata","    # Extract data_fingerprint_sha1 from manifests (prefer Stage1, fallback to others)","    data_fingerprint_sha1 = (","        stage1_manifest.get(\"data_fingerprint_sha1\") or","        stage0_manifest.get(\"data_fingerprint_sha1\") or","        stage2_manifest.get(\"data_fingerprint_sha1\") or","        \"\"","    )","    ","    metadata = {","        \"governance_id\": stage1_manifest.get(\"run_id\", \"unknown\"),  # Use Stage1 run_id as base","        \"season\": stage1_manifest.get(\"season\", \"unknown\"),","        \"created_at\": created_at,","        \"git_sha\": git_sha,","        \"data_fingerprint_sha1\": data_fingerprint_sha1,  # Phase 6.5: Mandatory fingerprint","        \"stage0_run_id\": stage0_manifest.get(\"run_id\", \"unknown\"),","        \"stage1_run_id\": stage1_manifest.get(\"run_id\", \"unknown\"),","        \"stage2_run_id\": stage2_manifest.get(\"run_id\", \"unknown\"),","        \"total_candidates\": len(items),","        \"decisions\": {","            \"KEEP\": sum(1 for item in items if item.decision == Decision.KEEP),","            \"FREEZE\": sum(1 for item in items if item.decision == Decision.FREEZE),","            \"DROP\": sum(1 for item in items if item.decision == Decision.DROP),","        },","    }","    ","    return GovernanceReport(items=items, metadata=metadata)","",""]}
{"type":"file_footer","path":"src/pipeline/governance_eval.py","complete":true,"emitted_chunks":4}
{"type":"file_header","path":"src/pipeline/metrics_schema.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":434,"sha256":"b0834c86d116fe97b6cf18ad1de87825e484f6927c9a8786d0a102af2ef873ea","total_lines":21,"chunk_count":1}
{"type":"file_chunk","path":"src/pipeline/metrics_schema.py","chunk_index":0,"line_start":1,"line_end":21,"content":["","from __future__ import annotations","","\"\"\"","Metrics column schema (single source of truth).","","Defines the column order for metrics arrays returned by run_grid().","\"\"\"","","# Column indices for metrics array (n_params, 3)","METRICS_COL_NET_PROFIT = 0","METRICS_COL_TRADES = 1","METRICS_COL_MAX_DD = 2","","# Column names (for documentation/debugging)","METRICS_COLUMN_NAMES = [\"net_profit\", \"trades\", \"max_dd\"]","","# Number of columns","METRICS_N_COLUMNS = 3","",""]}
{"type":"file_footer","path":"src/pipeline/metrics_schema.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/pipeline/param_sort.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":842,"sha256":"5a1587b17dce3884102e38513f384bf82d436e0ec44846f3f5fef4e96c2f627a","total_lines":35,"chunk_count":1}
{"type":"file_chunk","path":"src/pipeline/param_sort.py","chunk_index":0,"line_start":1,"line_end":35,"content":["","from __future__ import annotations","","import numpy as np","","","def sort_params_cache_friendly(params: np.ndarray) -> tuple[np.ndarray, np.ndarray]:","    \"\"\"","    Cache-friendly sorting for parameter matrix.","","    params: shape (n, k) float64.","      Convention (Phase 3B v1):","        col0 = channel_len","        col1 = atr_len","        col2 = stop_mult","","    Returns:","      sorted_params: params reordered (view/copy depending on numpy)","      order: indices such that sorted_params = params[order]","    \"\"\"","    if params.ndim != 2 or params.shape[1] < 3:","        raise ValueError(\"params must be (n, >=3) array\")","","    # Primary: channel_len (int-like)","    # Secondary: atr_len (int-like)","    # Tertiary: stop_mult","    ch = params[:, 0]","    atr = params[:, 1]","    sm = params[:, 2]","","    order = np.lexsort((sm, atr, ch))","    return params[order], order","","",""]}
{"type":"file_footer","path":"src/pipeline/param_sort.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/pipeline/portfolio_runner.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2023,"sha256":"59c1ff8e1acdaddc97bea72da53692aad6a3b99aa18a20384fdfa43745ebab3c","total_lines":69,"chunk_count":1}
{"type":"file_chunk","path":"src/pipeline/portfolio_runner.py","chunk_index":0,"line_start":1,"line_end":69,"content":["","\"\"\"Portfolio runner - compile and write portfolio artifacts.","","Phase 8: Load, validate, compile, and write portfolio artifacts.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","from typing import Dict, Any","","from portfolio.artifacts import write_portfolio_artifacts","from portfolio.compiler import compile_portfolio","from portfolio.loader import load_portfolio_spec","from portfolio.validate import validate_portfolio_spec","","","def run_portfolio(spec_path: Path, outputs_root: Path) -> Dict[str, Any]:","    \"\"\"Run portfolio compilation pipeline.","    ","    Process:","    1. Load portfolio spec","    2. Validate spec","    3. Compile jobs","    4. Write portfolio artifacts","    ","    Args:","        spec_path: Path to portfolio spec file","        outputs_root: Root outputs directory","        ","    Returns:","        Dict with:","            - portfolio_id: Portfolio ID","            - portfolio_version: Portfolio version","            - portfolio_hash: Portfolio hash","            - artifacts: Dict mapping artifact names to relative paths","            - artifacts_dir: Absolute path to artifacts directory","    \"\"\"","    # Load spec","    spec = load_portfolio_spec(spec_path)","    ","    # Validate spec","    validate_portfolio_spec(spec)","    ","    # Compile jobs","    jobs = compile_portfolio(spec)","    ","    # Determine artifacts directory","    # Format: outputs_root/portfolios/{portfolio_id}/{version}/","    artifacts_dir = outputs_root / \"portfolios\" / spec.portfolio_id / spec.version","    artifacts_dir.mkdir(parents=True, exist_ok=True)","    ","    # Write artifacts","    artifact_paths = write_portfolio_artifacts(spec, jobs, artifacts_dir)","    ","    # Compute hash","    from portfolio.artifacts import compute_portfolio_hash","    portfolio_hash = compute_portfolio_hash(spec)","    ","    return {","        \"portfolio_id\": spec.portfolio_id,","        \"portfolio_version\": spec.version,","        \"portfolio_hash\": portfolio_hash,","        \"artifacts\": artifact_paths,","        \"artifacts_dir\": str(artifacts_dir),","        \"jobs_count\": len(jobs),","    }","",""]}
{"type":"file_footer","path":"src/pipeline/portfolio_runner.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/pipeline/runner_adapter.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8761,"sha256":"6cdbf85ad195979d4ba95ff69bdc11b69292f306d94b8e90309059a908863057","total_lines":286,"chunk_count":2}
{"type":"file_chunk","path":"src/pipeline/runner_adapter.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Runner adapter for funnel pipeline.","","Provides unified interface to existing runners without exposing engine details.","Adapter returns data only (no file I/O) - all file writing is done by artifacts system.","\"\"\"","","from __future__ import annotations","","from typing import Any, Dict, List","","import numpy as np","","from pipeline.runner_grid import run_grid","from pipeline.stage0_runner import run_stage0","from pipeline.stage2_runner import run_stage2","from pipeline.topk import select_topk","","","def _coerce_1d_float64(x):","    if isinstance(x, np.ndarray):","        return x.astype(np.float64, copy=False)","    return np.asarray(x, dtype=np.float64)","","","def _coerce_2d_float64(x):","    if isinstance(x, np.ndarray):","        return x.astype(np.float64, copy=False)","    return np.asarray(x, dtype=np.float64)","","","def _coerce_arrays(cfg: dict) -> dict:","    # in-place is ok (stage_cfg is per-stage copy anyway)","    if \"open_\" in cfg:","        cfg[\"open_\"] = _coerce_1d_float64(cfg[\"open_\"])","    if \"high\" in cfg:","        cfg[\"high\"] = _coerce_1d_float64(cfg[\"high\"])","    if \"low\" in cfg:","        cfg[\"low\"] = _coerce_1d_float64(cfg[\"low\"])","    if \"close\" in cfg:","        cfg[\"close\"] = _coerce_1d_float64(cfg[\"close\"])","    if \"params_matrix\" in cfg:","        cfg[\"params_matrix\"] = _coerce_2d_float64(cfg[\"params_matrix\"])","    return cfg","","","def run_stage_job(stage_cfg: dict) -> dict:","    \"\"\"","    Run a stage job and return metrics and winners.","    ","    This adapter wraps existing runners (run_grid, run_stage0, run_stage2)","    to provide a unified interface. It does NOT write any files - all file","    writing must be done by the artifacts system.","    ","    Args:","        stage_cfg: Stage configuration dictionary containing:","            - stage_name: Stage identifier (\"stage0_coarse\", \"stage1_topk\", \"stage2_confirm\")","            - param_subsample_rate: Subsample rate for this stage","            - topk: Optional top-K count (for Stage0/1)","            - open_, high, low, close: OHLC arrays","            - params_matrix: Parameter matrix","            - commission, slip, order_qty: Trading parameters","            - Other stage-specific parameters","    ","    Returns:","        Dictionary with:","        - metrics: dict containing performance metrics","        - winners: dict with schema {\"topk\": [...], \"notes\": {\"schema\": \"v1\", ...}}","    ","    Note:","        - This function does NOT write any files","        - All file writing must be done by core/artifacts.py","        - Returns data only for artifact system to consume","    \"\"\"","    stage_cfg = _coerce_arrays(stage_cfg)","    ","    stage_name = stage_cfg.get(\"stage_name\", \"\")","    ","    if stage_name == \"stage0_coarse\":","        return _run_stage0_job(stage_cfg)","    elif stage_name == \"stage1_topk\":","        return _run_stage1_job(stage_cfg)","    elif stage_name == \"stage2_confirm\":","        return _run_stage2_job(stage_cfg)","    else:","        raise ValueError(f\"Unknown stage_name: {stage_name}\")","","","def _run_stage0_job(cfg: dict) -> dict:","    \"\"\"Run Stage0 coarse exploration job.\"\"\"","    close = cfg[\"close\"]","    params_matrix = cfg[\"params_matrix\"]","    proxy_name = cfg.get(\"proxy_name\", \"ma_proxy_v0\")","    ","    # Apply subsample if needed","    param_subsample_rate = cfg.get(\"param_subsample_rate\", 1.0)","    if param_subsample_rate < 1.0:","        n_total = params_matrix.shape[0]","        n_effective = int(n_total * param_subsample_rate)","        # Deterministic selection (use seed from config if available)","        seed = cfg.get(\"subsample_seed\", 42)","        rng = np.random.default_rng(seed)","        perm = rng.permutation(n_total)","        selected_indices = np.sort(perm[:n_effective])","        params_matrix = params_matrix[selected_indices]","    ","    # Run Stage0","    stage0_results = run_stage0(close, params_matrix, proxy_name=proxy_name)","    ","    # Extract metrics","    metrics = {","        \"params_total\": cfg.get(\"params_total\", params_matrix.shape[0]),","        \"params_effective\": len(stage0_results),","        \"bars\": len(close),","        \"stage_name\": \"stage0_coarse\",","    }","    ","    # Convert to winners format","    topk = cfg.get(\"topk\", 50)","    topk_param_ids = select_topk(stage0_results, k=topk)","    ","    winners = {","        \"topk\": [","            {","                \"param_id\": int(r.param_id),","                \"proxy_value\": float(r.proxy_value),","            }","            for r in stage0_results","            if r.param_id in topk_param_ids","        ],","        \"notes\": {","            \"schema\": \"v1\",","            \"stage\": \"stage0_coarse\",","            \"topk_count\": len(topk_param_ids),","        },","    }","    ","    return {\"metrics\": metrics, \"winners\": winners}","","","def _run_stage1_job(cfg: dict) -> dict:","    \"\"\"Run Stage1 Top-K refinement job.\"\"\"","    # Stage1 uses grid runner with increased subsample","    open_ = cfg[\"open_\"]","    high = cfg[\"high\"]","    low = cfg[\"low\"]","    close = cfg[\"close\"]","    params_matrix = cfg[\"params_matrix\"]","    commission = cfg.get(\"commission\", 0.0)","    slip = cfg.get(\"slip\", 0.0)","    order_qty = cfg.get(\"order_qty\", 1)","    ","    param_subsample_rate = cfg.get(\"param_subsample_rate\", 1.0)","    ","    # Apply subsample","    if param_subsample_rate < 1.0:","        n_total = params_matrix.shape[0]","        n_effective = int(n_total * param_subsample_rate)","        seed = cfg.get(\"subsample_seed\", 42)","        rng = np.random.default_rng(seed)","        perm = rng.permutation(n_total)","        selected_indices = np.sort(perm[:n_effective])","        params_matrix = params_matrix[selected_indices]","    ","    # Run grid","    result = run_grid(","        open_,","        high,","        low,","        close,","        params_matrix,","        commission=commission,","        slip=slip,","        order_qty=order_qty,","        sort_params=True,","    )","    ","    metrics_array = result.get(\"metrics\", np.array([]))","    perf = result.get(\"perf\", {})","    ","    # Extract metrics","    metrics = {","        \"params_total\": cfg.get(\"params_total\", params_matrix.shape[0]),","        \"params_effective\": metrics_array.shape[0] if metrics_array.size > 0 else 0,","        \"bars\": len(close),","        \"stage_name\": \"stage1_topk\",","    }","    ","    if isinstance(perf, dict):","        runtime_s = perf.get(\"t_total_s\", 0.0)","        if runtime_s:","            metrics[\"runtime_s\"] = float(runtime_s)","    ","    # Select top-K","    topk = cfg.get(\"topk\", 20)","    if metrics_array.size > 0:","        # Sort by net_profit (column 0)","        net_profits = metrics_array[:, 0]","        top_indices = np.argsort(net_profits)[::-1][:topk]","        "]}
{"type":"file_chunk","path":"src/pipeline/runner_adapter.py","chunk_index":1,"line_start":201,"line_end":286,"content":["        winners_list = []","        for idx in top_indices:","            winners_list.append({","                \"param_id\": int(idx),","                \"net_profit\": float(metrics_array[idx, 0]),","                \"trades\": int(metrics_array[idx, 1]),","                \"max_dd\": float(metrics_array[idx, 2]),","            })","    else:","        winners_list = []","    ","    winners = {","        \"topk\": winners_list,","        \"notes\": {","            \"schema\": \"v1\",","            \"stage\": \"stage1_topk\",","            \"topk_count\": len(winners_list),","        },","    }","    ","    return {\"metrics\": metrics, \"winners\": winners}","","","def _run_stage2_job(cfg: dict) -> dict:","    \"\"\"Run Stage2 full confirmation job.\"\"\"","    open_ = cfg[\"open_\"]","    high = cfg[\"high\"]","    low = cfg[\"low\"]","    close = cfg[\"close\"]","    params_matrix = cfg[\"params_matrix\"]","    commission = cfg.get(\"commission\", 0.0)","    slip = cfg.get(\"slip\", 0.0)","    order_qty = cfg.get(\"order_qty\", 1)","    ","    # Stage2 must use all params (subsample_rate = 1.0)","    # Get top-K from previous stage if available","    prev_winners = cfg.get(\"prev_stage_winners\", [])","    if prev_winners:","        param_ids = [w.get(\"param_id\") for w in prev_winners if \"param_id\" in w]","    else:","        # Fallback: use all params","        param_ids = list(range(params_matrix.shape[0]))","    ","    # Run Stage2","    stage2_results = run_stage2(","        open_,","        high,","        low,","        close,","        params_matrix,","        param_ids,","        commission=commission,","        slip=slip,","        order_qty=order_qty,","    )","    ","    # Extract metrics","    metrics = {","        \"params_total\": cfg.get(\"params_total\", params_matrix.shape[0]),","        \"params_effective\": len(stage2_results),","        \"bars\": len(close),","        \"stage_name\": \"stage2_confirm\",","    }","    ","    # Convert to winners format","    winners_list = []","    for r in stage2_results:","        winners_list.append({","            \"param_id\": int(r.param_id),","            \"net_profit\": float(r.net_profit),","            \"trades\": int(r.trades),","            \"max_dd\": float(r.max_dd),","        })","    ","    winners = {","        \"topk\": winners_list,","        \"notes\": {","            \"schema\": \"v1\",","            \"stage\": \"stage2_confirm\",","            \"full_confirm\": True,","        },","    }","    ","    return {\"metrics\": metrics, \"winners\": winners}","",""]}
{"type":"file_footer","path":"src/pipeline/runner_adapter.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/pipeline/runner_grid.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":37624,"sha256":"4f443a7a56612ec651293d7a790a3f84dfc5facbbead684aff8552718c9fd836","total_lines":755,"chunk_count":4}
{"type":"file_chunk","path":"src/pipeline/runner_grid.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","from __future__ import annotations","","from typing import Dict, Tuple","","import numpy as np","import os","import time","","from data.layout import normalize_bars","from engine.types import BarArrays, Fill, OrderIntent, OrderKind, OrderRole, Side","from pipeline.metrics_schema import (","    METRICS_COL_MAX_DD,","    METRICS_COL_NET_PROFIT,","    METRICS_COL_TRADES,","    METRICS_N_COLUMNS,",")","from pipeline.param_sort import sort_params_cache_friendly","from strategy.kernel import DonchianAtrParams, PrecomputedIndicators, run_kernel","from indicators.numba_indicators import rolling_max, rolling_min, atr_wilder","","","def _max_drawdown(equity: np.ndarray) -> float:","    \"\"\"","    Vectorized max drawdown on an equity curve.","    Handles empty arrays gracefully.","    \"\"\"","    if equity.size == 0:","        return 0.0","    peak = np.maximum.accumulate(equity)","    dd = equity - peak","    mdd = float(np.min(dd))  # negative or 0","    return mdd","","","def _ensure_contiguous_bars(bars: BarArrays) -> BarArrays:","    if bars.open.flags[\"C_CONTIGUOUS\"] and bars.high.flags[\"C_CONTIGUOUS\"] and bars.low.flags[\"C_CONTIGUOUS\"] and bars.close.flags[\"C_CONTIGUOUS\"]:","        return bars","    return BarArrays(","        open=np.ascontiguousarray(bars.open, dtype=np.float64),","        high=np.ascontiguousarray(bars.high, dtype=np.float64),","        low=np.ascontiguousarray(bars.low, dtype=np.float64),","        close=np.ascontiguousarray(bars.close, dtype=np.float64),","    )","","","def run_grid(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,","    *,","    commission: float,","    slip: float,","    order_qty: int = 1,","    sort_params: bool = True,","    force_close_last: bool = False,","    return_debug: bool = False,",") -> Dict[str, object]:","    \"\"\"","    Phase 3B v1: Dynamic Grid Runner (homology locked).","","    params_matrix: shape (n, >=3) float64","      col0 channel_len (int-like)","      col1 atr_len (int-like)","      col2 stop_mult (float)","","    Args:","        force_close_last: If True, force close any open positions at the last bar","            using close[-1] as exit price. This ensures trades > 0 when fills exist.","","    Returns:","      dict with:","        - metrics: np.ndarray shape (n, 3) float64 columns:","            [net_profit, trades, max_dd] (see pipeline.metrics_schema for column indices)","        - order: np.ndarray indices mapping output rows back to original params (or identity)","    \"\"\"","    profile_grid = os.environ.get(\"FISHBRO_PROFILE_GRID\", \"\").strip() == \"1\"","    profile_kernel = os.environ.get(\"FISHBRO_PROFILE_KERNEL\", \"\").strip() == \"1\"","    ","    # Stage P2-1.8: Bridge (B) - if user turns on GRID profiling, kernel timing must be enabled too.","    # This provides stable UX: grid breakdown automatically enables kernel timing.","    # Only restore if we set it ourselves, to avoid polluting external caller's environment.","    _set_kernel_profile = False","    if profile_grid and not profile_kernel:","        os.environ[\"FISHBRO_PROFILE_KERNEL\"] = \"1\"","        _set_kernel_profile = True","    ","    # Treat either flag as \"profile mode\" for grid aggregation.","    profile = profile_grid or profile_kernel","    ","    sim_only = os.environ.get(\"FISHBRO_PERF_SIM_ONLY\", \"\").strip() == \"1\"","    t0 = time.perf_counter()","","    bars = _ensure_contiguous_bars(normalize_bars(open_, high, low, close))","    t_prep1 = time.perf_counter()","","    if params_matrix.ndim != 2 or params_matrix.shape[1] < 3:","        raise ValueError(\"params_matrix must be (n, >=3)\")","","    from config.dtypes import INDEX_DTYPE","    from config.dtypes import PRICE_DTYPE_STAGE2","    ","    # runner_grid is used in Stage2, so keep float64 for params_matrix (conservative)","    pm = np.asarray(params_matrix, dtype=PRICE_DTYPE_STAGE2)","    if sort_params:","        pm_sorted, order = sort_params_cache_friendly(pm)","        # Convert order to INDEX_DTYPE (int32) for memory optimization","        order = order.astype(INDEX_DTYPE)","    else:","        pm_sorted = pm","        order = np.arange(pm.shape[0], dtype=INDEX_DTYPE)","    t_sort = time.perf_counter()","","    n = pm_sorted.shape[0]","    metrics = np.zeros((n, METRICS_N_COLUMNS), dtype=np.float64)","    ","    # Debug arrays: per-param first trade snapshot (only if return_debug=True)","    if return_debug:","        debug_fills_first = np.full((n, 6), np.nan, dtype=np.float64)","        # Columns: entry_bar, entry_price, exit_bar, exit_price, net_profit, trades","    else:","        debug_fills_first = None","","    # Initialize result dict early (minimal structure)","    perf: Dict[str, object] = {}","    ","    # Stage P2-2 Step A: Memoization potential assessment - unique counts","    # Extract channel_len and atr_len values (as int32 for unique counting)","    ch_vals = pm_sorted[:, 0].astype(np.int32, copy=False)","    atr_vals = pm_sorted[:, 1].astype(np.int32, copy=False)","    ","    perf[\"unique_channel_len_count\"] = int(np.unique(ch_vals).size)","    perf[\"unique_atr_len_count\"] = int(np.unique(atr_vals).size)","    ","    # Pack pair to int64 key: (ch<<32) | atr","    pair_keys = (ch_vals.astype(np.int64) << 32) | (atr_vals.astype(np.int64) & 0xFFFFFFFF)","    perf[\"unique_ch_atr_pair_count\"] = int(np.unique(pair_keys).size)","    ","    # Stage P2-2 Step B3: Pre-compute indicators for unique channel_len and atr_len","    unique_ch = np.unique(ch_vals)","    unique_atr = np.unique(atr_vals)","    ","    # Build caches for precomputed indicators","    donch_cache_hi: Dict[int, np.ndarray] = {}","    donch_cache_lo: Dict[int, np.ndarray] = {}","    atr_cache: Dict[int, np.ndarray] = {}","    ","    # Pre-compute timing (if profiling enabled)","    t_precompute_start = time.perf_counter() if profile else 0.0","    ","    # Pre-compute Donchian indicators for unique channel_len values","    for ch_len in unique_ch:","        ch_len_int = int(ch_len)","        donch_cache_hi[ch_len_int] = rolling_max(bars.high, ch_len_int)","        donch_cache_lo[ch_len_int] = rolling_min(bars.low, ch_len_int)","    ","    # Pre-compute ATR indicators for unique atr_len values","    for atr_len in unique_atr:","        atr_len_int = int(atr_len)","        atr_cache[atr_len_int] = atr_wilder(bars.high, bars.low, bars.close, atr_len_int)","    ","    t_precompute_end = time.perf_counter() if profile else 0.0","    ","    # Stage P2-2 Step B4: Memory observation fields","    precomp_bytes_donchian = sum(arr.nbytes for arr in donch_cache_hi.values()) + sum(arr.nbytes for arr in donch_cache_lo.values())","    precomp_bytes_atr = sum(arr.nbytes for arr in atr_cache.values())","    precomp_bytes_total = precomp_bytes_donchian + precomp_bytes_atr","    ","    perf[\"precomp_unique_channel_len_count\"] = int(len(unique_ch))","    perf[\"precomp_unique_atr_len_count\"] = int(len(unique_atr))","    perf[\"precomp_bytes_donchian\"] = int(precomp_bytes_donchian)","    perf[\"precomp_bytes_atr\"] = int(precomp_bytes_atr)","    perf[\"precomp_bytes_total\"] = int(precomp_bytes_total)","    if profile:","        perf[\"t_precompute_indicators_s\"] = float(t_precompute_end - t_precompute_start)","    ","    # CURSOR TASK 3: Grid 層把 intent sparse 傳到底","    # Read FISHBRO_PERF_TRIGGER_RATE as intent_sparse_rate and pass to kernel","    intent_sparse_rate_env = os.environ.get(\"FISHBRO_PERF_TRIGGER_RATE\", \"\").strip()","    intent_sparse_rate = 1.0","    if intent_sparse_rate_env:","        try:","            intent_sparse_rate = float(intent_sparse_rate_env)","            if not (0.0 <= intent_sparse_rate <= 1.0):","                intent_sparse_rate = 1.0","        except ValueError:","            intent_sparse_rate = 1.0","    ","    # Stage P2-3: Param-subsample (deterministic selection)","    # FISHBRO_PERF_PARAM_SUBSAMPLE_RATE controls param subsampling (separate from trigger_rate)","    # FISHBRO_PERF_TRIGGER_RATE is for bar/intent-level sparsity (handled in kernel)","    param_subsample_rate_env = os.environ.get(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", \"\").strip()","    param_subsample_seed_env = os.environ.get(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", \"\").strip()","    ","    param_subsample_rate = 1.0","    if param_subsample_rate_env:","        try:","            param_subsample_rate = float(param_subsample_rate_env)"]}
{"type":"file_chunk","path":"src/pipeline/runner_grid.py","chunk_index":1,"line_start":201,"line_end":400,"content":["            if not (0.0 <= param_subsample_rate <= 1.0):","                param_subsample_rate = 1.0","        except ValueError:","            param_subsample_rate = 1.0","    ","    param_subsample_seed = 42","    if param_subsample_seed_env:","        try:","            param_subsample_seed = int(param_subsample_seed_env)","        except ValueError:","            param_subsample_seed = 42","    ","    # Stage P2-3: Determine selected params (deterministic)","    # CURSOR TASK 1: Use \"pos\" (sorted space position) for selection, \"orig\" (original index) for scatter-back","    if param_subsample_rate < 1.0:","        k = max(1, int(round(n * param_subsample_rate)))","        rng = np.random.default_rng(param_subsample_seed)","        # Generate deterministic permutation","        perm = rng.permutation(n)","        selected_pos = np.sort(perm[:k]).astype(INDEX_DTYPE)  # Sort to maintain deterministic loop order","    else:","        selected_pos = np.arange(n, dtype=INDEX_DTYPE)","    ","    # CURSOR TASK 1: Map selected_pos (sorted space) to selected_orig (original space)","    selected_orig = order[selected_pos].astype(np.int64)  # Map sorted positions to original indices","    ","    selected_params_count = len(selected_pos)","    selected_params_ratio = float(selected_params_count) / float(n) if n > 0 else 0.0","    ","    # Create metrics_computed_mask: boolean array indicating which rows were computed","    metrics_computed_mask = np.zeros(n, dtype=bool)","    for orig_i in selected_orig:","        metrics_computed_mask[orig_i] = True","    ","    # Add param subsample info to perf","    perf[\"param_subsample_rate_configured\"] = float(param_subsample_rate)","    perf[\"selected_params_count\"] = int(selected_params_count)","    perf[\"selected_params_ratio\"] = float(selected_params_ratio)","    perf[\"metrics_rows_computed\"] = int(selected_params_count)","    perf[\"metrics_computed_mask\"] = metrics_computed_mask.tolist()  # Convert to list for JSON serialization","    ","    # Stage P2-1.8: Initialize granular timing and count accumulators (only if profile enabled)","    if profile:","        # Stage P2-2 Step A: Micro-profiling timing keys","        perf[\"t_ind_donchian_s\"] = 0.0","        perf[\"t_ind_atr_s\"] = 0.0","        perf[\"t_build_entry_intents_s\"] = 0.0","        perf[\"t_simulate_entry_s\"] = 0.0","        perf[\"t_calc_exits_s\"] = 0.0","        perf[\"t_simulate_exit_s\"] = 0.0","        perf[\"t_total_kernel_s\"] = 0.0","        perf[\"entry_fills_total\"] = 0","        perf[\"exit_intents_total\"] = 0","        perf[\"exit_fills_total\"] = 0","    result: Dict[str, object] = {\"metrics\": metrics, \"order\": order, \"perf\": perf}","","    if sim_only:","        # Debug mode: bypass strategy/orchestration and only benchmark matcher simulate.","        # This provides A/B evidence: if sim-only is fast, bottleneck is in kernel (indicators/intents).","        from engine import engine_jit","","        intents_per_bar = int(os.environ.get(\"FISHBRO_SIM_ONLY_INTENTS_PER_BAR\", \"2\"))","        intents: list[OrderIntent] = []","        oid = 1","        nbars = int(bars.open.shape[0])","        for t in range(1, nbars):","            for _ in range(intents_per_bar):","                intents.append(","                    OrderIntent(","                        order_id=oid,","                        created_bar=t - 1,","                        role=OrderRole.ENTRY,","                        kind=OrderKind.STOP,","                        side=Side.BUY,","                        price=float(bars.high[t - 1]),","                        qty=1,","                    )","                )","                oid += 1","                intents.append(","                    OrderIntent(","                        order_id=oid,","                        created_bar=t - 1,","                        role=OrderRole.EXIT,","                        kind=OrderKind.STOP,","                        side=Side.SELL,","                        price=float(bars.low[t - 1]),","                        qty=1,","                    )","                )","                oid += 1","","        t_sim0 = time.perf_counter()","        _fills = engine_jit.simulate(bars, intents)","        t_sim1 = time.perf_counter()","        jt = engine_jit.get_jit_truth()","        numba_env = os.environ.get(\"NUMBA_DISABLE_JIT\", \"\")","        sigs = jt.get(\"kernel_signatures\") or []","        perf = {","            \"t_features\": float(t_prep1 - t0),","            \"t_indicators\": None,","            \"t_intent_gen\": None,","            \"t_simulate\": float(t_sim1 - t_sim0),","            \"simulate_impl\": \"jit\" if jt.get(\"jit_path_used\") else \"py\",","            \"jit_path_used\": bool(jt.get(\"jit_path_used\")),","            \"simulate_signatures_count\": int(len(sigs)),","            \"numba_disable_jit_env\": str(numba_env),","            \"intents_total\": int(len(intents)),","            \"intents_per_bar_avg\": float(len(intents) / float(max(1, bars.open.shape[0]))),","            \"fills_total\": int(len(_fills)),","            \"intent_mode\": \"objects\",","        }","        result[\"perf\"] = perf","        if return_debug and debug_fills_first is not None:","            result[\"debug_fills_first\"] = debug_fills_first","        return result","","    # Homology: only call run_kernel, never compute strategy/metrics here.","    # Perf observability is env-gated so default usage stays unchanged.","    t_ind = 0.0","    t_intgen = 0.0","    t_sim = 0.0","    intents_total = 0","    fills_total = 0","    any_profile_missing = False","    intent_mode: str | None = None","    # Stage P2-1.5: Entry sparse observability (accumulate across params)","    entry_valid_mask_sum = 0","    entry_intents_total = 0","    n_bars_for_entry_obs = None  # Will be set from first kernel result","    # Stage P2-3: Sparse builder observability (accumulate across params)","    allowed_bars_total = 0  # Total allowed bars (before trigger rate filtering)","    intents_generated_total = 0  # Total intents generated (after trigger rate filtering)","    ","    # CURSOR TASK 1: Collect metrics_subset (will be scattered back after loop)","    metrics_subset = np.zeros((len(selected_pos), METRICS_N_COLUMNS), dtype=np.float64)","    debug_fills_first_subset = None","    if return_debug:","        debug_fills_first_subset = np.full((len(selected_pos), 6), np.nan, dtype=np.float64)","    ","    # Stage P2-3: Only loop selected params (param-subsample)","    # CURSOR TASK 1: Use selected_pos (sorted space) to access pm_sorted, selected_orig for scatter-back","    for subset_idx, pos in enumerate(selected_pos):","        # Initialize row for this iteration (will be written at loop end regardless of any continue/early exit)","        row = np.array([0.0, 0, 0.0], dtype=np.float64)","        ","        # CURSOR TASK 1: Use pos (sorted space position) to access params_sorted","        ch = int(pm_sorted[pos, 0])","        atr = int(pm_sorted[pos, 1])","        sm = float(pm_sorted[pos, 2])","","        # Stage P2-2 Step B3: Lookup precomputed indicators and create PrecomputedIndicators pack","        precomp_pack = PrecomputedIndicators(","            donch_hi=donch_cache_hi[ch],","            donch_lo=donch_cache_lo[ch],","            atr=atr_cache[atr],","        )","","        # Stage P2-1.8: Kernel profiling is already enabled at function start if profile=True","        # No need to set FISHBRO_PROFILE_KERNEL here again","        out = run_kernel(","            bars,","            DonchianAtrParams(channel_len=ch, atr_len=atr, stop_mult=sm),","            commission=float(commission),","            slip=float(slip),","            order_qty=int(order_qty),","            return_debug=return_debug,","            precomp=precomp_pack,","            intent_sparse_rate=intent_sparse_rate,  # CURSOR TASK 3: Pass intent sparse rate","        )","        obs = out.get(\"_obs\", None)  # type: ignore","        if isinstance(obs, dict):","            # Phase 3.0-B: Trust kernel's evidence fields, do not recompute","            if intent_mode is None and isinstance(obs.get(\"intent_mode\"), str):","                intent_mode = str(obs.get(\"intent_mode\"))","            # Use intents_total directly from kernel (Source of Truth), not recompute from entry+exit","            intents_total += int(obs.get(\"intents_total\", 0))","            fills_total += int(obs.get(\"fills_total\", 0))","            ","            # CURSOR TASK 2: Accumulate entry_valid_mask_sum (after intent sparse)","            # entry_valid_mask_sum must be sum(allow_mask) - not dense valid bars, not multiplied by params","            if \"entry_valid_mask_sum\" in obs:","                entry_valid_mask_sum += int(obs.get(\"entry_valid_mask_sum\", 0))","            elif \"allowed_bars\" in obs:","                # Fallback: use allowed_bars if entry_valid_mask_sum not present","                entry_valid_mask_sum += int(obs.get(\"allowed_bars\", 0))","            # CURSOR TASK 2: entry_intents_total should come from obs[\"entry_intents_total\"] (set by kernel)","            if \"entry_intents_total\" in obs:","                entry_intents_total += int(obs.get(\"entry_intents_total\", 0))","            elif \"entry_intents\" in obs:","                # Fallback: use entry_intents if entry_intents_total not present","                entry_intents_total += int(obs.get(\"entry_intents\", 0))","            elif \"n_entry\" in obs:","                # Fallback: use n_entry if entry_intents_total not present","                entry_intents_total += int(obs.get(\"n_entry\", 0))","            # Capture n_bars from first kernel result (should be same for all params)","            if n_bars_for_entry_obs is None and \"n_bars\" in obs:","                n_bars_for_entry_obs = int(obs.get(\"n_bars\", 0))","            ","            # Stage P2-3: Accumulate sparse builder observability (from new builder_sparse)"]}
{"type":"file_chunk","path":"src/pipeline/runner_grid.py","chunk_index":2,"line_start":401,"line_end":600,"content":["            if \"allowed_bars\" in obs:","                allowed_bars_total += int(obs.get(\"allowed_bars\", 0))","            if \"intents_generated\" in obs:","                intents_generated_total += int(obs.get(\"intents_generated\", 0))","            elif \"n_entry\" in obs:","                # Fallback: if intents_generated not present, use n_entry","                intents_generated_total += int(obs.get(\"n_entry\", 0))","            ","            # Stage P2-1.8: Accumulate timing keys from _obs (timing is now in _obs, not _perf)","            # Timing keys have pattern: t_*_s","            for key, value in obs.items():","                if key.startswith(\"t_\") and key.endswith(\"_s\"):","                    if key not in perf:","                        perf[key] = 0.0","                    perf[key] = float(perf[key]) + float(value)","            ","            # Stage P2-1.8: Accumulate downstream counts from _obs","            if \"entry_fills_total\" in obs:","                perf[\"entry_fills_total\"] = int(perf.get(\"entry_fills_total\", 0)) + int(obs.get(\"entry_fills_total\", 0))","            if \"exit_intents_total\" in obs:","                perf[\"exit_intents_total\"] = int(perf.get(\"exit_intents_total\", 0)) + int(obs.get(\"exit_intents_total\", 0))","            if \"exit_fills_total\" in obs:","                perf[\"exit_fills_total\"] = int(perf.get(\"exit_fills_total\", 0)) + int(obs.get(\"exit_fills_total\", 0))","        ","        # Stage P2-1.8: Fallback - also check _perf for backward compatibility","        # Handle cases where old kernel versions put timing in _perf instead of _obs","        # Only use fallback if _obs doesn't have timing keys","        obs_has_timing = isinstance(obs, dict) and any(k.startswith(\"t_\") and k.endswith(\"_s\") for k in obs.keys())","        if not obs_has_timing:","            kernel_perf = out.get(\"_perf\", None)","            if isinstance(kernel_perf, dict):","                # Accumulate timings across params (for grid-level aggregation)","                # Note: For grid-level, we sum timings across params","                for key, value in kernel_perf.items():","                    if key.startswith(\"t_\") and key.endswith(\"_s\"):","                        if key not in perf:","                            perf[key] = 0.0","                        perf[key] = float(perf[key]) + float(value)","","        # Get metrics from kernel output (always available, even if profile missing)","        m = out.get(\"metrics\", {})","        if not isinstance(m, dict):","            # Fallback: kernel didn't return metrics dict, use zeros","            m_net_profit = 0.0","            m_trades = 0","            m_max_dd = 0.0","        else:","            m_net_profit = float(m.get(\"net_profit\", 0.0))","            m_trades = int(m.get(\"trades\", 0))","            m_max_dd = float(m.get(\"max_dd\", 0.0))","            # Clean NaN/Inf at source","            m_net_profit = float(np.nan_to_num(m_net_profit, nan=0.0, posinf=0.0, neginf=0.0))","            m_max_dd = float(np.nan_to_num(m_max_dd, nan=0.0, posinf=0.0, neginf=0.0))","        ","        # Get fills count for debug assert","        fills_this_param = out.get(\"fills\", [])","        fills_count_this_param = len(fills_this_param) if isinstance(fills_this_param, list) else 0","        ","        # Collect debug data if requested","        if return_debug:","            debug_info = out.get(\"_debug\", {})","            entry_bar = debug_info.get(\"entry_bar\", -1)","            entry_price = debug_info.get(\"entry_price\", np.nan)","            exit_bar = debug_info.get(\"exit_bar\", -1)","            exit_price = debug_info.get(\"exit_price\", np.nan)","        ","        # Handle force_close_last: if still in position, force close at last bar","        if force_close_last:","            fills = out.get(\"fills\", [])","            if isinstance(fills, list) and len(fills) > 0:","                # Count entry and exit fills","                entry_fills = [f for f in fills if f.role == OrderRole.ENTRY and f.side == Side.BUY]","                exit_fills = [f for f in fills if f.role == OrderRole.EXIT and f.side == Side.SELL]","                ","                # If there are unpaired entries, force close at last bar","                if len(entry_fills) > len(exit_fills):","                    n_unpaired = len(entry_fills) - len(exit_fills)","                    last_bar_idx = int(bars.open.shape[0] - 1)","                    last_close_price = float(bars.close[last_bar_idx])","                    ","                    # Create forced exit fills for unpaired entries","                    # Use entry prices from the unpaired entries","                    unpaired_entry_prices = [float(f.price) for f in entry_fills[-n_unpaired:]]","                    ","                    # Calculate additional pnl from forced closes","                    forced_pnl = []","                    costs_per_trade = (float(commission) + float(slip)) * 2.0","                    for entry_price in unpaired_entry_prices:","                        # PnL = (exit_price - entry_price) * qty - costs","                        trade_pnl = (last_close_price - entry_price) * float(order_qty) - costs_per_trade","                        forced_pnl.append(trade_pnl)","                    ","                    # Update metrics with forced closes","                    original_net_profit = m_net_profit","                    original_trades = m_trades","                    ","                    # Add forced close trades","                    new_net_profit = original_net_profit + sum(forced_pnl)","                    new_trades = original_trades + n_unpaired","                    ","                    # Update debug exit info for force_close_last","                    if return_debug and n_unpaired > 0:","                        exit_bar = last_bar_idx","                        exit_price = last_close_price","                    ","                    # Recalculate equity and max_dd","                    forced_pnl_arr = np.asarray(forced_pnl, dtype=np.float64)","                    if original_trades > 0 and \"equity\" in out:","                        original_equity = out[\"equity\"]","                        if isinstance(original_equity, np.ndarray) and original_equity.size > 0:","                            # Append forced pnl to existing equity curve","                            # Start from last equity value","                            start_equity = float(original_equity[-1])","                            forced_equity = np.cumsum(forced_pnl_arr) + start_equity","                            new_equity = np.concatenate([original_equity, forced_equity])","                        else:","                            # No previous equity array, start from 0","                            new_equity = np.cumsum(forced_pnl_arr)","                    else:","                        # No previous trades, start from 0","                        new_equity = np.cumsum(forced_pnl_arr)","                    ","                    new_max_dd = _max_drawdown(new_equity)","                    ","                    # Update row with forced close metrics","                    row = np.array([new_net_profit, new_trades, new_max_dd], dtype=np.float64)","                    ","                    # Update debug subset with final metrics after force_close_last","                    if return_debug:","                        debug_fills_first_subset[subset_idx, 0] = entry_bar","                        debug_fills_first_subset[subset_idx, 1] = entry_price","                        debug_fills_first_subset[subset_idx, 2] = exit_bar","                        debug_fills_first_subset[subset_idx, 3] = exit_price","                        debug_fills_first_subset[subset_idx, 4] = new_net_profit","                        debug_fills_first_subset[subset_idx, 5] = float(new_trades)","                else:","                    # No unpaired entries, use original metrics","                    row = np.array([m_net_profit, m_trades, m_max_dd], dtype=np.float64)","                    ","                    # Store debug data in subset","                    if return_debug:","                        debug_fills_first_subset[subset_idx, 0] = entry_bar","                        debug_fills_first_subset[subset_idx, 1] = entry_price","                        debug_fills_first_subset[subset_idx, 2] = exit_bar","                        debug_fills_first_subset[subset_idx, 3] = exit_price","                        debug_fills_first_subset[subset_idx, 4] = m_net_profit","                        debug_fills_first_subset[subset_idx, 5] = float(m_trades)","            else:","                # No fills, use original metrics","                row = np.array([m_net_profit, m_trades, m_max_dd], dtype=np.float64)","                ","                # Store debug data in subset (no fills case)","                if return_debug:","                    debug_fills_first_subset[subset_idx, 0] = entry_bar","                    debug_fills_first_subset[subset_idx, 1] = entry_price","                    debug_fills_first_subset[subset_idx, 2] = exit_bar","                    debug_fills_first_subset[subset_idx, 3] = exit_price","                    debug_fills_first_subset[subset_idx, 4] = m_net_profit","                    debug_fills_first_subset[subset_idx, 5] = float(m_trades)","        else:","            # Zero-trade safe: kernel guarantees valid numbers (0.0/0)","            row = np.array([m_net_profit, m_trades, m_max_dd], dtype=np.float64)","            ","            # Store debug data in subset","            if return_debug:","                debug_fills_first_subset[subset_idx, 0] = entry_bar","                debug_fills_first_subset[subset_idx, 1] = entry_price","                debug_fills_first_subset[subset_idx, 2] = exit_bar","                debug_fills_first_subset[subset_idx, 3] = exit_price","                debug_fills_first_subset[subset_idx, 4] = m_net_profit","                debug_fills_first_subset[subset_idx, 5] = float(m_trades)","        ","        # HARD CONTRACT: Always write metrics_subset at loop end, regardless of any continue/early exit","        metrics_subset[subset_idx, :] = row","        ","        # Debug assert: if trades > 0 (completed trades), metrics must be non-zero","        # Note: entry fills without exits yield trades=0 and all-zero metrics, which is valid","        if os.environ.get(\"FISHBRO_DEBUG_ASSERT\", \"\").strip() == \"1\":","            if m_trades > 0:","                assert np.any(np.abs(metrics_subset[subset_idx, :]) > 0), (","                    f\"subset_idx={subset_idx}: trades={m_trades} > 0, \"","                    f\"but metrics_subset[{subset_idx}, :]={metrics_subset[subset_idx, :]} is all zeros\"","                )","        ","        # Handle profile timing accumulation (after metrics written)","        if profile:","            kp = out.get(\"_profile\", None)  # type: ignore","            if not isinstance(kp, dict):","                any_profile_missing = True","                # Continue after metrics already written","                continue","            t_ind += float(kp.get(\"indicators_s\", 0.0))","            # include both entry+exit intent generation as \"intent generation\"","            t_intgen += float(kp.get(\"intent_gen_s\", 0.0)) + float(kp.get(\"exit_intent_gen_s\", 0.0))","            t_sim += float(kp.get(\"simulate_entry_s\", 0.0)) + float(kp.get(\"simulate_exit_s\", 0.0))","    ","    # CURSOR TASK 2: Handle NaN before scatter-back (avoid computed_non_zero being eaten by NaN)","    # Note: Already handled at source (m_net_profit, m_max_dd), but double-check here for safety","    metrics_subset = np.nan_to_num(metrics_subset, nan=0.0, posinf=0.0, neginf=0.0)","    "]}
{"type":"file_chunk","path":"src/pipeline/runner_grid.py","chunk_index":3,"line_start":601,"line_end":755,"content":["    # CURSOR TASK 3: Assert that if fills_total > 0, metrics_subset should have non-zero values","    # This helps catch cases where metrics computation was skipped or returned zeros","    # Only assert if FISHBRO_DEBUG_ASSERT=1 (not triggered by profile, as tests often enable profile)","    if os.environ.get(\"FISHBRO_DEBUG_ASSERT\", \"\").strip() == \"1\":","        metrics_subset_abs_sum = float(np.sum(np.abs(metrics_subset)))","        assert fills_total == 0 or metrics_subset_abs_sum > 0, (","            f\"CURSOR TASK B violation: fills_total={fills_total} > 0 but metrics_subset_abs_sum={metrics_subset_abs_sum} == 0. \"","            f\"This indicates metrics computation was skipped or returned zeros.\"","        )","    ","    # CURSOR TASK 3: Add perf debug field (metrics_subset_nonzero_rows)","    metrics_subset_nonzero_rows = int(np.sum(np.any(np.abs(metrics_subset) > 1e-10, axis=1)))","    perf[\"metrics_subset_nonzero_rows\"] = metrics_subset_nonzero_rows","    ","    # === HARD CONTRACT: scatter metrics back to original param space ===","    # CRITICAL: This must happen after all metrics computation and before any return","    # Variables: selected_pos (sorted-space index), order (sorted_pos -> original_index), metrics_subset (computed metrics)","    # For each selected param: metrics[orig_param_idx] must be written with non-zero values","    for subset_i, pos in enumerate(selected_pos):","        orig_i = int(order[int(pos)])","        metrics[orig_i, :] = metrics_subset[subset_i, :]","        ","        if return_debug and debug_fills_first is not None and debug_fills_first_subset is not None:","            debug_fills_first[orig_i, :] = debug_fills_first_subset[subset_i, :]","    ","    # CRITICAL: After scatter-back, metrics must not be modified (no metrics = np.zeros, no metrics[:] = 0, no result[\"metrics\"] = metrics_subset)","    ","    # CURSOR TASK 2: Add perf debug fields (for diagnostic)","    perf[\"intent_sparse_rate_effective\"] = float(intent_sparse_rate)","    perf[\"fills_total\"] = int(fills_total)","    perf[\"metrics_subset_abs_sum\"] = float(np.sum(np.abs(metrics_subset)))","    ","    # CURSOR TASK A: Add entry_intents_total (subsample run) for diagnostic","    # This helps distinguish: entry_intents_total > 0 but fills_total == 0 → matcher/engine issue","    # vs entry_intents_total == 0 → builder didn't generate intents","    perf[\"entry_intents_total\"] = int(entry_intents_total)","","    # Phase 3.0-E: Ensure intent_mode is never None","    # If no kernel results (n == 0), default to \"arrays\" (default kernel path)","    # Otherwise, intent_mode should have been set from first kernel result","    if intent_mode is None:","        # Edge case: n == 0 (no params) - use default \"arrays\" since run_kernel defaults to array path","        intent_mode = \"arrays\"","","    if not profile:","        # Return minimal perf with evidence fields only","        # Stage P2-1.8: Preserve accumulated timings (already in perf dict from loop)","        perf[\"intent_mode\"] = intent_mode","        perf[\"intents_total\"] = int(intents_total)","        # fills_total already set in scatter-back section (line 592), but ensure it's here too for clarity","        if \"fills_total\" not in perf:","            perf[\"fills_total\"] = int(fills_total)","        # CURSOR TASK 3: Add intent sparse rate and entry observability to perf","        perf[\"intent_sparse_rate\"] = float(intent_sparse_rate)","        perf[\"entry_valid_mask_sum\"] = int(entry_valid_mask_sum)  # CURSOR TASK 2: After intent sparse (sum(allow_mask))","        perf[\"entry_intents_total\"] = int(entry_intents_total)","        ","        # Stage P2-1.5: Add entry sparse observability (always include, even if 0)","        perf[\"intents_total_reported\"] = int(intents_total)  # Preserve original for comparison","        if n_bars_for_entry_obs is not None and n_bars_for_entry_obs > 0:","            perf[\"entry_intents_per_bar_avg\"] = float(entry_intents_total / n_bars_for_entry_obs)","        else:","            # Fallback: use bars.open.shape[0] if n_bars_for_entry_obs not available","            perf[\"entry_intents_per_bar_avg\"] = float(entry_intents_total / max(1, bars.open.shape[0]))","        ","        # Stage P2-3: Add sparse builder observability (for scaling verification)","        perf[\"allowed_bars\"] = int(allowed_bars_total)","        perf[\"intents_generated\"] = int(intents_generated_total)","        perf[\"selected_params\"] = int(selected_params_count)","        ","        # CURSOR TASK 2: Ensure debug fields are present in non-profile branch too","        if \"intent_sparse_rate_effective\" not in perf:","            perf[\"intent_sparse_rate_effective\"] = float(intent_sparse_rate)","        if \"fills_total\" not in perf:","            perf[\"fills_total\"] = int(fills_total)","        if \"metrics_subset_abs_sum\" not in perf:","            perf[\"metrics_subset_abs_sum\"] = float(np.sum(np.abs(metrics_subset)))","        ","        result[\"perf\"] = perf","        if return_debug and debug_fills_first is not None:","            result[\"debug_fills_first\"] = debug_fills_first","        return result","","    from engine import engine_jit","","    jt = engine_jit.get_jit_truth()","    numba_env = os.environ.get(\"NUMBA_DISABLE_JIT\", \"\")","    sigs = jt.get(\"kernel_signatures\") or []","","    # Best-effort: avoid leaking this env to callers","    # Only clean up if we set it ourselves (Task A: bridge logic)","    if _set_kernel_profile:","        try:","            del os.environ[\"FISHBRO_PROFILE_KERNEL\"]","        except KeyError:","            pass","","    # Phase 3.0-E: Ensure intent_mode is never None","    # If no kernel results (n == 0), default to \"arrays\" (default kernel path)","    # Otherwise, intent_mode should have been set from first kernel result","    if intent_mode is None:","        # Edge case: n == 0 (no params) - use default \"arrays\" since run_kernel defaults to array path","        intent_mode = \"arrays\"","","    # Stage P2-1.8: Create summary dict and merge into accumulated perf (preserve t_*_s from loop)","    perf_summary = {","        \"t_features\": float(t_prep1 - t0),","        # current architecture: indicators are computed inside run_kernel per param","        \"t_indicators\": None if any_profile_missing else float(t_ind),","        \"t_intent_gen\": None if any_profile_missing else float(t_intgen),","        \"t_simulate\": None if any_profile_missing else float(t_sim),","        \"simulate_impl\": \"jit\" if jt.get(\"jit_path_used\") else \"py\",","        \"jit_path_used\": bool(jt.get(\"jit_path_used\")),","        \"simulate_signatures_count\": int(len(sigs)),","        \"numba_disable_jit_env\": str(numba_env),","        # Phase 3.0-B: Use kernel's evidence fields directly (Source of Truth), not recomputed","        \"intent_mode\": intent_mode,","        \"intents_total\": int(intents_total),","        \"fills_total\": int(fills_total),","        \"intents_per_bar_avg\": float(intents_total / float(max(1, bars.open.shape[0]))),","    }","    ","    # CURSOR TASK 3: Add intent sparse rate and entry observability to perf","    perf_summary[\"intent_sparse_rate\"] = float(intent_sparse_rate)","    perf_summary[\"entry_valid_mask_sum\"] = int(entry_valid_mask_sum)  # CURSOR TASK 2: After intent sparse","    perf_summary[\"entry_intents_total\"] = int(entry_intents_total)","    ","    # Stage P2-1.5: Add entry sparse observability and preserve original intents_total","    perf_summary[\"intents_total_reported\"] = int(intents_total)  # Preserve original for comparison","    if n_bars_for_entry_obs is not None and n_bars_for_entry_obs > 0:","        perf_summary[\"entry_intents_per_bar_avg\"] = float(entry_intents_total / n_bars_for_entry_obs)","    else:","        # Fallback: use bars.open.shape[0] if n_bars_for_entry_obs not available","        perf_summary[\"entry_intents_per_bar_avg\"] = float(entry_intents_total / max(1, bars.open.shape[0]))","    ","    # Stage P2-3: Add sparse builder observability (for scaling verification)","    perf_summary[\"allowed_bars\"] = int(allowed_bars_total)  # Total allowed bars across all params","    perf_summary[\"intents_generated\"] = int(intents_generated_total)  # Total intents generated across all params","    perf_summary[\"selected_params\"] = int(selected_params_count)  # Number of params actually computed","    ","    # CURSOR TASK 2: Ensure debug fields are present in profile branch too","    perf_summary[\"intent_sparse_rate_effective\"] = float(intent_sparse_rate)","    perf_summary[\"fills_total\"] = int(fills_total)","    perf_summary[\"metrics_subset_abs_sum\"] = float(np.sum(np.abs(metrics_subset)))","    ","    # Keep accumulated per-kernel timings already stored in `perf` (t_*_s, entry_fills_total, etc.)","    perf.update(perf_summary)","","    result[\"perf\"] = perf","    if return_debug and debug_fills_first is not None:","        result[\"debug_fills_first\"] = debug_fills_first","    return result","","",""]}
{"type":"file_footer","path":"src/pipeline/runner_grid.py","complete":true,"emitted_chunks":4}
{"type":"file_header","path":"src/pipeline/stage0_runner.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2683,"sha256":"8763b8e720a0c616c1c8d9ed4ebb3faf19ce483657f3965a976ab5ca132e16c5","total_lines":92,"chunk_count":1}
{"type":"file_chunk","path":"src/pipeline/stage0_runner.py","chunk_index":0,"line_start":1,"line_end":92,"content":["","\"\"\"Stage0 runner - proxy ranking without PnL metrics.","","Stage0 is a fast proxy filter that ranks parameters without running full backtests.","It MUST NOT compute any PnL-related metrics (Net/MDD/SQN/Sharpe/WinRate/Equity/DD).","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass","from typing import List, Optional","","import numpy as np","","from config.constants import STAGE0_PROXY_NAME","from stage0.ma_proxy import stage0_score_ma_proxy","","","@dataclass(frozen=True)","class Stage0Result:","    \"\"\"","    Stage0 result - proxy ranking only.","    ","    Contains ONLY:","    - param_id: parameter index","    - proxy_value: proxy ranking value (higher is better)","    - warmup_ok: optional warmup validation flag","    - meta: optional metadata dict","    ","    FORBIDDEN fields (must not exist):","    - Any PnL metrics: Net, MDD, SQN, Sharpe, WinRate, Equity, DD, etc.","    \"\"\"","    param_id: int","    proxy_value: float","    warmup_ok: Optional[bool] = None","    meta: Optional[dict] = None","","","def run_stage0(","    close: np.ndarray,","    params_matrix: np.ndarray,","    *,","    proxy_name: str = STAGE0_PROXY_NAME,",") -> List[Stage0Result]:","    \"\"\"","    Run Stage0 proxy ranking.","    ","    Args:","        close: float32 or float64 1D array (n_bars,) - close prices (will use float32 internally)","        params_matrix: float32 or float64 2D array (n_params, >=2) (will use float32 internally)","            - col0: fast_len (for MA proxy)","            - col1: slow_len (for MA proxy)","            - additional columns allowed and ignored","        proxy_name: name of proxy to use (default: ma_proxy_v0)","        ","    Returns:","        List of Stage0Result, one per parameter set.","        Results are in same order as params_matrix rows.","        ","    Note:","        - This function MUST NOT compute any PnL metrics","        - Only proxy_value is computed for ranking purposes","        - Uses float32 internally for memory optimization","    \"\"\"","    if proxy_name != \"ma_proxy_v0\":","        raise ValueError(f\"Unsupported proxy: {proxy_name}. Only 'ma_proxy_v0' is supported in Phase 4.\")","    ","    # Compute proxy scores","    scores = stage0_score_ma_proxy(close, params_matrix)","    ","    # Build results","    n_params = params_matrix.shape[0]","    results: List[Stage0Result] = []","    ","    for i in range(n_params):","        score = float(scores[i])","        ","        # Check warmup: if score is -inf, warmup failed","        warmup_ok = not np.isinf(score) if not np.isnan(score) else False","        ","        results.append(","            Stage0Result(","                param_id=i,","                proxy_value=score,","                warmup_ok=warmup_ok,","                meta=None,","            )","        )","    ","    return results","",""]}
{"type":"file_footer","path":"src/pipeline/stage0_runner.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/pipeline/stage2_runner.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4759,"sha256":"ccce6075377169371329335b48235d0636142ae3a7eefd983398e4e2766c5b0d","total_lines":162,"chunk_count":1}
{"type":"file_chunk","path":"src/pipeline/stage2_runner.py","chunk_index":0,"line_start":1,"line_end":162,"content":["","\"\"\"Stage2 runner - full backtest on Top-K parameters.","","Stage2 runs full backtests using the unified simulate_run() entry point.","It computes complete metrics including net_profit, trades, max_dd, etc.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass","from typing import Dict, List, Optional","","import numpy as np","","from data.layout import normalize_bars","from engine.types import BarArrays, Fill","from strategy.kernel import DonchianAtrParams, run_kernel","","","@dataclass(frozen=True)","class Stage2Result:","    \"\"\"","    Stage2 result - full backtest metrics.","    ","    Contains complete backtest results including:","    - param_id: parameter index","    - net_profit: total net profit","    - trades: number of trades","    - max_dd: maximum drawdown","    - fills: list of fills (optional, for detailed analysis)","    - equity: equity curve (optional)","    - meta: optional metadata","    \"\"\"","    param_id: int","    net_profit: float","    trades: int","    max_dd: float","    fills: Optional[List[Fill]] = None","    equity: Optional[np.ndarray] = None","    meta: Optional[dict] = None","","","def _max_drawdown(equity: np.ndarray) -> float:","    \"\"\"Compute max drawdown from equity curve.\"\"\"","    if equity.size == 0:","        return 0.0","    peak = np.maximum.accumulate(equity)","    dd = equity - peak","    mdd = float(np.min(dd))  # negative or 0","    return mdd","","","def run_stage2(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,","    param_ids: List[int],","    *,","    commission: float,","    slip: float,","    order_qty: int = 1,",") -> List[Stage2Result]:","    \"\"\"","    Run Stage2 full backtest on selected parameters.","    ","    Args:","        open_, high, low, close: OHLC arrays (float64, 1D, same length)","        params_matrix: float64 2D array (n_params, >=3)","            - col0: channel_len","            - col1: atr_len","            - col2: stop_mult","        param_ids: List of parameter indices to run (Top-K selection)","        commission: commission per trade (absolute)","        slip: slippage per trade (absolute)","        order_qty: order quantity (default: 1)","        ","    Returns:","        List of Stage2Result, one per selected parameter.","        Results are in same order as param_ids.","        ","    Note:","        - Only runs backtests for parameters in param_ids (Top-K subset)","        - Uses unified simulate_run() entry point (Cursor kernel)","        - Computes full metrics including PnL","    \"\"\"","    bars = normalize_bars(open_, high, low, close)","    ","    # Ensure contiguous arrays","    if not bars.open.flags[\"C_CONTIGUOUS\"]:","        bars = BarArrays(","            open=np.ascontiguousarray(bars.open, dtype=np.float64),","            high=np.ascontiguousarray(bars.high, dtype=np.float64),","            low=np.ascontiguousarray(bars.low, dtype=np.float64),","            close=np.ascontiguousarray(bars.close, dtype=np.float64),","        )","    ","    results: List[Stage2Result] = []","    ","    for param_id in param_ids:","        if param_id < 0 or param_id >= params_matrix.shape[0]:","            # Invalid param_id - create empty result","            results.append(","                Stage2Result(","                    param_id=param_id,","                    net_profit=0.0,","                    trades=0,","                    max_dd=0.0,","                    fills=None,","                    equity=None,","                    meta=None,","                )","            )","            continue","        ","        # Extract parameters","        params_row = params_matrix[param_id]","        channel_len = int(params_row[0])","        atr_len = int(params_row[1])","        stop_mult = float(params_row[2])","        ","        # Build DonchianAtrParams","        kernel_params = DonchianAtrParams(","            channel_len=channel_len,","            atr_len=atr_len,","            stop_mult=stop_mult,","        )","        ","        # Run kernel (uses unified simulate_run internally)","        kernel_result = run_kernel(","            bars,","            kernel_params,","            commission=commission,","            slip=slip,","            order_qty=order_qty,","        )","        ","        # Extract metrics","        net_profit = float(kernel_result[\"metrics\"][\"net_profit\"])","        trades = int(kernel_result[\"metrics\"][\"trades\"])","        max_dd = float(kernel_result[\"metrics\"][\"max_dd\"])","        ","        # Extract optional fields","        fills = kernel_result.get(\"fills\")","        equity = kernel_result.get(\"equity\")","        ","        results.append(","            Stage2Result(","                param_id=param_id,","                net_profit=net_profit,","                trades=trades,","                max_dd=max_dd,","                fills=fills,","                equity=equity,","                meta=None,","            )","        )","    ","    return results","",""]}
{"type":"file_footer","path":"src/pipeline/stage2_runner.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/pipeline/topk.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1508,"sha256":"57dddbe97b633a8536a5c3b7afd96866251136eecb0fba053c80e0adfcac1eaf","total_lines":54,"chunk_count":1}
{"type":"file_chunk","path":"src/pipeline/topk.py","chunk_index":0,"line_start":1,"line_end":54,"content":["","\"\"\"Top-K selector - deterministic parameter selection.","","Selects top K parameters based on Stage0 proxy_value.","Tie-breaking uses param_id to ensure deterministic results.","\"\"\"","","from __future__ import annotations","","from typing import List","","from config.constants import TOPK_K","from pipeline.stage0_runner import Stage0Result","","","def select_topk(","    stage0_results: List[Stage0Result],","    k: int = TOPK_K,",") -> List[int]:","    \"\"\"","    Select top K parameters based on proxy_value.","    ","    Args:","        stage0_results: List of Stage0Result from Stage0 runner","        k: number of top parameters to select (default: TOPK_K from config)","        ","    Returns:","        List of param_id values (indices) for top K parameters.","        Results are sorted by proxy_value (descending), then by param_id (ascending) for tie-break.","        ","    Note:","        - Sorting is deterministic: same input always produces same output","        - Tie-break uses param_id (ascending) to ensure stability","        - No manual include/exclude - purely based on proxy_value","    \"\"\"","    if k <= 0:","        return []","    ","    if len(stage0_results) == 0:","        return []","    ","    # Sort by proxy_value (descending), then param_id (ascending) for tie-break","    sorted_results = sorted(","        stage0_results,","        key=lambda r: (-r.proxy_value, r.param_id),  # Negative for descending value","    )","    ","    # Take top K","    topk_results = sorted_results[:k]","    ","    # Return param_id list","    return [r.param_id for r in topk_results]","",""]}
{"type":"file_footer","path":"src/pipeline/topk.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":633,"sha256":"3624b56634698069599cfb4bb7cf113067e746e2e31cdebeed10f05d3c0eae09","total_lines":24,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/__init__.py","chunk_index":0,"line_start":1,"line_end":24,"content":["","\"\"\"Portfolio package exports.","","Single source of truth: PortfolioSpec in spec.py","Phase 11 research bridge uses PortfolioSpec (no spec split).","\"\"\"","","from __future__ import annotations","","from portfolio.decisions_reader import parse_decisions_log_lines, read_decisions_log","from portfolio.research_bridge import build_portfolio_from_research","from portfolio.spec import PortfolioLeg, PortfolioSpec","from portfolio.writer import write_portfolio_artifacts","","__all__ = [","    \"PortfolioLeg\",","    \"PortfolioSpec\",","    \"parse_decisions_log_lines\",","    \"read_decisions_log\",","    \"build_portfolio_from_research\",","    \"write_portfolio_artifacts\",","]","",""]}
{"type":"file_footer","path":"src/portfolio/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/portfolio/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":744,"sha256":"043d681758ddc7b5c61181331748e13e8e70d675b8a580c551410406f13a5cc3","note":"skipped by policy"}
{"type":"file_skipped","path":"src/portfolio/__pycache__/decisions_reader.cpython-312.pyc","reason":"cache","bytes":4139,"sha256":"00ae1b7adb7e957c3ef469aa1fab7f0ad4256a5312bcd74803e60970a005ea7a","note":"skipped by policy"}
{"type":"file_skipped","path":"src/portfolio/__pycache__/hash_utils.cpython-312.pyc","reason":"cache","bytes":1379,"sha256":"86b81e9982406dacb53d74b10a12593d589ca447dc1cd475920a436aad423982","note":"skipped by policy"}
{"type":"file_skipped","path":"src/portfolio/__pycache__/plan_builder.cpython-312.pyc","reason":"cache","bytes":29661,"sha256":"9668345db6dd4034dc55ca42c5c33b929e0b0ca34047f268690e01b6559be946","note":"skipped by policy"}
{"type":"file_skipped","path":"src/portfolio/__pycache__/plan_quality.cpython-312.pyc","reason":"cache","bytes":16278,"sha256":"e197c1f0a36929d88c54ccc6ca986ff02127542c57b1c13237737f4942a1787b","note":"skipped by policy"}
{"type":"file_skipped","path":"src/portfolio/__pycache__/plan_quality_writer.cpython-312.pyc","reason":"cache","bytes":5573,"sha256":"62c1e2ccc08d6c38f238e19ec160ae18c312bb0f112355859da098072a5f9582","note":"skipped by policy"}
{"type":"file_skipped","path":"src/portfolio/__pycache__/research_bridge.cpython-312.pyc","reason":"cache","bytes":10308,"sha256":"9d61e5fc1fc4a6303625d7d4b631ea7e35b3b19ec0e9502f94a2230644c4e41f","note":"skipped by policy"}
{"type":"file_skipped","path":"src/portfolio/__pycache__/spec.cpython-312.pyc","reason":"cache","bytes":4638,"sha256":"8644a7ee25ba55de61d190996af68c11b452f227473e3850474a6acdc00a1f1e","note":"skipped by policy"}
{"type":"file_skipped","path":"src/portfolio/__pycache__/writer.cpython-312.pyc","reason":"cache","bytes":9774,"sha256":"dc069d1a1832c7fe89e56069852d8404aa41f2d0f96d84b67cf8cc0735587853","note":"skipped by policy"}
{"type":"file_header","path":"src/portfolio/artifacts.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5292,"sha256":"77e21b796e1e17ed4c99abd82e271da0d1c43cac192be72ff40cf912be6e8ecc","total_lines":165,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/artifacts.py","chunk_index":0,"line_start":1,"line_end":165,"content":["","\"\"\"Portfolio artifacts writer.","","Phase 8: Write portfolio artifacts for replayability and audit.","\"\"\"","","from __future__ import annotations","","import hashlib","import json","from pathlib import Path","from typing import Any, Dict, List","","import yaml","","from portfolio.spec import PortfolioSpec","","","def _normalize_spec_for_hash(spec: PortfolioSpec) -> Dict[str, Any]:","    \"\"\"Normalize spec to dict for hashing (exclude runtime-dependent fields).","    ","    Excludes:","    - Absolute paths (convert to relative or normalize)","    - Timestamps","    - Runtime-dependent fields","    ","    Args:","        spec: Portfolio specification","        ","    Returns:","        Normalized dict suitable for hashing","    \"\"\"","    legs_dict = []","    for leg in spec.legs:","        # Normalize session_profile path (use relative path, not absolute)","        session_profile = leg.session_profile","        # Remove any absolute path components, keep relative structure","        if Path(session_profile).is_absolute():","            # Try to make relative to common base","            try:","                session_profile = str(Path(session_profile).relative_to(Path.cwd()))","            except ValueError:","                # If can't make relative, use basename as fallback","                session_profile = Path(session_profile).name","        ","        leg_dict = {","            \"leg_id\": leg.leg_id,","            \"symbol\": leg.symbol,","            \"timeframe_min\": leg.timeframe_min,","            \"session_profile\": session_profile,  # Normalized path","            \"strategy_id\": leg.strategy_id,","            \"strategy_version\": leg.strategy_version,","            \"params\": dict(sorted(leg.params.items())),  # Sort for determinism","            \"enabled\": leg.enabled,","            \"tags\": sorted(leg.tags),  # Sort for determinism","        }","        legs_dict.append(leg_dict)","    ","    # Sort legs by leg_id for determinism","    legs_dict.sort(key=lambda x: x[\"leg_id\"])","    ","    return {","        \"portfolio_id\": spec.portfolio_id,","        \"version\": spec.version,","        \"data_tz\": spec.data_tz,","        \"legs\": legs_dict,","    }","","","def compute_portfolio_hash(spec: PortfolioSpec) -> str:","    \"\"\"Compute deterministic hash of portfolio specification.","    ","    Uses SHA1 (consistent with Phase 6.5 fingerprint style).","    Hash is computed from normalized spec dict (sorted keys, stable serialization).","    ","    Args:","        spec: Portfolio specification","        ","    Returns:","        SHA1 hash hex string (40 chars)","    \"\"\"","    normalized = _normalize_spec_for_hash(spec)","    ","    # Stable JSON serialization","    spec_json = json.dumps(","        normalized,","        sort_keys=True,","        separators=(\",\", \":\"),  # Compact, no spaces","        ensure_ascii=False,","    )","    ","    # SHA1 hash","    return hashlib.sha1(spec_json.encode(\"utf-8\")).hexdigest()","","","def write_portfolio_artifacts(","    spec: PortfolioSpec,","    jobs: List[Dict[str, Any]],","    out_dir: Path,",") -> Dict[str, str]:","    \"\"\"Write portfolio artifacts to output directory.","    ","    Creates:","    - portfolio_spec_snapshot.yaml: Portfolio spec snapshot","    - compiled_jobs.json: Compiled job configurations","    - portfolio_index.json: Portfolio index with metadata","    - portfolio_hash.txt: Portfolio hash (single line)","    ","    Args:","        spec: Portfolio specification","        jobs: Compiled job configurations (from compile_portfolio)","        out_dir: Output directory (will be created if needed)","        ","    Returns:","        Dict mapping artifact names to file paths (relative to out_dir)","    \"\"\"","    out_dir.mkdir(parents=True, exist_ok=True)","    ","    # Compute hash","    portfolio_hash = compute_portfolio_hash(spec)","    ","    # Write portfolio_spec_snapshot.yaml","    spec_snapshot_path = out_dir / \"portfolio_spec_snapshot.yaml\"","    normalized_spec = _normalize_spec_for_hash(spec)","    with spec_snapshot_path.open(\"w\", encoding=\"utf-8\") as f:","        yaml.dump(normalized_spec, f, default_flow_style=False, sort_keys=True)","    ","    # Write compiled_jobs.json","    jobs_path = out_dir / \"compiled_jobs.json\"","    with jobs_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(jobs, f, indent=2, sort_keys=True, ensure_ascii=False)","    ","    # Write portfolio_index.json","    index = {","        \"portfolio_id\": spec.portfolio_id,","        \"version\": spec.version,","        \"portfolio_hash\": portfolio_hash,","        \"legs\": [","            {","                \"leg_id\": leg.leg_id,","                \"symbol\": leg.symbol,","                \"timeframe_min\": leg.timeframe_min,","                \"strategy_id\": leg.strategy_id,","                \"strategy_version\": leg.strategy_version,","            }","            for leg in spec.legs","        ],","    }","    index_path = out_dir / \"portfolio_index.json\"","    with index_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(index, f, indent=2, sort_keys=True, ensure_ascii=False)","    ","    # Write portfolio_hash.txt (single line)","    hash_path = out_dir / \"portfolio_hash.txt\"","    hash_path.write_text(portfolio_hash + \"\\n\", encoding=\"utf-8\")","    ","    # Return artifact paths (relative to out_dir)","    return {","        \"spec_snapshot\": str(spec_snapshot_path.relative_to(out_dir)),","        \"compiled_jobs\": str(jobs_path.relative_to(out_dir)),","        \"index\": str(index_path.relative_to(out_dir)),","        \"hash\": str(hash_path.relative_to(out_dir)),","    }","",""]}
{"type":"file_footer","path":"src/portfolio/artifacts.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/artifacts_writer_v1.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5684,"sha256":"f6acfce9be4cdaa099256d58f8e6703ee2aa7b72c88ea520a24779f3e7f64bb0","total_lines":179,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/artifacts_writer_v1.py","chunk_index":0,"line_start":1,"line_end":179,"content":["\"\"\"Portfolio artifacts writer V1.\"\"\"","","import json","import hashlib","from pathlib import Path","from typing import Dict, List, Any","import pandas as pd","","from core.schemas.portfolio_v1 import (","    AdmissionDecisionV1,","    PortfolioStateV1,","    PortfolioSummaryV1,","    PortfolioPolicyV1,","    PortfolioSpecV1,",")","from control.artifacts import (","    canonical_json_bytes,","    sha256_bytes,","    write_json_atomic,",")","","","def write_portfolio_artifacts(","    output_dir: Path,","    decisions: List[AdmissionDecisionV1],","    bar_states: Dict[Any, PortfolioStateV1],","    summary: PortfolioSummaryV1,","    policy: PortfolioPolicyV1,","    spec: PortfolioSpecV1,","    replay_mode: bool = False,",") -> Dict[str, str]:","    \"\"\"","    Write portfolio artifacts to disk.","    ","    Args:","        output_dir: Directory to write artifacts","        decisions: List of admission decisions","        bar_states: Dict mapping (bar_index, bar_ts) to PortfolioStateV1","        summary: Portfolio summary","        policy: Portfolio policy","        spec: Portfolio specification","        replay_mode: If True, read-only mode (no writes)","        ","    Returns:","        Dict mapping filename to SHA256 hash","    \"\"\"","    if replay_mode:","        logger.info(\"Replay mode: skipping artifact writes\")","        return {}","    ","    # Ensure output directory exists","    output_dir.mkdir(parents=True, exist_ok=True)","    ","    hashes = {}","    ","    # 1. Write portfolio_admission.parquet","    if decisions:","        admission_df = pd.DataFrame([d.model_dump() for d in decisions])","        admission_path = output_dir / \"portfolio_admission.parquet\"","        admission_df.to_parquet(admission_path, index=False)","        ","        # Compute hash","        admission_bytes = admission_path.read_bytes()","        hashes[\"portfolio_admission.parquet\"] = sha256_bytes(admission_bytes)","    ","    # 2. Write portfolio_state_timeseries.parquet","    if bar_states:","        # Convert bar_states to list of dicts","        states_list = []","        for state in bar_states.values():","            state_dict = state.model_dump()","            # Convert open_positions to count for simplicity","            state_dict[\"open_positions_count\"] = len(state.open_positions)","            # Remove the actual positions to keep file size manageable","            state_dict.pop(\"open_positions\", None)","            states_list.append(state_dict)","        ","        states_df = pd.DataFrame(states_list)","        states_path = output_dir / \"portfolio_state_timeseries.parquet\"","        states_df.to_parquet(states_path, index=False)","        ","        states_bytes = states_path.read_bytes()","        hashes[\"portfolio_state_timeseries.parquet\"] = sha256_bytes(states_bytes)","    ","    # 3. Write portfolio_summary.json","    summary_dict = summary.model_dump()","    summary_path = output_dir / \"portfolio_summary.json\"","    write_json_atomic(summary_path, summary_dict)","    ","    summary_bytes = canonical_json_bytes(summary_dict)","    hashes[\"portfolio_summary.json\"] = sha256_bytes(summary_bytes)","    ","    # 4. Write policy and spec for audit","    policy_dict = policy.model_dump()","    policy_path = output_dir / \"portfolio_policy.json\"","    write_json_atomic(policy_path, policy_dict)","    ","    spec_dict = spec.model_dump()","    spec_path = output_dir / \"portfolio_spec.json\"","    write_json_atomic(spec_path, spec_dict)","    ","    # 5. Create manifest","    manifest = {","        \"version\": \"PORTFOLIO_MANIFEST_V1\",","        \"created_at\": pd.Timestamp.now().isoformat(),","        \"policy_sha256\": sha256_bytes(canonical_json_bytes(policy_dict)),","        \"spec_sha256\": spec.spec_sha256 if hasattr(spec, \"spec_sha256\") else \"\",","        \"artifacts\": [","            {","                \"path\": path,","                \"sha256\": hash_val,","                \"type\": \"parquet\" if path.endswith(\".parquet\") else \"json\",","            }","            for path, hash_val in hashes.items()","        ],","        \"summary\": {","            \"total_candidates\": summary.total_candidates,","            \"accepted_count\": summary.accepted_count,","            \"rejected_count\": summary.rejected_count,","            \"final_slots_used\": summary.final_slots_used,","            \"final_margin_ratio\": summary.final_margin_ratio,","        },","    }","    ","    # Compute manifest hash (excluding the hash field itself)","    manifest_without_hash = manifest.copy()","    manifest_without_hash.pop(\"manifest_hash\", None)","    manifest_hash = sha256_bytes(canonical_json_bytes(manifest_without_hash))","    manifest[\"manifest_hash\"] = manifest_hash","    ","    # Write manifest","    manifest_path = output_dir / \"portfolio_manifest.json\"","    write_json_atomic(manifest_path, manifest)","    ","    hashes[\"portfolio_manifest.json\"] = manifest_hash","    ","    logger.info(f\"Portfolio artifacts written to {output_dir}\")","    logger.info(f\"Artifacts: {list(hashes.keys())}\")","    ","    return hashes","","","def compute_spec_sha256(spec: PortfolioSpecV1) -> str:","    \"\"\"","    Compute SHA256 hash of canonicalized portfolio spec.","    ","    Args:","        spec: Portfolio specification","        ","    Returns:","        SHA256 hex digest","    \"\"\"","    # Create dict without spec_sha256 field","    spec_dict = spec.model_dump()","    spec_dict.pop(\"spec_sha256\", None)","    ","    # Canonicalize and hash","    canonical = canonical_json_bytes(spec_dict)","    return sha256_bytes(canonical)","","","def compute_policy_sha256(policy: PortfolioPolicyV1) -> str:","    \"\"\"","    Compute SHA256 hash of canonicalized portfolio policy.","    ","    Args:","        policy: Portfolio policy","        ","    Returns:","        SHA256 hex digest","    \"\"\"","    policy_dict = policy.model_dump()","    canonical = canonical_json_bytes(policy_dict)","    return sha256_bytes(canonical)","","","# Setup logging","import logging","logger = logging.getLogger(__name__)"]}
{"type":"file_footer","path":"src/portfolio/artifacts_writer_v1.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/candidate_export.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6131,"sha256":"7fc50663ef86478c2d7bef4169133e7fbdefa2526649842a31470032e8b66eb5","total_lines":186,"chunk_count":1}
