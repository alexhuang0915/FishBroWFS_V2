{"type":"meta","schema_version":2,"run_id":"20251227_194933Z","repo_root":"/home/fishbro/FishBroWFS_V2","snapshot_root":"SNAPSHOT","part":7,"parts":10,"created_at":"2025-12-27T19:49:33Z","generator":{"name":"dump_context","version":"vNext-balanced"},"policies":{"max_parts":10,"target_bytes_per_part":500000,"chunk_max_lines":200,"chunk_max_bytes":120000,"outputs_max_bytes_for_content":2000000,"outputs_policy":"manifest_only_for_big_files","forbid_file_truncated":true,"balance_parts":true,"estimated_total_bytes":3588792,"balance_buffer_ratio":1.08}}
{"type":"file_chunk","path":"tests/test_state_processor_serialization.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Test StateProcessor serial execution for Attack #9.","","Tests that StateProcessor executes intents sequentially (single consumer)","and produces consistent SystemState snapshots.","\"\"\"","","import pytest","import asyncio","import time","from datetime import date, datetime","from concurrent.futures import ThreadPoolExecutor","","from core.intents import (","    CreateJobIntent, CalculateUnitsIntent, DataSpecIntent,","    IntentStatus, IntentType",")","from control.action_queue import ActionQueue, reset_action_queue","from core.processor import StateProcessor, ProcessingError, get_processor","from core.state import SystemState, JobStatus, create_initial_state","","","@pytest.fixture","def action_queue():","    \"\"\"Create a fresh ActionQueue for each test.\"\"\"","    reset_action_queue()","    queue = ActionQueue(max_size=100)","    yield queue","    queue.clear()","","","@pytest.fixture","def processor(action_queue):","    \"\"\"Create a StateProcessor with fresh queue.\"\"\"","    return StateProcessor(action_queue)","","","@pytest.fixture","def sample_data_spec():","    \"\"\"Create a sample DataSpecIntent for testing.\"\"\"","    return DataSpecIntent(","        dataset_id=\"test_dataset\",","        symbols=[\"MNQ\", \"MXF\"],","        timeframes=[\"60m\", \"120m\"],","        start_date=date(2020, 1, 1),","        end_date=date(2024, 12, 31)","    )","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_sequential_execution(processor, action_queue, sample_data_spec):","    \"\"\"Test that processor executes intents sequentially.\"\"\"","    # Start processor","    await processor.start()","    ","    # Submit multiple intents","    intent_ids = []","    for i in range(5):","        intent = CalculateUnitsIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        intent_id = processor.submit_intent(intent)","        intent_ids.append(intent_id)","    ","    # Wait for all intents to complete","    completed_count = 0","    start_time = time.time()","    timeout = 5.0","    ","    while completed_count < 5 and time.time() - start_time < timeout:","        completed_count = 0","        for intent_id in intent_ids:","            intent = action_queue.get_intent(intent_id)","            if intent and intent.status in [IntentStatus.COMPLETED, IntentStatus.FAILED]:","                completed_count += 1","        await asyncio.sleep(0.1)","    ","    # All intents should be completed","    assert completed_count == 5","    ","    # Check that they were processed in order (FIFO)","    # Since we can't easily track exact order without timestamps in test,","    # we at least verify all were processed","    metrics = action_queue.get_metrics()","    assert metrics[\"processed\"] == 5","    ","    await processor.stop()","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_state_updates(processor, action_queue, sample_data_spec):","    \"\"\"Test that processor updates system state correctly.\"\"\"","    # Start processor","    await processor.start()","    ","    # Get initial state","    initial_state = processor.get_state()","    assert initial_state.metrics.total_jobs == 0","    ","    # Submit a job creation intent","    intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10, \"window_slow\": 30}","    )","    ","    intent_id = processor.submit_intent(intent)","    ","    # Wait for completion","    completed = await processor.wait_for_intent(intent_id, timeout=5.0)","    assert completed is not None","    assert completed.status == IntentStatus.COMPLETED","    ","    # Check that state was updated","    final_state = processor.get_state()","    assert final_state.metrics.total_jobs == 1","    assert final_state.metrics.queued_jobs == 1","    ","    # Job should be in state","    job_id = completed.result[\"job_id\"]","    job = final_state.get_job(job_id)","    assert job is not None","    assert job.season == \"2024Q1\"","    assert job.status == JobStatus.QUEUED","    ","    await processor.stop()","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_error_handling(processor, action_queue):","    \"\"\"Test that processor handles errors gracefully.\"\"\"","    # Start processor","    await processor.start()","    ","    # Submit an invalid intent (missing required fields)","    # We'll create a malformed intent by directly manipulating a valid one","    from core.intents import CreateJobIntent, DataSpecIntent","    ","    # Create a data spec with empty symbols (should fail validation)","    invalid_data_spec = DataSpecIntent(","        dataset_id=\"test_dataset\",","        symbols=[],  # Empty - should fail validation","        timeframes=[\"60m\"],","        start_date=date(2020, 1, 1),","        end_date=date(2024, 12, 31)","    )","    ","    intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=invalid_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    intent_id = processor.submit_intent(intent)","    ","    # Wait for completion (should fail)","    completed = await processor.wait_for_intent(intent_id, timeout=5.0)","    assert completed is not None","    assert completed.status == IntentStatus.FAILED","    assert completed.error_message is not None","    assert \"validation\" in completed.error_message.lower() or \"empty\" in completed.error_message.lower()","    ","    # Check metrics","    state = processor.get_state()","    assert state.intent_queue.failed_count == 1","    ","    await processor.stop()","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_concurrent_submission(processor, action_queue, sample_data_spec):","    \"\"\"Test that processor handles concurrent intent submissions correctly.\"\"\"","    # Start processor","    await processor.start()","    ","    # Submit intents from multiple threads","    intent_ids = []","    ","    async def submit_intent(i: int):","        intent = CalculateUnitsIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        intent_id = processor.submit_intent(intent)","        intent_ids.append(intent_id)","        return intent_id","    ","    # Submit concurrently","    tasks = [submit_intent(i) for i in range(10)]","    await asyncio.gather(*tasks)"]}
{"type":"file_chunk","path":"tests/test_state_processor_serialization.py","chunk_index":1,"line_start":201,"line_end":354,"content":["    ","    # Wait for all to complete","    for intent_id in intent_ids:","        completed = await processor.wait_for_intent(intent_id, timeout=5.0)","        assert completed is not None","        assert completed.status == IntentStatus.COMPLETED","    ","    # All should be processed","    state = processor.get_state()","    assert state.intent_queue.completed_count == 10","    ","    await processor.stop()","","","def test_state_immutability():","    \"\"\"Test that SystemState is immutable (read-only).\"\"\"","    # Create initial state","    state = create_initial_state()","    ","    # Try to modify attributes (should fail or create new object)","    # Since Pydantic models with frozen=True raise ValidationError on modification","    with pytest.raises(Exception):","        state.metrics.total_jobs = 100  # Should fail","    ","    # Verify state hasn't changed","    assert state.metrics.total_jobs == 0","","","def test_state_snapshot_creation():","    \"\"\"Test creating state snapshots with updates.\"\"\"","    # Create initial state","    state = create_initial_state()","    ","    # Create snapshot with updates","    from core.state import create_state_snapshot, SystemMetrics","    ","    new_metrics = SystemMetrics(","        total_jobs=5,","        active_jobs=2,","        queued_jobs=3,","        completed_jobs=0,","        failed_jobs=0,","        total_units_processed=100,","        units_per_second=10.0,","        memory_usage_mb=512.0,","        cpu_usage_percent=25.0,","        disk_usage_gb=5.0,","        snapshot_timestamp=datetime.now(),","        uptime_seconds=3600.0","    )","    ","    new_state = create_state_snapshot(","        state,","        metrics=new_metrics,","        is_healthy=True","    )","    ","    # New state should have updated values","    assert new_state.metrics.total_jobs == 5","    assert new_state.metrics.active_jobs == 2","    assert new_state.is_healthy is True","    ","    # Original state should be unchanged","    assert state.metrics.total_jobs == 0","    assert state.metrics.active_jobs == 0","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_get_state_snapshot(processor):","    \"\"\"Test that get_state() returns consistent snapshots.\"\"\"","    # Start processor","    await processor.start()","    ","    # Get multiple state snapshots","    state1 = processor.get_state()","    await asyncio.sleep(0.1)","    state2 = processor.get_state()","    ","    # Snapshots should be different objects","    assert state1 is not state2","    assert state1.state_id != state2.state_id","    ","    # But should have same basic structure","    assert isinstance(state1, SystemState)","    assert isinstance(state2, SystemState)","    ","    await processor.stop()","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_queue_status_updates(processor, action_queue, sample_data_spec):","    \"\"\"Test that processor updates queue status in state.\"\"\"","    # Start processor","    await processor.start()","    ","    # Submit some intents","    for i in range(3):","        intent = CalculateUnitsIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        processor.submit_intent(intent)","    ","    # Wait a bit for processing to start","    await asyncio.sleep(0.5)","    ","    # Check queue status in state","    state = processor.get_state()","    assert state.intent_queue.queue_size >= 0","    assert state.intent_queue.completed_count >= 0","    ","    # Wait for all to complete","    await asyncio.sleep(2.0)","    ","    # Final state should show all completed","    final_state = processor.get_state()","    assert final_state.intent_queue.completed_count == 3","    ","    await processor.stop()","","","def test_processor_singleton():","    \"\"\"Test that get_processor() returns singleton instance.\"\"\"","    # Reset to ensure clean state","    reset_action_queue()","    ","    # First call should create instance","    processor1 = get_processor()","    assert processor1 is not None","    ","    # Second call should return same instance","    processor2 = get_processor()","    assert processor2 is processor1","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_stop_before_start():","    \"\"\"Test that processor can be stopped even if not started.\"\"\"","    from control.action_queue import ActionQueue","    from core.processor import StateProcessor","    ","    queue = ActionQueue()","    processor = StateProcessor(queue)","    ","    # Should not raise exception","    await processor.stop()","","","if __name__ == \"__main__\":","    # Run tests","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_state_processor_serialization.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_strategy_contract_purity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3435,"sha256":"d2ce83161e6bf0994ed3e0794396eaad44a609238fcf10846bf313f48b83c47f","total_lines":133,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_strategy_contract_purity.py","chunk_index":0,"line_start":1,"line_end":133,"content":["","\"\"\"Test strategy contract purity.","","Phase 7: Test that same input produces same output (deterministic).","\"\"\"","","from __future__ import annotations","","import numpy as np","import pytest","","from strategy.registry import get, load_builtin_strategies, clear","from engine.types import OrderIntent","","","@pytest.fixture(autouse=True)","def setup_registry() -> None:","    \"\"\"Setup registry before each test.\"\"\"","    clear()","    load_builtin_strategies()","    yield","    clear()","","","def test_sma_cross_purity() -> None:","    \"\"\"Test SMA cross strategy is deterministic.\"\"\"","    spec = get(\"sma_cross\")","    ","    # Create test features","    sma_fast = np.array([10.0, 11.0, 12.0, 13.0, 14.0])","    sma_slow = np.array([15.0, 14.0, 13.0, 12.0, 11.0])  # Cross at index 3","    ","    context = {","        \"bar_index\": 3,","        \"order_qty\": 1,","        \"features\": {","            \"sma_fast\": sma_fast,","            \"sma_slow\": sma_slow,","        },","    }","    ","    params = {","        \"fast_period\": 10.0,","        \"slow_period\": 20.0,","    }","    ","    # Run multiple times","    result1 = spec.fn(context, params)","    result2 = spec.fn(context, params)","    result3 = spec.fn(context, params)","    ","    # All results should be identical","    assert result1 == result2 == result3","    ","    # Check intents are identical","    intents1 = result1[\"intents\"]","    intents2 = result2[\"intents\"]","    intents3 = result3[\"intents\"]","    ","    assert len(intents1) == len(intents2) == len(intents3)","    ","    if len(intents1) > 0:","        # Compare intent attributes","        for i, (i1, i2, i3) in enumerate(zip(intents1, intents2, intents3)):","            assert i1.order_id == i2.order_id == i3.order_id","            assert i1.created_bar == i2.created_bar == i3.created_bar","            assert i1.role == i2.role == i3.role","            assert i1.kind == i2.kind == i3.kind","            assert i1.side == i2.side == i3.side","            assert i1.price == i2.price == i3.price","            assert i1.qty == i2.qty == i3.qty","","","def test_breakout_channel_purity() -> None:","    \"\"\"Test breakout channel strategy is deterministic.\"\"\"","    spec = get(\"breakout_channel\")","    ","    # Create test features","    high = np.array([100.0, 101.0, 102.0, 103.0, 105.0])","    close = np.array([99.0, 100.0, 101.0, 102.0, 104.0])","    channel_high = np.array([102.0, 102.0, 102.0, 102.0, 102.0])","    ","    context = {","        \"bar_index\": 4,","        \"order_qty\": 1,","        \"features\": {","            \"high\": high,","            \"close\": close,","            \"channel_high\": channel_high,","        },","    }","    ","    params = {","        \"channel_period\": 20.0,","    }","    ","    # Run multiple times","    result1 = spec.fn(context, params)","    result2 = spec.fn(context, params)","    ","    # Results should be identical","    assert result1 == result2","","","def test_mean_revert_zscore_purity() -> None:","    \"\"\"Test mean reversion z-score strategy is deterministic.\"\"\"","    spec = get(\"mean_revert_zscore\")","    ","    # Create test features","    zscore = np.array([-1.0, -1.5, -2.0, -2.5, -3.0])","    close = np.array([100.0, 99.0, 98.0, 97.0, 96.0])","    ","    context = {","        \"bar_index\": 2,","        \"order_qty\": 1,","        \"features\": {","            \"zscore\": zscore,","            \"close\": close,","        },","    }","    ","    params = {","        \"zscore_threshold\": -2.0,","    }","    ","    # Run multiple times","    result1 = spec.fn(context, params)","    result2 = spec.fn(context, params)","    ","    # Results should be identical","    assert result1 == result2","",""]}
{"type":"file_footer","path":"tests/test_strategy_contract_purity.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_strategy_registry.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4096,"sha256":"3cb58d0cd55cf2179f11f50b5df95394f7310b0f90066fb4ff76e1acb3eb597c","total_lines":172,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_strategy_registry.py","chunk_index":0,"line_start":1,"line_end":172,"content":["","\"\"\"Test strategy registry.","","Phase 7: Test registry list/get/register behavior is deterministic.","\"\"\"","","from __future__ import annotations","","import pytest","","from strategy.registry import (","    register,","    get,","    list_strategies,","    unregister,","    clear,","    load_builtin_strategies,",")","from strategy.spec import StrategySpec","","","def test_register_and_get() -> None:","    \"\"\"Test register and get operations.\"\"\"","    clear()","    ","    # Create a test strategy","    def test_fn(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {}}","    ","    spec = StrategySpec(","        strategy_id=\"test_strategy\",","        version=\"v1\",","        param_schema={\"type\": \"object\", \"properties\": {}},","        defaults={},","        fn=test_fn,","    )","    ","    # Register","    register(spec)","    ","    # Get","    retrieved = get(\"test_strategy\")","    assert retrieved.strategy_id == \"test_strategy\"","    assert retrieved.version == \"v1\"","    ","    # Cleanup","    unregister(\"test_strategy\")","","","def test_register_duplicate_raises() -> None:","    \"\"\"Test registering duplicate strategy_id raises ValueError.\"\"\"","    clear()","    ","    def test_fn1(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {}}","    ","    def test_fn2(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {\"different\": True}}","    ","    spec1 = StrategySpec(","        strategy_id=\"duplicate\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=test_fn1,","    )","    ","    spec2 = StrategySpec(","        strategy_id=\"duplicate\",","        version=\"v2\",","        param_schema={},","        defaults={},","        fn=test_fn2,","    )","    ","    register(spec1)","    ","    with pytest.raises(ValueError, match=\"already registered\"):","        register(spec2)","    ","    # Cleanup","    unregister(\"duplicate\")","","","def test_get_nonexistent_raises() -> None:","    \"\"\"Test getting nonexistent strategy raises KeyError.\"\"\"","    clear()","    ","    with pytest.raises(KeyError, match=\"not found\"):","        get(\"nonexistent\")","","","def test_list_strategies() -> None:","    \"\"\"Test list_strategies returns sorted list.\"\"\"","    clear()","    ","    def test_fn_a(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {\"fn\": \"a\"}}","    ","    def test_fn_b(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {\"fn\": \"b\"}}","    ","    def test_fn_c(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {\"fn\": \"c\"}}","    ","    # Register multiple strategies with different functions","    spec_b = StrategySpec(","        strategy_id=\"b_strategy\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=test_fn_b,","    )","    ","    spec_a = StrategySpec(","        strategy_id=\"a_strategy\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=test_fn_a,","    )","    ","    spec_c = StrategySpec(","        strategy_id=\"c_strategy\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=test_fn_c,","    )","    ","    register(spec_b)","    register(spec_a)","    register(spec_c)","    ","    # List should be sorted by strategy_id","    strategies = list_strategies()","    assert len(strategies) == 3","    assert strategies[0].strategy_id == \"a_strategy\"","    assert strategies[1].strategy_id == \"b_strategy\"","    assert strategies[2].strategy_id == \"c_strategy\"","    ","    # Cleanup","    clear()","","","def test_load_builtin_strategies() -> None:","    \"\"\"Test load_builtin_strategies registers built-in strategies.\"\"\"","    clear()","    ","    load_builtin_strategies()","    ","    strategies = list_strategies()","    strategy_ids = [s.strategy_id for s in strategies]","    ","    assert \"sma_cross\" in strategy_ids","    assert \"breakout_channel\" in strategy_ids","    assert \"mean_revert_zscore\" in strategy_ids","    ","    # Verify they can be retrieved","    sma_spec = get(\"sma_cross\")","    assert sma_spec.version == \"v1\"","    ","    breakout_spec = get(\"breakout_channel\")","    assert breakout_spec.version == \"v1\"","    ","    zscore_spec = get(\"mean_revert_zscore\")","    assert zscore_spec.version == \"v1\"","    ","    # Cleanup","    clear()","",""]}
{"type":"file_footer","path":"tests/test_strategy_registry.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_strategy_runner_outputs_intents.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3822,"sha256":"061705e5e9d3df47400ffa9a35cb3649fccb18d969f463af7d9b1347ad897dca","total_lines":143,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_strategy_runner_outputs_intents.py","chunk_index":0,"line_start":1,"line_end":143,"content":["","\"\"\"Test strategy runner outputs valid intents.","","Phase 7: Test that runner returns valid OrderIntent schema.","\"\"\"","","from __future__ import annotations","","import numpy as np","import pytest","","from strategy.runner import run_strategy","from strategy.registry import load_builtin_strategies, clear","from engine.types import OrderIntent, OrderRole, OrderKind, Side","","","@pytest.fixture(autouse=True)","def setup_registry() -> None:","    \"\"\"Setup registry before each test.\"\"\"","    clear()","    load_builtin_strategies()","    yield","    clear()","","","def test_runner_outputs_intents_schema() -> None:","    \"\"\"Test runner outputs valid OrderIntent schema.\"\"\"","    # Create test features","    sma_fast = np.array([10.0, 11.0, 12.0, 13.0, 14.0])","    sma_slow = np.array([15.0, 14.0, 13.0, 12.0, 11.0])","    ","    features = {","        \"sma_fast\": sma_fast,","        \"sma_slow\": sma_slow,","    }","    ","    params = {","        \"fast_period\": 10.0,","        \"slow_period\": 20.0,","    }","    ","    context = {","        \"bar_index\": 3,","        \"order_qty\": 1,","    }","    ","    # Run strategy","    intents = run_strategy(\"sma_cross\", features, params, context)","    ","    # Verify intents is a list","    assert isinstance(intents, list)","    ","    # Verify each intent is OrderIntent","    for intent in intents:","        assert isinstance(intent, OrderIntent)","        ","        # Verify required fields","        assert isinstance(intent.order_id, int)","        assert isinstance(intent.created_bar, int)","        assert isinstance(intent.role, OrderRole)","        assert isinstance(intent.kind, OrderKind)","        assert isinstance(intent.side, Side)","        assert isinstance(intent.price, float)","        assert isinstance(intent.qty, int)","        ","        # Verify values are reasonable","        assert intent.order_id > 0","        assert intent.created_bar >= 0","        assert intent.price > 0","        assert intent.qty > 0","","","def test_runner_uses_defaults() -> None:","    \"\"\"Test runner uses default parameters when missing.\"\"\"","    features = {","        \"sma_fast\": np.array([10.0, 11.0]),","        \"sma_slow\": np.array([15.0, 14.0]),","    }","    ","    # Missing params - should use defaults","    params = {}","    ","    context = {","        \"bar_index\": 1,","        \"order_qty\": 1,","    }","    ","    # Should not raise - defaults should be used","    intents = run_strategy(\"sma_cross\", features, params, context)","    assert isinstance(intents, list)","","","def test_runner_allows_extra_params() -> None:","    \"\"\"Test runner allows extra parameters (logs warning but doesn't fail).\"\"\"","    features = {","        \"sma_fast\": np.array([10.0, 11.0]),","        \"sma_slow\": np.array([15.0, 14.0]),","    }","    ","    # Extra param not in schema","    params = {","        \"fast_period\": 10.0,","        \"slow_period\": 20.0,","        \"extra_param\": 999.0,  # Not in schema","    }","    ","    context = {","        \"bar_index\": 1,","        \"order_qty\": 1,","    }","    ","    # Should not raise - extra params allowed","    intents = run_strategy(\"sma_cross\", features, params, context)","    assert isinstance(intents, list)","","","def test_runner_invalid_output_raises() -> None:","    \"\"\"Test runner raises ValueError for invalid strategy output.\"\"\"","    from strategy.registry import register","    from strategy.spec import StrategySpec","    ","    # Create a bad strategy that returns invalid output","    def bad_strategy(context: dict, params: dict) -> dict:","        return {\"invalid\": \"output\"}  # Missing \"intents\" key","    ","    bad_spec = StrategySpec(","        strategy_id=\"bad_strategy\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=bad_strategy,","    )","    ","    register(bad_spec)","    ","    with pytest.raises(ValueError, match=\"must contain 'intents' key\"):","        run_strategy(\"bad_strategy\", {}, {}, {\"bar_index\": 0})","    ","    # Cleanup","    from strategy.registry import unregister","    unregister(\"bad_strategy\")","",""]}
{"type":"file_footer","path":"tests/test_strategy_runner_outputs_intents.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_submit_returns_503_when_worker_missing.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":16245,"sha256":"54c37644c882d760292e890644f35c112c85f541450c13e02db81e5d5b4cc443","total_lines":422,"chunk_count":3}
{"type":"file_chunk","path":"tests/test_submit_returns_503_when_worker_missing.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Test that job submission returns HTTP 503 when worker is unavailable.","","EOOR500 â†’ HTTP 503 (WORKER-AWARE) requirement:","- All job submission endpoints must return HTTP 503 Service Unavailable when worker is unavailable","- Never return HTTP 500 for worker unavailability","- Error message must mention worker explicitly","- JSON response must include diagnostic details","\"\"\"","","from __future__ import annotations","","import tempfile","import os","from pathlib import Path","from unittest.mock import patch, MagicMock","","import pytest","from fastapi.testclient import TestClient","","from control.api import app, get_db_path","from control.jobs_db import init_db","","","@pytest.fixture","def test_client_no_worker() -> TestClient:","    \"\"\"Create test client with temporary database and no worker.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        db_path = Path(tmpdir) / \"test.db\"","        init_db(db_path)","        ","        # Save original environment variables","        original_jobs_db_path = os.environ.get(\"JOBS_DB_PATH\")","        original_allow_tmp_db = os.environ.get(\"FISHBRO_ALLOW_TMP_DB\")","        original_allow_spawn = os.environ.get(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\")","        ","        # Override DB path","        os.environ[\"JOBS_DB_PATH\"] = str(db_path)","        # Allow /tmp DB paths (required for temporary DB)","        os.environ[\"FISHBRO_ALLOW_TMP_DB\"] = \"1\"","        # DO NOT allow worker spawn in tests for this fixture (we want to test 503)","        os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = \"0\"","        ","        # Re-import to get new DB path","        from control import api","        ","        # Reinitialize","        api.init_db(db_path)","        ","        # Mock worker status to simulate no worker","        with patch('control.api._check_worker_status') as mock_check, \\","             patch('control.api.load_dataset_index') as mock_load_dataset:","            mock_check.return_value = {","                \"alive\": False,","                \"pid\": None,","                \"last_heartbeat_age_sec\": None,","                \"reason\": \"pidfile missing\",","                \"expected_db\": str(db_path),","            }","            # Mock dataset index to avoid FileNotFoundError","            from data.dataset_registry import DatasetIndex, DatasetRecord","            from datetime import date","            mock_index = DatasetIndex(","                generated_at=\"2024-01-01T00:00:00Z\",","                datasets=[","                    DatasetRecord(","                        id=\"test_dataset\",","                        symbol=\"TEST\",","                        exchange=\"TEST\",","                        timeframe=\"60m\",","                        path=\"TEST/60m/2020-2024.parquet\",","                        start_date=date(2020, 1, 1),","                        end_date=date(2024, 12, 31),","                        fingerprint_sha256_40=\"d\" * 40,","                        tz_provider=\"IANA\",","                        tz_version=\"unknown\",","                    )","                ]","            )","            mock_load_dataset.return_value = mock_index","            try:","                yield TestClient(app)","            finally:","                # Restore original environment variables","                if original_jobs_db_path is not None:","                    os.environ[\"JOBS_DB_PATH\"] = original_jobs_db_path","                else:","                    os.environ.pop(\"JOBS_DB_PATH\", None)","                if original_allow_tmp_db is not None:","                    os.environ[\"FISHBRO_ALLOW_TMP_DB\"] = original_allow_tmp_db","                else:","                    os.environ.pop(\"FISHBRO_ALLOW_TMP_DB\", None)","                if original_allow_spawn is not None:","                    os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = original_allow_spawn","                else:","                    os.environ.pop(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\", None)","","","@pytest.fixture","def test_client_with_worker() -> TestClient:","    \"\"\"Create test client with temporary database and worker running.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        db_path = Path(tmpdir) / \"test.db\"","        init_db(db_path)","        ","        # Save original environment variables","        original_jobs_db_path = os.environ.get(\"JOBS_DB_PATH\")","        original_allow_tmp_db = os.environ.get(\"FISHBRO_ALLOW_TMP_DB\")","        original_allow_spawn = os.environ.get(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\")","        ","        # Override DB path","        os.environ[\"JOBS_DB_PATH\"] = str(db_path)","        os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = \"1\"","        os.environ[\"FISHBRO_ALLOW_TMP_DB\"] = \"1\"","        ","        # Re-import to get new DB path","        from control import api","        ","        # Reinitialize","        api.init_db(db_path)","        ","        # Mock worker status to simulate worker running","        with patch('control.api._check_worker_status') as mock_check, \\","             patch('control.api.load_dataset_index') as mock_load_dataset:","            mock_check.return_value = {","                \"alive\": True,","                \"pid\": 12345,","                \"last_heartbeat_age_sec\": 1.0,","                \"reason\": \"worker alive\",","                \"expected_db\": str(db_path),","            }","            # Mock dataset index to avoid FileNotFoundError","            from data.dataset_registry import DatasetIndex, DatasetRecord","            from datetime import date","            mock_index = DatasetIndex(","                generated_at=\"2024-01-01T00:00:00Z\",","                datasets=[","                    DatasetRecord(","                        id=\"test_dataset\",","                        symbol=\"TEST\",","                        exchange=\"TEST\",","                        timeframe=\"60m\",","                        path=\"TEST/60m/2020-2024.parquet\",","                        start_date=date(2020, 1, 1),","                        end_date=date(2024, 12, 31),","                        fingerprint_sha256_40=\"d\" * 40,","                        tz_provider=\"IANA\",","                        tz_version=\"unknown\",","                    )","                ]","            )","            mock_load_dataset.return_value = mock_index","            try:","                yield TestClient(app)","            finally:","                # Restore original environment variables","                if original_jobs_db_path is not None:","                    os.environ[\"JOBS_DB_PATH\"] = original_jobs_db_path","                else:","                    os.environ.pop(\"JOBS_DB_PATH\", None)","                if original_allow_tmp_db is not None:","                    os.environ[\"FISHBRO_ALLOW_TMP_DB\"] = original_allow_tmp_db","                else:","                    os.environ.pop(\"FISHBRO_ALLOW_TMP_DB\", None)","                if original_allow_spawn is not None:","                    os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = original_allow_spawn","                else:","                    os.environ.pop(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\", None)","","","def test_submit_job_returns_503_when_worker_missing(test_client_no_worker: TestClient) -> None:","    \"\"\"Test POST /jobs returns 503 when worker is unavailable.\"\"\"","    req = {","        \"season\": \"test_season\",","        \"dataset_id\": \"test_dataset\",","        \"outputs_root\": \"outputs\",","        \"config_snapshot\": {\"bars\": 1000, \"params_total\": 100},","        \"config_hash\": \"abc123\",","        \"created_by\": \"b5c\",","    }","    ","    resp = test_client_no_worker.post(\"/jobs\", json=req)","    ","    # Must return HTTP 503, not 500","    assert resp.status_code == 503, f\"Expected 503, got {resp.status_code}\"","    ","    # Must have proper JSON structure","    data = resp.json()","    assert \"detail\" in data","    detail = data[\"detail\"]","    ","    # Error message must mention worker (detail is a dict with \"message\" field)","    assert isinstance(detail, dict)","    assert \"message\" in detail","    assert \"worker\" in detail[\"message\"].lower(), f\"Error message should mention worker: {detail['message']}\"","    ","    # Should not be generic 500 error","    assert \"internal server error\" not in detail[\"message\"].lower()","    ","    # Check response structure matches our error format","    assert \"error\" in detail"]}
{"type":"file_chunk","path":"tests/test_submit_returns_503_when_worker_missing.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    assert detail[\"error\"] == \"WORKER_UNAVAILABLE\"","    assert \"worker\" in detail","    assert \"action\" in detail","","","def test_batch_submit_returns_503_when_worker_missing(test_client_no_worker: TestClient) -> None:","    \"\"\"Test POST /jobs/batch returns 503 when worker is unavailable.\"\"\"","    from datetime import date","    ","    req = {","        \"jobs\": [","            {","                \"season\": \"test_season\",","                \"data1\": {","                    \"dataset_id\": \"test_dataset\",","                    \"start_date\": \"2020-01-01\",","                    \"end_date\": \"2024-12-31\"","                },","                \"data2\": None,","                \"strategy_id\": \"test_strategy\",","                \"params\": {},","                \"wfs\": {","                    \"stage0_subsample\": 1.0,","                    \"top_k\": 100,","                    \"mem_limit_mb\": 4096,","                    \"allow_auto_downsample\": True","                }","            }","        ]","    }","    ","    resp = test_client_no_worker.post(\"/jobs/batch\", json=req)","    ","    # Must return HTTP 503, not 500","    assert resp.status_code == 503, f\"Expected 503, got {resp.status_code}\"","    ","    # Must have proper JSON structure","    data = resp.json()","    assert \"detail\" in data","    detail = data[\"detail\"]","    ","    # Error message must mention worker (detail is a dict with \"message\" field)","    assert isinstance(detail, dict)","    assert \"message\" in detail","    assert \"worker\" in detail[\"message\"].lower(), f\"Error message should mention worker: {detail['message']}\"","    ","    # Should not be generic 500 error","    assert \"internal server error\" not in detail[\"message\"].lower()","    ","    # Check response structure matches our error format","    assert \"error\" in detail","    assert detail[\"error\"] == \"WORKER_UNAVAILABLE\"","","","def test_submit_job_succeeds_when_worker_running(test_client_with_worker: TestClient) -> None:","    \"\"\"Test POST /jobs succeeds when worker is available.\"\"\"","    req = {","        \"season\": \"test_season\",","        \"dataset_id\": \"test_dataset\",","        \"outputs_root\": \"outputs\",","        \"config_snapshot\": {\"bars\": 1000, \"params_total\": 100},","        \"config_hash\": \"abc123\",","        \"created_by\": \"b5c\",","    }","    ","    resp = test_client_with_worker.post(\"/jobs\", json=req)","    ","    # Should succeed (200 or 201)","    assert resp.status_code in (200, 201), f\"Expected success, got {resp.status_code}\"","    ","    data = resp.json()","    assert \"job_id\" in data","    assert isinstance(data[\"job_id\"], str)","","","def test_batch_submit_succeeds_when_worker_running(test_client_with_worker: TestClient) -> None:","    \"\"\"Test POST /jobs/batch succeeds when worker is available.\"\"\"","    from datetime import date","    ","    req = {","        \"jobs\": [","            {","                \"season\": \"test_season\",","                \"data1\": {","                    \"dataset_id\": \"test_dataset\",","                    \"start_date\": \"2020-01-01\",","                    \"end_date\": \"2024-12-31\"","                },","                \"data2\": None,","                \"strategy_id\": \"test_strategy\",","                \"params\": {},","                \"wfs\": {","                    \"stage0_subsample\": 1.0,","                    \"top_k\": 100,","                    \"mem_limit_mb\": 4096,","                    \"allow_auto_downsample\": True","                }","            }","        ]","    }","    ","    resp = test_client_with_worker.post(\"/jobs/batch\", json=req)","    ","    # Should succeed (200 or 201)","    assert resp.status_code in (200, 201), f\"Expected success, got {resp.status_code}\"","    ","    data = resp.json()","    assert \"batch_id\" in data","    assert isinstance(data[\"batch_id\"], str)","","","def test_worker_status_check_integration() -> None:","    \"\"\"Test that _check_worker_status function works correctly.\"\"\"","    from control.api import _check_worker_status","    from pathlib import Path","    ","    # Create a temporary DB path for testing","    db_path = Path(\"/tmp/test.db\")","    ","    # Mock the dependencies","    with patch('control.api.validate_pidfile') as mock_validate, \\","         patch('control.api.time.time') as mock_time, \\","         patch('control.api.Path.exists') as mock_exists, \\","         patch('control.api.Path.read_text') as mock_read_text, \\","         patch('control.api.Path.stat') as mock_stat:","        ","        # Test case 1: pidfile doesn't exist","        mock_exists.return_value = False","        result = _check_worker_status(db_path)","        assert not result[\"alive\"]","        assert result[\"reason\"] == \"pidfile missing\"","        ","        # Test case 2: pidfile exists but corrupted (read_text raises ValueError)","        mock_exists.return_value = True","        mock_validate.return_value = (True, \"\")  # pidfile is valid","        mock_read_text.side_effect = ValueError(\"invalid literal\")","        result = _check_worker_status(db_path)","        assert not result[\"alive\"]","        assert \"corrupted\" in result[\"reason\"]","        ","        # Test case 3: pidfile exists, validate_pidfile returns invalid","        mock_exists.return_value = True","        mock_validate.return_value = (False, \"pidfile stale\")","        # Clear any side effect from previous test","        mock_read_text.side_effect = None","        # read_text won't be called because validate_pidfile returns invalid","        result = _check_worker_status(db_path)","        assert not result[\"alive\"]","        assert result[\"reason\"] == \"pidfile stale\"","        ","        # Test case 4: pidfile exists, process alive, heartbeat stale","        mock_exists.return_value = True","        mock_read_text.side_effect = None  # Clear side effect","        mock_read_text.return_value = \"12345\"","        mock_validate.return_value = (True, \"\")","        # Mock stat for heartbeat file","        mock_stat_obj = MagicMock()","        mock_stat_obj.st_mtime = 1000.0","        mock_stat.return_value = mock_stat_obj","        mock_time.return_value = 2000.0  # Current time (1000 seconds later)","        result = _check_worker_status(db_path)","        assert result[\"alive\"]  # Process is alive","        assert result[\"last_heartbeat_age_sec\"] == 1000.0","        ","        # Test case 5: pidfile exists, process alive, heartbeat fresh","        mock_stat_obj = MagicMock()","        mock_stat_obj.st_mtime = 1995.0  # 5 seconds ago","        mock_stat.return_value = mock_stat_obj","        mock_time.return_value = 2000.0  # Current time","        result = _check_worker_status(db_path)","        assert result[\"alive\"]","        assert result[\"last_heartbeat_age_sec\"] == 5.0","","","def test_error_message_includes_diagnostics() -> None:","    \"\"\"Test that 503 error message includes diagnostic details.\"\"\"","    from control.api import require_worker_or_503","    from pathlib import Path","    import os","    ","    # Create a temporary DB path for testing","    db_path = Path(\"/tmp/test.db\")","    ","    # Mock _check_worker_status to return False with specific reason","    with patch('control.api._check_worker_status') as mock_check:","        mock_check.return_value = {","            \"alive\": False,","            \"pid\": None,","            \"last_heartbeat_age_sec\": None,","            \"reason\": \"pidfile missing\",","            \"expected_db\": str(db_path),","        }","        ","        # Ensure the environment variable does NOT allow spawn in tests","        original = os.environ.get(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\")","        os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = \"0\"","        try:","            # Should raise HTTPException with 503","            import fastapi","            try:"]}
{"type":"file_chunk","path":"tests/test_submit_returns_503_when_worker_missing.py","chunk_index":2,"line_start":401,"line_end":422,"content":["                require_worker_or_503(db_path)","                assert False, \"Should have raised HTTPException\"","            except fastapi.HTTPException as e:","                assert e.status_code == 503","                detail = e.detail","                # Check structure","                assert isinstance(detail, dict)","                assert \"error\" in detail","                assert detail[\"error\"] == \"WORKER_UNAVAILABLE\"","                assert \"worker\" in detail","                assert \"action\" in detail","                assert \"message\" in detail","                assert \"worker\" in detail[\"message\"].lower()","        finally:","            if original is not None:","                os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = original","            else:","                os.environ.pop(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\", None)","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_submit_returns_503_when_worker_missing.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"tests/test_trigger_rate_param_subsample_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":14011,"sha256":"c65f24b8d49cc43ffbf10358f8152582df4105b0250e749d7a800027c5dd7a96","total_lines":347,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_trigger_rate_param_subsample_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Stage P2-3: Contract Tests for Param-subsample Trigger Rate","","Verifies that trigger_rate controls param subsampling:","- selected_params_count scales with trigger_rate","- intents_total scales approximately linearly with trigger_rate","- Workload reduction is effective","\"\"\"","from __future__ import annotations","","import numpy as np","import os","","from pipeline.runner_grid import run_grid","","","def test_selected_params_count_reasonable() -> None:","    \"\"\"","    Test that selected_params_count is reasonable for given trigger_rate.","    ","    With n_params=1000 and trigger_rate=0.05, we expect selected_params_count","    to be approximately 50 (allowing rounding error).","    \"\"\"","    # Ensure clean environment","    old_param_subsample_rate = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","    old_param_subsample_seed = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","    ","    try:","        n_bars = 500","        n_params = 1000","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)","            atr_len = 10 + (i % 5)","            stop_mult = 1.0 + (i % 3) * 0.5","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Set param_subsample_rate=0.05","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"0.05\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"","        ","        result = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify perf dict contains trigger rate info","        assert \"perf\" in result, \"perf must exist in run_grid result\"","        perf = result[\"perf\"]","        assert isinstance(perf, dict), \"perf must be a dict\"","        ","        selected_params_count = perf.get(\"selected_params_count\")","        param_subsample_rate_configured = perf.get(\"param_subsample_rate_configured\")","        selected_params_ratio = perf.get(\"selected_params_ratio\")","        ","        assert selected_params_count is not None, \"selected_params_count must exist\"","        assert param_subsample_rate_configured is not None, \"param_subsample_rate_configured must exist\"","        assert selected_params_ratio is not None, \"selected_params_ratio must exist\"","        ","        assert param_subsample_rate_configured == 0.05, f\"param_subsample_rate_configured should be 0.05, got {param_subsample_rate_configured}\"","        ","        # Contract: selected_params_count should be approximately 5% of n_params","        # Allow rounding error: [45, 55] for n_params=1000, rate=0.05","        assert 45 <= selected_params_count <= 55, (","            f\"selected_params_count ({selected_params_count}) should be approximately 50 \"","            f\"(5% of {n_params}), got {selected_params_count}\"","        )","        ","        # Contract: selected_params_ratio should match trigger_rate approximately","        expected_ratio = 0.05","        assert 0.04 <= selected_params_ratio <= 0.06, (","            f\"selected_params_ratio ({selected_params_ratio}) should be approximately \"","            f\"{expected_ratio}, got {selected_params_ratio}\"","        )","        ","        # Contract: metrics_rows_computed should equal selected_params_count","        metrics_rows_computed = perf.get(\"metrics_rows_computed\")","        assert metrics_rows_computed == selected_params_count, (","            f\"metrics_rows_computed ({metrics_rows_computed}) should equal \"","            f\"selected_params_count ({selected_params_count})\"","        )","        ","    finally:","        # Restore environment","        if old_param_subsample_rate is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = old_param_subsample_rate","        ","        if old_param_subsample_seed is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = old_param_subsample_seed","","","def test_intents_total_linear_scaling() -> None:","    \"\"\"","    Test that intents_total scales approximately linearly with trigger_rate.","    ","    This verifies workload reduction: when we run 5% of params, intents_total","    should be approximately 5% of baseline.","    \"\"\"","    # Ensure clean environment","    old_param_subsample_rate = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","    old_param_subsample_seed = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","    ","    try:","        n_bars = 500","        n_params = 200","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)","            atr_len = 10 + (i % 5)","            stop_mult = 1.0 + (i % 3) * 0.5","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Run A: param_subsample_rate=1.0 (baseline, all params)","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"1.0\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"","        ","        result_a = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Run B: param_subsample_rate=0.05 (5% of params)","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"0.05\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"  # Same seed for deterministic selection","        ","        result_b = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify perf dicts","        perf_a = result_a.get(\"perf\", {})","        perf_b = result_b.get(\"perf\", {})","        ","        assert isinstance(perf_a, dict), \"perf_a must be a dict\"","        assert isinstance(perf_b, dict), \"perf_b must be a dict\"","        ","        intents_total_a = perf_a.get(\"intents_total\")","        intents_total_b = perf_b.get(\"intents_total\")","        ","        assert intents_total_a is not None, \"intents_total_a must exist\"","        assert intents_total_b is not None, \"intents_total_b must exist\"","        ","        # Contract: intents_total_B should be <= intents_total_A * 0.07 (allowing overhead)","        # With 5% params, we expect approximately 5% workload, but allow up to 7% for overhead","        if intents_total_a > 0:"]}
{"type":"file_chunk","path":"tests/test_trigger_rate_param_subsample_contract.py","chunk_index":1,"line_start":201,"line_end":347,"content":["            ratio = intents_total_b / intents_total_a","            assert ratio <= 0.07, (","                f\"intents_total_B ({intents_total_b}) should be <= intents_total_A * 0.07 \"","                f\"({intents_total_a * 0.07}), got ratio {ratio:.4f}\"","            )","        ","        # Verify selected_params_count scaling","        selected_count_a = perf_a.get(\"selected_params_count\", n_params)","        selected_count_b = perf_b.get(\"selected_params_count\")","        ","        assert selected_count_b is not None, \"selected_params_count_B must exist\"","        assert selected_count_b < selected_count_a, (","            f\"selected_params_count_B ({selected_count_b}) should be < \"","            f\"selected_params_count_A ({selected_count_a})\"","        )","        ","    finally:","        # Restore environment","        if old_param_subsample_rate is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = old_param_subsample_rate","        ","        if old_param_subsample_seed is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = old_param_subsample_seed","","","def test_metrics_shape_preserved() -> None:","    \"\"\"","    Test that metrics shape is preserved (n_params, METRICS_N_COLUMNS) even with subsampling.","    ","    Only selected rows should be computed; unselected rows remain zeros.","    Uses metrics_computed_mask to verify which rows were computed.","    \"\"\"","    # Ensure clean environment","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    old_param_subsample_rate = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","    old_param_subsample_seed = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","    ","    try:","        n_bars = 300","        n_params = 100","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)","            atr_len = 10 + (i % 5)","            stop_mult = 1.0","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Fix trigger_rate=1.0 (no intent-level sparsity) to test param subsample only","        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","        # Set param_subsample_rate=0.1 (10% of params)","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"0.1\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"","        ","        result = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify metrics shape is preserved","        metrics = result.get(\"metrics\")","        assert metrics is not None, \"metrics must exist\"","        assert isinstance(metrics, np.ndarray), \"metrics must be np.ndarray\"","        assert metrics.shape == (n_params, 3), (","            f\"metrics shape should be ({n_params}, 3), got {metrics.shape}\"","        )","        ","        # Verify perf dict","        perf = result.get(\"perf\", {})","        metrics_rows_computed = perf.get(\"metrics_rows_computed\")","        selected_params_count = perf.get(\"selected_params_count\")","        metrics_computed_mask = perf.get(\"metrics_computed_mask\")","        ","        assert metrics_rows_computed == selected_params_count, (","            f\"metrics_rows_computed ({metrics_rows_computed}) should equal \"","            f\"selected_params_count ({selected_params_count})\"","        )","        ","        # Verify metrics_computed_mask exists and has correct shape","        assert metrics_computed_mask is not None, \"metrics_computed_mask must exist in perf\"","        assert isinstance(metrics_computed_mask, list), \"metrics_computed_mask must be a list\"","        assert len(metrics_computed_mask) == n_params, (","            f\"metrics_computed_mask length ({len(metrics_computed_mask)}) should equal n_params ({n_params})\"","        )","        ","        # Convert to numpy array for easier manipulation","        mask_array = np.array(metrics_computed_mask, dtype=bool)","        ","        # Verify that mask sum equals selected_params_count","        assert np.sum(mask_array) == selected_params_count, (","            f\"metrics_computed_mask sum ({np.sum(mask_array)}) should equal \"","            f\"selected_params_count ({selected_params_count})\"","        )","        ","        # Verify that uncomputed rows remain all zeros","        uncomputed_non_zero = np.sum(np.any(np.abs(metrics[~mask_array]) > 1e-10, axis=1))","        assert uncomputed_non_zero == 0, (","            f\"Uncomputed rows with non-zero metrics ({uncomputed_non_zero}) should be 0\"","        )","        ","        # NOTE: Do NOT require computed rows to be non-zero.","        # It's valid to have entry fills but no exits (trades=0), producing all-zero metrics.","        # Evidence of computation is provided by metrics_rows_computed == selected_params_count","        # and the metrics_computed_mask bookkeeping above.","        ","    finally:","        # Restore environment","        if old_trigger_rate is None:","            os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","        ","        if old_param_subsample_rate is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = old_param_subsample_rate","        ","        if old_param_subsample_seed is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = old_param_subsample_seed","",""]}
{"type":"file_footer","path":"tests/test_trigger_rate_param_subsample_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_ui_artifact_validation.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":19043,"sha256":"7290feedadbee1e240b8d7ab6b12191d04bd3acf239038d77ef80060a5dddc48","total_lines":541,"chunk_count":3}
{"type":"file_chunk","path":"tests/test_ui_artifact_validation.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Tests for UI artifact validation.","","Tests verify:","1. MISSING status when file does not exist","2. INVALID status when schema validation fails (with readable error messages)","3. DIRTY status when config_hash mismatch","4. OK status when validation passes","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","","import pytest","","from core.artifact_reader import ReadResult, SafeReadResult, try_read_artifact","from core.artifact_status import (","    ArtifactStatus,","    ValidationResult,","    validate_governance_status,","    validate_manifest_status,","    validate_winners_v2_status,",")","from core.schemas.governance import GovernanceReport","from core.schemas.manifest import RunManifest","from core.schemas.winners_v2 import WinnersV2","from gui.viewer.schema import EvidenceLink","","","# Fixtures","@pytest.fixture","def fixtures_dir() -> Path:","    \"\"\"Return path to test fixtures directory.\"\"\"","    return Path(__file__).parent / \"fixtures\" / \"artifacts\"","","","# Note: temp_dir fixture is now defined in conftest.py for all tests","# This local definition is kept for backward compatibility but will be shadowed by conftest.py","","","# Test: MISSING status","def test_manifest_missing_file(temp_dir: Path) -> None:","    \"\"\"Test that missing manifest.json returns MISSING status.\"\"\"","    manifest_path = temp_dir / \"manifest.json\"","    ","    result = validate_manifest_status(str(manifest_path))","    ","    assert result.status == ArtifactStatus.MISSING","    assert \"ä¸å­˜åœ¨\" in result.message or \"not found\" in result.message.lower()","","","def test_winners_v2_missing_file(temp_dir: Path) -> None:","    \"\"\"Test that missing winners_v2.json returns MISSING status.\"\"\"","    winners_path = temp_dir / \"winners_v2.json\"","    ","    result = validate_winners_v2_status(str(winners_path))","    ","    assert result.status == ArtifactStatus.MISSING","    assert \"ä¸å­˜åœ¨\" in result.message or \"not found\" in result.message.lower()","","","def test_governance_missing_file(temp_dir: Path) -> None:","    \"\"\"Test that missing governance.json returns MISSING status.\"\"\"","    governance_path = temp_dir / \"governance.json\"","    ","    result = validate_governance_status(str(governance_path))","    ","    assert result.status == ArtifactStatus.MISSING","    assert \"ä¸å­˜åœ¨\" in result.message or \"not found\" in result.message.lower()","","","# Test: INVALID status (schema validation errors)","def test_manifest_invalid_missing_field(fixtures_dir: Path) -> None:","    \"\"\"Test that manifest with missing required field returns INVALID.\"\"\"","    manifest_path = fixtures_dir / \"manifest_missing_field.json\"","    ","    # Load data","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        manifest_data = json.load(f)","    ","    result = validate_manifest_status(str(manifest_path), manifest_data=manifest_data)","    ","    assert result.status == ArtifactStatus.INVALID","    assert \"ç¼ºå°‘æ¬„ä½\" in result.message or \"missing\" in result.message.lower() or \"required\" in result.message.lower()","    # Should mention config_hash or season (required fields)","    assert \"config_hash\" in result.message or \"season\" in result.message or \"run_id\" in result.message","","","def test_winners_v2_invalid_missing_field(fixtures_dir: Path) -> None:","    \"\"\"Test that winners_v2 with missing required field returns INVALID.\"\"\"","    winners_path = fixtures_dir / \"winners_v2_missing_field.json\"","    ","    # Load data","    with winners_path.open(\"r\", encoding=\"utf-8\") as f:","        winners_data = json.load(f)","    ","    result = validate_winners_v2_status(str(winners_path), winners_data=winners_data)","    ","    assert result.status == ArtifactStatus.INVALID","    assert \"ç¼ºå°‘æ¬„ä½\" in result.message or \"missing\" in result.message.lower() or \"required\" in result.message.lower()","    # Should mention net_profit, max_drawdown, or trades (required in WinnerRow)","    assert any(field in result.message for field in [\"net_profit\", \"max_drawdown\", \"trades\", \"metrics\"])","","","def test_governance_invalid_missing_field(temp_dir: Path) -> None:","    \"\"\"Test that governance with missing required field returns INVALID.\"\"\"","    governance_path = temp_dir / \"governance.json\"","    ","    # Create invalid governance (missing run_id)","    invalid_data = {","        \"items\": [","            {","                \"candidate_id\": \"test:123\",","                \"decision\": \"KEEP\",","            }","        ]","    }","    ","    with governance_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(invalid_data, f)","    ","    result = validate_governance_status(str(governance_path), governance_data=invalid_data)","    ","    assert result.status == ArtifactStatus.INVALID","    assert \"ç¼ºå°‘æ¬„ä½\" in result.message or \"missing\" in result.message.lower() or \"required\" in result.message.lower()","","","# Test: DIRTY status (config_hash mismatch)","def test_manifest_dirty_config_hash(fixtures_dir: Path) -> None:","    \"\"\"Test that manifest with mismatched config_hash returns DIRTY.\"\"\"","    manifest_path = fixtures_dir / \"manifest_valid.json\"","    ","    # Load data","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        manifest_data = json.load(f)","    ","    # Validate with different expected config_hash","    result = validate_manifest_status(","        str(manifest_path),","        manifest_data=manifest_data,","        expected_config_hash=\"different_hash\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"config_hash\" in result.message.lower()","","","def test_winners_v2_dirty_config_hash(temp_dir: Path) -> None:","    \"\"\"Test that winners_v2 with mismatched config_hash returns DIRTY.\"\"\"","    winners_path = temp_dir / \"winners_v2.json\"","    ","    # Create winners with config_hash at top level","    winners_data = {","        \"config_hash\": \"abc123\",","        \"schema\": \"v2\",","        \"stage_name\": \"stage1_topk\",","        \"topk\": [","            {","                \"candidate_id\": \"donchian_atr:123\",","                \"strategy_id\": \"donchian_atr\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe\": \"60m\",","                \"params\": {},","                \"metrics\": {","                    \"net_profit\": 100.0,","                    \"max_dd\": -10.0,","                    \"trades\": 10,","                },","            }","        ],","    }","    ","    with winners_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(winners_data, f)","    ","    result = validate_winners_v2_status(","        str(winners_path),","        winners_data=winners_data,","        expected_config_hash=\"different_hash\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"config_hash\" in result.message.lower()","    assert \"winners_v2.config_hash\" in result.message  # Should reference top-level field","","","def test_governance_dirty_config_hash(temp_dir: Path) -> None:","    \"\"\"Test that governance with mismatched config_hash returns DIRTY.\"\"\"","    governance_path = temp_dir / \"governance.json\"","    ","    # Create governance with config_hash at top level","    governance_data = {","        \"config_hash\": \"abc123\",","        \"run_id\": \"test-run-123\",","        \"items\": [","            {","                \"candidate_id\": \"donchian_atr:123\",","                \"strategy_id\": \"donchian_atr\","]}
{"type":"file_chunk","path":"tests/test_ui_artifact_validation.py","chunk_index":1,"line_start":201,"line_end":400,"content":["                \"decision\": \"KEEP\",","                \"rule_id\": \"R1\",","                \"reason\": \"Test\",","                \"run_id\": \"test-run-123\",","                \"stage\": \"stage1_topk\",","                \"evidence\": [],","                \"key_metrics\": {},","            }","        ],","        \"metadata\": {},","    }","    ","    with governance_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(governance_data, f)","    ","    result = validate_governance_status(","        str(governance_path),","        governance_data=governance_data,","        expected_config_hash=\"different_hash\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"config_hash\" in result.message.lower()","    assert \"governance.config_hash\" in result.message  # Should reference top-level field","","","# Test: OK status (validation passes)","def test_manifest_ok(fixtures_dir: Path) -> None:","    \"\"\"Test that valid manifest returns OK status.\"\"\"","    manifest_path = fixtures_dir / \"manifest_valid.json\"","    ","    # Load data","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        manifest_data = json.load(f)","    ","    result = validate_manifest_status(","        str(manifest_path),","        manifest_data=manifest_data,","        expected_config_hash=\"abc123def456\",","    )","    ","    assert result.status == ArtifactStatus.OK","    assert \"é©—è­‰é€šéŽ\" in result.message or \"ok\" in result.message.lower()","","","def test_winners_v2_ok(fixtures_dir: Path) -> None:","    \"\"\"Test that valid winners_v2 returns OK status.\"\"\"","    winners_path = fixtures_dir / \"winners_v2_valid.json\"","    ","    # Load data","    with winners_path.open(\"r\", encoding=\"utf-8\") as f:","        winners_data = json.load(f)","    ","    result = validate_winners_v2_status(str(winners_path), winners_data=winners_data)","    ","    assert result.status == ArtifactStatus.OK","    assert \"é©—è­‰é€šéŽ\" in result.message or \"ok\" in result.message.lower()","","","def test_governance_ok(fixtures_dir: Path) -> None:","    \"\"\"Test that valid governance returns OK status.\"\"\"","    governance_path = fixtures_dir / \"governance_valid.json\"","    ","    # Load data","    with governance_path.open(\"r\", encoding=\"utf-8\") as f:","        governance_data = json.load(f)","    ","    result = validate_governance_status(","        str(governance_path),","        governance_data=governance_data,","        expected_config_hash=\"abc123def456\",","    )","    ","    assert result.status == ArtifactStatus.OK","    assert \"é©—è­‰é€šéŽ\" in result.message or \"ok\" in result.message.lower()","","","# Test: Phase 6.5 - Missing fingerprint must be DIRTY (Binding Constraint)","def test_manifest_missing_fingerprint_is_dirty(fixtures_dir: Path) -> None:","    \"\"\"Test that manifest without data_fingerprint_sha1 is marked DIRTY.","    ","    Binding Constraint: This test locks down the requirement that","    data_fingerprint_sha1 must be present and non-empty.","    Prevents future changes from making fingerprint optional.","    \"\"\"","    manifest_path = fixtures_dir / \"manifest_valid.json\"","    ","    # Load data and remove fingerprint","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        data = json.load(f)","    data.pop(\"data_fingerprint_sha1\", None)","    ","    result = validate_manifest_status(","        str(manifest_path),","        manifest_data=data,","        expected_config_hash=\"abc123def456\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"fingerprint\" in result.message.lower() or \"untrustworthy\" in result.message.lower()","","","def test_manifest_empty_fingerprint_is_dirty(fixtures_dir: Path) -> None:","    \"\"\"Test that manifest with empty data_fingerprint_sha1 is marked DIRTY.\"\"\"","    manifest_path = fixtures_dir / \"manifest_valid.json\"","    ","    # Load data and set fingerprint to empty string","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        data = json.load(f)","    data[\"data_fingerprint_sha1\"] = \"\"","    ","    result = validate_manifest_status(","        str(manifest_path),","        manifest_data=data,","        expected_config_hash=\"abc123def456\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"fingerprint\" in result.message.lower() or \"untrustworthy\" in result.message.lower()","","","def test_governance_missing_fingerprint_is_dirty(fixtures_dir: Path) -> None:","    \"\"\"Test that governance without data_fingerprint_sha1 in metadata is marked DIRTY.","    ","    Binding Constraint: This test locks down the requirement that","    data_fingerprint_sha1 must be present in governance metadata and non-empty.","    \"\"\"","    governance_path = fixtures_dir / \"governance_valid.json\"","    ","    # Load data and remove fingerprint from metadata","    with governance_path.open(\"r\", encoding=\"utf-8\") as f:","        data = json.load(f)","    ","    if \"metadata\" in data:","        data[\"metadata\"].pop(\"data_fingerprint_sha1\", None)","    else:","        data[\"metadata\"] = {}","    ","    result = validate_governance_status(","        str(governance_path),","        governance_data=data,","        expected_config_hash=\"abc123def456\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"fingerprint\" in result.message.lower() or \"untrustworthy\" in result.message.lower()","","","def test_governance_empty_fingerprint_is_dirty(fixtures_dir: Path) -> None:","    \"\"\"Test that governance with empty data_fingerprint_sha1 in metadata is marked DIRTY.\"\"\"","    governance_path = fixtures_dir / \"governance_valid.json\"","    ","    # Load data and set fingerprint to empty string in metadata","    with governance_path.open(\"r\", encoding=\"utf-8\") as f:","        data = json.load(f)","    ","    if \"metadata\" not in data:","        data[\"metadata\"] = {}","    data[\"metadata\"][\"data_fingerprint_sha1\"] = \"\"","    ","    result = validate_governance_status(","        str(governance_path),","        governance_data=data,","        expected_config_hash=\"abc123def456\",","    )","    ","    assert result.status == ArtifactStatus.DIRTY","    assert \"fingerprint\" in result.message.lower() or \"untrustworthy\" in result.message.lower()","","","# Test: ArtifactReader (safe version)","def test_try_read_artifact_json(fixtures_dir: Path) -> None:","    \"\"\"Test reading JSON artifact with safe version.\"\"\"","    manifest_path = fixtures_dir / \"manifest_valid.json\"","    ","    result = try_read_artifact(manifest_path)","    ","    assert isinstance(result, SafeReadResult)","    assert result.is_ok","    assert result.result is not None","    assert isinstance(result.result.raw, dict)","    assert result.result.meta.source_path == str(manifest_path.resolve())","    assert len(result.result.meta.sha256) == 64  # SHA256 hex length","    assert result.result.meta.mtime_s > 0","","","def test_try_read_artifact_missing_file(temp_dir: Path) -> None:","    \"\"\"Test that reading missing file returns error, never raises.\"\"\"","    missing_path = temp_dir / \"missing.json\"","    ","    result = try_read_artifact(missing_path)","    ","    assert isinstance(result, SafeReadResult)","    assert result.is_error","    assert result.error is not None","    assert result.error.error_code == \"FILE_NOT_FOUND\"","    assert \"not found\" in result.error.message.lower()","","","# Test: EvidenceLink"]}
{"type":"file_chunk","path":"tests/test_ui_artifact_validation.py","chunk_index":2,"line_start":401,"line_end":541,"content":["def test_evidence_link() -> None:","    \"\"\"Test EvidenceLink BaseModel.\"\"\"","    link = EvidenceLink(","        artifact=\"winners_v2\",","        json_pointer=\"/rows/0/net_profit\",","        description=\"Net profit from winners\",","    )","    ","    assert link.artifact == \"winners_v2\"","    assert link.json_pointer == \"/rows/0/net_profit\"","    assert link.description == \"Net profit from winners\"","    ","    # Test with None description","    link2 = EvidenceLink(","        artifact=\"governance\",","        json_pointer=\"/scoring/final_score\",","    )","    assert link2.artifact == \"governance\"","    assert link2.json_pointer == \"/scoring/final_score\"","    assert link2.description is None","","","# Test: Pydantic schemas can parse valid data","def test_manifest_schema_parse(fixtures_dir: Path) -> None:","    \"\"\"Test that RunManifest can parse valid manifest.\"\"\"","    manifest_path = fixtures_dir / \"manifest_valid.json\"","    ","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        manifest_data = json.load(f)","    ","    manifest = RunManifest(**manifest_data)","    ","    assert manifest.run_id == \"test-run-123\"","    assert manifest.season == \"2025Q4\"","    assert manifest.config_hash == \"abc123def456\"","    assert len(manifest.stages) == 1","    assert manifest.stages[0].name == \"stage0\"","","","def test_winners_v2_schema_parse(fixtures_dir: Path) -> None:","    \"\"\"Test that WinnersV2 can parse valid winners.\"\"\"","    winners_path = fixtures_dir / \"winners_v2_valid.json\"","    ","    with winners_path.open(\"r\", encoding=\"utf-8\") as f:","        winners_data = json.load(f)","    ","    winners = WinnersV2(**winners_data)","    ","    assert winners.schema_name == \"v2\"  # schema_name is alias for \"schema\" in JSON","    assert winners.stage_name == \"stage1_topk\"","    assert winners.topk is not None","    assert len(winners.topk) == 1","","","def test_governance_schema_parse(fixtures_dir: Path) -> None:","    \"\"\"Test that GovernanceReport can parse valid governance.\"\"\"","    governance_path = fixtures_dir / \"governance_valid.json\"","    ","    with governance_path.open(\"r\", encoding=\"utf-8\") as f:","        governance_data = json.load(f)","    ","    governance = GovernanceReport(**governance_data)","    ","    assert governance.run_id == \"test-run-123\"","    assert governance.items is not None","    assert len(governance.items) == 1","","","# Test: EvidenceLinkModel render_hint (PR-A)","def test_evidence_link_model_backward_compatibility() -> None:","    \"\"\"Test that EvidenceLinkModel can parse old data without render_hint.\"\"\"","    from core.schemas.governance import EvidenceLinkModel","    ","    # Old data format (without render_hint)","    old_data = {","        \"source_path\": \"winners_v2.json\",","        \"json_pointer\": \"/rows/0/net_profit\",","        \"note\": \"Net profit from winners\",","    }","    ","    # Should parse successfully with default render_hint=\"highlight\"","    link = EvidenceLinkModel(**old_data)","    ","    assert link.source_path == \"winners_v2.json\"","    assert link.json_pointer == \"/rows/0/net_profit\"","    assert link.note == \"Net profit from winners\"","    assert link.render_hint == \"highlight\"  # Default value","    assert link.render_payload == {}  # Default empty dict","","","def test_evidence_link_model_with_render_hint() -> None:","    \"\"\"Test that EvidenceLinkModel can parse new data with render_hint.\"\"\"","    from core.schemas.governance import EvidenceLinkModel","    ","    # New data format (with render_hint) - using allowed value","    new_data = {","        \"source_path\": \"winners_v2.json\",","        \"json_pointer\": \"/rows/0/net_profit\",","        \"note\": \"Net profit from winners\",","        \"render_hint\": \"highlight\",","        \"render_payload\": {\"start_idx\": 0, \"end_idx\": 0},","    }","    ","    link = EvidenceLinkModel(**new_data)","    ","    assert link.source_path == \"winners_v2.json\"","    assert link.json_pointer == \"/rows/0/net_profit\"","    assert link.note == \"Net profit from winners\"","    assert link.render_hint == \"highlight\"","    assert link.render_payload == {\"start_idx\": 0, \"end_idx\": 0}","","","def test_evidence_link_model_roundtrip() -> None:","    \"\"\"Test that EvidenceLinkModel can roundtrip through JSON.\"\"\"","    from core.schemas.governance import EvidenceLinkModel","    ","    # Create model with render_hint - using allowed value","    link = EvidenceLinkModel(","        source_path=\"governance.json\",","        json_pointer=\"/rows/0/decision\",","        note=\"Decision evidence\",","        render_hint=\"diff\",","        render_payload={\"lhs_pointer\": \"/rows/0/decision\", \"rhs_pointer\": \"/rows/0/decision\"},","    )","    ","    # Convert to dict","    link_dict = link.model_dump()","    ","    # Roundtrip: dict -> JSON -> dict -> model","    json_str = json.dumps(link_dict)","    link_dict_roundtrip = json.loads(json_str)","    link_roundtrip = EvidenceLinkModel(**link_dict_roundtrip)","    ","    # Verify all fields preserved","    assert link_roundtrip.source_path == link.source_path","    assert link_roundtrip.json_pointer == link.json_pointer","    assert link_roundtrip.note == link.note","    assert link_roundtrip.render_hint == link.render_hint","    assert link_roundtrip.render_payload == link.render_payload","",""]}
{"type":"file_footer","path":"tests/test_ui_artifact_validation.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"tests/test_ui_race_condition_headless.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":14442,"sha256":"8050434961467dd92b50d14bac87ad8342483b4477cc2889ca1b3130ed3dcaba","total_lines":376,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_ui_race_condition_headless.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Test UI race condition defense for Attack #9 â€“ Headless Intent-State Contract.","","Tests that UI cannot cause race conditions because:","1. UI only creates intents (no business logic)","2. All intents go through single ActionQueue","3. StateProcessor is single consumer (sequential execution)","4. UI only reads SystemState snapshots (immutable)","\"\"\"","","import pytest","import asyncio","import threading","import contextlib","from datetime import date","import anyio","from anyio import create_task_group, sleep","","from core.intents import (","    CreateJobIntent, CalculateUnitsIntent, DataSpecIntent,","    IntentStatus",")","from control.action_queue import ActionQueue, reset_action_queue","from core.processor import StateProcessor, reset_processor","from gui.adapters.intent_bridge import IntentBridge, get_intent_bridge","","","@pytest.fixture","def clean_system():","    \"\"\"Clean system state before each test.\"\"\"","    reset_action_queue()","    reset_processor()","    yield","    reset_action_queue()","    reset_processor()","","","@pytest.fixture","def sample_data_spec():","    \"\"\"Create a sample DataSpecIntent for testing.\"\"\"","    return DataSpecIntent(","        dataset_id=\"test_dataset\",","        symbols=[\"MNQ\", \"MXF\"],","        timeframes=[\"60m\", \"120m\"],","        start_date=date(2020, 1, 1),","        end_date=date(2024, 12, 31)","    )","","","@contextlib.asynccontextmanager","async def managed_processor(queue):","    \"\"\"","    Context manager to ensure processor is stopped even if tests fail.","    This prevents hangs caused by unclosed background tasks.","    ","    CRITICAL: Includes a hard timeout shield for stop() to prevent CI hangs.","    \"\"\"","    processor = StateProcessor(queue)","    await processor.start()","    try:","        yield processor","    finally:","        # Force stop with hard timeout to prevent deadlock in teardown.","        # If the processor code is buggy and hangs on stop, we kill it here","        # so pytest can finish and report the results.","        try:","            await asyncio.wait_for(processor.stop(), timeout=2.0)","        except (asyncio.TimeoutError, Exception) as e:","            print(f\"WARNING: Processor stop forced/timed out: {e}\")","","","def test_ui_only_creates_intents():","    \"\"\"Test that UI code only creates intents, doesn't execute logic.\"\"\"","    bridge = IntentBridge()","    ","    # UI creates intents through bridge","    data_spec = bridge.create_data_spec_intent(","        dataset_id=\"test\",","        symbols=[\"MNQ\"],","        timeframes=[\"60m\"]","    )","    ","    intent = bridge.create_job_intent(","        season=\"2024Q1\",","        data1=data_spec,","        data2=None,","        strategy_id=\"test\",","        params={}","    )","    ","    # Intent should be created but not executed","    assert intent.intent_type.value == \"create_job\"","    assert intent.status == IntentStatus.PENDING","    assert intent.result is None","","","@pytest.mark.anyio","async def test_single_consumer_sequential(clean_system, sample_data_spec):","    \"\"\"Test that StateProcessor is single consumer (sequential execution).\"\"\"","    with anyio.fail_after(10):","        queue = ActionQueue()","        ","        async with managed_processor(queue) as processor:","            # Submit multiple intents","            intent_ids = []","            for i in range(5):","                intent = CalculateUnitsIntent(","                    season=f\"2024Q{i}\",","                    data1=sample_data_spec,","                    data2=None,","                    strategy_id=\"sma_cross_v1\",","                    params={\"window_fast\": i}","                )","                intent_id = queue.submit(intent)","                intent_ids.append(intent_id)","            ","            # Wait for all to complete","            for intent_id in intent_ids:","                completed = await queue.wait_for_intent_async(intent_id, timeout=5.0)","                assert completed is not None","                # Even if they fail due to missing data, they are 'processed'","                assert completed.status in (IntentStatus.COMPLETED, IntentStatus.FAILED)","            ","            # Check that they were processed","            metrics = queue.get_metrics()","            assert metrics[\"processed\"] == 5","","","@pytest.mark.anyio","async def test_concurrent_ui_submissions(clean_system, sample_data_spec):","    \"\"\"Test that concurrent UI submissions don't cause race conditions.\"\"\"","    with anyio.fail_after(10):","        queue = ActionQueue()","        ","        async with managed_processor(queue) as processor:","            # Simulate multiple UI threads submitting intents concurrently","            results = []","            errors = []","            ","            async def ui_thread_submit(thread_id: int):","                \"\"\"Simulate a UI thread submitting intents.\"\"\"","                try:","                    # UI creates intent","                    intent = CalculateUnitsIntent(","                        season=f\"2024Q{thread_id}\",","                        data1=sample_data_spec,","                        data2=None,","                        strategy_id=\"sma_cross_v1\",","                        params={\"window_fast\": thread_id}","                    )","                    ","                    # Submit to queue","                    intent_id = queue.submit(intent)","                    ","                    # Wait for result","                    completed = await queue.wait_for_intent_async(intent_id, timeout=5.0)","                    if completed and completed.status in (IntentStatus.COMPLETED, IntentStatus.FAILED):","                        results.append((thread_id, completed.result))","                    else:","                        errors.append((thread_id, \"Failed or Timed out\"))","                        ","                except Exception as e:","                    errors.append((thread_id, str(e)))","            ","            # Launch multiple UI threads","            tasks = [ui_thread_submit(i) for i in range(10)]","            await asyncio.gather(*tasks)","            ","            # All should succeed without race conditions","            assert len(errors) == 0","            assert len(results) == 10","            ","            # Check that queue processed all intents","            metrics = queue.get_metrics()","            assert metrics[\"processed\"] == 10","","","def test_immutable_state_snapshots():","    \"\"\"Test that UI only reads immutable state snapshots.\"\"\"","    from core.state import create_initial_state","    ","    # Create initial state","    state = create_initial_state()","    ","    # UI reads state (this is allowed)","    total_jobs = state.metrics.total_jobs","    is_healthy = state.is_healthy","    ","    # UI cannot modify state (should raise exception or rely on dataclass frozen=True)","    # Note: If models aren't frozen, we rely on convention/architecture","    # But here we verify the read values are correct","    assert state.metrics.total_jobs == total_jobs","    assert state.is_healthy == is_healthy","","","@pytest.mark.anyio","async def test_state_consistency_during_concurrent_reads(clean_system, sample_data_spec):","    \"\"\"Test that concurrent state reads are consistent.\"\"\"","    with anyio.fail_after(10):","        queue = ActionQueue()","        "]}
{"type":"file_chunk","path":"tests/test_ui_race_condition_headless.py","chunk_index":1,"line_start":201,"line_end":376,"content":["        async with managed_processor(queue) as processor:","            # Submit a job to change state","            intent = CreateJobIntent(","                season=\"2024Q1\",","                data1=sample_data_spec,","                data2=None,","                strategy_id=\"sma_cross_v1\",","                params={\"window_fast\": 10}","            )","            ","            intent_id = processor.submit_intent(intent)","            ","            # Multiple UI threads reading state concurrently","            read_values = []","            ","            async def ui_thread_read(thread_id: int):","                \"\"\"UI thread reads state.\"\"\"","                # Get state snapshot","                state = processor.get_state()","                read_values.append((thread_id, state.metrics.total_jobs))","            ","            # Launch concurrent reads","            tasks = [ui_thread_read(i) for i in range(10)]","            await asyncio.gather(*tasks)","            ","            # All reads should see consistent state","            # (either all see initial state or all see updated state, usually initial)","            unique_values = set(value for _, value in read_values)","            assert len(unique_values) == 1","            ","            # Wait for intent to complete","            completed_intent = await processor.wait_for_intent(intent_id, timeout=5.0)","            ","            # Handle timeout case (wait_for_intent returns None on timeout)","            if completed_intent is None:","                # Debug: get current state to understand what happened","                st = processor.get_state()","                print(\"DEBUG: wait_for_intent timed out; state:\", st)","                pytest.fail(\"wait_for_intent timed out (must never hang; must resolve or fail deterministically)\")","            ","            # Now check final state","            final_state = processor.get_state()","            ","            # NOTE: In headless test without real data, job creation will FAIL validation.","            # So total_jobs will remain 0, but failed_count in queue should increase.","            # We check intent status to determine what to expect.","            if completed_intent.status == IntentStatus.FAILED:","                # Expect failure recorded in queue metrics, but not necessarily in business metrics (total_jobs)","                assert final_state.intent_queue.failed_count >= 1","            else:","                # If by some miracle it passed validation (e.g. if mocked)","                assert final_state.metrics.total_jobs == 1","","","def test_intent_bridge_singleton():","    \"\"\"Test that IntentBridge is singleton.\"\"\"","    bridge1 = get_intent_bridge()","    bridge2 = get_intent_bridge()","    ","    assert bridge1 is bridge2","","","@pytest.mark.anyio","async def test_bridge_concurrent_usage(clean_system):","    \"\"\"Test IntentBridge with concurrent UI access.\"\"\"","    with anyio.fail_after(10):","        bridge = IntentBridge()","        ","        # Start processor (bridge manages its own processor lifecycle)","        await bridge.start_processor()","        ","        try:","            # Multiple UI threads using bridge concurrently","            results = []","            ","            async def ui_thread_use_bridge(thread_id: int):","                \"\"\"UI thread uses bridge.\"\"\"","                # Create data spec","                data_spec = bridge.create_data_spec_intent(","                    dataset_id=f\"dataset_{thread_id}\",","                    symbols=[\"MNQ\"],","                    timeframes=[\"60m\"]","                )","                ","                # Create calculation intent","                intent = bridge.calculate_units_intent(","                    season=f\"2024Q{thread_id}\",","                    data1=data_spec,","                    data2=None,","                    strategy_id=\"test\",","                    params={\"param\": thread_id}","                )","                ","                # Submit and wait","                completed = await bridge.submit_and_wait_async(intent, timeout=5.0)","                # Both COMPLETED and FAILED are valid outcomes of \"processing\"","                if completed and completed.status in (IntentStatus.COMPLETED, IntentStatus.FAILED):","                    results.append((thread_id, completed.result))","            ","            # Launch concurrent UI threads","            tasks = [ui_thread_use_bridge(i) for i in range(5)]","            await asyncio.gather(*tasks)","            ","            # All should succeed (in terms of getting a response)","            assert len(results) == 5","            ","        finally:","            await bridge.stop_processor()","","","def test_no_direct_backend_imports():","    \"\"\"Test that UI modules shouldn't import backend logic directly.\"\"\"","    import sys","    ","    # Check that intent_bridge doesn't expose backend logic","    # Note: We need to ensure the module is loaded","    import gui.adapters.intent_bridge","    bridge_module = sys.modules['gui.adapters.intent_bridge']","    ","    # Bridge should not expose backend functions directly","    assert not hasattr(bridge_module, 'create_job_from_wizard_direct')","    assert not hasattr(bridge_module, 'calculate_units_direct')","    ","    # Bridge should expose intent creation methods","    assert hasattr(bridge_module, 'IntentBridge')","    assert hasattr(bridge_module, 'get_intent_bridge')","","","@pytest.mark.anyio","async def test_race_condition_prevention(clean_system, sample_data_spec):","    \"\"\"Test that race conditions are prevented by design.\"\"\"","    with anyio.fail_after(10):","        queue = ActionQueue()","        ","        async with managed_processor(queue) as processor:","            # Simulate race condition scenario:","            # Multiple UI threads trying to create jobs for same season/dataset","            ","            created_job_ids = set()","            lock = threading.Lock()","            ","            async def race_condition_thread(thread_id: int):","                \"\"\"Thread that could cause race condition in traditional system.\"\"\"","                # All threads use same parameters (potential race)","                intent = CreateJobIntent(","                    season=\"2024Q1\",  # Same season","                    data1=sample_data_spec,  # Same data","                    data2=None,","                    strategy_id=\"sma_cross_v1\",","                    params={\"window_fast\": 10}","                )","                ","                # Submit intent","                intent_id = processor.submit_intent(intent)","                completed = await processor.wait_for_intent(intent_id, timeout=5.0)","                ","                if completed and completed.status == IntentStatus.COMPLETED:","                    job_id = completed.result.get(\"job_id\")","                    if job_id:","                        with lock:","                            created_job_ids.add(job_id)","            ","            # Launch many threads simultaneously","            tasks = [race_condition_thread(i) for i in range(20)]","            await asyncio.gather(*tasks)","            ","            # 1. All intents should be processed","            state = processor.get_state()","            # Since these are duplicates, most will be rejected or failed.","            # But total processed (completed + failed) should be 20.","            # Note: Depending on implementation, duplicates might be 'rejected' before processing","            # or 'processed' and failed.","            ","            metrics = queue.get_metrics()","            # Ensure we processed something","            assert metrics[\"processed\"] > 0"]}
{"type":"file_footer","path":"tests/test_ui_race_condition_headless.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_vectorization_parity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2630,"sha256":"3058382f60fc15b786485e67b9d84760e63ecba10a9df1ad2105f15242519b5a","total_lines":77,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_vectorization_parity.py","chunk_index":0,"line_start":1,"line_end":77,"content":["","from __future__ import annotations","","import numpy as np","","from data.layout import normalize_bars","from engine.engine_jit import simulate_arrays","from engine.types import Fill, OrderKind, OrderRole, Side","from strategy.kernel import DonchianAtrParams, run_kernel_arrays, run_kernel_object_mode","","","def _assert_fills_equal(a: list[Fill], b: list[Fill]) -> None:","    assert len(a) == len(b)","    for fa, fb in zip(a, b):","        assert fa.bar_index == fb.bar_index","        assert fa.role == fb.role","        assert fa.kind == fb.kind","        assert fa.side == fb.side","        assert fa.qty == fb.qty","        assert fa.order_id == fb.order_id","        assert abs(fa.price - fb.price) <= 1e-9","","","def test_strategy_object_vs_array_mode_parity() -> None:","    rng = np.random.default_rng(42)","    n = 300","    close = 100.0 + np.cumsum(rng.standard_normal(n)).astype(np.float64)","    high = close + 1.0","    low = close - 1.0","    open_ = (high + low) * 0.5","","    bars = normalize_bars(open_, high, low, close)","    params = DonchianAtrParams(channel_len=20, atr_len=14, stop_mult=2.0)","","    out_obj = run_kernel_object_mode(bars, params, commission=0.0, slip=0.0, order_qty=1)","    out_arr = run_kernel_arrays(bars, params, commission=0.0, slip=0.0, order_qty=1)","","    _assert_fills_equal(out_obj[\"fills\"], out_arr[\"fills\"])  # type: ignore[arg-type]","","","def test_simulate_arrays_same_bar_entry_exit_parity() -> None:","    # Construct a same-bar entry then exit scenario (created_bar=-1 activates on bar0).","    bars = normalize_bars(","        np.array([100.0], dtype=np.float64),","        np.array([120.0], dtype=np.float64),","        np.array([80.0], dtype=np.float64),","        np.array([110.0], dtype=np.float64),","    )","","    # ENTRY BUY STOP 105, EXIT SELL STOP 95, both active on bar0.","    order_id = np.array([1, 2], dtype=np.int64)","    created_bar = np.array([-1, -1], dtype=np.int64)","    role = np.array([1, 0], dtype=np.int8)  # ENTRY then EXIT (order_id tie-break handles)","    kind = np.array([0, 0], dtype=np.int8)  # STOP","    side = np.array([1, -1], dtype=np.int8)  # BUY, SELL","    price = np.array([105.0, 95.0], dtype=np.float64)","    qty = np.array([1, 1], dtype=np.int64)","","    fills = simulate_arrays(","        bars,","        order_id=order_id,","        created_bar=created_bar,","        role=role,","        kind=kind,","        side=side,","        price=price,","        qty=qty,","        ttl_bars=1,","    )","","    assert len(fills) == 2","    assert fills[0].role == OrderRole.ENTRY and fills[0].side == Side.BUY and fills[0].kind == OrderKind.STOP","    assert fills[1].role == OrderRole.EXIT and fills[1].side == Side.SELL and fills[1].kind == OrderKind.STOP","","","",""]}
{"type":"file_footer","path":"tests/test_vectorization_parity.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_winners_schema_v2_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6418,"sha256":"8dedb61cacdcdaf800cfa38d26e7c7746d7789f12129f4ed9e76a63ba3e5e765","total_lines":205,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_winners_schema_v2_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for winners schema v2.","","Tests verify:","1. v2 schema structure (top-level fields)","2. WinnerItemV2 structure (required fields)","3. JSON serialization with sorted keys","4. Schema version detection","\"\"\"","","from __future__ import annotations","","import json","from datetime import datetime, timezone","","from core.winners_schema import (","    WinnerItemV2,","    build_winners_v2_dict,","    is_winners_legacy,","    is_winners_v2,","    WINNERS_SCHEMA_VERSION,",")","","","def test_winners_v2_top_level_schema() -> None:","    \"\"\"Test that v2 winners.json has required top-level fields.\"\"\"","    items = [","        WinnerItemV2(","            candidate_id=\"donchian_atr:123\",","            strategy_id=\"donchian_atr\",","            symbol=\"CME.MNQ\",","            timeframe=\"60m\",","            params={\"LE\": 8, \"LX\": 4, \"Z\": -0.4},","            score=1.234,","            metrics={\"net_profit\": 100.0, \"max_dd\": -10.0, \"trades\": 10, \"param_id\": 123},","            source={\"param_id\": 123, \"run_id\": \"test-123\", \"stage_name\": \"stage1_topk\"},","        ),","    ]","    ","    winners = build_winners_v2_dict(","        stage_name=\"stage1_topk\",","        run_id=\"test-123\",","        topk=items,","    )","    ","    # Verify top-level fields","    assert winners[\"schema\"] == WINNERS_SCHEMA_VERSION","    assert winners[\"stage_name\"] == \"stage1_topk\"","    assert \"generated_at\" in winners","    assert \"topk\" in winners","    assert \"notes\" in winners","    ","    # Verify notes schema","    assert winners[\"notes\"][\"schema\"] == WINNERS_SCHEMA_VERSION","","","def test_winner_item_v2_required_fields() -> None:","    \"\"\"Test that WinnerItemV2 has all required fields.\"\"\"","    item = WinnerItemV2(","        candidate_id=\"donchian_atr:c7bc8b64916c\",","        strategy_id=\"donchian_atr\",","        symbol=\"CME.MNQ\",","        timeframe=\"60m\",","        params={\"LE\": 8, \"LX\": 4, \"Z\": -0.4},","        score=1.234,","        metrics={\"net_profit\": 0.0, \"max_dd\": 0.0, \"trades\": 0, \"param_id\": 9},","        source={\"param_id\": 9, \"run_id\": \"stage1_topk-123\", \"stage_name\": \"stage1_topk\"},","    )","    ","    item_dict = item.to_dict()","    ","    # Verify all required fields exist","    assert \"candidate_id\" in item_dict","    assert \"strategy_id\" in item_dict","    assert \"symbol\" in item_dict","    assert \"timeframe\" in item_dict","    assert \"params\" in item_dict","    assert \"score\" in item_dict","    assert \"metrics\" in item_dict","    assert \"source\" in item_dict","    ","    # Verify field values","    assert item_dict[\"candidate_id\"] == \"donchian_atr:c7bc8b64916c\"","    assert item_dict[\"strategy_id\"] == \"donchian_atr\"","    assert item_dict[\"symbol\"] == \"CME.MNQ\"","    assert item_dict[\"timeframe\"] == \"60m\"","    assert isinstance(item_dict[\"params\"], dict)","    assert isinstance(item_dict[\"score\"], (int, float))","    assert isinstance(item_dict[\"metrics\"], dict)","    assert isinstance(item_dict[\"source\"], dict)","","","def test_winners_v2_json_serializable_sorted_keys() -> None:","    \"\"\"Test that v2 winners.json is JSON-serializable with sorted keys.\"\"\"","    items = [","        WinnerItemV2(","            candidate_id=\"donchian_atr:123\",","            strategy_id=\"donchian_atr\",","            symbol=\"CME.MNQ\",","            timeframe=\"60m\",","            params={\"LE\": 8},","            score=1.234,","            metrics={\"net_profit\": 100.0, \"max_dd\": -10.0, \"trades\": 10, \"param_id\": 123},","            source={\"param_id\": 123, \"run_id\": \"test-123\", \"stage_name\": \"stage1_topk\"},","        ),","    ]","    ","    winners = build_winners_v2_dict(","        stage_name=\"stage1_topk\",","        run_id=\"test-123\",","        topk=items,","    )","    ","    # Serialize to JSON with sorted keys","    json_str = json.dumps(winners, ensure_ascii=False, sort_keys=True, indent=2)","    ","    # Deserialize back","    winners_roundtrip = json.loads(json_str)","    ","    # Verify structure","    assert winners_roundtrip[\"schema\"] == WINNERS_SCHEMA_VERSION","    assert len(winners_roundtrip[\"topk\"]) == 1","    ","    item_dict = winners_roundtrip[\"topk\"][0]","    assert item_dict[\"candidate_id\"] == \"donchian_atr:123\"","    assert item_dict[\"strategy_id\"] == \"donchian_atr\"","    ","    # Verify JSON keys are sorted (check top-level)","    json_lines = json_str.split(\"\\n\")","    # Find line with \"generated_at\" and \"schema\" - should be in sorted order","    # (This is a simple check - full verification would require parsing)","    assert '\"generated_at\"' in json_str","    assert '\"schema\"' in json_str","","","def test_is_winners_v2_detection() -> None:","    \"\"\"Test schema version detection.\"\"\"","    # v2 format","    winners_v2 = {","        \"schema\": \"v2\",","        \"stage_name\": \"stage1_topk\",","        \"generated_at\": \"2025-12-18T00:00:00Z\",","        \"topk\": [],","        \"notes\": {\"schema\": \"v2\"},","    }","    assert is_winners_v2(winners_v2) is True","    assert is_winners_legacy(winners_v2) is False","    ","    # Legacy format","    winners_legacy = {","        \"topk\": [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}],","        \"notes\": {\"schema\": \"v1\"},","    }","    assert is_winners_v2(winners_legacy) is False","    assert is_winners_legacy(winners_legacy) is True","    ","    # Unknown format (no schema)","    winners_unknown = {","        \"topk\": [{\"param_id\": 0}],","    }","    assert is_winners_v2(winners_unknown) is False","    assert is_winners_legacy(winners_unknown) is True  # Falls back to legacy","","","def test_winner_item_v2_metrics_contains_legacy_fields() -> None:","    \"\"\"Test that metrics contains legacy fields for backward compatibility.\"\"\"","    item = WinnerItemV2(","        candidate_id=\"donchian_atr:123\",","        strategy_id=\"donchian_atr\",","        symbol=\"CME.MNQ\",","        timeframe=\"60m\",","        params={},","        score=1.234,","        metrics={","            \"net_profit\": 100.0,","            \"max_dd\": -10.0,","            \"trades\": 10,","            \"param_id\": 123,  # Legacy field","        },","        source={\"param_id\": 123, \"run_id\": \"test-123\", \"stage_name\": \"stage1_topk\"},","    )","    ","    item_dict = item.to_dict()","    metrics = item_dict[\"metrics\"]","    ","    # Verify legacy fields exist","    assert \"net_profit\" in metrics","    assert \"max_dd\" in metrics","    assert \"trades\" in metrics","    assert \"param_id\" in metrics","","","def test_winners_v2_empty_topk() -> None:","    \"\"\"Test that v2 schema handles empty topk correctly.\"\"\"","    winners = build_winners_v2_dict(","        stage_name=\"stage1_topk\",","        run_id=\"test-123\",","        topk=[],","    )","    "]}
{"type":"file_chunk","path":"tests/test_winners_schema_v2_contract.py","chunk_index":1,"line_start":201,"line_end":205,"content":["    assert winners[\"schema\"] == WINNERS_SCHEMA_VERSION","    assert winners[\"topk\"] == []","    assert isinstance(winners[\"topk\"], list)","",""]}
{"type":"file_footer","path":"tests/test_winners_schema_v2_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_worker_writes_traceback_to_log.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2250,"sha256":"f31dd89e83f997deb0014268e492743da3dd030a0a7ee99739a2c1cd917c57fb","total_lines":69,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_worker_writes_traceback_to_log.py","chunk_index":0,"line_start":1,"line_end":69,"content":["","\"\"\"Tests for worker writing full traceback to log.","","Tests that worker writes complete traceback.format_exc() to job_logs table","when job fails, while keeping last_error column short (500 chars).","\"\"\"","","from __future__ import annotations","","from pathlib import Path","from unittest.mock import Mock, patch","","import pytest","","from control.jobs_db import create_job, get_job, get_job_logs, init_db","from control.types import DBJobSpec, JobStatus","from control.worker import run_one_job","","","def test_worker_writes_traceback_to_log(tmp_path: Path) -> None:","    \"\"\"","    Test that worker writes full traceback to job_logs when job fails.","    ","    Verifies:","    - last_error is truncated to 500 chars","    - job_logs contains full traceback with \"Traceback (most recent call last):\"","    \"\"\"","    db = tmp_path / \"jobs.db\"","    init_db(db)","    ","    # Create a job","    spec = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=str(tmp_path / \"outputs\"),","        config_snapshot={\"test\": \"config\"},","        config_hash=\"test_hash\",","    )","    job_id = create_job(db, spec)","    ","    # Mock run_funnel to raise exception with traceback","    with patch(\"control.worker.run_funnel\", side_effect=ValueError(\"Test error with long message \" * 20)):","        # Run job (should catch exception and write traceback)","        run_one_job(db, job_id)","    ","    # Verify job is marked as FAILED","    job = get_job(db, job_id)","    assert job.status == JobStatus.FAILED","    assert job.last_error is not None","    assert len(job.last_error) <= 500  # Truncated","    ","    # Verify traceback is in job_logs","    logs = get_job_logs(db, job_id)","    assert len(logs) > 0, \"Should have at least one log entry\"","    ","    # Find error log entry","    error_logs = [log for log in logs if \"[ERROR]\" in log]","    assert len(error_logs) > 0, \"Should have error log entry\"","    ","    # Verify traceback format","    error_log = error_logs[0]","    assert \"Traceback (most recent call last):\" in error_log, \"Should contain full traceback\"","    assert \"ValueError\" in error_log, \"Should contain exception type\"","    assert \"Test error\" in error_log, \"Should contain error message\"","    ","    # Verify error message is in last_error (truncated)","    assert \"Test error\" in job.last_error","",""]}
{"type":"file_footer","path":"tests/test_worker_writes_traceback_to_log.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/wfs/test_wfs_no_io.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2816,"sha256":"4d3bfd2dd501f0e0484e73d9ed77bfdbc8889773785f467a9a408b51996915cf","total_lines":87,"chunk_count":1}
{"type":"file_chunk","path":"tests/wfs/test_wfs_no_io.py","chunk_index":0,"line_start":1,"line_end":87,"content":["","import builtins","from pathlib import Path","","import numpy as np","import pytest","","from core.feature_bundle import FeatureSeries, FeatureBundle","import wfs.runner as wfs_runner","","","class _DummySpec:","    \"\"\"","    Minimal strategy spec object for tests.","    Must provide:","      - defaults: dict","      - fn(strategy_input: dict, params: dict) -> dict with {\"intents\": [...]}","    \"\"\"","    def __init__(self):","        self.defaults = {}","","        def _fn(strategy_input, params):","            # Must not do IO; return valid structure for run_strategy().","            return {\"intents\": []}","","        self.fn = _fn","","","def test_run_wfs_with_features_disallows_file_io_without_real_strategy(monkeypatch):","    # 1) Hard deny all file IO primitives","    def _deny(*args, **kwargs):","        raise RuntimeError(\"IO is forbidden in run_wfs_with_features\")","","    monkeypatch.setattr(builtins, \"open\", _deny, raising=True)","    monkeypatch.setattr(Path, \"open\", _deny, raising=True)","    monkeypatch.setattr(Path, \"read_text\", _deny, raising=True)","    monkeypatch.setattr(Path, \"exists\", _deny, raising=True)","","    # 2) Inject dummy strategy spec so we don't rely on repo strategy registry/ids","    # Primary patch target: symbol referenced by wfs_runner module","    monkeypatch.setattr(wfs_runner, \"get_strategy_spec\", lambda strategy_id: _DummySpec(), raising=False)","","    # If get_strategy_spec isn't used in this repo layout, add fallback patches:","    # These should be kept harmless by raising=False.","    try:","        import strategy.registry as strat_registry","        monkeypatch.setattr(strat_registry, \"get\", lambda strategy_id: _DummySpec(), raising=False)","    except Exception:","        pass","","    try:","        import strategy.runner as strat_runner","        monkeypatch.setattr(strat_runner, \"get\", lambda strategy_id: _DummySpec(), raising=False)","    except Exception:","        pass","","    # 3) Build a minimal FeatureBundle","    ts = np.array(","        [\"2025-01-01T00:00:00\", \"2025-01-01T00:01:00\", \"2025-01-01T00:02:00\"],","        dtype=\"datetime64[s]\",","    )","    v = np.array([1.0, 2.0, 3.0], dtype=np.float64)","","    s1 = FeatureSeries(ts=ts, values=v, name=\"atr_14\", timeframe_min=60)","    s2 = FeatureSeries(ts=ts, values=v, name=\"ret_z_200\", timeframe_min=60)","    s3 = FeatureSeries(ts=ts, values=v, name=\"session_vwap\", timeframe_min=60)","","    # FeatureBundle requires meta dict with ts_dtype and breaks_policy","    meta = {","        \"ts_dtype\": \"datetime64[s]\",","        \"breaks_policy\": \"drop\",","    }","    bundle = FeatureBundle(","        dataset_id=\"D\",","        season=\"S\",","        series={(s.name, s.timeframe_min): s for s in [s1, s2, s3]},","        meta=meta,","    )","","    out = wfs_runner.run_wfs_with_features(","        strategy_id=\"__dummy__\",","        feature_bundle=bundle,","        config={\"params\": {}},","    )","","    assert isinstance(out, dict)",""]}
{"type":"file_footer","path":"tests/wfs/test_wfs_no_io.py","complete":true,"emitted_chunks":1}
