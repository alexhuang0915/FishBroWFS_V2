{"type":"meta","schema_version":2,"run_id":"20251227_195200Z","repo_root":"/home/fishbro/FishBroWFS_V2","snapshot_root":"SNAPSHOT","part":6,"parts":10,"created_at":"2025-12-27T19:52:00Z","generator":{"name":"dump_context","version":"vNext-balanced"},"policies":{"max_parts":10,"target_bytes_per_part":500000,"chunk_max_lines":200,"chunk_max_bytes":120000,"outputs_max_bytes_for_content":2000000,"outputs_policy":"manifest_only_for_big_files","forbid_file_truncated":true,"balance_parts":true,"estimated_total_bytes":3514686,"balance_buffer_ratio":1.08}}
{"type":"file_chunk","path":"tests/test_funnel_topk_no_human_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Funnel Top-K no-human contract tests - Phase 4 Stage D.","","These tests ensure that Top-K selection is purely automatic based on proxy_value,","with no possibility of human intervention or manual filtering.","\"\"\"","","import numpy as np","","from pipeline.funnel import run_funnel","from pipeline.stage0_runner import Stage0Result, run_stage0","from pipeline.topk import select_topk","","","def test_topk_only_uses_proxy_value():","    \"\"\"Test that Top-K selection uses ONLY proxy_value, not any other field.\"\"\"","    # Create Stage0 results with varying proxy_value and other fields","    results = [","        Stage0Result(param_id=0, proxy_value=5.0, warmup_ok=True, meta={\"custom\": \"data\"}),","        Stage0Result(param_id=1, proxy_value=10.0, warmup_ok=False, meta=None),","        Stage0Result(param_id=2, proxy_value=15.0, warmup_ok=True, meta={\"other\": 123}),","        Stage0Result(param_id=3, proxy_value=8.0, warmup_ok=True, meta=None),","        Stage0Result(param_id=4, proxy_value=12.0, warmup_ok=False, meta={\"test\": True}),","    ]","    ","    # Select top 3","    topk = select_topk(results, k=3)","    ","    # Expected: param_id=2 (value=15.0), param_id=4 (value=12.0), param_id=1 (value=10.0)","    # Should ignore warmup_ok and meta fields","    assert topk == [2, 4, 1], (","        f\"Top-K should only consider proxy_value, got {topk}, expected [2, 4, 1]\"","    )","","","def test_topk_tie_break_param_id():","    \"\"\"Test that tie-breaking uses param_id (ascending) when proxy_value is identical.\"\"\"","    # Create results with identical proxy_value","    results = [","        Stage0Result(param_id=5, proxy_value=10.0),","        Stage0Result(param_id=2, proxy_value=10.0),","        Stage0Result(param_id=8, proxy_value=10.0),","        Stage0Result(param_id=1, proxy_value=10.0),","        Stage0Result(param_id=3, proxy_value=15.0),  # Higher value","        Stage0Result(param_id=4, proxy_value=12.0),   # Medium value","    ]","    ","    # Select top 3","    topk = select_topk(results, k=3)","    ","    # Expected: param_id=3 (value=15.0), param_id=4 (value=12.0), param_id=1 (value=10.0, lowest param_id)","    assert topk == [3, 4, 1], (","        f\"Tie-break should use param_id ascending, got {topk}, expected [3, 4, 1]\"","    )","","","def test_topk_deterministic_same_input():","    \"\"\"Test that Top-K selection is deterministic: same input produces same output.\"\"\"","    np.random.seed(42)","    n_bars = 500","    n_params = 50","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    ","    params_matrix = np.column_stack([","        np.random.randint(10, 50, size=n_params),","        np.random.randint(5, 30, size=n_params),","        np.random.uniform(1.0, 3.0, size=n_params),","    ]).astype(np.float64)","    ","    # Run Stage0 twice","    stage0_results_1 = run_stage0(close, params_matrix)","    stage0_results_2 = run_stage0(close, params_matrix)","    ","    # Select Top-K twice","    topk_1 = select_topk(stage0_results_1, k=10)","    topk_2 = select_topk(stage0_results_2, k=10)","    ","    # Should be identical","    assert topk_1 == topk_2, (","        f\"Top-K selection not deterministic:\\n\"","        f\"  First run:  {topk_1}\\n\"","        f\"  Second run: {topk_2}\"","    )","","","def test_funnel_topk_no_manual_filtering():","    \"\"\"Test that funnel Top-K selection cannot be manually filtered.\"\"\"","    np.random.seed(42)","    n_bars = 300","    n_params = 20","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    open_ = close + np.random.randn(n_bars) * 2","    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3","    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3","    ","    params_matrix = np.column_stack([","        np.random.randint(10, 40, size=n_params),","        np.random.randint(5, 25, size=n_params),","        np.random.uniform(1.0, 2.5, size=n_params),","    ]).astype(np.float64)","    ","    # Run funnel","    result = run_funnel(","        open_,","        high,","        low,","        close,","        params_matrix,","        k=5,","    )","    ","    # Verify Top-K is based solely on proxy_value","    stage0_by_id = {r.param_id: r for r in result.stage0_results}","    ","    # Get proxy_values for Top-K","    topk_values = [stage0_by_id[pid].proxy_value for pid in result.topk_param_ids]","    ","    # Get proxy_values for all params","    all_values = [r.proxy_value for r in result.stage0_results]","    all_values_sorted = sorted(all_values, reverse=True)","    ","    # Top-K values should match top K values from all params","    assert topk_values == all_values_sorted[:5], (","        f\"Top-K should contain top 5 proxy_values:\\n\"","        f\"  Top-K values: {topk_values}\\n\"","        f\"  Top 5 values:  {all_values_sorted[:5]}\"","    )","","","def test_funnel_stage2_only_runs_topk():","    \"\"\"Test that Stage2 only runs on Top-K parameters, not all parameters.\"\"\"","    np.random.seed(42)","    n_bars = 200","    n_params = 15","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    open_ = close + np.random.randn(n_bars) * 2","    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3","    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3","    ","    params_matrix = np.column_stack([","        np.random.randint(10, 30, size=n_params),","        np.random.randint(5, 20, size=n_params),","        np.random.uniform(1.0, 2.0, size=n_params),","    ]).astype(np.float64)","    ","    result = run_funnel(","        open_,","        high,","        low,","        close,","        params_matrix,","        k=3,","    )","    ","    # Verify Stage0 ran on all params","    assert len(result.stage0_results) == n_params","    ","    # Verify Top-K selected","    assert len(result.topk_param_ids) == 3","    ","    # Verify Stage2 ran ONLY on Top-K (not all params)","    assert len(result.stage2_results) == 3, (","        f\"Stage2 should run only on Top-K (3 params), not all params ({n_params})\"","    )","    ","    # Verify Stage2 param_ids match Top-K","    stage2_param_ids = set(r.param_id for r in result.stage2_results)","    topk_param_ids_set = set(result.topk_param_ids)","    assert stage2_param_ids == topk_param_ids_set, (","        f\"Stage2 param_ids should match Top-K:\\n\"","        f\"  Stage2: {stage2_param_ids}\\n\"","        f\"  Top-K:  {topk_param_ids_set}\"","    )","","","def test_funnel_stage0_no_pnl_fields():","    \"\"\"Test that Stage0 results contain NO PnL-related fields.\"\"\"","    np.random.seed(42)","    n_bars = 200","    n_params = 10","    ","    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10","    open_ = close + np.random.randn(n_bars) * 2","    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3","    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3","    ","    params_matrix = np.column_stack([","        np.random.randint(10, 30, size=n_params),","        np.random.randint(5, 20, size=n_params),","        np.random.uniform(1.0, 2.0, size=n_params),","    ]).astype(np.float64)","    ","    result = run_funnel(","        open_,","        high,","        low,","        close,"]}
{"type":"file_chunk","path":"tests/test_funnel_topk_no_human_contract.py","chunk_index":1,"line_start":201,"line_end":225,"content":["        params_matrix,","        k=5,","    )","    ","    # Check all Stage0 results","    forbidden_fields = {\"net\", \"profit\", \"mdd\", \"dd\", \"drawdown\", \"sqn\", \"sharpe\", ","                       \"winrate\", \"equity\", \"pnl\", \"trades\", \"score\"}","    ","    for stage0_result in result.stage0_results:","        # Get field names","        if hasattr(stage0_result, \"__dataclass_fields__\"):","            field_names = set(stage0_result.__dataclass_fields__.keys())","        else:","            field_names = set(getattr(stage0_result, \"__dict__\", {}).keys())","        ","        # Check no forbidden fields","        for field_name in field_names:","            field_lower = field_name.lower()","            for forbidden in forbidden_fields:","                assert forbidden not in field_lower, (","                    f\"Stage0Result contains forbidden PnL field: {field_name} \"","                    f\"(contains '{forbidden}')\"","                )","",""]}
{"type":"file_footer","path":"tests/test_funnel_topk_no_human_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_generate_research_cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4080,"sha256":"560ba869562172f07c7455f5a127b136567a5119045942cb7dd10c6bd2cb9211","total_lines":118,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_generate_research_cli.py","chunk_index":0,"line_start":1,"line_end":118,"content":["\"\"\"Test generate_research.py CLI behavior.","","Ensure that:","1. -h / --help does not execute generate logic","2. --dry-run works without writing files","3. Script does not crash on import errors","\"\"\"","","from __future__ import annotations","","import subprocess","import sys","from pathlib import Path","import pytest","","","def test_generate_research_help_does_not_execute():","    \"\"\"Test that -h/--help does not execute generate logic.\"\"\"","    # Test -h","    result = subprocess.run(","        [sys.executable, \"scripts/generate_research.py\", \"-h\"],","        cwd=Path(__file__).parent.parent,","        capture_output=True,","        text=True,","    )","    ","    assert result.returncode == 0, f\"Help should exit with 0, got {result.returncode}\"","    assert \"usage:\" in result.stdout.lower() or \"help\" in result.stdout.lower()","    assert \"error\" not in result.stdout.lower()","    assert \"error\" not in result.stderr.lower()","    ","    # Test --help","    result = subprocess.run(","        [sys.executable, \"scripts/generate_research.py\", \"--help\"],","        cwd=Path(__file__).parent.parent,","        capture_output=True,","        text=True,","    )","    ","    assert result.returncode == 0, f\"Help should exit with 0, got {result.returncode}\"","    assert \"usage:\" in result.stdout.lower() or \"help\" in result.stdout.lower()","    assert \"error\" not in result.stdout.lower()","    assert \"error\" not in result.stderr.lower()","","","def test_generate_research_dry_run():","    \"\"\"Test that --dry-run works without writing files.\"\"\"","    # Create a temporary outputs directory to test","    import tempfile","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        outputs_root = tmp_path / \"outputs\"","        outputs_root.mkdir()","        ","        result = subprocess.run(","            [","                sys.executable,","                \"scripts/generate_research.py\",","                \"--outputs-root\", str(outputs_root),","                \"--dry-run\",","                \"--verbose\",","            ],","            cwd=Path(__file__).parent.parent,","            capture_output=True,","            text=True,","        )","        ","        assert result.returncode == 0, f\"Dry run should exit with 0, got {result.returncode}\"","        assert \"dry run\" in result.stdout.lower() or \"would generate\" in result.stdout.lower()","        ","        # Ensure no files were actually created","        research_dir = outputs_root / \"research\"","        assert not research_dir.exists() or not list(research_dir.glob(\"*.json\"))","","","def test_generate_research_without_outputs_dir():","    \"\"\"Test that script handles missing outputs directory gracefully.\"\"\"","    import tempfile","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        outputs_root = tmp_path / \"nonexistent\"","        ","        result = subprocess.run(","            [","                sys.executable,","                \"scripts/generate_research.py\",","                \"--outputs-root\", str(outputs_root),","            ],","            cwd=Path(__file__).parent.parent,","            capture_output=True,","            text=True,","        )","        ","        # Should either succeed (creating empty results) or fail gracefully","        # but not crash with import errors","        assert result.returncode in (0, 1), f\"Unexpected exit code: {result.returncode}\"","        assert \"import error\" not in result.stderr.lower(), f\"Import error occurred: {result.stderr}\"","","","def test_generate_research_import_fixed():","    \"\"\"Test that import errors are fixed (no NameError for extract_canonical_metrics).\"\"\"","    # This test imports the module directly to check for import errors","    # Note: conftest.py already adds src/ to sys.path, so no need to modify it here","    ","    try:","        from research.__main__ import generate_canonical_results","        from research.registry import build_research_index","        ","        # If we get here, imports succeeded","        assert True","    except ImportError as e:","        pytest.fail(f\"Import error: {e}\")","    except NameError as e:","        pytest.fail(f\"NameError (missing import): {e}\")","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_generate_research_cli.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_golden_kernel_verification.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2498,"sha256":"1a203a6a9a8ab7c3ed9c3716b9771115e5ae93fe0566e605098720f17c3055fa","total_lines":78,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_golden_kernel_verification.py","chunk_index":0,"line_start":1,"line_end":78,"content":["","import numpy as np","","from strategy.kernel import DonchianAtrParams, run_kernel, _max_drawdown","from engine.types import BarArrays","","","def _bars():","    # Small synthetic OHLC series","    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)","    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)","    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)","    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)","    return BarArrays(open=o, high=h, low=l, close=c)","","","def test_no_trade_case_does_not_crash_and_returns_zero_metrics():","    bars = _bars()","    params = DonchianAtrParams(channel_len=99999, atr_len=3, stop_mult=2.0)","","    out = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)","    pnl = out[\"pnl\"]","    equity = out[\"equity\"]","    metrics = out[\"metrics\"]","","    assert isinstance(pnl, np.ndarray)","    assert pnl.size == 0","    assert isinstance(equity, np.ndarray)","    assert equity.size == 0","    assert metrics[\"net_profit\"] == 0.0","    assert metrics[\"trades\"] == 0","    assert metrics[\"max_dd\"] == 0.0","","","def test_vectorized_metrics_are_self_consistent():","    bars = _bars()","    params = DonchianAtrParams(channel_len=2, atr_len=2, stop_mult=1.0)","","    out = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)","    pnl = out[\"pnl\"]","    equity = out[\"equity\"]","    metrics = out[\"metrics\"]","","    # If zero trades, still must be consistent","    if pnl.size == 0:","        assert metrics[\"net_profit\"] == 0.0","        assert metrics[\"trades\"] == 0","        assert metrics[\"max_dd\"] == 0.0","        return","","    # Vectorized checks","    np.testing.assert_allclose(equity, np.cumsum(pnl), rtol=0.0, atol=0.0)","    assert metrics[\"trades\"] == int(pnl.size)","    assert metrics[\"net_profit\"] == float(np.sum(pnl))","    assert metrics[\"max_dd\"] == _max_drawdown(equity)","","","def test_costs_are_parameterized_not_hardcoded():","    bars = _bars()","    params = DonchianAtrParams(channel_len=2, atr_len=2, stop_mult=1.0)","","    out0 = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)","    out1 = run_kernel(bars, params, commission=1.25, slip=0.75, order_qty=1)","","    pnl0 = out0[\"pnl\"]","    pnl1 = out1[\"pnl\"]","","    # Either both empty or both non-empty; if empty, pass","    if pnl0.size == 0:","        assert pnl1.size == 0","        return","","    # Costs increase => pnl decreases by 2*(commission+slip) per trade","    per_trade_delta = 2.0 * (1.25 + 0.75)","    np.testing.assert_allclose(pnl1, pnl0 - per_trade_delta, rtol=0.0, atol=1e-12)","","",""]}
{"type":"file_footer","path":"tests/test_golden_kernel_verification.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_governance_accepts_winners_v2.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8277,"sha256":"7fcca6f5b67a07de96a7836caecd5e5f5b67f6160e2f84d4d28a379221704fba","total_lines":235,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_governance_accepts_winners_v2.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for governance accepting winners v2.","","Tests verify that governance evaluator can read and process v2 winners.json.","\"\"\"","","from __future__ import annotations","","import json","import tempfile","from datetime import datetime, timezone","from pathlib import Path","","from core.governance_schema import Decision","from pipeline.governance_eval import evaluate_governance","","","def _create_fake_manifest(run_id: str, stage_name: str, season: str = \"test\") -> dict:","    \"\"\"Create fake manifest.json.\"\"\"","    return {","        \"run_id\": run_id,","        \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        \"git_sha\": \"abc123def456\",","        \"dirty_repo\": False,","        \"param_subsample_rate\": 0.1,","        \"config_hash\": \"test_hash\",","        \"season\": season,","        \"dataset_id\": \"test_dataset\",","        \"bars\": 1000,","        \"params_total\": 1000,","        \"params_effective\": 100,","        \"artifact_version\": \"v1\",","    }","","","def _create_fake_metrics(stage_name: str, stage_planned_subsample: float = 0.1) -> dict:","    \"\"\"Create fake metrics.json.\"\"\"","    return {","        \"params_total\": 1000,","        \"params_effective\": 100,","        \"bars\": 1000,","        \"stage_name\": stage_name,","        \"param_subsample_rate\": stage_planned_subsample,","        \"stage_planned_subsample\": stage_planned_subsample,","    }","","","def _create_fake_winners_v2(stage_name: str, topk_items: list[dict]) -> dict:","    \"\"\"Create fake winners.json v2.\"\"\"","    return {","        \"schema\": \"v2\",","        \"stage_name\": stage_name,","        \"generated_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        \"topk\": topk_items,","        \"notes\": {","            \"schema\": \"v2\",","            \"candidate_id_mode\": \"strategy_id:param_id\",","        },","    }","","","def _create_fake_config_snapshot() -> dict:","    \"\"\"Create fake config_snapshot.json.\"\"\"","    return {","        \"dataset_id\": \"test_dataset\",","        \"bars\": 1000,","        \"params_total\": 1000,","    }","","","def _write_artifacts(run_dir: Path, manifest: dict, metrics: dict, winners: dict, config: dict) -> None:","    \"\"\"Write artifacts to run directory.\"\"\"","    run_dir.mkdir(parents=True, exist_ok=True)","    ","    with (run_dir / \"manifest.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f, indent=2)","    ","    with (run_dir / \"metrics.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(metrics, f, indent=2)","    ","    with (run_dir / \"winners.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(winners, f, indent=2)","    ","    with (run_dir / \"config_snapshot.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(config, f, indent=2)","","","def test_governance_reads_winners_v2() -> None:","    \"\"\"Test that governance can read and process v2 winners.json.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Stage0 artifacts","        stage0_dir = tmp_path / \"stage0\"","        _write_artifacts(","            stage0_dir,","            _create_fake_manifest(\"stage0-123\", \"stage0_coarse\"),","            _create_fake_metrics(\"stage0_coarse\"),","            _create_fake_winners_v2(\"stage0_coarse\", [","                {","                    \"candidate_id\": \"donchian_atr:0\",","                    \"strategy_id\": \"donchian_atr\",","                    \"symbol\": \"CME.MNQ\",","                    \"timeframe\": \"60m\",","                    \"params\": {},","                    \"score\": 1.0,","                    \"metrics\": {\"proxy_value\": 1.0, \"param_id\": 0},","                    \"source\": {\"param_id\": 0, \"run_id\": \"stage0-123\", \"stage_name\": \"stage0_coarse\"},","                },","            ]),","            _create_fake_config_snapshot(),","        )","        ","        # Stage1 artifacts (v2 format)","        stage1_dir = tmp_path / \"stage1\"","        stage1_winners = _create_fake_winners_v2(\"stage1_topk\", [","            {","                \"candidate_id\": \"donchian_atr:0\",","                \"strategy_id\": \"donchian_atr\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe\": \"60m\",","                \"params\": {},","                \"score\": 100.0,","                \"metrics\": {\"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0, \"param_id\": 0},","                \"source\": {\"param_id\": 0, \"run_id\": \"stage1-123\", \"stage_name\": \"stage1_topk\"},","            },","        ])","        _write_artifacts(","            stage1_dir,","            _create_fake_manifest(\"stage1-123\", \"stage1_topk\"),","            _create_fake_metrics(\"stage1_topk\"),","            stage1_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Stage2 artifacts (v2 format)","        stage2_dir = tmp_path / \"stage2\"","        stage2_winners = _create_fake_winners_v2(\"stage2_confirm\", [","            {","                \"candidate_id\": \"donchian_atr:0\",","                \"strategy_id\": \"donchian_atr\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe\": \"60m\",","                \"params\": {},","                \"score\": 100.0,","                \"metrics\": {\"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0, \"param_id\": 0},","                \"source\": {\"param_id\": 0, \"run_id\": \"stage2-123\", \"stage_name\": \"stage2_confirm\"},","            },","        ])","        _write_artifacts(","            stage2_dir,","            _create_fake_manifest(\"stage2-123\", \"stage2_confirm\"),","            _create_fake_metrics(\"stage2_confirm\"),","            stage2_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Evaluate governance","        report = evaluate_governance(","            stage0_dir=stage0_dir,","            stage1_dir=stage1_dir,","            stage2_dir=stage2_dir,","        )","        ","        # Verify governance processed v2 format","        assert len(report.items) == 1","        item = report.items[0]","        ","        # Verify candidate_id is preserved","        assert item.candidate_id == \"donchian_atr:0\"","        ","        # Verify decision was made (should be KEEP since all rules pass)","        assert item.decision in (Decision.KEEP, Decision.FREEZE, Decision.DROP)","","","def test_governance_handles_mixed_v2_legacy() -> None:","    \"\"\"Test that governance handles mixed v2/legacy formats gracefully.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Stage0 artifacts (legacy)","        stage0_dir = tmp_path / \"stage0\"","        _write_artifacts(","            stage0_dir,","            _create_fake_manifest(\"stage0-123\", \"stage0_coarse\"),","            _create_fake_metrics(\"stage0_coarse\"),","            {\"topk\": [{\"param_id\": 0, \"proxy_value\": 1.0}], \"notes\": {\"schema\": \"v1\"}},","            _create_fake_config_snapshot(),","        )","        ","        # Stage1 artifacts (v2)","        stage1_dir = tmp_path / \"stage1\"","        stage1_winners = _create_fake_winners_v2(\"stage1_topk\", [","            {","                \"candidate_id\": \"donchian_atr:0\",","                \"strategy_id\": \"donchian_atr\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe\": \"60m\",","                \"params\": {},","                \"score\": 100.0,"]}
{"type":"file_chunk","path":"tests/test_governance_accepts_winners_v2.py","chunk_index":1,"line_start":201,"line_end":235,"content":["                \"metrics\": {\"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0, \"param_id\": 0},","                \"source\": {\"param_id\": 0, \"run_id\": \"stage1-123\", \"stage_name\": \"stage1_topk\"},","            },","        ])","        _write_artifacts(","            stage1_dir,","            _create_fake_manifest(\"stage1-123\", \"stage1_topk\"),","            _create_fake_metrics(\"stage1_topk\"),","            stage1_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Stage2 artifacts (legacy)","        stage2_dir = tmp_path / \"stage2\"","        _write_artifacts(","            stage2_dir,","            _create_fake_manifest(\"stage2-123\", \"stage2_confirm\"),","            _create_fake_metrics(\"stage2_confirm\"),","            {\"topk\": [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}], \"notes\": {\"schema\": \"v1\"}},","            _create_fake_config_snapshot(),","        )","        ","        # Evaluate governance (should handle mixed formats)","        report = evaluate_governance(","            stage0_dir=stage0_dir,","            stage1_dir=stage1_dir,","            stage2_dir=stage2_dir,","        )","        ","        # Verify governance processed successfully","        assert len(report.items) == 1","        item = report.items[0]","        assert item.candidate_id == \"donchian_atr:0\"","",""]}
{"type":"file_footer","path":"tests/test_governance_accepts_winners_v2.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_governance_eval_rules.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":11817,"sha256":"2269afcaf928ddf21acb7714026424aa0735e8dc5b698cce40e46cac30b957f1","total_lines":352,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_governance_eval_rules.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for governance evaluation rules.","","Tests that governance rules (R1/R2/R3) are correctly applied using fixture artifacts.","\"\"\"","","from __future__ import annotations","","import json","import tempfile","from pathlib import Path","from datetime import datetime, timezone","","import pytest","","from core.governance_schema import Decision","from pipeline.governance_eval import evaluate_governance","","","def _create_fake_manifest(run_id: str, stage_name: str, season: str = \"test\") -> dict:","    \"\"\"Create fake manifest.json.\"\"\"","    return {","        \"run_id\": run_id,","        \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        \"git_sha\": \"abc123def456\",","        \"dirty_repo\": False,","        \"param_subsample_rate\": 0.1,","        \"config_hash\": \"test_hash\",","        \"season\": season,","        \"dataset_id\": \"test_dataset\",","        \"bars\": 1000,","        \"params_total\": 1000,","        \"params_effective\": 100,","        \"artifact_version\": \"v1\",","    }","","","def _create_fake_metrics(stage_name: str, stage_planned_subsample: float = 0.1) -> dict:","    \"\"\"Create fake metrics.json.\"\"\"","    return {","        \"params_total\": 1000,","        \"params_effective\": 100,","        \"bars\": 1000,","        \"stage_name\": stage_name,","        \"param_subsample_rate\": stage_planned_subsample,","        \"stage_planned_subsample\": stage_planned_subsample,","    }","","","def _create_fake_winners(stage_name: str, topk_items: list[dict]) -> dict:","    \"\"\"Create fake winners.json.\"\"\"","    return {","        \"topk\": topk_items,","        \"notes\": {","            \"schema\": \"v1\",","            \"stage\": stage_name,","            \"topk_count\": len(topk_items),","        },","    }","","","def _create_fake_config_snapshot() -> dict:","    \"\"\"Create fake config_snapshot.json.\"\"\"","    return {","        \"dataset_id\": \"test_dataset\",","        \"bars\": 1000,","        \"params_total\": 1000,","    }","","","def _write_artifacts(run_dir: Path, manifest: dict, metrics: dict, winners: dict, config: dict) -> None:","    \"\"\"Write artifacts to run directory.\"\"\"","    run_dir.mkdir(parents=True, exist_ok=True)","    ","    with (run_dir / \"manifest.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f, indent=2)","    ","    with (run_dir / \"metrics.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(metrics, f, indent=2)","    ","    with (run_dir / \"winners.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(winners, f, indent=2)","    ","    with (run_dir / \"config_snapshot.json\").open(\"w\", encoding=\"utf-8\") as f:","        json.dump(config, f, indent=2)","","","def test_r1_drop_when_stage2_missing() -> None:","    \"\"\"","    Test R1: DROP when candidate in Stage1 but missing in Stage2.","    ","    Scenario:","    - Stage1 has candidate with param_id=0","    - Stage2 does not have candidate with param_id=0","    - Expected: DROP with reason \"unverified\"","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Stage0 artifacts","        stage0_dir = tmp_path / \"stage0\"","        _write_artifacts(","            stage0_dir,","            _create_fake_manifest(\"stage0-123\", \"stage0_coarse\"),","            _create_fake_metrics(\"stage0_coarse\"),","            _create_fake_winners(\"stage0_coarse\", [{\"param_id\": 0, \"proxy_value\": 1.0}]),","            _create_fake_config_snapshot(),","        )","        ","        # Stage1 artifacts (has candidate)","        stage1_dir = tmp_path / \"stage1\"","        stage1_winners = _create_fake_winners(","            \"stage1_topk\",","            [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}],","        )","        _write_artifacts(","            stage1_dir,","            _create_fake_manifest(\"stage1-123\", \"stage1_topk\"),","            _create_fake_metrics(\"stage1_topk\"),","            stage1_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Stage2 artifacts (missing candidate)","        stage2_dir = tmp_path / \"stage2\"","        stage2_winners = _create_fake_winners(","            \"stage2_confirm\",","            [{\"param_id\": 1, \"net_profit\": 200.0, \"trades\": 20, \"max_dd\": -20.0}],  # Different param_id","        )","        _write_artifacts(","            stage2_dir,","            _create_fake_manifest(\"stage2-123\", \"stage2_confirm\"),","            _create_fake_metrics(\"stage2_confirm\"),","            stage2_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Evaluate governance","        report = evaluate_governance(","            stage0_dir=stage0_dir,","            stage1_dir=stage1_dir,","            stage2_dir=stage2_dir,","        )","        ","        # Verify: candidate should be DROP","        assert len(report.items) == 1","        item = report.items[0]","        assert item.decision == Decision.DROP","        assert any(\"R1\" in reason for reason in item.reasons)","        assert any(\"unverified\" in reason.lower() for reason in item.reasons)","","","def test_r2_drop_when_metric_degrades_over_threshold() -> None:","    \"\"\"","    Test R2: DROP when metrics degrade > 20% from Stage1 to Stage2.","    ","    Scenario:","    - Stage1: net_profit=100, max_dd=-10 -> net_over_mdd = 10.0","    - Stage2: net_profit=70, max_dd=-10 -> net_over_mdd = 7.0","    - Degradation: (10.0 - 7.0) / 10.0 = 0.30 (30% > 20% threshold)","    - Expected: DROP with reason \"degraded\"","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Stage0 artifacts","        stage0_dir = tmp_path / \"stage0\"","        _write_artifacts(","            stage0_dir,","            _create_fake_manifest(\"stage0-123\", \"stage0_coarse\"),","            _create_fake_metrics(\"stage0_coarse\"),","            _create_fake_winners(\"stage0_coarse\", [{\"param_id\": 0, \"proxy_value\": 1.0}]),","            _create_fake_config_snapshot(),","        )","        ","        # Stage1 artifacts","        stage1_dir = tmp_path / \"stage1\"","        stage1_winners = _create_fake_winners(","            \"stage1_topk\",","            [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}],","        )","        _write_artifacts(","            stage1_dir,","            _create_fake_manifest(\"stage1-123\", \"stage1_topk\"),","            _create_fake_metrics(\"stage1_topk\"),","            stage1_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Stage2 artifacts (degraded metrics)","        stage2_dir = tmp_path / \"stage2\"","        stage2_winners = _create_fake_winners(","            \"stage2_confirm\",","            [{\"param_id\": 0, \"net_profit\": 70.0, \"trades\": 10, \"max_dd\": -10.0}],  # 30% degradation","        )","        _write_artifacts(","            stage2_dir,","            _create_fake_manifest(\"stage2-123\", \"stage2_confirm\"),","            _create_fake_metrics(\"stage2_confirm\"),","            stage2_winners,"]}
{"type":"file_chunk","path":"tests/test_governance_eval_rules.py","chunk_index":1,"line_start":201,"line_end":352,"content":["            _create_fake_config_snapshot(),","        )","        ","        # Evaluate governance","        report = evaluate_governance(","            stage0_dir=stage0_dir,","            stage1_dir=stage1_dir,","            stage2_dir=stage2_dir,","        )","        ","        # Verify: candidate should be DROP","        assert len(report.items) == 1","        item = report.items[0]","        assert item.decision == Decision.DROP","        assert any(\"R2\" in reason for reason in item.reasons)","        assert any(\"degraded\" in reason.lower() for reason in item.reasons)","","","def test_r3_freeze_when_density_over_threshold() -> None:","    \"\"\"","    Test R3: FREEZE when same strategy_id appears >= 3 times in Stage1 topk.","    ","    Scenario:","    - Stage1 has 5 candidates with same strategy_id (donchian_atr)","    - Expected: FREEZE with reason \"density\"","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Stage0 artifacts","        stage0_dir = tmp_path / \"stage0\"","        _write_artifacts(","            stage0_dir,","            _create_fake_manifest(\"stage0-123\", \"stage0_coarse\"),","            _create_fake_metrics(\"stage0_coarse\"),","            _create_fake_winners(\"stage0_coarse\", [{\"param_id\": i, \"proxy_value\": 1.0} for i in range(5)]),","            _create_fake_config_snapshot(),","        )","        ","        # Stage1 artifacts (5 candidates)","        stage1_dir = tmp_path / \"stage1\"","        stage1_winners = _create_fake_winners(","            \"stage1_topk\",","            [","                {\"param_id\": i, \"net_profit\": 100.0 + i, \"trades\": 10, \"max_dd\": -10.0}","                for i in range(5)","            ],","        )","        _write_artifacts(","            stage1_dir,","            _create_fake_manifest(\"stage1-123\", \"stage1_topk\"),","            _create_fake_metrics(\"stage1_topk\"),","            stage1_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Stage2 artifacts (all candidates present)","        stage2_dir = tmp_path / \"stage2\"","        stage2_winners = _create_fake_winners(","            \"stage2_confirm\",","            [","                {\"param_id\": i, \"net_profit\": 100.0 + i, \"trades\": 10, \"max_dd\": -10.0}","                for i in range(5)","            ],","        )","        _write_artifacts(","            stage2_dir,","            _create_fake_manifest(\"stage2-123\", \"stage2_confirm\"),","            _create_fake_metrics(\"stage2_confirm\"),","            stage2_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Evaluate governance","        report = evaluate_governance(","            stage0_dir=stage0_dir,","            stage1_dir=stage1_dir,","            stage2_dir=stage2_dir,","        )","        ","        # Verify: all candidates should be FREEZE (density >= 3)","        assert len(report.items) == 5","        for item in report.items:","            assert item.decision == Decision.FREEZE","            assert any(\"R3\" in reason for reason in item.reasons)","            assert any(\"density\" in reason.lower() for reason in item.reasons)","","","def test_keep_when_all_rules_pass() -> None:","    \"\"\"","    Test KEEP when all rules pass.","    ","    Scenario:","    - R1: Stage2 has candidate (pass)","    - R2: Metrics do not degrade (pass)","    - R3: Density < threshold (pass)","    - Expected: KEEP","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Stage0 artifacts","        stage0_dir = tmp_path / \"stage0\"","        _write_artifacts(","            stage0_dir,","            _create_fake_manifest(\"stage0-123\", \"stage0_coarse\"),","            _create_fake_metrics(\"stage0_coarse\"),","            _create_fake_winners(\"stage0_coarse\", [{\"param_id\": 0, \"proxy_value\": 1.0}]),","            _create_fake_config_snapshot(),","        )","        ","        # Stage1 artifacts (single candidate, low density)","        stage1_dir = tmp_path / \"stage1\"","        stage1_winners = _create_fake_winners(","            \"stage1_topk\",","            [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}],","        )","        _write_artifacts(","            stage1_dir,","            _create_fake_manifest(\"stage1-123\", \"stage1_topk\"),","            _create_fake_metrics(\"stage1_topk\"),","            stage1_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Stage2 artifacts (same metrics, no degradation)","        stage2_dir = tmp_path / \"stage2\"","        stage2_winners = _create_fake_winners(","            \"stage2_confirm\",","            [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}],","        )","        _write_artifacts(","            stage2_dir,","            _create_fake_manifest(\"stage2-123\", \"stage2_confirm\"),","            _create_fake_metrics(\"stage2_confirm\"),","            stage2_winners,","            _create_fake_config_snapshot(),","        )","        ","        # Evaluate governance","        report = evaluate_governance(","            stage0_dir=stage0_dir,","            stage1_dir=stage1_dir,","            stage2_dir=stage2_dir,","        )","        ","        # Verify: candidate should be KEEP","        assert len(report.items) == 1","        item = report.items[0]","        assert item.decision == Decision.KEEP","",""]}
{"type":"file_footer","path":"tests/test_governance_eval_rules.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_governance_schema_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3499,"sha256":"b65de1f36411a53f59cdec600187a4fbfc98cba7c79bb6b03a94c7b61585e4bc","total_lines":115,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_governance_schema_contract.py","chunk_index":0,"line_start":1,"line_end":115,"content":["","\"\"\"Contract tests for governance schema.","","Tests that governance schema is JSON-serializable and follows contracts.","\"\"\"","","from __future__ import annotations","","import json","from datetime import datetime, timezone","","from core.governance_schema import (","    Decision,","    EvidenceRef,","    GovernanceItem,","    GovernanceReport,",")","","","def test_governance_report_json_serializable() -> None:","    \"\"\"","    Test that GovernanceReport is JSON-serializable.","    ","    This is a critical contract: governance.json must be machine-readable.","    \"\"\"","    # Create sample evidence","    evidence = [","        EvidenceRef(","            run_id=\"test-run-123\",","            stage_name=\"stage1_topk\",","            artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","            key_metrics={\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10},","        ),","    ]","    ","    # Create sample item","    item = GovernanceItem(","        candidate_id=\"donchian_atr:abc123def456\",","        decision=Decision.KEEP,","        reasons=[\"R3: density_5_over_threshold_3\"],","        evidence=evidence,","        created_at=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        git_sha=\"abc123def456\",","    )","    ","    # Create report","    report = GovernanceReport(","        items=[item],","        metadata={","            \"governance_id\": \"gov-20251218T000000Z-12345678\",","            \"season\": \"test_season\",","            \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","            \"git_sha\": \"abc123def456\",","        },","    )","    ","    # Convert to dict","    report_dict = report.to_dict()","    ","    # Serialize to JSON","    json_str = json.dumps(report_dict, ensure_ascii=False, sort_keys=True, indent=2)","    ","    # Deserialize back","    report_dict_roundtrip = json.loads(json_str)","    ","    # Verify structure","    assert \"items\" in report_dict_roundtrip","    assert \"metadata\" in report_dict_roundtrip","    assert len(report_dict_roundtrip[\"items\"]) == 1","    ","    item_dict = report_dict_roundtrip[\"items\"][0]","    assert item_dict[\"candidate_id\"] == \"donchian_atr:abc123def456\"","    assert item_dict[\"decision\"] == \"KEEP\"","    assert len(item_dict[\"reasons\"]) == 1","    assert len(item_dict[\"evidence\"]) == 1","    ","    evidence_dict = item_dict[\"evidence\"][0]","    assert evidence_dict[\"run_id\"] == \"test-run-123\"","    assert evidence_dict[\"stage_name\"] == \"stage1_topk\"","    assert \"artifact_paths\" in evidence_dict","    assert \"key_metrics\" in evidence_dict","","","def test_decision_enum_values() -> None:","    \"\"\"Test that Decision enum has correct values.\"\"\"","    assert Decision.KEEP.value == \"KEEP\"","    assert Decision.FREEZE.value == \"FREEZE\"","    assert Decision.DROP.value == \"DROP\"","","","def test_evidence_ref_contains_subsample_fields() -> None:","    \"\"\"","    Test that EvidenceRef can contain subsample fields in key_metrics.","    ","    This is a critical requirement: subsample info must be in evidence.","    \"\"\"","    evidence = EvidenceRef(","        run_id=\"test-run-123\",","        stage_name=\"stage1_topk\",","        artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","        key_metrics={","            \"param_id\": 0,","            \"net_profit\": 100.0,","            \"stage_planned_subsample\": 0.1,","            \"param_subsample_rate\": 0.1,","            \"params_effective\": 100,","        },","    )","    ","    # Verify subsample fields are present","    assert \"stage_planned_subsample\" in evidence.key_metrics","    assert \"param_subsample_rate\" in evidence.key_metrics","    assert \"params_effective\" in evidence.key_metrics","",""]}
{"type":"file_footer","path":"tests/test_governance_schema_contract.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_governance_transition.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2738,"sha256":"acab20bb4fb6f9c10a38acc925f5af2f8ae3fdd7320a3c19ce54e6726c439514","total_lines":83,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_governance_transition.py","chunk_index":0,"line_start":1,"line_end":83,"content":["","\"\"\"Contract tests for governance lifecycle state transitions.","","Tests transition matrix: prev_state × decision → next_state","\"\"\"","","from __future__ import annotations","","import pytest","","from core.governance.transition import governance_transition","from core.schemas.governance import Decision, LifecycleState","","","# Transition test matrix: (prev_state, decision, expected_next_state)","TRANSITION_TEST_CASES = [","    # INCUBATION transitions","    (\"INCUBATION\", Decision.KEEP, \"CANDIDATE\"),","    (\"INCUBATION\", Decision.DROP, \"RETIRED\"),","    (\"INCUBATION\", Decision.FREEZE, \"INCUBATION\"),","    ","    # CANDIDATE transitions","    (\"CANDIDATE\", Decision.KEEP, \"LIVE\"),","    (\"CANDIDATE\", Decision.DROP, \"RETIRED\"),","    (\"CANDIDATE\", Decision.FREEZE, \"CANDIDATE\"),","    ","    # LIVE transitions","    (\"LIVE\", Decision.KEEP, \"LIVE\"),","    (\"LIVE\", Decision.DROP, \"RETIRED\"),","    (\"LIVE\", Decision.FREEZE, \"LIVE\"),","    ","    # RETIRED is terminal (no transitions)","    (\"RETIRED\", Decision.KEEP, \"RETIRED\"),","    (\"RETIRED\", Decision.DROP, \"RETIRED\"),","    (\"RETIRED\", Decision.FREEZE, \"RETIRED\"),","]","","","@pytest.mark.parametrize(\"prev_state,decision,expected_next_state\", TRANSITION_TEST_CASES)","def test_governance_transition_matrix(","    prev_state: LifecycleState,","    decision: Decision,","    expected_next_state: LifecycleState,",") -> None:","    \"\"\"","    Test governance transition for all state × decision combinations.","    ","    This is a table-driven test covering the complete transition matrix.","    \"\"\"","    result = governance_transition(prev_state, decision)","    ","    assert result == expected_next_state, (","        f\"Transition failed: {prev_state} + {decision.value} → {result}, \"","        f\"expected {expected_next_state}\"","    )","","","def test_governance_transition_incubation_to_candidate() -> None:","    \"\"\"Test INCUBATION → CANDIDATE transition.\"\"\"","    result = governance_transition(\"INCUBATION\", Decision.KEEP)","    assert result == \"CANDIDATE\"","","","def test_governance_transition_incubation_to_retired() -> None:","    \"\"\"Test INCUBATION → RETIRED transition.\"\"\"","    result = governance_transition(\"INCUBATION\", Decision.DROP)","    assert result == \"RETIRED\"","","","def test_governance_transition_candidate_to_live() -> None:","    \"\"\"Test CANDIDATE → LIVE transition.\"\"\"","    result = governance_transition(\"CANDIDATE\", Decision.KEEP)","    assert result == \"LIVE\"","","","def test_governance_transition_retired_terminal() -> None:","    \"\"\"Test that RETIRED is terminal state (no transitions).\"\"\"","    # RETIRED should remain RETIRED regardless of decision","    assert governance_transition(\"RETIRED\", Decision.KEEP) == \"RETIRED\"","    assert governance_transition(\"RETIRED\", Decision.DROP) == \"RETIRED\"","    assert governance_transition(\"RETIRED\", Decision.FREEZE) == \"RETIRED\"","",""]}
{"type":"file_footer","path":"tests/test_governance_transition.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_governance_writer_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7463,"sha256":"8be6d032d017879c3ad0b91713977950ba97f665546c8c13c64c63352d0edd7e","total_lines":212,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_governance_writer_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for governance writer.","","Tests that governance writer creates expected directory structure and files.","\"\"\"","","from __future__ import annotations","","import json","import tempfile","from pathlib import Path","from datetime import datetime, timezone","","from core.governance_schema import (","    Decision,","    EvidenceRef,","    GovernanceItem,","    GovernanceReport,",")","from core.governance_writer import write_governance_artifacts","","","def test_governance_writer_creates_expected_tree() -> None:","    \"\"\"","    Test that governance writer creates expected directory structure.","    ","    Expected:","    - governance.json (machine-readable)","    - README.md (human-readable)","    - evidence_index.json (optional but recommended)","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        governance_dir = Path(tmpdir) / \"governance\" / \"test-123\"","        ","        # Create sample report","        evidence = [","            EvidenceRef(","                run_id=\"stage1-123\",","                stage_name=\"stage1_topk\",","                artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","                key_metrics={","                    \"param_id\": 0,","                    \"net_profit\": 100.0,","                    \"stage_planned_subsample\": 0.1,","                    \"param_subsample_rate\": 0.1,","                    \"params_effective\": 100,","                },","            ),","        ]","        ","        item = GovernanceItem(","            candidate_id=\"donchian_atr:abc123def456\",","            decision=Decision.KEEP,","            reasons=[],","            evidence=evidence,","            created_at=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","            git_sha=\"abc123def456\",","        )","        ","        report = GovernanceReport(","            items=[item],","            metadata={","                \"governance_id\": \"gov-123\",","                \"season\": \"test_season\",","                \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","                \"git_sha\": \"abc123def456\",","                \"decisions\": {\"KEEP\": 1, \"FREEZE\": 0, \"DROP\": 0},","            },","        )","        ","        # Write artifacts","        write_governance_artifacts(governance_dir, report)","        ","        # Verify files exist","        assert governance_dir.exists()","        assert (governance_dir / \"governance.json\").exists()","        assert (governance_dir / \"README.md\").exists()","        assert (governance_dir / \"evidence_index.json\").exists()","        ","        # Verify governance.json is valid JSON","        with (governance_dir / \"governance.json\").open(\"r\", encoding=\"utf-8\") as f:","            governance_dict = json.load(f)","        ","        assert \"items\" in governance_dict","        assert \"metadata\" in governance_dict","        assert len(governance_dict[\"items\"]) == 1","        ","        # Verify README.md contains key information","        readme_text = (governance_dir / \"README.md\").read_text(encoding=\"utf-8\")","        assert \"Governance Report\" in readme_text","        assert \"governance_id\" in readme_text","        assert \"Decision Summary\" in readme_text","        assert \"KEEP\" in readme_text","        ","        # Verify evidence_index.json is valid JSON","        with (governance_dir / \"evidence_index.json\").open(\"r\", encoding=\"utf-8\") as f:","            evidence_index = json.load(f)","        ","        assert \"governance_id\" in evidence_index","        assert \"evidence_by_candidate\" in evidence_index","","","def test_governance_json_contains_subsample_fields_in_evidence() -> None:","    \"\"\"","    Test that governance.json contains subsample fields in evidence.","    ","    Critical requirement: subsample info must be in evidence chain.","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        governance_dir = Path(tmpdir) / \"governance\" / \"test-123\"","        ","        # Create report with subsample fields in evidence","        evidence = [","            EvidenceRef(","                run_id=\"stage1-123\",","                stage_name=\"stage1_topk\",","                artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","                key_metrics={","                    \"param_id\": 0,","                    \"net_profit\": 100.0,","                    \"stage_planned_subsample\": 0.1,","                    \"param_subsample_rate\": 0.1,","                    \"params_effective\": 100,","                },","            ),","        ]","        ","        item = GovernanceItem(","            candidate_id=\"donchian_atr:abc123def456\",","            decision=Decision.KEEP,","            reasons=[],","            evidence=evidence,","            created_at=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","            git_sha=\"abc123def456\",","        )","        ","        report = GovernanceReport(","            items=[item],","            metadata={","                \"governance_id\": \"gov-123\",","                \"season\": \"test_season\",","                \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","                \"git_sha\": \"abc123def456\",","                \"decisions\": {\"KEEP\": 1, \"FREEZE\": 0, \"DROP\": 0},","            },","        )","        ","        # Write artifacts","        write_governance_artifacts(governance_dir, report)","        ","        # Verify subsample fields are in governance.json","        with (governance_dir / \"governance.json\").open(\"r\", encoding=\"utf-8\") as f:","            governance_dict = json.load(f)","        ","        item_dict = governance_dict[\"items\"][0]","        evidence_dict = item_dict[\"evidence\"][0]","        key_metrics = evidence_dict[\"key_metrics\"]","        ","        assert \"stage_planned_subsample\" in key_metrics","        assert \"param_subsample_rate\" in key_metrics","        assert \"params_effective\" in key_metrics","","","def test_readme_contains_freeze_reasons() -> None:","    \"\"\"","    Test that README.md contains FREEZE reasons.","    ","    Requirement: README must list FREEZE reasons (concise).","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        governance_dir = Path(tmpdir) / \"governance\" / \"test-123\"","        ","        # Create report with FREEZE item","        evidence = [","            EvidenceRef(","                run_id=\"stage1-123\",","                stage_name=\"stage1_topk\",","                artifact_paths=[\"manifest.json\", \"metrics.json\", \"winners.json\"],","                key_metrics={\"param_id\": 0, \"net_profit\": 100.0},","            ),","        ]","        ","        freeze_item = GovernanceItem(","            candidate_id=\"donchian_atr:abc123def456\",","            decision=Decision.FREEZE,","            reasons=[\"R3: density_5_over_threshold_3\"],","            evidence=evidence,","            created_at=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","            git_sha=\"abc123def456\",","        )","        ","        report = GovernanceReport(","            items=[freeze_item],","            metadata={","                \"governance_id\": \"gov-123\",","                \"season\": \"test_season\",","                \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","                \"git_sha\": \"abc123def456\",","                \"decisions\": {\"KEEP\": 0, \"FREEZE\": 1, \"DROP\": 0},","            },"]}
{"type":"file_chunk","path":"tests/test_governance_writer_contract.py","chunk_index":1,"line_start":201,"line_end":212,"content":["        )","        ","        # Write artifacts","        write_governance_artifacts(governance_dir, report)","        ","        # Verify README contains FREEZE reasons","        readme_text = (governance_dir / \"README.md\").read_text(encoding=\"utf-8\")","        assert \"FREEZE Reasons\" in readme_text","        assert \"donchian_atr:abc123def456\" in readme_text","        assert \"density\" in readme_text","",""]}
{"type":"file_footer","path":"tests/test_governance_writer_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_grid_runner_smoke.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2020,"sha256":"af71811716352a4b8192f080bb2bf7a87f3fd9d70bf5323e248e7571bfe3f21c","total_lines":62,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_grid_runner_smoke.py","chunk_index":0,"line_start":1,"line_end":62,"content":["","import numpy as np","","from pipeline.runner_grid import run_grid","","","def _ohlc():","    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)","    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)","    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)","    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)","    return o, h, l, c","","","def test_grid_runner_smoke_shapes_and_no_crash():","    o, h, l, c = _ohlc()","","    # params: [channel_len, atr_len, stop_mult]","    params = np.array(","        [","            [2, 2, 1.0],","            [3, 2, 1.5],","            [99999, 3, 2.0],  # should produce 0 trades","            [2, 99999, 2.0],  # atr_len > n should be safe (atr_wilder returns all-NaN -> kernel => 0 trades)","        ],","        dtype=np.float64,","    )","","    out = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=True)","    m = out[\"metrics\"]","    order = out[\"order\"]","","    assert isinstance(m, np.ndarray)","    assert m.shape == (params.shape[0], 3)","    assert isinstance(order, np.ndarray)","    assert order.shape == (params.shape[0],)","    assert set(order.tolist()) == set(range(params.shape[0]))","    # Optional stronger assertion: at least one row should have 0 trades due to atr_len > n","    assert np.any(m[:, 1] == 0.0)","","","def test_grid_runner_sorting_toggle():","    o, h, l, c = _ohlc()","    params = np.array(","        [","            [3, 2, 1.5],","            [2, 2, 1.0],","            [2, 3, 2.0],","        ],","        dtype=np.float64,","    )","","    out_sorted = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=True)","    out_unsorted = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=False)","","    assert out_sorted[\"metrics\"].shape == out_unsorted[\"metrics\"].shape == (3, 3)","    assert out_sorted[\"order\"].shape == out_unsorted[\"order\"].shape == (3,)","    # unsorted order should be identity","    np.testing.assert_array_equal(out_unsorted[\"order\"], np.array([0, 1, 2], dtype=np.int64))","","",""]}
{"type":"file_footer","path":"tests/test_grid_runner_smoke.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_indicators_consistency.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2635,"sha256":"376717da36d1b11b1382c096814fe3c5a5745ae7cf549d0eae1f2e9f1dcd0afd","total_lines":102,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_indicators_consistency.py","chunk_index":0,"line_start":1,"line_end":102,"content":["","import numpy as np","","from indicators.numba_indicators import (","    rolling_max,","    rolling_min,","    atr_wilder,",")","","","def _py_rolling_max(arr: np.ndarray, window: int) -> np.ndarray:","    n = arr.shape[0]","    out = np.full(n, np.nan, dtype=np.float64)","    if window <= 0:","        return out","    for i in range(n):","        if i < window - 1:","            continue","        start = i - window + 1","        m = arr[start]","        for j in range(start + 1, i + 1):","            v = arr[j]","            if v > m:","                m = v","        out[i] = m","    return out","","","def _py_rolling_min(arr: np.ndarray, window: int) -> np.ndarray:","    n = arr.shape[0]","    out = np.full(n, np.nan, dtype=np.float64)","    if window <= 0:","        return out","    for i in range(n):","        if i < window - 1:","            continue","        start = i - window + 1","        m = arr[start]","        for j in range(start + 1, i + 1):","            v = arr[j]","            if v < m:","                m = v","        out[i] = m","    return out","","","def _py_atr_wilder(high, low, close, window):","    n = len(high)","    out = np.full(n, np.nan, dtype=np.float64)","    if window > n:","        return out","    tr = np.empty(n, dtype=np.float64)","    tr[0] = high[0] - low[0]","    for i in range(1, n):","        tr[i] = max(","            high[i] - low[i],","            abs(high[i] - close[i - 1]),","            abs(low[i] - close[i - 1]),","        )","    end = window","    out[end - 1] = np.mean(tr[:end])","    for i in range(window, n):","        out[i] = (out[i - 1] * (window - 1) + tr[i]) / window","    return out","","","def test_rolling_max_min_consistency():","    arr = np.array([1.0, 3.0, 2.0, 5.0, 4.0], dtype=np.float64)","    w = 3","","    mx_py = _py_rolling_max(arr, w)","    mn_py = _py_rolling_min(arr, w)","","    mx = rolling_max(arr, w)","    mn = rolling_min(arr, w)","","    np.testing.assert_allclose(mx, mx_py, rtol=0.0, atol=0.0)","    np.testing.assert_allclose(mn, mn_py, rtol=0.0, atol=0.0)","","","def test_atr_wilder_consistency():","    high = np.array([10, 11, 12, 11, 13, 14], dtype=np.float64)","    low = np.array([9, 9, 10, 9, 11, 12], dtype=np.float64)","    close = np.array([9.5, 10.5, 11.0, 10.0, 12.0, 13.0], dtype=np.float64)","    w = 3","","    atr_py = _py_atr_wilder(high, low, close, w)","    atr = atr_wilder(high, low, close, w)","","    np.testing.assert_allclose(atr, atr_py, rtol=0.0, atol=1e-12)","","","def test_atr_wilder_window_gt_n_returns_all_nan():","    high = np.array([10, 11], dtype=np.float64)","    low = np.array([9, 10], dtype=np.float64)","    close = np.array([9.5, 10.5], dtype=np.float64)","    atr = atr_wilder(high, low, close, 999)","    assert atr.shape == (2,)","    assert np.all(np.isnan(atr))","","",""]}
{"type":"file_footer","path":"tests/test_indicators_consistency.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_indicators_precompute_bit_exact.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4726,"sha256":"6907bf65c08848e514581b8ac04083727eba78baf51ac76e06d85a3ac6387438","total_lines":139,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_indicators_precompute_bit_exact.py","chunk_index":0,"line_start":1,"line_end":139,"content":["","\"\"\"","Stage P2-2 Step B: Bit-exact test for precomputed indicators.","","Verifies that using precomputed indicators produces identical results","to computing indicators inline in the kernel.","\"\"\"","from __future__ import annotations","","from dataclasses import asdict, is_dataclass","","import numpy as np","","from engine.types import BarArrays, Fill","from strategy.kernel import DonchianAtrParams, PrecomputedIndicators, run_kernel_arrays","from indicators.numba_indicators import rolling_max, rolling_min, atr_wilder","","","def _fill_to_tuple(f: Fill) -> tuple:","    \"\"\"","    Convert Fill to a comparable tuple representation.","    ","    Uses dataclasses.asdict for dataclass instances, falls back to __dict__ or repr.","    Returns sorted tuple to ensure deterministic comparison.","    \"\"\"","    if is_dataclass(f):","        d = asdict(f)","    else:","        # fallback: __dict__ (for normal classes)","        d = dict(getattr(f, \"__dict__\", {}))","        if not d:","            # last resort: repr","            return (repr(f),)","    # Fixed ordering to avoid dict order differences","    return tuple(sorted(d.items()))","","","def test_indicators_precompute_bit_exact() -> None:","    \"\"\"","    Test that precomputed indicators produce bit-exact results.","    ","    Strategy:","    - Generate random bars","    - Choose a channel_len and atr_len","    - Run kernel twice:","      A: Without precomputation (precomp=None)","      B: With precomputation (precomp=PrecomputedIndicators(...))","    - Compare: donch_hi/lo/atr arrays, metrics, fills, equity","    \"\"\"","    # Generate random bars","    rng = np.random.default_rng(42)","    n_bars = 500","    close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","    open_ = (high + low) / 2","    ","    high = np.maximum(high, np.maximum(open_, close))","    low = np.minimum(low, np.minimum(open_, close))","    ","    bars = BarArrays(","        open=open_.astype(np.float64),","        high=high.astype(np.float64),","        low=low.astype(np.float64),","        close=close.astype(np.float64),","    )","    ","    # Choose test parameters","    ch_len = 20","    atr_len = 10","    params = DonchianAtrParams(channel_len=ch_len, atr_len=atr_len, stop_mult=1.0)","    ","    # Pre-compute indicators (same logic as runner_grid)","    donch_hi_precomp = rolling_max(bars.high, ch_len)","    donch_lo_precomp = rolling_min(bars.low, ch_len)","    atr_precomp = atr_wilder(bars.high, bars.low, bars.close, atr_len)","    ","    precomp = PrecomputedIndicators(","        donch_hi=donch_hi_precomp,","        donch_lo=donch_lo_precomp,","        atr=atr_precomp,","    )","    ","    # Run A: Without precomputation","    result_a = run_kernel_arrays(","        bars=bars,","        params=params,","        commission=0.0,","        slip=0.0,","        order_qty=1,","        precomp=None,","    )","    ","    # Run B: With precomputation","    result_b = run_kernel_arrays(","        bars=bars,","        params=params,","        commission=0.0,","        slip=0.0,","        order_qty=1,","        precomp=precomp,","    )","    ","    # Verify indicators are bit-exact (if we could access them)","    # Note: We can't directly access internal arrays, but we verify outputs","    ","    # Verify metrics are identical","    metrics_a = result_a[\"metrics\"]","    metrics_b = result_b[\"metrics\"]","    assert metrics_a[\"net_profit\"] == metrics_b[\"net_profit\"], \"net_profit must be identical\"","    assert metrics_a[\"trades\"] == metrics_b[\"trades\"], \"trades must be identical\"","    assert metrics_a[\"max_dd\"] == metrics_b[\"max_dd\"], \"max_dd must be identical\"","    ","    # Verify fills are identical","    fills_a = result_a[\"fills\"]","    fills_b = result_b[\"fills\"]","    assert len(fills_a) == len(fills_b), \"fills count must be identical\"","    for i, (fill_a, fill_b) in enumerate(zip(fills_a, fills_b)):","        assert _fill_to_tuple(fill_a) == _fill_to_tuple(fill_b), f\"fill[{i}] must be identical\"","    ","    # Verify equity arrays are bit-exact","    equity_a = result_a[\"equity\"]","    equity_b = result_b[\"equity\"]","    assert equity_a.shape == equity_b.shape, \"equity shape must be identical\"","    np.testing.assert_array_equal(equity_a, equity_b, \"equity must be bit-exact\")","    ","    # Verify pnl arrays are bit-exact","    pnl_a = result_a[\"pnl\"]","    pnl_b = result_b[\"pnl\"]","    assert pnl_a.shape == pnl_b.shape, \"pnl shape must be identical\"","    np.testing.assert_array_equal(pnl_a, pnl_b, \"pnl must be bit-exact\")","    ","    # Verify observability counts are identical","    obs_a = result_a.get(\"_obs\", {})","    obs_b = result_b.get(\"_obs\", {})","    assert obs_a.get(\"intents_total\") == obs_b.get(\"intents_total\"), \"intents_total must be identical\"","    assert obs_a.get(\"fills_total\") == obs_b.get(\"fills_total\"), \"fills_total must be identical\"","",""]}
{"type":"file_footer","path":"tests/test_indicators_precompute_bit_exact.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_intent_idempotency.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9866,"sha256":"87681b498bd36cf277badd32a78e2cf838748d840966b2e8c2c038adc9b3802e","total_lines":330,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_intent_idempotency.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Test idempotency enforcement in ActionQueue for Attack #9.","","Tests that duplicate intents are rejected based on idempotency_key.","\"\"\"","","import pytest","import asyncio","from datetime import date","","from core.intents import (","    CreateJobIntent, CalculateUnitsIntent, DataSpecIntent,","    IntentStatus",")","from control.action_queue import ActionQueue, reset_action_queue","from core.processor import StateProcessor","","","@pytest.fixture","def action_queue():","    \"\"\"Create a fresh ActionQueue for each test.\"\"\"","    reset_action_queue()","    queue = ActionQueue(max_size=10)","    yield queue","    queue.clear()","","","@pytest.fixture","def sample_data_spec():","    \"\"\"Create a sample DataSpecIntent for testing.\"\"\"","    return DataSpecIntent(","        dataset_id=\"test_dataset\",","        symbols=[\"MNQ\", \"MXF\"],","        timeframes=[\"60m\", \"120m\"],","        start_date=date(2020, 1, 1),","        end_date=date(2024, 12, 31)","    )","","","def test_idempotency_basic(action_queue, sample_data_spec):","    \"\"\"Test basic idempotency: duplicate intents are rejected.\"\"\"","    # Create first intent","    intent1 = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10, \"window_slow\": 30}","    )","    ","    # Create second intent with same parameters (should have same idempotency_key)","    intent2 = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10, \"window_slow\": 30}","    )","    ","    # Submit first intent","    intent1_id = action_queue.submit(intent1)","    assert intent1_id == intent1.intent_id","    assert action_queue.get_queue_size() == 1","    ","    # Submit second intent (should be marked as duplicate)","    intent2_id = action_queue.submit(intent2)","    assert intent2_id == intent2.intent_id","    assert action_queue.get_queue_size() == 1  # Queue size shouldn't increase","    ","    # Check that second intent is marked as duplicate","    stored_intent2 = action_queue.get_intent(intent2_id)","    assert stored_intent2 is not None","    assert stored_intent2.status == IntentStatus.DUPLICATE","    ","    # Check metrics","    metrics = action_queue.get_metrics()","    assert metrics[\"submitted\"] == 1","    assert metrics[\"duplicate_rejected\"] == 1","","","def test_idempotency_different_params(action_queue, sample_data_spec):","    \"\"\"Test that intents with different parameters are not duplicates.\"\"\"","    # Create first intent","    intent1 = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10, \"window_slow\": 30}","    )","    ","    # Create second intent with different parameters","    intent2 = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 5, \"window_slow\": 20}  # Different params","    )","    ","    # Both should be accepted","    intent1_id = action_queue.submit(intent1)","    intent2_id = action_queue.submit(intent2)","    ","    assert intent1_id != intent2_id","    assert action_queue.get_queue_size() == 2","    ","    # Check metrics","    metrics = action_queue.get_metrics()","    assert metrics[\"submitted\"] == 2","    assert metrics[\"duplicate_rejected\"] == 0","","","def test_idempotency_calculate_units(action_queue, sample_data_spec):","    \"\"\"Test idempotency for CalculateUnitsIntent.\"\"\"","    # Create first calculation intent","    intent1 = CalculateUnitsIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    # Create duplicate calculation intent","    intent2 = CalculateUnitsIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    # Submit both","    action_queue.submit(intent1)","    action_queue.submit(intent2)","    ","    # Only one should be in queue","    assert action_queue.get_queue_size() == 1","    ","    metrics = action_queue.get_metrics()","    assert metrics[\"duplicate_rejected\"] == 1","","","def test_idempotency_manual_key(action_queue, sample_data_spec):","    \"\"\"Test idempotency with manually set idempotency_key.\"\"\"","    # Create intents with same manual idempotency_key","    intent1 = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10},","        idempotency_key=\"manual_key_123\"","    )","    ","    intent2 = CreateJobIntent(","        season=\"2024Q2\",  # Different season","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10},","        idempotency_key=\"manual_key_123\"  # Same key","    )","    ","    # Second should be duplicate despite different parameters","    action_queue.submit(intent1)","    action_queue.submit(intent2)","    ","    assert action_queue.get_queue_size() == 1","    metrics = action_queue.get_metrics()","    assert metrics[\"duplicate_rejected\"] == 1","","","def test_queue_full_rejection(action_queue, sample_data_spec):","    \"\"\"Test that queue rejects intents when full.\"\"\"","    # Fill the queue","    for i in range(10):  # max_size is 10","        intent = CreateJobIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        action_queue.submit(intent)","    ","    assert action_queue.get_queue_size() == 10","    ","    # Try to submit one more (should fail)","    extra_intent = CreateJobIntent(","        season=\"2024Q99\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 99}","    )","    ","    with pytest.raises(ValueError, match=\"ActionQueue is full\"):","        action_queue.submit(extra_intent)","    "]}
{"type":"file_chunk","path":"tests/test_intent_idempotency.py","chunk_index":1,"line_start":201,"line_end":330,"content":["    metrics = action_queue.get_metrics()","    assert metrics[\"queue_full_rejected\"] == 1","","","def test_intent_retrieval(action_queue, sample_data_spec):","    \"\"\"Test retrieving intents by ID.\"\"\"","    intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    intent_id = action_queue.submit(intent)","    ","    # Retrieve intent","    retrieved = action_queue.get_intent(intent_id)","    assert retrieved is not None","    assert retrieved.intent_id == intent_id","    assert retrieved.season == \"2024Q1\"","    assert retrieved.status == IntentStatus.PENDING","    ","    # Try to retrieve non-existent intent","    assert action_queue.get_intent(\"non_existent_id\") is None","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_wait_for_intent(action_queue, sample_data_spec):","    \"\"\"Test waiting for intent completion.\"\"\"","    intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    intent_id = action_queue.submit(intent)","    ","    # Mark as completed in background","    async def mark_completed():","        await asyncio.sleep(0.1)","        action_queue.mark_completed(intent_id, {\"result\": \"success\"})","    ","    # Wait for completion","    task = asyncio.create_task(mark_completed())","    completed = await action_queue.wait_for_intent_async(intent_id, timeout=1.0)","    ","    await task","    ","    assert completed is not None","    assert completed.status == IntentStatus.COMPLETED","    assert completed.result == {\"result\": \"success\"}","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_wait_for_intent_timeout(action_queue, sample_data_spec):","    \"\"\"Test timeout when waiting for intent.\"\"\"","    intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    intent_id = action_queue.submit(intent)","    ","    # Wait with short timeout (intent won't be completed)","    completed = await action_queue.wait_for_intent_async(intent_id, timeout=0.1)","    ","    assert completed is None  # Should timeout","","","def test_queue_state_debugging(action_queue, sample_data_spec):","    \"\"\"Test queue state debugging method.\"\"\"","    # Add some intents","    for i in range(3):","        intent = CreateJobIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        action_queue.submit(intent)","    ","    # Get queue state","    state = action_queue.get_queue_state()","    ","    assert len(state) == 3","    for i, item in enumerate(state):","        assert \"intent_id\" in item","        assert item[\"type\"] == \"create_job\"","        assert item[\"status\"] == \"pending\"","","","def test_clear_queue(action_queue, sample_data_spec):","    \"\"\"Test clearing the queue.\"\"\"","    # Add some intents","    for i in range(5):","        intent = CreateJobIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        action_queue.submit(intent)","    ","    assert action_queue.get_queue_size() == 5","    ","    # Clear queue","    action_queue.clear()","    ","    assert action_queue.get_queue_size() == 0","    assert action_queue.get_metrics()[\"submitted\"] == 0","    ","    # Should be able to submit new intents after clear","    new_intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    action_queue.submit(new_intent)","    assert action_queue.get_queue_size() == 1"]}
{"type":"file_footer","path":"tests/test_intent_idempotency.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_jobs_db_concurrency_smoke.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1777,"sha256":"2829edcb2d733126083d2c8a57e1746662c2c831e2cf4a6861c30b8f5fcaa656","total_lines":67,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_jobs_db_concurrency_smoke.py","chunk_index":0,"line_start":1,"line_end":67,"content":["","\"\"\"Smoke test for jobs_db concurrency (WAL + retry + state machine).\"\"\"","","from __future__ import annotations","","import multiprocessing as mp","from pathlib import Path","","import pytest","","from control.jobs_db import (","    append_log,","    create_job,","    init_db,","    list_jobs,","    mark_done,","    mark_running,",")","from control.types import DBJobSpec","","","def _proc(db_path: str, n: int) -> None:","    \"\"\"Worker process: create n jobs and complete them.\"\"\"","    p = Path(db_path)","    for i in range(n):","        spec = DBJobSpec(","            season=\"test\",","            dataset_id=\"test\",","            outputs_root=\"outputs\",","            config_snapshot={\"test\": i},","            config_hash=f\"hash{i}\",","        )","        job_id = create_job(p, spec)","        mark_running(p, job_id, pid=1000 + i)","        append_log(p, job_id, f\"hi {i}\")","        mark_done(p, job_id, run_id=f\"R{i}\", report_link=f\"/b5?i={i}\")","","","@pytest.mark.parametrize(\"n\", [50])","def test_jobs_db_concurrency_smoke(tmp_path: Path, n: int) -> None:","    \"\"\"","    Test concurrent job creation and completion across multiple processes.","    ","    This test ensures WAL mode, retry logic, and state machine work correctly","    under concurrent access.","    \"\"\"","    db = tmp_path / \"jobs.db\"","    init_db(db)","","    ps = [mp.Process(target=_proc, args=(str(db), n)) for _ in range(2)]","    for p in ps:","        p.start()","    for p in ps:","        p.join()","","    for p in ps:","        assert p.exitcode == 0, f\"Process {p.pid} exited with code {p.exitcode}\"","","    # Verify job count","    jobs = list_jobs(db, limit=1000)","    assert len(jobs) == 2 * n, f\"Expected {2 * n} jobs, got {len(jobs)}\"","","    # Verify all jobs are DONE","    for job in jobs:","        assert job.status.value == \"DONE\", f\"Job {job.job_id} status is {job.status}, expected DONE\"","",""]}
{"type":"file_footer","path":"tests/test_jobs_db_concurrency_smoke.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_jobs_db_concurrency_wal.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1665,"sha256":"52064c6fa42004d248130567ee6102e760787336d73384b14ca0ed963fa59622","total_lines":58,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_jobs_db_concurrency_wal.py","chunk_index":0,"line_start":1,"line_end":58,"content":["","\"\"\"Tests for jobs_db concurrency with WAL mode.","","Tests concurrent writes from multiple processes to ensure no database locked errors.","\"\"\"","","from __future__ import annotations","","import multiprocessing as mp","from pathlib import Path","","import pytest","","import os","","from control.jobs_db import append_log, create_job, init_db, mark_done, update_running","from control.types import DBJobSpec","","","def _worker(db_path: str, n: int) -> None:","    \"\"\"Worker function: create job, append log, mark done.\"\"\"","    p = Path(db_path)","    pid = os.getpid()","    for i in range(n):","        spec = DBJobSpec(","            season=\"2026Q1\",","            dataset_id=\"test_dataset\",","            outputs_root=\"/tmp/outputs\",","            config_snapshot={\"test\": f\"config_{i}\"},","            config_hash=f\"hash_{i}\",","        )","        job_id = create_job(p, spec, tags=[\"test\", f\"worker_{i}\"])","        append_log(p, job_id, f\"hello {i}\")","        update_running(p, job_id, pid=pid)  # ✅ 對齊狀態機：QUEUED → RUNNING","        mark_done(p, job_id, run_id=f\"R_{i}\", report_link=f\"/b5?x=y&i={i}\")","","","@pytest.mark.parametrize(\"n\", [50])","def test_jobs_db_concurrent_writes(tmp_path: Path, n: int) -> None:","    \"\"\"","    Test concurrent writes from multiple processes.","    ","    Two processes each create n jobs, append logs, and mark done.","    Should not raise database locked errors.","    \"\"\"","    db = tmp_path / \"jobs.db\"","    init_db(db)","","    procs = [mp.Process(target=_worker, args=(str(db), n)) for _ in range(2)]","    for pr in procs:","        pr.start()","    for pr in procs:","        pr.join()","","    for pr in procs:","        assert pr.exitcode == 0, f\"Process {pr.pid} exited with code {pr.exitcode}\"","",""]}
{"type":"file_footer","path":"tests/test_jobs_db_concurrency_wal.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_jobs_db_tags.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5508,"sha256":"10f5adee9f14985177b601943a5a7c02c5371679691c7209c58950167037e40a","total_lines":199,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_jobs_db_tags.py","chunk_index":0,"line_start":1,"line_end":199,"content":["","\"\"\"Tests for jobs_db tags functionality.","","Tests:","1. Create job with tags","2. Read job with tags","3. Old rows without tags fallback to []","4. search_by_tag query helper","\"\"\"","","from __future__ import annotations","","import tempfile","from pathlib import Path","","import pytest","","from control.jobs_db import (","    create_job,","    get_job,","    init_db,","    list_jobs,","    search_by_tag,",")","from control.types import DBJobSpec","","","@pytest.fixture","def temp_db(tmp_path: Path) -> Path:","    \"\"\"Create temporary database for testing.\"\"\"","    db_path = tmp_path / \"test_jobs.db\"","    init_db(db_path)","    return db_path","","","def test_create_job_with_tags(temp_db: Path) -> None:","    \"\"\"Test creating a job with tags.\"\"\"","    spec = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config\"},","        config_hash=\"abc123\",","    )","    ","    job_id = create_job(temp_db, spec, tags=[\"production\", \"high-priority\"])","    ","    # Read back and verify tags","    record = get_job(temp_db, job_id)","    assert record.tags == [\"production\", \"high-priority\"]","","","def test_create_job_without_tags(temp_db: Path) -> None:","    \"\"\"Test creating a job without tags (defaults to empty list).\"\"\"","    spec = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config\"},","        config_hash=\"abc123\",","    )","    ","    job_id = create_job(temp_db, spec)","    ","    # Read back and verify tags is empty list","    record = get_job(temp_db, job_id)","    assert record.tags == []","","","def test_read_job_with_tags(temp_db: Path) -> None:","    \"\"\"Test reading a job with tags.\"\"\"","    spec = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config\"},","        config_hash=\"abc123\",","    )","    ","    job_id = create_job(temp_db, spec, tags=[\"test\", \"debug\"])","    ","    # Read back","    record = get_job(temp_db, job_id)","    assert isinstance(record.tags, list)","    assert \"test\" in record.tags","    assert \"debug\" in record.tags","    assert len(record.tags) == 2","","","def test_old_rows_fallback_to_empty_tags(temp_db: Path) -> None:","    \"\"\"","    Test that old rows without tags_json fallback to empty list.","    ","    This tests backward compatibility: existing jobs without tags_json","    should be readable and have tags=[].","    \"\"\"","    import sqlite3","    import json","    ","    # Manually insert a job without tags_json (simulating old schema)","    conn = sqlite3.connect(str(temp_db))","    try:","        # Insert job with old schema (no tags_json)","        conn.execute(\"\"\"","            INSERT INTO jobs (","                job_id, status, created_at, updated_at,","                season, dataset_id, outputs_root, config_hash,","                config_snapshot_json, requested_pause","            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)","        \"\"\", (","            \"old-job-123\",","            \"QUEUED\",","            \"2026-01-01T00:00:00Z\",","            \"2026-01-01T00:00:00Z\",","            \"2026Q1\",","            \"test_dataset\",","            \"/tmp/outputs\",","            \"abc123\",","            json.dumps({\"test\": \"config\"}),","            0,","        ))","        conn.commit()","    finally:","        conn.close()","    ","    # Read back - should have tags=[]","    record = get_job(temp_db, \"old-job-123\")","    assert record.tags == []","","","def test_search_by_tag(temp_db: Path) -> None:","    \"\"\"Test search_by_tag query helper.\"\"\"","    spec1 = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config1\"},","        config_hash=\"abc123\",","    )","    spec2 = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config2\"},","        config_hash=\"def456\",","    )","    spec3 = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config3\"},","        config_hash=\"ghi789\",","    )","    ","    # Create jobs with different tags","    job1 = create_job(temp_db, spec1, tags=[\"production\", \"high-priority\"])","    job2 = create_job(temp_db, spec2, tags=[\"staging\", \"low-priority\"])","    job3 = create_job(temp_db, spec3, tags=[\"production\", \"medium-priority\"])","    ","    # Search for \"production\" tag","    results = search_by_tag(temp_db, \"production\")","    assert len(results) == 2","    job_ids = {r.job_id for r in results}","    assert job1 in job_ids","    assert job3 in job_ids","    assert job2 not in job_ids","    ","    # Search for \"staging\" tag","    results = search_by_tag(temp_db, \"staging\")","    assert len(results) == 1","    assert results[0].job_id == job2","    ","    # Search for non-existent tag","    results = search_by_tag(temp_db, \"non-existent\")","    assert len(results) == 0","","","def test_list_jobs_includes_tags(temp_db: Path) -> None:","    \"\"\"Test that list_jobs includes tags in records.\"\"\"","    spec = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=\"/tmp/outputs\",","        config_snapshot={\"test\": \"config\"},","        config_hash=\"abc123\",","    )","    ","    job_id = create_job(temp_db, spec, tags=[\"test\", \"debug\"])","    ","    # List jobs","    jobs = list_jobs(temp_db, limit=10)","    assert len(jobs) >= 1","    ","    # Find our job","    our_job = next((j for j in jobs if j.job_id == job_id), None)","    assert our_job is not None","    assert our_job.tags == [\"test\", \"debug\"]","",""]}
{"type":"file_footer","path":"tests/test_jobs_db_tags.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_kbar_anchor_alignment.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3158,"sha256":"fd386f23b19272dca49a1ea91a1514055549d9f0a3821fddadcdb3085c96dcbe","total_lines":86,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_kbar_anchor_alignment.py","chunk_index":0,"line_start":1,"line_end":86,"content":["","\"\"\"Test K-bar aggregation: anchor alignment to Session.start.\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pandas as pd","import pytest","","from data.session.kbar import aggregate_kbar","from data.session.loader import load_session_profile","","","@pytest.fixture","def mnq_profile(profiles_root: Path) -> Path:","    \"\"\"Load CME.MNQ session profile.\"\"\"","    profile_path = profiles_root / \"CME_MNQ_TPE_v1.yaml\"","    return profile_path","","","def test_anchor_to_session_start_60m(mnq_profile: Path) -> None:","    \"\"\"Test 60-minute bars are anchored to session start (08:45:00).\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Create bars starting from session start","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/1/1 08:45:00\",  # Session start","            \"2013/1/1 08:50:00\",","            \"2013/1/1 09:00:00\",","            \"2013/1/1 09:30:00\",","            \"2013/1/1 09:45:00\",  # Should be start of next 60m bucket","            \"2013/1/1 10:00:00\",","        ],","        \"open\": [100.0, 101.0, 102.0, 103.0, 104.0, 105.0],","        \"high\": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5],","        \"low\": [99.5, 100.5, 101.5, 102.5, 103.5, 104.5],","        \"close\": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5],","        \"volume\": [1000, 1100, 1200, 1300, 1400, 1500],","    })","    ","    result = aggregate_kbar(df, 60, profile)","    ","    # Verify first bar is anchored to session start","    first_bar_time = result[\"ts_str\"].iloc[0].split(\" \")[1]","    assert first_bar_time == \"08:45:00\", f\"First bar should be anchored to 08:45:00, got {first_bar_time}\"","    ","    # Verify subsequent bars are at 60-minute intervals from start","    if len(result) > 1:","        second_bar_time = result[\"ts_str\"].iloc[1].split(\" \")[1]","        assert second_bar_time == \"09:45:00\", f\"Second bar should be at 09:45:00, got {second_bar_time}\"","","","def test_anchor_to_session_start_30m(mnq_profile: Path) -> None:","    \"\"\"Test 30-minute bars are anchored to session start (08:45:00).\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Create bars starting from session start","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/1/1 08:45:00\",  # Session start","            \"2013/1/1 08:50:00\",","            \"2013/1/1 09:00:00\",","            \"2013/1/1 09:15:00\",  # Should be start of next 30m bucket","            \"2013/1/1 09:30:00\",","        ],","        \"open\": [100.0, 101.0, 102.0, 103.0, 104.0],","        \"high\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"low\": [99.5, 100.5, 101.5, 102.5, 103.5],","        \"close\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"volume\": [1000, 1100, 1200, 1300, 1400],","    })","    ","    result = aggregate_kbar(df, 30, profile)","    ","    # Verify first bar is anchored to session start","    first_bar_time = result[\"ts_str\"].iloc[0].split(\" \")[1]","    assert first_bar_time == \"08:45:00\", f\"First bar should be anchored to 08:45:00, got {first_bar_time}\"","    ","    # Verify subsequent bars are at 30-minute intervals from start","    if len(result) > 1:","        second_bar_time = result[\"ts_str\"].iloc[1].split(\" \")[1]","        assert second_bar_time == \"09:15:00\", f\"Second bar should be at 09:15:00, got {second_bar_time}\"","",""]}
{"type":"file_footer","path":"tests/test_kbar_anchor_alignment.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_kbar_no_cross_session.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3116,"sha256":"89016a7006feeb798b281c7b3a40ed87b3eb0152c1c06b2bf13ea0095c7b1e07","total_lines":88,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_kbar_no_cross_session.py","chunk_index":0,"line_start":1,"line_end":88,"content":["","\"\"\"Test K-bar aggregation: no cross-session aggregation.\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pandas as pd","import pytest","","from data.session.kbar import aggregate_kbar","from data.session.loader import load_session_profile","","","@pytest.fixture","def mnq_profile(profiles_root: Path) -> Path:","    \"\"\"Load CME.MNQ session profile.\"\"\"","    profile_path = profiles_root / \"CME_MNQ_TPE_v1.yaml\"","    return profile_path","","","def test_no_cross_session_60m(mnq_profile: Path) -> None:","    \"\"\"Test 60-minute bars do not cross session boundaries.\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Create bars that span DAY session end and NIGHT session start","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/1/1 13:30:00\",  # DAY session","            \"2013/1/1 13:40:00\",  # DAY session","            \"2013/1/1 13:44:00\",  # DAY session (last bar before end)","            \"2013/1/1 21:00:00\",  # NIGHT session start","            \"2013/1/1 21:10:00\",  # NIGHT session","        ],","        \"open\": [100.0, 101.0, 102.0, 103.0, 104.0],","        \"high\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"low\": [99.5, 100.5, 101.5, 102.5, 103.5],","        \"close\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"volume\": [1000, 1100, 1200, 1300, 1400],","    })","    ","    result = aggregate_kbar(df, 60, profile)","    ","    # Verify no bar contains both DAY and NIGHT session bars","    # Use session column instead of string contains (more robust)","    assert \"session\" in result.columns, \"Result must include session column\"","    ","    # Must have both DAY and NIGHT sessions","    assert set(result[\"session\"].dropna()) == {\"DAY\", \"NIGHT\"}, (","        f\"Should have both DAY and NIGHT sessions, got {set(result['session'].dropna())}\"","    )","    ","    day_bars = result[result[\"session\"] == \"DAY\"]","    night_bars = result[result[\"session\"] == \"NIGHT\"]","    ","    assert len(day_bars) > 0, \"Should have DAY session bars\"","    assert len(night_bars) > 0, \"Should have NIGHT session bars\"","    ","    # Verify no bar mixes sessions (each row has exactly one session)","    assert result[\"session\"].notna().all(), \"All bars must have a session label\"","    assert len(result[result[\"session\"].isna()]) == 0, \"No bar should have session=None\"","","","def test_no_cross_session_30m(mnq_profile: Path) -> None:","    \"\"\"Test 30-minute bars do not cross session boundaries.\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Create bars at DAY session end","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/1/1 13:30:00\",","            \"2013/1/1 13:40:00\",","            \"2013/1/1 13:44:00\",  # Last bar in DAY session","        ],","        \"open\": [100.0, 101.0, 102.0],","        \"high\": [100.5, 101.5, 102.5],","        \"low\": [99.5, 100.5, 101.5],","        \"close\": [100.5, 101.5, 102.5],","        \"volume\": [1000, 1100, 1200],","    })","    ","    result = aggregate_kbar(df, 30, profile)","    ","    # All bars should be in DAY session","    assert \"session\" in result.columns, \"Result must include session column\"","    assert all(result[\"session\"] == \"DAY\"), f\"All bars should be DAY session, got {result['session'].unique()}\"","",""]}
{"type":"file_footer","path":"tests/test_kbar_no_cross_session.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_kernel_parity_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":17243,"sha256":"b767856f5b5f8768cc1ad1713cce3759c68c51ccd2e7392be848a22b23c022c3","total_lines":413,"chunk_count":3}
{"type":"file_chunk","path":"tests/test_kernel_parity_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Kernel parity contract tests - Phase 4 Stage C.","","These tests ensure that Cursor kernel results are bit-level identical to matcher_core.","This is a critical contract: any deviation indicates a semantic bug.","","Tests use simulate_run() unified entry point to ensure we test the actual API used in production.","\"\"\"","","import numpy as np","","from data.layout import normalize_bars","from engine.simulate import simulate_run","from engine.types import OrderIntent, OrderKind, OrderRole, Side","","","def _bars1(o, h, l, c):","    \"\"\"Helper to create single-bar BarArrays.\"\"\"","    return normalize_bars(","        np.array([o], dtype=np.float64),","        np.array([h], dtype=np.float64),","        np.array([l], dtype=np.float64),","        np.array([c], dtype=np.float64),","    )","","","def _bars2(o0, h0, l0, c0, o1, h1, l1, c1):","    \"\"\"Helper to create two-bar BarArrays.\"\"\"","    return normalize_bars(","        np.array([o0, o1], dtype=np.float64),","        np.array([h0, h1], dtype=np.float64),","        np.array([l0, l1], dtype=np.float64),","        np.array([c0, c1], dtype=np.float64),","    )","","","def _bars3(o0, h0, l0, c0, o1, h1, l1, c1, o2, h2, l2, c2):","    \"\"\"Helper to create three-bar BarArrays.\"\"\"","    return normalize_bars(","        np.array([o0, o1, o2], dtype=np.float64),","        np.array([h0, h1, h2], dtype=np.float64),","        np.array([l0, l1, l2], dtype=np.float64),","        np.array([c0, c1, c2], dtype=np.float64),","    )","","","def _compute_position_path(fills):","    \"\"\"","    Compute position path from fills sequence.","    ","    Returns list of (bar_index, position) tuples where position is:","    - 0: flat","    - 1: long","    - -1: short","    \"\"\"","    pos_path = []","    current_pos = 0","    ","    # Group fills by bar_index","    fills_by_bar = {}","    for fill in fills:","        bar_idx = fill.bar_index","        if bar_idx not in fills_by_bar:","            fills_by_bar[bar_idx] = []","        fills_by_bar[bar_idx].append(fill)","    ","    # Process fills chronologically","    for bar_idx in sorted(fills_by_bar.keys()):","        bar_fills = fills_by_bar[bar_idx]","        # Sort by role (ENTRY first), then kind, then order_id","        bar_fills.sort(key=lambda f: (","            0 if f.role == OrderRole.ENTRY else 1,","            0 if f.kind == OrderKind.STOP else 1,","            f.order_id","        ))","        ","        for fill in bar_fills:","            if fill.role == OrderRole.ENTRY:","                if fill.side == Side.BUY:","                    current_pos = 1","                else:","                    current_pos = -1","            elif fill.role == OrderRole.EXIT:","                current_pos = 0","        ","        pos_path.append((bar_idx, current_pos))","    ","    return pos_path","","","def _assert_fills_identical(cursor_fills, reference_fills):","    \"\"\"Assert that two fill sequences are bit-level identical.\"\"\"","    assert len(cursor_fills) == len(reference_fills), (","        f\"Fill count mismatch: cursor={len(cursor_fills)}, reference={len(reference_fills)}\"","    )","    ","    for i, (c_fill, r_fill) in enumerate(zip(cursor_fills, reference_fills)):","        assert c_fill.bar_index == r_fill.bar_index, (","            f\"Fill {i}: bar_index mismatch: cursor={c_fill.bar_index}, reference={r_fill.bar_index}\"","        )","        assert c_fill.role == r_fill.role, (","            f\"Fill {i}: role mismatch: cursor={c_fill.role}, reference={r_fill.role}\"","        )","        assert c_fill.kind == r_fill.kind, (","            f\"Fill {i}: kind mismatch: cursor={c_fill.kind}, reference={r_fill.kind}\"","        )","        assert c_fill.side == r_fill.side, (","            f\"Fill {i}: side mismatch: cursor={c_fill.side}, reference={r_fill.side}\"","        )","        assert c_fill.price == r_fill.price, (","            f\"Fill {i}: price mismatch: cursor={c_fill.price}, reference={r_fill.price}\"","        )","        assert c_fill.qty == r_fill.qty, (","            f\"Fill {i}: qty mismatch: cursor={c_fill.qty}, reference={r_fill.qty}\"","        )","        assert c_fill.order_id == r_fill.order_id, (","            f\"Fill {i}: order_id mismatch: cursor={c_fill.order_id}, reference={r_fill.order_id}\"","        )","","","def _assert_position_path_identical(cursor_fills, reference_fills):","    \"\"\"Assert that position paths are identical.\"\"\"","    cursor_path = _compute_position_path(cursor_fills)","    reference_path = _compute_position_path(reference_fills)","    ","    assert cursor_path == reference_path, (","        f\"Position path mismatch:\\n\"","        f\"  cursor: {cursor_path}\\n\"","        f\"  reference: {reference_path}\"","    )","","","def test_parity_next_bar_activation():","    \"\"\"Test next-bar activation rule: order created at bar N activates at bar N+1.\"\"\"","    # Order created at bar 0, should activate at bar 1","    bars = _bars2(","        100, 105, 95, 100,  # bar 0: high=105 would hit stop 102, but order not active yet","        100, 105, 95, 100,  # bar 1: order activates, should fill","    )","    intents = [","        OrderIntent(order_id=1, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    _assert_position_path_identical(cursor_result.fills, reference_result.fills)","    ","    # Verify: should fill at bar 1, not bar 0","    assert len(cursor_result.fills) == 1","    assert cursor_result.fills[0].bar_index == 1","","","def test_parity_stop_fill_price_exact():","    \"\"\"Test stop fill price = stop_price (not max(open, stop_price)).\"\"\"","    # Buy stop at 100, open=95, high=105 -> should fill at 100 (stop_price), not 105","    bars = _bars1(95, 105, 90, 100)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].price == 100.0  # stop_price, not high","","","def test_parity_stop_fill_price_gap_up():","    \"\"\"Test stop fill price on gap up: fill at open if open >= stop_price.\"\"\"","    # Buy stop at 100, open=105 (gap up) -> should fill at 105 (open), not 100","    bars = _bars1(105, 110, 105, 108)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].price == 105.0  # open (gap branch)","","","def test_parity_stop_fill_price_gap_down():","    \"\"\"Test stop fill price on gap down: fill at open if open <= stop_price.\"\"\"","    # Sell stop at 100, open=90 (gap down) -> should fill at 90 (open), not 100","    bars = _bars2(","        100, 100, 100, 100,  # bar 0: enter long","        90, 95, 80, 85,      # bar 1: exit stop gap down","    )","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point"]}
{"type":"file_chunk","path":"tests/test_kernel_parity_contract.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    _assert_position_path_identical(cursor_result.fills, reference_result.fills)","    # Exit fill should be at open (90) due to gap down","    assert cursor_result.fills[1].price == 90.0","","","def test_parity_same_bar_entry_then_exit():","    \"\"\"Test same-bar entry then exit is allowed.\"\"\"","    # Same bar: entry buy stop 105, exit sell stop 95","    # Bar: O=100 H=120 L=90","    # Entry: Buy Stop 105 -> fills at 105","    # Exit: Sell Stop 95 -> fills at 95 (after entry)","    bars = _bars1(100, 120, 90, 110)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),","        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    _assert_position_path_identical(cursor_result.fills, reference_result.fills)","    ","    assert len(cursor_result.fills) == 2","    assert cursor_result.fills[0].role == OrderRole.ENTRY","    assert cursor_result.fills[0].price == 105.0","    assert cursor_result.fills[1].role == OrderRole.EXIT","    assert cursor_result.fills[1].price == 95.0","    assert cursor_result.fills[0].bar_index == cursor_result.fills[1].bar_index","","","def test_parity_stop_priority_over_limit():","    \"\"\"Test STOP priority over LIMIT (same role, same bar).\"\"\"","    # Entry: Buy Stop 102 and Buy Limit 110 both triggerable","    # STOP must win","    bars = _bars1(100, 115, 95, 105)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),","        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.BUY, price=110.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].kind == OrderKind.STOP","    assert cursor_result.fills[0].order_id == 1","","","def test_parity_stop_priority_exit():","    \"\"\"Test STOP priority over LIMIT on exit.\"\"\"","    # Enter long first, then exit with both stop and limit triggerable","    # STOP must win","    bars = _bars2(","        100, 100, 100, 100,  # bar 0: enter long","        100, 110, 80, 90,    # bar 1: exit stop 90 and exit limit 110 both triggerable","    )","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=90.0),","        OrderIntent(order_id=3, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.LIMIT, side=Side.SELL, price=110.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    _assert_position_path_identical(cursor_result.fills, reference_result.fills)","    ","    # Exit fill should be STOP","    assert cursor_result.fills[1].kind == OrderKind.STOP","    assert cursor_result.fills[1].order_id == 2","","","def test_parity_order_id_tie_break():","    \"\"\"Test order_id tie-break when kind is same.\"\"\"","    # Two STOP orders, lower order_id should win","    bars = _bars1(100, 110, 95, 105)","    intents = [","        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].order_id == 1  # Lower order_id wins","","","def test_parity_limit_gap_down_better_fill():","    \"\"\"Test limit order gap down: fill at open if better.\"\"\"","    # Buy limit at 100, open=90 (gap down) -> should fill at 90 (open), not 100","    bars = _bars1(90, 95, 85, 92)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.BUY, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].price == 90.0  # open (better fill)","","","def test_parity_limit_gap_up_better_fill():","    \"\"\"Test limit order gap up: fill at open if better.\"\"\"","    # Sell limit at 100, open=105 (gap up) -> should fill at 105 (open), not 100","    bars = _bars1(105, 110, 100, 108)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.SELL, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].price == 105.0  # open (better fill)","","","def test_parity_no_fill_when_not_touched():","    \"\"\"Test no fill when price not touched.\"\"\"","    bars = _bars1(90, 95, 90, 92)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert len(cursor_result.fills) == 0","","","def test_parity_open_equals_stop_gap_branch():","    \"\"\"Test open equals stop price: gap branch but same price.\"\"\"","    bars = _bars1(100, 100, 90, 95)","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    assert cursor_result.fills[0].price == 100.0  # open == stop_price","","","def test_parity_multiple_bars_complex():","    \"\"\"Test complex multi-bar scenario with entry and exit.\"\"\"","    bars = _bars3(","        100, 105, 95, 100,   # bar 0: enter long at 102 (buy stop)","        100, 110, 80, 90,    # bar 1: exit stop 90 triggers","        95, 100, 90, 95,     # bar 2: no fills","    )","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),","        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=90.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)","    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    _assert_position_path_identical(cursor_result.fills, reference_result.fills)","    ","    # Verify position path","    pos_path = _compute_position_path(cursor_result.fills)","    assert pos_path == [(0, 1), (1, 0)]  # Enter at bar 0, exit at bar 1","","","def test_parity_entry_skipped_when_position_exists():","    \"\"\"Test that entry is skipped when position already exists.\"\"\"","    # Enter long at bar 0, then at bar 1 try to enter again (should be skipped) and exit","    bars = _bars2(","        100, 100, 100, 100,  # bar 0: enter long","        100, 110, 90, 100,   # bar 1: exit stop 95 triggers, entry stop 105 also triggerable but skipped","    )","    intents = [","        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),","        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),","        OrderIntent(order_id=3, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),","    ]","    ","    # Use unified simulate_run() entry point","    cursor_result = simulate_run(bars, intents, use_reference=False)","    reference_result = simulate_run(bars, intents, use_reference=True)"]}
{"type":"file_chunk","path":"tests/test_kernel_parity_contract.py","chunk_index":2,"line_start":401,"line_end":413,"content":["    ","    _assert_fills_identical(cursor_result.fills, reference_result.fills)","    _assert_position_path_identical(cursor_result.fills, reference_result.fills)","    ","    # Should have entry at bar 0 and exit at bar 1","    # Entry at bar 1 should be skipped (position already exists)","    assert len(cursor_result.fills) == 2","    assert cursor_result.fills[0].bar_index == 0","    assert cursor_result.fills[0].role == OrderRole.ENTRY","    assert cursor_result.fills[1].bar_index == 1","    assert cursor_result.fills[1].role == OrderRole.EXIT","",""]}
{"type":"file_footer","path":"tests/test_kernel_parity_contract.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"tests/test_kpi_drilldown_no_raise.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7910,"sha256":"3b7ddf9b73df30360fa1844525b2a52a245a6a5a9072b38fb8ce95b1a4630522","total_lines":240,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_kpi_drilldown_no_raise.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Tests for KPI drill-down - no raise contract.","","Tests missing artifacts, wrong pointers, empty session_state.","UI functions should never raise exceptions.","","Zero-side-effect imports: All I/O and stateful operations are inside test functions.","","NOTE: This test is skipped because streamlit has been removed from the project.","\"\"\"","","from __future__ import annotations","","import pytest","","pytest.skip(\"Streamlit tests skipped - streamlit removed from project\", allow_module_level=True)","","# Original test code below is not executed","","","def test_kpi_table_missing_name() -> None:","    \"\"\"Test KPI table handles missing name field.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.kpi_table import render_kpi_table","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.columns\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.text\"), \\","         patch(\"streamlit.button\"):","        ","        # Row without name","        kpi_rows = [","            {\"value\": 100}","        ]","        ","        # Should not raise","        render_kpi_table(kpi_rows)","","","def test_kpi_table_missing_value() -> None:","    \"\"\"Test KPI table handles missing value field.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.kpi_table import render_kpi_table","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.columns\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.text\"), \\","         patch(\"streamlit.button\"):","        ","        # Row without value","        kpi_rows = [","            {\"name\": \"net_profit\"}","        ]","        ","        # Should not raise","        render_kpi_table(kpi_rows)","","","def test_kpi_table_empty_rows() -> None:","    \"\"\"Test KPI table handles empty rows list.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.kpi_table import render_kpi_table","    ","    with patch(\"streamlit.info\"):","        # Empty list","        render_kpi_table([])","        ","        # Should not raise","","","def test_kpi_table_unknown_kpi() -> None:","    \"\"\"Test KPI table handles unknown KPI (not in registry).\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.kpi_table import render_kpi_table","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.columns\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.text\"), \\","         patch(\"streamlit.button\"):","        ","        # KPI not in registry","        kpi_rows = [","            {\"name\": \"unknown_kpi\", \"value\": 100}","        ]","        ","        # Should not raise - displays but not clickable","        render_kpi_table(kpi_rows)","","","def test_evidence_panel_missing_artifact() -> None:","    \"\"\"Test evidence panel handles missing artifact.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.evidence_panel import render_evidence_panel","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.warning\"), \\","         patch(\"streamlit.caption\"):","        ","        # Mock session state with missing artifact","        with patch.dict(st.session_state, {","            \"active_evidence\": {","                \"kpi_name\": \"net_profit\",","                \"artifact\": \"winners_v2\",","                \"json_pointer\": \"/summary/net_profit\",","            }","        }):","            # Artifacts dict missing winners_v2","            artifacts = {","                \"manifest\": {},","            }","            ","            # Should not raise - shows warning","            render_evidence_panel(artifacts)","","","def test_evidence_panel_wrong_pointer() -> None:","    \"\"\"Test evidence panel handles wrong JSON pointer.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.evidence_panel import render_evidence_panel","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.warning\"), \\","         patch(\"streamlit.info\"), \\","         patch(\"streamlit.caption\"):","        ","        # Mock session state","        with patch.dict(st.session_state, {","            \"active_evidence\": {","                \"kpi_name\": \"net_profit\",","                \"artifact\": \"winners_v2\",","                \"json_pointer\": \"/nonexistent/pointer\",","            }","        }):","            # Artifact exists but pointer is wrong","            artifacts = {","                \"winners_v2\": {","                    \"summary\": {","                        \"net_profit\": 100","                    }","                }","            }","            ","            # Should not raise - shows warning","            render_evidence_panel(artifacts)","","","def test_evidence_panel_empty_session_state() -> None:","    \"\"\"Test evidence panel handles empty session_state.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.evidence_panel import render_evidence_panel","    ","    with patch(\"streamlit.subheader\"):","        # Empty session state","        with patch.dict(st.session_state, {}, clear=True):","            artifacts = {","                \"winners_v2\": {}","            }","            ","            # Should not raise - returns early","            render_evidence_panel(artifacts)","","","def test_evidence_panel_invalid_session_state() -> None:","    \"\"\"Test evidence panel handles invalid session_state structure.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.evidence_panel import render_evidence_panel","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.warning\"):","        ","        # Invalid session state structure","        with patch.dict(st.session_state, {","            \"active_evidence\": \"not_a_dict\"","        }):","            artifacts = {}","            ","            # Should not raise - handles gracefully","            render_evidence_panel(artifacts)","","","def test_evidence_panel_missing_fields() -> None:","    \"\"\"Test evidence panel handles missing fields in session_state.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.evidence_panel import render_evidence_panel","    ","    with patch(\"streamlit.subheader\"), \\","         patch(\"streamlit.markdown\"), \\","         patch(\"streamlit.warning\"):","        ","        # Missing fields in active_evidence","        with patch.dict(st.session_state, {","            \"active_evidence\": {","                \"kpi_name\": \"net_profit\",","                # Missing artifact, json_pointer"]}
{"type":"file_chunk","path":"tests/test_kpi_drilldown_no_raise.py","chunk_index":1,"line_start":201,"line_end":240,"content":["            }","        }):","            artifacts = {}","            ","            # Should not raise - handles gracefully","            render_evidence_panel(artifacts)","","","def test_kpi_table_exception_handling() -> None:","    \"\"\"Test KPI table handles exceptions gracefully.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.kpi_table import render_kpi_table","    ","    # Mock streamlit to raise exception","    with patch(\"streamlit.subheader\", side_effect=Exception(\"Streamlit error\")):","        kpi_rows = [","            {\"name\": \"net_profit\", \"value\": 100}","        ]","        ","        # Should catch exception and show error","        with patch(\"streamlit.error\"):","            render_kpi_table(kpi_rows)","            # Should not raise","","","def test_evidence_panel_exception_handling() -> None:","    \"\"\"Test evidence panel handles exceptions gracefully.\"\"\"","    # Import inside test function to prevent collection errors","    from gui.viewer.components.evidence_panel import render_evidence_panel","    ","    # Mock streamlit to raise exception","    with patch(\"streamlit.subheader\", side_effect=Exception(\"Streamlit error\")):","        artifacts = {}","        ","        # Should catch exception and show error","        with patch(\"streamlit.error\"):","            render_evidence_panel(artifacts)","            # Should not raise","",""]}
{"type":"file_footer","path":"tests/test_kpi_drilldown_no_raise.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_local_scan_policy.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6885,"sha256":"31dda43f16b0a1898a747ec60caa2165fdea656952e2f002e33a20c92a3ff8e4","total_lines":190,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_local_scan_policy.py","chunk_index":0,"line_start":1,"line_end":190,"content":["#!/usr/bin/env python3","\"\"\"","Test Local-Strict scanner policy and file inclusion logic.","","Contract:","- Build a temp repo-like structure","- src/a.py included","- tests/t.py included","- .venv/x.py excluded","- outputs/jobs.db excluded","- outputs/snapshots/full/REPO_TREE.txt included","- node_modules/pkg/index.js excluded","- root Makefile included","","Assert iter_repo_files_local_strict returns exactly expected list, sorted.","\"\"\"","","import tempfile","import shutil","from pathlib import Path","import pytest","","from control.local_scan import (","    LocalScanPolicy,","    default_local_strict_policy,","    iter_repo_files_local_strict,","    should_include_file,",")","","","def test_default_policy():","    \"\"\"Test that default policy matches spec.\"\"\"","    policy = default_local_strict_policy()","    ","    assert policy.allowed_roots == (\"src\", \"tests\", \"scripts\", \"docs\")","    assert \"Makefile\" in policy.allowed_root_files_glob","    assert \"pyproject.toml\" in policy.allowed_root_files_glob","    assert \".git\" in policy.deny_segments","    assert \".venv\" in policy.deny_segments","    assert \"node_modules\" in policy.deny_segments","    assert \"outputs\" in policy.deny_segments","    assert policy.outputs_allow == (\"outputs/snapshots\",)","    assert policy.max_files == 20000","    assert policy.max_bytes == 2000000","    assert policy.gitignore_respected is False","","","def test_should_include_file():","    \"\"\"Test individual file inclusion decisions.\"\"\"","    policy = default_local_strict_policy()","    ","    # Root files","    assert should_include_file(Path(\"Makefile\"), policy) is True","    assert should_include_file(Path(\"pyproject.toml\"), policy) is True","    assert should_include_file(Path(\"README.md\"), policy) is True","    assert should_include_file(Path(\"random.txt\"), policy) is False  # not in glob","    ","    # Allowed roots","    assert should_include_file(Path(\"src/a.py\"), policy) is True","    assert should_include_file(Path(\"tests/test.py\"), policy) is True","    assert should_include_file(Path(\"scripts/run.py\"), policy) is True","    assert should_include_file(Path(\"docs/index.md\"), policy) is True","    ","    # Denied segments anywhere in path","    assert should_include_file(Path(\"src/.venv/foo.py\"), policy) is False","    assert should_include_file(Path(\"tests/.git/config\"), policy) is False","    assert should_include_file(Path(\"scripts/node_modules/pkg/index.js\"), policy) is False","    assert should_include_file(Path(\"docs/__pycache__/module.cpython-310.pyc\"), policy) is False","    ","    # Outputs exception","    assert should_include_file(Path(\"outputs/jobs.db\"), policy) is False","    assert should_include_file(Path(\"outputs/snapshots/full/REPO_TREE.txt\"), policy) is True","    assert should_include_file(Path(\"outputs/snapshots/full/LOCAL_SCAN_RULES.json\"), policy) is True","    assert should_include_file(Path(\"outputs/snapshots/\"), policy) is True  # exact match","    assert should_include_file(Path(\"outputs/snapshots\"), policy) is True  # exact match","    ","    # Other directories not allowed","    assert should_include_file(Path(\"configs/something.yaml\"), policy) is False","    assert should_include_file(Path(\"data/raw.csv\"), policy) is False","","","def test_iter_repo_files_local_strict_integration(tmp_path: Path):","    \"\"\"Build a temp repo structure and verify scanning.\"\"\"","    repo_root = tmp_path","    ","    # Create expected included files","    (repo_root / \"src\").mkdir()","    (repo_root / \"src\" / \"a.py\").write_text(\"# included\")","    (repo_root / \"src\" / \"subdir\").mkdir()","    (repo_root / \"src\" / \"subdir\" / \"b.py\").write_text(\"# included\")","    ","    (repo_root / \"tests\").mkdir()","    (repo_root / \"tests\" / \"t.py\").write_text(\"# included\")","    ","    (repo_root / \"scripts\").mkdir()","    (repo_root / \"scripts\" / \"run.py\").write_text(\"# included\")","    ","    (repo_root / \"docs\").mkdir()","    (repo_root / \"docs\" / \"index.md\").write_text(\"# included\")","    ","    # Create outputs exception","    (repo_root / \"outputs\").mkdir()","    (repo_root / \"outputs\" / \"jobs.db\").write_text(\"binary\")  # should be excluded","    (repo_root / \"outputs\" / \"snapshots\").mkdir()","    (repo_root / \"outputs\" / \"snapshots\" / \"full\").mkdir()","    (repo_root / \"outputs\" / \"snapshots\" / \"full\" / \"REPO_TREE.txt\").write_text(\"# included\")","    ","    # Create excluded directories","    (repo_root / \".venv\").mkdir()","    (repo_root / \".venv\" / \"x.py\").write_text(\"# excluded\")","    ","    (repo_root / \"node_modules\").mkdir()","    (repo_root / \"node_modules\" / \"pkg\").mkdir()","    (repo_root / \"node_modules\" / \"pkg\" / \"index.js\").write_text(\"// excluded\")","    ","    (repo_root / \"__pycache__\").mkdir()","    (repo_root / \"__pycache__\" / \"module.cpython-310.pyc\").write_bytes(b\"\\x00\\x01\")","    ","    # Root files","    (repo_root / \"Makefile\").write_text(\"# included\")","    (repo_root / \"pyproject.toml\").write_text(\"# included\")","    (repo_root / \"README.md\").write_text(\"# included\")","    (repo_root / \"random.txt\").write_text(\"# excluded - not in glob\")","    ","    # Create a file in disallowed root (configs)","    (repo_root / \"configs\").mkdir()","    (repo_root / \"configs\" / \"profile.yaml\").write_text(\"# excluded\")","    ","    policy = default_local_strict_policy()","    files = iter_repo_files_local_strict(repo_root, policy)","    ","    # files are already relative to repo_root","    rel_files_str = sorted(str(p) for p in files)","    ","    expected = [","        \"Makefile\",","        \"README.md\",","        \"pyproject.toml\",","        \"docs/index.md\",","        \"outputs/snapshots/full/REPO_TREE.txt\",","        \"scripts/run.py\",","        \"src/a.py\",","        \"src/subdir/b.py\",","        \"tests/t.py\",","    ]","    ","    assert rel_files_str == sorted(expected), f\"Got {rel_files_str}, expected {sorted(expected)}\"","    ","    # Verify deterministic ordering","    files2 = iter_repo_files_local_strict(repo_root, policy)","    assert list(files) == list(files2), \"Should be deterministic\"","","","def test_max_files_limit(tmp_path: Path):","    \"\"\"Test max_files limit is respected.\"\"\"","    repo_root = tmp_path","    (repo_root / \"src\").mkdir()","    ","    # Create many files","    for i in range(100):","        (repo_root / \"src\" / f\"file{i}.py\").write_text(\"# content\")","    ","    policy = LocalScanPolicy(","        allowed_roots=(\"src\",),","        allowed_root_files_glob=(),","        deny_segments=(),","        outputs_allow=(),","        max_files=10,  # Low limit","        max_bytes=1000000,","        gitignore_respected=False,","    )","    ","    files = iter_repo_files_local_strict(repo_root, policy)","    assert len(files) == 10, f\"Should be limited to 10 files, got {len(files)}\"","    ","    # Should be sorted deterministically","    file_names = [f.name for f in files]","    assert file_names == sorted(file_names), \"Files should be sorted\"","","","def test_policy_immutability():","    \"\"\"Test that policy dataclass is frozen.\"\"\"","    policy = default_local_strict_policy()","    ","    with pytest.raises(Exception):","        policy.allowed_roots = (\"something\",)  # Should raise because frozen","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_local_scan_policy.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_log_tail_reads_last_n_lines.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1820,"sha256":"e30c41f8faf9bb81714345099d2c150186d29e8bef8306006353ed9a583fe8a2","total_lines":65,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_log_tail_reads_last_n_lines.py","chunk_index":0,"line_start":1,"line_end":65,"content":["","\"\"\"Test that read_tail reads last n lines efficiently without loading entire file.\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from control.api import read_tail","","","def test_read_tail_returns_last_n_lines(tmp_path: Path) -> None:","    \"\"\"Test that read_tail returns exactly the last n lines.\"\"\"","    p = tmp_path / \"big.log\"","    lines = [f\"line {i}\\n\" for i in range(5000)]","    p.write_text(\"\".join(lines), encoding=\"utf-8\")","","    lines_list, truncated = read_tail(p, n=200)","    out = \"\".join(lines_list)","    out_lines = out.splitlines()","","    assert len(out_lines) == 200","    assert out_lines[0] == \"line 4800\"","    assert out_lines[-1] == \"line 4999\"","    assert truncated is True","","","def test_read_tail_handles_small_file(tmp_path: Path) -> None:","    \"\"\"Test that read_tail handles files with fewer lines than requested.\"\"\"","    p = tmp_path / \"small.log\"","    lines = [f\"line {i}\\n\" for i in range(50)]","    p.write_text(\"\".join(lines), encoding=\"utf-8\")","","    lines_list, truncated = read_tail(p, n=200)","    out = \"\".join(lines_list)","    out_lines = out.splitlines()","","    assert len(out_lines) == 50","    assert out_lines[0] == \"line 0\"","    assert out_lines[-1] == \"line 49\"","    assert truncated is False","","","def test_read_tail_handles_empty_file(tmp_path: Path) -> None:","    \"\"\"Test that read_tail handles empty files.\"\"\"","    p = tmp_path / \"empty.log\"","    p.touch()","","    lines_list, truncated = read_tail(p, n=200)","    out = \"\".join(lines_list)","    assert out == \"\"","    assert truncated is False","","","def test_read_tail_handles_missing_file(tmp_path: Path) -> None:","    \"\"\"Test that read_tail handles missing files gracefully.\"\"\"","    p = tmp_path / \"missing.log\"","","    lines_list, truncated = read_tail(p, n=200)","    out = \"\".join(lines_list)","    assert out == \"\"","    assert truncated is False","",""]}
{"type":"file_footer","path":"tests/test_log_tail_reads_last_n_lines.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_mnq_maintenance_break_no_cross.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4644,"sha256":"1ed5d0747ee78baffb45e81cb140e73636e683fe66ff0957483f77e6c9eb505e","total_lines":117,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_mnq_maintenance_break_no_cross.py","chunk_index":0,"line_start":1,"line_end":117,"content":["","\"\"\"Test MNQ maintenance break: no cross-session aggregation.","","Phase 6.6: Verify that MNQ bars before and after maintenance window","are not aggregated into the same K-bar.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pandas as pd","import pytest","","from data.session.kbar import aggregate_kbar","from data.session.loader import load_session_profile","","","@pytest.fixture","def mnq_exchange_profile(profiles_root: Path) -> Path:","    \"\"\"Load CME.MNQ EXCHANGE_RULE profile.\"\"\"","    profile_path = profiles_root / \"CME_MNQ_EXCHANGE_v1.yaml\"","    return profile_path","","","def test_mnq_maintenance_break_no_cross_30m(mnq_exchange_profile: Path) -> None:","    \"\"\"Test 30-minute bars do not cross maintenance boundary.","    ","    MNQ maintenance: 16:00-17:00 CT (approximately 06:00-07:00 TPE, varies with DST).","    Bars just before maintenance (15:59 CT) and just after (17:01 CT)","    should not be in the same 30m bar.","    \"\"\"","    profile = load_session_profile(mnq_exchange_profile)","    ","    # Create bars around maintenance window","    # Using dates that avoid DST transitions for simplicity","    # 2013/3/10 is a Sunday (before DST spring forward on 3/10/2013)","    # Maintenance window: 16:00-17:00 CT = approximately 06:00-07:00 TPE (before DST)","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/3/10 05:55:00\",  # TRADING (before maintenance, ~15:55 CT)","            \"2013/3/10 05:59:00\",  # TRADING (just before maintenance, ~15:59 CT)","            \"2013/3/10 06:30:00\",  # MAINTENANCE (during maintenance, ~16:30 CT)","            \"2013/3/10 07:01:00\",  # TRADING (just after maintenance, ~17:01 CT)","            \"2013/3/10 07:05:00\",  # TRADING (after maintenance, ~17:05 CT)","        ],","        \"open\": [100.0, 101.0, 102.0, 103.0, 104.0],","        \"high\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"low\": [99.5, 100.5, 101.5, 102.5, 103.5],","        \"close\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"volume\": [1000, 1100, 1200, 1300, 1400],","    })","    ","    result = aggregate_kbar(df, 30, profile)","    ","    # Verify result has session column","    assert \"session\" in result.columns, \"Result must include session column\"","    ","    # Verify no bar mixes TRADING and MAINTENANCE","    # Each row must have exactly one session","    assert result[\"session\"].notna().all(), \"All bars must have a session label\"","    ","    # Check that TRADING and MAINTENANCE are separate","    trading_bars = result[result[\"session\"] == \"TRADING\"]","    maintenance_bars = result[result[\"session\"] == \"MAINTENANCE\"]","    ","    # Should have both TRADING and MAINTENANCE bars (if maintenance bars exist)","    if len(maintenance_bars) > 0:","        # Verify no bar contains both sessions","        assert len(result) == len(trading_bars) + len(maintenance_bars), (","            \"Total bars should equal sum of TRADING and MAINTENANCE bars\"","        )","        ","        # Verify bars before maintenance are TRADING","        # Verify bars during maintenance are MAINTENANCE","        # Verify bars after maintenance are TRADING","        # (This is verified by the session column)","","","def test_mnq_maintenance_break_no_cross_60m(mnq_exchange_profile: Path) -> None:","    \"\"\"Test 60-minute bars do not cross maintenance boundary.\"\"\"","    profile = load_session_profile(mnq_exchange_profile)","    ","    # Similar to 30m test, but with 60m interval","    df = pd.DataFrame({","        \"ts_str\": [","            \"2013/3/10 05:50:00\",  # TRADING (before maintenance)","            \"2013/3/10 05:59:00\",  # TRADING (just before maintenance)","            \"2013/3/10 06:30:00\",  # MAINTENANCE (during maintenance)","            \"2013/3/10 07:01:00\",  # TRADING (just after maintenance)","            \"2013/3/10 07:10:00\",  # TRADING (after maintenance)","        ],","        \"open\": [100.0, 101.0, 102.0, 103.0, 104.0],","        \"high\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"low\": [99.5, 100.5, 101.5, 102.5, 103.5],","        \"close\": [100.5, 101.5, 102.5, 103.5, 104.5],","        \"volume\": [1000, 1100, 1200, 1300, 1400],","    })","    ","    result = aggregate_kbar(df, 60, profile)","    ","    # Verify result has session column","    assert \"session\" in result.columns, \"Result must include session column\"","    ","    # Verify no bar mixes TRADING and MAINTENANCE","    assert result[\"session\"].notna().all(), \"All bars must have a session label\"","    ","    # Verify session separation","    trading_bars = result[result[\"session\"] == \"TRADING\"]","    maintenance_bars = result[result[\"session\"] == \"MAINTENANCE\"]","    ","    if len(maintenance_bars) > 0:","        assert len(result) == len(trading_bars) + len(maintenance_bars), (","            \"Total bars should equal sum of TRADING and MAINTENANCE bars\"","        )","",""]}
{"type":"file_footer","path":"tests/test_mnq_maintenance_break_no_cross.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_no_fog_gate_smoke.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7567,"sha256":"a1ae442c911f99ea7a284535a818d7a4f8bf077b1b1420a655fb11a5f1273b09","total_lines":205,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_no_fog_gate_smoke.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"","Smoke test for No-Fog Gate Automation.","","Tests that the no-fog gate:","1. Can be imported and run","2. Has correct structure and dependencies","3. Can run in check-only mode without side effects","4. Validates core contract tests exist","\"\"\"","","import subprocess","import sys","import tempfile","import json","from pathlib import Path","import pytest","","# Project root","PROJECT_ROOT = Path(__file__).parent.parent.resolve()","NO_FOG_GATE_PY = PROJECT_ROOT / \"scripts\" / \"no_fog\" / \"no_fog_gate.py\"","NO_FOG_GATE_SH = PROJECT_ROOT / \"scripts\" / \"no_fog\" / \"no_fog_gate.sh\"","","","def test_no_fog_gate_py_exists():","    \"\"\"Test that the Python script exists.\"\"\"","    assert NO_FOG_GATE_PY.exists(), f\"No-Fog Gate Python script not found: {NO_FOG_GATE_PY}\"","    assert NO_FOG_GATE_PY.is_file()","","","def test_no_fog_gate_sh_exists():","    \"\"\"Test that the shell script exists.\"\"\"","    assert NO_FOG_GATE_SH.exists(), f\"No-Fog Gate shell script not found: {NO_FOG_GATE_SH}\"","    assert NO_FOG_GATE_SH.is_file()","","","def test_no_fog_gate_py_importable():","    \"\"\"Test that the Python script can be imported (syntax check).\"\"\"","    # Read the file and check for syntax errors","    import ast","    source = NO_FOG_GATE_PY.read_text(encoding=\"utf-8\")","    try:","        ast.parse(source)","    except SyntaxError as e:","        pytest.fail(f\"Syntax error in {NO_FOG_GATE_PY}: {e}\")","","","def test_no_fog_gate_sh_executable():","    \"\"\"Test that the shell script is executable (or can be made executable).\"\"\"","    # Check if it has execute permission, but don't fail if not","    # (it will be made executable by the Makefile)","    pass","","","def test_core_contract_tests_exist():","    \"\"\"Test that all core contract test files exist.\"\"\"","    core_tests = [","        \"tests/strategy/test_ast_identity.py\",","        \"tests/test_ui_race_condition_headless.py\",","        \"tests/features/test_feature_causality.py\",","        \"tests/features/test_feature_lookahead_rejection.py\",","        \"tests/features/test_feature_window_honesty.py\",","    ]","    ","    missing = []","    for test_path in core_tests:","        full_path = PROJECT_ROOT / test_path","        if not full_path.exists():","            missing.append(test_path)","    ","    assert not missing, f\"Core contract test files missing: {missing}\"","","","def test_no_fog_gate_check_only_mode():","    \"\"\"Test that the gate can run in check-only (dry run) mode.\"\"\"","    # Run the Python script with --check-only flag","    cmd = [sys.executable, str(NO_FOG_GATE_PY), \"--check-only\"]","    ","    result = subprocess.run(","        cmd,","        cwd=PROJECT_ROOT,","        capture_output=True,","        text=True,","        timeout=10,","        env={**dict(os.environ), \"PYTHONPATH\": str(PROJECT_ROOT / \"src\")}","    )","    ","    # Should exit with 0 (success) even in check-only mode","    assert result.returncode == 0, f\"Check-only mode failed:\\nStdout: {result.stdout}\\nStderr: {result.stderr}\"","    ","    # Should mention \"Dry run\" or \"check-only\" in output","    output = result.stdout + result.stderr","    assert any(phrase in output.lower() for phrase in [\"dry run\", \"check-only\", \"would run\"]), \\","        f\"Expected dry run message in output:\\n{output}\"","","","def test_no_fog_gate_help():","    \"\"\"Test that the gate shows help when requested.\"\"\"","    # Test Python script help","    cmd_py = [sys.executable, str(NO_FOG_GATE_PY), \"--help\"]","    result_py = subprocess.run(","        cmd_py,","        cwd=PROJECT_ROOT,","        capture_output=True,","        text=True,","        timeout=5","    )","    ","    assert result_py.returncode == 0, f\"Python script help failed: {result_py.stderr}\"","    assert \"usage:\" in result_py.stdout.lower() or \"help\" in result_py.stdout.lower(), \\","        f\"Help not shown in Python script:\\n{result_py.stdout}\"","    ","    # Test shell script help (if executable)","    if NO_FOG_GATE_SH.stat().st_mode & 0o111:  # If executable","        cmd_sh = [\"bash\", str(NO_FOG_GATE_SH), \"--help\"]","        result_sh = subprocess.run(","            cmd_sh,","            cwd=PROJECT_ROOT,","            capture_output=True,","            text=True,","            timeout=5","        )","        ","        # Shell script help should also work","        assert result_sh.returncode in [0, 2], f\"Shell script help failed: {result_sh.stderr}\"","        assert \"usage:\" in result_sh.stdout.lower() or \"help\" in result_sh.stdout.lower() or \"No-Fog Gate\" in result_sh.stdout, \\","            f\"Help not shown in shell script:\\n{result_sh.stdout}\"","","","def test_make_no_fog_target():","    \"\"\"Test that 'make no-fog' target is defined in Makefile.\"\"\"","    makefile_path = PROJECT_ROOT / \"Makefile\"","    assert makefile_path.exists(), \"Makefile not found\"","    ","    makefile_content = makefile_path.read_text(encoding=\"utf-8\")","    ","    # Check for no-fog target definition","    assert \"no-fog:\" in makefile_content, \"'no-fog' target not defined in Makefile\"","    ","    # Check that it's in .PHONY","    assert \"no-fog\" in makefile_content.split(\".PHONY:\")[1].split(\"\\n\")[0], \\","        \"'no-fog' not in .PHONY targets\"","    ","    # Check that it's in help","    assert \"make no-fog\" in makefile_content, \"'make no-fog' not in help section\"","","","def test_pre_commit_config():","    \"\"\"Test that pre-commit configuration includes no-fog gate.\"\"\"","    pre_commit_config = PROJECT_ROOT / \".pre-commit-config.yaml\"","    if pre_commit_config.exists():","        content = pre_commit_config.read_text(encoding=\"utf-8\")","        # Should contain no-fog-gate hook","        assert \"no-fog-gate\" in content, \"No-Fog Gate not in pre-commit config\"","        assert \"scripts/no_fog/no_fog_gate.sh\" in content, \"Shell script path not in pre-commit config\"","","","def test_github_workflow():","    \"\"\"Test that GitHub workflow exists.\"\"\"","    workflow_path = PROJECT_ROOT / \".github\" / \"workflows\" / \"no_fog_gate.yml\"","    if workflow_path.exists():","        content = workflow_path.read_text(encoding=\"utf-8\")","        assert \"No-Fog Gate\" in content, \"Workflow doesn't mention No-Fog Gate\"","        assert \"scripts/no_fog/no_fog_gate.sh\" in content, \"Shell script not in workflow\"","","","def test_snapshot_directory_structure():","    \"\"\"Test that snapshot directory structure is correct.\"\"\"","    snapshot_dir = PROJECT_ROOT / \"SYSTEM_FULL_SNAPSHOT\"","    ","    # Directory might not exist yet, that's OK","    if snapshot_dir.exists():","        # Should have MANIFEST.json if it's a valid snapshot","        manifest_path = snapshot_dir / \"MANIFEST.json\"","        if manifest_path.exists():","            try:","                manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))","                assert \"generated_at\" in manifest, \"Manifest missing generated_at\"","                assert \"chunks\" in manifest, \"Manifest missing chunks\"","                assert \"files\" in manifest, \"Manifest missing files\"","            except json.JSONDecodeError:","                pytest.fail(\"MANIFEST.json is not valid JSON\")","","","def test_gate_timeout_configuration():","    \"\"\"Test that timeout is configurable and defaults to 30 seconds.\"\"\"","    # Read the Python script to check default timeout","    content = NO_FOG_GATE_PY.read_text(encoding=\"utf-8\")","    ","    # Should have GATE_TIMEOUT = 30","    assert \"GATE_TIMEOUT = 30\" in content or \"GATE_TIMEOUT=30\" in content or \"TIMEOUT=30\" in content, \\","        \"Default timeout not set to 30 seconds in Python script\"","    ","    # Check shell script for timeout argument","    sh_content = NO_FOG_GATE_SH.read_text(encoding=\"utf-8\")","    assert \"--timeout\" in sh_content, \"Shell script missing --timeout argument\"","    assert \"30\" in sh_content, \"Shell script missing default timeout value\"","","","# Import os for environment variable","import os"]}
{"type":"file_chunk","path":"tests/test_no_fog_gate_smoke.py","chunk_index":1,"line_start":201,"line_end":205,"content":["","","if __name__ == \"__main__\":","    # Run tests","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_no_fog_gate_smoke.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_no_ui_imports_anywhere.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1427,"sha256":"938bf55f0f6d149ffba03b69d015067e7e7f93a431b02f9f7f561553e05718fa","total_lines":37,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_no_ui_imports_anywhere.py","chunk_index":0,"line_start":1,"line_end":37,"content":["","\"\"\"Contract test: No ui namespace imports anywhere in ","","Ensures the entire FishBroWFS_V2 package does not import from ui namespace.","This is a \"truth test\" to prevent any ui.* imports from being reintroduced.","\"\"\"","","from __future__ import annotations","","import pkgutil","","import pytest","","","def test_no_ui_namespace_anywhere() -> None:","    \"\"\"Test that FishBroWFS_V2 package does not import from ui namespace.\"\"\"","    import FishBroWFS_V2","    ","    # If any module imports ui.*, it will fail during import","    for importer, modname, ispkg in pkgutil.walk_packages(__path__, __name__ + \".\"):","        try:","            # Import module - this will fail if it imports ui.* and ui doesn't exist","            __import__(modname, fromlist=[\"\"])","        except ImportError as e:","            # Check if error is related to ui namespace","            if \"ui\" in str(e) and (\"No module named\" in str(e) or \"cannot import name\" in str(e)):","                pytest.fail(","                    f\"Module {modname} imports from ui namespace (ui module no longer exists): {e}\"","                )","            # 跳過 viewer 模組的 streamlit 導入錯誤","            if \"gui.viewer\" in modname and \"No module named 'streamlit'\" in str(e):","                # viewer 模組依賴 streamlit，但 streamlit 已移除，這是預期的","                continue","            # Re-raise other ImportErrors (might be legitimate missing dependencies)","            raise","",""]}
{"type":"file_footer","path":"tests/test_no_ui_imports_anywhere.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_no_ui_namespace.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6586,"sha256":"a74d7dfc6ffa996bc79b4737dcb29b98c7b0b017c9121bffdc5621941f9eea6b","total_lines":152,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_no_ui_namespace.py","chunk_index":0,"line_start":1,"line_end":152,"content":["","\"\"\"Contract test: No ui namespace imports allowed.","","Ensures the entire FishBroWFS_V2 package does not import from ui namespace.","\"\"\"","","from __future__ import annotations","","import ast","import pkgutil","from pathlib import Path","","import pytest","","","def test_no_ui_namespace_importable() -> None:","    \"\"\"Test that FishBroWFS_V2 package does not import from ui namespace.\"\"\"","    import FishBroWFS_V2 as pkg","    ","    ui_imports: list[tuple[str, str]] = []","    ","    for importer, modname, ispkg in pkgutil.walk_packages(pkg.__path__, pkg.__name__ + \".\"):","        try:","            # Import module to trigger any import errors","            module = __import__(modname, fromlist=[\"\"])","            ","            # Get source file path","            if hasattr(module, \"__file__\") and module.__file__:","                source_path = Path(module.__file__)","                if source_path.exists() and source_path.suffix == \".py\":","                    # Parse AST to find imports","                    try:","                        with source_path.open(\"r\", encoding=\"utf-8\") as f:","                            tree = ast.parse(f.read(), filename=str(source_path))","                        ","                        # Check all imports","                        for node in ast.walk(tree):","                            if isinstance(node, ast.Import):","                                for alias in node.names:","                                    if alias.name.startswith(\"ui.\"):","                                        ui_imports.append((modname, alias.name))","                            elif isinstance(node, ast.ImportFrom):","                                if node.module and node.module.startswith(\"ui.\"):","                                    ui_imports.append((modname, f\"from {node.module}\"))","                    except (SyntaxError, UnicodeDecodeError):","                        # Skip files that can't be parsed (might be binary or invalid)","                        pass","        except Exception as e:","            # Skip modules that fail to import (might be missing dependencies)","            # But log for debugging if it's not an ImportError","            if \"ImportError\" not in str(type(e)) and \"ModuleNotFoundError\" not in str(type(e)):","                pytest.fail(f\"Unexpected error importing {modname}: {e}\")","    ","    # Should have no ui.* imports","    if ui_imports:","        pytest.fail(","            f\"FishBroWFS_V2 package contains ui.* imports:\\n\"","            + \"\\n\".join(f\"  {mod}: {imp}\" for mod, imp in ui_imports)","        )","","","def test_viewer_no_ui_imports() -> None:","    \"\"\"Test that Viewer package specifically does not import from ui namespace.\"\"\"","    import gui.viewer as viewer","    ","    ui_imports: list[tuple[str, str]] = []","    ","    # Walk through all modules in viewer package","    for importer, modname, ispkg in pkgutil.walk_packages(viewer.__path__, viewer.__name__ + \".\"):","        try:","            module = __import__(modname, fromlist=[\"\"])","            ","            if hasattr(module, \"__file__\") and module.__file__:","                source_path = Path(module.__file__)","                if source_path.exists() and source_path.suffix == \".py\":","                    try:","                        with source_path.open(\"r\", encoding=\"utf-8\") as f:","                            tree = ast.parse(f.read(), filename=str(source_path))","                        ","                        for node in ast.walk(tree):","                            if isinstance(node, ast.Import):","                                for alias in node.names:","                                    if alias.name.startswith(\"ui.\"):","                                        ui_imports.append((modname, alias.name))","                            elif isinstance(node, ast.ImportFrom):","                                if node.module and node.module.startswith(\"ui.\"):","                                    ui_imports.append((modname, f\"from {node.module}\"))","                    except (SyntaxError, UnicodeDecodeError):","                        pass","        except Exception as e:","            if \"ImportError\" not in str(type(e)) and \"ModuleNotFoundError\" not in str(type(e)):","                pytest.fail(f\"Unexpected error importing {modname}: {e}\")","    ","    if ui_imports:","        pytest.fail(","            f\"Viewer package contains ui.* imports:\\n\"","            + \"\\n\".join(f\"  {mod}: {imp}\" for mod, imp in ui_imports)","        )","","","def test_no_ui_directory_exists() -> None:","    \"\"\"Test that ui/ directory does not exist in repo root (repo structure contract).\"\"\"","    repo_root = Path(__file__).parent.parent","    ui_dir = repo_root / \"ui\"","    ","    if ui_dir.exists():","        pytest.fail(f\"ui/ directory must not exist in repo root, but found at {ui_dir}\")","","","def test_makefile_no_ui_paths() -> None:","    \"\"\"Test that Makefile does not reference ui/ paths (old namespace).\"\"\"","    repo_root = Path(__file__).parent.parent","    makefile_path = repo_root / \"Makefile\"","    ","    assert makefile_path.exists()","    ","    content = makefile_path.read_text()","    ","    # Check for ui/ references (excluding comments)","    lines = content.split(\"\\n\")","    for i, line in enumerate(lines, 1):","        # Skip comments","        if line.strip().startswith(\"#\"):","            continue","        ","        # Normalize line for checking","        line_lower = line.lower()","        ","        # Prohibited patterns (old ui namespace)","        # 1. Path references containing \"/ui/\" (excluding \"gui/\")","        if \"/ui/\" in line and \"/gui/\" not in line:","            pytest.fail(f\"Makefile line {i} contains prohibited /ui/ path: {line.strip()}\")","        ","        if \"fishbro_wfs_v2.ui.\" in line_lower:","            pytest.fail(f\"Makefile line {i} contains prohibited ui. import: {line.strip()}\")","        ","        # 3. Specific old module \"ui.app_streamlit\"","        if \"ui.app_streamlit\" in line_lower:","            pytest.fail(f\"Makefile line {i} contains prohibited ui.app_streamlit: {line.strip()}\")","        ","        # 4. Standalone \"ui.\" as a module prefix (with word boundary)","        # We'll use a simple check: \"ui.\" preceded by whitespace or start of line","        # but exclude \"gui.\" and \"build\" etc.","        import re","        if re.search(r'(^|\\s)ui\\.', line) and not re.search(r'(^|\\s)gui\\.', line):","            # Allow if it's part of a longer word like \"build\" (but \"ui.\" is likely a module)","            # Additional check: ensure it's not part of a larger word like \"build\"","            if not re.search(r'\\bui\\.', line):  # word boundary check","                continue","            pytest.fail(f\"Makefile line {i} contains prohibited ui. module reference: {line.strip()}\")","",""]}
{"type":"file_footer","path":"tests/test_no_ui_namespace.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_oom_gate.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7813,"sha256":"16927b422047ced80d15af00b616f80dfaf06c922296082623e89e2c6b19eeb6","total_lines":236,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_oom_gate.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Tests for OOM gate decision maker.","","Tests verify:","1. PASS case (estimated <= 60% of budget)","2. BLOCK case (estimated > 90% of budget)","3. AUTO_DOWNSAMPLE case (between 60% and 90%, with recommended_rate in (0,1])","4. Invalid input validation (bars<=0, rate<=0, etc.)","\"\"\"","","from __future__ import annotations","","import pytest","","from core.oom_gate import decide_gate, decide_oom_action, estimate_bytes","from core.schemas.oom_gate import OomGateInput","","","def test_estimate_bytes() -> None:","    \"\"\"Test memory estimation formula.\"\"\"","    inp = OomGateInput(","        bars=1000,","        params=100,","        param_subsample_rate=0.5,","        intents_per_bar=2.0,","        bytes_per_intent_est=64,","    )","    ","    estimated = estimate_bytes(inp)","    ","    # Formula: bars * params * subsample * intents_per_bar * bytes_per_intent_est","    expected = 1000 * 100 * 0.5 * 2.0 * 64","    assert estimated == expected","","","def test_decide_gate_pass() -> None:","    \"\"\"Test PASS decision when estimated <= 60% of budget.\"\"\"","    # Small workload: 1M bytes, budget is 6GB (6_000_000_000)","    inp = OomGateInput(","        bars=100,","        params=10,","        param_subsample_rate=0.1,","        intents_per_bar=2.0,","        bytes_per_intent_est=64,","        ram_budget_bytes=6_000_000_000,","    )","    ","    decision = decide_gate(inp)","    ","    assert decision.decision == \"PASS\"","    assert decision.estimated_bytes <= inp.ram_budget_bytes * 0.6","    assert decision.recommended_subsample_rate is None","    assert \"PASS\" not in decision.notes  # Notes should describe the decision, not repeat it","    assert decision.estimated_bytes > 0","","","def test_decide_gate_block() -> None:","    \"\"\"Test BLOCK decision when estimated > 90% of budget.\"\"\"","    # Large workload: exceed 90% of budget","    # Set budget to 1GB for easier testing","    budget = 1_000_000_000  # 1GB","    # Need estimated > budget * 0.9 = 900MB","    # Let's use: 10000 bars * 10000 params * 1.0 rate * 2.0 intents * 64 bytes = 12.8GB","    inp = OomGateInput(","        bars=10000,","        params=10000,","        param_subsample_rate=1.0,","        intents_per_bar=2.0,","        bytes_per_intent_est=64,","        ram_budget_bytes=budget,","    )","    ","    decision = decide_gate(inp)","    ","    assert decision.decision == \"BLOCK\"","    assert decision.estimated_bytes > budget * 0.9","    assert decision.recommended_subsample_rate is None","    assert \"BLOCKED\" in decision.notes or \"BLOCK\" in decision.notes","","","def test_decide_gate_auto_downsample() -> None:","    \"\"\"Test AUTO_DOWNSAMPLE decision when estimated between 60% and 90%.\"\"\"","    # Medium workload: between 60% and 90% of budget","    # Set budget to 1GB for easier testing","    budget = 1_000_000_000  # 1GB","    # Need: budget * 0.6 < estimated < budget * 0.9","    # 600MB < estimated < 900MB","    # Let's use: 5000 bars * 5000 params * 1.0 rate * 2.0 intents * 64 bytes = 3.2GB","    # That's too high. Let's adjust:","    # For 700MB: 700_000_000 = bars * params * 1.0 * 2.0 * 64","    # bars * params = 700_000_000 / (2.0 * 64) = 5_468_750","    # Let's use: 5000 bars * 1094 params * 1.0 rate * 2.0 * 64 = ~700MB","    inp = OomGateInput(","        bars=5000,","        params=1094,","        param_subsample_rate=1.0,","        intents_per_bar=2.0,","        bytes_per_intent_est=64,","        ram_budget_bytes=budget,","    )","    ","    decision = decide_gate(inp)","    ","    assert decision.decision == \"AUTO_DOWNSAMPLE\"","    assert decision.estimated_bytes > budget * 0.6","    assert decision.estimated_bytes <= budget * 0.9","    assert decision.recommended_subsample_rate is not None","    assert 0.0 < decision.recommended_subsample_rate <= 1.0","    assert \"recommended\" in decision.notes.lower() or \"subsample\" in decision.notes.lower()","","","def test_decide_gate_auto_downsample_recommended_rate_calculation() -> None:","    \"\"\"Test that recommended_rate is calculated correctly for AUTO_DOWNSAMPLE.\"\"\"","    budget = 1_000_000_000  # 1GB","    bars = 1000","    params = 1000","    intents_per_bar = 2.0","    bytes_per_intent = 64","    ","    # Use current rate that puts us in AUTO_DOWNSAMPLE zone","    inp = OomGateInput(","        bars=bars,","        params=params,","        param_subsample_rate=1.0,","        intents_per_bar=intents_per_bar,","        bytes_per_intent_est=bytes_per_intent,","        ram_budget_bytes=budget,","    )","    ","    decision = decide_gate(inp)","    ","    if decision.decision == \"AUTO_DOWNSAMPLE\":","        # Verify recommended_rate formula: (ram_budget * 0.6) / (bars * params * intents_per_bar * bytes_per_intent_est)","        expected_rate = (budget * 0.6) / (bars * params * intents_per_bar * bytes_per_intent)","        expected_rate = max(0.0, min(1.0, expected_rate))","        ","        assert decision.recommended_subsample_rate is not None","        assert abs(decision.recommended_subsample_rate - expected_rate) < 0.0001  # Allow small floating point error","","","def test_invalid_input_bars_zero() -> None:","    \"\"\"Test that bars <= 0 raises validation error.\"\"\"","    with pytest.raises(Exception):  # Pydantic ValidationError","        OomGateInput(","            bars=0,","            params=100,","            param_subsample_rate=0.5,","        )","","","def test_invalid_input_bars_negative() -> None:","    \"\"\"Test that bars < 0 raises validation error.\"\"\"","    with pytest.raises(Exception):  # Pydantic ValidationError","        OomGateInput(","            bars=-1,","            params=100,","            param_subsample_rate=0.5,","        )","","","def test_invalid_input_params_zero() -> None:","    \"\"\"Test that params <= 0 raises validation error.\"\"\"","    with pytest.raises(Exception):  # Pydantic ValidationError","        OomGateInput(","            bars=1000,","            params=0,","            param_subsample_rate=0.5,","        )","","","def test_invalid_input_subsample_rate_zero() -> None:","    \"\"\"Test that param_subsample_rate <= 0 raises validation error.\"\"\"","    with pytest.raises(Exception):  # Pydantic ValidationError","        OomGateInput(","            bars=1000,","            params=100,","            param_subsample_rate=0.0,","        )","","","def test_invalid_input_subsample_rate_negative() -> None:","    \"\"\"Test that param_subsample_rate < 0 raises validation error.\"\"\"","    with pytest.raises(Exception):  # Pydantic ValidationError","        OomGateInput(","            bars=1000,","            params=100,","            param_subsample_rate=-0.1,","        )","","","def test_invalid_input_subsample_rate_over_one() -> None:","    \"\"\"Test that param_subsample_rate > 1.0 raises validation error.\"\"\"","    with pytest.raises(Exception):  # Pydantic ValidationError","        OomGateInput(","            bars=1000,","            params=100,","            param_subsample_rate=1.1,","        )","",""]}
{"type":"file_chunk","path":"tests/test_oom_gate.py","chunk_index":1,"line_start":201,"line_end":236,"content":["def test_default_values() -> None:","    \"\"\"Test that default values work correctly.\"\"\"","    inp = OomGateInput(","        bars=1000,","        params=100,","        param_subsample_rate=0.5,","    )","    ","    assert inp.intents_per_bar == 2.0","    assert inp.bytes_per_intent_est == 64","    assert inp.ram_budget_bytes == 6_000_000_000  # 6GB","    ","    decision = decide_gate(inp)","    assert decision.decision in (\"PASS\", \"BLOCK\", \"AUTO_DOWNSAMPLE\")","    assert decision.estimated_bytes >= 0","    assert decision.ram_budget_bytes == inp.ram_budget_bytes","","","def test_decide_oom_action_returns_dict_schema() -> None:","    \"\"\"Test legacy decide_oom_action() returns dict schema.\"\"\"","    cfg = {\"bars\": 1000, \"params_total\": 100, \"param_subsample_rate\": 0.1}","    res = decide_oom_action(cfg, mem_limit_mb=10_000.0)","    ","    assert isinstance(res, dict)","    assert res[\"action\"] in {\"PASS\", \"BLOCK\", \"AUTO_DOWNSAMPLE\"}","    assert \"estimated_bytes\" in res","    assert \"estimated_mb\" in res","    assert \"mem_limit_mb\" in res","    assert \"mem_limit_bytes\" in res","    assert \"original_subsample\" in res  # Contract key name","    assert \"final_subsample\" in res  # Contract key name","    assert \"params_total\" in res","    assert \"params_effective\" in res","    assert \"reason\" in res","",""]}
{"type":"file_footer","path":"tests/test_oom_gate.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_oom_gate_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7148,"sha256":"c5ed7f99bd6647a75d2c70617be6979781541f08c37790f4480df4a7fa1b2c95","total_lines":204,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_oom_gate_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for OOM gate.","","Tests verify:","1. Gate PASS when under limit","2. Gate BLOCK when over limit and no auto-downsample","3. Gate AUTO_DOWNSAMPLE when allowed","\"\"\"","","from __future__ import annotations","","import numpy as np","import pytest","","from core.oom_gate import decide_oom_action","from core.oom_cost_model import estimate_memory_bytes, summarize_estimates","","","def test_oom_gate_pass_when_under_limit():","    \"\"\"Test that gate PASSes when memory estimate is under limit.\"\"\"","    cfg = {","        \"bars\": 1000,","        \"params_total\": 100,","        \"param_subsample_rate\": 0.1,","        \"open_\": np.random.randn(1000).astype(np.float64),","        \"high\": np.random.randn(1000).astype(np.float64),","        \"low\": np.random.randn(1000).astype(np.float64),","        \"close\": np.random.randn(1000).astype(np.float64),","        \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","    }","    ","    # Use a very high limit to ensure PASS","    mem_limit_mb = 10000.0","    ","    result = decide_oom_action(cfg, mem_limit_mb=mem_limit_mb)","    ","    assert result[\"action\"] == \"PASS\"","    assert result[\"original_subsample\"] == 0.1","    assert result[\"final_subsample\"] == 0.1","    assert \"estimates\" in result","    assert result[\"estimates\"][\"mem_est_mb\"] <= mem_limit_mb","","","def test_oom_gate_block_when_over_limit_and_no_auto():","    \"\"\"Test that gate BLOCKs when over limit and auto-downsample is disabled.\"\"\"","    cfg = {","        \"bars\": 100000,","        \"params_total\": 10000,","        \"param_subsample_rate\": 1.0,","        \"open_\": np.random.randn(100000).astype(np.float64),","        \"high\": np.random.randn(100000).astype(np.float64),","        \"low\": np.random.randn(100000).astype(np.float64),","        \"close\": np.random.randn(100000).astype(np.float64),","        \"params_matrix\": np.random.randn(10000, 3).astype(np.float64),","    }","    ","    # Use a very low limit to ensure BLOCK","    mem_limit_mb = 1.0","    ","    result = decide_oom_action(","        cfg,","        mem_limit_mb=mem_limit_mb,","        allow_auto_downsample=False,","    )","    ","    assert result[\"action\"] == \"BLOCK\"","    assert result[\"original_subsample\"] == 1.0","    assert result[\"final_subsample\"] == 1.0  # Not changed","    assert \"reason\" in result","    assert \"mem_est_mb\" in result[\"reason\"] or \"limit\" in result[\"reason\"]","","","def test_oom_gate_auto_downsample_when_allowed(monkeypatch):","    \"\"\"Test that gate AUTO_DOWNSAMPLEs when allowed and over limit.\"\"\"","    # Monkeypatch estimate_memory_bytes to make it subsample-sensitive for testing","    def mock_estimate_memory_bytes(cfg, work_factor=2.0):","        \"\"\"Mock that makes memory estimate sensitive to subsample.\"\"\"","        bars = int(cfg.get(\"bars\", 0))","        params_total = int(cfg.get(\"params_total\", 0))","        subsample_rate = float(cfg.get(\"param_subsample_rate\", 1.0))","        params_effective = int(params_total * subsample_rate)","        ","        # Simplified: mem scales with bars and effective params","        base_mem = bars * 8 * 4  # 4 price arrays","        params_mem = params_effective * 3 * 8  # params_matrix","        total_mem = (base_mem + params_mem) * work_factor","        return int(total_mem)","    ","    monkeypatch.setattr(","        \"core.oom_cost_model.estimate_memory_bytes\",","        mock_estimate_memory_bytes,","    )","    ","    cfg = {","        \"bars\": 10000,","        \"params_total\": 1000,","        \"param_subsample_rate\": 0.5,  # Start at 50%","        \"open_\": np.random.randn(10000).astype(np.float64),","        \"high\": np.random.randn(10000).astype(np.float64),","        \"low\": np.random.randn(10000).astype(np.float64),","        \"close\": np.random.randn(10000).astype(np.float64),","        \"params_matrix\": np.random.randn(1000, 3).astype(np.float64),","    }","    ","    # Dynamic calculation: compute mem_mb for two subsample rates, use midpoint","    def _mem_mb(cfg_dict):","        b = mock_estimate_memory_bytes(cfg_dict, work_factor=2.0)","        return b / (1024.0 * 1024.0)","    ","    cfg_half = dict(cfg)","    cfg_half[\"param_subsample_rate\"] = 0.5","    cfg_quarter = dict(cfg)","    cfg_quarter[\"param_subsample_rate\"] = 0.25","    ","    mb_half = _mem_mb(cfg_half)  # ~0.633","    mb_quarter = _mem_mb(cfg_quarter)  # ~0.622","    ","    # Set limit between these two values → guaranteed to trigger AUTO_DOWNSAMPLE","    mem_limit_mb = (mb_half + mb_quarter) / 2.0","    ","    result = decide_oom_action(","        cfg,","        mem_limit_mb=mem_limit_mb,","        allow_auto_downsample=True,","        auto_downsample_step=0.5,","        auto_downsample_min=0.02,","    )","    ","    assert result[\"action\"] == \"AUTO_DOWNSAMPLE\"","    assert result[\"original_subsample\"] == 0.5","    assert result[\"final_subsample\"] < result[\"original_subsample\"]","    assert result[\"final_subsample\"] >= 0.02  # Above minimum","    assert \"reason\" in result","    assert \"auto-downsample\" in result[\"reason\"].lower()","    assert result[\"estimates\"][\"mem_est_mb\"] <= mem_limit_mb","","","def test_oom_gate_block_when_min_still_over_limit(monkeypatch):","    \"\"\"Test that gate BLOCKs when even at minimum subsample still over limit.\"\"\"","    def mock_estimate_memory_bytes(cfg, work_factor=2.0):","        \"\"\"Mock that always returns high memory.\"\"\"","        return 100 * 1024 * 1024  # Always 100MB","    ","    monkeypatch.setattr(","        \"core.oom_cost_model.estimate_memory_bytes\",","        mock_estimate_memory_bytes,","    )","    ","    cfg = {","        \"bars\": 1000,","        \"params_total\": 100,","        \"param_subsample_rate\": 0.5,","        \"open_\": np.random.randn(1000).astype(np.float64),","        \"high\": np.random.randn(1000).astype(np.float64),","        \"low\": np.random.randn(1000).astype(np.float64),","        \"close\": np.random.randn(1000).astype(np.float64),","        \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","    }","    ","    mem_limit_mb = 50.0  # Lower than mock estimate","    ","    result = decide_oom_action(","        cfg,","        mem_limit_mb=mem_limit_mb,","        allow_auto_downsample=True,","        auto_downsample_min=0.02,","    )","    ","    assert result[\"action\"] == \"BLOCK\"","    assert \"min_subsample\" in result[\"reason\"].lower() or \"still too large\" in result[\"reason\"].lower()","","","def test_oom_gate_result_schema():","    \"\"\"Test that gate result has correct schema.\"\"\"","    cfg = {","        \"bars\": 1000,","        \"params_total\": 100,","        \"param_subsample_rate\": 0.1,","        \"open_\": np.random.randn(1000).astype(np.float64),","        \"high\": np.random.randn(1000).astype(np.float64),","        \"low\": np.random.randn(1000).astype(np.float64),","        \"close\": np.random.randn(1000).astype(np.float64),","        \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","    }","    ","    result = decide_oom_action(cfg, mem_limit_mb=10000.0)","    ","    # Verify schema","    assert \"action\" in result","    assert result[\"action\"] in (\"PASS\", \"BLOCK\", \"AUTO_DOWNSAMPLE\")","    assert \"reason\" in result","    assert isinstance(result[\"reason\"], str)","    assert \"original_subsample\" in result","    assert \"final_subsample\" in result","    assert \"estimates\" in result","    ","    # Verify estimates structure","    estimates = result[\"estimates\"]","    assert \"mem_est_bytes\" in estimates","    assert \"mem_est_mb\" in estimates"]}
{"type":"file_chunk","path":"tests/test_oom_gate_contract.py","chunk_index":1,"line_start":201,"line_end":204,"content":["    assert \"ops_est\" in estimates","    assert \"time_est_s\" in estimates","",""]}
{"type":"file_footer","path":"tests/test_oom_gate_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_oom_gate_pure_function_hash_consistency.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3240,"sha256":"be3eeef3a13b904e34a116e8c388cfbf9a922625203a75b9bbe23e2ef8ec504c","total_lines":84,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_oom_gate_pure_function_hash_consistency.py","chunk_index":0,"line_start":1,"line_end":84,"content":["","\"\"\"Tests for OOM gate pure function hash consistency.","","Tests that decide_oom_action never mutates input cfg and returns new_cfg SSOT.","\"\"\"","","from __future__ import annotations","","import pytest","","from core.config_hash import stable_config_hash","from core.config_snapshot import make_config_snapshot","from core.oom_gate import decide_oom_action","","","def test_oom_gate_pure_function_hash_consistency(monkeypatch) -> None:","    \"\"\"","    Test that decide_oom_action is pure function (no mutation).","    ","    Uses monkeypatch to ensure subsample-sensitive memory estimation,","    guaranteeing that subsample=1.0 exceeds limit and subsample reduction","    triggers AUTO_DOWNSAMPLE.","    ","    Verifies:","    - Original cfg subsample remains unchanged","    - decision.new_cfg has modified subsample","    - Hash computed from new_cfg differs from original","    - manifest/snapshot records final_subsample correctly","    \"\"\"","    def mock_estimate_memory_bytes(cfg, work_factor=2.0):","        \"\"\"Make mem scale with subsample so AUTO_DOWNSAMPLE is meaningful.\"\"\"","        subsample = float(cfg.get(\"param_subsample_rate\", 1.0))","        # 100MB at subsample=1.0, 50MB at 0.5, etc.","        base = 100 * 1024 * 1024","        return int(base * subsample)","    ","    monkeypatch.setattr(","        \"core.oom_cost_model.estimate_memory_bytes\",","        mock_estimate_memory_bytes,","    )","    ","    cfg = {","        \"bars\": 1,","        \"params_total\": 1,","        \"param_subsample_rate\": 1.0,","    }","    mem_limit_mb = 60.0  # 1.0 => 100MB (over), 0.5 => 50MB (under)","    ","    decision = decide_oom_action(cfg, mem_limit_mb=mem_limit_mb, allow_auto_downsample=True)","    ","    # Verify original cfg unchanged","    assert cfg[\"param_subsample_rate\"] == 1.0, \"Original cfg must not be mutated\"","    ","    # Verify decision has new_cfg","    assert \"new_cfg\" in decision, \"decision must contain new_cfg\"","    new_cfg = decision[\"new_cfg\"]","    ","    # Lock behavior: allow_auto_downsample=True 時不得 PASS，必須 AUTO_DOWNSAMPLE（除非低於 min）","    assert decision[\"action\"] == \"AUTO_DOWNSAMPLE\", \"Should trigger AUTO_DOWNSAMPLE when allow_auto_downsample=True\"","    ","    # Verify new_cfg has modified subsample","    assert new_cfg[\"param_subsample_rate\"] < 1.0, \"new_cfg should have reduced subsample\"","    assert decision[\"final_subsample\"] < 1.0, \"final_subsample should be reduced\"","    assert decision[\"final_subsample\"] < decision[\"original_subsample\"], \"final_subsample must be < original_subsample\"","    assert decision[\"new_cfg\"][\"param_subsample_rate\"] == decision[\"final_subsample\"], \"new_cfg subsample must match final_subsample\"","    ","    # Verify hash consistency","    original_snapshot = make_config_snapshot(cfg)","    original_hash = stable_config_hash(original_snapshot)","    ","    new_snapshot = make_config_snapshot(new_cfg)","    new_hash = stable_config_hash(new_snapshot)","    ","    assert original_hash != new_hash, \"Hash should differ after subsample change\"","    ","    # Verify final_subsample matches new_cfg","    assert decision[\"final_subsample\"] == new_cfg[\"param_subsample_rate\"], (","        \"final_subsample must match new_cfg subsample\"","    )","    ","    # Verify original_subsample preserved","    assert decision[\"original_subsample\"] == 1.0, \"original_subsample must be preserved\"","",""]}
{"type":"file_footer","path":"tests/test_oom_gate_pure_function_hash_consistency.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_perf_breakdown_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":13001,"sha256":"04ef300ef6ef0142f998ddc47b64d6fc8e63d91a967d73098426a7e8fa303d40","total_lines":349,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_perf_breakdown_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Stage P2-1.8: Contract Tests for Granular Breakdown and Extended Observability","","Tests that verify:","- Granular timing keys exist and are non-negative floats","- Extended observability keys exist (entry/exit intents/fills totals)","- Accounting consistency (intents_total == entry + exit, fills_total == entry + exit)","- run_grid output contains timing keys in perf dict","\"\"\"","from __future__ import annotations","","import os","import numpy as np","","from strategy.kernel import run_kernel_arrays, DonchianAtrParams","from engine.types import BarArrays","from pipeline.runner_grid import run_grid","","","def test_perf_breakdown_keys_existence() -> None:","    \"\"\"","    D1: Contract test - Verify granular timing keys exist in _obs and are floats >= 0.0","    Also verify that t_total_kernel_s >= max(stage_times) for sanity check.","    ","    Contract: keys always exist, values always float >= 0.0.","    (When perf harness runs with profiling enabled, these will naturally become >0 real data.)","    \"\"\"","    import os","    # Ensure clean environment for test","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    # Task 2: Kernel profiling is optional - keys will always exist (may be 0.0 if not profiled)","    # We can optionally enable profiling to get real timing data, but it's not required for contract","    old_profile_kernel = os.environ.get(\"FISHBRO_PROFILE_KERNEL\")","    # Optionally enable profiling to get real timing values (not required - keys exist regardless)","    # Uncomment the line below if you want to test with profiling enabled:","    # os.environ[\"FISHBRO_PROFILE_KERNEL\"] = \"1\"","    ","    try:","        n_bars = 200","        warmup = 20","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        bars = BarArrays(","            open=open_.astype(np.float64),","            high=high.astype(np.float64),","            low=low.astype(np.float64),","            close=close.astype(np.float64),","        )","        ","        params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)","        ","        result = run_kernel_arrays(","            bars=bars,","            params=params,","            commission=0.0,","            slip=0.0,","            order_qty=1,","        )","        ","        # Verify _obs exists and contains timing keys","        assert \"_obs\" in result, \"_obs must exist in kernel result\"","        obs = result[\"_obs\"]","        assert isinstance(obs, dict), \"_obs must be a dict\"","        ","        # Required timing keys (now in _obs, not _perf)","        # Task 2: Contract - keys always exist, values always float >= 0.0","        timing_keys = [","            \"t_calc_indicators_s\",","            \"t_build_entry_intents_s\",","            \"t_simulate_entry_s\",","            \"t_calc_exits_s\",","            \"t_simulate_exit_s\",","            \"t_total_kernel_s\",","        ]","        ","        stage_times = []","        for key in timing_keys:","            assert key in obs, f\"{key} must exist in _obs (keys always exist, even if 0.0)\"","            value = obs[key]","            assert isinstance(value, float), f\"{key} must be float, got {type(value)}\"","            assert value >= 0.0, f\"{key} must be >= 0.0, got {value}\"","            if key != \"t_total_kernel_s\":","                stage_times.append(value)","        ","        # Sanity check: total time should be >= max of individual stage times","        # (allowing some overhead for timer calls and other operations)","        # Note: This check only makes sense if profiling was enabled (values > 0)","        t_total = obs[\"t_total_kernel_s\"]","        if stage_times and t_total > 0.0:","            max_stage = max(stage_times)","            # Allow equality or small overhead","            assert t_total >= max_stage, (","                f\"t_total_kernel_s ({t_total}) should be >= max(stage_times) ({max_stage})\"","            )","    finally:","        # Restore environment","        # restore trigger rate","        if old_trigger_rate is None:","            os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","        ","        # restore kernel profiling flag","        if old_profile_kernel is None:","            os.environ.pop(\"FISHBRO_PROFILE_KERNEL\", None)","        else:","            os.environ[\"FISHBRO_PROFILE_KERNEL\"] = old_profile_kernel","","","def test_extended_observability_keys_existence() -> None:","    \"\"\"","    D1: Contract test - Verify extended observability keys exist in _obs","    \"\"\"","    import os","    # Ensure clean environment for test","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    ","    try:","        n_bars = 200","        warmup = 20","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        bars = BarArrays(","            open=open_.astype(np.float64),","            high=high.astype(np.float64),","            low=low.astype(np.float64),","            close=close.astype(np.float64),","        )","        ","        params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)","        ","        result = run_kernel_arrays(","            bars=bars,","            params=params,","            commission=0.0,","            slip=0.0,","            order_qty=1,","        )","        ","        # Verify _obs exists and contains extended keys","        assert \"_obs\" in result, \"_obs must exist in kernel result\"","        obs = result[\"_obs\"]","        assert isinstance(obs, dict), \"_obs must be a dict\"","        ","        # Required observability keys","        obs_keys = [","            \"entry_intents_total\",","            \"entry_fills_total\",","            \"exit_intents_total\",","            \"exit_fills_total\",","        ]","        ","        for key in obs_keys:","            assert key in obs, f\"{key} must exist in _obs\"","            value = obs[key]","            assert isinstance(value, int), f\"{key} must be int, got {type(value)}\"","            assert value >= 0, f\"{key} must be >= 0, got {value}\"","    finally:","        # Restore environment","        if old_trigger_rate is not None:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","","","def test_accounting_consistency() -> None:","    \"\"\"","    D2: Contract test - Verify accounting consistency","    intents_total == entry_intents_total + exit_intents_total","    fills_total == entry_fills_total + exit_fills_total","    Also verify entry_intents_total == valid_mask_sum in arrays mode","    \"\"\"","    import os","    # Ensure clean environment for test","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    ","    try:","        n_bars = 200","        warmup = 20","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))"]}
{"type":"file_chunk","path":"tests/test_perf_breakdown_contract.py","chunk_index":1,"line_start":201,"line_end":349,"content":["        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        bars = BarArrays(","            open=open_.astype(np.float64),","            high=high.astype(np.float64),","            low=low.astype(np.float64),","            close=close.astype(np.float64),","        )","        ","        params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)","        ","        result = run_kernel_arrays(","            bars=bars,","            params=params,","            commission=0.0,","            slip=0.0,","            order_qty=1,","        )","        ","        obs = result[\"_obs\"]","        ","        # Verify intents_total consistency","        intents_total = obs.get(\"intents_total\", 0)","        entry_intents_total = obs.get(\"entry_intents_total\", 0)","        exit_intents_total = obs.get(\"exit_intents_total\", 0)","        ","        assert intents_total == entry_intents_total + exit_intents_total, (","            f\"intents_total ({intents_total}) must equal \"","            f\"entry_intents_total ({entry_intents_total}) + exit_intents_total ({exit_intents_total})\"","        )","        ","        # Verify fills_total consistency","        fills_total = obs.get(\"fills_total\", 0)","        entry_fills_total = obs.get(\"entry_fills_total\", 0)","        exit_fills_total = obs.get(\"exit_fills_total\", 0)","        ","        assert fills_total == entry_fills_total + exit_fills_total, (","            f\"fills_total ({fills_total}) must equal \"","            f\"entry_fills_total ({entry_fills_total}) + exit_fills_total ({exit_fills_total})\"","        )","        ","        # Verify entry_intents_total == valid_mask_sum (arrays mode contract)","        if \"valid_mask_sum\" in obs and \"entry_intents_total\" in obs:","            valid_mask_sum = obs.get(\"valid_mask_sum\", 0)","            entry_intents = obs.get(\"entry_intents_total\", 0)","            assert entry_intents == valid_mask_sum, (","                f\"entry_intents_total ({entry_intents}) must equal valid_mask_sum ({valid_mask_sum})\"","            )","    finally:","        # Restore environment","        if old_trigger_rate is not None:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","","","def test_run_grid_perf_contains_timing_keys(monkeypatch) -> None:","    \"\"\"","    Contract test - Verify run_grid output contains timing keys in perf dict.","    This ensures timing aggregation works correctly at grid level.","    \"\"\"","    # Task 1: Explicitly enable kernel profiling (required for timing collection)","    old_profile_kernel = os.environ.get(\"FISHBRO_PROFILE_KERNEL\")","    os.environ[\"FISHBRO_PROFILE_KERNEL\"] = \"1\"","    ","    # Enable profile mode to ensure timing collection","    monkeypatch.setenv(\"FISHBRO_PROFILE_GRID\", \"1\")","    ","    try:","        n_bars = 200","        n_params = 5","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate minimal params","        params = np.array([","            [20, 10, 1.0],","            [25, 12, 1.5],","            [30, 15, 2.0],","            [35, 18, 1.0],","            [40, 20, 1.5],","        ], dtype=np.float64)","        ","        result = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=False,","        )","        ","        # Verify perf dict exists","        assert \"perf\" in result, \"perf must exist in run_grid result\"","        perf = result[\"perf\"]","        assert isinstance(perf, dict), \"perf must be a dict\"","        ","        # Stage P2-2 Step A: Required micro-profiling timing keys (aggregated across params)","        # Task 2: Since profile is enabled, timing keys must exist","        timing_keys = [","            \"t_ind_donchian_s\",","            \"t_ind_atr_s\",","            \"t_build_entry_intents_s\",","            \"t_simulate_entry_s\",","            \"t_calc_exits_s\",","            \"t_simulate_exit_s\",","            \"t_total_kernel_s\",","        ]","        ","        for key in timing_keys:","            assert key in perf, f\"{key} must exist in perf dict when profile is enabled\"","            value = perf[key]","            assert isinstance(value, float), f\"{key} must be float, got {type(value)}\"","            assert value >= 0.0, f\"{key} must be >= 0.0, got {value}\"","        ","        # Stage P2-2 Step A: Memoization potential assessment keys","        unique_keys = [","            \"unique_channel_len_count\",","            \"unique_atr_len_count\",","            \"unique_ch_atr_pair_count\",","        ]","        ","        for key in unique_keys:","            assert key in perf, f\"{key} must exist in perf dict\"","            value = perf[key]","            assert isinstance(value, int), f\"{key} must be int, got {type(value)}\"","            assert value >= 1, f\"{key} must be >= 1, got {value}\"","    finally:","        # Task 1: Restore FISHBRO_PROFILE_KERNEL environment variable","        if old_profile_kernel is None:","            os.environ.pop(\"FISHBRO_PROFILE_KERNEL\", None)","        else:","            os.environ[\"FISHBRO_PROFILE_KERNEL\"] = old_profile_kernel","",""]}
{"type":"file_footer","path":"tests/test_perf_breakdown_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_perf_env_config_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3257,"sha256":"0cef44b16c3126aa66e42ab08dfa39c3c1cac14231f5e3f6bf24c8659e541f87","total_lines":91,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_perf_env_config_contract.py","chunk_index":0,"line_start":1,"line_end":91,"content":["","\"\"\"Test perf harness environment variable configuration contract.","","Ensures that FISHBRO_PERF_BARS and FISHBRO_PERF_PARAMS env vars are correctly parsed.","\"\"\"","","import os","import sys","from pathlib import Path","from unittest.mock import patch","","","def _get_perf_config():","    \"\"\"","    Helper to get perf config values by reading the script file.","    This avoids import issues with scripts/ module.","    \"\"\"","    script_path = Path(__file__).parent.parent / \"scripts\" / \"perf_grid.py\"","    ","    # Read and parse the constants","    with open(script_path, \"r\", encoding=\"utf-8\") as f:","        content = f.read()","    ","    # Extract default values from the file","    # Look for: TIER_JIT_BARS = int(os.environ.get(\"FISHBRO_PERF_BARS\", \"20000\"))","    import re","    ","    bars_match = re.search(r'TIER_JIT_BARS\\s*=\\s*int\\(os\\.environ\\.get\\(\"FISHBRO_PERF_BARS\",\\s*\"(\\d+)\"\\)\\)', content)","    params_match = re.search(r'TIER_JIT_PARAMS\\s*=\\s*int\\(os\\.environ\\.get\\(\"FISHBRO_PERF_PARAMS\",\\s*\"(\\d+)\"\\)\\)', content)","    ","    default_bars = int(bars_match.group(1)) if bars_match else None","    default_params = int(params_match.group(1)) if params_match else None","    ","    return default_bars, default_params","","","def test_perf_env_bars_parsing():","    \"\"\"Test that FISHBRO_PERF_BARS env var is correctly parsed.\"\"\"","    with patch.dict(os.environ, {\"FISHBRO_PERF_BARS\": \"50000\"}, clear=False):","        # Simulate the parsing logic","        bars = int(os.environ.get(\"FISHBRO_PERF_BARS\", \"20000\"))","        assert bars == 50000","","","def test_perf_env_params_parsing():","    \"\"\"Test that FISHBRO_PERF_PARAMS env var is correctly parsed.\"\"\"","    with patch.dict(os.environ, {\"FISHBRO_PERF_PARAMS\": \"5000\"}, clear=False):","        # Simulate the parsing logic","        params = int(os.environ.get(\"FISHBRO_PERF_PARAMS\", \"1000\"))","        assert params == 5000","","","def test_perf_env_both_parsing():","    \"\"\"Test that both env vars can be set simultaneously.\"\"\"","    with patch.dict(os.environ, {","        \"FISHBRO_PERF_BARS\": \"30000\",","        \"FISHBRO_PERF_PARAMS\": \"3000\",","    }, clear=False):","        bars = int(os.environ.get(\"FISHBRO_PERF_BARS\", \"20000\"))","        params = int(os.environ.get(\"FISHBRO_PERF_PARAMS\", \"1000\"))","        ","        assert bars == 30000","        assert params == 3000","","","def test_perf_env_defaults():","    \"\"\"Test that defaults are baseline (20000×1000) when env vars are not set.\"\"\"","    # Ensure env vars are not set for this test","    env_backup = {}","    for key in [\"FISHBRO_PERF_BARS\", \"FISHBRO_PERF_PARAMS\"]:","        if key in os.environ:","            env_backup[key] = os.environ[key]","            del os.environ[key]","    ","    try:","        # Check defaults match baseline","        default_bars, default_params = _get_perf_config()","        assert default_bars == 20000, f\"Expected default bars=20000, got {default_bars}\"","        assert default_params == 1000, f\"Expected default params=1000, got {default_params}\"","        ","        # Verify parsing logic uses defaults","        bars = int(os.environ.get(\"FISHBRO_PERF_BARS\", \"20000\"))","        params = int(os.environ.get(\"FISHBRO_PERF_PARAMS\", \"1000\"))","        assert bars == 20000","        assert params == 1000","    finally:","        # Restore env vars","        for key, value in env_backup.items():","            os.environ[key] = value","",""]}
{"type":"file_footer","path":"tests/test_perf_env_config_contract.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_perf_evidence_chain.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2927,"sha256":"ff79bead60c0d830feeadeebdeb8a55ce716ed756542dbb8fce97f7a6cd89702","total_lines":85,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_perf_evidence_chain.py","chunk_index":0,"line_start":1,"line_end":85,"content":["","from __future__ import annotations","","import numpy as np","","from pipeline.runner_grid import run_grid","","","def test_perf_evidence_chain_exists() -> None:","    \"\"\"","    Phase 3.0-D: Contract Test - Evidence Chain Existence","    ","    Purpose: Lock down that evidence fields always exist and are non-null.","    This test only verifies evidence existence, not timing or strategy quality.","    \"\"\"","    # Use minimal data: bars=50, params=3","    n_bars = 50","    n_params = 3","    ","    # Generate synthetic OHLC data","    rng = np.random.default_rng(42)","    close = 100.0 + np.cumsum(rng.standard_normal(n_bars)).astype(np.float64)","    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","    open_ = (high + low) / 2 + rng.standard_normal(n_bars) * 0.5","    ","    # Ensure high >= max(open, close) and low <= min(open, close)","    high = np.maximum(high, np.maximum(open_, close))","    low = np.minimum(low, np.minimum(open_, close))","    ","    # Generate minimal params: [channel_len, atr_len, stop_mult]","    params = np.array(","        [","            [10, 5, 1.0],","            [15, 7, 1.5],","            [20, 10, 2.0],","        ],","        dtype=np.float64,","    )","    ","    # Run grid runner (array path)","    # Note: perf field is always present in runner output (Phase 3.0-B)","    out = run_grid(","        open_=open_,","        high=high,","        low=low,","        close=close,","        params_matrix=params,","        commission=0.0,","        slip=0.0,","        order_qty=1,","        sort_params=False,","    )","    ","    # Verify perf field exists","    assert \"perf\" in out, \"perf field must exist in runner output\"","    perf = out[\"perf\"]","    assert isinstance(perf, dict), \"perf must be a dict\"","    ","    # Phase 3.0-D: Assert evidence fields exist and are non-null","    # 1. intent_mode must be \"arrays\"","    assert \"intent_mode\" in perf, \"intent_mode must exist in perf\"","    assert perf[\"intent_mode\"] == \"arrays\", (","        f\"intent_mode expected 'arrays' but got '{perf['intent_mode']}'\"","    )","    ","    # 2. intents_total must exist, be non-null, and > 0","    assert \"intents_total\" in perf, \"intents_total must exist in perf\"","    assert perf[\"intents_total\"] is not None, \"intents_total must not be None\"","    assert isinstance(perf[\"intents_total\"], (int, np.integer)), (","        f\"intents_total must be an integer, got {type(perf['intents_total'])}\"","    )","    assert int(perf[\"intents_total\"]) > 0, (","        f\"intents_total must be > 0, got {perf['intents_total']}\"","    )","    ","    # 3. fills_total must exist and be non-null (can be 0, but not None)","    assert \"fills_total\" in perf, \"fills_total must exist in perf\"","    assert perf[\"fills_total\"] is not None, \"fills_total must not be None\"","    assert isinstance(perf[\"fills_total\"], (int, np.integer)), (","        f\"fills_total must be an integer, got {type(perf['fills_total'])}\"","    )","    # fills_total can be 0 (no trades), but must not be None","",""]}
{"type":"file_footer","path":"tests/test_perf_evidence_chain.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_perf_grid_profile_report.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":619,"sha256":"d04f3674e1625e436dda8058520c945b739a3872961db35df796befbe0fd07da","total_lines":30,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_perf_grid_profile_report.py","chunk_index":0,"line_start":1,"line_end":30,"content":["","from __future__ import annotations","","import cProfile","","from perf.profile_report import _format_profile_report","","","def test_profile_report_markers_present() -> None:","    pr = cProfile.Profile()","    pr.enable()","    _ = sum(range(10_000))  # tiny workload, deterministic","    pr.disable()","    report = _format_profile_report(","        lane_id=\"3\",","        n_bars=2000,","        n_params=100,","        jit_enabled=True,","        sort_params=False,","        topn=10,","        mode=\"\",","        pr=pr,","    )","    assert \"__PROFILE_START__\" in report","    assert \"pstats sort: cumtime\" in report","    assert \"__PROFILE_END__\" in report","","","",""]}
{"type":"file_footer","path":"tests/test_perf_grid_profile_report.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_perf_obs_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6538,"sha256":"fddec6aa753e3a538f4844c59f7b4706ee5d89590050e4d8870f752835875a38","total_lines":171,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_perf_obs_contract.py","chunk_index":0,"line_start":1,"line_end":171,"content":["","\"\"\"Contract tests for perf observability (Stage P2-1.5).","","These tests ensure that entry sparse observability fields are correctly","propagated from kernel to perf JSON output.","\"\"\"","","import numpy as np","import pytest","","from pipeline.runner_grid import run_grid","","","def test_perf_obs_entry_sparse_fields():","    \"\"\"","    Contract: perf dict must contain entry sparse observability fields.","    ","    This test directly calls run_grid (no subprocess) to verify that:","    1. entry_valid_mask_sum is present in perf dict","    2. entry_intents_total is present in perf dict","    3. entry_valid_mask_sum == entry_intents_total (contract)","    4. entry_intents_per_bar_avg is correctly calculated","    \"\"\"","    # Generate small synthetic data (fast test)","    n_bars = 2000","    n_params = 50","    ","    rng = np.random.default_rng(42)","    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10","    high = close + np.abs(rng.standard_normal(n_bars)) * 5","    low = close - np.abs(rng.standard_normal(n_bars)) * 5","    open_ = (high + low) / 2 + rng.standard_normal(n_bars)","    ","    high = np.maximum(high, np.maximum(open_, close))","    low = np.minimum(low, np.minimum(open_, close))","    ","    # Generate params matrix (channel_len, atr_len, stop_mult)","    params_matrix = np.column_stack([","        np.random.randint(10, 30, size=n_params),  # channel_len","        np.random.randint(5, 20, size=n_params),   # atr_len","        np.random.uniform(1.0, 2.0, size=n_params),  # stop_mult","    ]).astype(np.float64)","    ","    # Call run_grid (will use arrays mode by default)","    result = run_grid(","        open_=open_,","        high=high,","        low=low,","        close=close,","        params_matrix=params_matrix,","        commission=0.0,","        slip=0.0,","        order_qty=1,","        sort_params=False,","    )","    ","    # Verify result structure","    assert \"perf\" in result, \"result must contain 'perf' dict\"","    perf = result[\"perf\"]","    assert isinstance(perf, dict), \"perf must be a dict\"","    ","    # Verify entry sparse observability fields exist","    assert \"entry_valid_mask_sum\" in perf, (","        \"perf must contain 'entry_valid_mask_sum' field\"","    )","    assert \"entry_intents_total\" in perf, (","        \"perf must contain 'entry_intents_total' field\"","    )","    ","    entry_valid_mask_sum = perf[\"entry_valid_mask_sum\"]","    entry_intents_total = perf[\"entry_intents_total\"]","    ","    # Verify types","    assert isinstance(entry_valid_mask_sum, (int, np.integer)), (","        f\"entry_valid_mask_sum must be int, got {type(entry_valid_mask_sum)}\"","    )","    assert isinstance(entry_intents_total, (int, np.integer)), (","        f\"entry_intents_total must be int, got {type(entry_intents_total)}\"","    )","    ","    # Contract: entry_valid_mask_sum == entry_intents_total","    assert entry_valid_mask_sum == entry_intents_total, (","        f\"entry_valid_mask_sum ({entry_valid_mask_sum}) must equal \"","        f\"entry_intents_total ({entry_intents_total})\"","    )","    ","    # Verify entry_intents_per_bar_avg if present","    if \"entry_intents_per_bar_avg\" in perf:","        entry_intents_per_bar_avg = perf[\"entry_intents_per_bar_avg\"]","        assert isinstance(entry_intents_per_bar_avg, (float, np.floating)), (","            f\"entry_intents_per_bar_avg must be float, got {type(entry_intents_per_bar_avg)}\"","        )","        ","        # Verify calculation: entry_intents_per_bar_avg == entry_intents_total / n_bars","        expected_avg = entry_intents_total / n_bars","        assert abs(entry_intents_per_bar_avg - expected_avg) <= 1e-12, (","            f\"entry_intents_per_bar_avg ({entry_intents_per_bar_avg}) must equal \"","            f\"entry_intents_total / n_bars ({expected_avg})\"","        )","    ","    # Verify intents_total_reported is present (preserves original)","    if \"intents_total_reported\" in perf:","        intents_total_reported = perf[\"intents_total_reported\"]","        assert isinstance(intents_total_reported, (int, np.integer)), (","            f\"intents_total_reported must be int, got {type(intents_total_reported)}\"","        )","        # intents_total_reported should equal original intents_total","        if \"intents_total\" in perf:","            assert intents_total_reported == perf[\"intents_total\"], (","                f\"intents_total_reported ({intents_total_reported}) should equal \"","                f\"intents_total ({perf['intents_total']})\"","            )","","","def test_perf_obs_entry_sparse_non_zero():","    \"\"\"","    Contract: With valid data, entry sparse fields should be non-zero.","    ","    This ensures that sparse masking is actually working and producing","    observable results.","    \"\"\"","    # Generate data that should produce some valid intents","    n_bars = 1000","    n_params = 20","    ","    rng = np.random.default_rng(42)","    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10","    high = close + np.abs(rng.standard_normal(n_bars)) * 5","    low = close - np.abs(rng.standard_normal(n_bars)) * 5","    open_ = (high + low) / 2 + rng.standard_normal(n_bars)","    ","    high = np.maximum(high, np.maximum(open_, close))","    low = np.minimum(low, np.minimum(open_, close))","    ","    # Use reasonable params (should produce valid donch_hi)","    params_matrix = np.column_stack([","        np.full(n_params, 20, dtype=np.float64),  # channel_len = 20","        np.full(n_params, 14, dtype=np.float64),  # atr_len = 14","        np.full(n_params, 2.0, dtype=np.float64),  # stop_mult = 2.0","    ])","    ","    result = run_grid(","        open_=open_,","        high=high,","        low=low,","        close=close,","        params_matrix=params_matrix,","        commission=0.0,","        slip=0.0,","        order_qty=1,","        sort_params=False,","    )","    ","    perf = result.get(\"perf\", {})","    if \"entry_valid_mask_sum\" in perf and \"entry_intents_total\" in perf:","        entry_valid_mask_sum = perf[\"entry_valid_mask_sum\"]","        entry_intents_total = perf[\"entry_intents_total\"]","        ","        # With valid data and reasonable params, we should have some intents","        # (but allow for edge cases where all are filtered)","        assert entry_valid_mask_sum >= 0, \"entry_valid_mask_sum must be non-negative\"","        assert entry_intents_total >= 0, \"entry_intents_total must be non-negative\"","        ","        # With n_bars=1000 and channel_len=20, we should have some valid intents","        # after warmup (at least a few)","        if n_bars > 100:  # Only check if we have enough bars","            # Conservative: allow for edge cases but expect some intents","            # In practice, with valid data, we should have >> 0","            pass  # Just verify non-negative, don't enforce minimum","",""]}
{"type":"file_footer","path":"tests/test_perf_obs_contract.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_perf_trigger_rate_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9899,"sha256":"db87bda10984645f730e091005977713c33470ccec2948cf406c29e0d9e67a85","total_lines":315,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_perf_trigger_rate_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Stage P2-1.6: Contract Tests for Trigger Rate Masking","","Tests that verify trigger_rate control works correctly:","- entry_intents_total scales linearly with trigger_rate","- entry_valid_mask_sum == entry_intents_total","- Deterministic behavior (same seed → same result)","\"\"\"","from __future__ import annotations","","import numpy as np","import os","","from perf.scenario_control import apply_trigger_rate_mask","","","def test_trigger_rate_mask_rate_1_0_no_change() -> None:","    \"\"\"","    Test that trigger_rate=1.0 preserves all valid triggers unchanged.","    \"\"\"","    n_bars = 2000","    warmup = 100","    ","    # Create trigger array: warmup period NaN, rest are valid positive values","    trigger = np.full(n_bars, np.nan, dtype=np.float64)","    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    # Apply mask with rate=1.0","    masked = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=1.0,","        warmup=warmup,","        seed=42,","    )","    ","    # Should be unchanged","    assert np.array_equal(trigger, masked, equal_nan=True), (","        \"trigger_rate=1.0 should not change trigger array\"","    )","","","def test_trigger_rate_mask_rate_0_05_approximately_5_percent() -> None:","    \"\"\"","    Test that trigger_rate=0.05 results in approximately 5% of valid triggers.","    Allows ±20% relative error to account for random fluctuations.","    \"\"\"","    n_bars = 2000","    warmup = 100","    n_valid_expected = n_bars - warmup  # Valid positions after warmup","    ","    # Create trigger array: warmup period NaN, rest are valid positive values","    trigger = np.full(n_bars, np.nan, dtype=np.float64)","    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    # Apply mask with rate=0.05","    masked = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=0.05,","        warmup=warmup,","        seed=42,","    )","    ","    # Count valid (finite) positions after warmup","    valid_after_warmup = np.isfinite(masked[warmup:])","    n_valid_actual = int(np.sum(valid_after_warmup))","    ","    # Expected: approximately 5% of valid positions","    expected_min = int(n_valid_expected * 0.05 * 0.8)  # 80% of 5% (lower bound)","    expected_max = int(n_valid_expected * 0.05 * 1.2)  # 120% of 5% (upper bound)","    ","    assert expected_min <= n_valid_actual <= expected_max, (","        f\"Expected ~5% valid triggers ({expected_min}-{expected_max}), \"","        f\"got {n_valid_actual} ({n_valid_actual/n_valid_expected*100:.2f}%)\"","    )","","","def test_trigger_rate_mask_deterministic() -> None:","    \"\"\"","    Test that same seed and same input produce identical mask results.","    \"\"\"","    n_bars = 2000","    warmup = 100","    ","    # Create trigger array","    trigger = np.full(n_bars, np.nan, dtype=np.float64)","    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    # Apply mask twice with same parameters","    masked1 = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=0.05,","        warmup=warmup,","        seed=42,","    )","    ","    masked2 = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=0.05,","        warmup=warmup,","        seed=42,","    )","    ","    # Should be identical","    assert np.array_equal(masked1, masked2, equal_nan=True), (","        \"Same seed and input should produce identical mask results\"","    )","","","def test_trigger_rate_mask_different_seeds_different_results() -> None:","    \"\"\"","    Test that different seeds produce different mask results (when rate < 1.0).","    \"\"\"","    n_bars = 2000","    warmup = 100","    ","    # Create trigger array","    trigger = np.full(n_bars, np.nan, dtype=np.float64)","    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    # Apply mask with different seeds","    masked1 = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=0.05,","        warmup=warmup,","        seed=42,","    )","    ","    masked2 = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=0.05,","        warmup=warmup,","        seed=999,","    )","    ","    # Should be different (very unlikely to be identical with different seeds)","    assert not np.array_equal(masked1, masked2, equal_nan=True), (","        \"Different seeds should produce different mask results\"","    )","","","def test_trigger_rate_mask_preserves_warmup_nan() -> None:","    \"\"\"","    Test that warmup period NaN positions are preserved (not masked).","    \"\"\"","    n_bars = 2000","    warmup = 100","    ","    # Create trigger array: warmup period NaN, rest are valid","    trigger = np.full(n_bars, np.nan, dtype=np.float64)","    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    # Apply mask","    masked = apply_trigger_rate_mask(","        trigger=trigger,","        trigger_rate=0.05,","        warmup=warmup,","        seed=42,","    )","    ","    # Warmup period should remain NaN","    assert np.all(np.isnan(masked[:warmup])), (","        \"Warmup period should remain NaN after masking\"","    )","","","def test_trigger_rate_mask_linear_scaling() -> None:","    \"\"\"","    Test that valid trigger count scales approximately linearly with trigger_rate.","    \"\"\"","    n_bars = 2000","    warmup = 100","    n_valid_expected = n_bars - warmup","    ","    # Create trigger array","    trigger = np.full(n_bars, np.nan, dtype=np.float64)","    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    rates = [0.1, 0.3, 0.5, 0.7, 0.9]","    valid_counts = []","    ","    for rate in rates:","        masked = apply_trigger_rate_mask(","            trigger=trigger,","            trigger_rate=rate,","            warmup=warmup,","            seed=42,","        )","        n_valid = int(np.sum(np.isfinite(masked[warmup:])))","        valid_counts.append(n_valid)","    ","    # Check approximate linearity: valid_counts[i] / valid_counts[j] ≈ rates[i] / rates[j]","    # Use first and last as reference","    ratio_expected = rates[-1] / rates[0]  # 0.9 / 0.1 = 9.0","    ratio_actual = valid_counts[-1] / valid_counts[0] if valid_counts[0] > 0 else 0","    ","    # Allow ±30% error for random fluctuations","    assert 0.7 * ratio_expected <= ratio_actual <= 1.3 * ratio_expected, (","        f\"Valid counts should scale linearly with rate. \"","        f\"Expected ratio ~{ratio_expected:.2f}, got {ratio_actual:.2f}. \""]}
{"type":"file_chunk","path":"tests/test_perf_trigger_rate_contract.py","chunk_index":1,"line_start":201,"line_end":315,"content":["        f\"Counts: {valid_counts}\"","    )","","","def test_trigger_rate_mask_preserves_dtype() -> None:","    \"\"\"","    Test that masking preserves the input dtype.","    \"\"\"","    n_bars = 200","    warmup = 20","    ","    # Test with float64","    trigger_f64 = np.full(n_bars, np.nan, dtype=np.float64)","    trigger_f64[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)","    ","    masked_f64 = apply_trigger_rate_mask(","        trigger=trigger_f64,","        trigger_rate=0.5,","        warmup=warmup,","        seed=42,","    )","    ","    assert masked_f64.dtype == np.float64, (","        f\"Expected float64, got {masked_f64.dtype}\"","    )","    ","    # Test with float32","    trigger_f32 = np.full(n_bars, np.nan, dtype=np.float32)","    trigger_f32[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float32)","    ","    masked_f32 = apply_trigger_rate_mask(","        trigger=trigger_f32,","        trigger_rate=0.5,","        warmup=warmup,","        seed=42,","    )","    ","    assert masked_f32.dtype == np.float32, (","        f\"Expected float32, got {masked_f32.dtype}\"","    )","","","def test_trigger_rate_mask_integration_with_kernel() -> None:","    \"\"\"","    Integration test: verify that trigger_rate affects entry_intents_total in run_kernel_arrays.","    This test uses run_kernel_arrays directly (no subprocess) to verify the integration.","    \"\"\"","    from strategy.kernel import run_kernel_arrays, DonchianAtrParams","    from engine.types import BarArrays","    ","    n_bars = 200","    warmup = 20","    ","    # Generate simple OHLC data","    rng = np.random.default_rng(42)","    close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","    open_ = (high + low) / 2","    ","    high = np.maximum(high, np.maximum(open_, close))","    low = np.minimum(low, np.minimum(open_, close))","    ","    bars = BarArrays(","        open=open_.astype(np.float64),","        high=high.astype(np.float64),","        low=low.astype(np.float64),","        close=close.astype(np.float64),","    )","    ","    params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)","    ","    # Test with trigger_rate=1.0 (baseline) - explicitly set to avoid env interference","    os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","    result_1_0 = run_kernel_arrays(","        bars=bars,","        params=params,","        commission=0.0,","        slip=0.0,","        order_qty=1,","    )","    ","    # Contract test: fail fast if keys missing (no .get() with defaults)","    entry_intents_1_0 = result_1_0[\"_obs\"][\"entry_intents_total\"]","    valid_mask_sum_1_0 = result_1_0[\"_obs\"][\"entry_valid_mask_sum\"]","    assert entry_intents_1_0 == valid_mask_sum_1_0","    ","    # Test with trigger_rate=0.5","    os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"0.5\"","    result_0_5 = run_kernel_arrays(","        bars=bars,","        params=params,","        commission=0.0,","        slip=0.0,","        order_qty=1,","    )","    ","    # Contract test: fail fast if keys missing (no .get() with defaults)","    entry_intents_0_5 = result_0_5[\"_obs\"][\"entry_intents_total\"]","    valid_mask_sum_0_5 = result_0_5[\"_obs\"][\"entry_valid_mask_sum\"]","    assert entry_intents_0_5 == valid_mask_sum_0_5","    ","    # Cleanup","    os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    ","    # Verify that entry_intents_0_5 is approximately 50% of entry_intents_1_0","    # Allow ±30% error for random fluctuations and warmup/NaN deterministic effects","    if entry_intents_1_0 > 0:","        ratio = entry_intents_0_5 / entry_intents_1_0","        assert 0.35 <= ratio <= 0.65, (","            f\"With trigger_rate=0.5, expected entry_intents ~50% of baseline, \"","            f\"got {ratio*100:.1f}% (baseline={entry_intents_1_0}, actual={entry_intents_0_5})\"","        )","",""]}
{"type":"file_footer","path":"tests/test_perf_trigger_rate_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_phase13_batch_submit.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6391,"sha256":"ef0f03299b63d60f0549962c7a467b4cd0c03fa61f113ef80fef76e2ef1d0059","total_lines":193,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase13_batch_submit.py","chunk_index":0,"line_start":1,"line_end":193,"content":["","\"\"\"Unit tests for batch_submit module (Phase 13).\"\"\"","","import pytest","from control.batch_submit import (","    BatchSubmitRequest,","    BatchSubmitResponse,","    compute_batch_id,","    wizard_to_db_jobspec,","    submit_batch,",")","from control.job_spec import WizardJobSpec, DataSpec, WFSSpec","from control.types import DBJobSpec","from datetime import date","","","def test_batch_submit_request():","    \"\"\"BatchSubmitRequest creation.\"\"\"","    jobs = [","        WizardJobSpec(","            season=\"2024Q1\",","            data1=DataSpec(dataset_id=\"test\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","            strategy_id=\"s1\",","            params={\"p\": 1},","            wfs=WFSSpec()","        ),","        WizardJobSpec(","            season=\"2024Q1\",","            data1=DataSpec(dataset_id=\"test\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","            strategy_id=\"s1\",","            params={\"p\": 2},","            wfs=WFSSpec()","        ),","    ]","    req = BatchSubmitRequest(jobs=jobs)","    assert len(req.jobs) == 2","    assert req.jobs[0].params[\"p\"] == 1","    assert req.jobs[1].params[\"p\"] == 2","","","def test_batch_submit_response():","    \"\"\"BatchSubmitResponse creation.\"\"\"","    resp = BatchSubmitResponse(","        batch_id=\"batch-123\",","        total_jobs=5,","        job_ids=[\"job1\", \"job2\", \"job3\", \"job4\", \"job5\"]","    )","    assert resp.batch_id == \"batch-123\"","    assert resp.total_jobs == 5","    assert len(resp.job_ids) == 5","","","def test_compute_batch_id_deterministic():","    \"\"\"Batch ID is deterministic based on sorted JobSpec JSON.\"\"\"","    jobs = [","        WizardJobSpec(","            season=\"2024Q1\",","            data1=DataSpec(dataset_id=\"test\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","            strategy_id=\"s1\",","            params={\"a\": 1, \"b\": 2},","            wfs=WFSSpec()","        ),","        WizardJobSpec(","            season=\"2024Q1\",","            data1=DataSpec(dataset_id=\"test\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","            strategy_id=\"s1\",","            params={\"a\": 3, \"b\": 4},","            wfs=WFSSpec()","        ),","    ]","    batch_id1 = compute_batch_id(jobs)","    # Same jobs, different order should produce same batch ID","    jobs_reversed = list(reversed(jobs))","    batch_id2 = compute_batch_id(jobs_reversed)","    assert batch_id1 == batch_id2","    # Different jobs produce different ID","    jobs2 = [jobs[0]]","    batch_id3 = compute_batch_id(jobs2)","    assert batch_id1 != batch_id3","","","def test_wizard_to_db_jobspec():","    \"\"\"Convert Wizard JobSpec to DB JobSpec.\"\"\"","    wizard_spec = WizardJobSpec(","        season=\"2024Q1\",","        data1=DataSpec(dataset_id=\"CME_MNQ_v2\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","        strategy_id=\"my_strategy\",","        params={\"param1\": 42},","        wfs=WFSSpec(stage0_subsample=0.5, top_k=100, mem_limit_mb=2048, allow_auto_downsample=True)","    )","    # Mock dataset record with fingerprint","    dataset_record = {","        \"fingerprint_sha256_40\": \"abc123def456ghi789jkl012mno345pqr678stu901\",","        \"normalized_sha256_40\": \"abc123def456ghi789jkl012mno345pqr678stu901\"","    }","    db_spec = wizard_to_db_jobspec(wizard_spec, dataset_record)","    assert isinstance(db_spec, DBJobSpec)","    assert db_spec.season == \"2024Q1\"","    assert db_spec.dataset_id == \"CME_MNQ_v2\"","    assert db_spec.outputs_root == \"outputs/seasons/2024Q1/runs\"","    # config_snapshot should contain params and wfs","    config = db_spec.config_snapshot","    assert config[\"params\"][\"param1\"] == 42","    assert config[\"wfs\"][\"stage0_subsample\"] == 0.5","    assert config[\"wfs\"][\"top_k\"] == 100","    # config_hash should be non-empty","    assert db_spec.config_hash","    assert db_spec.created_by == \"wizard_batch\"","    # fingerprint should be set","    assert db_spec.data_fingerprint_sha256_40 == \"abc123def456ghi789jkl012mno345pqr678stu901\"","","","def test_submit_batch_mocked(monkeypatch):","    \"\"\"Test submit_batch with mocked DB calls.\"\"\"","    # Mock create_job to return predictable job IDs","    job_ids = [\"job-a\", \"job-b\", \"job-c\"]","    call_count = 0","    def mock_create_job(db_path, spec):","        nonlocal call_count","        # Ensure spec is DBJobSpec","        assert isinstance(spec, DBJobSpec)","        # Return sequential ID","        result = job_ids[call_count]","        call_count += 1","        return result","    ","    import control.batch_submit as batch_module","    monkeypatch.setattr(batch_module, \"create_job\", mock_create_job)","    ","    # Prepare request","    jobs = [","        WizardJobSpec(","            season=\"2024Q1\",","            data1=DataSpec(dataset_id=\"test\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","            strategy_id=\"s1\",","            params={\"p\": i},","            wfs=WFSSpec()","        ) for i in range(3)","    ]","    req = BatchSubmitRequest(jobs=jobs)","    ","    # Mock dataset index","    dataset_index = {","        \"test\": {","            \"fingerprint_sha256_40\": \"abc123def456ghi789jkl012mno345pqr678stu901\",","            \"normalized_sha256_40\": \"abc123def456ghi789jkl012mno345pqr678stu901\"","        }","    }","    ","    # Call submit_batch with dummy db_path","    from pathlib import Path","    db_path = Path(\"/tmp/test.db\")","    resp = submit_batch(db_path, req, dataset_index)","    ","    assert resp.batch_id.startswith(\"batch-\")","    assert resp.total_jobs == 3","    assert resp.job_ids == job_ids","    assert call_count == 3","","","def test_submit_batch_empty_jobs():","    \"\"\"Empty jobs list raises.\"\"\"","    req = BatchSubmitRequest(jobs=[])","    from pathlib import Path","    db_path = Path(\"/tmp/test.db\")","    dataset_index = {\"test\": {\"fingerprint_sha256_40\": \"abc123\"}}","    with pytest.raises(ValueError, match=\"jobs list cannot be empty\"):","        submit_batch(db_path, req, dataset_index)","","","def test_submit_batch_too_many_jobs():","    \"\"\"Jobs exceed cap raises.\"\"\"","    jobs = [","        WizardJobSpec(","            season=\"2024Q1\",","            data1=DataSpec(dataset_id=\"test\", start_date=date(2020,1,1), end_date=date(2020,12,31)),","            strategy_id=\"s1\",","            params={\"p\": i},","            wfs=WFSSpec()","        ) for i in range(1001)  # exceed default cap of 1000","    ]","    req = BatchSubmitRequest(jobs=jobs)","    from pathlib import Path","    db_path = Path(\"/tmp/test.db\")","    dataset_index = {\"test\": {\"fingerprint_sha256_40\": \"abc123\"}}","    with pytest.raises(ValueError, match=\"exceeds maximum\"):","        submit_batch(db_path, req, dataset_index)","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])","",""]}
{"type":"file_footer","path":"tests/test_phase13_batch_submit.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase13_job_expand.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6884,"sha256":"d1c964fc59f3ded9cd3f61f9e178170d704faf3a1e811b6bbc8016a55a232b51","total_lines":227,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_phase13_job_expand.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Unit tests for job_expand module (Phase 13).\"\"\"","","import pytest","from control.param_grid import GridMode, ParamGridSpec","from control.job_expand import JobTemplate, expand_job_template, estimate_total_jobs, validate_template","from control.job_spec import WFSSpec","","","def test_job_template_creation():","    \"\"\"JobTemplate creation and serialization.\"\"\"","    param_grid = {","        \"param1\": ParamGridSpec(mode=GridMode.SINGLE, single_value=10),","        \"param2\": ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=2, range_step=1),","    }","    wfs = WFSSpec(stage0_subsample=0.5, top_k=100, mem_limit_mb=2048, allow_auto_downsample=True)","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"CME_MNQ_v2\",","        strategy_id=\"my_strategy\",","        param_grid=param_grid,","        wfs=wfs","    )","    assert template.season == \"2024Q1\"","    assert template.dataset_id == \"CME_MNQ_v2\"","    assert template.strategy_id == \"my_strategy\"","    assert len(template.param_grid) == 2","    assert template.wfs == wfs","","","def test_expand_job_template_single():","    \"\"\"Expand single parameter.\"\"\"","    param_grid = {","        \"p\": ParamGridSpec(mode=GridMode.SINGLE, single_value=42),","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    jobs = list(expand_job_template(template))","    assert len(jobs) == 1","    job = jobs[0]","    assert job.season == \"2024Q1\"","    assert job.dataset_id == \"test\"","    assert job.strategy_id == \"s\"","    assert job.params == {\"p\": 42}","","","def test_expand_job_template_range():","    \"\"\"Expand range parameter.\"\"\"","    param_grid = {","        \"p\": ParamGridSpec(mode=GridMode.RANGE, range_start=1, range_end=3, range_step=1),","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    jobs = list(expand_job_template(template))","    assert len(jobs) == 3","    values = [job.params[\"p\"] for job in jobs]","    assert values == [1, 2, 3]","    # Order should be deterministic (sorted by param name, then values)","    assert jobs[0].params[\"p\"] == 1","    assert jobs[1].params[\"p\"] == 2","    assert jobs[2].params[\"p\"] == 3","","","def test_expand_job_template_multi():","    \"\"\"Expand multi values parameter.\"\"\"","    param_grid = {","        \"p\": ParamGridSpec(mode=GridMode.MULTI, multi_values=[\"a\", \"b\", \"c\"]),","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    jobs = list(expand_job_template(template))","    assert len(jobs) == 3","    values = [job.params[\"p\"] for job in jobs]","    assert values == [\"a\", \"b\", \"c\"]","","","def test_expand_job_template_two_params():","    \"\"\"Expand two parameters (cartesian product).\"\"\"","    param_grid = {","        \"p1\": ParamGridSpec(mode=GridMode.RANGE, range_start=1, range_end=2, range_step=1),","        \"p2\": ParamGridSpec(mode=GridMode.MULTI, multi_values=[\"x\", \"y\"]),","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    jobs = list(expand_job_template(template))","    assert len(jobs) == 4  # 2 * 2","    # Order: param names sorted alphabetically, then values","    # p1 values: 1,2 ; p2 values: x,y","    # Expected order: (p1=1, p2=x), (p1=1, p2=y), (p1=2, p2=x), (p1=2, p2=y)","    expected = [","        {\"p1\": 1, \"p2\": \"x\"},","        {\"p1\": 1, \"p2\": \"y\"},","        {\"p1\": 2, \"p2\": \"x\"},","        {\"p1\": 2, \"p2\": \"y\"},","    ]","    for i, job in enumerate(jobs):","        assert job.params == expected[i]","","","def test_estimate_total_jobs():","    \"\"\"Estimate total jobs count.\"\"\"","    param_grid = {","        \"p1\": ParamGridSpec(mode=GridMode.RANGE, range_start=1, range_end=10, range_step=1),  # 10 values","        \"p2\": ParamGridSpec(mode=GridMode.MULTI, multi_values=[\"a\", \"b\", \"c\"]),  # 3 values","        \"p3\": ParamGridSpec(mode=GridMode.SINGLE, single_value=99),  # 1 value","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    total = estimate_total_jobs(template)","    assert total == 10 * 3 * 1  # 30","","","def test_validate_template_ok():","    \"\"\"Valid template passes.\"\"\"","    param_grid = {","        \"p\": ParamGridSpec(mode=GridMode.SINGLE, single_value=5),","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    validate_template(template)  # no exception","","","def test_validate_template_empty_param_grid():","    \"\"\"Empty param grid raises.\"\"\"","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid={},","        wfs=WFSSpec()","    )","    with pytest.raises(ValueError, match=\"param_grid cannot be empty\"):","        validate_template(template)","","","def test_validate_template_missing_season():","    \"\"\"Missing season raises.\"\"\"","    param_grid = {\"p\": ParamGridSpec(mode=GridMode.SINGLE, single_value=1)}","    template = JobTemplate(","        season=\"\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    with pytest.raises(ValueError, match=\"season must be non-empty\"):","        validate_template(template)","","","def test_validate_template_missing_dataset_id():","    \"\"\"Missing dataset_id raises.\"\"\"","    param_grid = {\"p\": ParamGridSpec(mode=GridMode.SINGLE, single_value=1)}","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    with pytest.raises(ValueError, match=\"dataset_id must be non-empty\"):","        validate_template(template)","","","def test_validate_template_missing_strategy_id():","    \"\"\"Missing strategy_id raises.\"\"\"","    param_grid = {\"p\": ParamGridSpec(mode=GridMode.SINGLE, single_value=1)}","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"\","]}
{"type":"file_chunk","path":"tests/test_phase13_job_expand.py","chunk_index":1,"line_start":201,"line_end":227,"content":["        param_grid=param_grid,","        wfs=WFSSpec()","    )","    with pytest.raises(ValueError, match=\"strategy_id must be non-empty\"):","        validate_template(template)","","","def test_validate_template_param_grid_invalid():","    \"\"\"ParamGrid validation errors propagate.\"\"\"","    param_grid = {","        \"p\": ParamGridSpec(mode=GridMode.RANGE, range_start=10, range_end=0, range_step=1),  # invalid","    }","    template = JobTemplate(","        season=\"2024Q1\",","        dataset_id=\"test\",","        strategy_id=\"s\",","        param_grid=param_grid,","        wfs=WFSSpec()","    )","    with pytest.raises(ValueError, match=\"start <= end\"):","        validate_template(template)","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])","",""]}
{"type":"file_footer","path":"tests/test_phase13_job_expand.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_phase13_param_grid.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5128,"sha256":"ac31fc7b55401f4806325c144e926ef0c53796ab5d2f85a103cc073a62027fac","total_lines":146,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase13_param_grid.py","chunk_index":0,"line_start":1,"line_end":146,"content":["","\"\"\"Unit tests for param_grid module (Phase 13).\"\"\"","","import pytest","from control.param_grid import GridMode, ParamGridSpec, values_for_param, count_for_param, validate_grid_for_param","","","def test_grid_mode_enum():","    \"\"\"GridMode enum values.\"\"\"","    assert GridMode.SINGLE.value == \"single\"","    assert GridMode.RANGE.value == \"range\"","    assert GridMode.MULTI.value == \"multi\"","","","def test_param_grid_spec_single():","    \"\"\"Single mode spec.\"\"\"","    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=42)","    assert spec.mode == GridMode.SINGLE","    assert spec.single_value == 42","    assert spec.range_start is None","    assert spec.range_end is None","    assert spec.range_step is None","    assert spec.multi_values is None","","","def test_param_grid_spec_range():","    \"\"\"Range mode spec.\"\"\"","    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=10, range_step=2)","    assert spec.mode == GridMode.RANGE","    assert spec.range_start == 0","    assert spec.range_end == 10","    assert spec.range_step == 2","    assert spec.single_value is None","    assert spec.multi_values is None","","","def test_param_grid_spec_multi():","    \"\"\"Multi mode spec.\"\"\"","    spec = ParamGridSpec(mode=GridMode.MULTI, multi_values=[1, 2, 3])","    assert spec.mode == GridMode.MULTI","    assert spec.multi_values == [1, 2, 3]","    assert spec.single_value is None","    assert spec.range_start is None","","","def test_values_for_param_single():","    \"\"\"Single mode yields single value.\"\"\"","    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=5.5)","    vals = list(values_for_param(spec))","    assert vals == [5.5]","","","def test_values_for_param_range_int():","    \"\"\"Range mode with integer step.\"\"\"","    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=5, range_step=1)","    vals = list(values_for_param(spec))","    assert vals == [0, 1, 2, 3, 4, 5]","","","def test_values_for_param_range_float():","    \"\"\"Range mode with float step.\"\"\"","    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=0.0, range_end=1.0, range_step=0.5)","    vals = list(values_for_param(spec))","    assert vals == [0.0, 0.5, 1.0]","","","def test_values_for_param_multi():","    \"\"\"Multi mode yields list of values.\"\"\"","    spec = ParamGridSpec(mode=GridMode.MULTI, multi_values=[\"a\", \"b\", \"c\"])","    vals = list(values_for_param(spec))","    assert vals == [\"a\", \"b\", \"c\"]","","","def test_count_for_param():","    \"\"\"Count of values.\"\"\"","    spec_single = ParamGridSpec(mode=GridMode.SINGLE, single_value=1)","    assert count_for_param(spec_single) == 1","    ","    spec_range = ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=10, range_step=2)","    # 0,2,4,6,8,10 => 6 values","    assert count_for_param(spec_range) == 6","    ","    spec_multi = ParamGridSpec(mode=GridMode.MULTI, multi_values=[1, 2, 3, 4])","    assert count_for_param(spec_multi) == 4","","","def test_validate_grid_for_param_single_ok():","    \"\"\"Single mode validation passes.\"\"\"","    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=100)","    validate_grid_for_param(spec, \"int\", min=0, max=200)","    # No exception","","","def test_validate_grid_for_param_single_out_of_range():","    \"\"\"Single mode value out of range raises.\"\"\"","    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=300)","    with pytest.raises(ValueError, match=\"out of range\"):","        validate_grid_for_param(spec, \"int\", min=0, max=200)","","","def test_validate_grid_for_param_range_invalid_step():","    \"\"\"Range mode with zero step raises.\"\"\"","    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=10, range_step=0)","    with pytest.raises(ValueError, match=\"step must be positive\"):","        validate_grid_for_param(spec, \"int\", min=0, max=100)","","","def test_validate_grid_for_param_range_start_gt_end():","    \"\"\"Range start > end raises.\"\"\"","    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=10, range_end=0, range_step=1)","    with pytest.raises(ValueError, match=\"start <= end\"):","        validate_grid_for_param(spec, \"int\", min=0, max=100)","","","def test_validate_grid_for_param_multi_empty():","    \"\"\"Multi mode with empty list raises.\"\"\"","    spec = ParamGridSpec(mode=GridMode.MULTI, multi_values=[])","    with pytest.raises(ValueError, match=\"at least one value\"):","        validate_grid_for_param(spec, \"int\", min=0, max=100)","","","def test_validate_grid_for_param_multi_duplicates():","    \"\"\"Multi mode with duplicates raises.\"\"\"","    spec = ParamGridSpec(mode=GridMode.MULTI, multi_values=[1, 2, 2, 3])","    with pytest.raises(ValueError, match=\"duplicate values\"):","        validate_grid_for_param(spec, \"int\", min=0, max=100)","","","def test_validate_grid_for_param_enum():","    \"\"\"Enum type validation passes if value in choices.\"\"\"","    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=\"buy\")","    validate_grid_for_param(spec, \"enum\", choices=[\"buy\", \"sell\", \"hold\"])","    # No exception","","","def test_validate_grid_for_param_enum_invalid():","    \"\"\"Enum value not in choices raises.\"\"\"","    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=\"invalid\")","    with pytest.raises(ValueError, match=\"not in choices\"):","        validate_grid_for_param(spec, \"enum\", choices=[\"buy\", \"sell\"])","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])","",""]}
{"type":"file_footer","path":"tests/test_phase13_param_grid.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase141_batch_status_summary.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4224,"sha256":"6bd14a2e209af2335351359162cd2335e646a957a3b2d2685384ce1f7ccd18a8","total_lines":124,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase141_batch_status_summary.py","chunk_index":0,"line_start":1,"line_end":124,"content":["","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","","","@pytest.fixture","def client():","    return TestClient(app)","","","def _write_json(p: Path, obj):","    p.parent.mkdir(parents=True, exist_ok=True)","    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")","","","def test_batch_status_reads_execution_json(client):","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        batch_id = \"batch1\"","","        # execution schema: jobs mapping","        _write_json(","            root / batch_id / \"execution.json\",","            {","                \"batch_state\": \"RUNNING\",","                \"jobs\": {","                    \"jobA\": {\"state\": \"SUCCESS\"},","                    \"jobB\": {\"state\": \"FAILED\"},","                    \"jobC\": {\"state\": \"RUNNING\"},","                },","            },","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            r = client.get(f\"/batches/{batch_id}/status\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"batch_id\"] == batch_id","            assert data[\"state\"] == \"RUNNING\"","            assert data[\"jobs_total\"] == 3","            assert data[\"jobs_done\"] == 1","            assert data[\"jobs_failed\"] == 1","","","def test_batch_status_missing_execution_json(client):","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            r = client.get(\"/batches/batchX/status\")","            assert r.status_code == 404","","","def test_batch_summary_reads_summary_json(client):","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        batch_id = \"batch1\"","        _write_json(","            root / batch_id / \"summary.json\",","            {\"topk\": [{\"job_id\": \"jobA\", \"score\": 1.23}], \"metrics\": {\"n\": 10}},","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            r = client.get(f\"/batches/{batch_id}/summary\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"batch_id\"] == batch_id","            assert isinstance(data[\"topk\"], list)","            assert data[\"topk\"][0][\"job_id\"] == \"jobA\"","            assert data[\"metrics\"][\"n\"] == 10","","","def test_batch_summary_missing_summary_json(client):","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            r = client.get(\"/batches/batchX/summary\")","            assert r.status_code == 404","","","def test_batch_index_endpoint(client):","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        batch_id = \"batch1\"","        _write_json(root / batch_id / \"index.json\", {\"batch_id\": batch_id, \"jobs\": [\"jobA\", \"jobB\"]})","","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            r = client.get(f\"/batches/{batch_id}/index\")","            assert r.status_code == 200","            assert r.json()[\"batch_id\"] == batch_id","","","def test_batch_artifacts_listing(client):","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        batch_id = \"batch1\"","","        # artifacts tree","        _write_json(","            root / batch_id / \"jobA\" / \"attempt_1\" / \"manifest.json\",","            {\"job_id\": \"jobA\", \"score\": 2.0},","        )","        _write_json(","            root / batch_id / \"jobA\" / \"attempt_2\" / \"manifest.json\",","            {\"job_id\": \"jobA\", \"metrics\": {\"score\": 3.0}},","        )","        (root / batch_id / \"jobB\" / \"attempt_1\").mkdir(parents=True, exist_ok=True)  # no manifest ok","","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            r = client.get(f\"/batches/{batch_id}/artifacts\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"batch_id\"] == batch_id","            assert [j[\"job_id\"] for j in data[\"jobs\"]] == [\"jobA\", \"jobB\"]","            jobA = data[\"jobs\"][0]","            assert [a[\"attempt\"] for a in jobA[\"attempts\"]] == [1, 2]","",""]}
{"type":"file_footer","path":"tests/test_phase141_batch_status_summary.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase14_api_batches.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7351,"sha256":"64637eaa919d9ba45f1dfe0563f451bd06eafc2ecdca501830aef17466ddc1ce","total_lines":215,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_phase14_api_batches.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Phase 14: API batch endpoints tests.\"\"\"","","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","","","@pytest.fixture","def client():","    \"\"\"FastAPI test client.\"\"\"","    return TestClient(app)","","","@pytest.fixture","def mock_governance_store():","    \"\"\"Mock governance store.","","    NOTE:","    Governance store now uses artifacts root and stores metadata at:","      artifacts/{batch_id}/metadata.json","    \"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        artifacts_root = Path(tmpdir) / \"artifacts\"","        artifacts_root.mkdir(parents=True, exist_ok=True)","","        with patch(\"control.api._get_artifacts_root\") as mock_root, \\","             patch(\"control.api._get_governance_store\") as mock_store:","            from control.governance import BatchGovernanceStore","            real_store = BatchGovernanceStore(artifacts_root)","            mock_root.return_value = artifacts_root","            mock_store.return_value = real_store","            yield real_store","","","def test_get_batch_metadata(client, mock_governance_store):","    \"\"\"GET /batches/{batch_id}/metadata returns metadata.\"\"\"","    # Create metadata","    from control.governance import BatchMetadata","    meta = BatchMetadata(","        batch_id=\"batch1\",","        season=\"2026Q1\",","        tags=[\"test\"],","        note=\"hello\",","        frozen=False,","        created_at=\"2025-01-01T00:00:00Z\",","        updated_at=\"2025-01-01T00:00:00Z\",","        created_by=\"system\",","    )","    mock_governance_store.set_metadata(\"batch1\", meta)","","    response = client.get(\"/batches/batch1/metadata\")","    assert response.status_code == 200","    data = response.json()","    assert data[\"batch_id\"] == \"batch1\"","    assert data[\"season\"] == \"2026Q1\"","    assert data[\"tags\"] == [\"test\"]","    assert data[\"note\"] == \"hello\"","    assert data[\"frozen\"] is False","","","def test_get_batch_metadata_not_found(client, mock_governance_store):","    \"\"\"GET /batches/{batch_id}/metadata returns 404 if not found.\"\"\"","    response = client.get(\"/batches/nonexistent/metadata\")","    assert response.status_code == 404","    assert \"not found\" in response.json()[\"detail\"].lower()","","","def test_update_batch_metadata(client, mock_governance_store):","    \"\"\"PATCH /batches/{batch_id}/metadata updates metadata.\"\"\"","    # First create","    from control.governance import BatchMetadata","    meta = BatchMetadata(","        batch_id=\"batch1\",","        season=\"2026Q1\",","        tags=[],","        note=\"\",","        frozen=False,","        created_at=\"2025-01-01T00:00:00Z\",","        updated_at=\"2025-01-01T00:00:00Z\",","        created_by=\"system\",","    )","    mock_governance_store.set_metadata(\"batch1\", meta)","","    # Update","    update = {\"season\": \"2026Q2\", \"tags\": [\"newtag\"], \"note\": \"updated\"}","    response = client.patch(\"/batches/batch1/metadata\", json=update)","    assert response.status_code == 200","    data = response.json()","    assert data[\"season\"] == \"2026Q2\"","    assert data[\"tags\"] == [\"newtag\"]","    assert data[\"note\"] == \"updated\"","    assert data[\"frozen\"] is False","    assert data[\"updated_at\"] != \"2025-01-01T00:00:00Z\"  # timestamp updated","","","def test_update_batch_metadata_frozen_restrictions(client, mock_governance_store):","    \"\"\"PATCH respects frozen rules.\"\"\"","    # Create frozen batch","    from control.governance import BatchMetadata","    meta = BatchMetadata(","        batch_id=\"frozenbatch\",","        season=\"2026Q1\",","        tags=[],","        note=\"\",","        frozen=True,","        created_at=\"2025-01-01T00:00:00Z\",","        updated_at=\"2025-01-01T00:00:00Z\",","        created_by=\"system\",","    )","    mock_governance_store.set_metadata(\"frozenbatch\", meta)","","    # Attempt to change season -> 400","    response = client.patch(\"/batches/frozenbatch/metadata\", json={\"season\": \"2026Q2\"})","    assert response.status_code == 400","    assert \"Cannot change season\" in response.json()[\"detail\"]","","    # Attempt to unfreeze -> 400","    response = client.patch(\"/batches/frozenbatch/metadata\", json={\"frozen\": False})","    assert response.status_code == 400","    assert \"Cannot unfreeze\" in response.json()[\"detail\"]","","    # Append tags should work","    response = client.patch(\"/batches/frozenbatch/metadata\", json={\"tags\": [\"newtag\"]})","    assert response.status_code == 200","    data = response.json()","    assert \"newtag\" in data[\"tags\"]","","    # Update note should work","    response = client.patch(\"/batches/frozenbatch/metadata\", json={\"note\": \"updated\"})","    assert response.status_code == 200","    assert response.json()[\"note\"] == \"updated\"","","","def test_freeze_batch(client, mock_governance_store):","    \"\"\"POST /batches/{batch_id}/freeze freezes batch.\"\"\"","    # Create unfrozen batch","    from control.governance import BatchMetadata","    meta = BatchMetadata(","        batch_id=\"batch1\",","        season=\"2026Q1\",","        tags=[],","        note=\"\",","        frozen=False,","        created_at=\"2025-01-01T00:00:00Z\",","        updated_at=\"2025-01-01T00:00:00Z\",","        created_by=\"system\",","    )","    mock_governance_store.set_metadata(\"batch1\", meta)","","    response = client.post(\"/batches/batch1/freeze\")","    assert response.status_code == 200","    data = response.json()","    assert data[\"status\"] == \"frozen\"","    assert data[\"batch_id\"] == \"batch1\"","","    # Verify frozen","    assert mock_governance_store.is_frozen(\"batch1\") is True","","","def test_freeze_batch_not_found(client, mock_governance_store):","    \"\"\"POST /batches/{batch_id}/freeze returns 404 if batch not found.\"\"\"","    response = client.post(\"/batches/nonexistent/freeze\")","    assert response.status_code == 404","","","def test_retry_batch_frozen(client, mock_governance_store):","    \"\"\"POST /batches/{batch_id}/retry rejects frozen batch.\"\"\"","    # Create frozen batch","    from control.governance import BatchMetadata","    meta = BatchMetadata(","        batch_id=\"frozenbatch\",","        season=\"2026Q1\",","        tags=[],","        note=\"\",","        frozen=True,","        created_at=\"2025-01-01T00:00:00Z\",","        updated_at=\"2025-01-01T00:00:00Z\",","        created_by=\"system\",","    )","    mock_governance_store.set_metadata(\"frozenbatch\", meta)","","    response = client.post(\"/batches/frozenbatch/retry\", json={\"force\": False})","    assert response.status_code == 403","    assert \"frozen\" in response.json()[\"detail\"].lower()","","","def test_batch_status_not_implemented(client):","    \"\"\"GET /batches/{batch_id}/status returns 404 when execution.json missing.\"\"\"","    # Mock artifacts root to return a path that doesn't have execution.json","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        root.mkdir(parents=True, exist_ok=True)","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            response = client.get(\"/batches/batch1/status\")"]}
{"type":"file_chunk","path":"tests/test_phase14_api_batches.py","chunk_index":1,"line_start":201,"line_end":215,"content":["            assert response.status_code == 404","            assert \"execution.json not found\" in response.json()[\"detail\"]","","","def test_batch_summary_not_implemented(client):","    \"\"\"GET /batches/{batch_id}/summary returns 404 when summary.json missing.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        root = Path(tmp) / \"artifacts\"","        root.mkdir(parents=True, exist_ok=True)","        with patch(\"control.api._get_artifacts_root\", return_value=root):","            response = client.get(\"/batches/batch1/summary\")","            assert response.status_code == 404","            assert \"summary.json not found\" in response.json()[\"detail\"]","",""]}
{"type":"file_footer","path":"tests/test_phase14_api_batches.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_phase14_artifacts.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2569,"sha256":"9cd4b79f7ffbf55042fc6326d3ec3cb43b9e7deb3f69196355cd7ce322e0eb9b","total_lines":88,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase14_artifacts.py","chunk_index":0,"line_start":1,"line_end":88,"content":["","\"\"\"Phase 14: Artifacts module tests.\"\"\"","","import json","import tempfile","from pathlib import Path","","from control.artifacts import (","    canonical_json_bytes,","    compute_sha256,","    write_atomic_json,","    build_job_manifest,",")","","","def test_canonical_json_bytes_deterministic():","    \"\"\"Canonical JSON must be deterministic regardless of dict order.\"\"\"","    obj1 = {\"a\": 1, \"b\": 2, \"c\": [3, 4]}","    obj2 = {\"c\": [3, 4], \"b\": 2, \"a\": 1}","    ","    bytes1 = canonical_json_bytes(obj1)","    bytes2 = canonical_json_bytes(obj2)","    ","    assert bytes1 == bytes2","    # Ensure no extra whitespace","    decoded = json.loads(bytes1.decode(\"utf-8\"))","    assert decoded == obj1","","","def test_canonical_json_bytes_unicode():","    \"\"\"Canonical JSON handles Unicode characters.\"\"\"","    obj = {\"name\": \"測試\", \"value\": \"🎯\"}","    bytes_out = canonical_json_bytes(obj)","    decoded = json.loads(bytes_out.decode(\"utf-8\"))","    assert decoded == obj","","","def test_compute_sha256():","    \"\"\"SHA256 hash matches known value.\"\"\"","    data = b\"hello world\"","    hash_hex = compute_sha256(data)","    # Expected SHA256 of \"hello world\"","    expected = \"b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9\"","    assert hash_hex == expected","","","def test_write_atomic_json():","    \"\"\"Atomic write creates file with correct content.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        path = Path(tmpdir) / \"test.json\"","        obj = {\"x\": 42, \"y\": \"text\"}","        ","        write_atomic_json(path, obj)","        ","        assert path.exists()","        content = json.loads(path.read_text(encoding=\"utf-8\"))","        assert content == obj","","","def test_build_job_manifest():","    \"\"\"Job manifest includes required fields.\"\"\"","    job_spec = {","        \"season\": \"2026Q1\",","        \"dataset_id\": \"CME_MNQ_v2\",","        \"outputs_root\": \"/tmp/outputs\",","        \"config_snapshot\": {\"param\": 1.0},","        \"config_hash\": \"abc123\",","        \"created_by\": \"test\",","    }","    job_id = \"job-123\"","    ","    manifest = build_job_manifest(job_spec, job_id)","    ","    assert manifest[\"job_id\"] == job_id","    assert manifest[\"season\"] == job_spec[\"season\"]","    assert manifest[\"dataset_id\"] == job_spec[\"dataset_id\"]","    assert manifest[\"config_hash\"] == job_spec[\"config_hash\"]","    assert \"created_at\" in manifest","    assert \"manifest_hash\" in manifest","    ","    # Verify manifest_hash is SHA256 of canonical JSON","    import copy","    manifest_copy = copy.deepcopy(manifest)","    expected_hash = manifest_copy.pop(\"manifest_hash\")","    computed = compute_sha256(canonical_json_bytes(manifest_copy))","    assert expected_hash == computed","",""]}
{"type":"file_footer","path":"tests/test_phase14_artifacts.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase14_batch_aggregate.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3502,"sha256":"4c7c8d41b3fe65fec48f4b85ca434d13bcda331f3cff070329019bd1d5cceedc","total_lines":110,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase14_batch_aggregate.py","chunk_index":0,"line_start":1,"line_end":110,"content":["","\"\"\"Phase 14: Batch aggregation tests.\"\"\"","","import tempfile","from pathlib import Path","","from control.batch_aggregate import compute_batch_summary","from control.artifacts import canonical_json_bytes, compute_sha256","","","def test_compute_batch_summary_topk():","    \"\"\"Batch summary selects top K jobs by score.\"\"\"","    job_entries = [","        {\"job_id\": \"job1\", \"score\": 0.1},","        {\"job_id\": \"job2\", \"score\": 0.9},","        {\"job_id\": \"job3\", \"score\": 0.5},","        {\"job_id\": \"job4\", \"score\": 0.7},","        {\"job_id\": \"job5\", \"score\": 0.3},","    ]","    ","    summary = compute_batch_summary(job_entries, top_k=3)","    ","    assert summary[\"total_jobs\"] == 5","    assert len(summary[\"top_k\"]) == 3","    # Should be sorted descending by score","    assert [e[\"job_id\"] for e in summary[\"top_k\"]] == [\"job2\", \"job4\", \"job3\"]","    assert [e[\"score\"] for e in summary[\"top_k\"]] == [0.9, 0.7, 0.5]","    ","    # Stats should contain counts","    stats = summary[\"stats\"]","    assert stats[\"count\"] == 5","    assert \"mean_score\" in stats","    assert \"median_score\" in stats","    assert \"std_score\" in stats","    ","    # summary_hash should be SHA256 of canonical JSON of summary without hash","    import copy","    summary_copy = copy.deepcopy(summary)","    expected_hash = summary_copy.pop(\"summary_hash\")","    computed = compute_sha256(canonical_json_bytes(summary_copy))","    assert expected_hash == computed","","","def test_compute_batch_summary_no_score():","    \"\"\"Batch summary uses job_id ordering when score missing.\"\"\"","    job_entries = [","        {\"job_id\": \"jobC\", \"config\": {\"x\": 1}},","        {\"job_id\": \"jobA\", \"config\": {\"x\": 2}},","        {\"job_id\": \"jobB\", \"config\": {\"x\": 3}},","    ]","    ","    summary = compute_batch_summary(job_entries, top_k=2)","    ","    # Top K by job_id alphabetical","    assert [e[\"job_id\"] for e in summary[\"top_k\"]] == [\"jobA\", \"jobB\"]","    ","    # Stats should not contain score statistics","    stats = summary[\"stats\"]","    assert stats[\"count\"] == 3","    assert \"mean_score\" not in stats","    assert \"median_score\" not in stats","    assert \"std_score\" not in stats","","","def test_compute_batch_summary_empty():","    \"\"\"Batch summary handles empty job list.\"\"\"","    summary = compute_batch_summary([], top_k=5)","    ","    assert summary[\"total_jobs\"] == 0","    assert summary[\"top_k\"] == []","    stats = summary[\"stats\"]","    assert stats[\"count\"] == 0","    assert \"mean_score\" not in stats","","","def test_compute_batch_summary_k_larger_than_total():","    \"\"\"Top K larger than total jobs returns all jobs.\"\"\"","    job_entries = [","        {\"job_id\": \"job1\", \"score\": 0.5},","        {\"job_id\": \"job2\", \"score\": 0.8},","    ]","    ","    summary = compute_batch_summary(job_entries, top_k=10)","    ","    assert len(summary[\"top_k\"]) == 2","    assert [e[\"job_id\"] for e in summary[\"top_k\"]] == [\"job2\", \"job1\"]","","","def test_compute_batch_summary_deterministic():","    \"\"\"Summary is deterministic regardless of input order.\"\"\"","    job_entries1 = [","        {\"job_id\": \"job1\", \"score\": 0.5},","        {\"job_id\": \"job2\", \"score\": 0.8},","    ]","    job_entries2 = [","        {\"job_id\": \"job2\", \"score\": 0.8},","        {\"job_id\": \"job1\", \"score\": 0.5},","    ]","    ","    summary1 = compute_batch_summary(job_entries1, top_k=5)","    summary2 = compute_batch_summary(job_entries2, top_k=5)","    ","    # Top K order should be same (descending score)","    assert summary1[\"top_k\"] == summary2[\"top_k\"]","    # Stats should be identical","    assert summary1[\"stats\"] == summary2[\"stats\"]","    # Hash should match","    assert summary1[\"summary_hash\"] == summary2[\"summary_hash\"]","",""]}
{"type":"file_footer","path":"tests/test_phase14_batch_aggregate.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase14_batch_execute.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4513,"sha256":"469be8b4387b47282f2829b81463b287dfce9afb0f4e8023432cf6ff36a1198d","total_lines":132,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase14_batch_execute.py","chunk_index":0,"line_start":1,"line_end":132,"content":["","\"\"\"Phase 14: Batch execution tests.\"\"\"","","import tempfile","from pathlib import Path","from unittest.mock import Mock, patch","","from control.batch_execute import (","    BatchExecutor,","    BatchExecutionState,","    JobExecutionState,","    run_batch,","    retry_failed,",")","","","def test_batch_execution_state_enum():","    \"\"\"Batch execution state enum values.\"\"\"","    assert BatchExecutionState.PENDING.value == \"PENDING\"","    assert BatchExecutionState.RUNNING.value == \"RUNNING\"","    assert BatchExecutionState.DONE.value == \"DONE\"","    assert BatchExecutionState.FAILED.value == \"FAILED\"","    assert BatchExecutionState.PARTIAL_FAILED.value == \"PARTIAL_FAILED\"","","","def test_job_execution_state_enum():","    \"\"\"Job execution state enum values.\"\"\"","    assert JobExecutionState.PENDING.value == \"PENDING\"","    assert JobExecutionState.RUNNING.value == \"RUNNING\"","    assert JobExecutionState.SUCCESS.value == \"SUCCESS\"","    assert JobExecutionState.FAILED.value == \"FAILED\"","    assert JobExecutionState.SKIPPED.value == \"SKIPPED\"","","","def test_batch_executor_initial_state():","    \"\"\"BatchExecutor initializes with correct state.\"\"\"","    batch_id = \"batch-123\"","    job_ids = [\"job1\", \"job2\", \"job3\"]","    ","    executor = BatchExecutor(batch_id, job_ids)","    ","    assert executor.batch_id == batch_id","    assert executor.job_ids == job_ids","    assert executor.state == BatchExecutionState.PENDING","    assert executor.job_states == {","        \"job1\": JobExecutionState.PENDING,","        \"job2\": JobExecutionState.PENDING,","        \"job3\": JobExecutionState.PENDING,","    }","    assert executor.created_at is not None","    assert executor.updated_at is not None","","","def test_batch_executor_transition():","    \"\"\"BatchExecutor transitions state based on job states.\"\"\"","    executor = BatchExecutor(\"batch\", [\"job1\", \"job2\"])","    ","    # Initially PENDING","    assert executor.state == BatchExecutionState.PENDING","    ","    # Start first job -> RUNNING","    executor._set_job_state(\"job1\", JobExecutionState.RUNNING)","    assert executor.state == BatchExecutionState.RUNNING","    ","    # Finish first job successfully, second still pending -> RUNNING","    executor._set_job_state(\"job1\", JobExecutionState.SUCCESS)","    assert executor.state == BatchExecutionState.RUNNING","    ","    # Start second job -> RUNNING","    executor._set_job_state(\"job2\", JobExecutionState.RUNNING)","    assert executor.state == BatchExecutionState.RUNNING","    ","    # Finish second job successfully -> DONE","    executor._set_job_state(\"job2\", JobExecutionState.SUCCESS)","    assert executor.state == BatchExecutionState.DONE","    ","    # If one job fails -> PARTIAL_FAILED","    executor._set_job_state(\"job1\", JobExecutionState.FAILED)","    executor._set_job_state(\"job2\", JobExecutionState.SUCCESS)","    executor._recompute_state()","    assert executor.state == BatchExecutionState.PARTIAL_FAILED","    ","    # If all jobs fail -> FAILED","    executor._set_job_state(\"job2\", JobExecutionState.FAILED)","    executor._recompute_state()","    assert executor.state == BatchExecutionState.FAILED","","","def test_batch_executor_skipped_jobs():","    \"\"\"SKIPPED jobs count as completed for state computation.\"\"\"","    executor = BatchExecutor(\"batch\", [\"job1\", \"job2\"])","    ","    executor._set_job_state(\"job1\", JobExecutionState.SUCCESS)","    executor._set_job_state(\"job2\", JobExecutionState.SKIPPED)","    ","    # Both jobs are completed (SUCCESS + SKIPPED) -> DONE","    assert executor.state == BatchExecutionState.DONE","","","@patch(\"control.batch_execute.BatchExecutor\")","def test_run_batch_mock(mock_executor_cls):","    \"\"\"run_batch creates executor and runs jobs.\"\"\"","    mock_executor = Mock()","    mock_executor_cls.return_value = mock_executor","    ","    batch_id = \"batch-test\"","    job_ids = [\"job1\", \"job2\"]","    artifacts_root = Path(\"/tmp/artifacts\")","    ","    result = run_batch(batch_id, job_ids, artifacts_root)","    ","    mock_executor_cls.assert_called_once_with(batch_id, job_ids)","    mock_executor.run.assert_called_once_with(artifacts_root)","    assert result == mock_executor","","","@patch(\"control.batch_execute.BatchExecutor\")","def test_retry_failed_mock(mock_executor_cls):","    \"\"\"retry_failed creates executor and retries failed jobs.\"\"\"","    mock_executor = Mock()","    mock_executor_cls.return_value = mock_executor","    ","    batch_id = \"batch-retry\"","    artifacts_root = Path(\"/tmp/artifacts\")","    ","    result = retry_failed(batch_id, artifacts_root)","    ","    mock_executor_cls.assert_called_once_with(batch_id, [])","    mock_executor.retry_failed.assert_called_once_with(artifacts_root)","    assert result == mock_executor","",""]}
{"type":"file_footer","path":"tests/test_phase14_batch_execute.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase14_batch_index.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3631,"sha256":"d7ded17228f43b758c9430482492e0935bcfb7bf9599cea1c40425330db32a9a","total_lines":87,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase14_batch_index.py","chunk_index":0,"line_start":1,"line_end":87,"content":["","\"\"\"Phase 14: Batch index tests.\"\"\"","","import json","import tempfile","from pathlib import Path","","from control.batch_index import build_batch_index","from control.artifacts import canonical_json_bytes, compute_sha256","","","def test_build_batch_index_deterministic():","    \"\"\"Batch index is deterministic regardless of job entry order.\"\"\"","    job_entries = [","        {\"job_id\": \"job1\", \"score\": 0.5, \"manifest_hash\": \"abc123\", \"manifest_path\": \"batch-123/job1/manifest.json\"},","        {\"job_id\": \"job2\", \"score\": 0.3, \"manifest_hash\": \"def456\", \"manifest_path\": \"batch-123/job2/manifest.json\"},","        {\"job_id\": \"job3\", \"score\": 0.8, \"manifest_hash\": \"ghi789\", \"manifest_path\": \"batch-123/job3/manifest.json\"},","    ]","    job_entries_shuffled = [","        {\"job_id\": \"job3\", \"score\": 0.8, \"manifest_hash\": \"ghi789\", \"manifest_path\": \"batch-123/job3/manifest.json\"},","        {\"job_id\": \"job1\", \"score\": 0.5, \"manifest_hash\": \"abc123\", \"manifest_path\": \"batch-123/job1/manifest.json\"},","        {\"job_id\": \"job2\", \"score\": 0.3, \"manifest_hash\": \"def456\", \"manifest_path\": \"batch-123/job2/manifest.json\"},","    ]","    ","    with tempfile.TemporaryDirectory() as tmpdir:","        artifacts_root = Path(tmpdir)","        batch_id = \"batch-123\"","        ","        index1 = build_batch_index(artifacts_root, batch_id, job_entries)","        index2 = build_batch_index(artifacts_root, batch_id, job_entries_shuffled)","        ","        # Index should be identical (entries sorted by job_id)","        assert index1 == index2","        ","        # Verify structure","        assert index1[\"batch_id\"] == batch_id","        assert index1[\"job_count\"] == 3","        assert len(index1[\"jobs\"]) == 3","        # Entries should be sorted by job_id","        assert [e[\"job_id\"] for e in index1[\"jobs\"]] == [\"job1\", \"job2\", \"job3\"]","        ","        # Verify index_hash is SHA256 of canonical JSON of index without hash","        import copy","        index_copy = copy.deepcopy(index1)","        expected_hash = index_copy.pop(\"index_hash\")","        computed = compute_sha256(canonical_json_bytes(index_copy))","        assert expected_hash == computed","","","def test_build_batch_index_without_score():","    \"\"\"Batch index works when jobs have no score field.\"\"\"","    job_entries = [","        {\"job_id\": \"jobA\", \"config\": {\"x\": 1}, \"manifest_hash\": \"hashA\", \"manifest_path\": \"batch-no-score/jobA/manifest.json\"},","        {\"job_id\": \"jobB\", \"config\": {\"x\": 2}, \"manifest_hash\": \"hashB\", \"manifest_path\": \"batch-no-score/jobB/manifest.json\"},","    ]","    ","    with tempfile.TemporaryDirectory() as tmpdir:","        artifacts_root = Path(tmpdir)","        batch_id = \"batch-no-score\"","        ","        index = build_batch_index(artifacts_root, batch_id, job_entries)","        ","        assert index[\"batch_id\"] == batch_id","        assert index[\"job_count\"] == 2","        # Entries sorted by job_id","        assert [e[\"job_id\"] for e in index[\"jobs\"]] == [\"jobA\", \"jobB\"]","","","def test_build_batch_index_writes_file():","    \"\"\"Batch index writes index.json to artifacts directory.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        artifacts_root = Path(tmpdir)","        batch_id = \"batch-write\"","        job_entries = [{\"job_id\": \"job1\", \"manifest_hash\": \"hash1\", \"manifest_path\": \"batch-write/job1/manifest.json\"}]","        ","        index = build_batch_index(artifacts_root, batch_id, job_entries)","        ","        # Check file exists","        batch_dir = artifacts_root / batch_id","        index_file = batch_dir / \"index.json\"","        assert index_file.exists()","        ","        # Content matches returned index","        loaded = json.loads(index_file.read_text(encoding=\"utf-8\"))","        assert loaded == index","",""]}
{"type":"file_footer","path":"tests/test_phase14_batch_index.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase14_governance.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5716,"sha256":"489d6dd71f475c385815b0d02689f92ecd1514f008196f233325fd448af59f9c","total_lines":166,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase14_governance.py","chunk_index":0,"line_start":1,"line_end":166,"content":["","\"\"\"Phase 14: Governance tests.\"\"\"","","import tempfile","from pathlib import Path","","from control.governance import (","    BatchGovernanceStore,","    BatchMetadata,",")","","","def test_batch_metadata_creation():","    \"\"\"BatchMetadata can be created with defaults.\"\"\"","    meta = BatchMetadata(batch_id=\"batch1\", season=\"2026Q1\", tags=[\"test\"], note=\"hello\")","    assert meta.batch_id == \"batch1\"","    assert meta.season == \"2026Q1\"","    assert meta.tags == [\"test\"]","    assert meta.note == \"hello\"","    assert meta.frozen is False","    assert meta.created_at == \"\"","    assert meta.updated_at == \"\"","","","def test_batch_governance_store_init():","    \"\"\"Store creates directory if not exists.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        store_root = Path(tmpdir) / \"artifacts\"","        store = BatchGovernanceStore(store_root)","        assert store.artifacts_root.exists()","        assert store.artifacts_root.is_dir()","","","def test_batch_governance_store_set_get():","    \"\"\"Store can set and retrieve metadata.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        store_root = Path(tmpdir) / \"artifacts\"","        store = BatchGovernanceStore(store_root)","","        meta = BatchMetadata(","            batch_id=\"batch1\",","            season=\"2026Q1\",","            tags=[\"tag1\", \"tag2\"],","            note=\"test note\",","            frozen=False,","            created_at=\"2025-01-01T00:00:00Z\",","            updated_at=\"2025-01-01T00:00:00Z\",","            created_by=\"user\",","        )","","        store.set_metadata(\"batch1\", meta)","","        retrieved = store.get_metadata(\"batch1\")","        assert retrieved is not None","        assert retrieved.batch_id == meta.batch_id","        assert retrieved.season == meta.season","        assert retrieved.tags == meta.tags","        assert retrieved.note == meta.note","        assert retrieved.frozen == meta.frozen","        assert retrieved.created_at == meta.created_at","        assert retrieved.updated_at == meta.updated_at","        assert retrieved.created_by == meta.created_by","","","def test_batch_governance_store_update_metadata_new():","    \"\"\"Update metadata creates new metadata if not exists.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        store_root = Path(tmpdir) / \"artifacts\"","        store = BatchGovernanceStore(store_root)","","        meta = store.update_metadata(","            \"newbatch\",","            season=\"2026Q2\",","            tags=[\"new\"],","            note=\"created\",","        )","","        assert meta.batch_id == \"newbatch\"","        assert meta.season == \"2026Q2\"","        assert meta.tags == [\"new\"]","        assert meta.note == \"created\"","        assert meta.frozen is False","        assert meta.created_at != \"\"","        assert meta.updated_at != \"\"","","","def test_batch_governance_store_update_metadata_frozen_rules():","    \"\"\"Frozen batch restricts updates.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        store_root = Path(tmpdir) / \"artifacts\"","        store = BatchGovernanceStore(store_root)","","        # Create a frozen batch","        meta = store.update_metadata(\"frozenbatch\", season=\"2026Q1\", frozen=True)","        assert meta.frozen is True","","        import pytest","        # Attempt to change season -> should raise","        with pytest.raises(ValueError, match=\"Cannot change season of frozen batch\"):","            store.update_metadata(\"frozenbatch\", season=\"2026Q2\")","","        # Attempt to unfreeze -> should raise","        with pytest.raises(ValueError, match=\"Cannot unfreeze a frozen batch\"):","            store.update_metadata(\"frozenbatch\", frozen=False)","","        # Append tags should work","        meta2 = store.update_metadata(\"frozenbatch\", tags=[\"newtag\"])","        assert \"newtag\" in meta2.tags","        assert meta2.season == \"2026Q1\"  # unchanged","","        # Update note should work","        meta3 = store.update_metadata(\"frozenbatch\", note=\"updated note\")","        assert meta3.note == \"updated note\"","","        # Setting frozen=True again is no-op","        meta4 = store.update_metadata(\"frozenbatch\", frozen=True)","        assert meta4.frozen is True","","","def test_batch_governance_store_freeze():","    \"\"\"Freeze method sets frozen flag.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        store_root = Path(tmpdir) / \"artifacts\"","        store = BatchGovernanceStore(store_root)","","        store.update_metadata(\"batch1\", season=\"2026Q1\")","        assert store.is_frozen(\"batch1\") is False","","        store.freeze(\"batch1\")","        assert store.is_frozen(\"batch1\") is True","","        # Freeze again is idempotent","        store.freeze(\"batch1\")","        assert store.is_frozen(\"batch1\") is True","","","def test_batch_governance_store_list_batches():","    \"\"\"List batches with filters.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        store_root = Path(tmpdir) / \"artifacts\"","        store = BatchGovernanceStore(store_root)","","        store.update_metadata(\"batch1\", season=\"2026Q1\", tags=[\"a\", \"b\"])","        store.update_metadata(\"batch2\", season=\"2026Q1\", tags=[\"b\", \"c\"], frozen=True)","        store.update_metadata(\"batch3\", season=\"2026Q2\", tags=[\"a\"])","","        # All batches","        all_batches = store.list_batches()","        assert len(all_batches) == 3","        ids = [m.batch_id for m in all_batches]","        assert sorted(ids) == [\"batch1\", \"batch2\", \"batch3\"]","","        # Filter by season","        season_batches = store.list_batches(season=\"2026Q1\")","        assert len(season_batches) == 2","        assert {m.batch_id for m in season_batches} == {\"batch1\", \"batch2\"}","","        # Filter by tag","        tag_batches = store.list_batches(tag=\"a\")","        assert {m.batch_id for m in tag_batches} == {\"batch1\", \"batch3\"}","","        # Filter by frozen","        frozen_batches = store.list_batches(frozen=True)","        assert {m.batch_id for m in frozen_batches} == {\"batch2\"}","",""]}
{"type":"file_footer","path":"tests/test_phase14_governance.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase150_season_index.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4832,"sha256":"ea4dc10939e7ae8ab52f088f7370b59ade187585de4807357a6413f104398283","total_lines":131,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase150_season_index.py","chunk_index":0,"line_start":1,"line_end":131,"content":["","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","","","@pytest.fixture","def client():","    return TestClient(app)","","","def _wjson(p: Path, obj):","    p.parent.mkdir(parents=True, exist_ok=True)","    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")","","","def test_rebuild_season_index_collects_batches_and_is_deterministic(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        # batch2 (lexicographically after batch1) — write first to verify sorting","        _wjson(","            artifacts_root / \"batch2\" / \"metadata.json\",","            {\"batch_id\": \"batch2\", \"season\": season, \"tags\": [\"b\", \"a\"], \"note\": \"n2\", \"frozen\": False},","        )","        _wjson(artifacts_root / \"batch2\" / \"index.json\", {\"x\": 1})","        _wjson(artifacts_root / \"batch2\" / \"summary.json\", {\"topk\": [], \"metrics\": {}})","","        # batch1","        _wjson(","            artifacts_root / \"batch1\" / \"metadata.json\",","            {\"batch_id\": \"batch1\", \"season\": season, \"tags\": [\"z\"], \"note\": \"n1\", \"frozen\": True},","        )","        _wjson(artifacts_root / \"batch1\" / \"index.json\", {\"y\": 2})","        _wjson(artifacts_root / \"batch1\" / \"summary.json\", {\"topk\": [{\"job_id\": \"j\", \"score\": 1.0}], \"metrics\": {\"n\": 1}})","","        # different season should be ignored","        _wjson(","            artifacts_root / \"batchX\" / \"metadata.json\",","            {\"batch_id\": \"batchX\", \"season\": \"2026Q2\", \"tags\": [\"ignore\"], \"note\": \"\", \"frozen\": False},","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.post(f\"/seasons/{season}/rebuild_index\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","            assert len(data[\"batches\"]) == 2","","            # deterministic order by batch_id","            assert [b[\"batch_id\"] for b in data[\"batches\"]] == [\"batch1\", \"batch2\"]","","            # tags dedupe+sort in index entries","            b2 = data[\"batches\"][1]","            assert b2[\"tags\"] == [\"a\", \"b\"]","","            # index file exists","            idx_path = season_root / season / \"season_index.json\"","            assert idx_path.exists()","","","def test_season_metadata_lifecycle_and_freeze_rules(client):","    with tempfile.TemporaryDirectory() as tmp:","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        with patch(\"control.api._get_season_index_root\", return_value=season_root):","            # metadata not exist -> 404","            r = client.get(f\"/seasons/{season}/metadata\")","            assert r.status_code == 404","","            # create/update metadata","            r = client.patch(f\"/seasons/{season}/metadata\", json={\"tags\": [\"core\", \"core\"], \"note\": \"hello\"})","            assert r.status_code == 200","            meta = r.json()","            assert meta[\"season\"] == season","            assert meta[\"tags\"] == [\"core\"]","            assert meta[\"note\"] == \"hello\"","            assert meta[\"frozen\"] is False","","            # freeze","            r = client.post(f\"/seasons/{season}/freeze\")","            assert r.status_code == 200","            assert r.json()[\"status\"] == \"frozen\"","","            # cannot unfreeze","            r = client.patch(f\"/seasons/{season}/metadata\", json={\"frozen\": False})","            assert r.status_code == 400","","            # tags/note still allowed","            r = client.patch(f\"/seasons/{season}/metadata\", json={\"tags\": [\"z\"], \"note\": \"n2\"})","            assert r.status_code == 200","            meta2 = r.json()","            assert meta2[\"tags\"] == [\"core\", \"z\"]","            assert meta2[\"note\"] == \"n2\"","            assert meta2[\"frozen\"] is True","","","def test_rebuild_index_forbidden_when_season_frozen(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        # prepare one batch","        _wjson(","            artifacts_root / \"batch1\" / \"metadata.json\",","            {\"batch_id\": \"batch1\", \"season\": season, \"tags\": [], \"note\": \"\", \"frozen\": False},","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","","            # freeze season first","            r = client.post(f\"/seasons/{season}/freeze\")","            assert r.status_code == 200","","            # rebuild should be forbidden","            r = client.post(f\"/seasons/{season}/rebuild_index\")","            assert r.status_code == 403","",""]}
{"type":"file_footer","path":"tests/test_phase150_season_index.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase151_season_compare_topk.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4664,"sha256":"3fc5ba4441c684f8289835633c0c4a3aad3524106ac4936f4093500b6e0b1f88","total_lines":134,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase151_season_compare_topk.py","chunk_index":0,"line_start":1,"line_end":134,"content":["","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","","","@pytest.fixture","def client():","    return TestClient(app)","","","def _wjson(p: Path, obj):","    p.parent.mkdir(parents=True, exist_ok=True)","    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")","","","def test_season_compare_topk_merge_and_tiebreak(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        # season index lists two batches","        _wjson(","            season_root / season / \"season_index.json\",","            {","                \"season\": season,","                \"generated_at\": \"2025-12-21T00:00:00Z\",","                \"batches\": [{\"batch_id\": \"batchA\"}, {\"batch_id\": \"batchB\"}],","            },","        )","","        # batchA summary","        _wjson(","            artifacts_root / \"batchA\" / \"summary.json\",","            {","                \"topk\": [","                    {\"job_id\": \"j2\", \"score\": 2.0},","                    {\"job_id\": \"j1\", \"score\": 2.0},  # tie on score, job_id decides inside same batch later","                    {\"job_id\": \"j0\", \"score\": 1.0},","                ],","                \"metrics\": {\"n\": 3},","            },","        )","","        # batchB summary (tie score with batchA to test tie-break by batch_id then job_id)","        _wjson(","            artifacts_root / \"batchB\" / \"summary.json\",","            {","                \"topk\": [","                    {\"job_id\": \"j9\", \"score\": 2.0},","                    {\"job_id\": \"j8\", \"score\": None},  # None goes last","                ],","                \"metrics\": {},","            },","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.get(f\"/seasons/{season}/compare/topk?k=10\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","            items = data[\"items\"]","","            # score desc, tie-break batch_id asc, tie-break job_id asc","            # score=2.0 items are: batchA j1/j2, batchB j9","            # batchA < batchB => all batchA first; within batchA j1 < j2","            assert [(x[\"batch_id\"], x[\"job_id\"], x[\"score\"]) for x in items[:3]] == [","                (\"batchA\", \"j1\", 2.0),","                (\"batchA\", \"j2\", 2.0),","                (\"batchB\", \"j9\", 2.0),","            ]","","            # None score should be at the end","            assert items[-1][\"score\"] is None","","","def test_season_compare_skips_missing_or_corrupt_summaries(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        _wjson(","            season_root / season / \"season_index.json\",","            {","                \"season\": season,","                \"generated_at\": \"2025-12-21T00:00:00Z\",","                \"batches\": [{\"batch_id\": \"batchOK\"}, {\"batch_id\": \"batchMissing\"}, {\"batch_id\": \"batchBad\"}],","            },","        )","","        _wjson(","            artifacts_root / \"batchOK\" / \"summary.json\",","            {\"topk\": [{\"job_id\": \"j1\", \"score\": 1.0}], \"metrics\": {}},","        )","","        # batchMissing -> no summary.json","","        # batchBad -> corrupt json","        bad_path = artifacts_root / \"batchBad\" / \"summary.json\"","        bad_path.parent.mkdir(parents=True, exist_ok=True)","        bad_path.write_text(\"{not-json\", encoding=\"utf-8\")","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.get(f\"/seasons/{season}/compare/topk?k=20\")","            assert r.status_code == 200","            data = r.json()","            assert [(x[\"batch_id\"], x[\"job_id\"]) for x in data[\"items\"]] == [(\"batchOK\", \"j1\")]","","            skipped = set(data[\"skipped_batches\"])","            assert \"batchMissing\" in skipped","            assert \"batchBad\" in skipped","","","def test_season_compare_404_when_season_index_missing(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.get(\"/seasons/NOPE/compare/topk?k=20\")","            assert r.status_code == 404","",""]}
{"type":"file_footer","path":"tests/test_phase151_season_compare_topk.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase152_season_compare_batches.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5637,"sha256":"4cc12233ac76daf6d4d7b7c369ca17172b3b35504b1474cf41ede9041b340ea1","total_lines":151,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase152_season_compare_batches.py","chunk_index":0,"line_start":1,"line_end":151,"content":["","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","","","@pytest.fixture","def client():","    return TestClient(app)","","","def _wjson(p: Path, obj):","    p.parent.mkdir(parents=True, exist_ok=True)","    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")","","","def test_compare_batches_cards_and_robust_summary(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        # season index includes 3 batches; ensure order is batchA, batchB, batchC","        _wjson(","            season_root / season / \"season_index.json\",","            {","                \"season\": season,","                \"generated_at\": \"2025-12-21T00:00:00Z\",","                \"batches\": [","                    {\"batch_id\": \"batchB\", \"frozen\": False, \"tags\": [\"b\"], \"note\": \"nB\", \"index_hash\": \"iB\", \"summary_hash\": \"sB\"},","                    {\"batch_id\": \"batchA\", \"frozen\": True, \"tags\": [\"a\"], \"note\": \"nA\", \"index_hash\": \"iA\", \"summary_hash\": \"sA\"},","                    {\"batch_id\": \"batchC\", \"frozen\": False, \"tags\": [], \"note\": \"\", \"index_hash\": None, \"summary_hash\": None},","                ],","            },","        )","","        # batchA: ok summary","        _wjson(","            artifacts_root / \"batchA\" / \"summary.json\",","            {\"topk\": [{\"job_id\": \"j1\", \"score\": 1.23}], \"metrics\": {\"n\": 1}},","        )","","        # batchB: corrupt summary","        p_bad = artifacts_root / \"batchB\" / \"summary.json\"","        p_bad.parent.mkdir(parents=True, exist_ok=True)","        p_bad.write_text(\"{not-json\", encoding=\"utf-8\")","","        # batchC: missing summary","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.get(f\"/seasons/{season}/compare/batches\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","","            batches = data[\"batches\"]","            assert [b[\"batch_id\"] for b in batches] == [\"batchA\", \"batchB\", \"batchC\"]","","            bA = batches[0]","            assert bA[\"summary_ok\"] is True","            assert bA[\"top_job_id\"] == \"j1\"","            assert bA[\"top_score\"] == 1.23","            assert bA[\"topk_size\"] == 1","","            bB = batches[1]","            assert bB[\"summary_ok\"] is False","","            bC = batches[2]","            assert bC[\"summary_ok\"] is False","            assert bC[\"topk_size\"] == 0","","            skipped = set(data[\"skipped_summaries\"])","            assert \"batchB\" in skipped","            assert \"batchC\" in skipped","","","def test_compare_leaderboard_grouping_and_determinism(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        season = \"2026Q1\"","","        _wjson(","            season_root / season / \"season_index.json\",","            {","                \"season\": season,","                \"generated_at\": \"2025-12-21T00:00:00Z\",","                \"batches\": [{\"batch_id\": \"batchA\"}, {\"batch_id\": \"batchB\"}],","            },","        )","","        # Include strategy_id and dataset_id in rows for grouping","        _wjson(","            artifacts_root / \"batchA\" / \"summary.json\",","            {","                \"topk\": [","                    {\"job_id\": \"a2\", \"score\": 2.0, \"strategy_id\": \"S1\"},","                    {\"job_id\": \"a1\", \"score\": 2.0, \"strategy_id\": \"S1\"},  # tie within same group","                    {\"job_id\": \"a0\", \"score\": 1.0, \"strategy_id\": \"S2\"},","                ]","            },","        )","        _wjson(","            artifacts_root / \"batchB\" / \"summary.json\",","            {","                \"topk\": [","                    {\"job_id\": \"b9\", \"score\": 2.0, \"strategy_id\": \"S1\"},","                    {\"job_id\": \"b8\", \"score\": None, \"strategy_id\": \"S1\"},","                ]","            },","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.get(f\"/seasons/{season}/compare/leaderboard?group_by=strategy_id&per_group=3\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","            assert data[\"group_by\"] == \"strategy_id\"","            assert data[\"per_group\"] == 3","","            groups = {g[\"key\"]: g[\"items\"] for g in data[\"groups\"]}","            assert \"S1\" in groups","            # Deterministic ordering inside group S1 by score desc, tie-break batch_id asc, job_id asc","            # score=2.0: batchA a1/a2, batchB b9 => batchA first; within batchA a1 < a2","            assert [(x[\"batch_id\"], x[\"job_id\"], x[\"score\"]) for x in groups[\"S1\"][:3]] == [","                (\"batchA\", \"a1\", 2.0),","                (\"batchA\", \"a2\", 2.0),","                (\"batchB\", \"b9\", 2.0),","            ]","","","def test_compare_endpoints_404_when_season_index_missing(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.get(\"/seasons/NOPE/compare/batches\")","            assert r.status_code == 404","            r = client.get(\"/seasons/NOPE/compare/leaderboard\")","            assert r.status_code == 404","",""]}
{"type":"file_footer","path":"tests/test_phase152_season_compare_batches.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase153_season_export.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4325,"sha256":"882959ebae5f1030db7c756a7d52d94f187a8cb8f547f4e2bf6b4efeee20feee","total_lines":109,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_phase153_season_export.py","chunk_index":0,"line_start":1,"line_end":109,"content":["","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","from control.artifacts import compute_sha256","","","@pytest.fixture","def client():","    return TestClient(app)","","","def _wjson(p: Path, obj):","    p.parent.mkdir(parents=True, exist_ok=True)","    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")","","","def test_export_requires_frozen_season(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","","        # season index exists","        _wjson(","            season_root / season / \"season_index.json\",","            {\"season\": season, \"generated_at\": \"Z\", \"batches\": []},","        )","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root), \\","             patch(\"control.season_export.get_exports_root\", return_value=exports_root):","            r = client.post(f\"/seasons/{season}/export\")","            assert r.status_code == 403","","","def test_export_builds_package_and_manifest_sha_matches(client):","    with tempfile.TemporaryDirectory() as tmp:","        artifacts_root = Path(tmp) / \"artifacts\"","        season_root = Path(tmp) / \"season_index\"","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","","        # create season index with 2 batches","        _wjson(","            season_root / season / \"season_index.json\",","            {","                \"season\": season,","                \"generated_at\": \"2025-12-21T00:00:00Z\",","                \"batches\": [{\"batch_id\": \"batchB\"}, {\"batch_id\": \"batchA\"}],","            },","        )","","        # create season metadata and freeze it","        # (use API to freeze for realism)","        with patch(\"control.api._get_season_index_root\", return_value=season_root):","            r = client.post(f\"/seasons/{season}/freeze\")","            assert r.status_code == 200","","        # artifacts files","        _wjson(artifacts_root / \"batchA\" / \"metadata.json\", {\"season\": season, \"frozen\": True, \"tags\": [\"a\"], \"note\": \"\"})","        _wjson(artifacts_root / \"batchA\" / \"index.json\", {\"x\": 1})","        _wjson(artifacts_root / \"batchA\" / \"summary.json\", {\"topk\": [{\"job_id\": \"j1\", \"score\": 1.0}], \"metrics\": {}})","","        _wjson(artifacts_root / \"batchB\" / \"metadata.json\", {\"season\": season, \"frozen\": False, \"tags\": [\"b\"], \"note\": \"n\"})","        _wjson(artifacts_root / \"batchB\" / \"index.json\", {\"y\": 2})","        # omit batchB summary.json to test missing files recorded","","        with patch(\"control.api._get_artifacts_root\", return_value=artifacts_root), \\","             patch(\"control.api._get_season_index_root\", return_value=season_root), \\","             patch(\"control.season_export.get_exports_root\", return_value=exports_root):","            r = client.post(f\"/seasons/{season}/export\")","            assert r.status_code == 200","            out = r.json()","","            export_dir = Path(out[\"export_dir\"])","            manifest_path = Path(out[\"manifest_path\"])","            assert export_dir.exists()","            assert manifest_path.exists()","","            # verify manifest sha matches actual bytes","            actual_sha = compute_sha256(manifest_path.read_bytes())","            assert out[\"manifest_sha256\"] == actual_sha","","            # verify key files copied","            assert (export_dir / \"season_index.json\").exists()","            # metadata may exist (freeze created it)","            assert (export_dir / \"season_metadata.json\").exists()","            assert (export_dir / \"batches\" / \"batchA\" / \"metadata.json\").exists()","            assert (export_dir / \"batches\" / \"batchA\" / \"index.json\").exists()","            assert (export_dir / \"batches\" / \"batchA\" / \"summary.json\").exists()","","            # batchB summary missing -> recorded","            assert \"batches/batchB/summary.json\" in out[\"missing_files\"]","","            # manifest contains file hashes","            man = json.loads(manifest_path.read_text(encoding=\"utf-8\"))","            assert man[\"season\"] == season","            assert \"files\" in man and isinstance(man[\"files\"], list)","            assert \"manifest_sha256\" in man","",""]}
{"type":"file_footer","path":"tests/test_phase153_season_export.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_phase16_export_replay.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":15844,"sha256":"96a2906ae9202c6631f1ad6d200cc069884f74b874b10b1335405ebd2aa55bb0","total_lines":450,"chunk_count":3}
{"type":"file_chunk","path":"tests/test_phase16_export_replay.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 16: Export Pack Replay Mode regression tests.","","Tests that exported season packages can be replayed without artifacts.","\"\"\"","","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","from fastapi.testclient import TestClient","","from control.api import app","from control.season_export_replay import (","    load_replay_index,","    replay_season_topk,","    replay_season_batch_cards,","    replay_season_leaderboard,",")","","","@pytest.fixture","def client():","    return TestClient(app)","","","def _wjson(p: Path, obj):","    p.parent.mkdir(parents=True, exist_ok=True)","    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding=\"utf-8\")","","","def test_load_replay_index():","    \"\"\"Test loading replay_index.json.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"job1\", \"score\": 1.5, \"strategy_id\": \"S1\"}],","                        \"metrics\": {\"n\": 10},","                    },","                    \"index\": {\"jobs\": [\"job1\"]},","                }","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        loaded = load_replay_index(exports_root, season)","        assert loaded[\"season\"] == season","        assert len(loaded[\"batches\"]) == 1","        assert loaded[\"batches\"][0][\"batch_id\"] == \"batchA\"","","","def test_load_replay_index_missing():","    \"\"\"Test FileNotFoundError when replay_index.json missing.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        with pytest.raises(FileNotFoundError):","            load_replay_index(exports_root, season)","","","def test_replay_season_topk():","    \"\"\"Test replay season topk.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [","                            {\"job_id\": \"job1\", \"score\": 1.5, \"strategy_id\": \"S1\"},","                            {\"job_id\": \"job2\", \"score\": 1.2, \"strategy_id\": \"S2\"},","                        ],","                        \"metrics\": {},","                    },","                },","                {","                    \"batch_id\": \"batchB\",","                    \"summary\": {","                        \"topk\": [","                            {\"job_id\": \"job3\", \"score\": 1.8, \"strategy_id\": \"S1\"},","                        ],","                        \"metrics\": {},","                    },","                },","                {","                    \"batch_id\": \"batchC\",","                    \"summary\": None,  # missing summary","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        res = replay_season_topk(exports_root, season, k=5)","        assert res.season == season","        assert res.k == 5","        assert len(res.items) == 3  # all topk items merged","        assert res.skipped_batches == [\"batchC\"]","        ","        # Verify ordering by score descending","        scores = [item[\"score\"] for item in res.items]","        assert scores == [1.8, 1.5, 1.2]","        ","        # Verify batch_id added","        assert all(\"_batch_id\" in item for item in res.items)","","","def test_replay_season_batch_cards():","    \"\"\"Test replay season batch cards.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"job1\", \"score\": 1.5}],","                        \"metrics\": {\"n\": 10},","                    },","                    \"index\": {\"jobs\": [\"job1\"]},","                },","                {","                    \"batch_id\": \"batchB\",","                    \"summary\": None,  # missing summary","                    \"index\": {\"jobs\": [\"job2\"]},","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        res = replay_season_batch_cards(exports_root, season)","        assert res.season == season","        assert len(res.batches) == 1","        assert res.batches[0][\"batch_id\"] == \"batchA\"","        assert res.skipped_summaries == [\"batchB\"]","","","def test_replay_season_leaderboard():","    \"\"\"Test replay season leaderboard.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [","                            {\"job_id\": \"job1\", \"score\": 1.5, \"strategy_id\": \"S1\", \"dataset_id\": \"D1\"},","                            {\"job_id\": \"job2\", \"score\": 1.2, \"strategy_id\": \"S2\", \"dataset_id\": \"D1\"},","                        ],","                        \"metrics\": {},","                    },","                },","                {","                    \"batch_id\": \"batchB\",","                    \"summary\": {","                        \"topk\": [","                            {\"job_id\": \"job3\", \"score\": 1.8, \"strategy_id\": \"S1\", \"dataset_id\": \"D2\"},","                            {\"job_id\": \"job4\", \"score\": 0.9, \"strategy_id\": \"S2\", \"dataset_id\": \"D2\"},","                        ],","                        \"metrics\": {},","                    },","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        # Test group_by strategy_id","        res = replay_season_leaderboard(exports_root, season, group_by=\"strategy_id\", per_group=2)","        assert res.season == season","        assert res.group_by == \"strategy_id\"","        assert res.per_group == 2"]}
{"type":"file_chunk","path":"tests/test_phase16_export_replay.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        assert len(res.groups) == 2  # S1 and S2","        ","        # Find S1 group","        s1_group = next(g for g in res.groups if g[\"key\"] == \"S1\")","        assert s1_group[\"total\"] == 2","        assert len(s1_group[\"items\"]) == 2","        assert s1_group[\"items\"][0][\"score\"] == 1.8  # top score first","        ","        # Test group_by dataset_id","        res2 = replay_season_leaderboard(exports_root, season, group_by=\"dataset_id\", per_group=1)","        assert len(res2.groups) == 2  # D1 and D2","        d1_group = next(g for g in res2.groups if g[\"key\"] == \"D1\")","        assert len(d1_group[\"items\"]) == 1  # per_group=1","","","def test_export_season_compare_topk_endpoint(client):","    \"\"\"Test /exports/seasons/{season}/compare/topk endpoint.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"job1\", \"score\": 1.5}],","                        \"metrics\": {},","                    },","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        with patch(\"control.api.get_exports_root\", return_value=exports_root):","            r = client.get(f\"/exports/seasons/{season}/compare/topk?k=5\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","            assert data[\"k\"] == 5","            assert len(data[\"items\"]) == 1","            assert data[\"items\"][0][\"job_id\"] == \"job1\"","","","def test_export_season_compare_batches_endpoint(client):","    \"\"\"Test /exports/seasons/{season}/compare/batches endpoint.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"job1\", \"score\": 1.5}],","                        \"metrics\": {\"n\": 10},","                    },","                    \"index\": {\"jobs\": [\"job1\"]},","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        with patch(\"control.api.get_exports_root\", return_value=exports_root):","            r = client.get(f\"/exports/seasons/{season}/compare/batches\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","            assert len(data[\"batches\"]) == 1","            assert data[\"batches\"][0][\"batch_id\"] == \"batchA\"","","","def test_export_season_compare_leaderboard_endpoint(client):","    \"\"\"Test /exports/seasons/{season}/compare/leaderboard endpoint.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [","                            {\"job_id\": \"job1\", \"score\": 1.5, \"strategy_id\": \"S1\"},","                        ],","                        \"metrics\": {},","                    },","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        with patch(\"control.api.get_exports_root\", return_value=exports_root):","            r = client.get(f\"/exports/seasons/{season}/compare/leaderboard?group_by=strategy_id\")","            assert r.status_code == 200","            data = r.json()","            assert data[\"season\"] == season","            assert data[\"group_by\"] == \"strategy_id\"","            assert len(data[\"groups\"]) == 1","            assert data[\"groups\"][0][\"key\"] == \"S1\"","","","def test_export_endpoints_missing_replay_index(client):","    \"\"\"Test 404 when replay_index.json missing.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        with patch(\"control.api.get_exports_root\", return_value=exports_root):","            r = client.get(f\"/exports/seasons/{season}/compare/topk\")","            assert r.status_code == 404","            assert \"replay_index.json\" in r.json()[\"detail\"]","","","def test_deterministic_ordering():","    \"\"\"Test deterministic ordering in replay functions.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        # Create replay index with batches in non-alphabetical order","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchZ\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"jobZ\", \"score\": 1.0}],","                        \"metrics\": {},","                    },","                },","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"jobA\", \"score\": 2.0}],","                        \"metrics\": {},","                    },","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        # Test that batches are processed in sorted order (batchA before batchZ)","        res = replay_season_topk(exports_root, season, k=10)","        # The items should be sorted by score, not batch order","        scores = [item[\"score\"] for item in res.items]","        assert scores == [2.0, 1.0]  # score ordering, not batch ordering","        ","        # Test batch cards ordering","        res2 = replay_season_batch_cards(exports_root, season)","        batch_ids = [b[\"batch_id\"] for b in res2.batches]","        assert batch_ids == [\"batchA\", \"batchZ\"]  # sorted by batch_id","","","def test_replay_with_empty_topk():","    \"\"\"Test replay with empty topk lists.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [],","                        \"metrics\": {},","                    },","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        res = replay_season_topk(exports_root, season, k=5)","        assert res.season == season","        assert len(res.items) == 0","        assert res.skipped_batches == []  # not skipped because summary exists","","","def test_replay_endpoint_zero_write_guarantee(client):","    \"\"\"Ensure replay endpoints do NOT write to exports tree.\"\"\"","    import os","    import time","    "]}
{"type":"file_chunk","path":"tests/test_phase16_export_replay.py","chunk_index":2,"line_start":401,"line_end":450,"content":["    with tempfile.TemporaryDirectory() as tmp:","        exports_root = Path(tmp) / \"exports\"","        season = \"2026Q1\"","        ","        replay_index = {","            \"season\": season,","            \"generated_at\": \"2025-12-21T00:00:00Z\",","            \"batches\": [","                {","                    \"batch_id\": \"batchA\",","                    \"summary\": {","                        \"topk\": [{\"job_id\": \"job1\", \"score\": 1.5}],","                        \"metrics\": {},","                    },","                    \"index\": {\"jobs\": [\"job1\"]},","                },","            ],","        }","        ","        _wjson(exports_root / \"seasons\" / season / \"replay_index.json\", replay_index)","        ","        # Record initial state","        def get_file_state():","            files = []","            for root, dirs, filenames in os.walk(exports_root):","                for f in filenames:","                    path = Path(root) / f","                    files.append((str(path.relative_to(exports_root)), path.stat().st_mtime))","            return sorted(files)","        ","        initial_state = get_file_state()","        ","        with patch(\"control.api.get_exports_root\", return_value=exports_root):","            # Call each replay endpoint","            r1 = client.get(f\"/exports/seasons/{season}/compare/topk?k=5\")","            assert r1.status_code == 200","            r2 = client.get(f\"/exports/seasons/{season}/compare/batches\")","            assert r2.status_code == 200","            r3 = client.get(f\"/exports/seasons/{season}/compare/leaderboard?group_by=strategy_id\")","            assert r3.status_code == 200","        ","        # Wait a tiny bit to ensure mtime could change if write occurred","        time.sleep(0.01)","        ","        final_state = get_file_state()","        ","        # No new files should appear, no mtime changes","        assert initial_state == final_state, \"Replay endpoints must not write to exports tree\"","",""]}
{"type":"file_footer","path":"tests/test_phase16_export_replay.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"tests/test_portfolio_artifacts_hash_stable.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5939,"sha256":"62431b47ac72a5211408cacf3f3d77e9240aecb31e490cb34cd4de4536f0f4c1","total_lines":222,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_portfolio_artifacts_hash_stable.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Test portfolio artifacts hash stability.","","Phase 8: Test hash is deterministic and changes with spec changes.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from portfolio.artifacts import compute_portfolio_hash, write_portfolio_artifacts","from portfolio.compiler import compile_portfolio","from portfolio.loader import load_portfolio_spec","from strategy.registry import load_builtin_strategies, clear","","","@pytest.fixture(autouse=True)","def setup_registry() -> None:","    \"\"\"Setup strategy registry before each test.\"\"\"","    clear()","    load_builtin_strategies()","    yield","    clear()","","","def test_hash_same_spec_consistent(tmp_path: Path) -> None:","    \"\"\"Test hash is consistent for same spec.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    ","    # Compute hash multiple times","    hash1 = compute_portfolio_hash(spec)","    hash2 = compute_portfolio_hash(spec)","    hash3 = compute_portfolio_hash(spec)","    ","    # All hashes should be identical","    assert hash1 == hash2 == hash3","    assert len(hash1) == 40  # SHA1 hex string length","","","def test_hash_different_order_consistent(tmp_path: Path) -> None:","    \"\"\"Test hash is consistent even if legs are in different order.\"\"\"","    yaml_content1 = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","  - leg_id: \"leg2\"","    symbol: \"TWF.MXF\"","    timeframe_min: 60","    session_profile: \"configs/profiles/TWF_MXF_v2.yaml\"","    strategy_id: \"mean_revert_zscore\"","    strategy_version: \"v1\"","    params:","      zscore_threshold: -2.0","    enabled: true","\"\"\"","    ","    yaml_content2 = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg2\"  # Different order","    symbol: \"TWF.MXF\"","    timeframe_min: 60","    session_profile: \"configs/profiles/TWF_MXF_v2.yaml\"","    strategy_id: \"mean_revert_zscore\"","    strategy_version: \"v1\"","    params:","      zscore_threshold: -2.0","    enabled: true","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","\"\"\"","    ","    spec_path1 = tmp_path / \"test1.yaml\"","    spec_path1.write_text(yaml_content1, encoding=\"utf-8\")","    ","    spec_path2 = tmp_path / \"test2.yaml\"","    spec_path2.write_text(yaml_content2, encoding=\"utf-8\")","    ","    spec1 = load_portfolio_spec(spec_path1)","    spec2 = load_portfolio_spec(spec_path2)","    ","    hash1 = compute_portfolio_hash(spec1)","    hash2 = compute_portfolio_hash(spec2)","    ","    # Hashes should be identical (legs are sorted by leg_id before hashing)","    assert hash1 == hash2","","","def test_hash_changes_with_param_change(tmp_path: Path) -> None:","    \"\"\"Test hash changes when params change.\"\"\"","    yaml_content1 = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","\"\"\"","    ","    yaml_content2 = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 15.0  # Changed","      slow_period: 20.0","    enabled: true","\"\"\"","    ","    spec_path1 = tmp_path / \"test1.yaml\"","    spec_path1.write_text(yaml_content1, encoding=\"utf-8\")","    ","    spec_path2 = tmp_path / \"test2.yaml\"","    spec_path2.write_text(yaml_content2, encoding=\"utf-8\")","    ","    spec1 = load_portfolio_spec(spec_path1)","    spec2 = load_portfolio_spec(spec_path2)","    ","    hash1 = compute_portfolio_hash(spec1)","    hash2 = compute_portfolio_hash(spec2)","    ","    # Hashes should be different","    assert hash1 != hash2","","","def test_write_artifacts_creates_files(tmp_path: Path) -> None:","    \"\"\"Test write_portfolio_artifacts creates all required files.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)"]}
{"type":"file_chunk","path":"tests/test_portfolio_artifacts_hash_stable.py","chunk_index":1,"line_start":201,"line_end":222,"content":["    jobs = compile_portfolio(spec)","    ","    out_dir = tmp_path / \"artifacts\"","    artifact_paths = write_portfolio_artifacts(spec, jobs, out_dir)","    ","    # Check all files exist","    assert (out_dir / \"portfolio_spec_snapshot.yaml\").exists()","    assert (out_dir / \"compiled_jobs.json\").exists()","    assert (out_dir / \"portfolio_index.json\").exists()","    assert (out_dir / \"portfolio_hash.txt\").exists()","    ","    # Check hash file content","    hash_content = (out_dir / \"portfolio_hash.txt\").read_text(encoding=\"utf-8\").strip()","    computed_hash = compute_portfolio_hash(spec)","    assert hash_content == computed_hash","    ","    # Check index contains hash","    import json","    index_content = json.loads((out_dir / \"portfolio_index.json\").read_text(encoding=\"utf-8\"))","    assert index_content[\"portfolio_hash\"] == computed_hash","",""]}
{"type":"file_footer","path":"tests/test_portfolio_artifacts_hash_stable.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_portfolio_compile_jobs.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2938,"sha256":"b602a42dcf0e71653e9e96c215bb5a7dc098bce419132dd79ec207b97814dcf5","total_lines":119,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_portfolio_compile_jobs.py","chunk_index":0,"line_start":1,"line_end":119,"content":["","\"\"\"Test portfolio compiler.","","Phase 8: Test compilation produces correct job configs.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from portfolio.compiler import compile_portfolio","from portfolio.loader import load_portfolio_spec","from strategy.registry import load_builtin_strategies, clear","","","@pytest.fixture(autouse=True)","def setup_registry() -> None:","    \"\"\"Setup strategy registry before each test.\"\"\"","    clear()","    load_builtin_strategies()","    yield","    clear()","","","def test_compile_enabled_legs_only(tmp_path: Path) -> None:","    \"\"\"Test compilation only includes enabled legs.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","  - leg_id: \"leg2\"","    symbol: \"TWF.MXF\"","    timeframe_min: 60","    session_profile: \"configs/profiles/TWF_MXF_v2.yaml\"","    strategy_id: \"mean_revert_zscore\"","    strategy_version: \"v1\"","    params:","      zscore_threshold: -2.0","    enabled: false  # Disabled","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    jobs = compile_portfolio(spec)","    ","    # Should only have 1 job (leg1 enabled, leg2 disabled)","    assert len(jobs) == 1","    assert jobs[0][\"leg_id\"] == \"leg1\"","","","def test_compile_job_has_required_keys(tmp_path: Path) -> None:","    \"\"\"Test compiled jobs have all required keys.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","    tags: [\"test\"]","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    jobs = compile_portfolio(spec)","    ","    assert len(jobs) == 1","    job = jobs[0]","    ","    # Check required keys","    required_keys = {","        \"portfolio_id\",","        \"portfolio_version\",","        \"leg_id\",","        \"symbol\",","        \"timeframe_min\",","        \"session_profile\",","        \"strategy_id\",","        \"strategy_version\",","        \"params\",","    }","    ","    assert required_keys.issubset(job.keys())","    ","    # Check values","    assert job[\"portfolio_id\"] == \"test\"","    assert job[\"portfolio_version\"] == \"v1\"","    assert job[\"leg_id\"] == \"leg1\"","    assert job[\"symbol\"] == \"CME.MNQ\"","    assert job[\"timeframe_min\"] == 60","    assert job[\"strategy_id\"] == \"sma_cross\"","    assert job[\"strategy_version\"] == \"v1\"","    assert job[\"params\"] == {\"fast_period\": 10.0, \"slow_period\": 20.0}","    assert job[\"tags\"] == [\"test\"]","",""]}
{"type":"file_footer","path":"tests/test_portfolio_compile_jobs.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_portfolio_spec_loader.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3573,"sha256":"094a70166bb42b7a8ee0edc27812a5695913deb22c344918a17bdc1d6b424d8e","total_lines":133,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_portfolio_spec_loader.py","chunk_index":0,"line_start":1,"line_end":133,"content":["","\"\"\"Test portfolio spec loader.","","Phase 8: Test YAML/JSON loader can load and type is correct.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from portfolio.loader import load_portfolio_spec","from portfolio.spec import PortfolioLeg, PortfolioSpec","","","def test_load_yaml_spec(tmp_path: Path) -> None:","    \"\"\"Test loading YAML portfolio spec.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","data_tz: \"Asia/Taipei\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 20.0","    enabled: true","    tags: [\"test\"]","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    ","    assert isinstance(spec, PortfolioSpec)","    assert spec.portfolio_id == \"test\"","    assert spec.version == \"v1\"","    assert spec.data_tz == \"Asia/Taipei\"","    assert len(spec.legs) == 1","    ","    leg = spec.legs[0]","    assert isinstance(leg, PortfolioLeg)","    assert leg.leg_id == \"leg1\"","    assert leg.symbol == \"CME.MNQ\"","    assert leg.timeframe_min == 60","    assert leg.strategy_id == \"sma_cross\"","    assert leg.strategy_version == \"v1\"","    assert leg.params == {\"fast_period\": 10.0, \"slow_period\": 20.0}","    assert leg.enabled is True","    assert leg.tags == [\"test\"]","","","def test_load_json_spec(tmp_path: Path) -> None:","    \"\"\"Test loading JSON portfolio spec.\"\"\"","    import json","    ","    json_content = {","        \"portfolio_id\": \"test\",","        \"version\": \"v1\",","        \"data_tz\": \"Asia/Taipei\",","        \"legs\": [","            {","                \"leg_id\": \"leg1\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe_min\": 60,","                \"session_profile\": \"configs/profiles/CME_MNQ_v2.yaml\",","                \"strategy_id\": \"sma_cross\",","                \"strategy_version\": \"v1\",","                \"params\": {","                    \"fast_period\": 10.0,","                    \"slow_period\": 20.0,","                },","                \"enabled\": True,","                \"tags\": [\"test\"],","            }","        ],","    }","    ","    spec_path = tmp_path / \"test.json\"","    with spec_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(json_content, f)","    ","    spec = load_portfolio_spec(spec_path)","    ","    assert isinstance(spec, PortfolioSpec)","    assert spec.portfolio_id == \"test\"","    assert len(spec.legs) == 1","","","def test_load_missing_fields_raises(tmp_path: Path) -> None:","    \"\"\"Test loading spec with missing required fields raises ValueError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","# Missing version","legs: []","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    with pytest.raises(ValueError, match=\"missing 'version' field\"):","        load_portfolio_spec(spec_path)","","","def test_load_invalid_params_type_raises(tmp_path: Path) -> None:","    \"\"\"Test loading spec with invalid params type raises ValueError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params: \"invalid\"  # Should be dict","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    with pytest.raises(ValueError, match=\"params must be dict\"):","        load_portfolio_spec(spec_path)","",""]}
{"type":"file_footer","path":"tests/test_portfolio_spec_loader.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_portfolio_validate.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3845,"sha256":"98e708497f46b3592ad6a69e9d4245c21fdc1bfb7b29546125a6ae52abddbd57","total_lines":144,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_portfolio_validate.py","chunk_index":0,"line_start":1,"line_end":144,"content":["","\"\"\"Test portfolio validator.","","Phase 8: Test validation raises errors for invalid specs.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from portfolio.loader import load_portfolio_spec","from portfolio.validate import validate_portfolio_spec","from strategy.registry import load_builtin_strategies, clear","","","@pytest.fixture(autouse=True)","def setup_registry() -> None:","    \"\"\"Setup strategy registry before each test.\"\"\"","    clear()","    load_builtin_strategies()","    yield","    clear()","","","def test_validate_empty_legs_raises(tmp_path: Path) -> None:","    \"\"\"Test validating spec with empty legs raises ValueError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs: []","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    ","    with pytest.raises(ValueError, match=\"at least one leg\"):","        validate_portfolio_spec(spec)","","","def test_validate_duplicate_leg_id_raises(tmp_path: Path) -> None:","    \"\"\"Test validating spec with duplicate leg_id raises ValueError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params: {}","  - leg_id: \"leg1\"  # Duplicate","    symbol: \"TWF.MXF\"","    timeframe_min: 60","    session_profile: \"configs/profiles/TWF_MXF_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params: {}","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    with pytest.raises(ValueError, match=\"Duplicate leg_id\"):","        load_portfolio_spec(spec_path)","","","def test_validate_nonexistent_strategy_raises(tmp_path: Path) -> None:","    \"\"\"Test validating spec with nonexistent strategy raises KeyError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"nonexistent_strategy\"  # Not in registry","    strategy_version: \"v1\"","    params: {}","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    ","    with pytest.raises(KeyError, match=\"not found in registry\"):","        validate_portfolio_spec(spec)","","","def test_validate_strategy_version_mismatch_raises(tmp_path: Path) -> None:","    \"\"\"Test validating spec with strategy version mismatch raises ValueError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"configs/profiles/CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v2\"  # Mismatch (registry has v1)","    params: {}","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    ","    with pytest.raises(ValueError, match=\"strategy_version mismatch\"):","        validate_portfolio_spec(spec)","","","def test_validate_nonexistent_session_profile_raises(tmp_path: Path) -> None:","    \"\"\"Test validating spec with nonexistent session profile raises FileNotFoundError.\"\"\"","    yaml_content = \"\"\"","portfolio_id: \"test\"","version: \"v1\"","legs:","  - leg_id: \"leg1\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"nonexistent_profile.yaml\"  # Not found","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params: {}","\"\"\"","    ","    spec_path = tmp_path / \"test.yaml\"","    spec_path.write_text(yaml_content, encoding=\"utf-8\")","    ","    spec = load_portfolio_spec(spec_path)","    ","    with pytest.raises(FileNotFoundError):","        validate_portfolio_spec(spec)","",""]}
{"type":"file_footer","path":"tests/test_portfolio_validate.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_report_link_allows_minimal_artifacts.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6109,"sha256":"223c823e15d03612c3f7d7fc9252abdaed391624c48cb57ca69f609f83ac58ce","total_lines":170,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_report_link_allows_minimal_artifacts.py","chunk_index":0,"line_start":1,"line_end":170,"content":["","\"\"\"Tests for report link allowing minimal artifacts.","","Tests that report readiness only checks file existence,","and build_report_link always returns Viewer URL.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","","import pytest","","from control.report_links import (","    build_report_link,","    get_outputs_root,","    is_report_ready,",")","","","def test_is_report_ready_with_minimal_artifacts(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that is_report_ready returns True with only three files.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    run_dir = tmp_path / run_id","    run_dir.mkdir(parents=True)","    ","    # Create only the three required files","    (run_dir / \"manifest.json\").write_text(json.dumps({\"run_id\": run_id}))","    # Use winners_v2.json (preferred) or winners.json (fallback)","    (run_dir / \"winners_v2.json\").write_text(json.dumps({\"summary\": {}}))","    (run_dir / \"governance.json\").write_text(json.dumps({\"scoring\": {}}))","    ","    # Should return True","    assert is_report_ready(run_id) is True","","","def test_is_report_ready_missing_file(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that is_report_ready returns False if any file is missing.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    run_dir = tmp_path / run_id","    run_dir.mkdir(parents=True)","    ","    # Create only two files (missing governance.json)","    (run_dir / \"manifest.json\").write_text(json.dumps({\"run_id\": run_id}))","    (run_dir / \"winners.json\").write_text(json.dumps({\"summary\": {}}))","    ","    # Should return False","    assert is_report_ready(run_id) is False","","","def test_build_report_link_always_returns_url(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that build_report_link always returns Viewer URL.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    ","    # Should return URL even if artifacts don't exist","    report_link = build_report_link(run_id)","    ","    assert report_link is not None","    assert report_link.startswith(\"/?\")","    assert run_id in report_link","    assert \"season\" in report_link","","","def test_build_report_link_no_error_string(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that build_report_link never returns error string.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    ","    # Should never return error string","    report_link = build_report_link(run_id)","    ","    assert report_link is not None","    assert isinstance(report_link, str)","    assert \"error\" not in report_link.lower()","    assert \"not ready\" not in report_link.lower()","    assert \"missing\" not in report_link.lower()","","","def test_is_report_ready_never_raises(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that is_report_ready never raises exceptions.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    # Should not raise even with invalid run_id","    result = is_report_ready(\"nonexistent_run\")","    assert isinstance(result, bool)","    ","    # Should not raise even with None","    result = is_report_ready(None)  # type: ignore","    assert isinstance(result, bool)","","","def test_build_report_link_never_raises(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that build_report_link never raises exceptions.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    # Should not raise even with invalid run_id","    report_link = build_report_link(\"nonexistent_run\")","    assert report_link is not None","    assert isinstance(report_link, str)","    ","    # Should not raise even with empty string","    report_link = build_report_link(\"\")","    assert report_link is not None","    assert isinstance(report_link, str)","","","def test_minimal_artifacts_content_not_checked(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that is_report_ready does not check content validity.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    run_dir = tmp_path / run_id","    run_dir.mkdir(parents=True)","    ","    # Create files with invalid JSON content","    (run_dir / \"manifest.json\").write_text(\"invalid json\")","    (run_dir / \"winners_v2.json\").write_text(\"not json\")","    (run_dir / \"governance.json\").write_text(\"{}\")","    ","    # Should still return True (only checks existence)","    assert is_report_ready(run_id) is True","","","def test_is_report_ready_accepts_winners_json_fallback(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that is_report_ready accepts winners.json as fallback.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    run_dir = tmp_path / run_id","    run_dir.mkdir(parents=True)","    ","    # Create files with winners.json (not winners_v2.json)","    (run_dir / \"manifest.json\").write_text(json.dumps({\"run_id\": run_id}))","    (run_dir / \"winners.json\").write_text(json.dumps({\"summary\": {}}))","    (run_dir / \"governance.json\").write_text(json.dumps({\"scoring\": {}}))","    ","    # Should still return True (only checks existence)","    assert is_report_ready(run_id) is True","","","def test_ui_does_not_block_with_minimal_artifacts(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that UI flow does not block with minimal artifacts.\"\"\"","    monkeypatch.setenv(\"FISHBRO_OUTPUTS_ROOT\", str(tmp_path))","    ","    run_id = \"test_run_123\"","    run_dir = tmp_path / run_id","    run_dir.mkdir(parents=True)","    ","    # Create minimal artifacts","    (run_dir / \"manifest.json\").write_text(json.dumps({\"run_id\": run_id}))","    (run_dir / \"winners_v2.json\").write_text(json.dumps({\"summary\": {}}))","    (run_dir / \"governance.json\").write_text(json.dumps({\"scoring\": {}}))","    ","    # build_report_link should work","    report_link = build_report_link(run_id)","    assert report_link is not None","    assert \"error\" not in report_link.lower()","    ","    # is_report_ready should return True","    assert is_report_ready(run_id) is True","",""]}
{"type":"file_footer","path":"tests/test_report_link_allows_minimal_artifacts.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_research_decision.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3709,"sha256":"a52f40d8564f28554f0b4024f6866290515010956a1ee734f0ae635e816028c9","total_lines":116,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_research_decision.py","chunk_index":0,"line_start":1,"line_end":116,"content":["","\"\"\"Tests for research decision module.\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","","import pytest","","from research.decision import append_decision, load_decisions","","","def test_append_decision_new(tmp_path: Path) -> None:","    \"\"\"Test appending a new decision.\"\"\"","    out_dir = tmp_path / \"research\"","    ","    log_path = append_decision(out_dir, \"test-run-123\", \"KEEP\", \"Good results\")","    ","    # Verify log file exists","    assert log_path.exists()","    ","    # Verify log content (JSONL)","    with open(log_path, \"r\", encoding=\"utf-8\") as f:","        lines = [line.strip() for line in f if line.strip()]","        assert len(lines) == 1","        entry = json.loads(lines[0])","        assert entry[\"run_id\"] == \"test-run-123\"","        assert entry[\"decision\"] == \"KEEP\"","        assert entry[\"note\"] == \"Good results\"","        assert \"decided_at\" in entry","","","def test_append_decision_multiple(tmp_path: Path) -> None:","    \"\"\"Test appending multiple decisions (same run_id allowed).\"\"\"","    out_dir = tmp_path / \"research\"","    ","    # Append first decision","    log_path = append_decision(out_dir, \"test-run-123\", \"KEEP\", \"First decision\")","    ","    # Append second decision (same run_id, different decision)","    append_decision(out_dir, \"test-run-123\", \"DROP\", \"Changed mind\")","    ","    # Verify log has 2 lines","    with open(log_path, \"r\", encoding=\"utf-8\") as f:","        lines = [line.strip() for line in f if line.strip()]","        assert len(lines) == 2","    ","    # Verify both entries exist","    entries = []","    with open(log_path, \"r\", encoding=\"utf-8\") as f:","        for line in f:","            line = line.strip()","            if line:","                entries.append(json.loads(line))","    ","    assert len(entries) == 2","    assert entries[0][\"decision\"] == \"KEEP\"","    assert entries[1][\"decision\"] == \"DROP\"","    assert entries[1][\"run_id\"] == \"test-run-123\"","","","def test_load_decisions_empty(tmp_path: Path) -> None:","    \"\"\"Test loading decisions when log doesn't exist.\"\"\"","    out_dir = tmp_path / \"research\"","    ","    decisions = load_decisions(out_dir)","    assert decisions == []","","","def test_load_decisions_multiple(tmp_path: Path) -> None:","    \"\"\"Test loading multiple decisions.\"\"\"","    out_dir = tmp_path / \"research\"","    ","    # Append multiple decisions","    append_decision(out_dir, \"run-1\", \"KEEP\", \"Note 1\")","    append_decision(out_dir, \"run-2\", \"DROP\", \"Note 2\")","    append_decision(out_dir, \"run-3\", \"ARCHIVE\", \"Note 3\")","    ","    # Load decisions","    decisions = load_decisions(out_dir)","    ","    assert len(decisions) == 3","    ","    # Verify all decisions are present","    run_ids = {d[\"run_id\"] for d in decisions}","    assert run_ids == {\"run-1\", \"run-2\", \"run-3\"}","    ","    # Verify decisions","    decision_map = {d[\"run_id\"]: d[\"decision\"] for d in decisions}","    assert decision_map[\"run-1\"] == \"KEEP\"","    assert decision_map[\"run-2\"] == \"DROP\"","    assert decision_map[\"run-3\"] == \"ARCHIVE\"","","","def test_load_decisions_same_run_multiple_times(tmp_path: Path) -> None:","    \"\"\"Test loading decisions when same run_id appears multiple times.\"\"\"","    out_dir = tmp_path / \"research\"","    ","    # Append same run_id multiple times","    append_decision(out_dir, \"run-1\", \"KEEP\", \"First\")","    append_decision(out_dir, \"run-1\", \"DROP\", \"Second\")","    append_decision(out_dir, \"run-1\", \"ARCHIVE\", \"Third\")","    ","    # Load decisions - should return all entries","    decisions = load_decisions(out_dir)","    ","    assert len(decisions) == 3","    # All should have same run_id","    assert all(d[\"run_id\"] == \"run-1\" for d in decisions)","    # Decisions should be in order","    assert decisions[0][\"decision\"] == \"KEEP\"","    assert decisions[1][\"decision\"] == \"DROP\"","    assert decisions[2][\"decision\"] == \"ARCHIVE\"","",""]}
{"type":"file_footer","path":"tests/test_research_decision.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_research_extract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6366,"sha256":"e6c9ef6edb63a64d996268429b67c9c35351a9a19e77912771194e985fa9c7d4","total_lines":208,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_research_extract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Tests for research extract module.\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","","import pytest","","from research.extract import extract_canonical_metrics, ExtractionError","from research.metrics import CanonicalMetrics","","","def test_extract_canonical_metrics_success(tmp_path: Path) -> None:","    \"\"\"Test successful extraction of canonical metrics.\"\"\"","    run_dir = tmp_path / \"run\"","    run_dir.mkdir()","    ","    # Create manifest.json","    manifest = {","        \"run_id\": \"test-run-123\",","        \"bars\": 1000,","        \"created_at\": \"2025-01-01T00:00:00Z\",","    }","    with open(run_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f)","    ","    # Create metrics.json","    metrics_data = {","        \"stage_name\": \"stage2_confirm\",","    }","    with open(run_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(metrics_data, f)","    ","    # Create winners.json with topk","    winners = {","        \"schema\": \"v2\",","        \"stage_name\": \"stage2_confirm\",","        \"topk\": [","            {","                \"candidate_id\": \"test:1\",","                \"strategy_id\": \"donchian_atr\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe\": \"60m\",","                \"metrics\": {","                    \"net_profit\": 100.0,","                    \"max_dd\": -50.0,","                    \"trades\": 10,","                },","                \"score\": 100.0,","            },","            {","                \"candidate_id\": \"test:2\",","                \"strategy_id\": \"donchian_atr\",","                \"symbol\": \"CME.MNQ\",","                \"timeframe\": \"60m\",","                \"metrics\": {","                    \"net_profit\": 50.0,","                    \"max_dd\": -20.0,","                    \"trades\": 5,","                },","                \"score\": 50.0,","            },","        ],","    }","    with open(run_dir / \"winners.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(winners, f)","    ","    # Extract metrics","    metrics = extract_canonical_metrics(run_dir)","    ","    # Verify","    assert metrics.run_id == \"test-run-123\"","    assert metrics.bars == 1000","    assert metrics.trades == 15  # 10 + 5","    assert metrics.net_profit == 150.0  # 100 + 50","    assert metrics.max_drawdown == 50.0  # abs(-50)","    assert metrics.start_date == \"2025-01-01T00:00:00Z\"","    assert metrics.strategy_id == \"donchian_atr\"","    assert metrics.symbol == \"CME.MNQ\"","    assert metrics.timeframe_min == 60","    assert metrics.score_net_mdd == 150.0 / 50.0  # net_profit / max_drawdown","    assert metrics.score_final > 0  # score_net_mdd * (trades ** 0.25)","","","def test_extract_canonical_metrics_missing_artifacts(tmp_path: Path) -> None:","    \"\"\"Test extraction fails when no artifacts exist.\"\"\"","    run_dir = tmp_path / \"run\"","    run_dir.mkdir()","    ","    # No artifacts","    with pytest.raises(ExtractionError, match=\"No artifacts found\"):","        extract_canonical_metrics(run_dir)","","","def test_extract_canonical_metrics_missing_run_id(tmp_path: Path) -> None:","    \"\"\"Test extraction fails when run_id is missing.\"\"\"","    run_dir = tmp_path / \"run\"","    run_dir.mkdir()","    ","    # Create manifest without run_id","    with open(run_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({\"bars\": 100}, f)","    with open(run_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    # Should raise ExtractionError","    with pytest.raises(ExtractionError, match=\"Missing 'run_id'\"):","        extract_canonical_metrics(run_dir)","","","def test_extract_canonical_metrics_missing_bars(tmp_path: Path) -> None:","    \"\"\"Test extraction fails when bars is missing.\"\"\"","    run_dir = tmp_path / \"run\"","    run_dir.mkdir()","    ","    # Create manifest without bars","    with open(run_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({\"run_id\": \"test\"}, f)","    with open(run_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    # Should raise ExtractionError","    with pytest.raises(ExtractionError, match=\"Missing 'bars'\"):","        extract_canonical_metrics(run_dir)","","","def test_extract_canonical_metrics_zero_drawdown_with_profit(tmp_path: Path) -> None:","    \"\"\"Test extraction raises when max_drawdown is 0 but net_profit is non-zero.\"\"\"","    run_dir = tmp_path / \"run\"","    run_dir.mkdir()","    ","    manifest = {","        \"run_id\": \"test-run\",","        \"bars\": 1000,","        \"created_at\": \"2025-01-01T00:00:00Z\",","    }","    with open(run_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f)","    ","    with open(run_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    winners = {","        \"schema\": \"v2\",","        \"topk\": [","            {","                \"candidate_id\": \"test:1\",","                \"metrics\": {","                    \"net_profit\": 100.0,","                    \"max_dd\": 0.0,  # Zero drawdown","                    \"trades\": 10,","                },","            },","        ],","    }","    with open(run_dir / \"winners.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(winners, f)","    ","    # Should raise ExtractionError","    with pytest.raises(ExtractionError, match=\"cannot calculate score_net_mdd\"):","        extract_canonical_metrics(run_dir)","","","def test_extract_canonical_metrics_no_trades(tmp_path: Path) -> None:","    \"\"\"Test extraction with no trades.\"\"\"","    run_dir = tmp_path / \"run\"","    run_dir.mkdir()","    ","    manifest = {","        \"run_id\": \"test-run-no-trades\",","        \"bars\": 1000,","        \"created_at\": \"2025-01-01T00:00:00Z\",","    }","    with open(run_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f)","    ","    with open(run_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    winners = {","        \"schema\": \"v2\",","        \"topk\": [","            {","                \"candidate_id\": \"test:1\",","                \"metrics\": {","                    \"net_profit\": 0.0,","                    \"max_dd\": 0.0,","                    \"trades\": 0,","                },","            },","        ],","    }","    with open(run_dir / \"winners.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(winners, f)","    ","    # Extract metrics","    metrics = extract_canonical_metrics(run_dir)","    "]}
{"type":"file_chunk","path":"tests/test_research_extract.py","chunk_index":1,"line_start":201,"line_end":208,"content":["    # Verify zero metrics","    assert metrics.trades == 0","    assert metrics.net_profit == 0.0","    assert metrics.max_drawdown == 0.0","    assert metrics.score_net_mdd == 0.0","    assert metrics.score_final == 0.0","",""]}
{"type":"file_footer","path":"tests/test_research_extract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_research_registry.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5304,"sha256":"c7a465af2c21df44166467dec0e5659b693c0d7b73a9a8f2adafefe31449b1aa","total_lines":181,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_research_registry.py","chunk_index":0,"line_start":1,"line_end":181,"content":["","\"\"\"Tests for research registry module.\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","","import pytest","","from research.registry import build_research_index","","","def test_build_research_index_empty(tmp_path: Path) -> None:","    \"\"\"Test building index with empty outputs.\"\"\"","    outputs_root = tmp_path / \"outputs\"","    outputs_root.mkdir()","    out_dir = tmp_path / \"research\"","    ","    index_path = build_research_index(outputs_root, out_dir)","    ","    # Verify files created","    assert index_path.exists()","    assert (out_dir / \"canonical_results.json\").exists()","    ","    # Verify content","    with open(index_path, \"r\", encoding=\"utf-8\") as f:","        index_data = json.load(f)","    ","    assert index_data[\"total_runs\"] == 0","    assert index_data[\"entries\"] == []","","","def test_build_research_index_with_runs(tmp_path: Path) -> None:","    \"\"\"Test building index with multiple runs, verify sorting.\"\"\"","    outputs_root = tmp_path / \"outputs\"","    ","    # Create two runs with different scores","    run1_dir = outputs_root / \"seasons\" / \"2026Q1\" / \"runs\" / \"run-1\"","    run1_dir.mkdir(parents=True)","    ","    run2_dir = outputs_root / \"seasons\" / \"2026Q1\" / \"runs\" / \"run-2\"","    run2_dir.mkdir(parents=True)","    ","    # Run 1: Higher score_final","    manifest1 = {","        \"run_id\": \"run-1\",","        \"bars\": 1000,","        \"created_at\": \"2025-01-01T00:00:00Z\",","    }","    with open(run1_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(manifest1, f)","    ","    with open(run1_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    winners1 = {","        \"schema\": \"v2\",","        \"topk\": [","            {","                \"candidate_id\": \"test:1\",","                \"metrics\": {","                    \"net_profit\": 200.0,","                    \"max_dd\": -50.0,","                    \"trades\": 20,  # Higher trades -> higher score_final","                },","            },","        ],","    }","    with open(run1_dir / \"winners.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(winners1, f)","    ","    # Run 2: Lower score_final","    manifest2 = {","        \"run_id\": \"run-2\",","        \"bars\": 1000,","        \"created_at\": \"2025-01-01T00:00:00Z\",","    }","    with open(run2_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(manifest2, f)","    ","    with open(run2_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    winners2 = {","        \"schema\": \"v2\",","        \"topk\": [","            {","                \"candidate_id\": \"test:2\",","                \"metrics\": {","                    \"net_profit\": 100.0,","                    \"max_dd\": -50.0,","                    \"trades\": 10,  # Lower trades -> lower score_final","                },","            },","        ],","    }","    with open(run2_dir / \"winners.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(winners2, f)","    ","    # Build index","    out_dir = tmp_path / \"research\"","    index_path = build_research_index(outputs_root, out_dir)","    ","    # Verify files created","    assert index_path.exists()","    canonical_path = out_dir / \"canonical_results.json\"","    assert canonical_path.exists()","    ","    # Verify canonical_results.json","    with open(canonical_path, \"r\", encoding=\"utf-8\") as f:","        canonical_data = json.load(f)","    ","    assert len(canonical_data) == 2","    ","    # Verify research_index.json is sorted (score_final desc)","    with open(index_path, \"r\", encoding=\"utf-8\") as f:","        index_data = json.load(f)","    ","    assert index_data[\"total_runs\"] == 2","    entries = index_data[\"entries\"]","    assert len(entries) == 2","    ","    # Verify sorting: run-1 should be first (higher score_final)","    assert entries[0][\"run_id\"] == \"run-1\"","    assert entries[1][\"run_id\"] == \"run-2\"","    assert entries[0][\"score_final\"] > entries[1][\"score_final\"]","","","def test_build_research_index_preserves_decisions(tmp_path: Path) -> None:","    \"\"\"Test that building index preserves decisions from decisions.log.\"\"\"","    outputs_root = tmp_path / \"outputs\"","    out_dir = tmp_path / \"research\"","    out_dir.mkdir()","    ","    # Create a run","    run_dir = outputs_root / \"seasons\" / \"2026Q1\" / \"runs\" / \"run-1\"","    run_dir.mkdir(parents=True)","    ","    manifest = {","        \"run_id\": \"run-1\",","        \"bars\": 1000,","        \"created_at\": \"2025-01-01T00:00:00Z\",","    }","    with open(run_dir / \"manifest.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f)","    ","    with open(run_dir / \"metrics.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump({}, f)","    ","    winners = {","        \"schema\": \"v2\",","        \"topk\": [","            {","                \"candidate_id\": \"test:1\",","                \"metrics\": {","                    \"net_profit\": 100.0,","                    \"max_dd\": -50.0,","                    \"trades\": 10,","                },","            },","        ],","    }","    with open(run_dir / \"winners.json\", \"w\", encoding=\"utf-8\") as f:","        json.dump(winners, f)","    ","    # Add a decision","    from research.decision import append_decision","    ","    append_decision(out_dir, \"run-1\", \"KEEP\", \"Good results\")","    ","    # Build index","    index_path = build_research_index(outputs_root, out_dir)","    ","    # Verify decision is preserved","    with open(index_path, \"r\", encoding=\"utf-8\") as f:","        index_data = json.load(f)","    ","    assert index_data[\"entries\"][0][\"decision\"] == \"KEEP\"","",""]}
{"type":"file_footer","path":"tests/test_research_registry.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_runner_adapter_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4956,"sha256":"288d9ea08e710f3262c1a23c00518877745ce85e37e5a14e65804cf457b9db08","total_lines":148,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_runner_adapter_contract.py","chunk_index":0,"line_start":1,"line_end":148,"content":["","\"\"\"Contract tests for runner adapter.","","Tests verify:","1. Adapter returns data only (no file I/O)","2. Winners schema is stable","3. Metrics structure is consistent","\"\"\"","","from __future__ import annotations","","import tempfile","from pathlib import Path","","import numpy as np","import pytest","","from pipeline.runner_adapter import run_stage_job","","","def test_runner_adapter_returns_no_files_written():","    \"\"\"Test that adapter does not write any files.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        tmp_path = Path(tmpdir)","        ","        # Count files before","        files_before = list(tmp_path.rglob(\"*\"))","        file_count_before = len([f for f in files_before if f.is_file()])","        ","        # Run adapter","        cfg = {","            \"stage_name\": \"stage0_coarse\",","            \"param_subsample_rate\": 0.1,","            \"topk\": 10,","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"params_total\": 100,","            \"proxy_name\": \"ma_proxy_v0\",","        }","        ","        result = run_stage_job(cfg)","        ","        # Count files after","        files_after = list(tmp_path.rglob(\"*\"))","        file_count_after = len([f for f in files_after if f.is_file()])","        ","        # Verify no new files were created","        assert file_count_after == file_count_before, (","            \"Adapter should not write files, but new files were created\"","        )","        ","        # Verify result structure","        assert \"metrics\" in result","        assert \"winners\" in result","","","def test_winners_schema_is_stable():","    \"\"\"Test that winners schema is stable across all stages.\"\"\"","    test_cases = [","        {","            \"stage_name\": \"stage0_coarse\",","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"params_total\": 100,","            \"topk\": 10,","        },","        {","            \"stage_name\": \"stage1_topk\",","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"params_total\": 100,","            \"topk\": 5,","            \"commission\": 0.0,","            \"slip\": 0.0,","        },","        {","            \"stage_name\": \"stage2_confirm\",","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"params_total\": 100,","            \"commission\": 0.0,","            \"slip\": 0.0,","        },","    ]","    ","    for cfg in test_cases:","        cfg[\"param_subsample_rate\"] = 1.0  # Use full for simplicity","        ","        result = run_stage_job(cfg)","        ","        # Verify winners schema","        winners = result.get(\"winners\", {})","        assert \"topk\" in winners, f\"Missing 'topk' in winners for {cfg['stage_name']}\"","        assert \"notes\" in winners, f\"Missing 'notes' in winners for {cfg['stage_name']}\"","        assert isinstance(winners[\"topk\"], list)","        assert isinstance(winners[\"notes\"], dict)","        assert winners[\"notes\"].get(\"schema\") == \"v1\"","","","def test_metrics_structure_is_consistent():","    \"\"\"Test that metrics structure is consistent across stages.\"\"\"","    test_cases = [","        {","            \"stage_name\": \"stage0_coarse\",","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"params_total\": 100,","            \"topk\": 10,","        },","        {","            \"stage_name\": \"stage1_topk\",","            \"open_\": np.random.randn(1000).astype(np.float64),","            \"high\": np.random.randn(1000).astype(np.float64),","            \"low\": np.random.randn(1000).astype(np.float64),","            \"close\": np.random.randn(1000).astype(np.float64),","            \"params_matrix\": np.random.randn(100, 3).astype(np.float64),","            \"params_total\": 100,","            \"topk\": 5,","            \"commission\": 0.0,","            \"slip\": 0.0,","        },","    ]","    ","    required_fields = [\"params_total\", \"params_effective\", \"bars\", \"stage_name\"]","    ","    for cfg in test_cases:","        cfg[\"param_subsample_rate\"] = 0.5","        ","        result = run_stage_job(cfg)","        ","        metrics = result.get(\"metrics\", {})","        ","        # Verify required fields exist","        for field in required_fields:","            assert field in metrics, (","                f\"Missing required field '{field}' in metrics for {cfg['stage_name']}\"","            )","        ","        # Verify stage_name matches","        assert metrics[\"stage_name\"] == cfg[\"stage_name\"]","",""]}
{"type":"file_footer","path":"tests/test_runner_adapter_contract.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_runner_adapter_input_coercion.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5624,"sha256":"229442c9cfc908d561d6fd064632a0b886cd5cd9b2d6806ff67ddf83552b47e3","total_lines":165,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_runner_adapter_input_coercion.py","chunk_index":0,"line_start":1,"line_end":165,"content":["","\"\"\"Contract tests for runner adapter input coercion.","","Tests verify that input arrays are coerced to np.ndarray float64,","preventing .shape access errors when lists are passed.","\"\"\"","","from __future__ import annotations","","import numpy as np","import pytest","","from pipeline.runner_adapter import run_stage_job","","","def test_stage0_coercion_with_lists() -> None:","    \"\"\"Test that Stage0 accepts list inputs and coerces to np.ndarray.\"\"\"","    # Use list instead of np.ndarray","    close_list = [100.0 + i * 0.1 for i in range(1000)]","    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5], [20.0, 10.0, 2.0]]","    ","    cfg = {","        \"stage_name\": \"stage0_coarse\",","        \"param_subsample_rate\": 1.0,","        \"topk\": 3,","        \"close\": close_list,  # List, not np.ndarray","        \"params_matrix\": params_matrix_list,  # List, not np.ndarray","        \"params_total\": 3,","        \"proxy_name\": \"ma_proxy_v0\",","    }","    ","    # Should not raise AttributeError: 'list' object has no attribute 'shape'","    result = run_stage_job(cfg)","    ","    # Verify result structure","    assert \"metrics\" in result","    assert \"winners\" in result","    ","    # Verify that internal arrays are np.ndarray (by checking results work)","    assert isinstance(result[\"winners\"][\"topk\"], list)","    assert len(result[\"winners\"][\"topk\"]) <= 3","","","def test_stage1_coercion_with_lists() -> None:","    \"\"\"Test that Stage1 accepts list inputs and coerces to np.ndarray.\"\"\"","    # Use lists instead of np.ndarray","    open_list = [100.0 + i * 0.1 for i in range(100)]","    high_list = [101.0 + i * 0.1 for i in range(100)]","    low_list = [99.0 + i * 0.1 for i in range(100)]","    close_list = [100.0 + i * 0.1 for i in range(100)]","    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5]]","    ","    cfg = {","        \"stage_name\": \"stage1_topk\",","        \"param_subsample_rate\": 1.0,","        \"topk\": 2,","        \"open_\": open_list,  # List, not np.ndarray","        \"high\": high_list,  # List, not np.ndarray","        \"low\": low_list,  # List, not np.ndarray","        \"close\": close_list,  # List, not np.ndarray","        \"params_matrix\": params_matrix_list,  # List, not np.ndarray","        \"params_total\": 2,","        \"commission\": 0.0,","        \"slip\": 0.0,","    }","    ","    # Should not raise AttributeError: 'list' object has no attribute 'shape'","    result = run_stage_job(cfg)","    ","    # Verify result structure","    assert \"metrics\" in result","    assert \"winners\" in result","    ","    # Verify that internal arrays are np.ndarray (by checking results work)","    assert isinstance(result[\"winners\"][\"topk\"], list)","","","def test_stage2_coercion_with_lists() -> None:","    \"\"\"Test that Stage2 accepts list inputs and coerces to np.ndarray.\"\"\"","    # Use lists instead of np.ndarray","    open_list = [100.0 + i * 0.1 for i in range(100)]","    high_list = [101.0 + i * 0.1 for i in range(100)]","    low_list = [99.0 + i * 0.1 for i in range(100)]","    close_list = [100.0 + i * 0.1 for i in range(100)]","    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5]]","    ","    cfg = {","        \"stage_name\": \"stage2_confirm\",","        \"param_subsample_rate\": 1.0,","        \"open_\": open_list,  # List, not np.ndarray","        \"high\": high_list,  # List, not np.ndarray","        \"low\": low_list,  # List, not np.ndarray","        \"close\": close_list,  # List, not np.ndarray","        \"params_matrix\": params_matrix_list,  # List, not np.ndarray","        \"params_total\": 2,","        \"commission\": 0.0,","        \"slip\": 0.0,","    }","    ","    # Should not raise AttributeError: 'list' object has no attribute 'shape'","    result = run_stage_job(cfg)","    ","    # Verify result structure","    assert \"metrics\" in result","    assert \"winners\" in result","    ","    # Verify that internal arrays are np.ndarray (by checking results work)","    assert isinstance(result[\"winners\"][\"topk\"], list)","","","def test_coercion_preserves_dtype_float64() -> None:","    \"\"\"Test that coercion produces float64 arrays.\"\"\"","    # Test with float32 input (should be coerced to float64)","    close_float32 = np.array([100.0, 101.0, 102.0], dtype=np.float32)","    params_matrix_float32 = np.array([[10.0, 5.0, 1.0]], dtype=np.float32)","    ","    cfg = {","        \"stage_name\": \"stage0_coarse\",","        \"param_subsample_rate\": 1.0,","        \"topk\": 1,","        \"close\": close_float32,","        \"params_matrix\": params_matrix_float32,","        \"params_total\": 1,","        \"proxy_name\": \"ma_proxy_v0\",","    }","    ","    # Should not raise errors","    result = run_stage_job(cfg)","    ","    # Verify result structure","    assert \"metrics\" in result","    assert \"winners\" in result","","","def test_coercion_handles_mixed_inputs() -> None:","    \"\"\"Test that coercion handles mixed list/np.ndarray inputs.\"\"\"","    # Mix of lists and np.ndarray","    open_list = [100.0 + i * 0.1 for i in range(100)]","    high_array = np.array([101.0 + i * 0.1 for i in range(100)], dtype=np.float64)","    low_list = [99.0 + i * 0.1 for i in range(100)]","    close_array = np.array([100.0 + i * 0.1 for i in range(100)], dtype=np.float32)","    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5]]","    ","    cfg = {","        \"stage_name\": \"stage1_topk\",","        \"param_subsample_rate\": 1.0,","        \"topk\": 2,","        \"open_\": open_list,  # List","        \"high\": high_array,  # np.ndarray float64","        \"low\": low_list,  # List","        \"close\": close_array,  # np.ndarray float32 (should be coerced to float64)","        \"params_matrix\": params_matrix_list,  # List","        \"params_total\": 2,","        \"commission\": 0.0,","        \"slip\": 0.0,","    }","    ","    # Should not raise errors","    result = run_stage_job(cfg)","    ","    # Verify result structure","    assert \"metrics\" in result","    assert \"winners\" in result","",""]}
{"type":"file_footer","path":"tests/test_runner_adapter_input_coercion.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_runner_grid_perf_observability.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1497,"sha256":"ae2e0c54f91ae7f23ce5419ca72e47f7a7bffecc6a938f8766be6eaaa76c9636","total_lines":45,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_runner_grid_perf_observability.py","chunk_index":0,"line_start":1,"line_end":45,"content":["","from __future__ import annotations","","import numpy as np","","from pipeline.runner_grid import run_grid","","","def test_run_grid_perf_fields_present_and_non_negative(monkeypatch) -> None:","    # Enable perf observability.","    monkeypatch.setenv(\"FISHBRO_PROFILE_GRID\", \"1\")","","    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)","    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)","    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)","    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)","","    params = np.array([[2, 2, 1.0], [3, 2, 1.5]], dtype=np.float64)","    out = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=False)","","    assert \"perf\" in out","    perf = out[\"perf\"]","    assert isinstance(perf, dict)","","    for k in (\"t_features\", \"t_indicators\", \"t_intent_gen\", \"t_simulate\"):","        assert k in perf","        # allow None (JSON null) when measurement is unavailable; never assume 0 is meaningful","        if perf[k] is not None:","            assert float(perf[k]) >= 0.0","","    assert \"simulate_impl\" in perf","    assert perf[\"simulate_impl\"] in (\"jit\", \"py\")","","    assert \"intents_total\" in perf","    if perf[\"intents_total\"] is not None:","        assert int(perf[\"intents_total\"]) >= 0","","    # Perf harness hook: confirm we can observe intent mode when profiling is enabled.","    assert \"intent_mode\" in perf","    if perf[\"intent_mode\"] is not None:","        assert perf[\"intent_mode\"] in (\"arrays\", \"objects\")","","","",""]}
{"type":"file_footer","path":"tests/test_runner_grid_perf_observability.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_runtime_context.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9461,"sha256":"abc478c9dee18386ac73602ee53f07b9ffb39a5dd97d69899aaa9b9d02723321","total_lines":287,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_runtime_context.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Test runtime context generation.","","Contract:","- Call write_runtime_context(out_path=tmp_path/...) with dummy entrypoint.","- Monkeypatch subprocess calls to raise; verify file still written with headings and UNKNOWN.","- Assert policy hash section present (UNKNOWN allowed).","\"\"\"","","import tempfile","import json","import subprocess","import hashlib","import os","from pathlib import Path","from unittest.mock import patch, MagicMock","import pytest","","from gui.services.runtime_context import (","    write_runtime_context,","    get_snapshot_timestamp,","    get_git_info,","    get_policy_hash,",")","","","def test_write_runtime_context_basic(tmp_path: Path):","    \"\"\"Basic test that writes a runtime context file.\"\"\"","    out_path = tmp_path / \"RUNTIME_CONTEXT.md\"","    ","    result = write_runtime_context(","        out_path=out_path,","        entrypoint=\"test_entrypoint.py\",","        listen_host=\"127.0.0.1\",","        listen_port=9999,","    )","    ","    assert result == out_path","    assert out_path.exists()","    ","    content = out_path.read_text(encoding=\"utf-8\")","    ","    # Required headings","    assert \"# Runtime Context\" in content","    assert \"## Timestamp\" in content","    assert \"## Process\" in content","    assert \"## Build\" in content","    assert \"## Entrypoint\" in content","    assert \"## Network\" in content","    assert \"## Governance\" in content","    assert \"## Snapshot Policy Binding\" in content","    assert \"## Notes\" in content","    ","    # Specific content","    assert \"test_entrypoint.py\" in content","    assert \"127.0.0.1:9999\" in content or \":9999\" in content","    ","    # Should have PID","    import os","    assert f\"PID: {os.getpid()}\" in content","","","def test_write_runtime_context_no_crash_on_error(tmp_path: Path):","    \"\"\"Test that write_runtime_context never crashes.\"\"\"","    out_path = tmp_path / \"RUNTIME_CONTEXT.md\"","    ","    # Monkeypatch subprocess.check_output to raise","    with patch('subprocess.check_output', side_effect=Exception(\"Mock error\")):","        # Also patch psutil.Process to raise if psutil is available","        # First check if psutil module is imported in the runtime_context module","        import sys","        if 'psutil' in sys.modules:","            with patch('psutil.Process', side_effect=Exception(\"Psutil error\")):","                result = write_runtime_context(","                    out_path=out_path,","                    entrypoint=\"test.py\",","                )","        else:","            # psutil not available, just test without patching it","            result = write_runtime_context(","                out_path=out_path,","                entrypoint=\"test.py\",","            )","    ","    assert result == out_path","    assert out_path.exists()","    ","    content = out_path.read_text(encoding=\"utf-8\")","    # Should still have basic structure","    assert \"# Runtime Context\" in content","    assert \"## Timestamp\" in content","    # Might have error section or minimal info","    assert \"PID:\" in content or \"Error\" in content","","","def test_policy_hash_section(tmp_path: Path):","    \"\"\"Test that policy hash section is present.\"\"\"","    out_path = tmp_path / \"RUNTIME_CONTEXT.md\"","    ","    # Create a dummy LOCAL_SCAN_RULES.json","    policy_dir = tmp_path / \"outputs\" / \"snapshots\" / \"full\"","    policy_dir.mkdir(parents=True)","    policy_content = json.dumps({\"mode\": \"test\", \"allowed_roots\": [\"src\"]})","    (policy_dir / \"LOCAL_SCAN_RULES.json\").write_text(policy_content)","    ","    # Mock the policy path to point to our dummy","    with patch('gui.services.runtime_context.Path') as MockPath:","        mock_path_instance = MagicMock()","        mock_path_instance.exists.return_value = True","        mock_path_instance.__str__.return_value = str(policy_dir / \"LOCAL_SCAN_RULES.json\")","        ","        def side_effect(*args, **kwargs):","            if args[0] == \"outputs/snapshots/full/LOCAL_SCAN_RULES.json\":","                return mock_path_instance","            return Path(*args, **kwargs)","        ","        MockPath.side_effect = side_effect","        ","        result = write_runtime_context(","            out_path=out_path,","            entrypoint=\"test.py\",","        )","    ","    assert out_path.exists()","    content = out_path.read_text(encoding=\"utf-8\")","    ","    # Should have policy hash section","    assert \"## Snapshot Policy Binding\" in content","    assert \"Local scan rules sha256:\" in content","    # Hash could be UNKNOWN or actual hash","    assert \"Local scan rules source:\" in content","","","def test_get_snapshot_timestamp(tmp_path: Path):","    \"\"\"Test snapshot timestamp retrieval.\"\"\"","    # Test with MANIFEST.json","    manifest_dir = tmp_path / \"outputs\" / \"snapshots\" / \"full\"","    manifest_dir.mkdir(parents=True)","    manifest_path = manifest_dir / \"MANIFEST.json\"","    ","    expected_time = \"2025-12-26T12:00:00Z\"","    manifest_path.write_text(json.dumps({\"generated_at_utc\": expected_time}))","    ","    with patch('gui.services.runtime_context.Path') as MockPath:","        def side_effect(*args, **kwargs):","            if args[0] == \"outputs/snapshots/full/MANIFEST.json\":","                return manifest_path","            return Path(*args, **kwargs)","        ","        MockPath.side_effect = side_effect","        ","        timestamp = get_snapshot_timestamp()","        assert timestamp == expected_time","    ","    # Test with SYSTEM_FULL_SNAPSHOT.md mtime","    import time","    snapshot_path = tmp_path / \"SYSTEM_FULL_SNAPSHOT.md\"","    snapshot_path.write_text(\"# Snapshot\")","    expected_mtime = time.time() - 3600","    os.utime(snapshot_path, (expected_mtime, expected_mtime))","    ","    with patch('gui.services.runtime_context.Path') as MockPath:","        def side_effect(*args, **kwargs):","            if args[0] == \"outputs/snapshots/full/MANIFEST.json\":","                return Path(\"/nonexistent\")","            if args[0] == \"outputs/snapshots/SYSTEM_FULL_SNAPSHOT.md\":","                return snapshot_path","            return Path(*args, **kwargs)","        ","        MockPath.side_effect = side_effect","        ","        timestamp = get_snapshot_timestamp()","        # Should be ISO format","        assert \"T\" in timestamp","        assert \"Z\" in timestamp or \"+\" in timestamp","    ","    # Test UNKNOWN when neither exists","    with patch('gui.services.runtime_context.Path') as MockPath:","        MockPath.return_value.exists.return_value = False","        ","        timestamp = get_snapshot_timestamp()","        assert timestamp == \"UNKNOWN\"","","","def test_get_git_info():","    \"\"\"Test git info retrieval.\"\"\"","    with patch('subprocess.check_output') as mock_check_output:","        # Mock successful git commands","        mock_check_output.return_value = b\"abc123\\n\"","        ","        commit, dirty = get_git_info()","        assert commit == \"abc123\"","        # dirty could be \"yes\" or \"no\" depending on mock","        ","        # Test git error","        mock_check_output.side_effect = Exception(\"git not found\")","        commit, dirty = get_git_info()","        assert commit == \"UNKNOWN\"","        assert dirty == \"UNKNOWN\""]}
{"type":"file_chunk","path":"tests/test_runtime_context.py","chunk_index":1,"line_start":201,"line_end":287,"content":["","","# def test_port_occupancy():","#     \"\"\"Test port occupancy checking.\"\"\"","#         mock_run.return_value = \"LISTEN 0 128 0.0.0.0:8080 0.0.0.0:* users:(python)\"","#","#         result = port_occupancy(8080)","#         assert \"8080\" in result or \"python\" in result","#","#         # Test error case","#         mock_run.return_value = \"ERROR: something\"","#         result = port_occupancy(8080)","#         assert \"ERROR\" in result","","","def test_get_policy_hash(tmp_path: Path):","    \"\"\"Test policy hash computation.\"\"\"","    policy_path = tmp_path / \"policy.json\"","    content = b'{\"test\": \"data\"}'","    policy_path.write_bytes(content)","    ","    expected_hash = hashlib.sha256(content).hexdigest()","    ","    hash_val = get_policy_hash(policy_path)","    assert hash_val == expected_hash","    ","    # Test missing file","    missing_path = tmp_path / \"missing.json\"","    hash_val = get_policy_hash(missing_path)","    assert hash_val == \"UNKNOWN\"","    ","    # Test read error","    with patch('builtins.open', side_effect=Exception(\"I/O error\")):","        hash_val = get_policy_hash(policy_path)","        assert hash_val == \"UNKNOWN\"","","","def test_runtime_context_integration(tmp_path: Path):","    \"\"\"Integration test with real file system.\"\"\"","    # Create a minimal repo-like structure","    repo_root = tmp_path / \"repo\"","    repo_root.mkdir()","    ","    # Create outputs/snapshots/full/LOCAL_SCAN_RULES.json","    policy_dir = repo_root / \"outputs\" / \"snapshots\" / \"full\"","    policy_dir.mkdir(parents=True)","    policy_content = json.dumps({","        \"mode\": \"local-strict\",","        \"allowed_roots\": [\"src\", \"tests\"],","        \"max_files\": 20000,","    })","    (policy_dir / \"LOCAL_SCAN_RULES.json\").write_text(policy_content)","    ","    # Create MANIFEST.json","    (policy_dir / \"MANIFEST.json\").write_text(json.dumps({","        \"generated_at_utc\": \"2025-12-26T12:00:00Z\",","        \"git_head\": \"test123\",","    }))","    ","    # Change to repo directory","    import os","    old_cwd = os.getcwd()","    os.chdir(repo_root)","    ","    try:","        out_path = repo_root / \"runtime_test.md\"","        ","        result = write_runtime_context(","            out_path=out_path,","            entrypoint=\"scripts/launch_dashboard.py\",","            listen_port=8080,","        )","        ","        assert result.exists()","        content = result.read_text(encoding=\"utf-8\")","        ","        # Check key sections","        assert \"## Snapshot Policy Binding\" in content","        assert \"Local scan rules sha256:\" in content","        assert \"scripts/launch_dashboard.py\" in content","        ","    finally:","        os.chdir(old_cwd)","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_runtime_context.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_runtime_network_probe.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6035,"sha256":"8e9456b74e262a9ea0c40ca1502e924544ddc120c6765785ea4832a4110e7d0e","total_lines":208,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_runtime_network_probe.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Test runtime network probe hardening.","","Validates dual-probe strategy for port occupancy detection in runtime context.","\"\"\"","","import pytest","","from src.gui.services.runtime_context import (","    _probe_ss,","    _probe_lsof,","    _analyze_port_occupancy,",")","","","def test_probe_ss_mocked_empty(monkeypatch):","    \"\"\"Test when ss returns empty (simulating WSL permission issue).\"\"\"","    ","    def mock_run(cmd):","        return \"\"  # Empty output","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._run\",","        mock_run","    )","    ","    result = _probe_ss(8080)","    assert \"NOT AVAILABLE\" in result","    assert \"ss command failed\" in result or \"empty\" in result","","","def test_probe_lsof_mocked_pid(monkeypatch):","    \"\"\"Test when lsof returns a PID.\"\"\"","    ","    def mock_run(cmd):","        # cmd is a list like [\"bash\", \"-lc\", \"lsof -i :8080 -sTCP:LISTEN -n -P\"]","        # Check if any part of the command contains \"lsof\"","        cmd_str = \" \".join(cmd)","        if \"lsof\" in cmd_str:","            return \"python3 12345 user 3u IPv4 12345 0t0 TCP *:8080 (LISTEN)\"","        return \"\"","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._run\",","        mock_run","    )","    ","    result = _probe_lsof(8080)","    assert \"python3\" in result","    assert \"12345\" in result","    assert \"8080\" in result","","","def test_analyze_port_occupancy_both_fail(monkeypatch):","    \"\"\"Test when both probes fail -> UNRESOLVED.\"\"\"","    ","    def mock_probe_ss(port):","        return \"NOT AVAILABLE (ss command failed)\"","    ","    def mock_probe_lsof(port):","        return \"NOT AVAILABLE (lsof command failed)\"","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_ss\",","        mock_probe_ss","    )","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_lsof\",","        mock_probe_lsof","    )","    ","    ss_out, lsof_out, bound, verdict = _analyze_port_occupancy(8080)","    ","    assert \"NOT AVAILABLE\" in ss_out","    assert \"NOT AVAILABLE\" in lsof_out","    assert bound == \"no\"  # No LISTEN in output","    assert \"UNRESOLVED\" in verdict or \"PORT NOT BOUND\" in verdict","","","def test_analyze_port_occupancy_ss_has_pid(monkeypatch):","    \"\"\"Test when ss returns PID.\"\"\"","    ","    def mock_probe_ss(port):","        return 'State  Recv-Q Send-Q Local Address:Port Peer Address:Port Process\\nLISTEN 0      128    127.0.0.1:8080    0.0.0.0:*      users:((\"python3\",pid=12345,fd=3))'","    ","    def mock_probe_lsof(port):","        return \"NOT AVAILABLE (lsof command failed)\"","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_ss\",","        mock_probe_ss","    )","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_lsof\",","        mock_probe_lsof","    )","    ","    ss_out, lsof_out, bound, verdict = _analyze_port_occupancy(8080)","    ","    assert \"pid=12345\" in ss_out","    assert \"NOT AVAILABLE\" in lsof_out","    assert bound == \"yes\"","    assert \"PID 12345\" in verdict","","","def test_analyze_port_occupancy_lsof_has_pid(monkeypatch):","    \"\"\"Test when lsof returns PID (ss empty).\"\"\"","    ","    def mock_probe_ss(port):","        return \"State  Recv-Q Send-Q Local Address:Port Peer Address:Port\"","    ","    def mock_probe_lsof(port):","        return \"python3   12345  user    3u  IPv4  12345      0t0  TCP *:8080 (LISTEN)\"","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_ss\",","        mock_probe_ss","    )","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_lsof\",","        mock_probe_lsof","    )","    ","    ss_out, lsof_out, bound, verdict = _analyze_port_occupancy(8080)","    ","    assert \"LISTEN\" in ss_out or bound == \"yes\"  # ss shows LISTEN but no PID","    assert \"12345\" in lsof_out","    assert bound == \"yes\"","    assert \"PID 12345\" in verdict","","","def test_analyze_port_occupancy_bound_no_pid(monkeypatch):","    \"\"\"Test when port is bound but no PID identified.\"\"\"","    ","    def mock_probe_ss(port):","        return \"State  Recv-Q Send-Q Local Address:Port Peer Address:Port\\nLISTEN 0      128    127.0.0.1:8080    0.0.0.0:*\"","    ","    def mock_probe_lsof(port):","        return \"COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\"","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_ss\",","        mock_probe_ss","    )","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_lsof\",","        mock_probe_lsof","    )","    ","    ss_out, lsof_out, bound, verdict = _analyze_port_occupancy(8080)","    ","    assert bound == \"yes\"","    assert \"UNRESOLVED\" in verdict","    assert \"bound but no PID\" in verdict","","","def test_write_runtime_context_integration(monkeypatch, tmp_path):","    \"\"\"Integration test: write_runtime_context produces correct Network section.\"\"\"","    ","    # Mock probes to return known values","    def mock_probe_ss(port):","        return \"ss output with pid=9999\"","    ","    def mock_probe_lsof(port):","        return \"lsof output\"","    ","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_ss\",","        mock_probe_ss","    )","    monkeypatch.setattr(","        \"src.gui.services.runtime_context._probe_lsof\",","        mock_probe_lsof","    )","    ","    # Import after monkeypatching","    from src.gui.services.runtime_context import write_runtime_context","    ","    out_path = tmp_path / \"RUNTIME_CONTEXT.md\"","    ","    # Write runtime context","    result_path = write_runtime_context(","        out_path=str(out_path),","        entrypoint=\"test.py\",","        listen_port=9090,","    )","    ","    assert result_path.exists()","    content = result_path.read_text()","    ","    # Check required sections","    assert \"## Network\" in content","    assert \"Listen: :9090\" in content","    assert \"Port occupancy (9090):\" in content","    assert \"### ss\" in content","    assert \"### lsof\" in content","    assert \"### Resolution\" in content","    assert \"- Bound:\" in content","    assert \"- Process identified:\" in content"]}
{"type":"file_chunk","path":"tests/test_runtime_network_probe.py","chunk_index":1,"line_start":201,"line_end":208,"content":["    assert \"- Final verdict:\" in content","    ","    # Check our mocked PID appears","    assert \"pid=9999\" in content or \"PID 9999\" in content","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_runtime_network_probe.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_seed_demo_run.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4831,"sha256":"3d4840b848c0f0c71d0caa10648b80a1fdd9e33e5dc7f65977f8eddc50c9a4c1","total_lines":153,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_seed_demo_run.py","chunk_index":0,"line_start":1,"line_end":153,"content":["","\"\"\"Tests for seed_demo_run.","","Tests that seed_demo_run creates demo job and artifacts correctly.","\"\"\"","","from __future__ import annotations","","import json","import sqlite3","from pathlib import Path","","import pytest","","from control.seed_demo_run import main, get_db_path","","","def test_seed_demo_run_no_raise(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that seed_demo_run does not raise exceptions.\"\"\"","    # Set outputs root to tmp_path","    monkeypatch.chdir(tmp_path)","    monkeypatch.setenv(\"JOBS_DB_PATH\", str(tmp_path / \"jobs.db\"))","    ","    # Should not raise","    run_id = main()","    ","    assert run_id.startswith(\"demo_\")","    assert len(run_id) > 5","","","def test_outputs_directory_created(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that outputs/<season>/runs/<run_id>/ directory is created.\"\"\"","    monkeypatch.chdir(tmp_path)","    monkeypatch.setenv(\"JOBS_DB_PATH\", str(tmp_path / \"jobs.db\"))","    ","    run_id = main()","    ","    # Standard path structure: outputs/<season>/runs/<run_id>/","    run_dir = tmp_path / \"outputs\" / \"seasons\" / \"2026Q1\" / \"runs\" / run_id","    assert run_dir.exists()","    assert run_dir.is_dir()","","","def test_artifacts_exist(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that all required artifacts are created.\"\"\"","    monkeypatch.chdir(tmp_path)","    monkeypatch.setenv(\"JOBS_DB_PATH\", str(tmp_path / \"jobs.db\"))","    ","    run_id = main()","    # Standard path structure: outputs/<season>/runs/<run_id>/","    run_dir = tmp_path / \"outputs\" / \"seasons\" / \"2026Q1\" / \"runs\" / run_id","    ","    # Check manifest.json","    manifest_path = run_dir / \"manifest.json\"","    assert manifest_path.exists()","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        manifest = json.load(f)","    assert manifest[\"run_id\"] == run_id","    assert \"created_at\" in manifest","    ","    # Check winners_v2.json","    winners_path = run_dir / \"winners_v2.json\"","    assert winners_path.exists()","    ","    # Check governance.json","    governance_path = run_dir / \"governance.json\"","    assert governance_path.exists()","    ","    # Check kpi.json (KPI唯一來源)","    kpi_path = run_dir / \"kpi.json\"","    assert kpi_path.exists()","    with kpi_path.open(\"r\", encoding=\"utf-8\") as f:","        kpi = json.load(f)","    assert \"net_profit\" in kpi","    assert \"max_drawdown\" in kpi","    assert \"num_trades\" in kpi","    assert \"final_score\" in kpi","","","def test_job_in_db(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that job is created in database with DONE status.\"\"\"","    monkeypatch.chdir(tmp_path)","    db_path = tmp_path / \"jobs.db\"","    monkeypatch.setenv(\"JOBS_DB_PATH\", str(db_path))","    ","    run_id = main()","    ","    # Check database","    conn = sqlite3.connect(str(db_path))","    try:","        cursor = conn.execute(\"SELECT status, run_id, report_link FROM jobs WHERE run_id = ?\", (run_id,))","        row = cursor.fetchone()","        assert row is not None","        ","        status, db_run_id, report_link = row","        assert status == \"DONE\"","        assert db_run_id == run_id","        assert report_link is not None","        assert report_link.startswith(\"/b5?\")","        assert run_id in report_link","        assert \"season=2026Q1\" in report_link","    finally:","        conn.close()","","","def test_report_link_not_none(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that report_link is not None.\"\"\"","    monkeypatch.chdir(tmp_path)","    db_path = tmp_path / \"jobs.db\"","    monkeypatch.setenv(\"JOBS_DB_PATH\", str(db_path))","    ","    run_id = main()","    ","    conn = sqlite3.connect(str(db_path))","    try:","        cursor = conn.execute(\"SELECT report_link FROM jobs WHERE run_id = ?\", (run_id,))","        row = cursor.fetchone()","        assert row is not None","        ","        report_link = row[0]","        assert report_link is not None","        assert len(report_link) > 0","    finally:","        conn.close()","","","def test_kpi_values_aligned(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:","    \"\"\"Test that KPI values align with Phase 6.1 registry.\"\"\"","    monkeypatch.chdir(tmp_path)","    monkeypatch.setenv(\"JOBS_DB_PATH\", str(tmp_path / \"jobs.db\"))","    ","    run_id = main()","    # Standard path structure: outputs/<season>/runs/<run_id>/","    run_dir = tmp_path / \"outputs\" / \"seasons\" / \"2026Q1\" / \"runs\" / run_id","    ","    # Check kpi.json exists and has required KPIs (KPI唯一來源)","    kpi_path = run_dir / \"kpi.json\"","    assert kpi_path.exists()","    with kpi_path.open(\"r\", encoding=\"utf-8\") as f:","        kpi = json.load(f)","    ","    assert \"net_profit\" in kpi","    assert \"max_drawdown\" in kpi","    assert \"num_trades\" in kpi","    assert \"final_score\" in kpi","    ","    # Verify KPI values match expected","    assert kpi[\"net_profit\"] == 123456","    assert kpi[\"max_drawdown\"] == -0.18","    assert kpi[\"num_trades\"] == 42","    assert kpi[\"final_score\"] == 1.23","",""]}
{"type":"file_footer","path":"tests/test_seed_demo_run.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_service_identity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5938,"sha256":"0d200474f6d5a62558f8b043feea5bf208d2eaaa0a39713f4f9c9bf89fe6fc86","total_lines":166,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_service_identity.py","chunk_index":0,"line_start":1,"line_end":166,"content":["\"\"\"Tests for service_identity module.\"\"\"","","import os","import sys","from pathlib import Path","from unittest.mock import patch, mock_open","import pytest","","from core.service_identity import (","    get_service_identity,","    _safe_cmdline,","    _safe_git_commit,","    _ALLOWED_ENV_KEYS,",")","","","def test_get_service_identity_returns_required_keys():","    \"\"\"Basic smoke test: ensure required keys are present.\"\"\"","    ident = get_service_identity(service_name=\"test\", db_path=None)","    assert isinstance(ident, dict)","    assert ident[\"service_name\"] == \"test\"","    assert ident[\"pid\"] == os.getpid()","    assert ident[\"ppid\"] == os.getppid()","    assert isinstance(ident[\"cmdline\"], str)","    assert ident[\"cwd\"] == str(Path.cwd())","    assert ident[\"python\"] == sys.executable","    assert ident[\"python_version\"] == sys.version","    assert isinstance(ident[\"platform\"], str)","    assert isinstance(ident[\"repo_root\"], str)","    assert \"git_commit\" in ident","    assert isinstance(ident[\"build_time_utc\"], str)","    assert isinstance(ident[\"env\"], dict)","    assert ident[\"jobs_db_path\"] == \"\"","    assert ident[\"jobs_db_parent\"] == \"\"","    assert ident[\"worker_pidfile_path\"] == \"\"","    assert ident[\"worker_log_path\"] == \"\"","","","def test_get_service_identity_with_db_path():","    \"\"\"Test with a db_path.\"\"\"","    db = Path(\"/tmp/test.db\")","    ident = get_service_identity(service_name=\"test\", db_path=db)","    assert ident[\"jobs_db_path\"] == str(db.expanduser().resolve())","    assert ident[\"jobs_db_parent\"] == str(db.expanduser().resolve().parent)","    assert ident[\"worker_pidfile_path\"] == str(db.expanduser().resolve().parent / \"worker.pid\")","    assert ident[\"worker_log_path\"] == str(db.expanduser().resolve().parent / \"worker_process.log\")","","","def test_env_filtering():","    \"\"\"Ensure only allowed env keys appear.\"\"\"","    # Set some env vars","    os.environ[\"PYTHONPATH\"] = \"/some/path\"","    os.environ[\"JOBS_DB_PATH\"] = \"/tmp/db\"","    os.environ[\"FISHBRO_TESTING\"] = \"1\"","    os.environ[\"PYTEST_CURRENT_TEST\"] = \"test\"","    os.environ[\"TMPDIR\"] = \"/tmp\"","    # Set a forbidden key","    os.environ[\"FORBIDDEN_KEY\"] = \"should_not_appear\"","","    ident = get_service_identity(service_name=\"test\", db_path=None)","    env = ident[\"env\"]","    assert \"PYTHONPATH\" in env","    assert \"JOBS_DB_PATH\" in env","    assert \"FISHBRO_TESTING\" in env","    assert \"PYTEST_CURRENT_TEST\" in env","    assert \"TMPDIR\" in env","    assert \"FORBIDDEN_KEY\" not in env","    # Ensure only allowed keys","    for key in env:","        assert key in _ALLOWED_ENV_KEYS","","    # Clean up","    del os.environ[\"FORBIDDEN_KEY\"]","","","def test_git_commit_unknown_when_git_missing():","    \"\"\"Test that git commit returns 'unknown' when .git missing.\"\"\"","    with patch(\"pathlib.Path.exists\", return_value=False):","        commit = _safe_git_commit(Path(\"/nonexistent\"))","        assert commit == \"unknown\"","","","@pytest.mark.xfail(reason=\"Mocking complexity; functionality verified by other tests\")","def test_git_commit_extracts_from_head():","    \"\"\"Mock git HEAD file.\"\"\"","    mock_head = \"ref: refs/heads/main\\n\"","    mock_ref = \"abc123\\n\"","    # Use a simple mock that logs calls","    from unittest.mock import MagicMock","    mock_exists = MagicMock()","    mock_read_text = MagicMock()","    # Configure side effects","    def exists_side(path):","        # path is a Path instance","        return True  # both exist","    def read_text_side(self, *args, **kwargs):","        # self is Path instance","        if self.name == \"HEAD\":","            return mock_head","        else:","            return mock_ref","    mock_exists.side_effect = exists_side","    mock_read_text.side_effect = read_text_side","    with patch(\"core.service_identity.Path.exists\", mock_exists):","        with patch(\"core.service_identity.Path.read_text\", mock_read_text):","            commit = _safe_git_commit(Path(\"/repo\"))","            assert commit == \"abc123\"","","","def test_git_commit_direct_hash():","    \"\"\"Mock HEAD containing direct commit hash.\"\"\"","    mock_head = \"abc456\\n\"","    with patch(\"pathlib.Path.exists\", return_value=True):","        with patch(\"pathlib.Path.read_text\", return_value=mock_head):","            commit = _safe_git_commit(Path(\"/repo\"))","            assert commit == \"abc456\"","","","def test_safe_cmdline_fallback():","    \"\"\"Test cmdline fallback when /proc/self/cmdline not available.\"\"\"","    with patch(\"pathlib.Path.exists\", return_value=False):","        cmd = _safe_cmdline()","        # Should fallback to sys.argv","        assert isinstance(cmd, str)","","","def test_no_exception_on_git_error():","    \"\"\"Ensure git commit extraction never raises.\"\"\"","    with patch(\"pathlib.Path.exists\", side_effect=Exception(\"permission denied\")):","        commit = _safe_git_commit(Path(\"/repo\"))","        assert commit == \"unknown\"","","","def test_repo_root_fallback():","    \"\"\"Test repo root detection falls back to cwd.\"\"\"","    with patch(\"pathlib.Path.exists\", return_value=False):","        # Mock climbing loop","        ident = get_service_identity(service_name=\"test\", db_path=None)","        assert ident[\"repo_root\"] == str(Path.cwd())","","","def test_db_path_expanduser():","    \"\"\"Test that db_path is expanded and resolved.\"\"\"","    # Mock expanduser to return same path","    with patch.object(Path, \"expanduser\", return_value=Path(\"/home/user/test.db\")):","        with patch.object(Path, \"resolve\", return_value=Path(\"/home/user/test.db\")):","            ident = get_service_identity(service_name=\"test\", db_path=Path(\"~/test.db\"))","            assert ident[\"jobs_db_path\"] == \"/home/user/test.db\"","","","def test_env_keys_missing():","    \"\"\"Ensure missing env keys are omitted.\"\"\"","    # Remove some keys","    for key in list(_ALLOWED_ENV_KEYS):","        if key in os.environ:","            del os.environ[key]","    ident = get_service_identity(service_name=\"test\", db_path=None)","    assert ident[\"env\"] == {}","","","def test_identity_json_serializable():","    \"\"\"Ensure identity dict is JSON serializable.\"\"\"","    import json","    ident = get_service_identity(service_name=\"test\", db_path=None)","    # Should not raise","    json.dumps(ident)"]}
{"type":"file_footer","path":"tests/test_service_identity.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_session_classification_mnq.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1840,"sha256":"b2c684c4feef4b91a4d2e42cb703b1f275fe81e27235adf9455e82c61be63235","total_lines":56,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_session_classification_mnq.py","chunk_index":0,"line_start":1,"line_end":56,"content":["","\"\"\"Test session classification for CME.MNQ.\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from data.session.classify import classify_session","from data.session.loader import load_session_profile","","","@pytest.fixture","def mnq_profile(profiles_root: Path) -> Path:","    \"\"\"Load CME.MNQ session profile.\"\"\"","    profile_path = profiles_root / \"CME_MNQ_TPE_v1.yaml\"","    return profile_path","","","def test_mnq_day_session(mnq_profile: Path) -> None:","    \"\"\"Test DAY session classification for CME.MNQ.\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Test DAY session times","    assert classify_session(\"2013/1/1 08:45:00\", profile) == \"DAY\"","    assert classify_session(\"2013/1/1 10:00:00\", profile) == \"DAY\"","    assert classify_session(\"2013/1/1 13:44:59\", profile) == \"DAY\"","    ","    # Test boundary (end is exclusive)","    assert classify_session(\"2013/1/1 13:45:00\", profile) is None","","","def test_mnq_night_session(mnq_profile: Path) -> None:","    \"\"\"Test NIGHT session classification for CME.MNQ.\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Test NIGHT session times (spans midnight)","    assert classify_session(\"2013/1/1 21:00:00\", profile) == \"NIGHT\"","    assert classify_session(\"2013/1/1 23:59:59\", profile) == \"NIGHT\"","    assert classify_session(\"2013/1/2 00:00:00\", profile) == \"NIGHT\"","    assert classify_session(\"2013/1/2 05:59:59\", profile) == \"NIGHT\"","    ","    # Test boundary (end is exclusive)","    assert classify_session(\"2013/1/2 06:00:00\", profile) is None","","","def test_mnq_outside_session(mnq_profile: Path) -> None:","    \"\"\"Test timestamps outside trading sessions.\"\"\"","    profile = load_session_profile(mnq_profile)","    ","    # Between sessions","    assert classify_session(\"2013/1/1 14:00:00\", profile) is None","    assert classify_session(\"2013/1/1 20:59:59\", profile) is None","",""]}
{"type":"file_footer","path":"tests/test_session_classification_mnq.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_session_classification_mxf.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1840,"sha256":"49de68e848fe7cf62586bd65547849a80a6c673476f4a1ca86287e87c479ceb7","total_lines":56,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_session_classification_mxf.py","chunk_index":0,"line_start":1,"line_end":56,"content":["","\"\"\"Test session classification for TWF.MXF.\"\"\"","","from __future__ import annotations","","from pathlib import Path","","import pytest","","from data.session.classify import classify_session","from data.session.loader import load_session_profile","","","@pytest.fixture","def mxf_profile(profiles_root: Path) -> Path:","    \"\"\"Load TWF.MXF session profile.\"\"\"","    profile_path = profiles_root / \"TWF_MXF_TPE_v1.yaml\"","    return profile_path","","","def test_mxf_day_session(mxf_profile: Path) -> None:","    \"\"\"Test DAY session classification for TWF.MXF.\"\"\"","    profile = load_session_profile(mxf_profile)","    ","    # Test DAY session times","    assert classify_session(\"2013/1/1 08:45:00\", profile) == \"DAY\"","    assert classify_session(\"2013/1/1 10:00:00\", profile) == \"DAY\"","    assert classify_session(\"2013/1/1 13:44:59\", profile) == \"DAY\"","    ","    # Test boundary (end is exclusive)","    assert classify_session(\"2013/1/1 13:45:00\", profile) is None","","","def test_mxf_night_session(mxf_profile: Path) -> None:","    \"\"\"Test NIGHT session classification for TWF.MXF.\"\"\"","    profile = load_session_profile(mxf_profile)","    ","    # Test NIGHT session times (spans midnight)","    assert classify_session(\"2013/1/1 15:00:00\", profile) == \"NIGHT\"","    assert classify_session(\"2013/1/1 23:59:59\", profile) == \"NIGHT\"","    assert classify_session(\"2013/1/2 00:00:00\", profile) == \"NIGHT\"","    assert classify_session(\"2013/1/2 04:59:59\", profile) == \"NIGHT\"","    ","    # Test boundary (end is exclusive)","    assert classify_session(\"2013/1/2 05:00:00\", profile) is None","","","def test_mxf_outside_session(mxf_profile: Path) -> None:","    \"\"\"Test timestamps outside trading sessions.\"\"\"","    profile = load_session_profile(mxf_profile)","    ","    # Between sessions","    assert classify_session(\"2013/1/1 14:00:00\", profile) is None","    assert classify_session(\"2013/1/1 14:59:59\", profile) is None","",""]}
{"type":"file_footer","path":"tests/test_session_classification_mxf.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_session_dst_mnq.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6045,"sha256":"39f02f9d2f9f3f0b16bb3e44faf8430443284fb9ed4a5dc43df249d68ae7d067","total_lines":153,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_session_dst_mnq.py","chunk_index":0,"line_start":1,"line_end":153,"content":["","\"\"\"Test DST boundary handling for CME.MNQ.","","Tests that session classification remains correct across DST transitions.","Uses programmatic timezone conversion to avoid manual TPE time errors.","\"\"\"","","from __future__ import annotations","","from datetime import datetime","from pathlib import Path","","import pytest","from zoneinfo import ZoneInfo","","from data.session.classify import classify_session","from data.session.loader import load_session_profile","","","@pytest.fixture","def mnq_v2_profile(profiles_root: Path) -> Path:","    \"\"\"Load CME.MNQ v2 session profile with windows format.\"\"\"","    profile_path = profiles_root / \"CME_MNQ_v2.yaml\"","    return profile_path","","","def _chicago_to_tpe_ts_str(chicago_time_str: str, date_str: str) -> str:","    \"\"\"Convert Chicago time to Taiwan time ts_str for a given date.","    ","    Args:","        chicago_time_str: Time string \"HH:MM:SS\" in Chicago timezone","        date_str: Date string \"YYYY/M/D\" or \"YYYY/MM/DD\"","        ","    Returns:","        Full ts_str \"YYYY/M/D HH:MM:SS\" in Taiwan timezone","    \"\"\"","    # Parse date (handles non-zero-padded)","    date_parts = date_str.split(\"/\")","    y, m, d = int(date_parts[0]), int(date_parts[1]), int(date_parts[2])","    ","    # Parse Chicago time","    time_parts = chicago_time_str.split(\":\")","    hh, mm, ss = int(time_parts[0]), int(time_parts[1]), int(time_parts[2])","    ","    # Create datetime in Chicago timezone","    chicago_tz = ZoneInfo(\"America/Chicago\")","    dt_chicago = datetime(y, m, d, hh, mm, ss, tzinfo=chicago_tz)","    ","    # Convert to Taiwan time","    tpe_tz = ZoneInfo(\"Asia/Taipei\")","    dt_tpe = dt_chicago.astimezone(tpe_tz)","    ","    # Return as \"YYYY/M/D HH:MM:SS\" string (matching input format)","    return f\"{dt_tpe.year}/{dt_tpe.month}/{dt_tpe.day} {dt_tpe.hour:02d}:{dt_tpe.minute:02d}:{dt_tpe.second:02d}\"","","","def test_dst_spring_forward_break(mnq_v2_profile: Path) -> None:","    \"\"\"Test BREAK session classification during DST spring forward (March).","    ","    CME break: 16:00-17:00 CT (Chicago time)","    During DST transition, this break period maps to different Taiwan times.","    But classification should still correctly identify BREAK session.","    \"\"\"","    profile = load_session_profile(mnq_v2_profile)","    ","    # DST spring forward: Second Sunday in March (2024-03-10)","    # Before DST (Standard Time, UTC-6): 16:00 CT maps to different TPE time","    # After DST (Daylight Time, UTC-5): 16:00 CT maps to different TPE time","    ","    # Calculate TPE ts_str for Chicago 16:00:00 on specific dates","    # Before DST (March 9, 2024 - Saturday)","    tpe_before = _chicago_to_tpe_ts_str(\"16:00:00\", \"2024/3/9\")","    tpe_before_end = _chicago_to_tpe_ts_str(\"16:59:59\", \"2024/3/9\")","    ","    # After DST (March 11, 2024 - Monday)","    tpe_after = _chicago_to_tpe_ts_str(\"16:00:00\", \"2024/3/11\")","    tpe_after_end = _chicago_to_tpe_ts_str(\"16:59:59\", \"2024/3/11\")","    ","    # Test break period before DST","    assert classify_session(tpe_before, profile) == \"BREAK\"","    assert classify_session(tpe_before_end, profile) == \"BREAK\"","    ","    # Test break period after DST","    assert classify_session(tpe_after, profile) == \"BREAK\"","    assert classify_session(tpe_after_end, profile) == \"BREAK\"","    ","    # Verify: Same exchange time (16:00 CT) maps to different Taiwan times,","    # but classification is consistent (both are BREAK)","","","def test_dst_fall_back_break(mnq_v2_profile: Path) -> None:","    \"\"\"Test BREAK session classification during DST fall back (November).","    ","    CME break: 16:00-17:00 CT (Chicago time)","    During DST fall back, this break period maps to different Taiwan times.","    But classification should still correctly identify BREAK session.","    \"\"\"","    profile = load_session_profile(mnq_v2_profile)","    ","    # DST fall back: First Sunday in November (2024-11-03)","    # Before DST (Daylight Time, UTC-5): 16:00 CT maps to different TPE time","    # After DST (Standard Time, UTC-6): 16:00 CT maps to different TPE time","    ","    # Calculate TPE ts_str for Chicago 16:00:00 on specific dates","    # Before DST (November 2, 2024 - Saturday)","    tpe_before = _chicago_to_tpe_ts_str(\"16:00:00\", \"2024/11/2\")","    tpe_before_end = _chicago_to_tpe_ts_str(\"16:59:59\", \"2024/11/2\")","    ","    # After DST (November 4, 2024 - Monday)","    tpe_after = _chicago_to_tpe_ts_str(\"16:00:00\", \"2024/11/4\")","    tpe_after_end = _chicago_to_tpe_ts_str(\"16:59:59\", \"2024/11/4\")","    ","    # Test break period before DST","    assert classify_session(tpe_before, profile) == \"BREAK\"","    assert classify_session(tpe_before_end, profile) == \"BREAK\"","    ","    # Test break period after DST","    assert classify_session(tpe_after, profile) == \"BREAK\"","    assert classify_session(tpe_after_end, profile) == \"BREAK\"","    ","    # Verify: Same exchange time (16:00 CT) maps to different Taiwan times,","    # but classification is consistent (both are BREAK)","","","def test_dst_trading_session_consistency(mnq_v2_profile: Path) -> None:","    \"\"\"Test TRADING session classification remains consistent across DST.","    ","    CME trading: 17:00 CT - 16:00 CT (next day)","    This should be correctly identified regardless of DST transitions.","    \"\"\"","    profile = load_session_profile(mnq_v2_profile)","    ","    # Calculate TPE ts_str for Chicago 17:00:00 on specific dates","    # March (before DST, Standard Time)","    tpe_mar_before = _chicago_to_tpe_ts_str(\"17:00:00\", \"2024/3/9\")","    assert classify_session(tpe_mar_before, profile) == \"TRADING\"","    ","    # March (after DST, Daylight Time)","    tpe_mar_after = _chicago_to_tpe_ts_str(\"17:00:00\", \"2024/3/11\")","    assert classify_session(tpe_mar_after, profile) == \"TRADING\"","    ","    # November (before DST, Daylight Time)","    tpe_nov_before = _chicago_to_tpe_ts_str(\"17:00:00\", \"2024/11/2\")","    assert classify_session(tpe_nov_before, profile) == \"TRADING\"","    ","    # November (after DST, Standard Time)","    tpe_nov_after = _chicago_to_tpe_ts_str(\"17:00:00\", \"2024/11/4\")","    assert classify_session(tpe_nov_after, profile) == \"TRADING\"","    ","    # Verify: Exchange time 17:00 CT is consistently classified as TRADING,","    # regardless of how it maps to Taiwan time due to DST","",""]}
{"type":"file_footer","path":"tests/test_session_dst_mnq.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_snapshot_compiler.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8242,"sha256":"3ebc85bbfbacfa718b87b24bc82ecefc7a26ecc83087c12470fdea3ef580dab7","total_lines":233,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_snapshot_compiler.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Test snapshot compiler deterministic compilation.","","Contract:","- Temp snapshots/full/ with known content files including LOCAL_SCAN_RULES.json","- Run compile_full_snapshot(snapshots_root=tmp_path/...)","- Assert output exists.","- Assert section order includes LOCAL_SCAN_RULES.json after MANIFEST.json.","- Assert raw content substrings match verbatim.","- Determinism: run twice; assert output bytes identical.","\"\"\"","","import tempfile","import json","import hashlib","from pathlib import Path","import pytest","","from control.snapshot_compiler import (","    compile_full_snapshot,","    verify_deterministic,",")","","","def test_compile_full_snapshot_basic(tmp_path: Path):","    \"\"\"Test basic compilation with minimal artifacts.\"\"\"","    snapshots_root = tmp_path / \"snapshots\"","    full_dir = snapshots_root / \"full\"","    full_dir.mkdir(parents=True)","    ","    # Create required files (some may be missing, that's OK)","    (full_dir / \"MANIFEST.json\").write_text(json.dumps({","        \"generated_at_utc\": \"2025-12-26T11:00:00Z\",","        \"git_head\": \"abc123\",","        \"scan_mode\": \"local-strict\",","        \"file_count\": 1,","        \"files\": [],","    }))","    ","    (full_dir / \"LOCAL_SCAN_RULES.json\").write_text(json.dumps({","        \"mode\": \"local-strict\",","        \"allowed_roots\": [\"src\", \"tests\"],","        \"max_files\": 20000,","    }))","    ","    (full_dir / \"REPO_TREE.txt\").write_text(\"src/a.py\\ntests/test.py\\n\")","    (full_dir / \"SKIPPED_FILES.txt\").write_text(\"TOO_LARGE\\tbig.bin\\n\")","    (full_dir / \"AUDIT_IMPORTS.csv\").write_text(\"file,lineno,kind,module,name\\n\")","    (full_dir / \"AUDIT_ENTRYPOINTS.md\").write_text(\"# Entrypoints\\n\")","    (full_dir / \"AUDIT_CALL_GRAPH.txt\").write_text(\"call graph\\n\")","    (full_dir / \"AUDIT_RUNTIME_MUTATIONS.txt\").write_text(\"mutations\\n\")","    (full_dir / \"AUDIT_STATE_FLOW.md\").write_text(\"# State Flow\\n\")","    (full_dir / \"AUDIT_CONFIG_REFERENCES.txt\").write_text(\"config refs\\n\")","    (full_dir / \"AUDIT_TEST_SURFACE.txt\").write_text(\"test surface\\n\")","    ","    # Run compiler","    out_path = compile_full_snapshot(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"SYSTEM_FULL_SNAPSHOT.md\",","    )","    ","    assert out_path.exists()","    assert out_path.name == \"SYSTEM_FULL_SNAPSHOT.md\"","    assert out_path.parent == snapshots_root","    ","    content = out_path.read_text(encoding=\"utf-8\")","    ","    # Check section order","    lines = content.splitlines()","    section_titles = []","    for line in lines:","        if line.startswith(\"## \"):","            section_titles.append(line)","    ","    # Should have sections in order","    assert any(\"MANIFEST.json\" in title for title in section_titles)","    assert any(\"LOCAL_SCAN_RULES.json\" in title for title in section_titles)","    assert any(\"REPO_TREE.txt\" in title for title in section_titles)","    ","    # Check that LOCAL_SCAN_RULES.json appears after MANIFEST.json","    manifest_idx = next(i for i, t in enumerate(section_titles) if \"MANIFEST.json\" in t)","    local_scan_idx = next(i for i, t in enumerate(section_titles) if \"LOCAL_SCAN_RULES.json\" in t)","    assert local_scan_idx > manifest_idx, \"LOCAL_SCAN_RULES.json should be after MANIFEST.json\"","    ","    # Check content is embedded verbatim","    assert '\"allowed_roots\": [\"src\", \"tests\"]' in content","    assert \"src/a.py\" in content","    assert \"TOO_LARGE\" in content","    ","    # Check fenced code blocks","    assert \"```json\" in content","    assert \"```text\" in content or \"```txt\" in content or \"```\" in content","","","def test_compile_deterministic(tmp_path: Path):","    \"\"\"Run compilation twice and ensure identical bytes.\"\"\"","    snapshots_root = tmp_path / \"snapshots\"","    full_dir = snapshots_root / \"full\"","    full_dir.mkdir(parents=True)","    ","    # Create simple files","    (full_dir / \"MANIFEST.json\").write_text(json.dumps({\"test\": 1}))","    (full_dir / \"LOCAL_SCAN_RULES.json\").write_text(json.dumps({\"mode\": \"test\"}))","    (full_dir / \"REPO_TREE.txt\").write_text(\"tree\")","    ","    # First compilation","    out_path = compile_full_snapshot(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_SNAPSHOT.md\",","    )","    ","    first_content = out_path.read_bytes()","    first_hash = hashlib.sha256(first_content).hexdigest()","    ","    # Second compilation (should be identical)","    out_path2 = compile_full_snapshot(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_SNAPSHOT.md\",","    )","    ","    second_content = out_path2.read_bytes()","    second_hash = hashlib.sha256(second_content).hexdigest()","    ","    assert first_hash == second_hash, \"Output should be deterministic\"","    assert first_content == second_content, \"Bytes should be identical\"","","","def test_missing_files_section(tmp_path: Path):","    \"\"\"Test that missing files are listed in Missing Files section.\"\"\"","    snapshots_root = tmp_path / \"snapshots\"","    full_dir = snapshots_root / \"full\"","    full_dir.mkdir(parents=True)","    ","    # Create only one file","    (full_dir / \"MANIFEST.json\").write_text(json.dumps({}))","    ","    out_path = compile_full_snapshot(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_SNAPSHOT.md\",","    )","    ","    content = out_path.read_text(encoding=\"utf-8\")","    ","    # Should have Missing Files section","    assert \"Missing Files\" in content","    # Should list LOCAL_SCAN_RULES.json as missing","    assert \"LOCAL_SCAN_RULES.json\" in content","    assert \"REPO_TREE.txt\" in content","","","def test_verify_deterministic(tmp_path: Path):","    \"\"\"Test the verify_deterministic helper.\"\"\"","    snapshots_root = tmp_path / \"snapshots\"","    full_dir = snapshots_root / \"full\"","    full_dir.mkdir(parents=True)","    ","    (full_dir / \"MANIFEST.json\").write_text(json.dumps({\"a\": 1}))","    (full_dir / \"LOCAL_SCAN_RULES.json\").write_text(json.dumps({\"b\": 2}))","    ","    # Should not raise and return True","    result = verify_deterministic(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_VERIFY.md\",","    )","    assert result is True, \"Should be deterministic\"","    ","    # Now make a non-deterministic change (timestamp in MANIFEST)","    import time","    (full_dir / \"MANIFEST.json\").write_text(json.dumps({\"a\": 1, \"time\": time.time()}))","    ","    # This should still be deterministic because we read the same file twice","    # (content hasn't changed between the two runs)","    result = verify_deterministic(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_VERIFY2.md\",","    )","    assert result is True, \"Should still be deterministic (same input between runs)\"","","","def test_encoding_handling(tmp_path: Path):","    \"\"\"Test that non-UTF-8 files are handled gracefully.\"\"\"","    snapshots_root = tmp_path / \"snapshots\"","    full_dir = snapshots_root / \"full\"","    full_dir.mkdir(parents=True)","    ","    # Create a binary file (simulate corrupted text)","    (full_dir / \"MANIFEST.json\").write_text(json.dumps({\"test\": \"正常\"}))  # Chinese chars","    (full_dir / \"LOCAL_SCAN_RULES.json\").write_text(json.dumps({\"mode\": \"test\"}))","    ","    # Create a file with invalid UTF-8 sequence","    (full_dir / \"REPO_TREE.txt\").write_bytes(b\"normal text \\xff\\xfe invalid \\x00\")","    ","    # Should not crash"]}
{"type":"file_chunk","path":"tests/test_snapshot_compiler.py","chunk_index":1,"line_start":201,"line_end":233,"content":["    out_path = compile_full_snapshot(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_ENCODING.md\",","    )","    ","    assert out_path.exists()","    # Should contain replacement characters or survive","    content = out_path.read_text(encoding=\"utf-8\", errors=\"ignore\")","    assert \"normal text\" in content","","","def test_empty_snapshots_dir(tmp_path: Path):","    \"\"\"Test with empty snapshots/full directory.\"\"\"","    snapshots_root = tmp_path / \"snapshots\"","    full_dir = snapshots_root / \"full\"","    full_dir.mkdir(parents=True)","    ","    # No files at all","    out_path = compile_full_snapshot(","        snapshots_root=str(snapshots_root),","        full_dir_name=\"full\",","        out_name=\"TEST_EMPTY.md\",","    )","    ","    assert out_path.exists()","    content = out_path.read_text(encoding=\"utf-8\")","    assert \"Missing Files\" in content","    assert all(fname in content for fname in [\"MANIFEST.json\", \"LOCAL_SCAN_RULES.json\", \"REPO_TREE.txt\"])","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_snapshot_compiler.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_sparse_intents_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12830,"sha256":"0a8f0fd239ed566e25bf87ab035e95afef6f43d08c383821d8446325029ec2c6","total_lines":341,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_sparse_intents_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Stage P2-3A: Contract Tests for Sparse Entry Intents (Grid Level)","","Verifies that entry intents are truly sparse at grid level:","- entry_intents_total == entry_valid_mask_sum (not Bars × Params)","- Sparse builder produces identical results to dense builder (same triggers)","\"\"\"","from __future__ import annotations","","from dataclasses import asdict, is_dataclass","","import numpy as np","import os","","from engine.types import Fill","from pipeline.runner_grid import run_grid","","","def _fill_to_tuple(f: Fill) -> tuple:","    \"\"\"","    Convert Fill to a comparable tuple representation.","    ","    Uses dataclasses.asdict for dataclass instances, falls back to __dict__ or repr.","    Returns sorted tuple to ensure deterministic comparison.","    \"\"\"","    if is_dataclass(f):","        d = asdict(f)","    else:","        # fallback: __dict__ (for normal classes)","        d = dict(getattr(f, \"__dict__\", {}))","        if not d:","            # last resort: repr","            return (repr(f),)","    # Fixed ordering to avoid dict order differences","    return tuple(sorted(d.items()))","","","def test_grid_sparse_intents_count() -> None:","    \"\"\"","    Test that grid-level entry intents count scales with trigger_rate (param-subsample).","    ","    This test verifies the core sparse contract at grid level:","    - entry_intents_total == entry_valid_mask_sum","    - entry_intents_total scales approximately linearly with trigger_rate","    \"\"\"","    # Ensure clean environment","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    old_param_subsample_rate = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","    old_profile_grid = os.environ.pop(\"FISHBRO_PROFILE_GRID\", None)","    ","    try:","        n_bars = 500","        n_params = 30  # Enough params to make \"unique repetition\" meaningful","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix (at least 10-50 params for meaningful unique repetition)","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)  # Vary channel_len (20-29)","            atr_len = 10 + (i % 5)  # Vary atr_len (10-14)","            stop_mult = 1.0 + (i % 3) * 0.5  # Vary stop_mult (1.0, 1.5, 2.0)","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Fix param_subsample_rate=1.0 (all params) to test trigger_rate effect on intents","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"1.0\"","        os.environ[\"FISHBRO_PROFILE_GRID\"] = \"1\"","        ","        # Run Dense (trigger_rate=1.0) - baseline","        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","        ","        result_dense = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Run Sparse (trigger_rate=0.05) - bar/intent-level sparsity","        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"0.05\"","        ","        result_sparse = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify perf dicts exist","        perf_dense = result_dense.get(\"perf\", {})","        perf_sparse = result_sparse.get(\"perf\", {})","        ","        assert isinstance(perf_dense, dict), \"perf_dense must be a dict\"","        assert isinstance(perf_sparse, dict), \"perf_sparse must be a dict\"","        ","        # Core contract: entry_intents_total == entry_valid_mask_sum (both runs)","        entry_intents_dense = perf_dense.get(\"entry_intents_total\")","        entry_valid_mask_dense = perf_dense.get(\"entry_valid_mask_sum\")","        entry_intents_sparse = perf_sparse.get(\"entry_intents_total\")","        entry_valid_mask_sparse = perf_sparse.get(\"entry_valid_mask_sum\")","        ","        assert entry_intents_dense == entry_valid_mask_dense, (","            f\"Dense: entry_intents_total ({entry_intents_dense}) \"","            f\"must equal entry_valid_mask_sum ({entry_valid_mask_dense})\"","        )","        assert entry_intents_sparse == entry_valid_mask_sparse, (","            f\"Sparse: entry_intents_total ({entry_intents_sparse}) \"","            f\"must equal entry_valid_mask_sum ({entry_valid_mask_sparse})\"","        )","        ","        # Contract: entry_intents_sparse should be approximately trigger_rate * entry_intents_dense","        # With trigger_rate=0.05, we expect approximately 5% of dense baseline","        # Allow wide tolerance: [0.02, 0.08] (2% to 8% of dense)","        if entry_intents_dense is not None and entry_intents_dense > 0:","            ratio = entry_intents_sparse / entry_intents_dense","            assert 0.02 <= ratio <= 0.08, (","                f\"With trigger_rate=0.05, entry_intents_sparse ({entry_intents_sparse}) \"","                f\"should be approximately 5% of entry_intents_dense ({entry_intents_dense}), \"","                f\"got ratio {ratio:.4f} (expected [0.02, 0.08])\"","            )","        ","    finally:","        # Restore environment","        if old_trigger_rate is None:","            os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","        ","        if old_param_subsample_rate is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = old_param_subsample_rate","        ","        if old_profile_grid is None:","            os.environ.pop(\"FISHBRO_PROFILE_GRID\", None)","        else:","            os.environ[\"FISHBRO_PROFILE_GRID\"] = old_profile_grid","","","def test_sparse_vs_dense_builder_parity() -> None:","    \"\"\"","    Test that sparse builder produces identical results to dense builder (same triggers).","    ","    This test verifies determinism parity:","    - Same triggers set → same results (metrics, fills)","    - Order ID determinism","    - Bit-exact parity","    ","    Uses FISHBRO_FORCE_SPARSE_BUILDER=1 to test numba builder vs python builder.","    \"\"\"","    # Ensure clean environment","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    old_force_sparse = os.environ.pop(\"FISHBRO_FORCE_SPARSE_BUILDER\", None)","    ","    try:","        n_bars = 300","        n_params = 20","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)","            atr_len = 10 + (i % 5)","            stop_mult = 1.0 + (i % 3) * 0.5","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Run A: trigger_rate=1.0, force_sparse=0 (Python builder)"]}
{"type":"file_chunk","path":"tests/test_sparse_intents_contract.py","chunk_index":1,"line_start":201,"line_end":341,"content":["        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","        os.environ.pop(\"FISHBRO_FORCE_SPARSE_BUILDER\", None)  # Ensure not set","        ","        result_a = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Run B: trigger_rate=1.0, force_sparse=1 (Numba builder, same triggers)","        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","        os.environ[\"FISHBRO_FORCE_SPARSE_BUILDER\"] = \"1\"","        ","        result_b = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify metrics are identical (bit-exact)","        metrics_a = result_a.get(\"metrics\")","        metrics_b = result_b.get(\"metrics\")","        ","        assert metrics_a is not None, \"metrics_a must exist\"","        assert metrics_b is not None, \"metrics_b must exist\"","        ","        # Compare metrics arrays (should be bit-exact)","        np.testing.assert_array_equal(metrics_a, metrics_b, \"metrics must be bit-exact\")","        ","        # Verify sparse contract holds in both runs","        perf_a = result_a.get(\"perf\", {})","        perf_b = result_b.get(\"perf\", {})","        ","        if isinstance(perf_a, dict) and isinstance(perf_b, dict):","            entry_intents_a = perf_a.get(\"entry_intents_total\")","            entry_intents_b = perf_b.get(\"entry_intents_total\")","            ","            if entry_intents_a is not None and entry_intents_b is not None:","                assert entry_intents_a == entry_intents_b, (","                    f\"entry_intents_total should be identical (same triggers): \"","                    f\"A={entry_intents_a}, B={entry_intents_b}\"","                )","        ","    finally:","        # Restore environment","        if old_trigger_rate is None:","            os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","        ","        if old_force_sparse is None:","            os.environ.pop(\"FISHBRO_FORCE_SPARSE_BUILDER\", None)","        else:","            os.environ[\"FISHBRO_FORCE_SPARSE_BUILDER\"] = old_force_sparse","","","def test_created_bar_sorted() -> None:","    \"\"\"","    Test that created_bar arrays are sorted (ascending).","    ","    Note: This test verifies the sparse builder contract that created_bar must be","    sorted. We verify this indirectly through the sparse contract consistency.","    \"\"\"","    # Ensure clean environment","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    ","    try:","        n_bars = 200","        n_params = 10","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 5)","            atr_len = 10 + (i % 3)","            stop_mult = 1.0","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Run grid","        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","        ","        result = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify sparse contract: entry_intents_total == entry_valid_mask_sum","        perf = result.get(\"perf\", {})","        if isinstance(perf, dict):","            entry_intents_total = perf.get(\"entry_intents_total\")","            entry_valid_mask_sum = perf.get(\"entry_valid_mask_sum\")","            ","            if entry_intents_total is not None and entry_valid_mask_sum is not None:","                assert entry_intents_total == entry_valid_mask_sum, (","                    f\"Sparse contract: entry_intents_total ({entry_intents_total}) \"","                    f\"must equal entry_valid_mask_sum ({entry_valid_mask_sum})\"","                )","        ","        # Note: created_bar sorted verification would require accessing internal arrays","        # For now, we verify the sparse contract which implies created_bar is sorted","        # (since flatnonzero returns sorted indices)","        ","    finally:","        # Restore environment","        if old_trigger_rate is None:","            os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","",""]}
{"type":"file_footer","path":"tests/test_sparse_intents_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_sparse_intents_mvp_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9494,"sha256":"2e83caf5381232abf321f25a168af841a70f1988f9c3a9f0283e776450811c56","total_lines":255,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_sparse_intents_mvp_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for sparse intents MVP (Stage P2-1).","","These tests ensure:","1. created_bar is sorted (deterministic ordering)","2. intents_total drops significantly with sparse masking","3. Vectorization parity remains bit-exact","\"\"\"","","import numpy as np","import pytest","","from config.dtypes import INDEX_DTYPE","from engine.types import BarArrays","from strategy.kernel import (","    DonchianAtrParams,","    _build_entry_intents_from_trigger,","    run_kernel_arrays,",")","","","def _expected_entry_count(donch_prev: np.ndarray, warmup: int) -> int:","    \"\"\"","    Calculate expected entry count using the same mask rules as production.","    ","    Production mask (from _build_entry_intents_from_trigger):","    - i = np.arange(1, n)  # bar indices t (from 1 to n-1)","    - valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)","    ","    This helper replicates that exact logic.","    \"\"\"","    n = donch_prev.size","    # Create index array for bars 1..n-1 (bar indices t, where created_bar = t-1)","    i = np.arange(1, n, dtype=INDEX_DTYPE)","    # Sparse mask: valid entries must be finite, positive, and past warmup","    valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)","    return int(np.count_nonzero(valid_mask))","","","def _make_donch_hi_with_trigger_rate(","    n_bars: int,","    warmup: int,","    trigger_rate: float,","    seed: int = 42,",") -> np.ndarray:","    \"\"\"","    Generate donch_hi array with controlled trigger rate.","    ","    Args:","        n_bars: number of bars","        warmup: warmup period (bars before warmup are NaN)","        trigger_rate: fraction of bars after warmup that should be valid (0.0-1.0)","        seed: random seed","    ","    Returns:","        donch_hi array (float64, n_bars):","        - Bars 0..warmup-1: NaN","        - Bars warmup..n_bars-1: trigger_rate fraction are positive values, rest are NaN","    \"\"\"","    rng = np.random.default_rng(seed)","    ","    donch_hi = np.full(n_bars, np.nan, dtype=np.float64)","    ","    # After warmup, set trigger_rate fraction to positive values","    post_warmup_bars = n_bars - warmup","    if post_warmup_bars > 0:","        n_valid = int(post_warmup_bars * trigger_rate)","        if n_valid > 0:","            # Select random indices after warmup","            valid_indices = rng.choice(","                np.arange(warmup, n_bars),","                size=n_valid,","                replace=False,","            )","            # Set valid indices to positive values (e.g., 100.0 + small random)","            donch_hi[valid_indices] = 100.0 + rng.random(n_valid) * 10.0","    ","    return donch_hi","","","class TestSparseIntentsMVP:","    \"\"\"Test sparse intents MVP contract.\"\"\"","","    def test_sparse_intents_created_bar_is_sorted(self):","        \"\"\"","        Contract: created_bar must be sorted (non-decreasing).","        ","        This ensures deterministic ordering and that sparse masking preserves","        the original bar sequence.","        \"\"\"","        n_bars = 1000","        warmup = 20","        trigger_rate = 0.1","        ","        # Generate donch_hi with controlled trigger rate","        donch_hi = _make_donch_hi_with_trigger_rate(n_bars, warmup, trigger_rate, seed=42)","        ","        # Create donch_prev (shifted for next-bar active)","        donch_prev = np.empty_like(donch_hi)","        donch_prev[0] = np.nan","        donch_prev[1:] = donch_hi[:-1]","        ","        # Build entry intents","        result = _build_entry_intents_from_trigger(","            donch_prev=donch_prev,","            channel_len=warmup,","            order_qty=1,","        )","        ","        created_bar = result[\"created_bar\"]","        n_entry = result[\"n_entry\"]","        ","        # Verify n_entry matches expected count (exact match using production mask rules)","        expected = _expected_entry_count(donch_prev, warmup)","        assert n_entry == expected, (","            f\"n_entry ({n_entry}) should equal expected ({expected}) \"","            f\"calculated using production mask rules\"","        )","        ","        # Verify created_bar is sorted (non-decreasing)","        if n_entry > 1:","            assert np.all(created_bar[1:] >= created_bar[:-1]), (","                f\"created_bar must be sorted (non-decreasing). \"","                f\"Got: {created_bar[:10]} ... (showing first 10)\"","            )","        ","        # Hard consistency check: created_bar must match flatnonzero result exactly","        # This locks in the ordering contract","        i = np.arange(1, donch_prev.size, dtype=INDEX_DTYPE)","        valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)","        idx = np.flatnonzero(valid_mask).astype(created_bar.dtype)","        assert np.array_equal(created_bar[:n_entry], idx), (","            f\"created_bar must exactly match flatnonzero result. \"","            f\"Got: {created_bar[:min(10, n_entry)]}, \"","            f\"Expected: {idx[:min(10, len(idx))]}\"","        )","","    def test_sparse_intents_total_drops_order_of_magnitude(self):","        \"\"\"","        Contract: intents_total should drop significantly with sparse masking.","        ","        With controlled trigger rate (e.g., 5%), intents_total should be << n_bars.","        This test directly controls donch_hi to ensure precise trigger rate.","        \"\"\"","        n_bars = 1000","        warmup = 20","        trigger_rate = 0.05  # 5% trigger rate","        ","        # Generate donch_hi with controlled trigger rate","        donch_hi = _make_donch_hi_with_trigger_rate(n_bars, warmup, trigger_rate, seed=42)","        ","        # Create donch_prev (shifted for next-bar active)","        donch_prev = np.empty_like(donch_hi)","        donch_prev[0] = np.nan","        donch_prev[1:] = donch_hi[:-1]","        ","        # Build entry intents","        result = _build_entry_intents_from_trigger(","            donch_prev=donch_prev,","            channel_len=warmup,","            order_qty=1,","        )","        ","        n_entry = result[\"n_entry\"]","        obs = result[\"obs\"]","        ","        # Verify diagnostic observations","        assert obs[\"n_bars\"] == n_bars","        assert obs[\"warmup\"] == warmup","        assert obs[\"valid_mask_sum\"] == n_entry","        ","        # Verify n_entry matches expected count (exact match using production mask rules)","        expected = _expected_entry_count(donch_prev, warmup)","        assert n_entry == expected, (","            f\"n_entry ({n_entry}) should equal expected ({expected}) \"","            f\"calculated using production mask rules\"","        )","        ","        # Order-of-magnitude contract: n_entry should be significantly less than n_bars","        # This is the core contract of this test","        # Conservative threshold: 6% of (n_bars - warmup) as upper bound","        max_expected_ratio = 0.06  # 6% conservative upper bound","        max_expected = int((n_bars - warmup) * max_expected_ratio)","        ","        assert n_entry <= max_expected, (","            f\"n_entry ({n_entry}) should be <= {max_expected} \"","            f\"({max_expected_ratio*100}% of post-warmup bars) \"","            f\"with trigger_rate={trigger_rate}, n_bars={n_bars}, warmup={warmup}. \"","            f\"Sparse masking should significantly reduce intent count (order-of-magnitude reduction).\"","        )","        ","        # Also verify it's not zero (unless trigger_rate is too low)","        if trigger_rate > 0:","            # With 5% trigger rate, we should have some intents","            assert n_entry > 0, (","                f\"Expected some intents with trigger_rate={trigger_rate}, \"","                f\"but got n_entry={n_entry}\"","            )","","    def test_vectorization_parity_still_bit_exact(self):"]}
{"type":"file_chunk","path":"tests/test_sparse_intents_mvp_contract.py","chunk_index":1,"line_start":201,"line_end":255,"content":["        \"\"\"","        Contract: Vectorization parity tests should still pass after sparse masking.","        ","        This test ensures that sparse masking doesn't break existing parity contracts.","        We rely on the existing test_vectorization_parity.py to verify this.","        ","        This test is a placeholder to document the requirement.","        \"\"\"","        # This test doesn't need to re-implement parity checks.","        # It's sufficient to ensure that make check passes all existing tests.","        # The actual parity verification is in tests/test_vectorization_parity.py","        ","        # Basic sanity check: sparse masking should produce valid results","        n_bars = 100","        bars = BarArrays(","            open=np.arange(100, 200, dtype=np.float64),","            high=np.arange(101, 201, dtype=np.float64),","            low=np.arange(99, 199, dtype=np.float64),","            close=np.arange(100, 200, dtype=np.float64),","        )","        ","        params = DonchianAtrParams(","            channel_len=10,","            atr_len=5,","            stop_mult=1.5,","        )","        ","        result = run_kernel_arrays(","            bars,","            params,","            commission=0.0,","            slip=0.0,","            order_qty=1,","        )","        ","        # Verify result structure is intact","        assert \"fills\" in result","        assert \"metrics\" in result","        assert \"_obs\" in result","        assert \"intents_total\" in result[\"_obs\"]","        ","        # Verify diagnostic observations are present","        assert \"n_bars\" in result[\"_obs\"]","        assert \"warmup\" in result[\"_obs\"]","        assert \"valid_mask_sum\" in result[\"_obs\"]","        ","        # Verify intents_total is reasonable","        intents_total = result[\"_obs\"][\"intents_total\"]","        assert intents_total >= 0","        assert intents_total <= n_bars  # Should be <= n_bars due to sparse masking","        ","        # Note: Full parity verification is done by test_vectorization_parity.py","        # This test just ensures the basic contract is met","",""]}
{"type":"file_footer","path":"tests/test_sparse_intents_mvp_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_state_processor_serialization.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":10908,"sha256":"5afc673e001c00d7ebf93dbd1206797b7927187d0f8ff3b8180d638ef18d9bd2","total_lines":354,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_state_processor_serialization.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Test StateProcessor serial execution for Attack #9.","","Tests that StateProcessor executes intents sequentially (single consumer)","and produces consistent SystemState snapshots.","\"\"\"","","import pytest","import asyncio","import time","from datetime import date, datetime","from concurrent.futures import ThreadPoolExecutor","","from core.intents import (","    CreateJobIntent, CalculateUnitsIntent, DataSpecIntent,","    IntentStatus, IntentType",")","from control.action_queue import ActionQueue, reset_action_queue","from core.processor import StateProcessor, ProcessingError, get_processor","from core.state import SystemState, JobStatus, create_initial_state","","","@pytest.fixture","def action_queue():","    \"\"\"Create a fresh ActionQueue for each test.\"\"\"","    reset_action_queue()","    queue = ActionQueue(max_size=100)","    yield queue","    queue.clear()","","","@pytest.fixture","def processor(action_queue):","    \"\"\"Create a StateProcessor with fresh queue.\"\"\"","    return StateProcessor(action_queue)","","","@pytest.fixture","def sample_data_spec():","    \"\"\"Create a sample DataSpecIntent for testing.\"\"\"","    return DataSpecIntent(","        dataset_id=\"test_dataset\",","        symbols=[\"MNQ\", \"MXF\"],","        timeframes=[\"60m\", \"120m\"],","        start_date=date(2020, 1, 1),","        end_date=date(2024, 12, 31)","    )","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_sequential_execution(processor, action_queue, sample_data_spec):","    \"\"\"Test that processor executes intents sequentially.\"\"\"","    # Start processor","    await processor.start()","    ","    # Submit multiple intents","    intent_ids = []","    for i in range(5):","        intent = CalculateUnitsIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        intent_id = processor.submit_intent(intent)","        intent_ids.append(intent_id)","    ","    # Wait for all intents to complete","    completed_count = 0","    start_time = time.time()","    timeout = 5.0","    ","    while completed_count < 5 and time.time() - start_time < timeout:","        completed_count = 0","        for intent_id in intent_ids:","            intent = action_queue.get_intent(intent_id)","            if intent and intent.status in [IntentStatus.COMPLETED, IntentStatus.FAILED]:","                completed_count += 1","        await asyncio.sleep(0.1)","    ","    # All intents should be completed","    assert completed_count == 5","    ","    # Check that they were processed in order (FIFO)","    # Since we can't easily track exact order without timestamps in test,","    # we at least verify all were processed","    metrics = action_queue.get_metrics()","    assert metrics[\"processed\"] == 5","    ","    await processor.stop()","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_state_updates(processor, action_queue, sample_data_spec):","    \"\"\"Test that processor updates system state correctly.\"\"\"","    # Start processor","    await processor.start()","    ","    # Get initial state","    initial_state = processor.get_state()","    assert initial_state.metrics.total_jobs == 0","    ","    # Submit a job creation intent","    intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=sample_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10, \"window_slow\": 30}","    )","    ","    intent_id = processor.submit_intent(intent)","    ","    # Wait for completion","    completed = await processor.wait_for_intent(intent_id, timeout=5.0)","    assert completed is not None","    assert completed.status == IntentStatus.COMPLETED","    ","    # Check that state was updated","    final_state = processor.get_state()","    assert final_state.metrics.total_jobs == 1","    assert final_state.metrics.queued_jobs == 1","    ","    # Job should be in state","    job_id = completed.result[\"job_id\"]","    job = final_state.get_job(job_id)","    assert job is not None","    assert job.season == \"2024Q1\"","    assert job.status == JobStatus.QUEUED","    ","    await processor.stop()","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_error_handling(processor, action_queue):","    \"\"\"Test that processor handles errors gracefully.\"\"\"","    # Start processor","    await processor.start()","    ","    # Submit an invalid intent (missing required fields)","    # We'll create a malformed intent by directly manipulating a valid one","    from core.intents import CreateJobIntent, DataSpecIntent","    ","    # Create a data spec with empty symbols (should fail validation)","    invalid_data_spec = DataSpecIntent(","        dataset_id=\"test_dataset\",","        symbols=[],  # Empty - should fail validation","        timeframes=[\"60m\"],","        start_date=date(2020, 1, 1),","        end_date=date(2024, 12, 31)","    )","    ","    intent = CreateJobIntent(","        season=\"2024Q1\",","        data1=invalid_data_spec,","        data2=None,","        strategy_id=\"sma_cross_v1\",","        params={\"window_fast\": 10}","    )","    ","    intent_id = processor.submit_intent(intent)","    ","    # Wait for completion (should fail)","    completed = await processor.wait_for_intent(intent_id, timeout=5.0)","    assert completed is not None","    assert completed.status == IntentStatus.FAILED","    assert completed.error_message is not None","    assert \"validation\" in completed.error_message.lower() or \"empty\" in completed.error_message.lower()","    ","    # Check metrics","    state = processor.get_state()","    assert state.intent_queue.failed_count == 1","    ","    await processor.stop()","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_concurrent_submission(processor, action_queue, sample_data_spec):","    \"\"\"Test that processor handles concurrent intent submissions correctly.\"\"\"","    # Start processor","    await processor.start()","    ","    # Submit intents from multiple threads","    intent_ids = []","    ","    async def submit_intent(i: int):","        intent = CalculateUnitsIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        intent_id = processor.submit_intent(intent)","        intent_ids.append(intent_id)","        return intent_id","    ","    # Submit concurrently","    tasks = [submit_intent(i) for i in range(10)]","    await asyncio.gather(*tasks)"]}
{"type":"file_chunk","path":"tests/test_state_processor_serialization.py","chunk_index":1,"line_start":201,"line_end":354,"content":["    ","    # Wait for all to complete","    for intent_id in intent_ids:","        completed = await processor.wait_for_intent(intent_id, timeout=5.0)","        assert completed is not None","        assert completed.status == IntentStatus.COMPLETED","    ","    # All should be processed","    state = processor.get_state()","    assert state.intent_queue.completed_count == 10","    ","    await processor.stop()","","","def test_state_immutability():","    \"\"\"Test that SystemState is immutable (read-only).\"\"\"","    # Create initial state","    state = create_initial_state()","    ","    # Try to modify attributes (should fail or create new object)","    # Since Pydantic models with frozen=True raise ValidationError on modification","    with pytest.raises(Exception):","        state.metrics.total_jobs = 100  # Should fail","    ","    # Verify state hasn't changed","    assert state.metrics.total_jobs == 0","","","def test_state_snapshot_creation():","    \"\"\"Test creating state snapshots with updates.\"\"\"","    # Create initial state","    state = create_initial_state()","    ","    # Create snapshot with updates","    from core.state import create_state_snapshot, SystemMetrics","    ","    new_metrics = SystemMetrics(","        total_jobs=5,","        active_jobs=2,","        queued_jobs=3,","        completed_jobs=0,","        failed_jobs=0,","        total_units_processed=100,","        units_per_second=10.0,","        memory_usage_mb=512.0,","        cpu_usage_percent=25.0,","        disk_usage_gb=5.0,","        snapshot_timestamp=datetime.now(),","        uptime_seconds=3600.0","    )","    ","    new_state = create_state_snapshot(","        state,","        metrics=new_metrics,","        is_healthy=True","    )","    ","    # New state should have updated values","    assert new_state.metrics.total_jobs == 5","    assert new_state.metrics.active_jobs == 2","    assert new_state.is_healthy is True","    ","    # Original state should be unchanged","    assert state.metrics.total_jobs == 0","    assert state.metrics.active_jobs == 0","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_get_state_snapshot(processor):","    \"\"\"Test that get_state() returns consistent snapshots.\"\"\"","    # Start processor","    await processor.start()","    ","    # Get multiple state snapshots","    state1 = processor.get_state()","    await asyncio.sleep(0.1)","    state2 = processor.get_state()","    ","    # Snapshots should be different objects","    assert state1 is not state2","    assert state1.state_id != state2.state_id","    ","    # But should have same basic structure","    assert isinstance(state1, SystemState)","    assert isinstance(state2, SystemState)","    ","    await processor.stop()","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_queue_status_updates(processor, action_queue, sample_data_spec):","    \"\"\"Test that processor updates queue status in state.\"\"\"","    # Start processor","    await processor.start()","    ","    # Submit some intents","    for i in range(3):","        intent = CalculateUnitsIntent(","            season=f\"2024Q{i}\",","            data1=sample_data_spec,","            data2=None,","            strategy_id=\"sma_cross_v1\",","            params={\"window_fast\": i}","        )","        processor.submit_intent(intent)","    ","    # Wait a bit for processing to start","    await asyncio.sleep(0.5)","    ","    # Check queue status in state","    state = processor.get_state()","    assert state.intent_queue.queue_size >= 0","    assert state.intent_queue.completed_count >= 0","    ","    # Wait for all to complete","    await asyncio.sleep(2.0)","    ","    # Final state should show all completed","    final_state = processor.get_state()","    assert final_state.intent_queue.completed_count == 3","    ","    await processor.stop()","","","def test_processor_singleton():","    \"\"\"Test that get_processor() returns singleton instance.\"\"\"","    # Reset to ensure clean state","    reset_action_queue()","    ","    # First call should create instance","    processor1 = get_processor()","    assert processor1 is not None","    ","    # Second call should return same instance","    processor2 = get_processor()","    assert processor2 is processor1","","","@pytest.mark.skip(reason=\"Async tests require pytest-asyncio\")","async def test_processor_stop_before_start():","    \"\"\"Test that processor can be stopped even if not started.\"\"\"","    from control.action_queue import ActionQueue","    from core.processor import StateProcessor","    ","    queue = ActionQueue()","    processor = StateProcessor(queue)","    ","    # Should not raise exception","    await processor.stop()","","","if __name__ == \"__main__\":","    # Run tests","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_state_processor_serialization.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_strategy_contract_purity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3435,"sha256":"d2ce83161e6bf0994ed3e0794396eaad44a609238fcf10846bf313f48b83c47f","total_lines":133,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_strategy_contract_purity.py","chunk_index":0,"line_start":1,"line_end":133,"content":["","\"\"\"Test strategy contract purity.","","Phase 7: Test that same input produces same output (deterministic).","\"\"\"","","from __future__ import annotations","","import numpy as np","import pytest","","from strategy.registry import get, load_builtin_strategies, clear","from engine.types import OrderIntent","","","@pytest.fixture(autouse=True)","def setup_registry() -> None:","    \"\"\"Setup registry before each test.\"\"\"","    clear()","    load_builtin_strategies()","    yield","    clear()","","","def test_sma_cross_purity() -> None:","    \"\"\"Test SMA cross strategy is deterministic.\"\"\"","    spec = get(\"sma_cross\")","    ","    # Create test features","    sma_fast = np.array([10.0, 11.0, 12.0, 13.0, 14.0])","    sma_slow = np.array([15.0, 14.0, 13.0, 12.0, 11.0])  # Cross at index 3","    ","    context = {","        \"bar_index\": 3,","        \"order_qty\": 1,","        \"features\": {","            \"sma_fast\": sma_fast,","            \"sma_slow\": sma_slow,","        },","    }","    ","    params = {","        \"fast_period\": 10.0,","        \"slow_period\": 20.0,","    }","    ","    # Run multiple times","    result1 = spec.fn(context, params)","    result2 = spec.fn(context, params)","    result3 = spec.fn(context, params)","    ","    # All results should be identical","    assert result1 == result2 == result3","    ","    # Check intents are identical","    intents1 = result1[\"intents\"]","    intents2 = result2[\"intents\"]","    intents3 = result3[\"intents\"]","    ","    assert len(intents1) == len(intents2) == len(intents3)","    ","    if len(intents1) > 0:","        # Compare intent attributes","        for i, (i1, i2, i3) in enumerate(zip(intents1, intents2, intents3)):","            assert i1.order_id == i2.order_id == i3.order_id","            assert i1.created_bar == i2.created_bar == i3.created_bar","            assert i1.role == i2.role == i3.role","            assert i1.kind == i2.kind == i3.kind","            assert i1.side == i2.side == i3.side","            assert i1.price == i2.price == i3.price","            assert i1.qty == i2.qty == i3.qty","","","def test_breakout_channel_purity() -> None:","    \"\"\"Test breakout channel strategy is deterministic.\"\"\"","    spec = get(\"breakout_channel\")","    ","    # Create test features","    high = np.array([100.0, 101.0, 102.0, 103.0, 105.0])","    close = np.array([99.0, 100.0, 101.0, 102.0, 104.0])","    channel_high = np.array([102.0, 102.0, 102.0, 102.0, 102.0])","    ","    context = {","        \"bar_index\": 4,","        \"order_qty\": 1,","        \"features\": {","            \"high\": high,","            \"close\": close,","            \"channel_high\": channel_high,","        },","    }","    ","    params = {","        \"channel_period\": 20.0,","    }","    ","    # Run multiple times","    result1 = spec.fn(context, params)","    result2 = spec.fn(context, params)","    ","    # Results should be identical","    assert result1 == result2","","","def test_mean_revert_zscore_purity() -> None:","    \"\"\"Test mean reversion z-score strategy is deterministic.\"\"\"","    spec = get(\"mean_revert_zscore\")","    ","    # Create test features","    zscore = np.array([-1.0, -1.5, -2.0, -2.5, -3.0])","    close = np.array([100.0, 99.0, 98.0, 97.0, 96.0])","    ","    context = {","        \"bar_index\": 2,","        \"order_qty\": 1,","        \"features\": {","            \"zscore\": zscore,","            \"close\": close,","        },","    }","    ","    params = {","        \"zscore_threshold\": -2.0,","    }","    ","    # Run multiple times","    result1 = spec.fn(context, params)","    result2 = spec.fn(context, params)","    ","    # Results should be identical","    assert result1 == result2","",""]}
{"type":"file_footer","path":"tests/test_strategy_contract_purity.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_strategy_registry.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4096,"sha256":"3cb58d0cd55cf2179f11f50b5df95394f7310b0f90066fb4ff76e1acb3eb597c","total_lines":172,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_strategy_registry.py","chunk_index":0,"line_start":1,"line_end":172,"content":["","\"\"\"Test strategy registry.","","Phase 7: Test registry list/get/register behavior is deterministic.","\"\"\"","","from __future__ import annotations","","import pytest","","from strategy.registry import (","    register,","    get,","    list_strategies,","    unregister,","    clear,","    load_builtin_strategies,",")","from strategy.spec import StrategySpec","","","def test_register_and_get() -> None:","    \"\"\"Test register and get operations.\"\"\"","    clear()","    ","    # Create a test strategy","    def test_fn(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {}}","    ","    spec = StrategySpec(","        strategy_id=\"test_strategy\",","        version=\"v1\",","        param_schema={\"type\": \"object\", \"properties\": {}},","        defaults={},","        fn=test_fn,","    )","    ","    # Register","    register(spec)","    ","    # Get","    retrieved = get(\"test_strategy\")","    assert retrieved.strategy_id == \"test_strategy\"","    assert retrieved.version == \"v1\"","    ","    # Cleanup","    unregister(\"test_strategy\")","","","def test_register_duplicate_raises() -> None:","    \"\"\"Test registering duplicate strategy_id raises ValueError.\"\"\"","    clear()","    ","    def test_fn1(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {}}","    ","    def test_fn2(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {\"different\": True}}","    ","    spec1 = StrategySpec(","        strategy_id=\"duplicate\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=test_fn1,","    )","    ","    spec2 = StrategySpec(","        strategy_id=\"duplicate\",","        version=\"v2\",","        param_schema={},","        defaults={},","        fn=test_fn2,","    )","    ","    register(spec1)","    ","    with pytest.raises(ValueError, match=\"already registered\"):","        register(spec2)","    ","    # Cleanup","    unregister(\"duplicate\")","","","def test_get_nonexistent_raises() -> None:","    \"\"\"Test getting nonexistent strategy raises KeyError.\"\"\"","    clear()","    ","    with pytest.raises(KeyError, match=\"not found\"):","        get(\"nonexistent\")","","","def test_list_strategies() -> None:","    \"\"\"Test list_strategies returns sorted list.\"\"\"","    clear()","    ","    def test_fn_a(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {\"fn\": \"a\"}}","    ","    def test_fn_b(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {\"fn\": \"b\"}}","    ","    def test_fn_c(context: dict, params: dict) -> dict:","        return {\"intents\": [], \"debug\": {\"fn\": \"c\"}}","    ","    # Register multiple strategies with different functions","    spec_b = StrategySpec(","        strategy_id=\"b_strategy\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=test_fn_b,","    )","    ","    spec_a = StrategySpec(","        strategy_id=\"a_strategy\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=test_fn_a,","    )","    ","    spec_c = StrategySpec(","        strategy_id=\"c_strategy\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=test_fn_c,","    )","    ","    register(spec_b)","    register(spec_a)","    register(spec_c)","    ","    # List should be sorted by strategy_id","    strategies = list_strategies()","    assert len(strategies) == 3","    assert strategies[0].strategy_id == \"a_strategy\"","    assert strategies[1].strategy_id == \"b_strategy\"","    assert strategies[2].strategy_id == \"c_strategy\"","    ","    # Cleanup","    clear()","","","def test_load_builtin_strategies() -> None:","    \"\"\"Test load_builtin_strategies registers built-in strategies.\"\"\"","    clear()","    ","    load_builtin_strategies()","    ","    strategies = list_strategies()","    strategy_ids = [s.strategy_id for s in strategies]","    ","    assert \"sma_cross\" in strategy_ids","    assert \"breakout_channel\" in strategy_ids","    assert \"mean_revert_zscore\" in strategy_ids","    ","    # Verify they can be retrieved","    sma_spec = get(\"sma_cross\")","    assert sma_spec.version == \"v1\"","    ","    breakout_spec = get(\"breakout_channel\")","    assert breakout_spec.version == \"v1\"","    ","    zscore_spec = get(\"mean_revert_zscore\")","    assert zscore_spec.version == \"v1\"","    ","    # Cleanup","    clear()","",""]}
{"type":"file_footer","path":"tests/test_strategy_registry.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_strategy_runner_outputs_intents.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3822,"sha256":"061705e5e9d3df47400ffa9a35cb3649fccb18d969f463af7d9b1347ad897dca","total_lines":143,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_strategy_runner_outputs_intents.py","chunk_index":0,"line_start":1,"line_end":143,"content":["","\"\"\"Test strategy runner outputs valid intents.","","Phase 7: Test that runner returns valid OrderIntent schema.","\"\"\"","","from __future__ import annotations","","import numpy as np","import pytest","","from strategy.runner import run_strategy","from strategy.registry import load_builtin_strategies, clear","from engine.types import OrderIntent, OrderRole, OrderKind, Side","","","@pytest.fixture(autouse=True)","def setup_registry() -> None:","    \"\"\"Setup registry before each test.\"\"\"","    clear()","    load_builtin_strategies()","    yield","    clear()","","","def test_runner_outputs_intents_schema() -> None:","    \"\"\"Test runner outputs valid OrderIntent schema.\"\"\"","    # Create test features","    sma_fast = np.array([10.0, 11.0, 12.0, 13.0, 14.0])","    sma_slow = np.array([15.0, 14.0, 13.0, 12.0, 11.0])","    ","    features = {","        \"sma_fast\": sma_fast,","        \"sma_slow\": sma_slow,","    }","    ","    params = {","        \"fast_period\": 10.0,","        \"slow_period\": 20.0,","    }","    ","    context = {","        \"bar_index\": 3,","        \"order_qty\": 1,","    }","    ","    # Run strategy","    intents = run_strategy(\"sma_cross\", features, params, context)","    ","    # Verify intents is a list","    assert isinstance(intents, list)","    ","    # Verify each intent is OrderIntent","    for intent in intents:","        assert isinstance(intent, OrderIntent)","        ","        # Verify required fields","        assert isinstance(intent.order_id, int)","        assert isinstance(intent.created_bar, int)","        assert isinstance(intent.role, OrderRole)","        assert isinstance(intent.kind, OrderKind)","        assert isinstance(intent.side, Side)","        assert isinstance(intent.price, float)","        assert isinstance(intent.qty, int)","        ","        # Verify values are reasonable","        assert intent.order_id > 0","        assert intent.created_bar >= 0","        assert intent.price > 0","        assert intent.qty > 0","","","def test_runner_uses_defaults() -> None:","    \"\"\"Test runner uses default parameters when missing.\"\"\"","    features = {","        \"sma_fast\": np.array([10.0, 11.0]),","        \"sma_slow\": np.array([15.0, 14.0]),","    }","    ","    # Missing params - should use defaults","    params = {}","    ","    context = {","        \"bar_index\": 1,","        \"order_qty\": 1,","    }","    ","    # Should not raise - defaults should be used","    intents = run_strategy(\"sma_cross\", features, params, context)","    assert isinstance(intents, list)","","","def test_runner_allows_extra_params() -> None:","    \"\"\"Test runner allows extra parameters (logs warning but doesn't fail).\"\"\"","    features = {","        \"sma_fast\": np.array([10.0, 11.0]),","        \"sma_slow\": np.array([15.0, 14.0]),","    }","    ","    # Extra param not in schema","    params = {","        \"fast_period\": 10.0,","        \"slow_period\": 20.0,","        \"extra_param\": 999.0,  # Not in schema","    }","    ","    context = {","        \"bar_index\": 1,","        \"order_qty\": 1,","    }","    ","    # Should not raise - extra params allowed","    intents = run_strategy(\"sma_cross\", features, params, context)","    assert isinstance(intents, list)","","","def test_runner_invalid_output_raises() -> None:","    \"\"\"Test runner raises ValueError for invalid strategy output.\"\"\"","    from strategy.registry import register","    from strategy.spec import StrategySpec","    ","    # Create a bad strategy that returns invalid output","    def bad_strategy(context: dict, params: dict) -> dict:","        return {\"invalid\": \"output\"}  # Missing \"intents\" key","    ","    bad_spec = StrategySpec(","        strategy_id=\"bad_strategy\",","        version=\"v1\",","        param_schema={},","        defaults={},","        fn=bad_strategy,","    )","    ","    register(bad_spec)","    ","    with pytest.raises(ValueError, match=\"must contain 'intents' key\"):","        run_strategy(\"bad_strategy\", {}, {}, {\"bar_index\": 0})","    ","    # Cleanup","    from strategy.registry import unregister","    unregister(\"bad_strategy\")","",""]}
{"type":"file_footer","path":"tests/test_strategy_runner_outputs_intents.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_submit_returns_503_when_worker_missing.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":16245,"sha256":"54c37644c882d760292e890644f35c112c85f541450c13e02db81e5d5b4cc443","total_lines":422,"chunk_count":3}
{"type":"file_chunk","path":"tests/test_submit_returns_503_when_worker_missing.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Test that job submission returns HTTP 503 when worker is unavailable.","","EOOR500 → HTTP 503 (WORKER-AWARE) requirement:","- All job submission endpoints must return HTTP 503 Service Unavailable when worker is unavailable","- Never return HTTP 500 for worker unavailability","- Error message must mention worker explicitly","- JSON response must include diagnostic details","\"\"\"","","from __future__ import annotations","","import tempfile","import os","from pathlib import Path","from unittest.mock import patch, MagicMock","","import pytest","from fastapi.testclient import TestClient","","from control.api import app, get_db_path","from control.jobs_db import init_db","","","@pytest.fixture","def test_client_no_worker() -> TestClient:","    \"\"\"Create test client with temporary database and no worker.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        db_path = Path(tmpdir) / \"test.db\"","        init_db(db_path)","        ","        # Save original environment variables","        original_jobs_db_path = os.environ.get(\"JOBS_DB_PATH\")","        original_allow_tmp_db = os.environ.get(\"FISHBRO_ALLOW_TMP_DB\")","        original_allow_spawn = os.environ.get(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\")","        ","        # Override DB path","        os.environ[\"JOBS_DB_PATH\"] = str(db_path)","        # Allow /tmp DB paths (required for temporary DB)","        os.environ[\"FISHBRO_ALLOW_TMP_DB\"] = \"1\"","        # DO NOT allow worker spawn in tests for this fixture (we want to test 503)","        os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = \"0\"","        ","        # Re-import to get new DB path","        from control import api","        ","        # Reinitialize","        api.init_db(db_path)","        ","        # Mock worker status to simulate no worker","        with patch('control.api._check_worker_status') as mock_check, \\","             patch('control.api.load_dataset_index') as mock_load_dataset:","            mock_check.return_value = {","                \"alive\": False,","                \"pid\": None,","                \"last_heartbeat_age_sec\": None,","                \"reason\": \"pidfile missing\",","                \"expected_db\": str(db_path),","            }","            # Mock dataset index to avoid FileNotFoundError","            from data.dataset_registry import DatasetIndex, DatasetRecord","            from datetime import date","            mock_index = DatasetIndex(","                generated_at=\"2024-01-01T00:00:00Z\",","                datasets=[","                    DatasetRecord(","                        id=\"test_dataset\",","                        symbol=\"TEST\",","                        exchange=\"TEST\",","                        timeframe=\"60m\",","                        path=\"TEST/60m/2020-2024.parquet\",","                        start_date=date(2020, 1, 1),","                        end_date=date(2024, 12, 31),","                        fingerprint_sha256_40=\"d\" * 40,","                        tz_provider=\"IANA\",","                        tz_version=\"unknown\",","                    )","                ]","            )","            mock_load_dataset.return_value = mock_index","            try:","                yield TestClient(app)","            finally:","                # Restore original environment variables","                if original_jobs_db_path is not None:","                    os.environ[\"JOBS_DB_PATH\"] = original_jobs_db_path","                else:","                    os.environ.pop(\"JOBS_DB_PATH\", None)","                if original_allow_tmp_db is not None:","                    os.environ[\"FISHBRO_ALLOW_TMP_DB\"] = original_allow_tmp_db","                else:","                    os.environ.pop(\"FISHBRO_ALLOW_TMP_DB\", None)","                if original_allow_spawn is not None:","                    os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = original_allow_spawn","                else:","                    os.environ.pop(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\", None)","","","@pytest.fixture","def test_client_with_worker() -> TestClient:","    \"\"\"Create test client with temporary database and worker running.\"\"\"","    with tempfile.TemporaryDirectory() as tmpdir:","        db_path = Path(tmpdir) / \"test.db\"","        init_db(db_path)","        ","        # Save original environment variables","        original_jobs_db_path = os.environ.get(\"JOBS_DB_PATH\")","        original_allow_tmp_db = os.environ.get(\"FISHBRO_ALLOW_TMP_DB\")","        original_allow_spawn = os.environ.get(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\")","        ","        # Override DB path","        os.environ[\"JOBS_DB_PATH\"] = str(db_path)","        os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = \"1\"","        os.environ[\"FISHBRO_ALLOW_TMP_DB\"] = \"1\"","        ","        # Re-import to get new DB path","        from control import api","        ","        # Reinitialize","        api.init_db(db_path)","        ","        # Mock worker status to simulate worker running","        with patch('control.api._check_worker_status') as mock_check, \\","             patch('control.api.load_dataset_index') as mock_load_dataset:","            mock_check.return_value = {","                \"alive\": True,","                \"pid\": 12345,","                \"last_heartbeat_age_sec\": 1.0,","                \"reason\": \"worker alive\",","                \"expected_db\": str(db_path),","            }","            # Mock dataset index to avoid FileNotFoundError","            from data.dataset_registry import DatasetIndex, DatasetRecord","            from datetime import date","            mock_index = DatasetIndex(","                generated_at=\"2024-01-01T00:00:00Z\",","                datasets=[","                    DatasetRecord(","                        id=\"test_dataset\",","                        symbol=\"TEST\",","                        exchange=\"TEST\",","                        timeframe=\"60m\",","                        path=\"TEST/60m/2020-2024.parquet\",","                        start_date=date(2020, 1, 1),","                        end_date=date(2024, 12, 31),","                        fingerprint_sha256_40=\"d\" * 40,","                        tz_provider=\"IANA\",","                        tz_version=\"unknown\",","                    )","                ]","            )","            mock_load_dataset.return_value = mock_index","            try:","                yield TestClient(app)","            finally:","                # Restore original environment variables","                if original_jobs_db_path is not None:","                    os.environ[\"JOBS_DB_PATH\"] = original_jobs_db_path","                else:","                    os.environ.pop(\"JOBS_DB_PATH\", None)","                if original_allow_tmp_db is not None:","                    os.environ[\"FISHBRO_ALLOW_TMP_DB\"] = original_allow_tmp_db","                else:","                    os.environ.pop(\"FISHBRO_ALLOW_TMP_DB\", None)","                if original_allow_spawn is not None:","                    os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = original_allow_spawn","                else:","                    os.environ.pop(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\", None)","","","def test_submit_job_returns_503_when_worker_missing(test_client_no_worker: TestClient) -> None:","    \"\"\"Test POST /jobs returns 503 when worker is unavailable.\"\"\"","    req = {","        \"season\": \"test_season\",","        \"dataset_id\": \"test_dataset\",","        \"outputs_root\": \"outputs\",","        \"config_snapshot\": {\"bars\": 1000, \"params_total\": 100},","        \"config_hash\": \"abc123\",","        \"created_by\": \"b5c\",","    }","    ","    resp = test_client_no_worker.post(\"/jobs\", json=req)","    ","    # Must return HTTP 503, not 500","    assert resp.status_code == 503, f\"Expected 503, got {resp.status_code}\"","    ","    # Must have proper JSON structure","    data = resp.json()","    assert \"detail\" in data","    detail = data[\"detail\"]","    ","    # Error message must mention worker (detail is a dict with \"message\" field)","    assert isinstance(detail, dict)","    assert \"message\" in detail","    assert \"worker\" in detail[\"message\"].lower(), f\"Error message should mention worker: {detail['message']}\"","    ","    # Should not be generic 500 error","    assert \"internal server error\" not in detail[\"message\"].lower()","    ","    # Check response structure matches our error format","    assert \"error\" in detail"]}
