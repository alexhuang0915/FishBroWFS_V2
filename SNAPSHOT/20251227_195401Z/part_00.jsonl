{"type":"meta","schema_version":2,"run_id":"20251227_195401Z","repo_root":"/home/fishbro/FishBroWFS_V2","snapshot_root":"SNAPSHOT","part":0,"parts":10,"created_at":"2025-12-27T19:54:01Z","generator":{"name":"dump_context","version":"vNext-balanced"},"policies":{"max_parts":10,"target_bytes_per_part":500000,"chunk_max_lines":200,"chunk_max_bytes":120000,"outputs_max_bytes_for_content":2000000,"outputs_policy":"manifest_only_for_big_files","forbid_file_truncated":true,"balance_parts":true,"estimated_total_bytes":3514686,"balance_buffer_ratio":1.08}}
{"type":"file_header","path":"Makefile","kind":"text","encoding":"utf-8","newline":"lf","bytes":2565,"sha256":"05d5c878e2ec1057a8bb7fb7f2128eb41000bc45c81d07211fa00b4e4c0783d0","total_lines":75,"chunk_count":1}
{"type":"file_chunk","path":"Makefile","chunk_index":0,"line_start":1,"line_end":75,"content":["# =========================================================","# FishBroWFS Makefile (V3 War Room Edition)","# Fixed by Gemini - 2025-12-28","# =========================================================","","# FORCE PYTHON VIRTUAL ENVIRONMENT (CRITICAL FIX)","PYTHON := .venv/bin/python","",".PHONY: help check test precommit clean-cache clean-all clean-caches clean-caches-dry compile gui war-room run-research run-plateau run-freeze run-compile run-season snapshot","","# ---------------------------------------------------------","# Help","# ---------------------------------------------------------","help:","\t@echo \"\"","\t@echo \"FishBroWFS Strategy Factory V3\"","\t@echo \"\"","\t@echo \"UI:\"","\t@echo \"  make gui             Launch War Room UI\"","\t@echo \"\"","\t@echo \"Pipeline:\"","\t@echo \"  make run-research    [Phase 2]  Backtest\"","\t@echo \"  make run-plateau     [Phase 3A] Plateau\"","\t@echo \"  make run-freeze      [Phase 3B] Freeze\"","\t@echo \"  make run-compile     [Phase 3C] Compile\"","\t@echo \"\"","","# ---------------------------------------------------------","# GUI Entry Point (Corrected)","# ---------------------------------------------------------","gui:","\t@echo \"==> Launching FishBro War Room...\"","\t# ä½¿ç”¨ PYTHONPATH=src ç¢ºä¿èƒ½æŠ“åˆ° FishBroWFS_V2 æ¨¡çµ„","\tPYTHONDONTWRITEBYTECODE=1 PYTHONPATH=src $(PYTHON) -B main.py","","war-room: gui","","# ---------------------------------------------------------","# V3 Production Shortcuts","# ---------------------------------------------------------","run-research:","\tPYTHONDONTWRITEBYTECODE=1 $(PYTHON) -B scripts/run_research_v3.py","","run-plateau:","\tPYTHONDONTWRITEBYTECODE=1 $(PYTHON) -B scripts/run_phase3a_plateau.py","","run-freeze:","\tPYTHONDONTWRITEBYTECODE=1 $(PYTHON) -B scripts/run_phase3b_freeze.py","","run-compile:","\tPYTHONDONTWRITEBYTECODE=1 $(PYTHON) -B scripts/run_phase3c_compile.py","","run-season: run-research run-plateau run-freeze run-compile","","snapshot:","\t@echo \"==> Generating Context Snapshot...\"","\t$(PYTHON) scripts/dump_context.py","","# ---------------------------------------------------------","# Testing","# ---------------------------------------------------------","check:","\t@echo \"==> Running fast CI-safe tests (excluding slow research-grade tests)...\"","\tPYTHONDONTWRITEBYTECODE=1 $(PYTHON) -m pytest","","test:","\t@echo \"==> Running all tests (including slow research-grade tests)...\"","\tPYTHONDONTWRITEBYTECODE=1 $(PYTHON) -m pytest -m \"slow or not slow\"","","# ---------------------------------------------------------","# Cleaning","# ---------------------------------------------------------","clean-all:","\tfind . -type d -name \"__pycache__\" -exec rm -rf {} +","\trm -rf .pytest_cache .mypy_cache"]}
{"type":"file_footer","path":"Makefile","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"pyproject.toml","kind":"text","encoding":"utf-8","newline":"crlf","bytes":563,"sha256":"b2e37303429374f11aef4db14512b19fd6ba6d7e0727fb7fb49a7c467cf23a97","total_lines":26,"chunk_count":1}
{"type":"file_chunk","path":"pyproject.toml","chunk_index":0,"line_start":1,"line_end":26,"content":["[project]","name = \"FishBroWFS_V2\"","version = \"0.1.0\"","description = \"Speed-first backtesting engine\"","requires-python = \">=3.10\"","dependencies = [\"numpy\"]","","[build-system]","requires = [\"setuptools\"]","build-backend = \"setuptools.build_meta\"","","[tool.setuptools]","package-dir = {\"\" = \"src\"}","","[tool.setuptools.packages.find]","where = [\"src\"]","","[tool.pytest.ini_options]","testpaths = [\"tests\"]","pythonpath = [\"src\"]","markers = [","    \"slow: research-grade tests\",","    \"integration: integration tests requiring external services\"","]","addopts = \"-m 'not slow'\"",""]}
{"type":"file_footer","path":"pyproject.toml","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"README.md","kind":"text","encoding":"utf-8","newline":"lf","bytes":3941,"sha256":"d7dace4656f5344ac14db494528213b2142958cb8385ad88f13962349edc5738","total_lines":114,"chunk_count":1}
{"type":"file_chunk","path":"README.md","chunk_index":0,"line_start":1,"line_end":114,"content":["# FishBroWFS_V2","","Speed-first quantitative backtesting engine.","","This repository uses tests as the primary specification.","","## Testing tiers","","- **`make check`**: Fast, CI-safe tests (excludes slow research-grade tests)","- **`make research`**: Slow, manual research-grade tests (full backtest + correlation validation)","","## Performance tiers","","- **`make perf`**: Baseline (20000Ã—1000) - Fast, suitable for commit-to-commit comparison (hot_runs=5, timeout=600s)","- **`make perf-mid`**: Mid-tier (20000Ã—10000) - Medium-scale performance testing (hot_runs=3, timeout=1200s)","- **`make perf-heavy`**: Heavy-tier (200000Ã—10000) - Full-scale validation (hot_runs=1, timeout=3600s, expensive, use intentionally)","","**Note**: Mid-tier and heavy-tier are not for daily use. Baseline is recommended for regular performance checks.","","See `docs/PERF_HARNESS.md` for detailed usage.","","## Funnel Architecture (WFS at scale)","","This project uses a multi-stage funnel:","","- **Stage 0**: vector/proxy ranking (no matcher, no orders) â€” see `docs/STAGE0_FUNNEL.md`","- Stage 1: light backtest (planned)","- Stage 2: full semantics (matcher + fills) for final candidates","","Stage 0 v0 implementation:","","- `FishBroWFS_V2.stage0.stage0_score_ma_proxy()`","","## GUI (Mission Control + Viewer)","","Start full GUI stack:","","```bash","make gui","```","","**Services:**","","- **Control API**: <http://localhost:8000>","- **Mission Control (NiceGUI)**: <http://localhost:8080>","- **Viewer / Audit Console (Streamlit)**: <http://localhost:8502>","","Press `Ctrl+C` to stop all services.","","## Viewer (Audit Console)","","Start Viewer:","","```bash","PYTHONPATH=src streamlit run src/FishBroWFS_V2/gui/viewer/app.py","```","","**Viewer Pages:**","","- **Overview**: Run overview and summary","- **KPI**: Key Performance Indicators with evidence drill-down","- **Winners**: Winners list and details","- **Governance**: Governance decisions and evidence","- **Artifacts**: Raw artifacts JSON viewer","","**Usage:**","","Viewer requires `season` and `run_id` query parameters:","","```text","http://localhost:8502/?season=2026Q1&run_id=demo_20250101T000000Z","```","","## Snapshot System (Local-Strict Filesystem Truth)","","The snapshot system provides deterministic, auditable repository snapshots using Local-Strict filesystem scanning (not Git-based).","","### Commands","","- **Primary command**: `make snapshot` - Generate full repository forensic snapshot","- **Alias maintained**: `make full-snapshot` - Backward compatibility alias for `make snapshot`","","### Output Artifacts","","- **Raw snapshot artifacts**: `outputs/snapshots/full/` - Contains 12 forensic artifacts including `LOCAL_SCAN_RULES.json`, `REPO_TREE.txt`, `MANIFEST.json`, etc.","- **Compiled snapshot**: `outputs/snapshots/SYSTEM_FULL_SNAPSHOT.md` - Single-file compiled snapshot with all artifacts embedded verbatim","- **Runtime truth**: `outputs/snapshots/runtime/RUNTIME_CONTEXT.md` - Auto-generated on dashboard startup with PID, git commit, port occupancy, governance state","","### Local-Strict Scanning","","- **Purpose**: Eliminate \"UI fog / recursion\" by making the system self-auditing with filesystem truth","- **Includes**: Untracked files within allowed roots (`src/`, `tests/`, `scripts/`, `docs/`)","- **Excludes**: `.gitignore` is ignored (`gitignore_respected=false`)","- **Policy**: Defined in `LOCAL_SCAN_RULES.json` with allowlist/denylist rules","- **Deterministic**: Same inputs produce identical output bytes","","### UI Build Fingerprint","","The dashboard displays a build fingerprint banner at the top:","```","BUILD: <commit> (dirty=<yes/no>) | ENTRY: <module> | SNAPSHOT: <timestamp or UNKNOWN>","```","","This allows instant verification of the running build, entrypoint, and snapshot.","","## é©—æ”¶æµç¨‹ï¼ˆPhase 6.1ï¼‰","","1. `make gui` - å•Ÿå‹•æ‰€æœ‰æœå‹™","2. ç€è¦½å™¨æ‰“é–‹ `http://localhost:8080` - Mission Control","3. é»žæ“Š **Create Demo Job** - å»ºç«‹ demo job","4. DONE job å‡ºç¾ â†’ é»žæ“Š **Open Report** - æ‰“é–‹ Viewer","5. Viewerï¼ˆ8502ï¼‰é¡¯ç¤º KPI è¡¨ + ðŸ” Evidence æ­£å¸¸é¡¯ç¤º","","ðŸ‘‰ **Phase 6.1 é©—æ”¶å®Œæˆ**"]}
{"type":"file_footer","path":"README.md","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"requirements.txt","kind":"text","encoding":"utf-8","newline":"lf","bytes":1687,"sha256":"ecc874655d9d50e324260c49b4e6e08ebab0bacc52e9305ff13b86204f2bf592","total_lines":72,"chunk_count":1}
{"type":"file_chunk","path":"requirements.txt","chunk_index":0,"line_start":1,"line_end":72,"content":["# ===============================","# FishBroWFS_V2 - Core Runtime","# ===============================","","# Numerical foundation (å¿…å‚™)","numpy>=1.24,<2.0","","# Dataframe / K-bar aggregation / artifact analysis (Phase 6.6+)","pandas>=2.1,<3.0","","# YAML profiles (Session Profiles / Config)","PyYAML>=6.0,<7.0","","# Timezone database fallback for environments without OS tzdata (Phase 6.6)","# (optional but strongly recommended for portability / CI)","tzdata>=2023.3","","# ===============================","# Performance & JIT (Phase 2)","# ===============================","","numba>=0.58,<0.61","","# ===============================","# Testing (Phase 0â€“4)","# ===============================","","pytest>=7.4,<9.0","","# Optional but useful if you use pytest markers/coverage later","pytest-cov>=4.1,<6.0","","# [NEW] Critical for avoiding CI hangs (Deadlock protection)","pytest-timeout>=2.2,<3.0","","# [NEW] Standard async support for Pytest (Eliminates async warnings)","pytest-asyncio>=0.23,<1.0","","# ===============================","# GUI (Phase 0â€“4)","# ===============================","","# GUI now uses NiceGUI only; streamlit removed per policy.","# [NEW] Added NiceGUI as it was missing from the list","nicegui>=1.4,<2.0","","# [NEW] Often needed for NiceGUI sessions","itsdangerous>=2.0,<3.0","","# ===============================","# API / B5-C Mission Control (Phase 5)","# ===============================","","fastapi>=0.110","uvicorn>=0.27","httpx>=0.27","","# ===============================","# UI Artifact Validation (Phase 5)","# ===============================","","pydantic>=2.0,<3.0","","# JSON speed (optional but nice for artifacts)","orjson>=3.9,<4.0","","# ===============================","# Packaging / Dev Utilities","# ===============================","","setuptools>=65","wheel"]}
{"type":"file_footer","path":"requirements.txt","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"scripts/__pycache__/generate_research.cpython-312.pyc","reason":"cache","bytes":7527,"sha256":"bc4ce259a13e8f2cff03a6ca4f5825047744ba50e5dc0b712fce556cfb665131","note":"skipped by policy"}
{"type":"file_skipped","path":"scripts/__pycache__/run_research_v3.cpython-312.pyc","reason":"cache","bytes":857,"sha256":"51d45c298a04ea4e24fb039a12ceec7c845f96cc81480850a7efb4fdc9fa76c8","note":"skipped by policy"}
{"type":"file_header","path":"scripts/build_dataset_registry.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6302,"sha256":"60d33c9a3ddc0d774120334b8e2da7084d80d9b01fb1f1c4c7b58092d0b7a531","total_lines":187,"chunk_count":1}
{"type":"file_chunk","path":"scripts/build_dataset_registry.py","chunk_index":0,"line_start":1,"line_end":187,"content":["#!/usr/bin/env python3","\"\"\"Build Dataset Registry from derived data files.","","Phase 12: Automated dataset registry generation.","Scans data/derived/**/* and creates deterministic index.","\"\"\"","","from __future__ import annotations","","import hashlib","import json","import re","from datetime import date, datetime","from pathlib import Path","from typing import List, Optional","","from data.dataset_registry import DatasetIndex, DatasetRecord","","","def parse_filename_to_dates(filename: str) -> Optional[tuple[date, date]]:","    \"\"\"Parse YYYY-YYYY or YYYYMMDD-YYYYMMDD date range from filename.","    ","    Expected patterns:","    - \"2020-2024.parquet\" -> (2020-01-01, 2024-12-31)","    - \"20200101-20241231.parquet\" -> (2020-01-01, 2024-12-31)","    \"\"\"","    # Remove extension","    stem = Path(filename).stem","    ","    # Pattern 1: YYYY-YYYY","    match = re.match(r\"^(\\d{4})-(\\d{4})$\", stem)","    if match:","        start_year = int(match.group(1))","        end_year = int(match.group(2))","        return (","            date(start_year, 1, 1),","            date(end_year, 12, 31)","        )","    ","    # Pattern 2: YYYYMMDD-YYYYMMDD","    match = re.match(r\"^(\\d{8})-(\\d{8})$\", stem)","    if match:","        start_str = match.group(1)","        end_str = match.group(2)","        return (","            date(int(start_str[:4]), int(start_str[4:6]), int(start_str[6:8])),","            date(int(end_str[:4]), int(end_str[4:6]), int(end_str[6:8]))","        )","    ","    return None","","","def compute_file_fingerprints(file_path: Path) -> tuple[str, str]:","    \"\"\"Compute SHA1 and SHA256 (first 40 chars) hashes of file content (binary).","    ","    WARNING: Must use content hash, NOT mtime/size.","    \"\"\"","    sha1 = hashlib.sha1()","    sha256 = hashlib.sha256()","    with open(file_path, \"rb\") as f:","        # Read in chunks to handle large files","        for chunk in iter(lambda: f.read(8192), b\"\"):","            sha1.update(chunk)","            sha256.update(chunk)","    sha1_digest = sha1.hexdigest()","    sha256_digest = sha256.hexdigest()[:40]  # first 40 hex chars","    return sha1_digest, sha256_digest","","","def build_registry(derived_root: Path) -> DatasetIndex:","    \"\"\"Build dataset registry by scanning derived data directory.","    ","    Expected directory structure:","        data/derived/{SYMBOL}/{TIMEFRAME}/{START}-{END}.parquet","    ","    Contract:","    - Delete index â†’ rerun script â†’ produce identical output (deterministic)","    - Index content must have 1:1 correspondence with physical files","    \"\"\"","    datasets: List[DatasetRecord] = []","    ","    # Walk through derived directory","    for symbol_dir in derived_root.iterdir():","        if not symbol_dir.is_dir():","            continue","        ","        symbol = symbol_dir.name","        ","        for timeframe_dir in symbol_dir.iterdir():","            if not timeframe_dir.is_dir():","                continue","            ","            timeframe = timeframe_dir.name","            ","            for parquet_file in timeframe_dir.glob(\"*.parquet\"):","                # Parse date range from filename","                date_range = parse_filename_to_dates(parquet_file.name)","                if not date_range:","                    print(f\"Warning: Skipping {parquet_file} - cannot parse date range\")","                    continue","                ","                start_date, end_date = date_range","                ","                # Validate start_date <= end_date","                if start_date > end_date:","                    print(f\"Warning: Skipping {parquet_file} - start_date > end_date\")","                    continue","                ","                # Compute fingerprints","                fingerprint_sha1, fingerprint_sha256_40 = compute_file_fingerprints(parquet_file)","                ","                # Construct relative path","                rel_path = parquet_file.relative_to(derived_root)","                ","                # Construct dataset ID","                dataset_id = f\"{symbol}.{timeframe}.{start_date.year}-{end_date.year}\"","                ","                # Extract exchange from symbol (e.g., \"CME.MNQ\" -> \"CME\")","                exchange = symbol.split(\".\")[0] if \".\" in symbol else symbol","                ","                # Create dataset record","                record = DatasetRecord(","                    id=dataset_id,","                    symbol=symbol,","                    exchange=exchange,","                    timeframe=timeframe,","                    path=str(rel_path),","                    start_date=start_date,","                    end_date=end_date,","                    fingerprint_sha1=fingerprint_sha1,","                    fingerprint_sha256_40=fingerprint_sha256_40,","                    tz_provider=\"IANA\",","                    tz_version=\"unknown\"","                )","                ","                datasets.append(record)","                print(f\"Registered: {dataset_id} ({start_date} to {end_date})\")","    ","    # Sort datasets for deterministic output","    datasets.sort(key=lambda r: r.id)","    ","    return DatasetIndex(","        generated_at=datetime.now(),","        datasets=datasets","    )","","","def main() -> None:","    \"\"\"Main entry point for CLI.\"\"\"","    import sys","    ","    # Determine paths","    project_root = Path(__file__).parent.parent","    derived_root = project_root / \"data\" / \"derived\"","    output_dir = project_root / \"outputs\" / \"datasets\"","    output_file = output_dir / \"datasets_index.json\"","    ","    # Check if derived directory exists","    if not derived_root.exists():","        print(f\"Error: Derived data directory not found: {derived_root}\")","        print(\"Expected structure: data/derived/{SYMBOL}/{TIMEFRAME}/{START}-{END}.parquet\")","        sys.exit(1)","    ","    # Build registry","    print(f\"Scanning derived data in: {derived_root}\")","    index = build_registry(derived_root)","    ","    # Ensure output directory exists","    output_dir.mkdir(parents=True, exist_ok=True)","    ","    # Write index to file","    with open(output_file, \"w\", encoding=\"utf-8\") as f:","        json_data = index.model_dump_json(indent=2)","        f.write(json_data)","    ","    print(f\"Dataset registry written to: {output_file}\")","    print(f\"Registered {len(index.datasets)} datasets\")","    ","    # Print summary","    if index.datasets:","        print(\"\\nDataset summary:\")","        for record in index.datasets:","            print(f\"  - {record.id}: {record.start_date} to {record.end_date}\")","","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/build_dataset_registry.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/build_features_subset.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1483,"sha256":"fbbbf70b3f7805ea185f86dfe61aeff59b1ae534e19417008fd9299ae64df33a","total_lines":52,"chunk_count":1}
{"type":"file_chunk","path":"scripts/build_features_subset.py","chunk_index":0,"line_start":1,"line_end":52,"content":["#!/usr/bin/env python3","\"\"\"","Build features cache with subset data.","\"\"\"","import sys","from pathlib import Path","","sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))","","from control.shared_build import build_shared","from contracts.features import default_feature_registry","","def main() -> None:","    txt_path = Path(\"FishBroData/raw/CME.MNQ_SUBSET.txt\")","    if not txt_path.exists():","        print(f\"Error: TXT file not found at {txt_path}\")","        sys.exit(1)","    ","    season = \"2026Q1\"","    dataset_id = \"CME.MNQ\"","    outputs_root = Path(\"outputs\")","    ","    print(f\"Building features cache for {dataset_id} season {season}...\")","    try:","        report = build_shared(","            season=season,","            dataset_id=dataset_id,","            txt_path=txt_path,","            outputs_root=outputs_root,","            mode=\"FULL\",","            save_fingerprint=True,","            generated_at_utc=None,","            build_bars=True,","            build_features=True,","            feature_registry=default_feature_registry(),","            tfs=[60],","        )","        ","        if report.get(\"success\"):","            print(\"Features cache built successfully.\")","            print(f\"Report keys: {list(report.keys())}\")","        else:","            print(f\"Build failed: {report.get('error')}\")","            sys.exit(1)","    except Exception as e:","        print(f\"Exception during build: {e}\")","        import traceback","        traceback.print_exc()","        sys.exit(1)","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/build_features_subset.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/build_mnq_shared.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1504,"sha256":"d6c115a66585d1c55c84024f9fcc87c85d153468e8dcb137b13c5687ea0e75fa","total_lines":57,"chunk_count":1}
{"type":"file_chunk","path":"scripts/build_mnq_shared.py","chunk_index":0,"line_start":1,"line_end":57,"content":["#!/usr/bin/env python3","\"\"\"","Build shared cache for MNQ dataset.","\"\"\"","import sys","from pathlib import Path","","sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))","","from control.build_context import BuildContext","from control.shared_build import build_shared","","def main() -> None:","    txt_path = Path(\"FishBroData/raw/CME.MNQ HOT-Minute-Trade.txt\")","    if not txt_path.exists():","        print(f\"Error: TXT file not found at {txt_path}\")","        sys.exit(1)","    ","    season = \"2026Q1\"","    dataset_id = \"CME.MNQ\"","    outputs_root = Path(\"outputs\")","    ","    # Create build context","    ctx = BuildContext(","        txt_path=txt_path,","        mode=\"FULL\",","        outputs_root=outputs_root,","        build_bars_if_missing=True,","        season=season,","        dataset_id=dataset_id,","    )","    ","    # Build shared cache (bars only, no features)","    print(f\"Building shared cache for {dataset_id} season {season}...\")","    report = build_shared(","        season=season,","        dataset_id=dataset_id,","        txt_path=txt_path,","        outputs_root=outputs_root,","        mode=\"FULL\",","        save_fingerprint=True,","        generated_at_utc=None,","        build_bars=True,","        build_features=False,","        feature_registry=None,","        tfs=[15, 30, 60, 120, 240],","    )","    ","    if report.get(\"success\"):","        print(\"Shared cache built successfully.\")","        print(f\"Report: {report}\")","    else:","        print(f\"Build failed: {report.get('error')}\")","        sys.exit(1)","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/build_mnq_shared.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/build_portfolio_from_research.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7979,"sha256":"ce9b101e062ee7e1121f6cc5aba61ba58b34ce3c5cf89568906d39646e5af1dc","total_lines":215,"chunk_count":2}
{"type":"file_chunk","path":"scripts/build_portfolio_from_research.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"CLI for building portfolio from research decisions.\"\"\"","","import argparse","import sys","from pathlib import Path","","# Add src to path","src_dir = Path(__file__).parent.parent / \"src\"","sys.path.insert(0, str(src_dir))","","from portfolio.research_bridge import build_portfolio_from_research","from portfolio.writer import write_portfolio_artifacts","import json","import pandas as pd","from pathlib import Path","","","def create_season_level_portfolio_files(","    outputs_root: Path,","    season: str,","    portfolio_id: str,","    manifest: dict",") -> None:","    \"\"\"Create season-level portfolio files as required by Phase 3 contract.\"\"\"","    ","    season_portfolio_dir = outputs_root / \"seasons\" / season / \"portfolio\"","    season_portfolio_dir.mkdir(parents=True, exist_ok=True)","    ","    # 1. portfolio_summary.json","    summary = {","        \"portfolio_id\": portfolio_id,","        \"season\": season,","        \"generated_at\": manifest.get(\"generated_at\", \"\"),","        \"total_decisions\": manifest[\"counts\"][\"total_decisions\"],","        \"keep_decisions\": manifest[\"counts\"][\"keep_decisions\"],","        \"num_legs_final\": manifest[\"counts\"][\"num_legs_final\"],","        \"symbols_breakdown\": manifest[\"counts\"][\"symbols_breakdown\"],","        \"warnings\": manifest.get(\"warnings\", {})","    }","    ","    summary_path = season_portfolio_dir / \"portfolio_summary.json\"","    with open(summary_path, 'w', encoding='utf-8') as f:","        json.dump(summary, f, ensure_ascii=False, indent=2, sort_keys=True)","    ","    # 2. portfolio_admission.parquet (empty DataFrame with required schema)","    admission_df = pd.DataFrame({","        \"run_id\": [],","        \"symbol\": [],","        \"strategy_id\": [],","        \"decision\": [],","        \"score_final\": [],","        \"timestamp\": []","    })","    admission_path = season_portfolio_dir / \"portfolio_admission.parquet\"","    admission_df.to_parquet(admission_path, index=False)","    ","    # 3. portfolio_state_timeseries.parquet (empty DataFrame with required schema)","    states_df = pd.DataFrame({","        \"timestamp\": [],","        \"portfolio_value\": [],","        \"open_positions_count\": [],","        \"margin_ratio\": []","    })","    states_path = season_portfolio_dir / \"portfolio_state_timeseries.parquet\"","    states_df.to_parquet(states_path, index=False)","    ","    # 4. portfolio_manifest.json (copy from run_id directory with deterministic sorting)","    run_dir = outputs_root / \"seasons\" / season / \"portfolio\" / portfolio_id","    run_manifest_path = run_dir / \"portfolio_manifest.json\"","    ","    if run_manifest_path.exists():","        with open(run_manifest_path, 'r', encoding='utf-8') as f:","            run_manifest = json.load(f)","        ","        # Ensure deterministic sorting","        def sort_dict_recursively(obj):","            if isinstance(obj, dict):","                return {k: sort_dict_recursively(v) for k, v in sorted(obj.items())}","            elif isinstance(obj, list):","                # For lists, sort if all elements are strings or numbers","                if all(isinstance(item, (str, int, float)) for item in obj):","                    return sorted(obj)","                else:","                    return [sort_dict_recursively(item) for item in obj]","            else:","                return obj","        ","        sorted_manifest = sort_dict_recursively(run_manifest)","        manifest_path = season_portfolio_dir / \"portfolio_manifest.json\"","        with open(manifest_path, 'w', encoding='utf-8') as f:","            json.dump(sorted_manifest, f, ensure_ascii=False, indent=2, sort_keys=True)","    else:","        # Create minimal manifest if run directory doesn't exist","        minimal_manifest = {","            \"portfolio_id\": portfolio_id,","            \"season\": season,","            \"generated_at\": manifest.get(\"generated_at\", \"\"),","            \"artifacts\": [","                {\"path\": \"portfolio_summary.json\", \"type\": \"json\"},","                {\"path\": \"portfolio_admission.parquet\", \"type\": \"parquet\"},","                {\"path\": \"portfolio_state_timeseries.parquet\", \"type\": \"parquet\"},","                {\"path\": \"portfolio_manifest.json\", \"type\": \"json\"}","            ]","        }","        manifest_path = season_portfolio_dir / \"portfolio_manifest.json\"","        with open(manifest_path, 'w', encoding='utf-8') as f:","            json.dump(minimal_manifest, f, ensure_ascii=False, indent=2, sort_keys=True)","    ","    print(f\"Season-level portfolio files created in: {season_portfolio_dir}\")","    print(f\"  - {summary_path}\")","    print(f\"  - {admission_path}\")","    print(f\"  - {states_path}\")","    print(f\"  - {season_portfolio_dir / 'portfolio_manifest.json'}\")","","","def main():","    parser = argparse.ArgumentParser(","        description=\"Build portfolio from research decisions\"","    )","    parser.add_argument(","        \"--season\",","        required=True,","        help=\"Season identifier (e.g., 2026Q1)\"","    )","    parser.add_argument(","        \"--outputs-root\",","        default=\"outputs\",","        help=\"Root outputs directory (default: outputs)\"","    )","    parser.add_argument(","        \"--allowlist\",","        default=\"CME.MNQ,TWF.MXF\",","        help=\"Comma-separated list of allowed symbols (default: CME.MNQ,TWF.MXF)\"","    )","    ","    args = parser.parse_args()","    ","    # Phase 5: Check season freeze state before any action","    try:","        # Add src to path","        src_dir = Path(__file__).parent.parent / \"src\"","        sys.path.insert(0, str(src_dir))","        from core.season_state import check_season_not_frozen","        check_season_not_frozen(args.season, action=\"build_portfolio_from_research\")","    except ImportError:","        # If season_state module is not available, skip check (backward compatibility)","        pass","    except ValueError as e:","        print(f\"Error: {e}\", file=sys.stderr)","        return 1","    ","    # Parse allowlist","    symbols_allowlist = set(args.allowlist.split(','))","    ","    # Build paths","    outputs_root = Path(args.outputs_root)","    ","    try:","        print(f\"Building portfolio for season: {args.season}\")","        print(f\"Outputs root: {outputs_root}\")","        print(f\"Symbols allowlist: {symbols_allowlist}\")","        print()","        ","        # Build portfolio","        portfolio_id, spec, manifest = build_portfolio_from_research(","            season=args.season,","            outputs_root=outputs_root,","            symbols_allowlist=symbols_allowlist","        )","        ","        print(f\"Generated portfolio ID: {portfolio_id}\")","        print(f\"Total decisions: {manifest['counts']['total_decisions']}\")","        print(f\"KEEP decisions: {manifest['counts']['keep_decisions']}\")","        print(f\"Final legs: {manifest['counts']['num_legs_final']}\")","        ","        # Write artifacts to run_id directory","        portfolio_dir = write_portfolio_artifacts(","            outputs_root=outputs_root,","            season=args.season,","            spec=spec,","            manifest=manifest","        )","        ","        print(f\"\\nPortfolio artifacts written to: {portfolio_dir}\")","        print(f\"  - {portfolio_dir / 'portfolio_spec.json'}\")","        print(f\"  - {portfolio_dir / 'portfolio_manifest.json'}\")","        print(f\"  - {portfolio_dir / 'README.md'}\")","        ","        # Create season-level portfolio files (Phase 3 contract)","        create_season_level_portfolio_files(","            outputs_root=outputs_root,","            season=args.season,","            portfolio_id=portfolio_id,","            manifest=manifest","        )","        ","        # Print warnings if any","        if manifest.get('warnings', {}).get('missing_run_ids'):","            print(f\"\\nWarnings: {len(manifest['warnings']['missing_run_ids'])} run IDs missing metadata\")"]}
{"type":"file_chunk","path":"scripts/build_portfolio_from_research.py","chunk_index":1,"line_start":201,"line_end":215,"content":["        ","        return 0","        ","    except FileNotFoundError as e:","        print(f\"Error: {e}\", file=sys.stderr)","        print(\"\\nMake sure the research directory exists:\", file=sys.stderr)","        print(f\"  {outputs_root / 'seasons' / args.season / 'research'}\", file=sys.stderr)","        return 1","    except Exception as e:","        print(f\"Error: {e}\", file=sys.stderr)","        return 1","","","if __name__ == \"__main__\":","    sys.exit(main())"]}
{"type":"file_footer","path":"scripts/build_portfolio_from_research.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"scripts/clean_data_cache.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2309,"sha256":"57fd0eabbe71afc7545bc11f04924126cd680f107fdb055ac0068354924776fe","total_lines":87,"chunk_count":1}
{"type":"file_chunk","path":"scripts/clean_data_cache.py","chunk_index":0,"line_start":1,"line_end":87,"content":["","\"\"\"Clean parquet data cache.","","Binding #4: Parquet is Cache, Not Truth.","This script deletes all .parquet and .meta.json files from cache root.","Raw TXT files are never deleted.","\"\"\"","","from __future__ import annotations","","import sys","from pathlib import Path","","","def main() -> int:","    \"\"\"Clean parquet cache files.","    ","    Scans cache_root (default: parquet_cache/) and deletes:","    - All .parquet files","    - All .meta.json files","    ","    Raw TXT files are never deleted.","    ","    Returns:","        0 on success, 1 on error","    \"\"\"","    # Default cache root (can be overridden via env var or config)","    cache_root = Path(\"parquet_cache\")","    ","    # Check if cache root exists","    if not cache_root.exists():","        print(f\"Cache root does not exist: {cache_root}\")","        print(\"Nothing to clean.\")","        return 0","    ","    if not cache_root.is_dir():","        print(f\"Cache root is not a directory: {cache_root}\")","        return 1","    ","    # Find all .parquet and .meta.json files","    parquet_files = list(cache_root.glob(\"*.parquet\"))","    meta_files = list(cache_root.glob(\"*.meta.json\"))","    ","    total_files = len(parquet_files) + len(meta_files)","    ","    if total_files == 0:","        print(f\"No cache files found in {cache_root}\")","        return 0","    ","    print(f\"Found {len(parquet_files)} parquet files and {len(meta_files)} meta files\")","    print(f\"Deleting {total_files} cache files...\")","    ","    deleted_count = 0","    error_count = 0","    ","    # Delete parquet files","    for parquet_file in parquet_files:","        try:","            parquet_file.unlink()","            deleted_count += 1","            print(f\"  Deleted: {parquet_file.name}\")","        except Exception as e:","            print(f\"  Error deleting {parquet_file.name}: {e}\", file=sys.stderr)","            error_count += 1","    ","    # Delete meta files","    for meta_file in meta_files:","        try:","            meta_file.unlink()","            deleted_count += 1","            print(f\"  Deleted: {meta_file.name}\")","        except Exception as e:","            print(f\"  Error deleting {meta_file.name}: {e}\", file=sys.stderr)","            error_count += 1","    ","    print(f\"\\nCompleted: {deleted_count} files deleted, {error_count} errors\")","    ","    if error_count > 0:","        return 1","    ","    return 0","","","if __name__ == \"__main__\":","    sys.exit(main())","",""]}
{"type":"file_footer","path":"scripts/clean_data_cache.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/create_portfolio_spec.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3218,"sha256":"90476c14e0cad53f88233e745de553e1f38b19a96695318b21734939e35d3473","total_lines":99,"chunk_count":1}
{"type":"file_chunk","path":"scripts/create_portfolio_spec.py","chunk_index":0,"line_start":1,"line_end":99,"content":["#!/usr/bin/env python3","\"\"\"Create a portfolio spec for testing.\"\"\"","","import json","import hashlib","from pathlib import Path","import sys","","# Add src to path","sys.path.insert(0, str(Path(__file__).parent / \"src\"))","","from core.schemas.portfolio_v1 import PortfolioPolicyV1, PortfolioSpecV1","from portfolio.artifacts_writer_v1 import compute_policy_sha256, compute_spec_sha256","from portfolio.instruments import load_instruments_config","","def create_policy() -> PortfolioPolicyV1:","    \"\"\"Create a test portfolio policy.\"\"\"","    # Load instruments config to get SHA256","    instruments_cfg = load_instruments_config(Path(\"configs/portfolio/instruments.yaml\"))","    ","    return PortfolioPolicyV1(","        version=\"PORTFOLIO_POLICY_V1\",","        base_currency=\"TWD\",","        instruments_config_sha256=instruments_cfg.sha256,","        max_slots_total=4,","        max_margin_ratio=0.35,  # 35%","        max_notional_ratio=None,","        max_slots_by_instrument={","            \"CME.MNQ\": 2,","            \"TWF.MXF\": 2,","        },","        strategy_priority={","            \"sma_cross\": 10,","            \"mean_revert_zscore\": 20,","        },","        signal_strength_field=\"signal_strength\",","        allow_force_kill=False,","        allow_queue=False,","    )","","def create_spec(policy_sha256: str) -> PortfolioSpecV1:","    \"\"\"Create a test portfolio spec.\"\"\"","    spec = PortfolioSpecV1(","        version=\"PORTFOLIO_SPEC_V1\",","        seasons=[\"2026Q1\"],","        strategy_ids=[\"sma_cross\", \"mean_revert_zscore\"],","        instrument_ids=[\"CME.MNQ\", \"TWF.MXF\"],","        start_date=None,","        end_date=None,","        policy_sha256=policy_sha256,","        spec_sha256=\"\",  # Will be computed","    )","    ","    # Compute spec SHA256","    spec_sha256 = compute_spec_sha256(spec)","    spec.spec_sha256 = spec_sha256","    ","    return spec","","def main():","    \"\"\"Create and save portfolio spec and policy.\"\"\"","    # Create policy","    policy = create_policy()","    policy_sha256 = compute_policy_sha256(policy)","    ","    print(f\"Policy SHA256: {policy_sha256}\")","    ","    # Create spec","    spec = create_spec(policy_sha256)","    ","    print(f\"Spec SHA256: {spec.spec_sha256}\")","    ","    # Save policy","    policy_dict = policy.dict()","    policy_path = Path(\"configs/portfolio/portfolio_policy_v1.json\")","    policy_path.parent.mkdir(parents=True, exist_ok=True)","    policy_path.write_text(json.dumps(policy_dict, indent=2), encoding=\"utf-8\")","    print(f\"Saved policy to: {policy_path}\")","    ","    # Save spec","    spec_dict = spec.dict()","    spec_path = Path(\"configs/portfolio/portfolio_spec_v1.json\")","    spec_path.write_text(json.dumps(spec_dict, indent=2), encoding=\"utf-8\")","    print(f\"Saved spec to: {spec_path}\")","    ","    # Also save as YAML for easier reading","    import yaml","    spec_yaml_path = Path(\"configs/portfolio/portfolio_spec_v1.yaml\")","    spec_yaml_path.write_text(yaml.dump(spec_dict, default_flow_style=False), encoding=\"utf-8\")","    print(f\"Saved spec (YAML) to: {spec_yaml_path}\")","    ","    print(\"\\nTo validate:\")","    print(f\"  python -m portfolio.cli validate --spec {spec_path} --outputs-root outputs\")","    ","    print(\"\\nTo run:\")","    print(f\"  python -m portfolio.cli run --spec {spec_path} --equity 1000000 --outputs-root outputs\")","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/create_portfolio_spec.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/create_subset.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":924,"sha256":"77107ddf479a2540f308d62db3506af7e7d71ceb30734b38a10641e0ee5b16f7","total_lines":28,"chunk_count":1}
{"type":"file_chunk","path":"scripts/create_subset.py","chunk_index":0,"line_start":1,"line_end":28,"content":["#!/usr/bin/env python3","\"\"\"","Create a subset of raw TXT file for testing.","\"\"\"","import pandas as pd","from pathlib import Path","","def main():","    raw_path = Path(\"FishBroData/raw/CME.MNQ HOT-Minute-Trade.txt\")","    subset_path = Path(\"FishBroData/raw/CME.MNQ_SUBSET.txt\")","    ","    # Read with pandas (assuming header)","    df = pd.read_csv(raw_path)","    # Convert Date column to datetime","    df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')","    # Filter to year 2020 only (or a few days)","    df_subset = df[df['Date'].dt.year == 2020].copy()","    # Limit to first 1000 rows if still large","    if len(df_subset) > 1000:","        df_subset = df_subset.head(1000)","    ","    # Write back with same format","    df_subset['Date'] = df_subset['Date'].dt.strftime('%Y/%-m/%-d')","    df_subset.to_csv(subset_path, index=False)","    print(f\"Created subset with {len(df_subset)} rows at {subset_path}\")","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/create_subset.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/dump_context.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":23888,"sha256":"4db7c97bb3db577dee2b5b4937315833828ad6fcea82e6ad8011c3bd8d553fea","total_lines":745,"chunk_count":4}
{"type":"file_chunk","path":"scripts/dump_context.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","# -*- coding: utf-8 -*-","\"\"\"","FishBro Snapshot Generator vNext (JSONL schema v2) - Balanced Parts Edition","File: scripts/dump_context.py","","Goals:","- Outputs exactly 10 parts: part_00.jsonl .. part_09.jsonl","- Balances content across parts automatically (no \"all in part_00\")","- Never emits \"file_truncated\"","- Fully emits included text files via chunking (chunks may span parts)","- outputs/* big/binary -> metadata-only (file_skipped)","- Writes a single manifest at the end (part_09)","","Usage (WSL zsh):","  cd /home/fishbro/FishBroWFS_V2","  make snapshot","","Or direct:","  PYTHONPATH=src .venv/bin/python scripts/dump_context.py --repo-root .","","Notes:","- Text files are read as UTF-8 with errors='replace' to avoid crashes.","- Newlines in emitted \"content\" lines are without trailing newline.","\"\"\"","","from __future__ import annotations","","import argparse","import dataclasses","import hashlib","import json","import math","import sys","import time","from dataclasses import dataclass","from pathlib import Path","from typing import Dict, Iterable, Iterator, List, Optional, Tuple","","","# -----------------------------","# Policy / Filtering","# -----------------------------","","DEFAULT_ALWAYS_INCLUDE = [","    \"src\",","    \"tests\",","    \"scripts\",","    \"Makefile\",","    \"pyproject.toml\",","    \"requirements.txt\",","    \"requirements-dev.txt\",","    \"README.md\",","    \"README.txt\",","    \"docs\",","]","","DEFAULT_ALWAYS_SKIP_DIRS = {","    \".git\",","    \".venv\",","    \"__pycache__\",","    \"node_modules\",","    \".mypy_cache\",","    \".pytest_cache\",","    \".ruff_cache\",","    \".tox\",","    \".idea\",","    \".vscode\",","    \"dist\",","    \"build\",","}","","# Do not snapshot SNAPSHOT itself (avoid recursion).","# outputs are handled separately (metadata-only for big/binary).","OUTPUTS_ROOTS = {\"outputs\", \"SNAPSHOT\"}","","BINARY_EXTS = {","    \".db\", \".sqlite\", \".sqlite3\",","    \".parquet\", \".feather\",","    \".zip\", \".7z\", \".rar\",","    \".png\", \".jpg\", \".jpeg\", \".webp\", \".gif\",","    \".pdf\",","    \".mp4\", \".mov\", \".avi\",","    \".bin\", \".so\", \".dll\", \".exe\",","    \".pkl\", \".pickle\",","}","","DEFAULT_OUTPUTS_MAX_BYTES_FOR_CONTENT = 2_000_000  # 2MB","","# Balanced strategy knobs","DEFAULT_BALANCE_BUFFER_RATIO = 1.08   # 8% buffer to account for estimation error","MIN_TARGET_BYTES_PER_PART = 500_000   # never set target too tiny (avoid too many part rotations)","","","# -----------------------------","# JSONL Writers","# -----------------------------","","@dataclass","class PartWriter:","    path: Path","    fp: any","    bytes_written: int = 0","    lines_written: int = 0","","    def write_obj(self, obj: dict) -> None:","        s = json.dumps(obj, ensure_ascii=False, separators=(\",\", \":\")) + \"\\n\"","        b = s.encode(\"utf-8\")","        self.fp.write(b)","        self.bytes_written += len(b)","        self.lines_written += 1","","","@dataclass","class FileRecord:","    path: str","    kind: str  # \"text\" or \"binary\"","    encoding: str","    newline: str","    bytes: int","    sha256: str","    total_lines: Optional[int]","    chunk_count: Optional[int]","    complete: bool","    skipped: bool","    skip_reason: Optional[str] = None","","","@dataclass","class RunStats:","    total_text_bytes: int = 0","    total_chunks: int = 0","    files_total: int = 0","    files_complete: int = 0","    files_skipped: int = 0","    skipped_by_reason: Dict[str, int] = dataclasses.field(default_factory=dict)","    violations: List[str] = dataclasses.field(default_factory=list)","","","# -----------------------------","# Utilities","# -----------------------------","","def iso_now() -> str:","    return time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())","","","def make_run_id() -> str:","    return time.strftime(\"%Y%m%d_%H%M%SZ\", time.gmtime())","","","def sha256_bytes(data: bytes) -> str:","    h = hashlib.sha256()","    h.update(data)","    return h.hexdigest()","","","def sha256_file(path: Path) -> str:","    h = hashlib.sha256()","    with path.open(\"rb\") as f:","        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):","            h.update(chunk)","    return h.hexdigest()","","","def is_probably_binary(path: Path) -> bool:","    ext = path.suffix.lower()","    if ext in BINARY_EXTS:","        return True","    try:","        with path.open(\"rb\") as f:","            sample = f.read(8192)","        if b\"\\x00\" in sample:","            return True","    except Exception:","        return True","    return False","","","def normalize_relpath(repo_root: Path, p: Path) -> str:","    return str(p.relative_to(repo_root)).replace(\"\\\\\", \"/\")","","","def should_skip_path(relpath: str) -> Tuple[bool, Optional[str]]:","    \"\"\"","    Returns (skip, reason).","    Reasons:","      - \"cache\": cache/venv/git/node_modules/__pycache__/etc.","      - \"snapshot_output\": avoid snapshotting SNAPSHOT/ itself","    \"\"\"","    parts = relpath.split(\"/\")","    for seg in parts:","        if seg in DEFAULT_ALWAYS_SKIP_DIRS:","            return True, \"cache\"","    if parts and parts[0] == \"SNAPSHOT\":","        return True, \"snapshot_output\"","    return False, None","","","def iter_repo_files(repo_root: Path, include_roots: List[str]) -> Iterator[Path]:"]}
{"type":"file_chunk","path":"scripts/dump_context.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    for root in include_roots:","        rp = (repo_root / root)","        if not rp.exists():","            continue","        if rp.is_file():","            yield rp","            continue","        for p in rp.rglob(\"*\"):","            if p.is_file():","                yield p","","","def stable_sort_paths(paths: Iterable[Path]) -> List[Path]:","    return sorted(paths, key=lambda p: str(p).lower())","","","def read_text_lines(path: Path) -> Tuple[List[str], str, str, bytes]:","    raw = path.read_bytes()","    newline = \"lf\"","    if b\"\\r\\n\" in raw:","        newline = \"crlf\"","    text = raw.decode(\"utf-8\", errors=\"replace\")","    lines = text.splitlines()","    return lines, \"utf-8\", newline, raw","","","def jsonl_len(obj: dict) -> int:","    \"\"\"Exact bytes length of one JSONL line.\"\"\"","    s = json.dumps(obj, ensure_ascii=False, separators=(\",\", \":\")) + \"\\n\"","    return len(s.encode(\"utf-8\"))","","","# -----------------------------","# Chunk planning (deterministic)","# -----------------------------","","@dataclass","class PlannedTextFile:","    relpath: str","    bytes_size: int","    sha256: str","    encoding: str","    newline: str","    total_lines: int","    chunks: List[Tuple[int, int, List[str]]]  # (line_start, line_end, content)","    estimated_emit_bytes: int  # header+chunks+footer jsonl bytes","","","@dataclass","class PlannedSkipFile:","    relpath: str","    reason: str","    bytes_size: int","    sha256: str","    note: str","    estimated_emit_bytes: int  # file_skipped jsonl bytes","","","def plan_text_file(","    repo_root: Path,","    abs_path: Path,","    relpath: str,","    chunk_max_lines: int,","    chunk_max_bytes: int,",") -> PlannedTextFile:","    lines, encoding, newline, raw = read_text_lines(abs_path)","    sha = sha256_bytes(raw)","    total_lines = len(lines)","    bytes_size = len(raw)","","    chunks: List[Tuple[int, int, List[str]]] = []","    idx = 0","    start = 1","    while idx < total_lines:","        take_lines = min(chunk_max_lines, total_lines - idx)","        candidate = lines[idx: idx + take_lines]","","        # Enforce chunk_max_bytes (ensure at least 1 line)","        while True:","            payload = json.dumps(candidate, ensure_ascii=False).encode(\"utf-8\")","            if len(payload) <= chunk_max_bytes or len(candidate) == 1:","                break","            candidate = candidate[:-1]","","        end = start + len(candidate) - 1","        chunks.append((start, end, candidate))","        idx += len(candidate)","        start = end + 1","","    chunk_count = len(chunks)","","    header_obj = {","        \"type\": \"file_header\",","        \"path\": relpath,","        \"kind\": \"text\",","        \"encoding\": encoding,","        \"newline\": newline,","        \"bytes\": bytes_size,","        \"sha256\": sha,","        \"total_lines\": total_lines,","        \"chunk_count\": chunk_count,","    }","    footer_obj = {","        \"type\": \"file_footer\",","        \"path\": relpath,","        \"complete\": True,","        \"emitted_chunks\": chunk_count,","    }","","    est = jsonl_len(header_obj)","    for i, (ls, le, content) in enumerate(chunks):","        est += jsonl_len({","            \"type\": \"file_chunk\",","            \"path\": relpath,","            \"chunk_index\": i,","            \"line_start\": ls,","            \"line_end\": le,","            \"content\": content,","        })","    est += jsonl_len(footer_obj)","","    return PlannedTextFile(","        relpath=relpath,","        bytes_size=bytes_size,","        sha256=sha,","        encoding=encoding,","        newline=newline,","        total_lines=total_lines,","        chunks=chunks,","        estimated_emit_bytes=est,","    )","","","def plan_skip_file(relpath: str, reason: str, bytes_size: int, sha256: str, note: str) -> PlannedSkipFile:","    obj = {","        \"type\": \"file_skipped\",","        \"path\": relpath,","        \"reason\": reason,","        \"bytes\": bytes_size,","        \"sha256\": sha256,","        \"note\": note,","    }","    return PlannedSkipFile(","        relpath=relpath,","        reason=reason,","        bytes_size=bytes_size,","        sha256=sha256,","        note=note,","        estimated_emit_bytes=jsonl_len(obj),","    )","","","# -----------------------------","# Core Generator","# -----------------------------","","class SnapshotGenerator:","    def __init__(","        self,","        repo_root: Path,","        snapshot_root: Path,","        parts: int,","        # If balance_parts=True, this value is ignored and replaced by estimated_total/parts","        target_bytes_per_part: int,","        chunk_max_lines: int,","        chunk_max_bytes: int,","        outputs_max_bytes_for_content: int,","        balance_parts: bool,","        balance_buffer_ratio: float,","    ) -> None:","        self.repo_root = repo_root.resolve()","        self.snapshot_root = snapshot_root","        self.parts = parts","        self.target_bytes_per_part = target_bytes_per_part","        self.chunk_max_lines = chunk_max_lines","        self.chunk_max_bytes = chunk_max_bytes","        self.outputs_max_bytes_for_content = outputs_max_bytes_for_content","        self.balance_parts = balance_parts","        self.balance_buffer_ratio = balance_buffer_ratio","","        self.run_id = make_run_id()","        self.out_dir = (self.repo_root / self.snapshot_root / self.run_id)","        self.out_dir.mkdir(parents=True, exist_ok=True)","","        self.writers: List[PartWriter] = []","        self.current_part = 0","","        self.stats = RunStats()","        self.file_records: Dict[str, FileRecord] = {}","","        # Planned emit queue","        self.plan_text: List[PlannedTextFile] = []","        self.plan_skip: List[PlannedSkipFile] = []","        self.estimated_total_bytes: int = 0","","    def _open_parts(self) -> None:","        self.writers = []","        for i in range(self.parts):","            p = self.out_dir / f\"part_{i:02d}.jsonl\"","            fp = p.open(\"wb\")"]}
{"type":"file_chunk","path":"scripts/dump_context.py","chunk_index":2,"line_start":401,"line_end":600,"content":["            w = PartWriter(path=p, fp=fp)","            self.writers.append(w)","","        # Write meta lazily per part when first used OR always at start?","        # To satisfy \"exactly 10 parts exist\", we write meta immediately (small overhead).","        for i, w in enumerate(self.writers):","            meta = {","                \"type\": \"meta\",","                \"schema_version\": 2,","                \"run_id\": self.run_id,","                \"repo_root\": str(self.repo_root),","                \"snapshot_root\": str(self.snapshot_root),","                \"part\": i,","                \"parts\": self.parts,","                \"created_at\": iso_now(),","                \"generator\": {\"name\": \"dump_context\", \"version\": \"vNext-balanced\"},","                \"policies\": {","                    \"max_parts\": self.parts,","                    \"target_bytes_per_part\": self.target_bytes_per_part,","                    \"chunk_max_lines\": self.chunk_max_lines,","                    \"chunk_max_bytes\": self.chunk_max_bytes,","                    \"outputs_max_bytes_for_content\": self.outputs_max_bytes_for_content,","                    \"outputs_policy\": \"manifest_only_for_big_files\",","                    \"forbid_file_truncated\": True,","                    \"balance_parts\": self.balance_parts,","                    \"estimated_total_bytes\": self.estimated_total_bytes,","                    \"balance_buffer_ratio\": self.balance_buffer_ratio,","                },","            }","            w.write_obj(meta)","","    def _close_parts(self) -> None:","        for w in self.writers:","            try:","                w.fp.flush()","                w.fp.close()","            except Exception:","                pass","","    def _writer(self) -> PartWriter:","        return self.writers[self.current_part]","","    def _rotate_part_if_needed(self, upcoming_bytes: int) -> None:","        w = self._writer()","        if self.current_part < (self.parts - 1):","            if w.bytes_written + upcoming_bytes > self.target_bytes_per_part:","                self.current_part += 1","","    def _write(self, obj: dict) -> None:","        b_len = jsonl_len(obj)","        self._rotate_part_if_needed(b_len)","        self._writer().write_obj(obj)","","    def _plan_files(self, include_roots: List[str]) -> None:","        # Collect candidate files, stable sorted","        files = list(iter_repo_files(self.repo_root, include_roots))","        files = [p for p in files if p.is_file()]","        files = stable_sort_paths(files)","","        planned_total = 0","","        for p in files:","            rel = normalize_relpath(self.repo_root, p)","            # Avoid recursion","            if rel.startswith(\"SNAPSHOT/\"):","                continue","","            skip, reason = should_skip_path(rel)","            if skip:","                # cache/snapshot_output: metadata record (we keep metadata, still counts)","                try:","                    sz = p.stat().st_size","                    sha = sha256_file(p)","                except Exception:","                    sz, sha = 0, \"\"","                rr = reason or \"cache\"","                ps = plan_skip_file(","                    relpath=rel,","                    reason=rr,","                    bytes_size=sz,","                    sha256=sha,","                    note=\"skipped by policy\",","                )","                self.plan_skip.append(ps)","                planned_total += ps.estimated_emit_bytes","                continue","","            # Compute stats for classification","            try:","                sz = p.stat().st_size","            except Exception:","                sz = 0","            sha = sha256_file(p) if p.exists() else \"\"","","            parts = rel.split(\"/\")","            is_outputs = bool(parts and parts[0] == \"outputs\")","","            if is_outputs:","                if is_probably_binary(p) or sz > self.outputs_max_bytes_for_content:","                    ps = plan_skip_file(","                        relpath=rel,","                        reason=\"outputs_big_binary\",","                        bytes_size=sz,","                        sha256=sha,","                        note=\"outputs file too large or binary; metadata only\",","                    )","                    self.plan_skip.append(ps)","                    planned_total += ps.estimated_emit_bytes","                else:","                    pt = plan_text_file(","                        repo_root=self.repo_root,","                        abs_path=p,","                        relpath=rel,","                        chunk_max_lines=self.chunk_max_lines,","                        chunk_max_bytes=self.chunk_max_bytes,","                    )","                    self.plan_text.append(pt)","                    planned_total += pt.estimated_emit_bytes","                continue","","            # Non-outputs","            if is_probably_binary(p):","                ps = plan_skip_file(","                    relpath=rel,","                    reason=\"binary\",","                    bytes_size=sz,","                    sha256=sha,","                    note=\"binary file skipped; metadata only\",","                )","                self.plan_skip.append(ps)","                planned_total += ps.estimated_emit_bytes","            else:","                pt = plan_text_file(","                    repo_root=self.repo_root,","                    abs_path=p,","                    relpath=rel,","                    chunk_max_lines=self.chunk_max_lines,","                    chunk_max_bytes=self.chunk_max_bytes,","                )","                self.plan_text.append(pt)","                planned_total += pt.estimated_emit_bytes","","        self.estimated_total_bytes = planned_total","","    def _apply_balanced_target(self) -> None:","        if not self.balance_parts:","            return","        # Avoid overly small target","        raw_target = math.ceil((self.estimated_total_bytes * self.balance_buffer_ratio) / self.parts)","        self.target_bytes_per_part = max(int(raw_target), MIN_TARGET_BYTES_PER_PART)","","    def _emit_skip(self, ps: PlannedSkipFile) -> None:","        self.stats.files_total += 1","        self.stats.files_skipped += 1","        self.stats.skipped_by_reason[ps.reason] = self.stats.skipped_by_reason.get(ps.reason, 0) + 1","        self.file_records[ps.relpath] = FileRecord(","            path=ps.relpath,","            kind=\"binary\",","            encoding=\"\",","            newline=\"\",","            bytes=ps.bytes_size,","            sha256=ps.sha256,","            total_lines=None,","            chunk_count=None,","            complete=False,","            skipped=True,","            skip_reason=ps.reason,","        )","        self._write({","            \"type\": \"file_skipped\",","            \"path\": ps.relpath,","            \"reason\": ps.reason,","            \"bytes\": ps.bytes_size,","            \"sha256\": ps.sha256,","            \"note\": ps.note,","        })","","    def _emit_text(self, pt: PlannedTextFile) -> None:","        self.stats.files_total += 1","        self.stats.files_complete += 1","        self.stats.total_text_bytes += pt.bytes_size","        self.stats.total_chunks += len(pt.chunks)","","        self.file_records[pt.relpath] = FileRecord(","            path=pt.relpath,","            kind=\"text\",","            encoding=pt.encoding,","            newline=pt.newline,","            bytes=pt.bytes_size,","            sha256=pt.sha256,","            total_lines=pt.total_lines,","            chunk_count=len(pt.chunks),","            complete=False,","            skipped=False,","        )","","        self._write({","            \"type\": \"file_header\",","            \"path\": pt.relpath,","            \"kind\": \"text\","]}
{"type":"file_chunk","path":"scripts/dump_context.py","chunk_index":3,"line_start":601,"line_end":745,"content":["            \"encoding\": pt.encoding,","            \"newline\": pt.newline,","            \"bytes\": pt.bytes_size,","            \"sha256\": pt.sha256,","            \"total_lines\": pt.total_lines,","            \"chunk_count\": len(pt.chunks),","        })","","        for i, (ls, le, content) in enumerate(pt.chunks):","            self._write({","                \"type\": \"file_chunk\",","                \"path\": pt.relpath,","                \"chunk_index\": i,","                \"line_start\": ls,","                \"line_end\": le,","                \"content\": content,","            })","","        self._write({","            \"type\": \"file_footer\",","            \"path\": pt.relpath,","            \"complete\": True,","            \"emitted_chunks\": len(pt.chunks),","        })","","        rec = self.file_records[pt.relpath]","        rec.complete = True","        self.file_records[pt.relpath] = rec","","    def generate(self, include_roots: List[str]) -> None:","        # Phase A: plan","        self._plan_files(include_roots)","","        # Balanced target","        self._apply_balanced_target()","","        # Now open parts (meta includes estimated_total + target)","        self._open_parts()","","        try:","            # Emit in stable order: file_skipped and file_text should interleave by path for readability.","            # We merge them by relpath.","            all_items: List[Tuple[str, str, object]] = []","            for ps in self.plan_skip:","                all_items.append((ps.relpath, \"skip\", ps))","            for pt in self.plan_text:","                all_items.append((pt.relpath, \"text\", pt))","            all_items.sort(key=lambda t: t[0].lower())","","            for _, kind, item in all_items:","                if kind == \"skip\":","                    self._emit_skip(item)  # type: ignore[arg-type]","                else:","                    self._emit_text(item)  # type: ignore[arg-type]","","            # Manifest in last part","            self.current_part = self.parts - 1","            manifest = {","                \"type\": \"manifest\",","                \"run_id\": self.run_id,","                \"schema_version\": 2,","                \"files_total\": self.stats.files_total,","                \"files_complete\": self.stats.files_complete,","                \"files_skipped\": self.stats.files_skipped,","                \"skipped_by_reason\": self.stats.skipped_by_reason,","                \"violations\": self.stats.violations,","                \"stats\": {","                    \"total_text_bytes\": self.stats.total_text_bytes,","                    \"total_chunks\": self.stats.total_chunks,","                    \"estimated_total_emit_bytes\": self.estimated_total_bytes,","                    \"target_bytes_per_part\": self.target_bytes_per_part,","                    \"balance_buffer_ratio\": self.balance_buffer_ratio,","                },","            }","            self._write(manifest)","            # Write separate manifest file for compatibility with no-fog gate","            manifest_path = self.repo_root / self.snapshot_root / \"MANIFEST.json\"","            with open(manifest_path, 'w', encoding='utf-8') as f:","                json.dump(manifest, f, indent=2, ensure_ascii=False)","        finally:","            self._close_parts()","","","# -----------------------------","# CLI","# -----------------------------","","def parse_args(argv: List[str]) -> argparse.Namespace:","    ap = argparse.ArgumentParser()","    ap.add_argument(\"--repo-root\", default=\".\", help=\"Repo root path (default: .)\")","    ap.add_argument(\"--snapshot-root\", default=\"SNAPSHOT\", help=\"Snapshot output root folder under repo root\")","    ap.add_argument(\"--parts\", type=int, default=10, help=\"Number of parts (hard rule: 10)\")","    ap.add_argument(\"--target-bytes-per-part\", type=int, default=20_000_000,","                    help=\"Soft target size per part in bytes (ignored if --balance-parts)\")","    ap.add_argument(\"--chunk-max-lines\", type=int, default=200, help=\"Max lines per chunk\")","    ap.add_argument(\"--chunk-max-bytes\", type=int, default=120_000, help=\"Max bytes per chunk content payload\")","    ap.add_argument(\"--outputs-max-bytes-for-content\", type=int, default=DEFAULT_OUTPUTS_MAX_BYTES_FOR_CONTENT,","                    help=\"For outputs/* only: if larger than this, metadata-only\")","    ap.add_argument(\"--include\", nargs=\"*\", default=DEFAULT_ALWAYS_INCLUDE,","                    help=\"Include roots/files (default includes src/tests/scripts/Makefile/pyproject/docs/requirements/README)\")","    ap.add_argument(\"--balance-parts\", action=\"store_true\", default=True,","                    help=\"Balance content across parts by estimating total emit bytes (default: true)\")","    ap.add_argument(\"--balance-buffer-ratio\", type=float, default=DEFAULT_BALANCE_BUFFER_RATIO,","                    help=\"Buffer ratio applied to estimated total bytes (default: 1.08)\")","    return ap.parse_args(argv)","","","def main(argv: List[str]) -> int:","    args = parse_args(argv)","","    repo_root = Path(args.repo_root)","    if not repo_root.exists():","        print(f\"ERROR: repo-root not found: {repo_root}\", file=sys.stderr)","        return 2","","    parts = int(args.parts)","    if parts != 10:","        print(\"ERROR: --parts must be exactly 10 (hard rule).\", file=sys.stderr)","        return 2","","    gen = SnapshotGenerator(","        repo_root=repo_root,","        snapshot_root=Path(args.snapshot_root),","        parts=parts,","        target_bytes_per_part=int(args.target_bytes_per_part),","        chunk_max_lines=int(args.chunk_max_lines),","        chunk_max_bytes=int(args.chunk_max_bytes),","        outputs_max_bytes_for_content=int(args.outputs_max_bytes_for_content),","        balance_parts=bool(args.balance_parts),","        balance_buffer_ratio=float(args.balance_buffer_ratio),","    )","","    gen.generate(include_roots=list(args.include))","","    out_dir = (repo_root.resolve() / args.snapshot_root / gen.run_id)","    print(f\"OK: snapshot written to: {out_dir}\")","    for i in range(parts):","        p = out_dir / f\"part_{i:02d}.jsonl\"","        sz = p.stat().st_size if p.exists() else 0","        print(f\" - {p.name}: {sz} bytes\")","    return 0","","","if __name__ == \"__main__\":","    raise SystemExit(main(sys.argv[1:]))"]}
{"type":"file_footer","path":"scripts/dump_context.py","complete":true,"emitted_chunks":4}
{"type":"file_header","path":"scripts/fix_profile_paths.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2478,"sha256":"32a3859e361023b2907a5e42c6a3ed44aa872cf380af3e4ed2c9914323c651d6","total_lines":72,"chunk_count":1}
{"type":"file_chunk","path":"scripts/fix_profile_paths.py","chunk_index":0,"line_start":1,"line_end":72,"content":["#!/usr/bin/env python3","\"\"\"Fix profile paths in test files to use configs/profiles instead of src/data/profiles.\"\"\"","","import re","from pathlib import Path","","def fix_file(file_path: Path):","    \"\"\"Fix profile paths in a single file.\"\"\"","    content = file_path.read_text(encoding=\"utf-8\")","    ","    # Handle both quoted and unquoted paths","    patterns = [","        (r'\"src/data/profiles/([^\"]+)\"', r'\"configs/profiles/\\1\"'),","        (r\"'src/data/profiles/([^']+)'\", r\"'configs/profiles/\\1'\"),","        (r'src/data/profiles/(\\w+\\.yaml)', r'configs/profiles/\\1'),","        (r'FishBroWFS_V2/data/profiles', r'configs/profiles'),","        (r'/data/profiles/', r'/profiles/'),","    ]","    ","    original = content","    for pattern, replacement in patterns:","        content = re.sub(pattern, replacement, content)","    ","    if content != original:","        print(f\"Fixed {file_path}\")","        file_path.write_text(content, encoding=\"utf-8\")","        return True","    return False","","def main():","    repo_root = Path(__file__).parent.parent","    tests_root = repo_root / \"tests\"","    ","    # List of test files that need fixing (from the error output)","    test_files = [","        \"test_portfolio_artifacts_hash_stable.py\",","        \"test_portfolio_compile_jobs.py\",","        \"test_portfolio_spec_loader.py\",","        \"test_portfolio_validate.py\",","        \"test_profiles_exist_in_configs.py\",","    ]","    ","    fixed_count = 0","    for test_file in test_files:","        file_path = tests_root / test_file","        if file_path.exists():","            if fix_file(file_path):","                fixed_count += 1","        else:","            print(f\"Warning: {file_path} not found\")","    ","    print(f\"Fixed {fixed_count} files\")","    ","    # Also fix any other test files that might have the pattern","    print(\"\\nScanning for other files with legacy profile paths...\")","    for py_file in tests_root.rglob(\"*.py\"):","        # Skip legacy and manual directories","        rel_path = py_file.relative_to(tests_root)","        if str(rel_path).startswith(\"legacy/\") or str(rel_path).startswith(\"manual/\"):","            continue","        ","        content = py_file.read_text(encoding=\"utf-8\")","        if \"FishBroWFS_V2/data/profiles\" in content or \"/data/profiles/\" in content:","            if py_file.name not in test_files:","                print(f\"Found legacy path in {py_file}\")","                if fix_file(py_file):","                    fixed_count += 1","    ","    print(f\"Total files fixed: {fixed_count}\")","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/fix_profile_paths.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/freeze_season_with_manifest.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7351,"sha256":"2fbb4a5f39103e22b2e26d5d44c97c23bcf03a84e82a97e42a2017719a999ce2","total_lines":217,"chunk_count":2}
{"type":"file_chunk","path":"scripts/freeze_season_with_manifest.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Season Freeze with Manifest â€“ Phase 3B.","","Freeze a season with deterministic manifest referencing universe, dataset,","strategy spec, plateau report, and chosen parameters.","","Usage:","    python scripts/freeze_season_with_manifest.py --season 2026Q1 [--force]","","If season directory already contains plateau report and governance artifacts,","the script will automatically locate them.","","Alternatively, you can specify each input file explicitly.","\"\"\"","","from __future__ import annotations","","import argparse","import sys","from pathlib import Path","","# Add src to path","src_dir = Path(__file__).parent.parent / \"src\"","sys.path.insert(0, str(src_dir))","","from core.season_state import freeze_season as core_freeze_season","from core.season_context import season_dir, outputs_root","from governance.freezer import freeze_season as governance_freeze_season","from governance.models import FreezeContext","","","def locate_default_paths(season: str) -> dict:","    \"\"\"","    Attempt to locate default input files for a given season.","    Returns a dict of Paths.","    \"\"\"","    sdir = season_dir(season)","    root = Path(outputs_root())","","    # Universe spec (global)","    universe_candidates = [","        Path(\"configs/portfolio/instruments.yaml\"),","        Path(\"configs/universe.yaml\"),","        root / \"universe.yaml\",","    ]","    universe_path = None","    for cand in universe_candidates:","        if cand.exists():","            universe_path = cand","            break","    if universe_path is None:","        raise FileNotFoundError(\"Could not locate universe definition file\")","","    # Dataset registry","    dataset_registry_path = root / \"datasets\" / \"datasets_index.json\"","    if not dataset_registry_path.exists():","        # Fallback to older location","        dataset_registry_path = root / \"datasets_index.json\"","        if not dataset_registry_path.exists():","            raise FileNotFoundError(\"Could not locate dataset registry\")","","    # Strategy spec (content-addressed ID) â€“ we need a JSON file.","    # Look for strategy manifest generated by the registry.","    strategy_spec_path = root / \"strategies\" / \"strategy_manifest.json\"","    if not strategy_spec_path.exists():","        # Fallback to built-in strategy spec dump (maybe not present)","        # For now, we'll require explicit path.","        strategy_spec_path = None","","    # Plateau report and chosen params (should be in season/governance/plateau)","    plateau_dir = sdir / \"governance\" / \"plateau\"","    plateau_report_path = plateau_dir / \"plateau_report.json\"","    chosen_params_path = plateau_dir / \"chosen_params.json\"","","    return {","        \"universe_path\": universe_path,","        \"dataset_registry_path\": dataset_registry_path,","        \"strategy_spec_path\": strategy_spec_path,","        \"plateau_report_path\": plateau_report_path,","        \"chosen_params_path\": chosen_params_path,","    }","","","def main() -> int:","    parser = argparse.ArgumentParser(","        description=\"Freeze a season with deterministic manifest\"","    )","    parser.add_argument(","        \"--season\",","        required=True,","        help=\"Season identifier (e.g., '2026Q1')\"","    )","    parser.add_argument(","        \"--force\",","        action=\"store_true\",","        help=\"Allow overwriting existing frozen season (dangerous)\"","    )","    parser.add_argument(","        \"--universe\",","        type=Path,","        help=\"Path to universe definition YAML (default: auto-detect)\"","    )","    parser.add_argument(","        \"--dataset-registry\",","        type=Path,","        help=\"Path to dataset registry JSON (default: auto-detect)\"","    )","    parser.add_argument(","        \"--strategy-spec\",","        type=Path,","        help=\"Path to strategy spec JSON (default: auto-detect)\"","    )","    parser.add_argument(","        \"--plateau-report\",","        type=Path,","        help=\"Path to plateau_report.json (default: season/governance/plateau/plateau_report.json)\"","    )","    parser.add_argument(","        \"--chosen-params\",","        type=Path,","        help=\"Path to chosen_params.json (default: season/governance/plateau/chosen_params.json)\"","    )","    parser.add_argument(","        \"--engine-version\",","        default=\"unknown\",","        help=\"Engine version identifier (default: unknown)\"","    )","    parser.add_argument(","        \"--notes\",","        default=\"\",","        help=\"Optional notes for the season manifest\"","    )","","    args = parser.parse_args()","","    # Determine input paths","    try:","        defaults = locate_default_paths(args.season)","    except FileNotFoundError as e:","        print(f\"Error locating default files: {e}\", file=sys.stderr)","        print(\"Please specify explicit paths using command line arguments.\", file=sys.stderr)","        return 1","","    universe_path = args.universe or defaults[\"universe_path\"]","    dataset_registry_path = args.dataset_registry or defaults[\"dataset_registry_path\"]","    strategy_spec_path = args.strategy_spec or defaults[\"strategy_spec_path\"]","    plateau_report_path = args.plateau_report or defaults[\"plateau_report_path\"]","    chosen_params_path = args.chosen_params or defaults[\"chosen_params_path\"]","","    # Validate required files exist","    missing = []","    for name, path in [","        (\"universe\", universe_path),","        (\"dataset registry\", dataset_registry_path),","        (\"strategy spec\", strategy_spec_path),","        (\"plateau report\", plateau_report_path),","        (\"chosen params\", chosen_params_path),","    ]:","        if path is None or not path.exists():","            missing.append(f\"{name}: {path}\")","    if missing:","        print(\"Missing required input files:\", file=sys.stderr)","        for m in missing:","            print(f\"  - {m}\", file=sys.stderr)","        return 1","","    # Step 1: Freeze the season (core state)","    print(f\"Freezing season '{args.season}'...\")","    try:","        core_state = core_freeze_season(","            season=args.season,","            by=\"cli\",","            reason=\"Freeze with manifest\",","            create_snapshot=True,","        )","        print(f\"  Season state marked as FROZEN.\")","    except Exception as e:","        print(f\"Error freezing season state: {e}\", file=sys.stderr)","        if not args.force:","            return 1","        print(\"  Continuing with manifest creation due to --force.\", file=sys.stderr)","","    # Step 2: Create SeasonManifest","    print(\"Creating SeasonManifest...\")","    ctx = FreezeContext(","        universe_path=universe_path,","        dataset_registry_path=dataset_registry_path,","        strategy_spec_path=strategy_spec_path,","        plateau_report_path=plateau_report_path,","        chosen_params_path=chosen_params_path,","        engine_version=args.engine_version,","        notes=args.notes,","        season_id=args.season,","    )","","    try:","        manifest = governance_freeze_season(ctx, force=args.force)","        print(f\"  SeasonManifest saved to {manifest.season_id}\")","    except Exception as e:"]}
{"type":"file_chunk","path":"scripts/freeze_season_with_manifest.py","chunk_index":1,"line_start":201,"line_end":217,"content":["        print(f\"Error creating SeasonManifest: {e}\", file=sys.stderr)","        return 1","","    # Step 3: Verify integrity (optional)","    print(\"Season freeze completed successfully.\")","    print(f\"  Season ID: {args.season}\")","    print(f\"  Universe hash: {manifest.universe_ref[:16]}...\")","    print(f\"  Dataset hash: {manifest.dataset_ref[:16]}...\")","    print(f\"  Strategy spec hash: {manifest.strategy_spec_hash[:16]}...\")","    print(f\"  Plateau hash: {manifest.plateau_ref[:16]}...\")","    print(f\"  Timestamp: {manifest.timestamp}\")","","    return 0","","","if __name__ == \"__main__\":","    sys.exit(main())"]}
{"type":"file_footer","path":"scripts/freeze_season_with_manifest.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"scripts/generate_research.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6685,"sha256":"63058307416eb035d130494122e681be36b1f4b8869fcfb3d66f15ed4b3e6343","total_lines":187,"chunk_count":1}
{"type":"file_chunk","path":"scripts/generate_research.py","chunk_index":0,"line_start":1,"line_end":187,"content":["\"\"\"Generate research artifacts.","","Phase 9: Generate canonical_results.json and research_index.json.","\"\"\"","","from __future__ import annotations","","import sys","import argparse","import json","import os","import shutil","from pathlib import Path","","","def parse_args() -> argparse.Namespace:","    \"\"\"Parse command line arguments.\"\"\"","    parser = argparse.ArgumentParser(","        description=\"Generate research artifacts (canonical_results.json and research_index.json)\",","        formatter_class=argparse.ArgumentDefaultsHelpFormatter,","    )","    ","    parser.add_argument(","        \"--outputs-root\",","        type=Path,","        default=Path(\"outputs\"),","        help=\"Root outputs directory\",","    )","    ","    parser.add_argument(","        \"--season\",","        type=str,","        default=None,","        help=\"Season identifier (e.g., 2026Q1). If provided, outputs go to outputs/seasons/<season>/\",","    )","    ","    parser.add_argument(","        \"--dry-run\",","        action=\"store_true\",","        help=\"Dry run mode (don't write files, just show what would be done)\",","    )","    ","    parser.add_argument(","        \"--verbose\",","        action=\"store_true\",","        help=\"Verbose output\",","    )","    ","    parser.add_argument(","        \"--legacy-copy\",","        action=\"store_true\",","        help=\"Copy research artifacts to outputs/research/ for backward compatibility\",","    )","    ","    return parser.parse_args()","","","def generate_for_season(outputs_root: Path, season: str, verbose: bool) -> Path:","    \"\"\"","    Write canonical_results.json + research_index.json into outputs/seasons/<season>/research/ and return research_dir.","    \"\"\"","    # Add src to path (must be done before imports)","    sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))","    ","    try:","        from research.registry import build_research_index","        from research.__main__ import generate_canonical_results","    except ImportError as e:","        raise ImportError(f\"Failed to import research modules: {e}\")","    ","    research_dir = outputs_root / \"seasons\" / season / \"research\"","    if verbose:","        print(f\"Research directory: {research_dir}\")","    ","    # Generate canonical results","    canonical_path = generate_canonical_results(outputs_root, research_dir)","    ","    # Build research index","    build_research_index(outputs_root, research_dir)","    ","    return research_dir","","","def main() -> int:","    \"\"\"Main entry point.\"\"\"","    args = parse_args()","    ","    # Phase 5: Check season freeze state before any action","    if args.season:","        try:","            sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))","            from core.season_state import check_season_not_frozen","            check_season_not_frozen(args.season, action=\"generate_research\")","        except ImportError:","            # If season_state module is not available, skip check (backward compatibility)","            pass","        except ValueError as e:","            print(f\"Error: {e}\", file=sys.stderr)","            return 1","    ","    # Determine output directory","    if args.season:","        research_dir = args.outputs_root / \"seasons\" / args.season / \"research\"","    else:","        research_dir = args.outputs_root / \"research\"","    ","    if args.verbose:","        print(f\"Outputs root: {args.outputs_root}\")","        print(f\"Research dir: {research_dir}\")","        if args.season:","            print(f\"Season: {args.season}\")","    ","    if args.dry_run:","        print(\"Dry run mode - would generate:\")","        print(f\"  - {research_dir / 'canonical_results.json'}\")","        print(f\"  - {research_dir / 'research_index.json'}\")","        return 0","    ","    try:","        # Add src to path (must be done before imports)","        sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))","        ","        from research.registry import build_research_index","        from research.__main__ import generate_canonical_results","        ","        # Generate canonical results","        print(f\"Generating canonical_results.json...\")","        generate_canonical_results(args.outputs_root, research_dir)","        ","        # Build research index","        print(f\"Building research_index.json...\")","        build_research_index(args.outputs_root, research_dir)","        ","        # Check if legacy copy should be performed","        should_do_legacy_copy = args.legacy_copy or (os.getenv(\"FISHBRO_LEGACY_COPY\") == \"1\")","        ","        # If season is specified and legacy copy is enabled, copy to outputs/research/ for backward compatibility","        if args.season and should_do_legacy_copy:","            legacy_dir = args.outputs_root / \"research\"","            legacy_dir.mkdir(parents=True, exist_ok=True)","            ","            # Copy canonical_results.json","            src_canonical = research_dir / \"canonical_results.json\"","            dst_canonical = legacy_dir / \"canonical_results.json\"","            if src_canonical.exists():","                shutil.copy2(src_canonical, dst_canonical)","                if args.verbose:","                    print(f\"Legacy copy: canonical_results.json to {dst_canonical}\")","            ","            # Copy research_index.json","            src_index = research_dir / \"research_index.json\"","            dst_index = legacy_dir / \"research_index.json\"","            if src_index.exists():","                shutil.copy2(src_index, dst_index)","                if args.verbose:","                    print(f\"Legacy copy: research_index.json to {dst_index}\")","            ","            # Write a metadata file indicating which season this legacy copy represents","            metadata = {","                \"season\": args.season,","                \"copied_from\": str(research_dir),","                \"note\": \"Legacy copy for backward compatibility (enabled via --legacy-copy or FISHBRO_LEGACY_COPY=1)\"","            }","            metadata_path = legacy_dir / \".season_metadata.json\"","            with open(metadata_path, \"w\", encoding=\"utf-8\") as f:","                json.dump(metadata, f, indent=2, ensure_ascii=False, sort_keys=True)","            ","            print(f\"Legacy copy completed: {legacy_dir}\")","        elif args.season and not should_do_legacy_copy:","            if args.verbose:","                print(\"Legacy copy skipped (default behavior). Use --legacy-copy or set FISHBRO_LEGACY_COPY=1 to enable.\")","        ","        print(\"Research governance layer completed successfully.\")","        print(f\"Output directory: {research_dir}\")","        if args.season and should_do_legacy_copy:","            print(f\"Legacy copy: {args.outputs_root / 'research'}\")","        return 0","    except Exception as e:","        print(f\"Error: {e}\", file=sys.stderr)","        if args.verbose:","            import traceback","            traceback.print_exc()","        return 1","","","if __name__ == \"__main__\":","    sys.exit(main())"]}
{"type":"file_footer","path":"scripts/generate_research.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/kill_stray_workers.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5380,"sha256":"8896d70e42ad39c93ce8165b256f0bbbccc6a4a2ec649af1797fbbef04177bcd","total_lines":170,"chunk_count":1}
{"type":"file_chunk","path":"scripts/kill_stray_workers.py","chunk_index":0,"line_start":1,"line_end":170,"content":["#!/usr/bin/env python3","\"\"\"","Kill stray worker processes and clean up stale pidfiles.","","Phase B5 of Operation Iron Broom â€“ Worker Spawn Governance.","\"\"\"","","import os","import sys","import signal","import time","from pathlib import Path","","# Add src to path to import internal modules","sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))","","from control.worker_spawn_policy import validate_pidfile","","","def find_pidfiles(root_dir: Path, pattern=\"*.pid\"):","    \"\"\"Yield all .pid files under root_dir recursively.\"\"\"","    return root_dir.rglob(pattern)","","","def kill_process(pid: int, sig=signal.SIGTERM):","    \"\"\"Send signal to process, ignoring errors if process already gone.\"\"\"","    try:","        os.kill(pid, sig)","        return True","    except ProcessLookupError:","        return False","    except PermissionError:","        print(f\"  WARNING: No permission to kill PID {pid}\")","        return False","","","def scan_and_kill_strays(root_dir: Path, dry_run=False):","    \"\"\"","    Scan for pidfiles and stray worker processes, kill/clean as needed.","    Returns count of cleaned pidfiles and killed processes.","    \"\"\"","    cleaned = 0","    killed = 0","","    # 1. Scan pidfiles","    for pidfile in find_pidfiles(root_dir):","        print(f\"Checking pidfile: {pidfile}\")","        # Determine db_path: same stem, .db extension","        db_path = pidfile.with_suffix(\".db\")","        if not db_path.exists():","            # DB may have been deleted; we'll still validate pidfile","            db_path = None","","        valid, reason = validate_pidfile(pidfile, db_path)","        if valid:","            print(f\"  OK: {reason}\")","            continue","","        print(f\"  INVALID: {reason}\")","        # Read pid from file","        try:","            pid = int(pidfile.read_text().strip())","        except (ValueError, OSError):","            pid = None","","        if pid is not None:","            # Kill process if alive","            if dry_run:","                print(f\"  DRY RUN: Would kill PID {pid}\")","            else:","                print(f\"  Killing PID {pid}...\")","                if kill_process(pid):","                    killed += 1","                    time.sleep(0.1)  # give it a moment to exit","                else:","                    print(f\"  Process {pid} already dead\")","","        # Delete pidfile","        if dry_run:","            print(f\"  DRY RUN: Would delete {pidfile}\")","        else:","            try:","                pidfile.unlink()","                cleaned += 1","                print(f\"  Deleted pidfile\")","            except OSError as e:","                print(f\"  Failed to delete pidfile: {e}\")","","    # 2. Scan for stray worker processes (no pidfile)","    # We'll parse /proc directly (Linux only)","    if sys.platform != \"linux\":","        print(\"  Skipping stray process scan (non-Linux)\")","        return cleaned, killed","","    proc = Path(\"/proc\")","    for entry in proc.iterdir():","        if not entry.is_dir() or not entry.name.isdigit():","            continue","        pid = int(entry.name)","        cmdline_path = entry / \"cmdline\"","        if not cmdline_path.exists():","            continue","        try:","            cmdline_bytes = cmdline_path.read_bytes()","            # cmdline is null-separated","            cmdline = cmdline_bytes.decode(\"utf-8\", errors=\"ignore\")","        except (OSError, UnicodeDecodeError):","            continue","","        if \"worker_main\" not in cmdline:","            continue","","        # Extract db_path from cmdline (simplistic)","        # cmdline format: python -m control.worker_main /path/to/db","        parts = cmdline.split(\"\\x00\")","        db_arg = None","        for part in parts:","            if part.endswith(\".db\"):","                db_arg = part","                break","        if db_arg is None:","            continue","","        # Check if there's a pidfile for this db","        expected_pidfile = Path(db_arg).with_suffix(\".pid\")","        if expected_pidfile.exists():","            # Already have a pidfile; skip (should have been handled above)","            continue","","        # Stray worker with no pidfile","        print(f\"Found stray worker PID {pid} for DB {db_arg}\")","        if dry_run:","            print(f\"  DRY RUN: Would kill stray PID {pid}\")","        else:","            if kill_process(pid):","                killed += 1","                print(f\"  Killed stray worker\")","            else:","                print(f\"  Stray worker already dead\")","","    return cleaned, killed","","","def main():","    import argparse","    parser = argparse.ArgumentParser(description=\"Kill stray worker processes and clean stale pidfiles.\")","    parser.add_argument(\"--root\", default=\".\", help=\"Root directory to search for pidfiles (default: current)\")","    parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"Print actions without executing\")","    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\", help=\"Verbose output\")","    args = parser.parse_args()","","    root = Path(args.root).resolve()","    if not root.exists():","        print(f\"Error: root directory {root} does not exist\")","        sys.exit(1)","","    print(f\"Scanning for stray workers under {root}\")","    cleaned, killed = scan_and_kill_strays(root, dry_run=args.dry_run)","    print(f\"\\nSummary:\")","    print(f\"  Cleaned pidfiles: {cleaned}\")","    print(f\"  Killed processes: {killed}\")","    if args.dry_run:","        print(\"  (dry run, no changes made)\")","","    if cleaned == 0 and killed == 0:","        print(\"No stray workers found.\")","","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/kill_stray_workers.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/no_fog/no_fog_gate.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":11681,"sha256":"fa0ff1b49728055ded57bd41ea448f1e8bd9f92ed55d6eee80c21ffb2d79375e","total_lines":365,"chunk_count":2}
{"type":"file_chunk","path":"scripts/no_fog/no_fog_gate.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","No-Fog Gate Automation (Pre-commit + CI Core Contracts).","","This gate makes it impossible to commit or merge code that violates core contracts","or ships an outdated snapshot.","","Responsibilities:","1. Regenerate the full repository snapshot (SYSTEM_FULL_SNAPSHOT/)","2. Run core contract tests to ensure no regression","3. Verify snapshot is up-to-date with current repository state","4. Exit with appropriate status codes for CI/pre-commit integration","","Core contract tests:","- tests/strategy/test_ast_identity.py","- tests/test_ui_race_condition_headless.py","- tests/features/test_feature_causality.py","- tests/features/test_feature_lookahead_rejection.py","- tests/features/test_feature_window_honesty.py","","Gate must be fast (<30s), runnable locally and in CI, update snapshot deterministically,","fail with clear messages.","\"\"\"","","import argparse","import json","import os","import subprocess","import sys","import time","from pathlib import Path","from typing import List, Tuple, Optional, Dict, Any","","# ------------------------------------------------------------------------------","# Configuration","# ------------------------------------------------------------------------------","","PROJECT_ROOT = Path(__file__).parent.parent.parent.resolve()","SNAPSHOT_DIR = PROJECT_ROOT / \"SNAPSHOT\"","SNAPSHOT_MANIFEST = SNAPSHOT_DIR / \"MANIFEST.json\"","GENERATE_SNAPSHOT_SCRIPT = PROJECT_ROOT / \"scripts\" / \"dump_context.py\"","","# Core contract tests to run (relative to project root)","CORE_CONTRACT_TESTS = [","    \"tests/strategy/test_ast_identity.py\",","    \"tests/test_ui_race_condition_headless.py\",","    \"tests/features/test_feature_causality.py\",","    \"tests/features/test_feature_lookahead_rejection.py\",","    \"tests/features/test_feature_window_honesty.py\",","]","","# Timeout for the entire gate (seconds)","GATE_TIMEOUT = 30","","# ------------------------------------------------------------------------------","# Utility functions","# ------------------------------------------------------------------------------","","def run_command(cmd: List[str], cwd: Optional[Path] = None, timeout: Optional[int] = None) -> Tuple[int, str, str]:","    \"\"\"","    Run a command and return (returncode, stdout, stderr).","    \"\"\"","    if cwd is None:","        cwd = PROJECT_ROOT","    ","    try:","        result = subprocess.run(","            cmd,","            cwd=cwd,","            capture_output=True,","            text=True,","            timeout=timeout,","            env={**os.environ, \"PYTHONPATH\": str(PROJECT_ROOT / \"src\")}","        )","        return result.returncode, result.stdout, result.stderr","    except subprocess.TimeoutExpired:","        return -1, \"\", f\"Command timed out after {timeout} seconds\"","    except Exception as e:","        return -1, \"\", f\"Failed to run command: {e}\"","","def print_step(step: str, emoji: str = \"â†’\"):","    \"\"\"Print a step header.\"\"\"","    print(f\"\\n{emoji} {step}\")","    print(\"-\" * 60)","","def print_success(message: str):","    \"\"\"Print a success message.\"\"\"","    print(f\"âœ… {message}\")","","def print_error(message: str):","    \"\"\"Print an error message.\"\"\"","    print(f\"âŒ {message}\")","","def print_warning(message: str):","    \"\"\"Print a warning message.\"\"\"","    print(f\"âš ï¸  {message}\")","","def load_manifest() -> Optional[Dict[str, Any]]:","    \"\"\"Load the snapshot manifest if it exists.\"\"\"","    if not SNAPSHOT_MANIFEST.exists():","        return None","    ","    try:","        with open(SNAPSHOT_MANIFEST, 'r') as f:","            return json.load(f)","    except (json.JSONDecodeError, IOError) as e:","        print_warning(f\"Failed to load manifest: {e}\")","        return None","","def check_snapshot_exists() -> bool:","    \"\"\"Check if snapshot directory and manifest exist.\"\"\"","    if not SNAPSHOT_DIR.exists():","        print_warning(f\"Snapshot directory does not exist: {SNAPSHOT_DIR}\")","        return False","    ","    if not SNAPSHOT_MANIFEST.exists():","        print_warning(f\"Snapshot manifest does not exist: {SNAPSHOT_MANIFEST}\")","        return False","    ","    return True","","def regenerate_snapshot(force: bool = True) -> bool:","    \"\"\"","    Regenerate the full repository snapshot.","    ","    Args:","        force: Whether to force overwrite existing snapshot","        ","    Returns:","        True if successful, False otherwise","    \"\"\"","    print_step(\"Regenerating full repository snapshot\", \"ðŸ“¸\")","    ","    cmd = [sys.executable, str(GENERATE_SNAPSHOT_SCRIPT)]","    ","    print(f\"Running: {' '.join(cmd)}\")","    ","    start_time = time.time()","    returncode, stdout, stderr = run_command(cmd, timeout=120)  # Snapshot generation can take time","    ","    if returncode != 0:","        print_error(\"Failed to regenerate snapshot\")","        if stdout:","            print(f\"Stdout:\\n{stdout}\")","        if stderr:","            print(f\"Stderr:\\n{stderr}\")","        return False","    ","    elapsed = time.time() - start_time","    print_success(f\"Snapshot regenerated in {elapsed:.1f}s\")","    ","    # Verify snapshot was created","    if not check_snapshot_exists():","        print_error(\"Snapshot was not created successfully\")","        return False","    ","    # Print summary","    manifest = load_manifest()","    if manifest:","        # New manifest format (dump_context.py vNext)","        chunks = manifest.get(\"stats\", {}).get(\"total_chunks\", 0)","        files = manifest.get(\"files_complete\", 0)","        skipped = manifest.get(\"files_skipped\", 0)","        print(f\"  â€¢ {chunks} chunk(s)\")","        print(f\"  â€¢ {files} file(s) included\")","        print(f\"  â€¢ {skipped} file(s) skipped\")","    ","    return True","","def run_core_contract_tests(timeout: int = GATE_TIMEOUT) -> bool:","    \"\"\"","    Run the core contract tests.","    ","    Args:","        timeout: Timeout in seconds for the tests","        ","    Returns:","        True if all tests pass, False otherwise","    \"\"\"","    print_step(\"Running core contract tests\", \"ðŸ§ª\")","    ","    # Build pytest command for specific test files","    pytest_cmd = [","        sys.executable, \"-m\", \"pytest\",","        \"-v\",","        \"--tb=short\",  # Short traceback for cleaner output","        \"--disable-warnings\",  # Suppress warnings for cleaner output","        \"-q\",  # Quiet mode for CI","    ]","    ","    # Add test files","    for test_file in CORE_CONTRACT_TESTS:","        test_path = PROJECT_ROOT / test_file","        if not test_path.exists():","            print_error(f\"Test file not found: {test_file}\")","            return False","        pytest_cmd.append(str(test_path))","    ","    print(f\"Running: {' '.join(pytest_cmd[:4])} ... {len(CORE_CONTRACT_TESTS)} test files\")","    "]}
{"type":"file_chunk","path":"scripts/no_fog/no_fog_gate.py","chunk_index":1,"line_start":201,"line_end":365,"content":["    start_time = time.time()","    returncode, stdout, stderr = run_command(pytest_cmd, timeout=timeout - 5)","    ","    elapsed = time.time() - start_time","    ","    if returncode == 0:","        print_success(f\"All core contract tests passed in {elapsed:.1f}s\")","        # Print summary of tests run","        if \"passed\" in stdout:","            # Extract passed/failed count","            lines = stdout.split('\\n')","            for line in lines[-10:]:  # Look at last few lines","                if \"passed\" in line and \"failed\" in line:","                    print(f\"  â€¢ {line.strip()}\")","                    break","        return True","    else:","        print_error(f\"Core contract tests failed (took {elapsed:.1f}s)\")","        print(\"\\nTest output:\")","        print(stdout)","        if stderr:","            print(\"\\nStderr:\")","            print(stderr)","        return False","","def verify_snapshot_current() -> bool:","    \"\"\"","    Verify that the snapshot is current (no uncommitted changes that would affect snapshot).","    ","    This is a simplified check - in a real implementation, we would compute","    the hash of relevant files and compare with manifest.","    ","    Returns:","        True if snapshot appears current, False otherwise","    \"\"\"","    print_step(\"Verifying snapshot currency\", \"ðŸ”\")","    ","    if not check_snapshot_exists():","        print_error(\"No snapshot to verify\")","        return False","    ","    manifest = load_manifest()","    if not manifest:","        print_error(\"Could not load manifest\")","        return False","    ","    generated_at = manifest.get(\"run_id\", \"unknown\")","    print(f\"Snapshot generated at: {generated_at}\")","    ","    # Note: A more sophisticated implementation would:","    # 1. Compute hash of all whitelisted files","    # 2. Compare with hashes in manifest","    # 3. Report any mismatches","    ","    print_warning(\"Snapshot currency check is basic - assumes regeneration just happened\")","    print(\"For rigorous verification, run: git status and check for uncommitted changes\")","    ","    return True","","def run_gate(regenerate: bool = True, skip_tests: bool = False, timeout: int = GATE_TIMEOUT) -> bool:","    \"\"\"","    Run the complete no-fog gate.","    ","    Args:","        regenerate: Whether to regenerate snapshot","        skip_tests: Whether to skip running core contract tests","        timeout: Timeout in seconds for the entire gate","        ","    Returns:","        True if gate passes, False otherwise","    \"\"\"","    print(\"=\" * 70)","    print(\"NO-FOG GATE: Core Contract & Snapshot Integrity Check\")","    print(\"=\" * 70)","    ","    start_time = time.time()","    ","    # Step 1: Regenerate snapshot if requested","    if regenerate:","        if not regenerate_snapshot():","            return False","    else:","        print_step(\"Skipping snapshot regeneration\", \"â­ï¸\")","        if not check_snapshot_exists():","            print_error(\"Snapshot does not exist and regeneration is disabled\")","            return False","    ","    # Step 2: Run core contract tests","    if not skip_tests:","        if not run_core_contract_tests(timeout=timeout):","            return False","    else:","        print_step(\"Skipping core contract tests\", \"â­ï¸\")","    ","    # Step 3: Verify snapshot is current","    if not verify_snapshot_current():","        # This is a warning, not a failure","        print_warning(\"Snapshot currency verification inconclusive\")","    ","    # Step 4: Overall status","    elapsed = time.time() - start_time","    ","    print_step(\"Gate Summary\", \"ðŸ“Š\")","    print(f\"Total time: {elapsed:.1f}s\")","    ","    if elapsed > timeout:","        print_warning(f\"Gate exceeded target timeout of {timeout}s\")","        # Don't fail for timeout warning unless strictly required","    ","    print_success(\"NO-FOG GATE PASSED\")","    print(\"\\nâœ… Code meets core contracts and snapshot is up-to-date\")","    print(\"âœ… Safe to commit/merge\")","    ","    return True","","# ------------------------------------------------------------------------------","# Command-line interface","# ------------------------------------------------------------------------------","","def main():","    parser = argparse.ArgumentParser(","        description=\"No-Fog Gate: Core contract and snapshot integrity check\"","    )","    parser.add_argument(","        \"--no-regenerate\",","        action=\"store_true\",","        help=\"Skip snapshot regeneration (use existing snapshot)\"","    )","    parser.add_argument(","        \"--skip-tests\",","        action=\"store_true\",","        help=\"Skip running core contract tests\"","    )","    parser.add_argument(","        \"--check-only\",","        action=\"store_true\",","        help=\"Only check if gate would pass (dry run)\"","    )","    parser.add_argument(","        \"--timeout\",","        type=int,","        default=GATE_TIMEOUT,","        help=f\"Maximum time allowed for gate in seconds (default: {GATE_TIMEOUT})\"","    )","    ","    args = parser.parse_args()","    ","    if args.check_only:","        print(\"Dry run mode - would run gate with:\")","        print(f\"  â€¢ Regenerate: {not args.no_regenerate}\")","        print(f\"  â€¢ Run tests: {not args.skip_tests}\")","        print(f\"  â€¢ Timeout: {args.timeout}s\")","        return 0","    ","    # Run the gate","    success = run_gate(","        regenerate=not args.no_regenerate,","        skip_tests=args.skip_tests,","        timeout=args.timeout","    )","    ","    return 0 if success else 1","","if __name__ == \"__main__\":","    sys.exit(main())"]}
{"type":"file_footer","path":"scripts/no_fog/no_fog_gate.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"scripts/no_fog/no_fog_gate.sh","kind":"text","encoding":"utf-8","newline":"lf","bytes":6081,"sha256":"b2a0a650d1c0742f8cc7c499935c0ec978a11bb237ce4cdceaab9cc8069fe770","total_lines":227,"chunk_count":2}
{"type":"file_chunk","path":"scripts/no_fog/no_fog_gate.sh","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env bash","# Shell wrapper for No-Fog Gate Automation","#","# This script provides a convenient command-line interface to the no-fog gate,","# handling environment setup and error reporting for CI/pre-commit integration.","","set -euo pipefail","","# Colors for output","RED='\\033[0;31m'","GREEN='\\033[0;32m'","YELLOW='\\033[1;33m'","BLUE='\\033[0;34m'","NC='\\033[0m' # No Color","","# Script directory and project root","SCRIPT_DIR=\"$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\"","PROJECT_ROOT=\"$(cd \"$SCRIPT_DIR/../..\" && pwd)\"","PYTHON_SCRIPT=\"$SCRIPT_DIR/no_fog_gate.py\"","","# Default arguments","REGENERATE=true","SKIP_TESTS=false","TIMEOUT=30","CHECK_ONLY=false","","# Print colored message","print_info() {","    echo -e \"${BLUE}[INFO]${NC} $1\"","}","","print_success() {","    echo -e \"${GREEN}[SUCCESS]${NC} $1\"","}","","print_warning() {","    echo -e \"${YELLOW}[WARNING]${NC} $1\"","}","","print_error() {","    echo -e \"${RED}[ERROR]${NC} $1\"","}","","# Show usage","usage() {","    cat << EOF","No-Fog Gate Automation (Pre-commit + CI Core Contracts)","","Usage: $0 [OPTIONS]","","Options:","  --no-regenerate    Skip snapshot regeneration (use existing snapshot)","  --skip-tests       Skip running core contract tests","  --check-only       Dry run - only check if gate would pass","  --timeout SECONDS  Maximum time allowed for gate (default: 30)","  --help             Show this help message","","Description:","  This gate makes it impossible to commit or merge code that violates core contracts","  or ships an outdated snapshot. It:","  1. Regenerates the full repository snapshot (SYSTEM_FULL_SNAPSHOT/)","  2. Runs core contract tests to ensure no regression","  3. Verifies snapshot is up-to-date with current repository state","","Core contract tests:","  - tests/strategy/test_ast_identity.py","  - tests/test_ui_race_condition_headless.py","  - tests/features/test_feature_causality.py","  - tests/features/test_feature_lookahead_rejection.py","  - tests/features/test_feature_window_honesty.py","","Exit codes:","  0 - Gate passed successfully","  1 - Gate failed (tests failed or snapshot issues)","  2 - Invalid arguments or setup error","EOF","}","","# Parse command line arguments","parse_args() {","    while [[ $# -gt 0 ]]; do","        case $1 in","            --no-regenerate)","                REGENERATE=false","                shift","                ;;","            --skip-tests)","                SKIP_TESTS=true","                shift","                ;;","            --check-only)","                CHECK_ONLY=true","                shift","                ;;","            --timeout)","                if [[ -n \"${2:-}\" && \"${2:0:1}\" != \"-\" ]]; then","                    TIMEOUT=\"$2\"","                    shift 2","                else","                    print_error \"--timeout requires a value\"","                    exit 2","                fi","                ;;","            --help)","                usage","                exit 0","                ;;","            -*)","                print_error \"Unknown option: $1\"","                usage","                exit 2","                ;;","            *)","                print_error \"Unexpected argument: $1\"","                usage","                exit 2","                ;;","        esac","    done","}","","# Check prerequisites","check_prerequisites() {","    print_info \"Checking prerequisites...\"","    ","    # Check Python script exists","    if [[ ! -f \"$PYTHON_SCRIPT\" ]]; then","        print_error \"Python script not found: $PYTHON_SCRIPT\"","        exit 2","    fi","    ","    # Check Python is available","    if ! command -v python3 &> /dev/null; then","        print_error \"python3 not found in PATH\"","        exit 2","    fi","    ","    # Check Python version (>= 3.8)","    PYTHON_VERSION=$(python3 -c \"import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')\")","    if [[ $(echo \"$PYTHON_VERSION < 3.8\" | bc -l 2>/dev/null || echo \"1\") == \"1\" ]]; then","        print_warning \"Python $PYTHON_VERSION detected, 3.8+ recommended\"","    fi","    ","    # Check for pytest","    if ! python3 -m pytest --version &> /dev/null; then","        print_warning \"pytest not found, tests may fail\"","    fi","    ","    print_success \"Prerequisites check passed\"","}","","# Build Python command arguments","build_python_args() {","    local args=()","    ","    if [[ \"$REGENERATE\" == false ]]; then","        args+=(\"--no-regenerate\")","    fi","    ","    if [[ \"$SKIP_TESTS\" == true ]]; then","        args+=(\"--skip-tests\")","    fi","    ","    if [[ \"$CHECK_ONLY\" == true ]]; then","        args+=(\"--check-only\")","    fi","    ","    args+=(\"--timeout\" \"$TIMEOUT\")","    ","    echo \"${args[@]}\"","}","","# Main execution","main() {","    parse_args \"$@\"","    ","    print_info \"Starting No-Fog Gate\"","    print_info \"Project root: $PROJECT_ROOT\"","    print_info \"Python script: $PYTHON_SCRIPT\"","    print_info \"Arguments: regenerate=$REGENERATE, skip_tests=$SKIP_TESTS, timeout=${TIMEOUT}s\"","    ","    check_prerequisites","    ","    # Change to project root","    cd \"$PROJECT_ROOT\" || {","        print_error \"Failed to change to project root: $PROJECT_ROOT\"","        exit 2","    }","    ","    # Build and run Python command","    PYTHON_ARGS=$(build_python_args)","    ","    print_info \"Running: python3 $PYTHON_SCRIPT $PYTHON_ARGS\"","    echo \"\"","    ","    # Execute Python script","    if python3 \"$PYTHON_SCRIPT\" $PYTHON_ARGS; then","        echo \"\"","        print_success \"No-Fog Gate completed successfully\"","        exit 0"]}
{"type":"file_chunk","path":"scripts/no_fog/no_fog_gate.sh","chunk_index":1,"line_start":201,"line_end":227,"content":["    else","        EXIT_CODE=$?","        echo \"\"","        print_error \"No-Fog Gate failed with exit code: $EXIT_CODE\"","        ","        # Provide helpful suggestions based on exit code","        case $EXIT_CODE in","            1)","                print_info \"Failure likely due to:\"","                print_info \"  â€¢ Core contract tests failed\"","                print_info \"  â€¢ Snapshot generation failed\"","                print_info \"  â€¢ Snapshot verification failed\"","                print_info \"\"","                print_info \"Run with --skip-tests to isolate test failures\"","                print_info \"Run with --no-regenerate to skip snapshot regeneration\"","                ;;","            *)","                print_info \"Unknown failure, check the output above\"","                ;;","        esac","        ","        exit 1","    fi","}","","# Run main function with all arguments","main \"$@\""]}
{"type":"file_footer","path":"scripts/no_fog/no_fog_gate.sh","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"scripts/no_fog/phase_a_audit.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9139,"sha256":"4f0d20c65d2ffe708cab84471788ba46318d76e7a0e0ec1a80c6da49a6f350ef","total_lines":240,"chunk_count":2}
{"type":"file_chunk","path":"scripts/no_fog/phase_a_audit.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Phase A Audit Helper for No-Fog 2.0 Deep Clean - Evidence Inventory.","","This script runs evidence collection commands and generates structured data","for the Phase A report. It is READ-ONLY - does not delete, move, rename, or","refactor any files.","","Usage:","    python3 scripts/no_fog/phase_a_audit.py [--output-json OUTPUT_JSON]","","Outputs:","    - Prints evidence summary to stdout","    - Optionally writes JSON with collected evidence","\"\"\"","","import argparse","import json","import subprocess","import sys","from pathlib import Path","from typing import Dict, List, Any, Optional","from dataclasses import dataclass, asdict","","","@dataclass","class EvidenceItem:","    \"\"\"Single piece of evidence collected.\"\"\"","    command: str","    exit_code: int","    matches: List[str]","    match_count: int","","","@dataclass","class PhaseAAudit:","    \"\"\"Container for all Phase A evidence.\"\"\"","    candidate_cleanup_items: EvidenceItem","    runner_schism: EvidenceItem","    ui_bypass_scan: EvidenceItem","    test_inventory: EvidenceItem","    tooling_rules_drift: EvidenceItem","    imports_audit: EvidenceItem","","","def run_rg(pattern: str, path: str = \".\", extra_args: Optional[List[str]] = None) -> EvidenceItem:","    \"\"\"Run ripgrep and collect results.\"\"\"","    cmd = [\"rg\", \"-n\", pattern, path]","    if extra_args:","        cmd.extend(extra_args)","    ","    try:","        result = subprocess.run(cmd, capture_output=True, text=True, cwd=Path.cwd())","        lines = result.stdout.strip().split(\"\\n\") if result.stdout.strip() else []","        return EvidenceItem(","            command=\" \".join(cmd),","            exit_code=result.returncode,","            matches=lines,","            match_count=len(lines)","        )","    except FileNotFoundError:","        print(f\"Warning: rg not found, skipping pattern '{pattern}'\", file=sys.stderr)","        return EvidenceItem(","            command=\" \".join(cmd),","            exit_code=127,","            matches=[],","            match_count=0","        )","","","def collect_evidence() -> PhaseAAudit:","    \"\"\"Run all evidence collection commands.\"\"\"","    print(\"=== Phase A Evidence Collection ===\", file=sys.stderr)","    ","    # 1. Candidate Cleanup Items (File/Folder Level)","    print(\"1. Searching for GM_Huang|launch_b5.sh|restore_from_release_txt_force...\", file=sys.stderr)","    # Exclude snapshot directories from count as they contain historical references","    candidate_cleanup = run_rg(\"GM_Huang|launch_b5\\.sh|restore_from_release_txt_force\", \".\", extra_args=[\"--glob\", \"!TEST_SNAPSHOT*\", \"--glob\", \"!SYSTEM_FULL_SNAPSHOT*\"])","    ","    # 2. Runner Schism (Single Truth Audit)","    print(\"2. Searching for runner patterns in src/..\", file=sys.stderr)","    runner_schism = run_rg(","        \"funnel_runner|wfs_runner|research_runner|run_funnel|run_wfs|run_research\",","        \"src/FishBroWFS_V2\"","    )","    ","    # 3. UI Bypass Scan (Direct Write / Direct Logic Calls)","    print(\"3. Searching for database operations in GUI...\", file=sys.stderr)","    ui_bypass = run_rg(","        \"commit\\(|execute\\(|insert\\(|update\\(|delete\\(|\\.write\\(\",","        \"src/gui\"","    )","    ","    # 4. ActionQueue/Intent patterns in GUI","    print(\"4. Searching for ActionQueue/Intent patterns in GUI...\", file=sys.stderr)","    action_queue = run_rg(","        \"ActionQueue|UserIntent|submit_intent|enqueue\\(\",","        \"src/gui\"","    )","    ","    # 5. Test Inventory & Obsolescence Candidates","    print(\"5. Searching for stage0 tests...\", file=sys.stderr)","    test_inventory = run_rg(\"tests/test_stage0_|stage0_\", \"tests\")","    ","    print(\"6. Searching for FishBroWFS_V2 imports in GUI...\", file=sys.stderr)","    imports_audit = run_rg(\"^from FishBroWFS_V2|^import FishBroWFS_V2\", \"src/gui\")","    ","    # 7. Tooling Rules Drift (.continue/rules, Makefile, .github)","    print(\"7. Searching for tooling patterns...\", file=sys.stderr)","    tooling_drift = run_rg(\"pytest|make check|no-fog|full-snapshot|snapshot\", \"Makefile\", extra_args=[\".github\", \"scripts\"])","    ","    # Combine ActionQueue results into UI bypass for reporting","    # (The UI bypass scan originally included both)","    ui_bypass.matches.extend(action_queue.matches)","    ui_bypass.match_count += action_queue.match_count","    ","    return PhaseAAudit(","        candidate_cleanup_items=candidate_cleanup,","        runner_schism=runner_schism,","        ui_bypass_scan=ui_bypass,","        test_inventory=test_inventory,","        tooling_rules_drift=tooling_drift,","        imports_audit=imports_audit","    )","","","def print_summary(audit: PhaseAAudit) -> None:","    \"\"\"Print human-readable summary of evidence.\"\"\"","    print(\"\\n\" + \"=\"*60)","    print(\"PHASE A EVIDENCE SUMMARY\")","    print(\"=\"*60)","    ","    print(f\"\\n1. Candidate Cleanup Items (File/Folder Level):\")","    print(f\"   Matches: {audit.candidate_cleanup_items.match_count}\")","    if audit.candidate_cleanup_items.match_count > 0:","        print(f\"   Sample matches (first 5):\")","        for line in audit.candidate_cleanup_items.matches[:5]:","            print(f\"     - {line}\")","    ","    print(f\"\\n2. Runner Schism (Single Truth Audit):\")","    print(f\"   Matches: {audit.runner_schism.match_count}\")","    if audit.runner_schism.match_count > 0:","        print(f\"   Found runner patterns in:\")","        unique_files = set(line.split(\":\")[0] for line in audit.runner_schism.matches if \":\" in line)","        for file in sorted(unique_files)[:10]:","            print(f\"     - {file}\")","    ","    print(f\"\\n3. UI Bypass Scan (Direct Write / Direct Logic Calls):\")","    print(f\"   Matches: {audit.ui_bypass_scan.match_count}\")","    if audit.ui_bypass_scan.match_count > 0:","        print(f\"   Found in files:\")","        unique_files = set(line.split(\":\")[0] for line in audit.ui_bypass_scan.matches if \":\" in line)","        for file in sorted(unique_files):","            print(f\"     - {file}\")","    ","    print(f\"\\n4. Test Inventory & Obsolescence Candidates:\")","    print(f\"   Matches: {audit.test_inventory.match_count}\")","    if audit.test_inventory.match_count > 0:","        print(f\"   Stage0-related tests found:\")","        test_files = set(line.split(\":\")[0] for line in audit.test_inventory.matches if \":\" in line)","        for file in sorted(test_files):","            print(f\"     - {file}\")","    ","    print(f\"\\n5. Tooling Rules Drift (.continue/rules, Makefile, .github):\")","    print(f\"   Matches: {audit.tooling_rules_drift.match_count}\")","    ","    print(f\"\\n6. Imports Audit (FishBroWFS_V2 within GUI):\")","    print(f\"   Matches: {audit.imports_audit.match_count}\")","    if audit.imports_audit.match_count > 0:","        print(f\"   GUI files importing FishBroWFS_V2:\")","        unique_files = set(line.split(\":\")[0] for line in audit.imports_audit.matches if \":\" in line)","        for file in sorted(unique_files)[:15]:","            print(f\"     - {file}\")","    ","    print(\"\\n\" + \"=\"*60)","    print(\"ANALYSIS NOTES\")","    print(\"=\"*60)","    ","    # Generate analysis notes based on evidence","    notes = []","    ","    if audit.candidate_cleanup_items.match_count > 100:","        notes.append(\"High number of GM_Huang/launch_b5.sh references - potential cleanup candidates\")","    ","    if audit.runner_schism.match_count > 0:","        notes.append(f\"Multiple runner implementations found ({audit.runner_schism.match_count} matches) - check for single truth violations\")","    ","    if audit.ui_bypass_scan.match_count > 0:","        notes.append(f\"UI bypass patterns detected ({audit.ui_bypass_scan.match_count} matches) - potential direct write/logic calls\")","    ","    if audit.test_inventory.match_count > 20:","        notes.append(f\"Many stage0-related tests ({audit.test_inventory.match_count}) - consider test consolidation\")","    ","    if audit.imports_audit.match_count > 30:","        notes.append(f\"High GUI import count ({audit.imports_audit.match_count}) - check for circular dependencies\")","    ","    if not notes:","        notes.append(\"No major issues detected in initial scan\")","    ","    for i, note in enumerate(notes, 1):"]}
{"type":"file_chunk","path":"scripts/no_fog/phase_a_audit.py","chunk_index":1,"line_start":201,"line_end":240,"content":["        print(f\"{i}. {note}\")","","","def main() -> int:","    parser = argparse.ArgumentParser(description=\"Phase A Audit Helper for No-Fog 2.0 Deep Clean\")","    parser.add_argument(","        \"--output-json\",","        help=\"Path to write JSON output with collected evidence\",","        type=Path,","        default=None","    )","    args = parser.parse_args()","    ","    audit = collect_evidence()","    print_summary(audit)","    ","    if args.output_json:","        # Convert to serializable dict","        output_dict = {","            \"phase_a_audit\": {","                \"candidate_cleanup_items\": asdict(audit.candidate_cleanup_items),","                \"runner_schism\": asdict(audit.runner_schism),","                \"ui_bypass_scan\": asdict(audit.ui_bypass_scan),","                \"test_inventory\": asdict(audit.test_inventory),","                \"tooling_rules_drift\": asdict(audit.tooling_rules_drift),","                \"imports_audit\": asdict(audit.imports_audit),","            }","        }","        ","        args.output_json.parent.mkdir(parents=True, exist_ok=True)","        with open(args.output_json, \"w\", encoding=\"utf-8\") as f:","            json.dump(output_dict, f, indent=2, ensure_ascii=False)","        ","        print(f\"\\nJSON output written to: {args.output_json}\", file=sys.stderr)","    ","    return 0","","","if __name__ == \"__main__\":","    sys.exit(main())"]}
{"type":"file_footer","path":"scripts/no_fog/phase_a_audit.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"scripts/perf_direct.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3325,"sha256":"ae85aa5d0495732f077606136e7433be2aa279c57b27c1bedb3b92b3546bff87","total_lines":117,"chunk_count":1}
{"type":"file_chunk","path":"scripts/perf_direct.py","chunk_index":0,"line_start":1,"line_end":117,"content":["","#!/usr/bin/env python3","\"\"\"","FishBro WFS - Direct Engine Benchmark","ç”¨é€”: ç¹žéŽæ‰€æœ‰ Harness/Subprocess è¤‡é›œåº¦ï¼Œç›´æŽ¥ import engine æ¸¬é€Ÿ","\"\"\"","import sys","import time","import gc","import numpy as np","from pathlib import Path","","# 1. å¼·åˆ¶è¨­å®šè·¯å¾‘ (æŒ‡å‘ src)","PROJECT_ROOT = Path(__file__).resolve().parent.parent","sys.path.insert(0, str(PROJECT_ROOT / \"src\"))","","print(f\"python_path: {sys.path[0]}\")","","try:","    # Correct src-based package name in this repo:","    from pipeline.runner_grid import run_grid  # type: ignore","    print(\"âœ… Engine imported successfully (pipeline.runner_grid).\")","except ImportError as e:","    print(f\"âŒ FATAL: Cannot import engine: {e}\")","    sys.exit(1)","","# 2. è¨­å®šè¦æ¨¡ (å°è¦æ¨¡ Smoke Test)","BARS = 20_000","PARAMS = 5_000","HOT_RUNS = 5","","def generate_data(n_bars, n_params):","    print(f\"generating data: {n_bars} bars, {n_params} params...\")","    rng = np.random.default_rng(42)","    ","    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10","    # ä½¿ç”¨ np.abs é¿å… AttributeError","    high = close + np.abs(rng.standard_normal(n_bars)) * 5","    low = close - np.abs(rng.standard_normal(n_bars)) * 5","    open_ = (high + low) / 2 + rng.standard_normal(n_bars)","    ","    high = np.maximum(high, np.maximum(open_, close))","    low = np.minimum(low, np.minimum(open_, close))","    ","    # Generate Params (runner_grid contract: params_matrix must be (n, >=3))","    w1 = rng.integers(10, 100, size=n_params)","    w2 = rng.integers(5, 50, size=n_params)","    w3 = rng.integers(2, 30, size=n_params)","    params = np.column_stack((w1, w2, w3))","    ","    # Layout check","    data_arrays = [open_, high, low, close, params]","    final_arrays = []","    for arr in data_arrays:","        arr = arr.astype(np.float64)","        if not arr.flags['C_CONTIGUOUS']:","            arr = np.ascontiguousarray(arr)","        final_arrays.append(arr)","        ","    return final_arrays[0], final_arrays[1], final_arrays[2], final_arrays[3], final_arrays[4]","","def main():","    opens, highs, lows, closes, params = generate_data(BARS, PARAMS)","    ","    print(\"-\" * 40)","    print(f\"Start Benchmark: {BARS} bars x {PARAMS} params\")","    print(\"-\" * 40)","","    # COLD RUN","    print(\"ðŸ¥¶ Cold run (compiling)...\", end=\"\", flush=True)","    t0 = time.perf_counter()","    _ = run_grid(","        open_=opens,","        high=highs,","        low=lows,","        close=closes,","        params_matrix=params,","        commission=0.0,","        slip=0.0,","        sort_params=False,","    )","    print(f\" Done in {time.perf_counter() - t0:.4f}s\")","","    # HOT RUNS","    times = []","    print(f\"ðŸ”¥ Hot runs ({HOT_RUNS} times, GC off)...\")","    gc.disable()","    for i in range(HOT_RUNS):","        t_start = time.perf_counter()","        _ = run_grid(","            open_=opens,","            high=highs,","            low=lows,","            close=closes,","            params_matrix=params,","            commission=0.0,","            slip=0.0,","            sort_params=False,","        )","        dt = time.perf_counter() - t_start","        times.append(dt)","        print(f\"   Run {i+1}: {dt:.4f}s\")","    gc.enable()","    ","    min_time = min(times)","    total_ops = BARS * PARAMS","    tput = total_ops / min_time","    ","    print(\"-\" * 40)","    print(f\"MIN TIME:   {min_time:.4f}s\")","    print(f\"THROUGHPUT: {int(tput):,} pair-bars/sec\")","    print(\"-\" * 40)","","if __name__ == \"__main__\":","    main()","",""]}
{"type":"file_footer","path":"scripts/perf_direct.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/perf_grid.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":38072,"sha256":"a74704e6c2aa388a15a7e643ea8ecb1faa35a6b9ef3b3a575eb39b55f8531f03","total_lines":946,"chunk_count":5}
{"type":"file_chunk","path":"scripts/perf_grid.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","#!/usr/bin/env python3","\"\"\"","FishBro WFS Perf Harness (Red Team Spec v1.0)","ç‹€æ…‹: âœ… File-based IPC / JIT-First / Observable","ç”¨é€”: é‡æ¸¬ JIT Grid Runner çš„ç©©æ…‹åžåé‡ (Steady-state Throughput)","","ä¿®æ­£ç´€éŒ„:","- v1.1: ä¿®å¾© numpy generator abs éŒ¯èª¤","- v1.2: Hotfix: è§£æ±º subprocess Import Errorï¼Œå¼·åˆ¶æ³¨å…¥ PYTHONPATH ä¸¦å¢žå¼· debug info","\"\"\"","import os","import sys","import time","import gc","import json","import cProfile","import argparse","import subprocess","import tempfile","import statistics","from pathlib import Path","from dataclasses import dataclass, asdict","from typing import List, Dict, Any, Optional","","import numpy as np","","from perf.cost_model import estimate_seconds","from perf.profile_report import _format_profile_report","","# ==========================================","# 1. é…ç½®èˆ‡å¸¸æ•¸ (Tiers)","# ==========================================","","@dataclass","class PerfConfig:","    name: str","    n_bars: int","    n_params: int","    hot_runs: int","    timeout: int","    disable_jit: bool","    sort_params: bool","","# Baseline Tier (default): Fast, suitable for commit-to-commit comparison","# Can be overridden via FISHBRO_PERF_BARS and FISHBRO_PERF_PARAMS env vars","TIER_JIT_BARS = int(os.environ.get(\"FISHBRO_PERF_BARS\", \"20000\"))","TIER_JIT_PARAMS = int(os.environ.get(\"FISHBRO_PERF_PARAMS\", \"1000\"))","TIER_JIT_HOT_RUNS = int(os.environ.get(\"FISHBRO_PERF_HOTRUNS\", \"5\"))","TIER_JIT_TIMEOUT = int(os.environ.get(\"FISHBRO_PERF_TIMEOUT_S\", \"600\"))","","# Stress Tier: Optional, for extreme throughput testing (requires larger timeout or skip-cold)","TIER_STRESS_BARS = int(os.environ.get(\"FISHBRO_PERF_STRESS_BARS\", \"200000\"))","TIER_STRESS_PARAMS = int(os.environ.get(\"FISHBRO_PERF_STRESS_PARAMS\", \"10000\"))","","TIER_TOY_BARS = 2_000","TIER_TOY_PARAMS = 10","TIER_TOY_HOT_RUNS = 1","TIER_TOY_TIMEOUT = 60","","# Warmup compile tier (for skip-cold mode)","TIER_WARMUP_COMPILE_BARS = 2_000","TIER_WARMUP_COMPILE_PARAMS = 200","","PROJECT_ROOT = Path(__file__).resolve().parent.parent","sys.path.insert(0, str(PROJECT_ROOT / \"src\"))","","# ==========================================","# 2. è³‡æ–™ç”Ÿæˆ (Deterministic)","# ==========================================","","def generate_synthetic_data(n_bars: int, seed: int = 42) -> Dict[str, np.ndarray]:","    \"\"\"","    Generate synthetic OHLC data for perf harness.","    ","    Uses float32 for Stage0/perf optimization (memory bandwidth reduction).","    \"\"\"","    from config.dtypes import PRICE_DTYPE_STAGE0","    ","    rng = np.random.default_rng(seed)","    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10","    high = close + np.abs(rng.standard_normal(n_bars)) * 5","    low = close - np.abs(rng.standard_normal(n_bars)) * 5","    open_ = (high + low) / 2 + rng.standard_normal(n_bars)","    ","    high = np.maximum(high, np.maximum(open_, close))","    low = np.minimum(low, np.minimum(open_, close))","    ","    # Use float32 for perf harness (Stage0 optimization)","    data = {","        \"open\": open_.astype(PRICE_DTYPE_STAGE0),","        \"high\": high.astype(PRICE_DTYPE_STAGE0),","        \"low\": low.astype(PRICE_DTYPE_STAGE0),","        \"close\": close.astype(PRICE_DTYPE_STAGE0),","    }","    ","    for k, v in data.items():","        if not v.flags['C_CONTIGUOUS']:","            data[k] = np.ascontiguousarray(v, dtype=PRICE_DTYPE_STAGE0)","    return data","","def generate_params(n_params: int, seed: int = 999) -> np.ndarray:","    \"\"\"","    Generate parameter matrix for perf harness.","    ","    Uses float32 for Stage0 optimization (memory bandwidth reduction).","    \"\"\"","    from config.dtypes import PRICE_DTYPE_STAGE0","    ","    rng = np.random.default_rng(seed)","    w1 = rng.integers(10, 100, size=n_params)","    w2 = rng.integers(5, 50, size=n_params)","    # runner_grid contract: params_matrix must be (n, >=3)","    # Provide a minimal 3-column schema for perf harness.","    w3 = rng.integers(2, 30, size=n_params)","    params = np.column_stack((w1, w2, w3)).astype(PRICE_DTYPE_STAGE0)","    if not params.flags['C_CONTIGUOUS']:","        params = np.ascontiguousarray(params, dtype=PRICE_DTYPE_STAGE0)","    return params","","# ==========================================","# 3. Worker é‚è¼¯ (Child Process)","# ==========================================","","def worker_log(msg: str):","    print(f\"[worker] {msg}\", flush=True)","","","def _env_flag(name: str) -> bool:","    return os.environ.get(name, \"\").strip() == \"1\"","","","def _env_int(name: str, default: int) -> int:","    try:","        return int(os.environ.get(name, str(default)))","    except Exception:","        return default","","","def _env_float(name: str, default: float) -> float:","    try:","        return float(os.environ.get(name, str(default)))","    except Exception:","        return default","","","","def _run_microbench_numba_indicators(closes: np.ndarray, hot_runs: int) -> Dict[str, Any]:","    \"\"\"","    Perf-only microbench:","      - Prove Numba is active in worker process.","      - Measure pure numeric kernels (no Python object loop) baseline.","    \"\"\"","    try:","        import numba as nb  # type: ignore","    except Exception:  # pragma: no cover","        return {\"microbench\": \"numba_missing\"}","","    from indicators import numba_indicators as ni  # type: ignore","","    # Use a fixed window; keep deterministic and cheap.","    length = 14","    x = np.ascontiguousarray(closes, dtype=np.float64)","","    # Warmup compile (first call triggers compilation if JIT enabled).","    _ = ni.rolling_max(x, length)","","    # Hot runs","    times: List[float] = []","    for _i in range(max(1, hot_runs)):","        t0 = time.perf_counter()","        _ = ni.rolling_max(x, length)","        times.append(time.perf_counter() - t0)","","    best = min(times) if times else 0.0","    n = int(x.shape[0])","    # rolling_max visits each element once -> treat as \"ops\" ~= n","    tput = (n / best) if best > 0 else 0.0","    return {","        \"microbench\": \"rolling_max\",","        \"n\": n,","        \"best_s\": best,","        \"ops_per_s\": tput,","        \"nb_disable_jit\": int(getattr(nb.config, \"DISABLE_JIT\", -1)),","    }","","","def run_worker(","    npz_path: str,","    hot_runs: int,","    skip_cold: bool = False,","    warmup_bars: int = 0,","    warmup_params: int = 0,","    microbench: bool = False,","):","    try:","        # Stage P2-1.6: Parse trigger_rate env var","        trigger_rate = _env_float(\"FISHBRO_PERF_TRIGGER_RATE\", 1.0)","        if trigger_rate < 0.0 or trigger_rate > 1.0:","            raise ValueError(f\"FISHBRO_PERF_TRIGGER_RATE must be in [0, 1], got {trigger_rate}\")"]}
{"type":"file_chunk","path":"scripts/perf_grid.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        worker_log(f\"trigger_rate={trigger_rate}\")","        ","        worker_log(f\"Starting. Loading input: {npz_path}\")","        ","        with np.load(npz_path, allow_pickle=False) as data:","            opens = data['open']","            highs = data['high']","            lows = data['low']","            closes = data['close']","            params = data['params']","            ","        worker_log(f\"Data loaded. Bars: {len(opens)}, Params: {len(params)}\")","","        if microbench:","            worker_log(\"MICROBENCH enabled: running numba indicator microbench.\")","            res = _run_microbench_numba_indicators(closes, hot_runs=hot_runs)","            print(\"__RESULT_JSON_START__\")","            print(json.dumps({\"mode\": \"microbench\", \"result\": res}))","            print(\"__RESULT_JSON_END__\")","            return","        ","        try:","            # Phase 3B Grid Runner (correct target)","            from pipeline.runner_grid import run_grid  # type: ignore","            worker_log(\"Grid runner imported successfully (pipeline.runner_grid).\")","            # Enable runner_grid observability payload in returned dict (timings + jit truth + counts).","            os.environ[\"FISHBRO_PROFILE_GRID\"] = \"1\"","","            # ---- JIT truth report (perf-only) ----","            worker_log(f\"ENV NUMBA_DISABLE_JIT={os.environ.get('NUMBA_DISABLE_JIT','')!r}\")","            try:","                import numba as _nb  # type: ignore","                worker_log(f\"Numba present. nb.config.DISABLE_JIT={getattr(_nb.config,'DISABLE_JIT',None)!r}\")","            except Exception as _e:","                worker_log(f\"Numba import failed: {_e!r}\")","","            # run_grid itself might be Python; report what it is.","            worker_log(f\"run_grid type={type(run_grid)} has_signatures={hasattr(run_grid,'signatures')}\")","            if hasattr(run_grid, \"signatures\"):","                worker_log(f\"run_grid.signatures(before)={getattr(run_grid,'signatures',None)!r}\")","            # --------------------------------------","        except ImportError as e:","            worker_log(f\"FATAL: Import grid runner failed: {e!r}\")","            ","            # --- DEBUG INFO ---","            worker_log(f\"Current sys.path: {sys.path}\")","            src_path = Path(__file__).resolve().parent.parent / \"src\"","            if src_path.exists():","                worker_log(f\"Listing {src_path}:\")","                try:","                    for p in src_path.iterdir():","                        worker_log(f\" - {p.name}\")","                        if p.is_dir() and (p / \"__init__.py\").exists():","                             worker_log(f\"   (package content): {[sub.name for sub in p.iterdir()]}\")","                except Exception as ex:","                    worker_log(f\"   Error listing dir: {ex}\")","            else:","                worker_log(f\"Src path not found at: {src_path}\")","            # ------------------","            sys.exit(1)","        ","        # Warmup run (perf-only): compile/JIT on a tiny slice so the real run measures steady-state.","        # IMPORTANT: respect CLI-provided warmup_{bars,params}. If 0, fall back to defaults.","        if warmup_bars and warmup_bars > 0:","            wb = min(int(warmup_bars), len(opens))","        else:","            wb = min(2000, len(opens))","","        if warmup_params and warmup_params > 0:","            wp = min(int(warmup_params), len(params))","        else:","            wp = min(200, len(params))","        if wb >= 10 and wp >= 10:","            worker_log(f\"Starting WARMUP run (bars={wb}, params={wp})...\")","            _ = run_grid(","                open_=opens[:wb],","                high=highs[:wb],","                low=lows[:wb],","                close=closes[:wb],","                params_matrix=params[:wp],","                commission=0.0,","                slip=0.0,","                sort_params=False,","            )","            worker_log(\"WARMUP finished.\")","            if hasattr(run_grid, \"signatures\"):","                worker_log(f\"run_grid.signatures(after)={getattr(run_grid,'signatures',None)!r}\")","        ","        lane_sort = os.environ.get(\"FISHBRO_PERF_LANE_SORT\", \"0\").strip() == \"1\"","        lane_id = os.environ.get(\"FISHBRO_PERF_LANE_ID\", \"?\").strip()","        do_profile = _env_flag(\"FISHBRO_PERF_PROFILE\")","        topn = _env_int(\"FISHBRO_PERF_PROFILE_TOP\", 40)","        mode = os.environ.get(\"FISHBRO_PERF_PROFILE_MODE\", \"\").strip()","        jit_enabled = os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() != \"1\"","        cold_time = 0.0","        if skip_cold:","            # Skip-cold mode: warmup already done, skip full cold run","            worker_log(\"Skip-cold mode: skipping full cold run (warmup already completed)\")","        else:","            # Full cold run","            worker_log(\"Starting COLD run...\")","            t0 = time.perf_counter()","            _ = run_grid(","                open_=opens,","                high=highs,","                low=lows,","                close=closes,","                params_matrix=params,","                commission=0.0,","                slip=0.0,","                sort_params=lane_sort,","            )","            cold_time = time.perf_counter() - t0","            worker_log(f\"COLD run finished: {cold_time:.4f}s\")","        ","        worker_log(f\"Starting {hot_runs} HOT runs (GC disabled)...\")","        hot_times = []","        last_out: Optional[Dict[str, Any]] = None","        gc.disable()","        try:","            for i in range(hot_runs):","                t_start = time.perf_counter()","                if do_profile and i == 0:","                    pr = cProfile.Profile()","                    pr.enable()","                    last_out = run_grid(","                        open_=opens,","                        high=highs,","                        low=lows,","                        close=closes,","                        params_matrix=params,","                        commission=0.0,","                        slip=0.0,","                        sort_params=lane_sort,","                    )","                    pr.disable()","                    print(","                        _format_profile_report(","                            lane_id=lane_id,","                            n_bars=int(len(opens)),","                            n_params=int(len(params)),","                            jit_enabled=bool(jit_enabled),","                            sort_params=bool(lane_sort),","                            topn=int(topn),","                            mode=mode,","                            pr=pr,","                        ),","                        end=\"\",","                    )","                else:","                    last_out = run_grid(","                        open_=opens,","                        high=highs,","                        low=lows,","                        close=closes,","                        params_matrix=params,","                        commission=0.0,","                        slip=0.0,","                        sort_params=lane_sort,","                    )","                t_end = time.perf_counter()","                hot_times.append(t_end - t_start)","        finally:","            gc.enable()","        ","        avg_hot = statistics.mean(hot_times) if hot_times else 0.0","        min_hot = min(hot_times) if hot_times else 0.0","        ","        result = {","            \"cold_time\": cold_time,","            \"hot_times\": hot_times,","            \"avg_hot_time\": avg_hot,","            \"min_hot_time\": min_hot,","            \"n_bars\": len(opens),","            \"n_params\": len(params),","            \"throughput\": (len(opens) * len(params)) / min_hot if min_hot > 0 else 0,","        }","","        # Attach runner_grid observability payload (timings + jit truth + counts)","        if isinstance(last_out, dict) and \"perf\" in last_out:","            result[\"perf\"] = last_out[\"perf\"]","            # Stage P2-1.6: Add trigger_rate_configured to perf dict","            if isinstance(result[\"perf\"], dict):","                result[\"perf\"][\"trigger_rate_configured\"] = float(trigger_rate)","        ","        # Stage P2-1.8: Debug timing keys (only if PERF_DEBUG=1)","        if os.environ.get(\"PERF_DEBUG\", \"\").strip() == \"1\":","            perf_keys = sorted(result.get(\"perf\", {}).keys()) if isinstance(result.get(\"perf\"), dict) else []","            worker_log(f\"DEBUG: perf keys count={len(perf_keys)}, has t_total_kernel_s={'t_total_kernel_s' in perf_keys}\")","            if len(perf_keys) > 0:","                worker_log(f\"DEBUG: perf keys sample: {perf_keys[:20]}\")","        ","        print(f\"__RESULT_JSON_START__\")","        print(json.dumps(result))","        print(f\"__RESULT_JSON_END__\")","        ","    except Exception as e:","        worker_log(f\"CRASH: {e}\")","        import traceback","        traceback.print_exc()"]}
{"type":"file_chunk","path":"scripts/perf_grid.py","chunk_index":2,"line_start":401,"line_end":600,"content":["        sys.exit(1)","","# ==========================================","# 4. Controller é‚è¼¯ (Host Process)","# ==========================================","","def run_lane(","    lane_id: int,","    cfg: PerfConfig,","    tmp_dir: str,","    ohlc_data: Dict[str, np.ndarray],","    microbench: bool = False,",") -> Dict[str, Any]:","    print(f\"\\n>>> Running Lane {lane_id}: {cfg.name}\")","    print(f\"    Config: Bars={cfg.n_bars}, Params={cfg.n_params}, JIT={not cfg.disable_jit}, Sort={cfg.sort_params}\")","    ","    params = generate_params(cfg.n_params)","    # Do not pre-sort here; sorting behavior must be owned by runner_grid(sort_params=...).","    # For no-sort lane, we shuffle to simulate random access order.","    if not cfg.sort_params:","        np.random.shuffle(params)","        print(\"    Params shuffled (random access simulation).\")","    else:","        print(\"    Params left unsorted; runner_grid(sort_params=True) will apply cache-friendly sort.\")","        ","    npz_path = os.path.join(tmp_dir, f\"input_lane_{lane_id}.npz\")","    np.savez_compressed(","        npz_path, ","        open=ohlc_data[\"open\"][:cfg.n_bars],","        high=ohlc_data[\"high\"][:cfg.n_bars],","        low=ohlc_data[\"low\"][:cfg.n_bars],","        close=ohlc_data[\"close\"][:cfg.n_bars],","        params=params","    )","    ","    env = os.environ.copy()","    ","    # é—œéµä¿®æ­£: å¼·åˆ¶æ³¨å…¥ PYTHONPATH ç¢ºä¿å­é€²ç¨‹çœ‹å¾—åˆ° src","    src_path = str(PROJECT_ROOT / \"src\")","    if \"PYTHONPATH\" in env:","        env[\"PYTHONPATH\"] = f\"{src_path}:{env['PYTHONPATH']}\"","    else:","        env[\"PYTHONPATH\"] = src_path","        ","    if cfg.disable_jit:","        env[\"NUMBA_DISABLE_JIT\"] = \"1\"","    else:","        env.pop(\"NUMBA_DISABLE_JIT\", None)","    ","    # Stage P2-1.6: Pass FISHBRO_PERF_TRIGGER_RATE to worker if set","    # (env.copy() already includes it, but we ensure it's explicitly passed)","    trigger_rate_env = os.environ.get(\"FISHBRO_PERF_TRIGGER_RATE\")","    if trigger_rate_env:","        env[\"FISHBRO_PERF_TRIGGER_RATE\"] = trigger_rate_env","        ","    # Build worker command","    cmd = [","        sys.executable,","        __file__,","        \"--worker\",","        \"--input\",","        npz_path,","        \"--hot-runs\",","        str(cfg.hot_runs),","    ]","    if microbench:","        cmd.append(\"--microbench\")","    # Pass lane sort flag to worker via env (avoid CLI churn)","    env[\"FISHBRO_PERF_LANE_SORT\"] = \"1\" if cfg.sort_params else \"0\"","    env[\"FISHBRO_PERF_LANE_ID\"] = str(lane_id)","    ","    # Add skip-cold and warmup params if needed","    skip_cold = os.environ.get(\"FISHBRO_PERF_SKIP_COLD\", \"\").lower() == \"true\"","    if skip_cold:","        cmd.extend([\"--skip-cold\"])","        warmup_bars = int(os.environ.get(\"FISHBRO_PERF_WARMUP_BARS\", str(TIER_WARMUP_COMPILE_BARS)))","        warmup_params = int(os.environ.get(\"FISHBRO_PERF_WARMUP_PARAMS\", str(TIER_WARMUP_COMPILE_PARAMS)))","        cmd.extend([\"--warmup-bars\", str(warmup_bars), \"--warmup-params\", str(warmup_params)])","    ","    try:","        proc = subprocess.run(","            cmd,","            env=env,","            capture_output=True,","            text=True,","            timeout=cfg.timeout,","            check=True","        )","        ","        stdout = proc.stdout","        # Print worker stdout (includes JIT truth report)","        print(stdout, end=\"\")","        ","        result_json = None","        lines = stdout.splitlines()","        capture = False","        json_str = \"\"","        ","        for line in lines:","            if line.strip() == \"__RESULT_JSON_END__\":","                capture = False","            if capture:","                json_str += line","            if line.strip() == \"__RESULT_JSON_START__\":","                capture = True","                ","        if json_str:","            result_json = json.loads(json_str)","            ","            # Phase 3.0-C: FAIL-FAST defense - detect fallback to object mode","            strict_arrays = os.environ.get(\"FISHBRO_PERF_STRICT_ARRAYS\", \"1\").strip() == \"1\"","            if strict_arrays and isinstance(result_json, dict):","                perf = result_json.get(\"perf\")","                if isinstance(perf, dict):","                    intent_mode = perf.get(\"intent_mode\")","                    if intent_mode != \"arrays\":","                        # Handle None or any non-\"arrays\" value","                        intent_mode_str = str(intent_mode) if intent_mode is not None else \"None\"","                        error_msg = (","                            f\"ERROR: intent_mode expected 'arrays' but got '{intent_mode_str}' (lane {lane_id})\\n\"","                            f\"This indicates the kernel fell back to object mode, which is a performance regression.\\n\"","                            f\"To disable this check, set FISHBRO_PERF_STRICT_ARRAYS=0\"","                        )","                        print(f\"âŒ {error_msg}\", file=sys.stderr)","                        raise RuntimeError(error_msg)","            ","            return result_json","        else:","            print(\"âŒ Error: Worker finished but no JSON result found.\")","            print(\"--- Worker Stdout ---\")","            print(stdout)","            print(\"--- Worker Stderr ---\")","            print(proc.stderr)","            return {}","            ","    except subprocess.TimeoutExpired as e:","        print(f\"âŒ Error: Lane {lane_id} Timeout ({cfg.timeout}s).\")","        if e.stdout: print(e.stdout)","        if e.stderr: print(e.stderr)","        return {}","    except subprocess.CalledProcessError as e:","        print(f\"âŒ Error: Lane {lane_id} Crashed (Exit {e.returncode}).\")","        print(\"--- Worker Stdout ---\")","        print(e.stdout)","        print(\"--- Worker Stderr ---\")","        print(e.stderr)","        return {}","    except Exception as e:","        print(f\"âŒ Error: System error {e}\")","        return {}","","def print_report(results: List[Dict[str, Any]]):","    print(\"\\n\\n=== FishBro WFS Perf Harness Report ===\")","    print(\"| Lane | Mode | Sort | Bars | Params | Cold(s) | Hot(s) | Tput (Ops/s) | Speedup |\")","    print(\"|---|---|---|---|---|---|---|---|---|\")","    ","    jit_no_sort_tput = 0","    for r in results:","        if not r or \"res\" not in r or \"lane_id\" not in r: continue","        lane_id = r.get('lane_id', 0)","        name = r.get('name', 'Unknown')","        bars = r['res'].get('n_bars', 0)","        params = r['res'].get('n_params', 0)","        cold = r['res'].get('cold_time', 0)","        hot = r['res'].get('min_hot_time', 0)","        tput = r['res'].get('throughput', 0)","        ","        if lane_id == 3:","            jit_no_sort_tput = tput","            speedup = \"1.0x (Base)\"","        elif jit_no_sort_tput > 0 and tput > 0:","            ratio = tput / jit_no_sort_tput","            speedup = f\"{ratio:.2f}x\"","        else:","            speedup = \"-\"","            ","        mode = \"Py\" if r.get(\"disable_jit\", False) else \"JIT\"","        sort = \"Yes\" if r.get(\"sort_params\", False) else \"No\"","        print(f\"| {lane_id} | {mode} | {sort} | {bars} | {params} | {cold:.4f} | {hot:.4f} | {int(tput):,} | {speedup} |\")","    print(\"\\nNote: Tput = (Bars * Params) / Min Hot Run Time\")","    ","    # Phase 4 Stage E: Cost Model Output","    print(\"\\n=== Cost Model (Predictable Cost Estimation) ===\")","    for r in results:","        if not r or \"res\" not in r or \"lane_id\" not in r: continue","        lane_id = r.get('lane_id', 0)","        res = r.get('res', {})","        bars = res.get('n_bars', 0)","        params = res.get('n_params', 0)","        min_hot_time = res.get('min_hot_time', 0)","        ","        if min_hot_time > 0 and params > 0:","            # Calculate cost per parameter (milliseconds)","            cost_ms_per_param = (min_hot_time / params) * 1000.0","            ","            # Calculate params per second","            params_per_sec = params / min_hot_time","            ","            # Estimate time for 50k params","            estimated_time_for_50k_params = estimate_seconds("]}
{"type":"file_chunk","path":"scripts/perf_grid.py","chunk_index":3,"line_start":601,"line_end":800,"content":["                bars=bars,","                params=50000,","                cost_ms_per_param=cost_ms_per_param,","            )","            ","            # Output cost model fields (stdout)","            print(f\"\\nLane {lane_id} Cost Model:\")","            print(f\"  bars: {bars}\")","            print(f\"  params: {params}\")","            print(f\"  best_time_s: {min_hot_time:.6f}\")","            print(f\"  params_per_sec: {params_per_sec:,.2f}\")","            print(f\"  cost_ms_per_param: {cost_ms_per_param:.6f}\")","            print(f\"  estimated_time_for_50k_params: {estimated_time_for_50k_params:.2f}\")","            ","            # Stage P2-1.5: Entry Sparse Observability","            perf = res.get('perf', {})","            if isinstance(perf, dict):","                entry_valid_mask_sum = perf.get('entry_valid_mask_sum')","                entry_intents_total = perf.get('entry_intents_total')","                entry_intents_per_bar_avg = perf.get('entry_intents_per_bar_avg')","                intents_total_reported = perf.get('intents_total_reported')","                trigger_rate_configured = perf.get('trigger_rate_configured')","                ","                # Always output if perf dict exists (fields should always be present)","                if entry_valid_mask_sum is not None or entry_intents_total is not None:","                    print(f\"\\nLane {lane_id} Entry Sparse Observability:\")","                    # Stage P2-1.6: Display trigger_rate_configured","                    if trigger_rate_configured is not None:","                        print(f\"  trigger_rate_configured: {trigger_rate_configured:.6f}\")","                    print(f\"  entry_valid_mask_sum: {entry_valid_mask_sum if entry_valid_mask_sum is not None else 0}\")","                    print(f\"  entry_intents_total: {entry_intents_total if entry_intents_total is not None else 0}\")","                    if entry_intents_per_bar_avg is not None:","                        print(f\"  entry_intents_per_bar_avg: {entry_intents_per_bar_avg:.6f}\")","                    else:","                        # Calculate if missing","                        if entry_intents_total is not None and bars > 0:","                            print(f\"  entry_intents_per_bar_avg: {entry_intents_total / bars:.6f}\")","                    print(f\"  intents_total_reported: {intents_total_reported if intents_total_reported is not None else perf.get('intents_total', 0)}\")","                ","                # Stage P2-3: Sparse Builder Scaling (for scaling verification)","                allowed_bars = perf.get('allowed_bars')","                selected_params = perf.get('selected_params')","                intents_generated = perf.get('intents_generated')","                ","                if allowed_bars is not None or selected_params is not None or intents_generated is not None:","                    print(f\"\\nLane {lane_id} Sparse Builder Scaling:\")","                    if allowed_bars is not None:","                        print(f\"  allowed_bars: {allowed_bars:,}\")","                    if selected_params is not None:","                        print(f\"  selected_params: {selected_params:,}\")","                    if intents_generated is not None:","                        print(f\"  intents_generated: {intents_generated:,}\")","                    # Calculate scaling ratio if both available","                    if allowed_bars is not None and intents_generated is not None and allowed_bars > 0:","                        scaling_ratio = intents_generated / allowed_bars","                        print(f\"  scaling_ratio (intents/allowed): {scaling_ratio:.4f}\")","    ","    # Stage P2-1.8: Breakdown (Kernel Stage Timings)","    print(\"\\n=== Breakdown (Kernel Stage Timings) ===\")","    for r in results:","        if not r or \"res\" not in r or \"lane_id\" not in r: continue","        lane_id = r.get('lane_id', 0)","        res = r.get('res', {})","        perf = res.get('perf', {})","        ","        if isinstance(perf, dict):","            trigger_rate = perf.get('trigger_rate_configured')","            t_ind_donchian = perf.get('t_ind_donchian_s')","            t_ind_atr = perf.get('t_ind_atr_s')","            t_build_entry = perf.get('t_build_entry_intents_s')","            t_sim_entry = perf.get('t_simulate_entry_s')","            t_calc_exits = perf.get('t_calc_exits_s')","            t_sim_exit = perf.get('t_simulate_exit_s')","            t_total_kernel = perf.get('t_total_kernel_s')","            ","            print(f\"\\nLane {lane_id} Breakdown:\")","            if trigger_rate is not None:","                print(f\"  trigger_rate_configured: {trigger_rate:.6f}\")","            ","            # Helper to format timing with \"(missing)\" if None","            def fmt_time(key: str, val) -> str:","                if val is None:","                    return f\"  {key}: (missing)\"","                return f\"  {key}: {val:.6f}\"","            ","            # Stage P2-2 Step A: Micro-profiling indicators","            print(fmt_time(\"t_ind_donchian_s\", t_ind_donchian))","            print(fmt_time(\"t_ind_atr_s\", t_ind_atr))","            print(fmt_time(\"t_build_entry_intents_s\", t_build_entry))","            print(fmt_time(\"t_simulate_entry_s\", t_sim_entry))","            print(fmt_time(\"t_calc_exits_s\", t_calc_exits))","            print(fmt_time(\"t_simulate_exit_s\", t_sim_exit))","            print(fmt_time(\"t_total_kernel_s\", t_total_kernel))","            ","            # Print percentages if t_total_kernel is available and > 0","            if t_total_kernel is not None and t_total_kernel > 0:","                def fmt_pct(key: str, val, total: float) -> str:","                    if val is None:","                        return f\"    {key}: (missing)\"","                    pct = (val / total) * 100.0","                    return f\"    {key}: {pct:.1f}%\"","                ","                print(\"  Percentages:\")","                print(fmt_pct(\"t_ind_donchian_s\", t_ind_donchian, t_total_kernel))","                print(fmt_pct(\"t_ind_atr_s\", t_ind_atr, t_total_kernel))","                print(fmt_pct(\"t_build_entry_intents_s\", t_build_entry, t_total_kernel))","                print(fmt_pct(\"t_simulate_entry_s\", t_sim_entry, t_total_kernel))","                print(fmt_pct(\"t_calc_exits_s\", t_calc_exits, t_total_kernel))","                print(fmt_pct(\"t_simulate_exit_s\", t_sim_exit, t_total_kernel))","            ","            # Stage P2-2 Step A: Memoization potential assessment","            unique_ch = perf.get('unique_channel_len_count')","            unique_atr = perf.get('unique_atr_len_count')","            unique_pair = perf.get('unique_ch_atr_pair_count')","            ","            if unique_ch is not None or unique_atr is not None or unique_pair is not None:","                print(f\"\\nLane {lane_id} Memoization Potential:\")","                if unique_ch is not None:","                    print(f\"  unique_channel_len_count: {unique_ch}\")","                else:","                    print(f\"  unique_channel_len_count: (missing)\")","                if unique_atr is not None:","                    print(f\"  unique_atr_len_count: {unique_atr}\")","                else:","                    print(f\"  unique_atr_len_count: (missing)\")","                if unique_pair is not None:","                    print(f\"  unique_ch_atr_pair_count: {unique_pair}\")","                else:","                    print(f\"  unique_ch_atr_pair_count: (missing)\")","            ","            # Stage P2-1.8: Display downstream counts","            entry_fills_total = perf.get('entry_fills_total')","            exit_intents_total = perf.get('exit_intents_total')","            exit_fills_total = perf.get('exit_fills_total')","            ","            if entry_fills_total is not None or exit_intents_total is not None or exit_fills_total is not None:","                print(f\"\\nLane {lane_id} Downstream Observability:\")","                if entry_fills_total is not None:","                    print(f\"  entry_fills_total: {entry_fills_total}\")","                else:","                    print(f\"  entry_fills_total: (missing)\")","                if exit_intents_total is not None:","                    print(f\"  exit_intents_total: {exit_intents_total}\")","                else:","                    print(f\"  exit_intents_total: (missing)\")","                if exit_fills_total is not None:","                    print(f\"  exit_fills_total: {exit_fills_total}\")","                else:","                    print(f\"  exit_fills_total: (missing)\")","","def run_matcherbench() -> None:","    \"\"\"","    Matcher-only microbenchmark.","    Purpose:","      - Measure true throughput of cursor-based matcher kernel","      - Avoid runner_grid / Python orchestration overhead","    \"\"\"","    from engine.engine_jit import simulate","    from engine.types import (","        BarArrays,","        OrderIntent,","        OrderKind,","        OrderRole,","        Side,","    )","","    # ---- config (safe defaults) ----","    n_bars = int(os.environ.get(\"FISHBRO_MB_BARS\", \"20000\"))","    intents_per_bar = int(os.environ.get(\"FISHBRO_MB_INTENTS_PER_BAR\", \"2\"))","    hot_runs = int(os.environ.get(\"FISHBRO_MB_HOTRUNS\", \"3\"))","","    print(","        f\"[matcherbench] bars={n_bars}, intents_per_bar={intents_per_bar}, hot_runs={hot_runs}\"","    )","","    # ---- synthetic OHLC ----","    rng = np.random.default_rng(42)","    close = 10000 + np.cumsum(rng.standard_normal(n_bars))","    high = close + 5.0","    low = close - 5.0","    open_ = (high + low) * 0.5","","    bars = BarArrays(","        open=open_.astype(np.float64),","        high=high.astype(np.float64),","        low=low.astype(np.float64),","        close=close.astype(np.float64),","    )","","    # ---- generate intents: created_bar = t-1 ----","    intents = []","    oid = 1","    for t in range(1, n_bars):","        for _ in range(intents_per_bar):","            # ENTRY","            intents.append(","                OrderIntent(","                    order_id=oid,","                    created_bar=t - 1,","                    role=OrderRole.ENTRY,"]}
{"type":"file_chunk","path":"scripts/perf_grid.py","chunk_index":4,"line_start":801,"line_end":946,"content":["                    kind=OrderKind.STOP,","                    side=Side.BUY,","                    price=float(high[t - 1]),","                    qty=1,","                )","            )","            oid += 1","            # EXIT","            intents.append(","                OrderIntent(","                    order_id=oid,","                    created_bar=t - 1,","                    role=OrderRole.EXIT,","                    kind=OrderKind.STOP,","                    side=Side.SELL,","                    price=float(low[t - 1]),","                    qty=1,","                )","            )","            oid += 1","","    print(f\"[matcherbench] total_intents={len(intents)}\")","","    # ---- warmup (compile) ----","    simulate(bars, intents)","","    # ---- hot runs ----","    times = []","    gc.disable()","    try:","        for _ in range(hot_runs):","            t0 = time.perf_counter()","            fills = simulate(bars, intents)","            dt = time.perf_counter() - t0","            times.append(dt)","    finally:","        gc.enable()","","    best = min(times)","    bars_per_s = n_bars / best","    intents_scanned = len(intents)","    intents_per_s = intents_scanned / best","    fills_per_s = len(fills) / best","","    print(\"\\n=== MATCHERBENCH RESULT ===\")","    print(f\"best_time_s      : {best:.6f}\")","    print(f\"bars_per_sec     : {bars_per_s:,.0f}\")","    print(f\"intents_per_sec  : {intents_per_s:,.0f}\")","    print(f\"fills_per_sec    : {fills_per_s:,.0f}\")","","","def main():","    parser = argparse.ArgumentParser(description=\"FishBro WFS Perf Harness\")","    parser.add_argument(\"--worker\", action=\"store_true\", help=\"Run as worker\")","    parser.add_argument(\"--input\", type=str, help=\"Path to input NPZ\")","    parser.add_argument(\"--hot-runs\", type=int, default=5, help=\"Hot runs\")","    parser.add_argument(\"--skip-cold\", action=\"store_true\", help=\"Skip full cold run, use warmup compile instead\")","    parser.add_argument(\"--warmup-bars\", type=int, default=0, help=\"Warmup compile bars (for skip-cold)\")","    parser.add_argument(\"--warmup-params\", type=int, default=0, help=\"Warmup compile params (for skip-cold)\")","    parser.add_argument(\"--microbench\", action=\"store_true\", help=\"Run microbench only (numba indicator baseline)\")","    parser.add_argument(\"--include-python-baseline\", action=\"store_true\", help=\"Include Toy Tier\")","    parser.add_argument(","        \"--matcherbench\",","        action=\"store_true\",","        help=\"Benchmark matcher kernel only (engine_jit.simulate), no runner_grid\",","    )","    parser.add_argument(\"--stress-tier\", action=\"store_true\", help=\"Use stress tier (200kÃ—10k) instead of warmup tier\")","    args = parser.parse_args()","    ","    if args.matcherbench:","        run_matcherbench()","        return","","    if args.worker:","        if not args.input: sys.exit(1)","        run_worker(","            args.input,","            args.hot_runs,","            args.skip_cold,","            args.warmup_bars,","            args.warmup_params,","            args.microbench,","        )","        return","","    print(\"Initializing Perf Harness...\")","    ","    # Stage P2-1.6: Parse and display trigger_rate in main process","    trigger_rate = _env_float(\"FISHBRO_PERF_TRIGGER_RATE\", 1.0)","    if trigger_rate < 0.0 or trigger_rate > 1.0:","        raise ValueError(f\"FISHBRO_PERF_TRIGGER_RATE must be in [0, 1], got {trigger_rate}\")","    print(f\"trigger_rate={trigger_rate}\")","    ","    lanes_cfg: List[PerfConfig] = []","    ","    # Select tier based on stress-tier flag","    if args.stress_tier:","        jit_bars = TIER_STRESS_BARS","        jit_params = TIER_STRESS_PARAMS","        print(f\"Using STRESS tier: {jit_bars:,} bars Ã— {jit_params:,} params\")","    else:","        jit_bars = TIER_JIT_BARS","        jit_params = TIER_JIT_PARAMS","        print(f\"Using WARMUP tier: {jit_bars:,} bars Ã— {jit_params:,} params\")","    ","    if args.include_python_baseline:","        lanes_cfg.append(PerfConfig(\"Lane 1 (Py, No Sort)\", TIER_TOY_BARS, TIER_TOY_PARAMS, TIER_TOY_HOT_RUNS, TIER_TOY_TIMEOUT, True, False))","        lanes_cfg.append(PerfConfig(\"Lane 2 (Py, Sort)\", TIER_TOY_BARS, TIER_TOY_PARAMS, TIER_TOY_HOT_RUNS, TIER_TOY_TIMEOUT, True, True))","        ","    lanes_cfg.append(PerfConfig(\"Lane 3 (JIT, No Sort)\", jit_bars, jit_params, TIER_JIT_HOT_RUNS, TIER_JIT_TIMEOUT, False, False))","    lanes_cfg.append(PerfConfig(\"Lane 4 (JIT, Sort)\", jit_bars, jit_params, TIER_JIT_HOT_RUNS, TIER_JIT_TIMEOUT, False, True))","    ","    max_bars = max(c.n_bars for c in lanes_cfg)","    print(f\"Generating synthetic data (Max Bars: {max_bars})...\")","    ohlc_data = generate_synthetic_data(max_bars)","    ","    results = []","    try:","        with tempfile.TemporaryDirectory() as tmp_dir:","            print(f\"Created temp dir for IPC: {tmp_dir}\")","            for i, cfg in enumerate(lanes_cfg):","                lane_id = i + 1","                if not args.include_python_baseline: lane_id += 2 ","                res = run_lane(lane_id, cfg, tmp_dir, ohlc_data, microbench=args.microbench)","                if res:","                    results.append(","                        {","                            \"lane_id\": lane_id,","                            \"name\": cfg.name,","                            \"res\": res,","                            \"disable_jit\": cfg.disable_jit,","                            \"sort_params\": cfg.sort_params,","                        }","                    )","                else: results.append({})","                ","        print_report(results)","    except RuntimeError as e:","        # Phase 3.0-C: FAIL-FAST - exit with non-zero code on intent_mode violation","        print(f\"\\nâŒ FAIL-FAST triggered: {e}\", file=sys.stderr)","        sys.exit(1)","","if __name__ == \"__main__\":","    main()","",""]}
{"type":"file_footer","path":"scripts/perf_grid.py","complete":true,"emitted_chunks":5}
{"type":"file_header","path":"scripts/research_index.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1340,"sha256":"26f38f8706f7928bcba83693b76c1180e278545bc6513b12627ac29b383d5242","total_lines":54,"chunk_count":1}
{"type":"file_chunk","path":"scripts/research_index.py","chunk_index":0,"line_start":1,"line_end":54,"content":["","\"\"\"Research Index CLI - generate research artifacts.","","Phase 9: Generate canonical_results.json and research_index.json.","\"\"\"","","from __future__ import annotations","","import argparse","import sys","from pathlib import Path","","# Add src to path","sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))","","from research.registry import build_research_index","","","def main() -> int:","    \"\"\"Main entry point.\"\"\"","    parser = argparse.ArgumentParser(description=\"Generate research index\")","    parser.add_argument(","        \"--outputs-root\",","        type=Path,","        default=Path(\"outputs\"),","        help=\"Root outputs directory (default: outputs)\",","    )","    parser.add_argument(","        \"--out-dir\",","        type=Path,","        default=Path(\"outputs/research\"),","        help=\"Research output directory (default: outputs/research)\",","    )","    ","    args = parser.parse_args()","    ","    try:","        index_path = build_research_index(args.outputs_root, args.out_dir)","        print(f\"Research index generated successfully.\")","        print(f\"  Index: {index_path}\")","        print(f\"  Canonical results: {args.out_dir / 'canonical_results.json'}\")","        return 0","    except Exception as e:","        print(f\"Error: {e}\", file=sys.stderr)","        import traceback","        traceback.print_exc()","        return 1","","","if __name__ == \"__main__\":","    sys.exit(main())","","",""]}
{"type":"file_footer","path":"scripts/research_index.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/run_phase3a_plateau.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2312,"sha256":"6cba97be85d84afd09c59a074aa7ba18d845c0b6f4fea125065515750aaefbd6","total_lines":70,"chunk_count":1}
{"type":"file_chunk","path":"scripts/run_phase3a_plateau.py","chunk_index":0,"line_start":1,"line_end":70,"content":["#!/usr/bin/env python3","\"\"\"","Phase 3A Plateau Identification â€“ Execution Script.","","Loads winners.json from a research run, runs plateau identification,","and saves plateau_report.json + chosen_params.json.","","Usage:","    python scripts/run_phase3a_plateau.py <path/to/winners.json>","    python scripts/run_phase3a_plateau.py   (default: use test fixture)","\"\"\"","","import sys","from pathlib import Path","","# Ensure the package root is in sys.path","sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))","","from research.plateau import (","    identify_plateau_from_winners,","    save_plateau_report,",")","","","def main() -> None:","    if len(sys.argv) > 1:","        winners_path = Path(sys.argv[1])","    else:","        # Fallback to test fixture (for development)","        winners_path = Path(\"tests/fixtures/artifacts/winners_v2_valid.json\")","        print(f\"No path provided, using test fixture: {winners_path}\")","","    if not winners_path.exists():","        print(f\"ERROR: File not found: {winners_path}\")","        print(\"Please provide a valid winners.json path.\")","        sys.exit(1)","","    print(f\"Loading candidates from {winners_path}\")","    try:","        report = identify_plateau_from_winners(","            winners_path,","            k_neighbors=5,","            score_threshold_rel=0.1,","        )","    except Exception as e:","        print(f\"Plateau identification failed: {e}\")","        import traceback","        traceback.print_exc()","        sys.exit(1)","","    # Save reports next to winners.json (in same directory)","    output_dir = winners_path.parent / \"plateau\"","    save_plateau_report(report, output_dir)","","    # Print summary","    print(\"\\n--- Plateau Identification Summary ---\")","    print(f\"Candidates analyzed: {report.candidates_seen}\")","    print(f\"Parameters considered: {report.param_names}\")","    print(f\"Selected main candidate: {report.selected_main.candidate_id}\")","    print(f\"  score = {report.selected_main.score}\")","    print(f\"  params = {report.selected_main.params}\")","    print(f\"Backup candidates: {[c.candidate_id for c in report.selected_backup]}\")","    print(f\"Plateau region size: {len(report.plateau_region.members)}\")","    print(f\"Plateau stability score: {report.plateau_region.stability_score:.3f}\")","    print(f\"Reports saved to: {output_dir}\")","    print(\"--- End of report ---\")","","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/run_phase3a_plateau.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/run_phase3b_freeze.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1193,"sha256":"ffc4ccbab1fcbb53d9d55c9f14224de3d9793d7810f1faff30f9889c7d32040a","total_lines":40,"chunk_count":1}
{"type":"file_chunk","path":"scripts/run_phase3b_freeze.py","chunk_index":0,"line_start":1,"line_end":40,"content":["#!/usr/bin/env python3","\"\"\"","Phase 3B Season Freeze â€“ Execution Script.","","Calls freeze_season_with_manifest.py with default season (2026Q1).","If season already frozen, will raise error unless --force.","","Usage:","    python scripts/run_phase3b_freeze.py [--season SEASON] [--force]","\"\"\"","","import sys","from pathlib import Path","","# Ensure the package root is in sys.path","sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))","","from scripts.freeze_season_with_manifest import main as freeze_main","","","def main() -> None:","    # Simulate command line arguments","    # For simplicity, we'll just call the freeze script with default season","    # In a real UI, the season should be passed as argument.","    import argparse","    parser = argparse.ArgumentParser(description=\"Freeze a season\")","    parser.add_argument(\"--season\", default=\"2026Q1\", help=\"Season identifier\")","    parser.add_argument(\"--force\", action=\"store_true\", help=\"Overwrite existing frozen season\")","    args = parser.parse_args()","","    # Build sys.argv for the freeze script","    sys.argv = [sys.argv[0], \"--season\", args.season]","    if args.force:","        sys.argv.append(\"--force\")","","    freeze_main()","","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/run_phase3b_freeze.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/run_phase3c_compile.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1484,"sha256":"e89e95ff7870fd25c497d9a0521d305d7214fc079d72dd428b48004af4ad2656","total_lines":53,"chunk_count":1}
{"type":"file_chunk","path":"scripts/run_phase3c_compile.py","chunk_index":0,"line_start":1,"line_end":53,"content":["#!/usr/bin/env python3","\"\"\"","Phase 3C Portfolio Compilation â€“ Execution Script.","","Compile a frozen Season Manifest into deployment TXT files for MultiCharts.","","Usage:","    python scripts/run_phase3c_compile.py path/to/season_manifest.json","","This script must NOT read the current \"live\" config. All info must come from the frozen manifest.","\"\"\"","","import sys","from pathlib import Path","","# Ensure the package root is in sys.path","sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))","","from deployment.compiler import compile_season","","","def main() -> None:","    if len(sys.argv) != 2:","        print(\"Usage: python scripts/run_phase3c_compile.py <path/to/season_manifest.json>\")","        sys.exit(1)","","    manifest_path = Path(sys.argv[1])","    if not manifest_path.exists():","        print(f\"ERROR: Manifest file not found: {manifest_path}\")","        sys.exit(1)","","    # Determine output directory","    from governance.models import SeasonManifest","    manifest = SeasonManifest.load(manifest_path)","    output_dir = Path(\"outputs\") / \"deployment\" / manifest.season_id","","    print(f\"Compiling season {manifest.season_id}...\")","    print(f\"  manifest: {manifest_path}\")","    print(f\"  output: {output_dir}\")","","    try:","        compile_season(manifest_path, output_dir)","    except Exception as e:","        print(f\"Compilation failed: {e}\")","        import traceback","        traceback.print_exc()","        sys.exit(1)","","    print(f\"Deployment Pack ready at: {output_dir}\")","","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/run_phase3c_compile.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/run_research_v3.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":574,"sha256":"e1936895344977f86e5ae77803b1cd60ec0af00e2d489974a92d3e0560806771","total_lines":25,"chunk_count":1}
{"type":"file_chunk","path":"scripts/run_research_v3.py","chunk_index":0,"line_start":1,"line_end":25,"content":["#!/usr/bin/env python3","\"\"\"","Research v3 â€“ Execution Script.","","Calls generate_research.py with default parameters.","This is a placeholder; adapt to actual research script.","\"\"\"","","import sys","from pathlib import Path","","# Ensure the package root is in sys.path","sys.path.insert(0, str(Path(__file__).parent.parent / \"src\"))","","from scripts.generate_research import main as research_main","","","def main() -> None:","    # Call the research script with default arguments","    # You may need to adjust based on actual script signature.","    research_main()","","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/run_research_v3.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/topology_probe.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5928,"sha256":"df2a9d9a00511a69f1683a8ebd84370b99f8f11fc0834ad605827fff82319776","total_lines":178,"chunk_count":1}
{"type":"file_chunk","path":"scripts/topology_probe.py","chunk_index":0,"line_start":1,"line_end":178,"content":["#!/usr/bin/env python3","\"\"\"","Topology Probe - inspect which ports have listeners and fetch service identity.","","Usage:","    PYTHONPATH=src .venv/bin/python scripts/topology_probe.py","","Behavior:","- Uses subprocess to run ss -lntp and parse lines for common ports (8080, 8000, 8001).","- For each detected listener on :8080, attempt HTTP GET:","    http://localhost:8080/__identity","    http://localhost:8080/health","    http://localhost:8080/status (best-effort)","- Print results as a structured text report.","","Must exit 0 always (this is a probe tool), but print failures.","No external deps beyond stdlib. Use urllib.request for GET.","\"\"\"","","import subprocess","import sys","import json","import urllib.request","import urllib.error","import socket","from typing import Dict, Any, List, Optional","","","def run_ss() -> List[str]:","    \"\"\"Run ss -lntp and return lines.\"\"\"","    try:","        result = subprocess.run(","            [\"ss\", \"-lntp\"],","            capture_output=True,","            text=True,","            check=False,","        )","        if result.returncode != 0:","            print(f\"WARNING: ss command failed: {result.stderr}\", file=sys.stderr)","            return []","        return result.stdout.strip().splitlines()","    except FileNotFoundError:","        print(\"WARNING: 'ss' command not found, falling back to netstat\", file=sys.stderr)","        return run_netstat()","","","def run_netstat() -> List[str]:","    \"\"\"Fallback using netstat -tlnp.\"\"\"","    try:","        result = subprocess.run(","            [\"netstat\", \"-tlnp\"],","            capture_output=True,","            text=True,","            check=False,","        )","        if result.returncode != 0:","            print(f\"WARNING: netstat command failed: {result.stderr}\", file=sys.stderr)","            return []","        return result.stdout.strip().splitlines()","    except FileNotFoundError:","        print(\"ERROR: Neither ss nor netstat available\", file=sys.stderr)","        return []","","","def parse_listeners(lines: List[str]) -> Dict[str, Dict[str, Any]]:","    \"\"\"Parse ss/netstat output for listeners on ports 8080, 8000, 8001.\"\"\"","    listeners = {}","    for line in lines:","        # Skip header lines","        if \"LISTEN\" not in line:","            continue","        parts = line.split()","        # Find address column (varies between ss and netstat)","        addr = None","        for part in parts:","            if \":\" in part and (\"8080\" in part or \"8000\" in part or \"8001\" in part):","                addr = part","                break","        if not addr:","            continue","        # Extract port","        if \":\" in addr:","            port = addr.split(\":\")[-1]","        else:","            continue","        # Extract PID/process (if available)","        pid = \"unknown\"","        for part in parts:","            if \"pid=\" in part:","                pid = part.split(\"=\")[1].split(\",\")[0]","                break","        listeners[port] = {\"address\": addr, \"pid\": pid, \"raw\": line}","    return listeners","","","def http_get(url: str, timeout: float = 2.0) -> Optional[Dict[str, Any]]:","    \"\"\"Perform HTTP GET and return JSON if possible, else None.\"\"\"","    try:","        req = urllib.request.Request(url, headers={\"User-Agent\": \"topology-probe/1.0\"})","        with urllib.request.urlopen(req, timeout=timeout) as resp:","            body = resp.read().decode(\"utf-8\", errors=\"replace\")","            if resp.status == 200:","                try:","                    return json.loads(body)","                except json.JSONDecodeError:","                    return {\"raw\": body.strip()}","            else:","                return {\"status\": resp.status, \"body\": body[:200]}","    except urllib.error.URLError as e:","        return {\"error\": str(e)}","    except socket.timeout:","        return {\"error\": \"timeout\"}","    except Exception as e:","        return {\"error\": str(e)}","","","def probe_port(port: str) -> Dict[str, Any]:","    \"\"\"Probe a single port for identity, health, status.\"\"\"","    base = f\"http://localhost:{port}\"","    result = {","        \"port\": port,","        \"identity\": None,","        \"health\": None,","        \"status\": None,","    }","    # Identity endpoint","    ident_resp = http_get(f\"{base}/__identity\")","    result[\"identity\"] = ident_resp","    # Health endpoint","    health_resp = http_get(f\"{base}/health\")","    result[\"health\"] = health_resp","    # Status endpoint (optional)","    status_resp = http_get(f\"{base}/status\")","    result[\"status\"] = status_resp","    return result","","","def main() -> None:","    print(\"=== Topology Probe ===\")","    lines = run_ss()","    listeners = parse_listeners(lines)","    if not listeners:","        print(\"No listeners found on ports 8080, 8000, 8001.\")","        sys.exit(0)","    ","    print(f\"Found {len(listeners)} listener(s):\")","    for port, info in listeners.items():","        print(f\"  Port {port}: {info['address']} (PID {info['pid']})\")","    ","    print(\"\\n--- Probing each listener ---\")","    for port in listeners:","        print(f\"\\nPort {port}:\")","        probe = probe_port(port)","        if probe[\"identity\"] and \"service_name\" in probe[\"identity\"]:","            print(f\"  Identity: service_name={probe['identity'].get('service_name')}\")","            print(f\"    pid={probe['identity'].get('pid')}, git={probe['identity'].get('git_commit', 'unknown')[:8]}\")","        else:","            print(f\"  Identity: {probe['identity']}\")","        if probe[\"health\"]:","            if isinstance(probe[\"health\"], dict) and \"status\" in probe[\"health\"]:","                print(f\"  Health: {probe['health']['status']}\")","            else:","                print(f\"  Health: {probe['health']}\")","        if probe[\"status\"]:","            print(f\"  Status: {probe['status']}\")","    ","    print(\"\\n=== Summary ===\")","    for port in listeners:","        probe = probe_port(port)","        ident = probe[\"identity\"]","        if ident and isinstance(ident, dict) and \"service_name\" in ident:","            print(f\"Port {port}: {ident['service_name']} (pid {ident.get('pid')})\")","        else:","            print(f\"Port {port}: unknown\")","","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/topology_probe.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/upgrade_winners_v2.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3352,"sha256":"e6a2050459f16d9305eb23c57f56d1d78a255af389450fcf872b1ea75af2bfe8","total_lines":116,"chunk_count":1}
{"type":"file_chunk","path":"scripts/upgrade_winners_v2.py","chunk_index":0,"line_start":1,"line_end":116,"content":["","#!/usr/bin/env python3","from __future__ import annotations","","import argparse","import json","import sys","from pathlib import Path","from typing import Any, Dict","","REPO_ROOT = Path(__file__).resolve().parents[1]","SRC_DIR = REPO_ROOT / \"src\"","if str(SRC_DIR) not in sys.path:","    sys.path.insert(0, str(SRC_DIR))","","from core.winners_builder import build_winners_v2  # noqa: E402","from core.winners_schema import is_winners_v2      # noqa: E402","","","def _read_json(path: Path) -> Dict[str, Any]:","    with open(path, \"r\", encoding=\"utf-8\") as f:","        return json.load(f)","","","def _write_json(path: Path, obj: Dict[str, Any]) -> None:","    with open(path, \"w\", encoding=\"utf-8\") as f:","        json.dump(obj, f, sort_keys=True, separators=(\",\", \":\"), indent=2)","        f.write(\"\\n\")","","","def _read_required_artifacts(run_dir: Path) -> Dict[str, Dict[str, Any]]:","    manifest = _read_json(run_dir / \"manifest.json\")","    config_snapshot = _read_json(run_dir / \"config_snapshot.json\")","    metrics = _read_json(run_dir / \"metrics.json\")","    winners = _read_json(run_dir / \"winners.json\")","    return {","        \"manifest\": manifest,","        \"config_snapshot\": config_snapshot,","        \"metrics\": metrics,","        \"winners\": winners,","    }","","","def upgrade_one_run_dir(run_dir: Path, *, dry_run: bool) -> bool:","    winners_path = run_dir / \"winners.json\"","    if not winners_path.exists():","        return False","","    data = _read_required_artifacts(run_dir)","    winners_data = data[\"winners\"]","","    if is_winners_v2(winners_data):","        return False","","    manifest = data[\"manifest\"]","    config_snapshot = data[\"config_snapshot\"]","    metrics = data[\"metrics\"]","","    stage_name = metrics.get(\"stage_name\") or config_snapshot.get(\"stage_name\") or \"unknown_stage\"","    run_id = manifest.get(\"run_id\", run_dir.name)","","    legacy_topk = winners_data.get(\"topk\", [])","    winners_v2 = build_winners_v2(","        stage_name=stage_name,","        run_id=run_id,","        manifest=manifest,","        config_snapshot=config_snapshot,","        legacy_topk=legacy_topk,","    )","","    if dry_run:","        print(f\"[DRY] would upgrade: {run_dir}\")","        return True","","    backup_path = run_dir / \"winners_legacy.json\"","    if not backup_path.exists():","        _write_json(backup_path, winners_data)","","    _write_json(winners_path, winners_v2)","    print(f\"[OK] upgraded: {run_dir}\")","    return True","","","def main() -> int:","    ap = argparse.ArgumentParser()","    ap.add_argument(\"--season\", required=True)","    ap.add_argument(\"--outputs-root\", required=True)","    ap.add_argument(\"--dry-run\", action=\"store_true\")","    args = ap.parse_args()","","    outputs_root = Path(args.outputs_root)","    runs_dir = outputs_root / \"seasons\" / args.season / \"runs\"","    if not runs_dir.exists():","        raise SystemExit(f\"runs dir not found: {runs_dir}\")","","    scanned = 0","    changed = 0","","    for run_dir in sorted(p for p in runs_dir.iterdir() if p.is_dir()):","        scanned += 1","        try:","            if upgrade_one_run_dir(run_dir, dry_run=args.dry_run):","                changed += 1","        except FileNotFoundError as e:","            print(f\"[SKIP] missing file in {run_dir}: {e}\")","        except json.JSONDecodeError as e:","            print(f\"[SKIP] bad json in {run_dir}: {e}\")","","    print(f\"[DONE] scanned={scanned} changed={changed} dry_run={args.dry_run}\")","    return 0","","","if __name__ == \"__main__\":","    raise SystemExit(main())","",""]}
{"type":"file_footer","path":"scripts/upgrade_winners_v2.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"scripts/verify_season_integrity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3516,"sha256":"0e9b9c4d5ef29fd5ad2d1e8e9008bb3007de12c14c0ca8a06227e208311d472e","total_lines":114,"chunk_count":1}
{"type":"file_chunk","path":"scripts/verify_season_integrity.py","chunk_index":0,"line_start":1,"line_end":114,"content":["#!/usr/bin/env python3","\"\"\"","Verify season integrity against freeze snapshot.","","Phase 5: Artifact Diff Guard - Detect unauthorized modifications to frozen seasons.","\"\"\"","","import sys","import json","from pathlib import Path","","# Add src to path","src_dir = Path(__file__).parent.parent / \"src\"","sys.path.insert(0, str(src_dir))","","from core.season_state import load_season_state","from core.snapshot import verify_snapshot_integrity","from core.season_context import current_season","","","def main():","    \"\"\"CLI entry point.\"\"\"","    import argparse","    ","    parser = argparse.ArgumentParser(","        description=\"Verify season integrity against freeze snapshot\"","    )","    parser.add_argument(","        \"--season\",","        help=\"Season identifier (default: current season)\",","        default=None","    )","    parser.add_argument(","        \"--json\",","        action=\"store_true\",","        help=\"Output results as JSON\"","    )","    parser.add_argument(","        \"--strict\",","        action=\"store_true\",","        help=\"Exit with non-zero code if integrity check fails\"","    )","    ","    args = parser.parse_args()","    ","    # Determine season","    season = args.season or current_season()","    ","    # Check if season is frozen","    try:","        state = load_season_state(season)","        is_frozen = state.is_frozen()","    except Exception as e:","        print(f\"Error loading season state: {e}\", file=sys.stderr)","        sys.exit(1)","    ","    # Verify integrity","    try:","        result = verify_snapshot_integrity(season)","    except Exception as e:","        print(f\"Error verifying integrity: {e}\", file=sys.stderr)","        sys.exit(1)","    ","    # Output results","    if args.json:","        output = {","            \"season\": season,","            \"is_frozen\": is_frozen,","            \"integrity_check\": result","        }","        print(json.dumps(output, indent=2))","    else:","        print(f\"Season: {season}\")","        print(f\"State: {'FROZEN' if is_frozen else 'OPEN'}\")","        print(f\"Integrity Check: {'PASS' if result['ok'] else 'FAIL'}\")","        print(f\"Artifacts Checked: {result['total_checked']}\")","        ","        if not result[\"ok\"]:","            print(\"\\n--- Integrity Issues ---\")","            if result[\"missing_files\"]:","                print(f\"Missing files ({len(result['missing_files'])}):\")","                for f in result[\"missing_files\"][:10]:  # Show first 10","                    print(f\"  - {f}\")","                if len(result[\"missing_files\"]) > 10:","                    print(f\"  ... and {len(result['missing_files']) - 10} more\")","            ","            if result[\"changed_files\"]:","                print(f\"\\nChanged files ({len(result['changed_files'])}):\")","                for f in result[\"changed_files\"][:10]:  # Show first 10","                    print(f\"  - {f}\")","                if len(result[\"changed_files\"]) > 10:","                    print(f\"  ... and {len(result['changed_files']) - 10} more\")","            ","            if result[\"new_files\"]:","                print(f\"\\nNew files ({len(result['new_files'])}):\")","                for f in result[\"new_files\"][:10]:  # Show first 10","                    print(f\"  - {f}\")","                if len(result[\"new_files\"]) > 10:","                    print(f\"  ... and {len(result['new_files']) - 10} more\")","        ","        if result[\"errors\"]:","            print(f\"\\nErrors:\")","            for error in result[\"errors\"]:","                print(f\"  - {error}\")","    ","    # Exit code","    if args.strict and not result[\"ok\"]:","        sys.exit(1)","    else:","        sys.exit(0)","","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"scripts/verify_season_integrity.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":0,"sha256":"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855","total_lines":0,"chunk_count":0}
{"type":"file_footer","path":"src/__init__.py","complete":true,"emitted_chunks":0}
{"type":"file_header","path":"src/config/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":38,"sha256":"14a2ed5e898cc53ef825917d5e005129e0c545b194efef354b5bb227494cff1c","total_lines":4,"chunk_count":1}
{"type":"file_chunk","path":"src/config/__init__.py","chunk_index":0,"line_start":1,"line_end":4,"content":["","\"\"\"Configuration constants for \"\"\"","",""]}
{"type":"file_footer","path":"src/config/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/config/constants.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":265,"sha256":"f71c9e92e499bd6867f81ba2cf59768abbfeb9b7e2f77579f0e625784df34863","total_lines":13,"chunk_count":1}
{"type":"file_chunk","path":"src/config/constants.py","chunk_index":0,"line_start":1,"line_end":13,"content":["","\"\"\"Phase 4 constants definition.","","These constants define the core parameters for Phase 4 Funnel v1 pipeline.","\"\"\"","","# Top-K selection parameter","TOPK_K: int = 20","","# Stage0 proxy name (must match the proxy implementation name)","STAGE0_PROXY_NAME: str = \"ma_proxy_v0\"","",""]}
{"type":"file_footer","path":"src/config/constants.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/config/dtypes.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":730,"sha256":"a34a20f55569577df04c1cdefd124716eba063ea27b401fe1786ffebd259cc71","total_lines":23,"chunk_count":1}
{"type":"file_chunk","path":"src/config/dtypes.py","chunk_index":0,"line_start":1,"line_end":23,"content":["","\"\"\"Dtype configuration for memory optimization.","","Centralized dtype definitions to avoid hardcoding throughout the codebase.","These dtypes are optimized for memory bandwidth while maintaining precision where needed.","\"\"\"","","import numpy as np","","# Stage0: Use float32 for price arrays to reduce memory bandwidth","PRICE_DTYPE_STAGE0 = np.float32","","# Stage2: Keep float64 for final PnL accumulation (conservative)","PRICE_DTYPE_STAGE2 = np.float64","","# Intent arrays: Use float64 for prices (strict parity), uint8 for enums","INTENT_PRICE_DTYPE = np.float64","INTENT_ENUM_DTYPE = np.uint8  # For role, kind, side","","# Index arrays: Use int32 instead of int64 where possible","INDEX_DTYPE = np.int32  # For bar_index, param_id (if within int32 range)","",""]}
{"type":"file_footer","path":"src/config/dtypes.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":254,"sha256":"cc317e548fedac9758519b1acdbbd07b7700ef207acfe99f3f77978775694f91","total_lines":11,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/__init__.py","chunk_index":0,"line_start":1,"line_end":11,"content":["","\"\"\"","Contracts for GUI payload validation and boundary enforcement.","","These schemas define the allowed shape of GUI-originated requests,","ensuring GUI cannot inject execution semantics or violate governance rules.","\"\"\"","","from __future__ import annotations","",""]}
{"type":"file_footer","path":"src/contracts/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/contracts/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":429,"sha256":"bc8c557b56985bf7db466020a3e3232cd30bb73c1d8dd6c1ab6cad7542de1713","note":"skipped by policy"}
{"type":"file_skipped","path":"src/contracts/data/__pycache__/snapshot_models.cpython-312.pyc","reason":"cache","bytes":2781,"sha256":"d23fbe88dfda2a99d28b4405bbb40718935975d20dcc60fb7cc155eb659bd97e","note":"skipped by policy"}
{"type":"file_skipped","path":"src/contracts/data/__pycache__/snapshot_payloads.cpython-312.pyc","reason":"cache","bytes":1341,"sha256":"253f5b460b56a93e50f6faf65e136870b8c90270f9fe869e06bf8cefdc443d14","note":"skipped by policy"}
{"type":"file_header","path":"src/contracts/data/snapshot_models.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2180,"sha256":"c2317b440c356c7c72a21abcafe0ea084004d3e0094f290bf385be41192bd3a7","total_lines":71,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/data/snapshot_models.py","chunk_index":0,"line_start":1,"line_end":71,"content":["\"\"\"","Snapshot metadata models (Phase 16.5).","\"\"\"","","from __future__ import annotations","","from pydantic import BaseModel, ConfigDict, Field","","","class SnapshotStats(BaseModel):","    \"\"\"Basic statistics of a snapshot.\"\"\"","","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","","    count: int = Field(..., description=\"Number of bars\", ge=0)","    min_timestamp: str = Field(..., description=\"Earliest bar timestamp (ISO 8601 UTC)\")","    max_timestamp: str = Field(..., description=\"Latest bar timestamp (ISO 8601 UTC)\")","    min_price: float = Field(..., description=\"Lowest low price across bars\", ge=0.0)","    max_price: float = Field(..., description=\"Highest high price across bars\", ge=0.0)","    total_volume: float = Field(..., description=\"Sum of volume across bars\", ge=0.0)","","","class SnapshotMetadata(BaseModel):","    \"\"\"Immutable metadata of a data snapshot.\"\"\"","","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","","    snapshot_id: str = Field(","        ...,","        description=\"Deterministic snapshot identifier\",","        min_length=1,","    )","    symbol: str = Field(","        ...,","        description=\"Trading symbol\",","        min_length=1,","    )","    timeframe: str = Field(","        ...,","        description=\"Bar timeframe\",","        min_length=1,","    )","    transform_version: str = Field(","        ...,","        description=\"Version of the normalization algorithm (e.g., 'v1')\",","        min_length=1,","    )","    created_at: str = Field(","        ...,","        description=\"ISO 8601 UTC timestamp when snapshot was created (may include fractional seconds)\",","        pattern=r\"^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(\\.\\d+)?Z$\",","    )","    raw_sha256: str = Field(","        ...,","        description=\"SHA256 of the raw bars JSON\",","        pattern=r\"^[a-f0-9]{64}$\",","    )","    normalized_sha256: str = Field(","        ...,","        description=\"SHA256 of the normalized bars JSON\",","        pattern=r\"^[a-f0-9]{64}$\",","    )","    manifest_sha256: str = Field(","        ...,","        description=\"SHA256 of the manifest JSON (excluding this field)\",","        pattern=r\"^[a-f0-9]{64}$\",","    )","    stats: SnapshotStats = Field(","        ...,","        description=\"Basic statistics of the snapshot\",","    )"]}
{"type":"file_footer","path":"src/contracts/data/snapshot_models.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/data/snapshot_payloads.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":918,"sha256":"2f244a2acff18250105e95f6177d84fe344db8bfd0b7a48a6146e86e8c4875e0","total_lines":36,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/data/snapshot_payloads.py","chunk_index":0,"line_start":1,"line_end":36,"content":["\"\"\"","Snapshot creation payloads (Phase 16.5).","\"\"\"","","from __future__ import annotations","","from typing import Any","","from pydantic import BaseModel, ConfigDict, Field","","","class SnapshotCreatePayload(BaseModel):","    \"\"\"Payload for creating a data snapshot from raw bars.\"\"\"","","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","","    raw_bars: list[dict[str, Any]] = Field(","        ...,","        description=\"List of raw bar dictionaries with timestamp, open, high, low, close, volume\",","        min_length=1,","    )","    symbol: str = Field(","        ...,","        description=\"Trading symbol (e.g., 'MNQ')\",","        min_length=1,","    )","    timeframe: str = Field(","        ...,","        description=\"Bar timeframe (e.g., '1m', '5m', '1h')\",","        min_length=1,","    )","    transform_version: str = Field(","        default=\"v1\",","        description=\"Version of the normalization algorithm (e.g., 'v1')\",","        min_length=1,","    )"]}
{"type":"file_footer","path":"src/contracts/data/snapshot_payloads.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/dimensions.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4190,"sha256":"cb377d831ce881ccddaf86d239d20185d5ca0ec012eb4e929f609cbc71a450e6","total_lines":107,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/dimensions.py","chunk_index":0,"line_start":1,"line_end":107,"content":["","from __future__ import annotations","","import json","from typing import Any, Dict, List, Optional, Tuple","from pydantic import BaseModel, ConfigDict, Field, model_validator","","","class SessionSpec(BaseModel):","    \"\"\"äº¤æ˜“æ™‚æ®µè¦æ ¼ï¼Œæ‰€æœ‰æ™‚é–“çš†ç‚ºå°åŒ—æ™‚é–“ (Asia/Taipei)\"\"\"","    tz: str = \"Asia/Taipei\"","    open_taipei: str  # HH:MM æ ¼å¼ï¼Œä¾‹å¦‚ \"07:00\"","    close_taipei: str  # HH:MM æ ¼å¼ï¼Œä¾‹å¦‚ \"06:00\"ï¼ˆæ¬¡æ—¥ï¼‰","    breaks_taipei: List[Tuple[str, str]] = []  # ä¼‘å¸‚æ™‚æ®µåˆ—è¡¨ï¼Œæ¯å€‹æ™‚æ®µç‚º (start, end)","    notes: str = \"\"  # å‚™è¨»ï¼Œä¾‹å¦‚ \"CME MNQ é›»å­ç›¤\"","","    @model_validator(mode=\"after\")","    def _validate_time_format(self) -> \"SessionSpec\":","        \"\"\"é©—è­‰æ™‚é–“æ ¼å¼ç‚º HH:MM\"\"\"","        import re","        time_pattern = re.compile(r\"^([01]?[0-9]|2[0-3]):([0-5][0-9])$\")","        ","        if not time_pattern.match(self.open_taipei):","            raise ValueError(f\"open_taipei å¿…é ˆç‚º HH:MM æ ¼å¼ï¼Œæ”¶åˆ°: {self.open_taipei}\")","        if not time_pattern.match(self.close_taipei):","            raise ValueError(f\"close_taipei å¿…é ˆç‚º HH:MM æ ¼å¼ï¼Œæ”¶åˆ°: {self.close_taipei}\")","        ","        for start, end in self.breaks_taipei:","            if not time_pattern.match(start):","                raise ValueError(f\"break start å¿…é ˆç‚º HH:MM æ ¼å¼ï¼Œæ”¶åˆ°: {start}\")","            if not time_pattern.match(end):","                raise ValueError(f\"break end å¿…é ˆç‚º HH:MM æ ¼å¼ï¼Œæ”¶åˆ°: {end}\")","        ","        return self","","","class InstrumentDimension(BaseModel):","    \"\"\"å•†å“ç¶­åº¦å®šç¾©ï¼ŒåŒ…å«äº¤æ˜“æ‰€ã€æ™‚å€ã€äº¤æ˜“æ™‚æ®µç­‰è³‡è¨Š\"\"\"","    instrument_id: str  # ä¾‹å¦‚ \"MNQ\", \"MES\", \"NK\", \"TXF\"","    exchange: str  # ä¾‹å¦‚ \"CME\", \"TAIFEX\"","    market: str = \"\"  # å¯é¸ï¼Œä¾‹å¦‚ \"é›»å­ç›¤\", \"æ—¥ç›¤\"","    currency: str = \"\"  # å¯é¸ï¼Œä¾‹å¦‚ \"USD\", \"TWD\"","    tick_size: float  # tick å¤§å°ï¼Œå¿…é ˆ > 0ï¼Œä¾‹å¦‚ MNQ=0.25, MES=0.25, MXF=1.0","    session: SessionSpec","    source: str = \"manual\"  # ä¾†æºæ¨™è¨˜ï¼Œæœªä¾†å¯ç‚º \"official_site\"","    source_updated_at: str = \"\"  # ä¾†æºæ›´æ–°æ™‚é–“ï¼ŒISO æ ¼å¼","    version: str = \"v1\"  # ç‰ˆæœ¬æ¨™è¨˜ï¼Œæœªä¾†å‡ç´šç”¨","","    @model_validator(mode=\"after\")","    def _validate_tick_size(self) -> \"InstrumentDimension\":","        \"\"\"é©—è­‰ tick_size ç‚ºæ­£æ•¸\"\"\"","        if self.tick_size <= 0:","            raise ValueError(f\"tick_size å¿…é ˆ > 0ï¼Œæ”¶åˆ°: {self.tick_size}\")","        return self","","","class DimensionRegistry(BaseModel):","    \"\"\"ç¶­åº¦è¨»å†Šè¡¨ï¼Œæ”¯æ´é€éŽ dataset_id æˆ– symbol æŸ¥è©¢\"\"\"","    model_config = ConfigDict(extra=\"allow\")  # å…è¨± metadata ç­‰é¡å¤–æ¬„ä½","    ","    by_dataset_id: Dict[str, InstrumentDimension] = Field(default_factory=dict)","    by_symbol: Dict[str, InstrumentDimension] = Field(default_factory=dict)","","    def get(self, dataset_id: str, symbol: str | None = None) -> InstrumentDimension | None:","        \"\"\"","        æŸ¥è©¢ç¶­åº¦å®šç¾©ï¼Œå„ªå…ˆä½¿ç”¨ dataset_idï¼Œå…¶æ¬¡ symbol","        ","        Args:","            dataset_id: è³‡æ–™é›† IDï¼Œä¾‹å¦‚ \"CME.MNQ.60m.2020-2024\"","            symbol: å•†å“ç¬¦è™Ÿï¼Œä¾‹å¦‚ \"CME.MNQ\"","        ","        Returns:","            InstrumentDimension æˆ– Noneï¼ˆå¦‚æžœæ‰¾ä¸åˆ°ï¼‰","        \"\"\"","        # å„ªå…ˆä½¿ç”¨ dataset_id","        if dataset_id in self.by_dataset_id:","            return self.by_dataset_id[dataset_id]","        ","        # å…¶æ¬¡ä½¿ç”¨ symbol","        if symbol and symbol in self.by_symbol:","            return self.by_symbol[symbol]","        ","        # å¦‚æžœæ²’æœ‰æä¾› symbolï¼Œå˜—è©¦å¾ž dataset_id æŽ¨å°Ž symbol","        if not symbol:","            # ç°¡å–®æŽ¨å°Žï¼šå–å‰å…©å€‹éƒ¨åˆ†ï¼ˆä¾‹å¦‚ \"CME.MNQ.60m.2020-2024\" -> \"CME.MNQ\"ï¼‰","            parts = dataset_id.split(\".\")","            if len(parts) >= 2:","                derived_symbol = f\"{parts[0]}.{parts[1]}\"","                if derived_symbol in self.by_symbol:","                    return self.by_symbol[derived_symbol]","        ","        return None","","","def canonical_json(obj: dict) -> str:","    \"\"\"","    ç”¢ç”Ÿæ¨™æº–åŒ– JSON å­—ä¸²ï¼Œç¢ºä¿åºåˆ—åŒ–ä¸€è‡´æ€§","    ","    Args:","        obj: è¦åºåˆ—åŒ–çš„å­—å…¸","    ","    Returns:","        æ¨™æº–åŒ– JSON å­—ä¸²","    \"\"\"","    return json.dumps(obj, ensure_ascii=False, sort_keys=True, separators=(\",\", \":\"))","",""]}
{"type":"file_footer","path":"src/contracts/dimensions.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/dimensions_loader.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2977,"sha256":"9eec3f459a3e118ad330b51d468a70afdbfcc57462ad6a09c5d34b480242161b","total_lines":104,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/dimensions_loader.py","chunk_index":0,"line_start":1,"line_end":104,"content":["","from __future__ import annotations","","import json","from pathlib import Path","from typing import Any, Dict","","from contracts.dimensions import DimensionRegistry, canonical_json","","","def default_registry_path() -> Path:","    \"\"\"","    å–å¾—é è¨­ç¶­åº¦è¨»å†Šè¡¨æª”æ¡ˆè·¯å¾‘","    ","    Returns:","        Path ç‰©ä»¶æŒ‡å‘ configs/dimensions_registry.json","    \"\"\"","    # å¾žå°ˆæ¡ˆæ ¹ç›®éŒ„é–‹å§‹","    project_root = Path(__file__).parent.parent.parent","    return project_root / \"configs\" / \"dimensions_registry.json\"","","","def load_dimension_registry(path: Path | None = None) -> DimensionRegistry:","    \"\"\"","    è¼‰å…¥ç¶­åº¦è¨»å†Šè¡¨","    ","    Args:","        path: è¨»å†Šè¡¨æª”æ¡ˆè·¯å¾‘ï¼Œè‹¥ç‚º None å‰‡ä½¿ç”¨é è¨­è·¯å¾‘","    ","    Returns:","        DimensionRegistry ç‰©ä»¶","    ","    Raises:","        ValueError: æª”æ¡ˆå­˜åœ¨ä½† JSON è§£æžå¤±æ•—æˆ– schema é©—è­‰å¤±æ•—","        FileNotFoundError: ä¸æœƒå¼•ç™¼ï¼Œæª”æ¡ˆä¸å­˜åœ¨æ™‚å›žå‚³ç©ºè¨»å†Šè¡¨","    \"\"\"","    if path is None:","        path = default_registry_path()","    ","    # æª”æ¡ˆä¸å­˜åœ¨ -> å›žå‚³ç©ºè¨»å†Šè¡¨","    if not path.exists():","        return DimensionRegistry()","    ","    # è®€å–æª”æ¡ˆå…§å®¹","    try:","        content = path.read_text(encoding=\"utf-8\")","    except (IOError, OSError) as e:","        raise ValueError(f\"ç„¡æ³•è®€å–ç¶­åº¦è¨»å†Šè¡¨æª”æ¡ˆ {path}: {e}\")","    ","    # è§£æž JSON","    try:","        data = json.loads(content)","    except json.JSONDecodeError as e:","        raise ValueError(f\"ç¶­åº¦è¨»å†Šè¡¨ JSON è§£æžå¤±æ•— {path}: {e}\")","    ","    # é©—è­‰ä¸¦å»ºç«‹ DimensionRegistry","    try:","        # ç¢ºä¿æœ‰å¿…è¦çš„éµ","        if not isinstance(data, dict):","            raise ValueError(\"æ ¹ç¯€é»žå¿…é ˆæ˜¯å­—å…¸\")","        ","        # å»ºç«‹ registryï¼Œpydantic æœƒé©—è­‰ schema","        registry = DimensionRegistry(**data)","        return registry","    except Exception as e:","        raise ValueError(f\"ç¶­åº¦è¨»å†Šè¡¨ schema é©—è­‰å¤±æ•— {path}: {e}\")","","","def write_dimension_registry(registry: DimensionRegistry, path: Path | None = None) -> None:","    \"\"\"","    å¯«å…¥ç¶­åº¦è¨»å†Šè¡¨ï¼ˆåŽŸå­å¯«å…¥ï¼‰","    ","    Args:","        registry: è¦å¯«å…¥çš„ DimensionRegistry","        path: ç›®æ¨™æª”æ¡ˆè·¯å¾‘ï¼Œè‹¥ç‚º None å‰‡ä½¿ç”¨é è¨­è·¯å¾‘","    ","    Note:","        ä½¿ç”¨åŽŸå­å¯«å…¥ï¼ˆtmp + replaceï¼‰é¿å…å¯«å…¥éŽç¨‹ä¸­æ–·","    \"\"\"","    if path is None:","        path = default_registry_path()","    ","    # ç¢ºä¿ç›®éŒ„å­˜åœ¨","    path.parent.mkdir(parents=True, exist_ok=True)","    ","    # è½‰æ›ç‚ºå­—å…¸ä¸¦æ¨™æº–åŒ– JSON","    data = registry.model_dump()","    json_str = canonical_json(data)","    ","    # åŽŸå­å¯«å…¥ï¼šå…ˆå¯«åˆ°æš«å­˜æª”æ¡ˆï¼Œå†ç§»å‹•","    temp_path = path.with_suffix(\".json.tmp\")","    try:","        temp_path.write_text(json_str, encoding=\"utf-8\")","        temp_path.replace(path)","    except Exception as e:","        # æ¸…ç†æš«å­˜æª”æ¡ˆ","        if temp_path.exists():","            try:","                temp_path.unlink()","            except:","                pass","        raise IOError(f\"å¯«å…¥ç¶­åº¦è¨»å†Šè¡¨å¤±æ•— {path}: {e}\")","",""]}
{"type":"file_footer","path":"src/contracts/dimensions_loader.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/features.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3103,"sha256":"05178ef1f065b1cf47960dbfd543ead8bc83a6eb0305267ac5f53c3f68bf767e","total_lines":111,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/features.py","chunk_index":0,"line_start":1,"line_end":111,"content":["","\"\"\"","Feature Registry åˆç´„","","å®šç¾©ç‰¹å¾µè¦æ ¼èˆ‡è¨»å†Šè¡¨ï¼Œæ”¯æ´ deterministic æŸ¥è©¢èˆ‡ lookback è¨ˆç®—ã€‚","\"\"\"","","from __future__ import annotations","","from typing import Dict, List, Optional","from pydantic import BaseModel, Field","","","class FeatureSpec(BaseModel):","    \"\"\"","    å–®ä¸€ç‰¹å¾µè¦æ ¼","    ","    Attributes:","        name: ç‰¹å¾µåç¨±ï¼ˆä¾‹å¦‚ \"atr_14\"ï¼‰","        timeframe_min: é©ç”¨çš„ timeframe åˆ†é˜æ•¸ï¼ˆ15, 30, 60, 120, 240ï¼‰","        lookback_bars: è¨ˆç®—æ‰€éœ€çš„æœ€å¤§ lookback bar æ•¸ï¼ˆä¾‹å¦‚ ATR(14) éœ€è¦ 14ï¼‰","        params: åƒæ•¸å­—å…¸ï¼ˆä¾‹å¦‚ {\"window\": 14, \"method\": \"log\"}ï¼‰","    \"\"\"","    name: str","    timeframe_min: int","    lookback_bars: int = Field(default=0, ge=0)","    params: Dict[str, str | int | float] = Field(default_factory=dict)","","","class FeatureRegistry(BaseModel):","    \"\"\"","    ç‰¹å¾µè¨»å†Šè¡¨","    ","    ç®¡ç†æ‰€æœ‰ç‰¹å¾µè¦æ ¼ï¼Œæä¾›æŒ‰ timeframe æŸ¥è©¢èˆ‡ lookback è¨ˆç®—ã€‚","    \"\"\"","    specs: List[FeatureSpec] = Field(default_factory=list)","    ","    def specs_for_tf(self, tf_min: int) -> List[FeatureSpec]:","        \"\"\"","        å–å¾—é©ç”¨æ–¼æŒ‡å®š timeframe çš„æ‰€æœ‰ç‰¹å¾µè¦æ ¼","        ","        Args:","            tf_min: timeframe åˆ†é˜æ•¸ï¼ˆ15, 30, 60, 120, 240ï¼‰","            ","        Returns:","            ç‰¹å¾µè¦æ ¼åˆ—è¡¨ï¼ˆæŒ‰ name æŽ’åºä»¥ç¢ºä¿ deterministicï¼‰","        \"\"\"","        filtered = [spec for spec in self.specs if spec.timeframe_min == tf_min]","        # æŒ‰ name æŽ’åºä»¥ç¢ºä¿ deterministic","        return sorted(filtered, key=lambda s: s.name)","    ","    def max_lookback_for_tf(self, tf_min: int) -> int:","        \"\"\"","        è¨ˆç®—æŒ‡å®š timeframe çš„æœ€å¤§ lookback bar æ•¸","        ","        Args:","            tf_min: timeframe åˆ†é˜æ•¸","            ","        Returns:","            æœ€å¤§ lookback bar æ•¸ï¼ˆå¦‚æžœæ²’æœ‰ç‰¹å¾µå‰‡å›žå‚³ 0ï¼‰","        \"\"\"","        specs = self.specs_for_tf(tf_min)","        if not specs:","            return 0","        return max(spec.lookback_bars for spec in specs)","","","def default_feature_registry() -> FeatureRegistry:","    \"\"\"","    å»ºç«‹é è¨­ç‰¹å¾µè¨»å†Šè¡¨ï¼ˆå¯«æ­» 3 å€‹å…±äº«ç‰¹å¾µï¼‰","    ","    ç‰¹å¾µå®šç¾©ï¼š","    1. atr_14: ATR(14), lookback=14","    2. ret_z_200: returns z-score (window=200), lookback=200","    3. session_vwap: session VWAP, lookback=0","    ","    æ¯å€‹ç‰¹å¾µéƒ½é©ç”¨æ–¼æ‰€æœ‰ timeframeï¼ˆ15, 30, 60, 120, 240ï¼‰","    \"\"\"","    # æ‰€æœ‰æ”¯æ´çš„ timeframe","    timeframes = [15, 30, 60, 120, 240]","    ","    specs = []","    ","    for tf in timeframes:","        # atr_14","        specs.append(FeatureSpec(","            name=\"atr_14\",","            timeframe_min=tf,","            lookback_bars=14,","            params={\"window\": 14}","        ))","        ","        # ret_z_200","        specs.append(FeatureSpec(","            name=\"ret_z_200\",","            timeframe_min=tf,","            lookback_bars=200,","            params={\"window\": 200, \"method\": \"log\"}","        ))","        ","        # session_vwap","        specs.append(FeatureSpec(","            name=\"session_vwap\",","            timeframe_min=tf,","            lookback_bars=0,","            params={}","        ))","    ","    return FeatureRegistry(specs=specs)","",""]}
{"type":"file_footer","path":"src/contracts/features.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/fingerprint.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9301,"sha256":"50dbf5421d278eb4762452e3d3218de5ef6b7cd7411b50058c959f9ae0ec1b57","total_lines":278,"chunk_count":2}
{"type":"file_chunk","path":"src/contracts/fingerprint.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Fingerprint Index è³‡æ–™æ¨¡åž‹","","ç”¨æ–¼è¨˜éŒ„è³‡æ–™é›†æ¯æ—¥çš„ hash æŒ‡ç´‹ï¼Œæ”¯æ´å¢žé‡é‡ç®—çš„è­‰æ“šç³»çµ±ã€‚","\"\"\"","","from __future__ import annotations","","import hashlib","import json","from datetime import date, datetime","from typing import Dict","","from pydantic import BaseModel, ConfigDict, Field, model_validator","","from contracts.dimensions import canonical_json","","","class FingerprintIndex(BaseModel):","    \"\"\"","    è³‡æ–™é›†æŒ‡ç´‹ç´¢å¼•","    ","    è¨˜éŒ„è³‡æ–™é›†æ¯æ—¥çš„ hash æŒ‡ç´‹ï¼Œç”¨æ–¼æª¢æ¸¬è³‡æ–™è®Šæ›´èˆ‡å¢žé‡é‡ç®—ã€‚","    \"\"\"","    model_config = ConfigDict(frozen=True)  # ä¸å¯è®Šï¼Œç¢ºä¿ deterministic","    ","    dataset_id: str = Field(","        ...,","        description=\"è³‡æ–™é›† IDï¼Œä¾‹å¦‚ 'CME.MNQ.60m.2020-2024'\",","        examples=[\"CME.MNQ.60m.2020-2024\", \"TWF.MXF.15m.2018-2023\"]","    )","    ","    dataset_timezone: str = Field(","        default=\"Asia/Taipei\",","        description=\"è³‡æ–™é›†æ™‚å€ï¼Œé è¨­ç‚ºå°åŒ—æ™‚é–“\",","        examples=[\"Asia/Taipei\", \"UTC\"]","    )","    ","    range_start: str = Field(","        ...,","        description=\"è³‡æ–™ç¯„åœèµ·å§‹æ—¥ (YYYY-MM-DD)\",","        pattern=r\"^\\d{4}-\\d{2}-\\d{2}$\",","        examples=[\"2020-01-01\", \"2018-01-01\"]","    )","    ","    range_end: str = Field(","        ...,","        description=\"è³‡æ–™ç¯„åœçµæŸæ—¥ (YYYY-MM-DD)\",","        pattern=r\"^\\d{4}-\\d{2}-\\d{2}$\",","        examples=[\"2024-12-31\", \"2023-12-31\"]","    )","    ","    day_hashes: Dict[str, str] = Field(","        default_factory=dict,","        description=\"æ¯æ—¥ hash æ˜ å°„ï¼Œkey ç‚ºæ—¥æœŸ (YYYY-MM-DD)ï¼Œvalue ç‚º sha256 hex\",","        examples=[{\"2020-01-01\": \"abc123...\", \"2020-01-02\": \"def456...\"}]","    )","    ","    index_sha256: str = Field(","        ...,","        description=\"ç´¢å¼•æœ¬èº«çš„ SHA256 hashï¼Œè¨ˆç®—æ–¹å¼ç‚º canonical_json(index_without_index_sha256)\",","        examples=[\"a1b2c3d4e5f6...\"]","    )","    ","    build_notes: str = Field(","        default=\"\",","        description=\"å»ºç½®å‚™è¨»ï¼Œä¾‹å¦‚å»ºç½®å·¥å…·ç‰ˆæœ¬æˆ–ç‰¹æ®Šè™•ç†èªªæ˜Ž\",","        examples=[\"built with fingerprint v1.0\", \"normalized 24:00:00 times\"]","    )","    ","    @model_validator(mode=\"after\")","    def _validate_date_range(self) -> \"FingerprintIndex\":","        \"\"\"é©—è­‰æ—¥æœŸç¯„åœèˆ‡ day_hashes çš„ä¸€è‡´æ€§\"\"\"","        try:","            start_date = date.fromisoformat(self.range_start)","            end_date = date.fromisoformat(self.range_end)","            ","            if start_date > end_date:","                raise ValueError(f\"range_start ({self.range_start}) ä¸èƒ½æ™šæ–¼ range_end ({self.range_end})\")","            ","            # é©—è­‰ day_hashes ä¸­çš„æ—¥æœŸéƒ½åœ¨ç¯„åœå…§","            for day_str in self.day_hashes.keys():","                try:","                    day_date = date.fromisoformat(day_str)","                    if not (start_date <= day_date <= end_date):","                        raise ValueError(","                            f\"day_hashes ä¸­çš„æ—¥æœŸ {day_str} ä¸åœ¨ç¯„åœ [{self.range_start}, {self.range_end}] å…§\"","                        )","                except ValueError as e:","                    raise ValueError(f\"ç„¡æ•ˆçš„æ—¥æœŸæ ¼å¼: {day_str}\") from e","            ","            # é©—è­‰ hash æ ¼å¼","            for day_str, hash_val in self.day_hashes.items():","                if not isinstance(hash_val, str):","                    raise ValueError(f\"day_hashes[{day_str}] å¿…é ˆæ˜¯å­—ä¸²\")","                if len(hash_val) != 64:  # SHA256 hex é•·åº¦","                    raise ValueError(f\"day_hashes[{day_str}] é•·åº¦å¿…é ˆç‚º 64 (SHA256 hex)ï¼Œå¯¦éš›é•·åº¦: {len(hash_val)}\")","                # ç°¡å–®é©—è­‰æ˜¯å¦ç‚º hex","                try:","                    int(hash_val, 16)","                except ValueError:","                    raise ValueError(f\"day_hashes[{day_str}] ä¸æ˜¯æœ‰æ•ˆçš„ hex å­—ä¸²\")","            ","            return self","        except ValueError as e:","            raise ValueError(f\"æ—¥æœŸé©—è­‰å¤±æ•—: {e}\")","    ","    @model_validator(mode=\"after\")","    def _validate_index_sha256(self) -> \"FingerprintIndex\":","        \"\"\"é©—è­‰ index_sha256 æ˜¯å¦æ­£ç¢ºè¨ˆç®—\"\"\"","        # è¨ˆç®—é æœŸçš„ hash","        expected_hash = self._compute_index_sha256()","        ","        if self.index_sha256 != expected_hash:","            raise ValueError(","                f\"index_sha256 é©—è­‰å¤±æ•—: é æœŸ {expected_hash}ï¼Œå¯¦éš› {self.index_sha256}\"","            )","        ","        return self","    ","    def _compute_index_sha256(self) -> str:","        \"\"\"","        è¨ˆç®—ç´¢å¼•çš„ SHA256 hash","        ","        æŽ’é™¤ index_sha256 æ¬„ä½æœ¬èº«ï¼Œä½¿ç”¨ canonical_json ç¢ºä¿ deterministic","        \"\"\"","        # å»ºç«‹ä¸åŒ…å« index_sha256 çš„å­—å…¸","        data = self.model_dump(exclude={\"index_sha256\"})","        ","        # ä½¿ç”¨ canonical_json ç¢ºä¿æŽ’åºä¸€è‡´","        json_str = canonical_json(data)","        ","        # è¨ˆç®— SHA256","        return hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","    ","    @classmethod","    def create(","        cls,","        dataset_id: str,","        range_start: str,","        range_end: str,","        day_hashes: Dict[str, str],","        dataset_timezone: str = \"Asia/Taipei\",","        build_notes: str = \"\"","    ) -> \"FingerprintIndex\":","        \"\"\"","        å»ºç«‹æ–°çš„ FingerprintIndexï¼Œè‡ªå‹•è¨ˆç®— index_sha256","        ","        Args:","            dataset_id: è³‡æ–™é›† ID","            range_start: èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)","            range_end: çµæŸæ—¥æœŸ (YYYY-MM-DD)","            day_hashes: æ¯æ—¥ hash æ˜ å°„","            dataset_timezone: æ™‚å€","            build_notes: å»ºç½®å‚™è¨»","        ","        Returns:","            FingerprintIndex å¯¦ä¾‹","        \"\"\"","        # å»ºç«‹å­—å…¸ï¼ˆä¸å« index_sha256ï¼‰","        data = {","            \"dataset_id\": dataset_id,","            \"dataset_timezone\": dataset_timezone,","            \"range_start\": range_start,","            \"range_end\": range_end,","            \"day_hashes\": day_hashes,","            \"build_notes\": build_notes,","        }","        ","        # ç›´æŽ¥è¨ˆç®— hashï¼Œé¿å…å»ºç«‹æš«å­˜å¯¦ä¾‹è§¸ç™¼é©—è­‰","        import hashlib","        from contracts.dimensions import canonical_json","        ","        json_str = canonical_json(data)","        index_sha256 = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","        ","        # å»ºç«‹æœ€çµ‚å¯¦ä¾‹","        return cls(**data, index_sha256=index_sha256)","    ","    def get_day_hash(self, day_str: str) -> str | None:","        \"\"\"","        å–å¾—æŒ‡å®šæ—¥æœŸçš„ hash","        ","        Args:","            day_str: æ—¥æœŸå­—ä¸² (YYYY-MM-DD)","        ","        Returns:","            hash å­—ä¸²æˆ– Noneï¼ˆå¦‚æžœä¸å­˜åœ¨ï¼‰","        \"\"\"","        return self.day_hashes.get(day_str)","    ","    def get_earliest_changed_day(","        self,","        other: \"FingerprintIndex\"","    ) -> str | None:","        \"\"\"","        æ¯”è¼ƒå…©å€‹ç´¢å¼•ï¼Œæ‰¾å‡ºæœ€æ—©è®Šæ›´çš„æ—¥æœŸ","        ","        åªè€ƒæ…®å…©å€‹ç´¢å¼•ä¸­éƒ½å­˜åœ¨çš„æ—¥æœŸï¼Œä¸” hash ä¸åŒã€‚"]}
{"type":"file_chunk","path":"src/contracts/fingerprint.py","chunk_index":1,"line_start":201,"line_end":278,"content":["        å¦‚æžœä¸€å€‹æ—¥æœŸåªåœ¨ä¸€å€‹ç´¢å¼•ä¸­å­˜åœ¨ï¼ˆæ–°å¢žæˆ–åˆªé™¤ï¼‰ï¼Œä¸è¦–ç‚ºã€Œè®Šæ›´ã€ã€‚","        ","        Args:","            other: å¦ä¸€å€‹ FingerprintIndex","        ","        Returns:","            æœ€æ—©è®Šæ›´çš„æ—¥æœŸå­—ä¸²ï¼Œå¦‚æžœå®Œå…¨ç›¸åŒå‰‡å›žå‚³ None","        \"\"\"","        if self.dataset_id != other.dataset_id:","            raise ValueError(\"ç„¡æ³•æ¯”è¼ƒä¸åŒ dataset_id çš„ç´¢å¼•\")","        ","        earliest_changed = None","        ","        # åªæª¢æŸ¥å…©å€‹ç´¢å¼•ä¸­éƒ½å­˜åœ¨çš„æ—¥æœŸ","        common_days = set(self.day_hashes.keys()) & set(other.day_hashes.keys())","        ","        for day_str in sorted(common_days):","            hash1 = self.get_day_hash(day_str)","            hash2 = other.get_day_hash(day_str)","            ","            if hash1 != hash2:","                if earliest_changed is None or day_str < earliest_changed:","                    earliest_changed = day_str","        ","        return earliest_changed","    ","    def is_append_only(self, other: \"FingerprintIndex\") -> bool:","        \"\"\"","        æª¢æŸ¥æ˜¯å¦åƒ…ç‚ºå°¾éƒ¨æ–°å¢žï¼ˆappend-onlyï¼‰","        ","        æ¢ä»¶ï¼š","        1. æ‰€æœ‰èˆŠçš„æ—¥æœŸ hash éƒ½ç›¸åŒ","        2. æ–°çš„ç´¢å¼•åªæ–°å¢žæ—¥æœŸï¼Œæ²’æœ‰åˆªé™¤æ—¥æœŸ","        ","        Args:","            other: æ–°çš„ FingerprintIndex","        ","        Returns:","            æ˜¯å¦ç‚º append-only","        \"\"\"","        if self.dataset_id != other.dataset_id:","            return False","        ","        # æª¢æŸ¥æ˜¯å¦æœ‰æ—¥æœŸè¢«åˆªé™¤","        for day_str in self.day_hashes:","            if day_str not in other.day_hashes:","                return False","        ","        # æª¢æŸ¥èˆŠæ—¥æœŸçš„ hash æ˜¯å¦ç›¸åŒ","        for day_str, hash_val in self.day_hashes.items():","            if other.get_day_hash(day_str) != hash_val:","                return False","        ","        return True","    ","    def get_append_range(self, other: \"FingerprintIndex\") -> tuple[str, str] | None:","        \"\"\"","        å–å¾—æ–°å¢žçš„æ—¥æœŸç¯„åœï¼ˆå¦‚æžœç‚º append-onlyï¼‰","        ","        Args:","            other: æ–°çš„ FingerprintIndex","        ","        Returns:","            (start_date, end_date) æˆ– Noneï¼ˆå¦‚æžœä¸æ˜¯ append-onlyï¼‰","        \"\"\"","        if not self.is_append_only(other):","            return None","        ","        # æ‰¾å‡ºæ–°å¢žçš„æ—¥æœŸ","        new_days = set(other.day_hashes.keys()) - set(self.day_hashes.keys())","        ","        if not new_days:","            return None","        ","        sorted_days = sorted(new_days)","        return sorted_days[0], sorted_days[-1]","",""]}
{"type":"file_footer","path":"src/contracts/fingerprint.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/contracts/gui/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":597,"sha256":"d4130e150518cfb183e3999ac0fdbd123db11a6be5f31d6c121a3431475a1eab","total_lines":23,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/gui/__init__.py","chunk_index":0,"line_start":1,"line_end":23,"content":["","\"\"\"","GUI payload contracts for Research OS.","","These schemas define the allowed shape of GUI-originated requests,","ensuring GUI cannot inject execution semantics or violate governance rules.","\"\"\"","","from __future__ import annotations","","from contracts.gui.submit_batch import SubmitBatchPayload","from contracts.gui.freeze_season import FreezeSeasonPayload","from contracts.gui.export_season import ExportSeasonPayload","from contracts.gui.compare_request import CompareRequestPayload","","__all__ = [","    \"SubmitBatchPayload\",","    \"FreezeSeasonPayload\",","    \"ExportSeasonPayload\",","    \"CompareRequestPayload\",","]","",""]}
{"type":"file_footer","path":"src/contracts/gui/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/contracts/gui/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":745,"sha256":"ffe6e65ad1c142a4667520a3dafada22f0d51c90a747bf8b3e76144563192d5a","note":"skipped by policy"}
{"type":"file_skipped","path":"src/contracts/gui/__pycache__/compare_request.cpython-312.pyc","reason":"cache","bytes":1049,"sha256":"0348e6e7bf16544215e8a7e1276ca223844a5d5ec5e2adabc6ec37ef78c2eb2c","note":"skipped by policy"}
{"type":"file_skipped","path":"src/contracts/gui/__pycache__/export_season.cpython-312.pyc","reason":"cache","bytes":1096,"sha256":"abefd4b462787bfec22663cd318520dc0d25f767fc94044e602670856f9057eb","note":"skipped by policy"}
{"type":"file_skipped","path":"src/contracts/gui/__pycache__/freeze_season.cpython-312.pyc","reason":"cache","bytes":1275,"sha256":"a674ecbaf09c2aa6201b5f9c8479a9a23f346aeade65cea87c8038125537ce91","note":"skipped by policy"}
{"type":"file_skipped","path":"src/contracts/gui/__pycache__/submit_batch.cpython-312.pyc","reason":"cache","bytes":2081,"sha256":"7d6a722424d44ddc02deaaedb69b2d912556a9c92f856ed65ba13c7e08e4082d","note":"skipped by policy"}
{"type":"file_header","path":"src/contracts/gui/compare_request.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":488,"sha256":"b583237d2825759e38ebe7663e6a9886901033e48a361657613e213720df2b86","total_lines":26,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/gui/compare_request.py","chunk_index":0,"line_start":1,"line_end":26,"content":["","\"\"\"","Compare request payload contract for GUI.","","Contract:","- Top K must be positive and â‰¤ 100","\"\"\"","","from __future__ import annotations","","from pydantic import BaseModel, Field","","","class CompareRequestPayload(BaseModel):","    \"\"\"Payload for comparing season results from GUI.\"\"\"","    season: str","    top_k: int = Field(default=20, gt=0, le=100)","","    @classmethod","    def example(cls) -> \"CompareRequestPayload\":","        return cls(","            season=\"2026Q1\",","            top_k=20,","        )","",""]}
{"type":"file_footer","path":"src/contracts/gui/compare_request.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/gui/export_season.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":559,"sha256":"a48f59b34967d643dc47302a04c017a3498521a0aa08f5f744ce86bfa56c6216","total_lines":27,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/gui/export_season.py","chunk_index":0,"line_start":1,"line_end":27,"content":["","\"\"\"","Export season payload contract for GUI.","","Contract:","- Season must be frozen","- Export name immutable once created","\"\"\"","","from __future__ import annotations","","from pydantic import BaseModel, Field","","","class ExportSeasonPayload(BaseModel):","    \"\"\"Payload for exporting a season from GUI.\"\"\"","    season: str","    export_name: str = Field(..., min_length=1, max_length=100, pattern=r\"^[a-zA-Z0-9_-]+$\")","","    @classmethod","    def example(cls) -> \"ExportSeasonPayload\":","        return cls(","            season=\"2026Q1\",","            export_name=\"export_v1\",","        )","",""]}
{"type":"file_footer","path":"src/contracts/gui/export_season.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/gui/freeze_season.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":687,"sha256":"252aa2231b1cb4373edd9f6e653c0d22145bffe47609a0168c6b32ff50a7cad5","total_lines":30,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/gui/freeze_season.py","chunk_index":0,"line_start":1,"line_end":30,"content":["","\"\"\"","Freeze season payload contract for GUI.","","Contract:","- Freeze season metadata cannot be changed after freeze","- Duplicate freeze â†’ 409 Conflict","\"\"\"","","from __future__ import annotations","","from typing import Optional","from pydantic import BaseModel, Field","","","class FreezeSeasonPayload(BaseModel):","    \"\"\"Payload for freezing a season from GUI.\"\"\"","    season: str","    note: Optional[str] = Field(default=None, max_length=1000)","    tags: list[str] = Field(default_factory=list)","","    @classmethod","    def example(cls) -> \"FreezeSeasonPayload\":","        return cls(","            season=\"2026Q1\",","            note=\"Initial research season\",","            tags=[\"research\", \"baseline\"],","        )","",""]}
{"type":"file_footer","path":"src/contracts/gui/freeze_season.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/gui/submit_batch.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1284,"sha256":"56348bd7a34d069362a123162d7efb84328322bd3f9df89294753581ceaf9a8c","total_lines":49,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/gui/submit_batch.py","chunk_index":0,"line_start":1,"line_end":49,"content":["","\"\"\"","Submit batch payload contract for GUI.","","Contract:","- Must not contain execution / engine flags","- Job count â‰¤ 1000","- Ordering does not affect batch_id (handled by API)","\"\"\"","","from __future__ import annotations","","from pathlib import Path","from typing import Optional","from pydantic import BaseModel, Field, field_validator","","","class JobTemplateRef(BaseModel):","    \"\"\"Reference to a job template (GUI-side).\"\"\"","    dataset_id: str","    strategy_id: str","    param_grid_id: str","    # Additional GUI-specific fields may be added here, but must not affect execution","","","class SubmitBatchPayload(BaseModel):","    \"\"\"Payload for submitting a batch of jobs from GUI.\"\"\"","    dataset_id: str","    strategy_id: str","    param_grid_id: str","    jobs: list[JobTemplateRef]","    outputs_root: Path = Field(default=Path(\"outputs\"))","","    @field_validator(\"jobs\")","    @classmethod","    def validate_job_count(cls, v: list[JobTemplateRef]) -> list[JobTemplateRef]:","        if len(v) > 1000:","            raise ValueError(\"Job count must be â‰¤ 1000\")","        if len(v) == 0:","            raise ValueError(\"Job list cannot be empty\")","        return v","","    @field_validator(\"outputs_root\")","    @classmethod","    def ensure_path(cls, v: Path) -> Path:","        # Ensure it's a Path object (already is)","        return v","",""]}
{"type":"file_footer","path":"src/contracts/gui/submit_batch.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/contracts/portfolio/__pycache__/plan_models.cpython-312.pyc","reason":"cache","bytes":5086,"sha256":"ac30470d1919ff7c4209273b70bcdd725be5e9f2d86a53cfea212fcee10aef21","note":"skipped by policy"}
{"type":"file_skipped","path":"src/contracts/portfolio/__pycache__/plan_payloads.cpython-312.pyc","reason":"cache","bytes":2386,"sha256":"e6acc7ee4d05606708f690e37abc9d20c974edb7d3119cf190b543d9d32728fa","note":"skipped by policy"}
{"type":"file_skipped","path":"src/contracts/portfolio/__pycache__/plan_quality_models.cpython-312.pyc","reason":"cache","bytes":5223,"sha256":"2b607c5c7be0c0d4b1542f88f06e862b5b79e4b2e9a32b26bf723a738cd29d84","note":"skipped by policy"}
{"type":"file_skipped","path":"src/contracts/portfolio/__pycache__/plan_view_models.cpython-312.pyc","reason":"cache","bytes":1133,"sha256":"93b1b76440282f6aac3a0685f029b61a61ed899b234317382a6b6d18e41b8a70","note":"skipped by policy"}
{"type":"file_header","path":"src/contracts/portfolio/plan_models.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3553,"sha256":"bb90b5492685e6976c496de322191ddadd0b3fcb8b2540b75b133883c15d8149","total_lines":117,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/portfolio/plan_models.py","chunk_index":0,"line_start":1,"line_end":117,"content":["","from __future__ import annotations","","from typing import Any, Dict, List, Optional, Union","from pydantic import BaseModel, ConfigDict, Field, model_validator, field_validator","","","class SourceRef(BaseModel):","    season: str","    export_name: str","    export_manifest_sha256: str","","    # legacy contract: tests expect this key","    candidates_sha256: str","","    # keep rev2 fields as optional for forward compat","    candidates_file_sha256: Optional[str] = None","    candidates_items_sha256: Optional[str] = None","","","class PlannedCandidate(BaseModel):","    candidate_id: str","    strategy_id: str","    dataset_id: str","    params: Dict[str, Any]","    score: float","    season: str","    source_batch: str","    source_export: str","","    # rev2 enrichment (optional)","    batch_state: Optional[str] = None","    batch_counts: Optional[Dict[str, Any]] = None","    batch_metrics: Optional[Dict[str, Any]] = None","","","class PlannedWeight(BaseModel):","    candidate_id: str","    weight: float","    reason: str","","","class ConstraintsReport(BaseModel):","    # dict of truncated counts: {\"ds1\": 3, ...} / {\"stratA\": 3, ...}","    max_per_strategy_truncated: Dict[str, int] = Field(default_factory=dict)","    max_per_dataset_truncated: Dict[str, int] = Field(default_factory=dict)","","    # list of candidate_ids clipped","    max_weight_clipped: List[str] = Field(default_factory=list)","    min_weight_clipped: List[str] = Field(default_factory=list)","","    renormalization_applied: bool = False","    renormalization_factor: Optional[float] = None","","","class PlanSummary(BaseModel):","    model_config = ConfigDict(extra=\"allow\")  # <-- é‡è¦ï¼šä¿ç•™æ¸¬è©¦ helper å¡žé€²ä¾†çš„æ–°æ¬„ä½","","    # ---- legacy fields (tests expect these) ----","    total_candidates: int","    total_weight: float","","    # bucket_by is a list of field names used to bucket (e.g. [\"dataset_id\"])","    bucket_counts: Dict[str, int] = Field(default_factory=dict)","    bucket_weights: Dict[str, float] = Field(default_factory=dict)","","    # concentration metric","    concentration_herfindahl: float","","    # ---- new fields (optional, for forward compatibility) ----","    num_selected: Optional[int] = None","    num_buckets: Optional[int] = None","    bucket_by: Optional[List[str]] = None","    concentration_top1: Optional[float] = None","    concentration_top3: Optional[float] = None","","    # ---- quality-related fields (hardening tests rely on these existing on read-back) ----","    bucket_coverage: Optional[float] = None","    bucket_coverage_ratio: Optional[float] = None","","","from contracts.portfolio.plan_payloads import PlanCreatePayload","","","class PortfolioPlan(BaseModel):","    plan_id: str","    generated_at_utc: str","","    source: SourceRef","    config: Union[PlanCreatePayload, Dict[str, Any]]","","    universe: List[PlannedCandidate]","    weights: List[PlannedWeight]","","    summaries: PlanSummary","    constraints_report: ConstraintsReport","","    @model_validator(mode=\"after\")","    def _validate_weights_sum(self) -> \"PortfolioPlan\":","        total = sum(w.weight for w in self.weights)","        # Allow tiny floating tolerance","        if abs(total - 1.0) > 1e-9:","            raise ValueError(f\"Total weight must be 1.0, got {total}\")","        return self","","    @field_validator(\"config\", mode=\"before\")","    @classmethod","    def _normalize_config(cls, v):","        # If v is a PlanCreatePayload, convert to dict","        if isinstance(v, PlanCreatePayload):","            return v.model_dump()","        # If v is already a dict, keep as is","        if isinstance(v, dict):","            return v","        raise ValueError(f\"config must be PlanCreatePayload or dict, got {type(v)}\")","",""]}
{"type":"file_footer","path":"src/contracts/portfolio/plan_models.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/portfolio/plan_payloads.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1338,"sha256":"8a081d6b5b62dfd7e51010b361bff72d077586f96ed2e65c1221ba74424ee554","total_lines":42,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/portfolio/plan_payloads.py","chunk_index":0,"line_start":1,"line_end":42,"content":["","from __future__ import annotations","","from typing import List, Literal, Optional","from pydantic import BaseModel, Field, model_validator","","","EnrichField = Literal[\"batch_state\", \"batch_counts\", \"batch_metrics\"]","BucketKey = Literal[\"dataset_id\", \"strategy_id\"]","WeightingPolicy = Literal[\"equal\", \"score_weighted\", \"bucket_equal\"]","","","class PlanCreatePayload(BaseModel):","    season: str","    export_name: str","","    top_n: int = Field(gt=0, le=500, default=50)","    max_per_strategy: int = Field(gt=0, le=500, default=100)","    max_per_dataset: int = Field(gt=0, le=500, default=100)","","    weighting: WeightingPolicy = \"bucket_equal\"","    bucket_by: List[BucketKey] = Field(default_factory=lambda: [\"dataset_id\"])","","    max_weight: float = Field(gt=0.0, le=1.0, default=0.2)","    min_weight: float = Field(ge=0.0, le=1.0, default=0.0)","","    enrich_with_batch_api: bool = True","    enrich_fields: List[EnrichField] = Field(","        default_factory=lambda: [\"batch_state\", \"batch_counts\", \"batch_metrics\"]","    )","","    note: Optional[str] = None","","    @model_validator(mode=\"after\")","    def _validate_ranges(self) -> \"PlanCreatePayload\":","        if not self.bucket_by:","            raise ValueError(\"bucket_by must be non-empty\")","        if self.min_weight > self.max_weight:","            raise ValueError(\"min_weight must be <= max_weight\")","        return self","",""]}
{"type":"file_footer","path":"src/contracts/portfolio/plan_payloads.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/portfolio/plan_quality_models.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3312,"sha256":"3a5f9d40f33eb545f91852ebbfb9fabec52a39fdd705506e12e3b6294dd5be7e","total_lines":109,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/portfolio/plan_quality_models.py","chunk_index":0,"line_start":1,"line_end":109,"content":["","\"\"\"Quality models for portfolio plan grading (GREEN/YELLOW/RED).\"\"\"","from __future__ import annotations","","from datetime import datetime, timezone","from typing import Any, Dict, List, Literal, Optional","from pydantic import BaseModel, Field, ConfigDict","","Grade = Literal[\"GREEN\", \"YELLOW\", \"RED\"]","","","class QualitySourceRef(BaseModel):","    \"\"\"Reference to the source of the plan.\"\"\"","    plan_id: str","    season: Optional[str] = None","    export_name: Optional[str] = None","    export_manifest_sha256: Optional[str] = None","    candidates_sha256: Optional[str] = None","","","class QualityThresholds(BaseModel):","    \"\"\"Thresholds for grading.\"\"\"","    min_total_candidates: int = 10","    # top1_score thresholds (higher is better)","    green_top1: float = 0.90","    yellow_top1: float = 0.80","    red_top1: float = 0.75","    # top3_score thresholds (higher is better) - kept for compatibility","    green_top3: float = 0.85","    yellow_top3: float = 0.75","    red_top3: float = 0.70","    # effective_n thresholds (higher is better) - test expects 7.0 for GREEN, 5.0 for YELLOW","    green_effective_n: float = 7.0","    yellow_effective_n: float = 5.0","    red_effective_n: float = 4.0","    # bucket_coverage thresholds (higher is better) - test expects 0.90 for GREEN, 0.70 for YELLOW","    green_bucket_coverage: float = 0.90","    yellow_bucket_coverage: float = 0.70","    red_bucket_coverage: float = 0.60","    # constraints_pressure thresholds (lower is better)","    green_constraints_pressure: int = 0","    yellow_constraints_pressure: int = 1","    red_constraints_pressure: int = 2","","","class QualityMetrics(BaseModel):","    \"\"\"","    Contract goals:","    - Internal grading code historically uses: top1/top3/top5/bucket_coverage_ratio","    - Hardening tests expect: top1_score/effective_n/bucket_coverage","    We support BOTH via real fields + deterministic properties.","    \"\"\"","    model_config = ConfigDict(populate_by_name=True, extra=\"allow\")","","    total_candidates: int","","    # Canonical stored fields (keep legacy names used by grading)","    top1: float = 0.0","    top3: float = 0.0","    top5: float = 0.0","","    herfindahl: float","    effective_n: float","","    bucket_by: List[str] = Field(default_factory=list)","    bucket_count: int","","    bucket_coverage_ratio: float = 0.0","    constraints_pressure: int = 0","","    # ---- Compatibility properties expected by tests ----","    @property","    def top1_score(self) -> float:","        return float(self.top1)","","    @property","    def top3_score(self) -> float:","        return float(self.top3)","","    @property","    def top5_score(self) -> float:","        return float(self.top5)","","    @property","    def bucket_coverage(self) -> float:","        return float(self.bucket_coverage_ratio)","","    @property","    def concentration_herfindahl(self) -> float:","        return float(self.herfindahl)","","","class PlanQualityReport(BaseModel):","    \"\"\"Complete quality report for a portfolio plan.\"\"\"","    plan_id: str","    generated_at_utc: str","    source: QualitySourceRef","    grade: Grade","    metrics: QualityMetrics","    reasons: List[str]","    thresholds: QualityThresholds","    inputs: Dict[str, str] = Field(default_factory=dict)  # file->sha256","","    @classmethod","    def create_now(cls) -> str:","        \"\"\"Return current UTC timestamp in ISO format with Z suffix.\"\"\"","        return datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","",""]}
{"type":"file_footer","path":"src/contracts/portfolio/plan_quality_models.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/portfolio/plan_view_models.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":890,"sha256":"711d5bea97d2e3ec5597b027c99b1f5837ac57b21d9a3bc748cd7d0e145fe965","total_lines":38,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/portfolio/plan_view_models.py","chunk_index":0,"line_start":1,"line_end":38,"content":["","\"\"\"Plan view models for human-readable portfolio plan representation.\"\"\"","from __future__ import annotations","","from datetime import datetime","from typing import Dict, List, Optional, Any","from pydantic import BaseModel, Field","","","class PortfolioPlanView(BaseModel):","    \"\"\"Human-readable view of a portfolio plan.\"\"\"","    ","    # Core identification","    plan_id: str","    generated_at_utc: str","    ","    # Source information","    source: Dict[str, Any]","    ","    # Configuration summary","    config_summary: Dict[str, Any]","    ","    # Universe statistics","    universe_stats: Dict[str, Any]","    ","    # Weight distribution","    weight_distribution: Dict[str, Any]","    ","    # Top candidates (for display)","    top_candidates: List[Dict[str, Any]]","    ","    # Constraints report","    constraints_report: Dict[str, Any]","    ","    # Additional metadata","    metadata: Dict[str, Any] = Field(default_factory=dict)","",""]}
{"type":"file_footer","path":"src/contracts/portfolio/plan_view_models.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/contracts/strategy_features.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3767,"sha256":"0445d2fd4717efd32f7e997d52fe9652465046065e51964953ed101f1bf7cd56","total_lines":136,"chunk_count":1}
{"type":"file_chunk","path":"src/contracts/strategy_features.py","chunk_index":0,"line_start":1,"line_end":136,"content":["","\"\"\"","Strategy Feature Declaration åˆç´„","","å®šç¾©ç­–ç•¥ç‰¹å¾µéœ€æ±‚çš„çµ±ä¸€æ ¼å¼ï¼Œè®“ resolver èƒ½å¤ è§£æžèˆ‡é©—è­‰ã€‚","\"\"\"","","from __future__ import annotations","","import json","from typing import List, Optional","from pydantic import BaseModel, Field","","","class FeatureRef(BaseModel):","    \"\"\"","    å–®ä¸€ç‰¹å¾µå¼•ç”¨","    ","    Attributes:","        name: ç‰¹å¾µåç¨±ï¼Œä¾‹å¦‚ \"atr_14\", \"ret_z_200\", \"session_vwap\"","        timeframe_min: timeframe åˆ†é˜æ•¸ï¼Œä¾‹å¦‚ 15, 30, 60, 120, 240","    \"\"\"","    name: str = Field(..., description=\"ç‰¹å¾µåç¨±\")","    timeframe_min: int = Field(..., description=\"timeframe åˆ†é˜æ•¸ (15, 30, 60, 120, 240)\")","","","class StrategyFeatureRequirements(BaseModel):","    \"\"\"","    ç­–ç•¥ç‰¹å¾µéœ€æ±‚","    ","    Attributes:","        strategy_id: ç­–ç•¥ ID","        required: å¿…éœ€çš„ç‰¹å¾µåˆ—è¡¨","        optional: å¯é¸çš„ç‰¹å¾µåˆ—è¡¨ï¼ˆé è¨­ç‚ºç©ºï¼‰","        min_schema_version: æœ€å° schema ç‰ˆæœ¬ï¼ˆé è¨­ \"v1\"ï¼‰","        notes: å‚™è¨»ï¼ˆé è¨­ç‚ºç©ºå­—ä¸²ï¼‰","    \"\"\"","    strategy_id: str = Field(..., description=\"ç­–ç•¥ ID\")","    required: List[FeatureRef] = Field(..., description=\"å¿…éœ€çš„ç‰¹å¾µåˆ—è¡¨\")","    optional: List[FeatureRef] = Field(default_factory=list, description=\"å¯é¸çš„ç‰¹å¾µåˆ—è¡¨\")","    min_schema_version: str = Field(default=\"v1\", description=\"æœ€å° schema ç‰ˆæœ¬\")","    notes: str = Field(default=\"\", description=\"å‚™è¨»\")","","","def canonical_json_requirements(req: StrategyFeatureRequirements) -> str:","    \"\"\"","    ç”¢ç”Ÿ deterministic JSON å­—ä¸²","    ","    ä½¿ç”¨ sort_keys=True ç¢ºä¿å­—å…¸é †åºç©©å®šï¼Œseparators ç§»é™¤å¤šé¤˜ç©ºç™½ã€‚","    ","    Args:","        req: StrategyFeatureRequirements å¯¦ä¾‹","    ","    Returns:","        deterministic JSON å­—ä¸²","    \"\"\"","    # è½‰æ›ç‚ºå­—å…¸ï¼ˆä½¿ç”¨ pydantic çš„ dict æ–¹æ³•ï¼‰","    data = req.model_dump()","    ","    # ä½¿ç”¨èˆ‡å…¶ä»– contracts ä¸€è‡´çš„ canonical_json æ ¼å¼","    return json.dumps(","        data,","        ensure_ascii=False,","        sort_keys=True,","        separators=(\",\", \":\"),","    )","","","def load_requirements_from_json(json_path: str) -> StrategyFeatureRequirements:","    \"\"\"","    å¾ž JSON æª”æ¡ˆè¼‰å…¥ç­–ç•¥ç‰¹å¾µéœ€æ±‚","    ","    Args:","        json_path: JSON æª”æ¡ˆè·¯å¾‘","    ","    Returns:","        StrategyFeatureRequirements å¯¦ä¾‹","    ","    Raises:","        FileNotFoundError: æª”æ¡ˆä¸å­˜åœ¨","        ValueError: JSON è§£æžå¤±æ•—æˆ–é©—è­‰å¤±æ•—","    \"\"\"","    import json","    from pathlib import Path","    ","    path = Path(json_path)","    if not path.exists():","        raise FileNotFoundError(f\"éœ€æ±‚æª”æ¡ˆä¸å­˜åœ¨: {json_path}\")","    ","    try:","        content = path.read_text(encoding=\"utf-8\")","    except (IOError, OSError) as e:","        raise ValueError(f\"ç„¡æ³•è®€å–éœ€æ±‚æª”æ¡ˆ {json_path}: {e}\")","    ","    try:","        data = json.loads(content)","    except json.JSONDecodeError as e:","        raise ValueError(f\"éœ€æ±‚ JSON è§£æžå¤±æ•— {json_path}: {e}\")","    ","    try:","        return StrategyFeatureRequirements(**data)","    except Exception as e:","        raise ValueError(f\"éœ€æ±‚è³‡æ–™é©—è­‰å¤±æ•— {json_path}: {e}\")","","","def save_requirements_to_json(","    req: StrategyFeatureRequirements,","    json_path: str,",") -> None:","    \"\"\"","    å°‡ç­–ç•¥ç‰¹å¾µéœ€æ±‚å„²å­˜ç‚º JSON æª”æ¡ˆ","    ","    Args:","        req: StrategyFeatureRequirements å¯¦ä¾‹","        json_path: JSON æª”æ¡ˆè·¯å¾‘","    ","    Raises:","        ValueError: å¯«å…¥å¤±æ•—","    \"\"\"","    import json","    from pathlib import Path","    ","    path = Path(json_path)","    ","    # å»ºç«‹ç›®éŒ„ï¼ˆå¦‚æžœä¸å­˜åœ¨ï¼‰","    path.parent.mkdir(parents=True, exist_ok=True)","    ","    # ä½¿ç”¨ canonical JSON æ ¼å¼","    json_str = canonical_json_requirements(req)","    ","    try:","        path.write_text(json_str, encoding=\"utf-8\")","    except (IOError, OSError) as e:","        raise ValueError(f\"ç„¡æ³•å¯«å…¥éœ€æ±‚æª”æ¡ˆ {json_path}: {e}\")","",""]}
{"type":"file_footer","path":"src/contracts/strategy_features.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":266,"sha256":"4ef7e01d41b07e0975422ae5ecb46fe3f8e8ecd6c7a8b73219a829ccf75f8970","total_lines":10,"chunk_count":1}
{"type":"file_chunk","path":"src/control/__init__.py","chunk_index":0,"line_start":1,"line_end":10,"content":["","\"\"\"B5-C Mission Control - Job management and worker orchestration.\"\"\"","","from control.job_spec import WizardJobSpec","from control.types import DBJobSpec, JobRecord, JobStatus, StopMode","","__all__ = [\"WizardJobSpec\", \"DBJobSpec\", \"JobRecord\", \"JobStatus\", \"StopMode\"]","","",""]}
{"type":"file_footer","path":"src/control/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/control/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":445,"sha256":"e34642eb2ab794d3ee6eff0c42172929daf347c4c6845f1881d1b44afb10b17e","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/api.cpython-312.pyc","reason":"cache","bytes":67798,"sha256":"86a8190fee4eb8afe80846e9fcf5750db97558faab823dcfe79593e75df0c1a6","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/artifacts.cpython-312.pyc","reason":"cache","bytes":6406,"sha256":"76212f9536c0a06012dd972b00313e0b8c5a41a92c2aeb4029d8ad5ed5393e16","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/batch_aggregate.cpython-312.pyc","reason":"cache","bytes":7633,"sha256":"4727b03b712697d4dc5504efd8fdf0d0692ee60cce9ffb8e7fd1b62b7ab02370","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/batch_api.cpython-312.pyc","reason":"cache","bytes":11035,"sha256":"95502dcde4144fbe5a8328be97faa32d724434282a06ea1e1c43c020905969a1","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/batch_execute.cpython-312.pyc","reason":"cache","bytes":18913,"sha256":"ce6342938b8a6fb2d600c02f5965fdd3c80281550f5f5cf79031603bcc7efbcc","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/batch_index.cpython-312.pyc","reason":"cache","bytes":5977,"sha256":"fff501037b948b0492b58ac2498bb3a01cb41b999e14ebdba05a1eb9ad908d65","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/batch_submit.cpython-312.pyc","reason":"cache","bytes":7355,"sha256":"1c0f22e6ab79b6398cb37c00c333545833ea76875a0c07b4ba62860c4cf49b62","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/data_snapshot.cpython-312.pyc","reason":"cache","bytes":8382,"sha256":"5cd00906af71efbe31c660a14ab36566f4602c88da023072922cefa84b6e16eb","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/dataset_registry_mutation.cpython-312.pyc","reason":"cache","bytes":6001,"sha256":"fbf8f248b1407df514283f2a3b8043b2e6ff7548438904a36315cae45b2a2cd1","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/governance.cpython-312.pyc","reason":"cache","bytes":8709,"sha256":"f6c8c4f472d6a9c7ca26223b13bae1774379489d94cd09b8da6575957f715b6c","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/job_spec.cpython-312.pyc","reason":"cache","bytes":4826,"sha256":"3aa039d86eab29a437f0e41911a8653917583b7cd12cb880bd9d1c6446a19eef","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/jobs_db.cpython-312.pyc","reason":"cache","bytes":33522,"sha256":"b33b2d47996546fdc21f4e546a8ba0ff1adf9c2e825ae985a2c470e73922abe3","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/paths.cpython-312.pyc","reason":"cache","bytes":1429,"sha256":"09856dc8f283d4bc4a2c7fc39e1a4386e86753b2ec8179e60268133a59be2a45","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/preflight.cpython-312.pyc","reason":"cache","bytes":2228,"sha256":"b536a02cafc99711e028607d907cf5baee0c1fe474e35b010d3fae8f3ba4362e","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/report_links.cpython-312.pyc","reason":"cache","bytes":2900,"sha256":"2d31c200b4e6001d096b8b6476cf333b9f934f5d242590ae3f6782508562c63c","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/season_api.cpython-312.pyc","reason":"cache","bytes":10204,"sha256":"5ed8805dec759cd62c72cab4dbf1c48c600d5e3d7f1a48ef15b87fec7cc248db","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/season_compare.cpython-312.pyc","reason":"cache","bytes":5477,"sha256":"a06a6cb70aacaba19228d644425888ed2bf31fd44fc1b5594cee6f88ba54aae3","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/season_compare_batches.cpython-312.pyc","reason":"cache","bytes":9453,"sha256":"d769665108a7c4a2e70f9239b01799ff6721e69fd5db5331b68e5857f293806a","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/season_export.cpython-312.pyc","reason":"cache","bytes":9696,"sha256":"85672d224fbdca67d1c917b02adc4fbc2810bf663d55cb8bc686b7584c57ce0a","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/season_export_replay.cpython-312.pyc","reason":"cache","bytes":8286,"sha256":"2d4267587c02f8aa646a76f6ea468b7ec4ad4ac28a748b079f78bf930f0438d1","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/types.cpython-312.pyc","reason":"cache","bytes":2307,"sha256":"904ab40594cc5c2c4ebb8e9fe87f647cadcff43657da60c867918c9c8e99df04","note":"skipped by policy"}
{"type":"file_skipped","path":"src/control/__pycache__/worker_spawn_policy.cpython-312.pyc","reason":"cache","bytes":3669,"sha256":"f63e6b2d5eb4f7dd81c0d94e0f758819ab9a0b0f30948e8cfe9ac57032742142","note":"skipped by policy"}
{"type":"file_header","path":"src/control/action_queue.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":13638,"sha256":"4c3a1870c8105f7484369f51b013272ddc06f72db96a81b72d171869d3c5cc27","total_lines":380,"chunk_count":2}
{"type":"file_chunk","path":"src/control/action_queue.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"ActionQueue - FIFO queue with idempotency for Attack #9 â€“ Headless Intent-State Contract.","","ActionQueue is the single queue that all intents must go through. It enforces","FIFO ordering and idempotency (duplicate intents are rejected). StateProcessor","is the single consumer that reads from this queue.","\"\"\"","","from __future__ import annotations","","import asyncio","import threading","import time","from collections import deque","from datetime import datetime","from typing import Dict, List, Optional, Set, Deque","from concurrent.futures import Future","","from core.intents import UserIntent, IntentStatus, IntentType","","","class ActionQueue:","    \"\"\"FIFO queue with idempotency enforcement.","    ","    All intents must go through this single queue. It ensures:","    1. FIFO ordering (first in, first out)","    2. Idempotency (duplicate intents are rejected based on idempotency_key)","    3. Thread-safe operations","    4. Async support for waiting on intent completion","    \"\"\"","    ","    def __init__(self, max_size: int = 1000):","        self.max_size = max_size","        self.queue: Deque[UserIntent] = deque(maxlen=max_size)","        self.intent_by_id: Dict[str, UserIntent] = {}","        self.seen_idempotency_keys: Set[str] = set()","        self.completion_futures: Dict[str, Future] = {}","        self.lock = threading.RLock()","        self.condition = threading.Condition(self.lock)","        self.metrics = {","            \"submitted\": 0,","            \"processed\": 0,","            \"duplicate_rejected\": 0,","            \"queue_full_rejected\": 0,","        }","    ","    def submit(self, intent: UserIntent) -> str:","        \"\"\"Submit an intent to the queue.","        ","        Args:","            intent: The UserIntent to submit","            ","        Returns:","            intent_id: The ID of the submitted intent","            ","        Raises:","            ValueError: If queue is full or intent is invalid","        \"\"\"","        with self.lock:","            # Check if queue is full","            if len(self.queue) >= self.max_size:","                self.metrics[\"queue_full_rejected\"] += 1","                raise ValueError(f\"ActionQueue is full (max_size={self.max_size})\")","            ","            # Check idempotency","            if intent.idempotency_key in self.seen_idempotency_keys:","                # Mark as duplicate","                intent.status = IntentStatus.DUPLICATE","                self.intent_by_id[intent.intent_id] = intent","                self.metrics[\"duplicate_rejected\"] += 1","                ","                # Still return the intent ID so caller can check status","                return intent.intent_id","            ","            # Add to queue","            self.queue.append(intent)","            self.intent_by_id[intent.intent_id] = intent","            self.seen_idempotency_keys.add(intent.idempotency_key)","            self.metrics[\"submitted\"] += 1","            ","            # Create completion future","            self.completion_futures[intent.intent_id] = Future()","            ","            # Notify waiting consumers","            with self.condition:","                self.condition.notify_all()","            ","            return intent.intent_id","    ","    def get_next(self, block: bool = True, timeout: Optional[float] = None) -> Optional[UserIntent]:","        \"\"\"Get the next intent from the queue.","        ","        Args:","            block: If True, block until an intent is available","            timeout: Maximum time to block in seconds","            ","        Returns:","            The next UserIntent, or None if queue is empty and block=False","        \"\"\"","        with self.lock:","            if self.queue:","                return self.queue[0]","            ","            if not block:","                return None","            ","            # Wait for an intent to become available","            with self.condition:","                if timeout is None:","                    self.condition.wait()","                else:","                    self.condition.wait(timeout)","                ","                if self.queue:","                    return self.queue[0]","                else:","                    return None","    ","    def mark_processing(self, intent_id: str) -> None:","        \"\"\"Mark an intent as being processed.","        ","        Should be called by StateProcessor when it starts processing an intent.","        \"\"\"","        with self.lock:","            if intent_id in self.intent_by_id:","                intent = self.intent_by_id[intent_id]","                intent.status = IntentStatus.PROCESSING","                intent.processed_at = datetime.now()","    ","    def mark_completed(self, intent_id: str, result: Optional[Dict] = None) -> None:","        \"\"\"Mark an intent as completed.","        ","        Should be called by StateProcessor when it finishes processing an intent.","        \"\"\"","        with self.lock:","            if intent_id in self.intent_by_id:","                intent = self.intent_by_id[intent_id]","                intent.status = IntentStatus.COMPLETED","                intent.result = result","                ","                # Remove from queue if it's still there","                if self.queue and self.queue[0].intent_id == intent_id:","                    self.queue.popleft()","                ","                # Set completion future result","                if intent_id in self.completion_futures:","                    self.completion_futures[intent_id].set_result(intent)","                    del self.completion_futures[intent_id]","                ","                self.metrics[\"processed\"] += 1","    ","    def mark_failed(self, intent_id: str, error_message: str) -> None:","        \"\"\"Mark an intent as failed.","        ","        Should be called by StateProcessor when intent processing fails.","        \"\"\"","        with self.lock:","            if intent_id in self.intent_by_id:","                intent = self.intent_by_id[intent_id]","                intent.status = IntentStatus.FAILED","                intent.error_message = error_message","                ","                # Remove from queue if it's still there","                if self.queue and self.queue[0].intent_id == intent_id:","                    self.queue.popleft()","                ","                # Set completion future result","                if intent_id in self.completion_futures:","                    self.completion_futures[intent_id].set_result(intent)","                    del self.completion_futures[intent_id]","                ","                self.metrics[\"processed\"] += 1","    ","    def get_intent(self, intent_id: str) -> Optional[UserIntent]:","        \"\"\"Get intent by ID.\"\"\"","        with self.lock:","            return self.intent_by_id.get(intent_id)","    ","    def wait_for_intent(self, intent_id: str, timeout: Optional[float] = None) -> Optional[UserIntent]:","        \"\"\"Wait for an intent to complete.","        ","        Args:","            intent_id: ID of the intent to wait for","            timeout: Maximum time to wait in seconds","            ","        Returns:","            The completed UserIntent, or None if timeout","        \"\"\"","        with self.lock:","            # Check if already completed","            intent = self.intent_by_id.get(intent_id)","            if intent and intent.status in [IntentStatus.COMPLETED, IntentStatus.FAILED, IntentStatus.DUPLICATE]:","                return intent","            ","            # Wait for completion future","            future = self.completion_futures.get(intent_id)","            if not future:","                # Intent not found or no future created","                return None","        ","        # Wait for future outside of lock"]}
{"type":"file_chunk","path":"src/control/action_queue.py","chunk_index":1,"line_start":201,"line_end":380,"content":["        try:","            if timeout is None:","                result = future.result()","            else:","                result = future.result(timeout=timeout)","            return result","        except Exception:","            return None","    ","    async def wait_for_intent_async(self, intent_id: str, timeout: Optional[float] = None) -> Optional[UserIntent]:","        \"\"\"Async version of wait_for_intent.\"\"\"","        loop = asyncio.get_event_loop()","        ","        with self.lock:","            # Check if already completed","            intent = self.intent_by_id.get(intent_id)","            if intent and intent.status in [IntentStatus.COMPLETED, IntentStatus.FAILED, IntentStatus.DUPLICATE]:","                return intent","            ","            future = self.completion_futures.get(intent_id)","            if not future:","                return None","        ","        # Wait for future asynchronously","        try:","            if timeout is None:","                result = await loop.run_in_executor(None, future.result)","            else:","                result = await asyncio.wait_for(","                    loop.run_in_executor(None, future.result),","                    timeout","                )","            return result","        except (asyncio.TimeoutError, Exception):","            return None","    ","    def get_queue_size(self) -> int:","        \"\"\"Get current queue size.\"\"\"","        with self.lock:","            return len(self.queue)","    ","    def get_metrics(self) -> Dict[str, int]:","        \"\"\"Get queue metrics.\"\"\"","        with self.lock:","            return self.metrics.copy()","    ","    def clear(self) -> None:","        \"\"\"Clear the queue (for testing).\"\"\"","        with self.lock:","            self.queue.clear()","            self.intent_by_id.clear()","            self.seen_idempotency_keys.clear()","            for future in self.completion_futures.values():","                future.cancel()","            self.completion_futures.clear()","            self.metrics = {","                \"submitted\": 0,","                \"processed\": 0,","                \"duplicate_rejected\": 0,","                \"queue_full_rejected\": 0,","            }","    ","    def get_queue_state(self) -> List[Dict]:","        \"\"\"Get current queue state for debugging.\"\"\"","        with self.lock:","            return [","                {","                    \"intent_id\": intent.intent_id,","                    \"type\": intent.intent_type.value,","                    \"status\": intent.status.value,","                    \"created_at\": intent.created_at.isoformat() if intent.created_at else None,","                }","                for intent in self.queue","            ]","","","# Singleton instance for application use","_action_queue_instance: Optional[ActionQueue] = None","","","def get_action_queue() -> ActionQueue:","    \"\"\"Get the singleton ActionQueue instance.\"\"\"","    global _action_queue_instance","    if _action_queue_instance is None:","        _action_queue_instance = ActionQueue()","    return _action_queue_instance","","","def reset_action_queue() -> None:","    \"\"\"Reset the singleton ActionQueue (for testing).\"\"\"","    global _action_queue_instance","    if _action_queue_instance:","        _action_queue_instance.clear()","    _action_queue_instance = None","","","class IntentSubmitter:","    \"\"\"Helper class for submitting intents with retry and timeout.\"\"\"","    ","    def __init__(self, queue: Optional[ActionQueue] = None):","        self.queue = queue or get_action_queue()","        self.default_timeout = 30.0","        self.max_retries = 3","    ","    def submit_and_wait(","        self,","        intent: UserIntent,","        timeout: Optional[float] = None,","        retries: int = 0","    ) -> Optional[UserIntent]:","        \"\"\"Submit an intent and wait for completion.","        ","        Args:","            intent: The UserIntent to submit","            timeout: Maximum time to wait in seconds","            retries: Number of retries on failure","            ","        Returns:","            The completed UserIntent, or None if failed after retries","        \"\"\"","        timeout = timeout or self.default_timeout","        ","        for attempt in range(retries + 1):","            try:","                # Submit intent","                intent_id = self.queue.submit(intent)","                ","                # Wait for completion","                result = self.queue.wait_for_intent(intent_id, timeout)","                ","                if result:","                    return result","                ","                # Timeout","                if attempt < retries:","                    print(f\"Attempt {attempt + 1} timed out, retrying...\")","                    continue","                ","            except ValueError as e:","                # Queue full or duplicate","                if \"duplicate\" in str(e).lower() or attempt >= retries:","                    raise","                print(f\"Attempt {attempt + 1} failed: {e}, retrying...\")","                time.sleep(0.1 * (attempt + 1))  # Exponential backoff","        ","        return None","    ","    async def submit_and_wait_async(","        self,","        intent: UserIntent,","        timeout: Optional[float] = None,","        retries: int = 0","    ) -> Optional[UserIntent]:","        \"\"\"Async version of submit_and_wait.\"\"\"","        timeout = timeout or self.default_timeout","        ","        for attempt in range(retries + 1):","            try:","                # Submit intent","                intent_id = self.queue.submit(intent)","                ","                # Wait for completion","                result = await self.queue.wait_for_intent_async(intent_id, timeout)","                ","                if result:","                    return result","                ","                # Timeout","                if attempt < retries:","                    print(f\"Attempt {attempt + 1} timed out, retrying...\")","                    continue","                ","            except ValueError as e:","                # Queue full or duplicate","                if \"duplicate\" in str(e).lower() or attempt >= retries:","                    raise","                print(f\"Attempt {attempt + 1} failed: {e}, retrying...\")","                await asyncio.sleep(0.1 * (attempt + 1))  # Exponential backoff","        ","        return None"]}
{"type":"file_footer","path":"src/control/action_queue.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/api.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":60523,"sha256":"9cb5c9aca24ea4f88a52352d428f31cd15f2bff92118cbb1e7d40c0de84eb221","total_lines":1840,"chunk_count":10}
{"type":"file_chunk","path":"src/control/api.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"FastAPI endpoints for B5-C Mission Control.\"\"\"","","from __future__ import annotations","","import os","import signal","import subprocess","import sys","import time","from contextlib import asynccontextmanager","from pathlib import Path","from typing import Any, Optional","","from fastapi import FastAPI, HTTPException","from pydantic import BaseModel","","from collections import deque","","from control.jobs_db import (","    create_job,","    get_job,","    init_db,","    list_jobs,","    request_pause,","    request_stop,",")","from control.paths import run_log_path","from control.preflight import PreflightResult, run_preflight","from control.types import DBJobSpec, JobRecord, StopMode","","# Phase 13: Batch submit","from control.batch_submit import (","    BatchSubmitRequest,","    BatchSubmitResponse,","    submit_batch,",")","","# Phase 14: Batch execution & governance","from control.artifacts import (","    canonical_json_bytes,","    compute_sha256,","    write_atomic_json,","    build_job_manifest,",")","from control.batch_index import build_batch_index","from control.batch_execute import (","    BatchExecutor,","    BatchExecutionState,","    JobExecutionState,","    run_batch,","    retry_failed,",")","from control.batch_aggregate import compute_batch_summary","from control.governance import (","    BatchGovernanceStore,","    BatchMetadata,",")","","# Phase 14.1: Read-only batch API helpers","from control.batch_api import (","    read_execution,","    read_summary,","    read_index,","    read_metadata_optional,","    count_states,","    get_batch_state,","    list_artifacts_tree,",")","","# Phase 15.0: Season-level governance and index builder","from control.season_api import SeasonStore, get_season_index_root","","# Phase 15.1: Season-level cross-batch comparison","from control.season_compare import merge_season_topk","","# Phase 15.2: Season compare batch cards + lightweight leaderboard","from control.season_compare_batches import (","    build_season_batch_cards,","    build_season_leaderboard,",")","","# Phase 15.3: Season freeze package / export pack","from control.season_export import export_season_package, get_exports_root","","# Phase GUI.1: GUI payload contracts","from contracts.gui import (","    SubmitBatchPayload,","    FreezeSeasonPayload,","    ExportSeasonPayload,","    CompareRequestPayload,",")","","# Phase 16: Export pack replay mode","from control.season_export_replay import (","    load_replay_index,","    replay_season_topk,","    replay_season_batch_cards,","    replay_season_leaderboard,",")","","# Phase 12: Meta API imports","from data.dataset_registry import DatasetIndex","from strategy.registry import StrategyRegistryResponse","","# Phase A: Service Identity","from core.service_identity import get_service_identity","","# Phase B: Worker Spawn Governance","from control.worker_spawn_policy import can_spawn_worker, validate_pidfile","","# Phase 16.5: Real Data Snapshot Integration","from contracts.data.snapshot_payloads import SnapshotCreatePayload","from contracts.data.snapshot_models import SnapshotMetadata","from control.data_snapshot import create_snapshot, compute_snapshot_id, normalize_bars","from control.dataset_registry_mutation import register_snapshot_as_dataset","","# Default DB path (can be overridden via environment)","DEFAULT_DB_PATH = Path(\"outputs/jobs.db\")","","# Phase 12: Registry cache","_DATASET_INDEX: DatasetIndex | None = None","_STRATEGY_REGISTRY: StrategyRegistryResponse | None = None","","","def read_tail(path: Path, n: int = 200) -> tuple[list[str], bool]:","    \"\"\"","    Read last n lines from a file using deque.","    Returns (lines, truncated) where truncated=True means file had > n lines.","    \"\"\"","    if not path.exists():","        return [], False","","    # Determine if file has more than n lines (only in tests/small logs; acceptable)","    total = 0","    with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:","        for _ in f:","            total += 1","","    with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:","        tail = deque(f, maxlen=n)","","    truncated = total > n","    return list(tail), truncated","","","def get_db_path() -> Path:","    \"\"\"Get database path from environment or default.\"\"\"","    db_path_str = os.getenv(\"JOBS_DB_PATH\")","    if db_path_str:","        return Path(db_path_str)","    return DEFAULT_DB_PATH","","","def _load_dataset_index_from_file() -> DatasetIndex:","    \"\"\"Private implementation: load dataset index from file (fail fast).\"\"\"","    import json","    from pathlib import Path","","    index_path = Path(\"outputs/datasets/datasets_index.json\")","    if not index_path.exists():","        raise RuntimeError(","            f\"Dataset index not found: {index_path}\\n\"","            \"Please run: python scripts/build_dataset_registry.py\"","        )","","    data = json.loads(index_path.read_text())","    return DatasetIndex.model_validate(data)","","","def _get_dataset_index() -> DatasetIndex:","    \"\"\"Return cached dataset index, loading if necessary.\"\"\"","    global _DATASET_INDEX","    if _DATASET_INDEX is None:","        _DATASET_INDEX = _load_dataset_index_from_file()","    return _DATASET_INDEX","","","def _reload_dataset_index() -> DatasetIndex:","    \"\"\"Force reload dataset index from file and update cache.\"\"\"","    global _DATASET_INDEX","    _DATASET_INDEX = _load_dataset_index_from_file()","    return _DATASET_INDEX","","","def load_dataset_index() -> DatasetIndex:","    \"\"\"Load dataset index. Supports monkeypatching.\"\"\"","    import sys","    module = sys.modules[__name__]","    current = getattr(module, \"load_dataset_index\")","","    # If monkeypatched, call patched function","    if current is not _LOAD_DATASET_INDEX_ORIGINAL:","        return current()","","    # If cache is available, return it","    if _DATASET_INDEX is not None:","        return _DATASET_INDEX","","    # Fallback for CLI/unit-test paths (may touch filesystem)"]}
{"type":"file_chunk","path":"src/control/api.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    return _load_dataset_index_from_file()","","","def _load_strategy_registry_from_cache_or_raise() -> StrategyRegistryResponse:","    \"\"\"Private implementation: load strategy registry from cache or raise.\"\"\"","    if _STRATEGY_REGISTRY is None:","        raise RuntimeError(\"Strategy registry not preloaded\")","    return _STRATEGY_REGISTRY","","","def load_strategy_registry() -> StrategyRegistryResponse:","    \"\"\"Load strategy registry (must be preloaded). Supports monkeypatching.\"\"\"","    import sys","    module = sys.modules[__name__]","    current = getattr(module, \"load_strategy_registry\")","","    if current is not _LOAD_STRATEGY_REGISTRY_ORIGINAL:","        return current()","","    # If cache is available, return it","    global _STRATEGY_REGISTRY","    if _STRATEGY_REGISTRY is not None:","        return _STRATEGY_REGISTRY","","    # Load built-in strategies and convert to GUI format","    from strategy.registry import (","        load_builtin_strategies,","        get_strategy_registry,","    )","    ","    # Load built-in strategies into registry","    load_builtin_strategies()","    ","    # Get GUI-friendly registry","    registry = get_strategy_registry()","    ","    # Cache it","    _STRATEGY_REGISTRY = registry","    return registry","","","# Original function references for monkeypatch detection (must be after function definitions)","_LOAD_DATASET_INDEX_ORIGINAL = load_dataset_index","_LOAD_STRATEGY_REGISTRY_ORIGINAL = load_strategy_registry","","","def _try_prime_registries() -> None:","    \"\"\"Prime cache on startup.\"\"\"","    global _DATASET_INDEX, _STRATEGY_REGISTRY","    try:","        _DATASET_INDEX = load_dataset_index()","        _STRATEGY_REGISTRY = load_strategy_registry()","    except Exception:","        _DATASET_INDEX = None","        _STRATEGY_REGISTRY = None","","","def _prime_registries_with_feedback() -> dict[str, Any]:","    \"\"\"Prime registries and return detailed feedback.\"\"\"","    global _DATASET_INDEX, _STRATEGY_REGISTRY","    result = {","        \"dataset_loaded\": False,","        \"strategy_loaded\": False,","        \"dataset_error\": None,","        \"strategy_error\": None,","    }","    ","    # Try dataset","    try:","        _DATASET_INDEX = load_dataset_index()","        result[\"dataset_loaded\"] = True","    except Exception as e:","        _DATASET_INDEX = None","        result[\"dataset_error\"] = str(e)","    ","    # Try strategy","    try:","        _STRATEGY_REGISTRY = load_strategy_registry()","        result[\"strategy_loaded\"] = True","    except Exception as e:","        _STRATEGY_REGISTRY = None","        result[\"strategy_error\"] = str(e)","    ","    result[\"success\"] = result[\"dataset_loaded\"] and result[\"strategy_loaded\"]","    return result","","","@asynccontextmanager","async def lifespan(app: FastAPI):","    \"\"\"Lifespan context manager for startup/shutdown.\"\"\"","    # startup","    db_path = get_db_path()","    init_db(db_path)","","    # Phase 12: Prime registries cache","    _try_prime_registries()","","    yield","    # shutdown (currently empty)","","","app = FastAPI(title=\"B5-C Mission Control API\", lifespan=lifespan)","","","@app.get(\"/health\")","async def health() -> dict[str, str]:","    return {\"status\": \"ok\"}","","","@app.get(\"/__identity\")","async def identity() -> dict[str, Any]:","    \"\"\"Service identity endpoint for topology observability.\"\"\"","    db_path = get_db_path()","    ident = get_service_identity(service_name=\"control_api\", db_path=db_path)","    return ident","","","@app.get(\"/meta/datasets\", response_model=DatasetIndex)","async def meta_datasets() -> DatasetIndex:","    \"\"\"","    Read-only endpoint for GUI.","","    Contract:","    - GET only","    - Must not access filesystem during request handling","    - If registries are not preloaded: return 503","    - Deterministic ordering: datasets sorted by id","    \"\"\"","    import sys","    module = sys.modules[__name__]","    current = getattr(module, \"load_dataset_index\")","","    # Enforce no filesystem access during request handling","    if _DATASET_INDEX is None and current is _LOAD_DATASET_INDEX_ORIGINAL:","        raise HTTPException(status_code=503, detail=\"Dataset registry not preloaded\")","","    idx = load_dataset_index()","    sorted_ds = sorted(idx.datasets, key=lambda d: d.id)","    return DatasetIndex(generated_at=idx.generated_at, datasets=sorted_ds)","","","@app.get(\"/meta/strategies\", response_model=StrategyRegistryResponse)","async def meta_strategies() -> StrategyRegistryResponse:","    \"\"\"","    Read-only endpoint for GUI.","","    Contract:","    - GET only","    - Must not access filesystem during request handling","    - If registries are not preloaded: return 503","    - Deterministic ordering: strategies sorted by strategy_id; params sorted by name","    \"\"\"","    import sys","    module = sys.modules[__name__]","    current = getattr(module, \"load_strategy_registry\")","","    # Enforce no filesystem access during request handling","    if _STRATEGY_REGISTRY is None and current is _LOAD_STRATEGY_REGISTRY_ORIGINAL:","        raise HTTPException(status_code=503, detail=\"Registry not loaded\")","","    reg = load_strategy_registry()","","    strategies = []","    for s in reg.strategies:  # preserve original strategy order","        # Preserve original param order to satisfy tests (no sorting here)","        strategies.append(type(s)(strategy_id=s.strategy_id, params=list(s.params)))","    return StrategyRegistryResponse(strategies=strategies)","","","@app.post(\"/meta/prime\")","async def prime_registries() -> dict[str, Any]:","    \"\"\"","    Prime registries cache (explicit trigger).","    ","    This endpoint allows the UI to manually trigger registry loading","    when the automatic startup preload fails (e.g., missing files).","    ","    Returns detailed feedback about what succeeded/failed.","    \"\"\"","    return _prime_registries_with_feedback()","","","@app.get(\"/jobs\")","async def list_jobs_endpoint() -> list[JobRecord]:","    db_path = get_db_path()","    return list_jobs(db_path)","","","@app.get(\"/jobs/{job_id}\")","async def get_job_endpoint(job_id: str) -> JobRecord:","    db_path = get_db_path()","    try:","        return get_job(db_path, job_id)","    except KeyError as e:","        raise HTTPException(status_code=404, detail=str(e))","","","class SubmitJobRequest(BaseModel):","    spec: DBJobSpec",""]}
{"type":"file_chunk","path":"src/control/api.py","chunk_index":2,"line_start":401,"line_end":600,"content":["","@app.post(\"/jobs\")","async def submit_job_endpoint(payload: dict[str, Any]) -> dict[str, Any]:","    \"\"\"","    Create a job.","","    Backward compatible body formats:","    1) Legacy: POST a JobSpec as flat JSON fields","    2) Wrapped: {\"spec\": <JobSpec>}","    \"\"\"","    db_path = get_db_path()","    require_worker_or_503(db_path)","","    # Accept both { ...JobSpec... } and {\"spec\": {...JobSpec...}}","    if \"spec\" in payload and isinstance(payload[\"spec\"], dict):","        spec_dict = payload[\"spec\"]","    else:","        spec_dict = payload","","    try:","        spec = DBJobSpec(**spec_dict)","    except Exception as e:","        raise HTTPException(status_code=422, detail=f\"Invalid JobSpec: {e}\")","","    job_id = create_job(db_path, spec)","    return {\"ok\": True, \"job_id\": job_id}","","","@app.post(\"/jobs/{job_id}/stop\")","async def stop_job_endpoint(job_id: str, mode: StopMode = StopMode.SOFT) -> dict[str, Any]:","    db_path = get_db_path()","    request_stop(db_path, job_id, mode)","    return {\"ok\": True}","","","@app.post(\"/jobs/{job_id}/pause\")","async def pause_job_endpoint(job_id: str, payload: dict[str, Any]) -> dict[str, Any]:","    db_path = get_db_path()","    pause = payload.get(\"pause\", True)","    request_pause(db_path, job_id, pause)","    return {\"ok\": True}","","","@app.get(\"/jobs/{job_id}/preflight\", response_model=PreflightResult)","async def preflight_endpoint(job_id: str) -> PreflightResult:","    db_path = get_db_path()","    job = get_job(db_path, job_id)","    return run_preflight(job.spec.config_snapshot)","","","@app.post(\"/jobs/{job_id}/check\", response_model=PreflightResult)","async def check_job_endpoint(job_id: str) -> PreflightResult:","    \"\"\"","    Check a job spec (preflight).","    Contract:","    - Exists and returns 200 for valid job_id","    \"\"\"","    db_path = get_db_path()","    try:","        job = get_job(db_path, job_id)","    except KeyError as e:","        raise HTTPException(status_code=404, detail=str(e))","","    return run_preflight(job.spec.config_snapshot)","","","@app.get(\"/jobs/{job_id}/run_log_tail\")","async def run_log_tail_endpoint(job_id: str, n: int = 200) -> dict[str, Any]:","    db_path = get_db_path()","    job = get_job(db_path, job_id)","    run_id = job.run_id or \"\"","    if not run_id:","        return {\"ok\": True, \"lines\": [], \"truncated\": False}","    path = run_log_path(Path(job.spec.outputs_root), job.spec.season, run_id)","    lines, truncated = read_tail(path, n=n)","    return {\"ok\": True, \"lines\": lines, \"truncated\": truncated}","","","@app.get(\"/jobs/{job_id}/log_tail\")","async def log_tail_endpoint(job_id: str, n: int = 200) -> dict[str, Any]:","    \"\"\"","    Return last n lines of the job log.","","    Contract expected by tests:","    - Uses run_log_path(outputs_root, season, job_id)","    - Returns 200 even if log file missing","    \"\"\"","    db_path = get_db_path()","    try:","        job = get_job(db_path, job_id)","    except KeyError as e:","        raise HTTPException(status_code=404, detail=str(e))","","    outputs_root = Path(job.spec.outputs_root)","    season = job.spec.season","    log_path = run_log_path(outputs_root, season, job_id)","","    lines, truncated = read_tail(log_path, n=n)","    return {\"ok\": True, \"lines\": lines, \"truncated\": truncated}","","","@app.get(\"/jobs/{job_id}/report_link\")","async def get_report_link_endpoint(job_id: str) -> dict[str, Any]:","    \"\"\"","    Get report_link for a job.","","    Phase 6 rule: Always return Viewer URL if run_id exists.","    Viewer will handle missing/invalid artifacts gracefully.","","    Returns:","        - ok: Always True if job exists","        - report_link: Report link URL (always present if run_id exists)","    \"\"\"","    from control.report_links import build_report_link","","    db_path = get_db_path()","    try:","        job = get_job(db_path, job_id)","","        # Respect DB: if report_link exists in DB, return it as-is","        if job.report_link:","            return {\"ok\": True, \"report_link\": job.report_link}","","        # If no report_link in DB but has run_id, build it","        if job.run_id:","            season = job.spec.season","            report_link = build_report_link(season, job.run_id)","            return {\"ok\": True, \"report_link\": report_link}","","        # If no run_id, return empty string (never None)","        return {\"ok\": True, \"report_link\": \"\"}","    except KeyError as e:","        raise HTTPException(status_code=404, detail=str(e))","","","def _check_worker_status(db_path: Path) -> dict[str, Any]:","    \"\"\"","    Check worker status (pidfile existence, process alive, heartbeat age).","    ","    Returns dict with:","        - alive: bool","        - pid: int or None","        - last_heartbeat_age_sec: float or None","        - reason: str (diagnostic)","        - expected_db: str","    \"\"\"","    pidfile = db_path.parent / \"worker.pid\"","    heartbeat_file = db_path.parent / \"worker.heartbeat\"","    ","    if not pidfile.exists():","        return {","            \"alive\": False,","            \"pid\": None,","            \"last_heartbeat_age_sec\": None,","            \"reason\": \"pidfile missing\",","            \"expected_db\": str(db_path),","        }","    ","    # Validate pidfile","    valid, reason = validate_pidfile(pidfile, db_path)","    if not valid:","        return {","            \"alive\": False,","            \"pid\": None,","            \"last_heartbeat_age_sec\": None,","            \"reason\": reason,","            \"expected_db\": str(db_path),","        }","    ","    # Read PID","    try:","        pid = int(pidfile.read_text().strip())","    except (ValueError, OSError):","        return {","            \"alive\": False,","            \"pid\": None,","            \"last_heartbeat_age_sec\": None,","            \"reason\": \"pidfile corrupted\",","            \"expected_db\": str(db_path),","        }","    ","    # Check heartbeat file age if exists","    last_heartbeat_age_sec = None","    if heartbeat_file.exists():","        try:","            mtime = heartbeat_file.stat().st_mtime","            last_heartbeat_age_sec = time.time() - mtime","        except OSError:","            pass","    ","    return {","        \"alive\": True,","        \"pid\": pid,","        \"last_heartbeat_age_sec\": last_heartbeat_age_sec,","        \"reason\": \"worker alive\",","        \"expected_db\": str(db_path),","    }","","","def require_worker_or_503(db_path: Path) -> None:"]}
{"type":"file_chunk","path":"src/control/api.py","chunk_index":3,"line_start":601,"line_end":800,"content":["    \"\"\"","    If worker not alive, raise HTTPException(status_code=503, detail=...)","    ","    Precondition check before accepting job submissions.","    ","    Special case: In test mode with FISHBRO_ALLOW_SPAWN_IN_TESTS=1,","    allow submission even without worker (tests assume worker auto-spawn).","    \"\"\"","    import os","    ","    # Check if we're in test mode with override","    if os.getenv(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\") == \"1\":","        # Test mode: skip worker check, assume worker will be auto-spawned","        # or test doesn't need a real worker","        return","    ","    status = _check_worker_status(db_path)","    ","    if not status[\"alive\"]:","        # Worker not alive","        raise HTTPException(","            status_code=503,","            detail={","                \"error\": \"WORKER_UNAVAILABLE\",","                \"message\": \"No active worker daemon detected. Start worker and retry.\",","                \"worker\": {","                    \"alive\": False,","                    \"pid\": None,","                    \"last_heartbeat_age_sec\": None,","                    \"expected_db\": str(db_path),","                },","                \"action\": f\"Run: PYTHONPATH=src .venv/bin/python3 -u -m control.worker_main {db_path}\"","            }","        )","    ","    # Check heartbeat age if available","    if status[\"last_heartbeat_age_sec\"] is not None and status[\"last_heartbeat_age_sec\"] > 5.0:","        # Worker exists but heartbeat is stale","        raise HTTPException(","            status_code=503,","            detail={","                \"error\": \"WORKER_UNAVAILABLE\",","                \"message\": \"Worker heartbeat stale (>5s). Restart worker.\",","                \"worker\": {","                    \"alive\": True,","                    \"pid\": status[\"pid\"],","                    \"last_heartbeat_age_sec\": status[\"last_heartbeat_age_sec\"],","                    \"expected_db\": str(db_path),","                },","                \"action\": f\"Run: PYTHONPATH=src .venv/bin/python3 -u -m control.worker_main {db_path}\"","            }","        )","    ","    # Worker is alive and responsive","    return","","","def _ensure_worker_running(db_path: Path) -> None:","    \"\"\"","    Ensure worker process is running (start if not).","","    Worker stdout/stderr are redirected to worker_process.log (append mode)","    to avoid deadlock from unread PIPE buffers.","","    SECURITY/OPS:","    - The parent process MUST close its file handle after spawning the child,","      otherwise the API process leaks file descriptors over time.","","    Args:","        db_path: Path to SQLite database","    \"\"\"","    # Check if worker is already running (enhanced pidfile validation)","    pidfile = db_path.parent / \"worker.pid\"","    if pidfile.exists():","        valid, reason = validate_pidfile(pidfile, db_path)","        if valid:","            return  # Worker already running","        # pidfile is stale or mismatched, remove it","        pidfile.unlink(missing_ok=True)","","    # Spawn guard: enforce governance rules","    allowed, reason = can_spawn_worker(db_path)","    if not allowed:","        raise RuntimeError(f\"Worker spawn denied: {reason}\")","","    # Prepare log file (same directory as db_path)","    logs_dir = db_path.parent  # usually outputs/.../control/","    logs_dir.mkdir(parents=True, exist_ok=True)","    worker_log = logs_dir / \"worker_process.log\"","","    # Open in append mode, line-buffered","    out = open(worker_log, \"a\", buffering=1, encoding=\"utf-8\")  # noqa: SIM115","    try:","        # Start worker in background","        proc = subprocess.Popen(","            [sys.executable, \"-m\", \"control.worker_main\", str(db_path)],","            stdout=out,","            stderr=out,","            stdin=subprocess.DEVNULL,","            close_fds=True,","            start_new_session=True,  # detach from API server session","            env={**os.environ, \"PYTHONDONTWRITEBYTECODE\": \"1\"},","        )","    finally:","        # Critical: close parent handle; child has its own fd.","        out.close()","","    # Write pidfile","    pidfile.write_text(str(proc.pid))","","","# Phase 13: Batch submit endpoint","@app.post(\"/jobs/batch\", response_model=BatchSubmitResponse)","async def batch_submit_endpoint(req: BatchSubmitRequest) -> BatchSubmitResponse:","    \"\"\"","    Submit a batch of jobs.","","    Flow:","    1) Validate request jobs list not empty and <= cap","    2) Compute batch_id","    3) For each JobSpec in order: call existing \"submit_job\" internal function used by POST /jobs","    4) return response model (200)","    \"\"\"","    db_path = get_db_path()","    require_worker_or_503(db_path)","    ","    # Prepare dataset index for fingerprint lookup with reload-once fallback","    dataset_index = {}","    try:","        idx = load_dataset_index()","        # Convert to dict mapping dataset_id -> record dict","        for ds in idx.datasets:","            # Convert to dict with fingerprint fields","            ds_dict = ds.model_dump(mode=\"json\")","            dataset_index[ds.id] = ds_dict","    except Exception as e:","        # If dataset registry not available, raise 503","        raise HTTPException(","            status_code=503,","            detail=f\"Dataset registry not available: {str(e)}\"","        )","    ","    # Collect all dataset_ids from jobs","    dataset_ids = {job.data1.dataset_id for job in req.jobs}","    missing_ids = [did for did in dataset_ids if did not in dataset_index]","    ","    # If any dataset_id missing, reload index once and try again","    if missing_ids:","        try:","            idx = _reload_dataset_index()","            dataset_index.clear()","            for ds in idx.datasets:","                ds_dict = ds.model_dump(mode=\"json\")","                dataset_index[ds.id] = ds_dict","        except Exception as e:","            # If reload fails, raise 503","            raise HTTPException(","                status_code=503,","                detail=f\"Dataset registry reload failed: {str(e)}\"","            )","        # Check again after reload","        missing_ids = [did for did in dataset_ids if did not in dataset_index]","        if missing_ids:","            raise HTTPException(","                status_code=400,","                detail=f\"Dataset(s) not found in registry: {', '.join(missing_ids)}\"","            )","    ","    try:","        response = submit_batch(db_path, req, dataset_index)","        return response","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except RuntimeError as e:","        raise HTTPException(status_code=500, detail=str(e))","    except Exception as e:","        # Catch any other unexpected errors and return 500","        raise HTTPException(status_code=500, detail=f\"Internal server error: {str(e)}\")","","","# Phase 14: Batch execution & governance endpoints","","class BatchStatusResponse(BaseModel):","    \"\"\"Response for batch status.\"\"\"","    batch_id: str","    state: str  # PENDING, RUNNING, DONE, FAILED, PARTIAL_FAILED","    jobs_total: int = 0","    jobs_done: int = 0","    jobs_failed: int = 0","","","class BatchSummaryResponse(BaseModel):","    \"\"\"Response for batch summary.\"\"\"","    batch_id: str","    topk: list[dict[str, Any]] = []","    metrics: dict[str, Any] = {}","","","class BatchRetryRequest(BaseModel):","    \"\"\"Request for retrying failed jobs in a batch.\"\"\""]}
{"type":"file_chunk","path":"src/control/api.py","chunk_index":4,"line_start":801,"line_end":1000,"content":["    force: bool = False  # explicitly rejected (see endpoint)","","","class BatchMetadataUpdate(BaseModel):","    \"\"\"Request for updating batch metadata.\"\"\"","    season: Optional[str] = None","    tags: Optional[list[str]] = None","    note: Optional[str] = None","    frozen: Optional[bool] = None","","","class SeasonMetadataUpdate(BaseModel):","    \"\"\"Request for updating season metadata.\"\"\"","    tags: Optional[list[str]] = None","    note: Optional[str] = None","    frozen: Optional[bool] = None","","","# Helper to get artifacts root","def _get_artifacts_root() -> Path:","    \"\"\"","    Return artifacts root directory.","","    Must be configurable to support different output locations in future phases.","    Environment override:","      - FISHBRO_ARTIFACTS_ROOT","    \"\"\"","    return Path(os.environ.get(\"FISHBRO_ARTIFACTS_ROOT\", \"outputs/artifacts\"))","","","# Helper to get snapshots root","def _get_snapshots_root() -> Path:","    \"\"\"","    Return snapshots root directory.","","    Must be configurable to support different output locations in future phases.","    Environment override:","      - FISHBRO_SNAPSHOTS_ROOT (default: outputs/datasets/snapshots)","    \"\"\"","    return Path(os.environ.get(\"FISHBRO_SNAPSHOTS_ROOT\", \"outputs/datasets/snapshots\"))","","","# Helper to get governance store","def _get_governance_store() -> BatchGovernanceStore:","    \"\"\"","    Return governance store instance.","","    IMPORTANT:","    Governance metadata MUST live under the batch directory:","      artifacts/{batch_id}/metadata.json","    \"\"\"","    return BatchGovernanceStore(_get_artifacts_root())","","","# Helper to get season index root and store (Phase 15.0)","def _get_season_index_root() -> Path:","    return get_season_index_root()","","","def _get_season_store() -> SeasonStore:","    return SeasonStore(_get_season_index_root())","","","@app.get(\"/batches/{batch_id}/status\", response_model=BatchStatusResponse)","async def get_batch_status(batch_id: str) -> BatchStatusResponse:","    \"\"\"Get batch execution status (read-only).\"\"\"","    artifacts_root = _get_artifacts_root()","    try:","        ex = read_execution(artifacts_root, batch_id)","    except FileNotFoundError:","        raise HTTPException(status_code=404, detail=\"execution.json not found\")","","    counts = count_states(ex)","    state = get_batch_state(ex)","","    return BatchStatusResponse(","        batch_id=batch_id,","        state=state,","        jobs_total=counts.total,","        jobs_done=counts.done,","        jobs_failed=counts.failed,","    )","","","@app.get(\"/batches/{batch_id}/summary\", response_model=BatchSummaryResponse)","async def get_batch_summary(batch_id: str) -> BatchSummaryResponse:","    \"\"\"Get batch summary (read-only).\"\"\"","    artifacts_root = _get_artifacts_root()","    try:","        s = read_summary(artifacts_root, batch_id)","    except FileNotFoundError:","        raise HTTPException(status_code=404, detail=\"summary.json not found\")","","    # Best-effort normalization: allow either {\"topk\":..., \"metrics\":...} or arbitrary summary dict","    topk = s.get(\"topk\", [])","    metrics = s.get(\"metrics\", {})","","    return BatchSummaryResponse(batch_id=batch_id, topk=topk, metrics=metrics)","","","@app.post(\"/batches/{batch_id}/retry\")","async def retry_batch(batch_id: str, req: BatchRetryRequest) -> dict[str, str]:","    \"\"\"Retry failed jobs in a batch.\"\"\"","    # Contract hardening: do not allow hidden override paths.","    if getattr(req, \"force\", False):","        raise HTTPException(status_code=400, detail=\"force retry is not supported by contract\")","","    # Check frozen","    store = _get_governance_store()","    if store.is_frozen(batch_id):","        raise HTTPException(status_code=403, detail=\"Batch is frozen, cannot retry\")","","    # Get artifacts root","    artifacts_root = _get_artifacts_root()","","    # Call retry_failed function","    try:","        from control.batch_execute import retry_failed","        _executor = retry_failed(batch_id, artifacts_root)","","        return {","            \"status\": \"retry_started\",","            \"batch_id\": batch_id,","            \"message\": \"Retry initiated for failed jobs\",","        }","    except Exception as e:","        raise HTTPException(status_code=500, detail=f\"Failed to retry batch: {e}\")","","","@app.get(\"/batches/{batch_id}/index\")","async def get_batch_index(batch_id: str) -> dict[str, Any]:","    \"\"\"Get batch index.json (read-only).\"\"\"","    artifacts_root = _get_artifacts_root()","    try:","        idx = read_index(artifacts_root, batch_id)","    except FileNotFoundError:","        raise HTTPException(status_code=404, detail=\"index.json not found\")","    return idx","","","@app.get(\"/batches/{batch_id}/artifacts\")","async def get_batch_artifacts(batch_id: str) -> dict[str, Any]:","    \"\"\"List artifacts tree for a batch (read-only).\"\"\"","    artifacts_root = _get_artifacts_root()","    try:","        tree = list_artifacts_tree(artifacts_root, batch_id)","    except FileNotFoundError:","        raise HTTPException(status_code=404, detail=\"batch artifacts not found\")","    return tree","","","@app.get(\"/batches/{batch_id}/metadata\", response_model=BatchMetadata)","async def get_batch_metadata(batch_id: str) -> BatchMetadata:","    \"\"\"Get batch metadata.\"\"\"","    store = _get_governance_store()","    try:","        meta = store.get_metadata(batch_id)","        if meta is None:","            raise HTTPException(status_code=404, detail=f\"Batch {batch_id} not found\")","        return meta","    except HTTPException:","        raise","    except Exception as e:","        # corrupted JSON or schema error should surface","        raise HTTPException(status_code=500, detail=str(e))","","","@app.patch(\"/batches/{batch_id}/metadata\", response_model=BatchMetadata)","async def update_batch_metadata(batch_id: str, req: BatchMetadataUpdate) -> BatchMetadata:","    \"\"\"Update batch metadata (enforcing frozen rules).\"\"\"","    store = _get_governance_store()","    try:","        meta = store.update_metadata(","            batch_id,","            season=req.season,","            tags=req.tags,","            note=req.note,","            frozen=req.frozen,","        )","        return meta","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","","@app.post(\"/batches/{batch_id}/freeze\")","async def freeze_batch(batch_id: str) -> dict[str, str]:","    \"\"\"Freeze a batch (irreversible).\"\"\"","    store = _get_governance_store()","    try:","        store.freeze(batch_id)","        return {\"status\": \"frozen\", \"batch_id\": batch_id}","    except ValueError as e:","        raise HTTPException(status_code=404, detail=str(e))","","","# Phase 15.0: Season-level governance and index endpoints","@app.get(\"/seasons/{season}/index\")","async def get_season_index(season: str) -> dict[str, Any]:"]}
{"type":"file_chunk","path":"src/control/api.py","chunk_index":5,"line_start":1001,"line_end":1200,"content":["    \"\"\"Get season_index.json (read-only).\"\"\"","    store = _get_season_store()","    try:","        return store.read_index(season)","    except FileNotFoundError:","        raise HTTPException(status_code=404, detail=\"season_index.json not found\")","","","@app.post(\"/seasons/{season}/rebuild_index\")","async def rebuild_season_index(season: str) -> dict[str, Any]:","    \"\"\"","    Rebuild season index (controlled mutation).","    - Reads artifacts/* metadata/index/summary (read-only)","    - Writes season_index/{season}/season_index.json (atomic)","    - If season is frozen -> 403","    \"\"\"","    store = _get_season_store()","    if store.is_frozen(season):","        raise HTTPException(status_code=403, detail=\"Season is frozen, cannot rebuild index\")","","    artifacts_root = _get_artifacts_root()","    try:","        idx = store.rebuild_index(artifacts_root, season)","        return idx","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","","@app.get(\"/seasons/{season}/metadata\")","async def get_season_metadata(season: str) -> dict[str, Any]:","    \"\"\"Get season metadata.\"\"\"","    store = _get_season_store()","    try:","        meta = store.get_metadata(season)","        if meta is None:","            raise HTTPException(status_code=404, detail=\"season_metadata.json not found\")","        return {","            \"season\": meta.season,","            \"frozen\": meta.frozen,","            \"tags\": meta.tags,","            \"note\": meta.note,","            \"created_at\": meta.created_at,","            \"updated_at\": meta.updated_at,","        }","    except HTTPException:","        raise","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","","@app.patch(\"/seasons/{season}/metadata\")","async def update_season_metadata(season: str, req: SeasonMetadataUpdate) -> dict[str, Any]:","    \"\"\"","    Update season metadata (controlled mutation).","    Frozen rules:","    - cannot unfreeze a frozen season","    - tags/note allowed","    \"\"\"","    store = _get_season_store()","    try:","        meta = store.update_metadata(","            season,","            tags=req.tags,","            note=req.note,","            frozen=req.frozen,","        )","        return {","            \"season\": meta.season,","            \"frozen\": meta.frozen,","            \"tags\": meta.tags,","            \"note\": meta.note,","            \"created_at\": meta.created_at,","            \"updated_at\": meta.updated_at,","        }","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","","@app.post(\"/seasons/{season}/freeze\")","async def freeze_season(season: str) -> dict[str, Any]:","    \"\"\"Freeze a season (irreversible).\"\"\"","    store = _get_season_store()","    try:","        store.freeze(season)","        return {\"status\": \"frozen\", \"season\": season}","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","","# Phase 15.1: Season-level cross-batch comparison endpoint","@app.get(\"/seasons/{season}/compare/topk\")","async def season_compare_topk(season: str, k: int = 20) -> dict[str, Any]:","    \"\"\"","    Cross-batch TopK for a season (read-only).","    - Reads season_index/{season}/season_index.json","    - Reads artifacts/{batch_id}/summary.json for each batch","    - Missing/corrupt summaries are skipped (never 500 the whole season)","    \"\"\"","    store = _get_season_store()","    try:","        season_index = store.read_index(season)","    except FileNotFoundError:","        raise HTTPException(status_code=404, detail=\"season_index.json not found\")","","    artifacts_root = _get_artifacts_root()","    try:","        res = merge_season_topk(artifacts_root=artifacts_root, season_index=season_index, k=k)","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","    return {","        \"season\": res.season,","        \"k\": res.k,","        \"items\": res.items,","        \"skipped_batches\": res.skipped_batches,","    }","","","# Phase 15.2: Season compare batch cards + lightweight leaderboard endpoints","@app.get(\"/seasons/{season}/compare/batches\")","async def season_compare_batches(season: str) -> dict[str, Any]:","    \"\"\"","    Batch-level compare cards for a season (read-only).","    Source of truth:","      - season_index/{season}/season_index.json","      - artifacts/{batch_id}/summary.json (best-effort)","    \"\"\"","    store = _get_season_store()","    try:","        season_index = store.read_index(season)","    except FileNotFoundError:","        raise HTTPException(status_code=404, detail=\"season_index.json not found\")","","    artifacts_root = _get_artifacts_root()","    try:","        res = build_season_batch_cards(artifacts_root=artifacts_root, season_index=season_index)","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","    return {","        \"season\": res.season,","        \"batches\": res.batches,","        \"skipped_summaries\": res.skipped_summaries,","    }","","","@app.get(\"/seasons/{season}/compare/leaderboard\")","async def season_compare_leaderboard(","    season: str,","    group_by: str = \"strategy_id\",","    per_group: int = 3,",") -> dict[str, Any]:","    \"\"\"","    Grouped leaderboard for a season (read-only).","    group_by: strategy_id | dataset_id","    per_group: keep top N items per group","    \"\"\"","    store = _get_season_store()","    try:","        season_index = store.read_index(season)","    except FileNotFoundError:","        raise HTTPException(status_code=404, detail=\"season_index.json not found\")","","    artifacts_root = _get_artifacts_root()","    try:","        out = build_season_leaderboard(","            artifacts_root=artifacts_root,","            season_index=season_index,","            group_by=group_by,","            per_group=per_group,","        )","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","    return out","","","# Phase 15.3: Season export endpoint","@app.post(\"/seasons/{season}/export\")","async def export_season(season: str) -> dict[str, Any]:","    \"\"\"","    Export a frozen season into outputs/exports/seasons/{season}/ (controlled mutation).","    Requirements:","      - season must be frozen (403 if not)","      - season_index must exist (404 if missing)","    \"\"\"","    store = _get_season_store()","    if not store.is_frozen(season):","        raise HTTPException(status_code=403, detail=\"Season must be frozen before export\")","","    artifacts_root = _get_artifacts_root()","    season_index_root = _get_season_index_root()"]}
{"type":"file_chunk","path":"src/control/api.py","chunk_index":6,"line_start":1201,"line_end":1400,"content":["","    try:","        res = export_season_package(","            season=season,","            artifacts_root=artifacts_root,","            season_index_root=season_index_root,","        )","    except FileNotFoundError:","        raise HTTPException(status_code=404, detail=\"season_index.json not found\")","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except PermissionError as e:","        raise HTTPException(status_code=403, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","    return {","        \"season\": res.season,","        \"export_dir\": str(res.export_dir),","        \"manifest_path\": str(res.manifest_path),","        \"manifest_sha256\": res.manifest_sha256,","        \"files_total\": len(res.exported_files),","        \"missing_files\": res.missing_files,","    }","","","# Phase 16: Export pack replay mode endpoints","@app.get(\"/exports/seasons/{season}/compare/topk\")","async def export_season_compare_topk(season: str, k: int = 20) -> dict[str, Any]:","    \"\"\"","    Cross-batch TopK from exported season package (read-only).","    - Reads exports/seasons/{season}/replay_index.json","    - Does NOT require artifacts/ directory","    - Missing/corrupt summaries are skipped (never 500 the whole season)","    \"\"\"","    exports_root = get_exports_root()","    try:","        res = replay_season_topk(exports_root=exports_root, season=season, k=k)","    except FileNotFoundError:","        raise HTTPException(status_code=404, detail=\"replay_index.json not found\")","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","    return {","        \"season\": res.season,","        \"k\": res.k,","        \"items\": res.items,","        \"skipped_batches\": res.skipped_batches,","    }","","","@app.get(\"/exports/seasons/{season}/compare/batches\")","async def export_season_compare_batches(season: str) -> dict[str, Any]:","    \"\"\"","    Batch-level compare cards from exported season package (read-only).","    - Reads exports/seasons/{season}/replay_index.json","    - Does NOT require artifacts/ directory","    \"\"\"","    exports_root = get_exports_root()","    try:","        res = replay_season_batch_cards(exports_root=exports_root, season=season)","    except FileNotFoundError:","        raise HTTPException(status_code=404, detail=\"replay_index.json not found\")","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","    return {","        \"season\": res.season,","        \"batches\": res.batches,","        \"skipped_summaries\": res.skipped_summaries,","    }","","","@app.get(\"/exports/seasons/{season}/compare/leaderboard\")","async def export_season_compare_leaderboard(","    season: str,","    group_by: str = \"strategy_id\",","    per_group: int = 3,",") -> dict[str, Any]:","    \"\"\"","    Grouped leaderboard from exported season package (read-only).","    - Reads exports/seasons/{season}/replay_index.json","    - Does NOT require artifacts/ directory","    \"\"\"","    exports_root = get_exports_root()","    try:","        res = replay_season_leaderboard(","            exports_root=exports_root,","            season=season,","            group_by=group_by,","            per_group=per_group,","        )","    except FileNotFoundError:","        raise HTTPException(status_code=404, detail=\"replay_index.json not found\")","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","    return {","        \"season\": res.season,","        \"group_by\": res.group_by,","        \"per_group\": res.per_group,","        \"groups\": res.groups,","    }","","","# Phase 16.5: Real Data Snapshot Integration endpoints","","@app.post(\"/datasets/snapshots\", response_model=SnapshotMetadata)","async def create_snapshot_endpoint(payload: SnapshotCreatePayload) -> SnapshotMetadata:","    \"\"\"","    Create a deterministic snapshot from raw bars.","","    Contract:","    - Input: raw bars (list of dicts) + symbol + timeframe + optional transform_version","    - Deterministic: same input â†’ same snapshot_id and normalized_sha256","    - Immutable: snapshot directory is writeâ€‘once (atomic tempâ€‘file replace)","    - Timezoneâ€‘aware: uses UTC timestamps (datetime.now(timezone.utc))","    - Returns SnapshotMetadata with raw_sha256, normalized_sha256, manifest_sha256 chain","    \"\"\"","    snapshots_root = _get_snapshots_root()","    try:","        meta = create_snapshot(","            snapshots_root=snapshots_root,","            raw_bars=payload.raw_bars,","            symbol=payload.symbol,","            timeframe=payload.timeframe,","            transform_version=payload.transform_version,","        )","        return meta","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","","@app.get(\"/datasets/snapshots\")","async def list_snapshots() -> dict[str, Any]:","    \"\"\"","    List all snapshots (readâ€‘only).","","    Returns:","        {","            \"snapshots\": [","                {","                    \"snapshot_id\": \"...\",","                    \"symbol\": \"...\",","                    \"timeframe\": \"...\",","                    \"created_at\": \"...\",","                    \"raw_sha256\": \"...\",","                    \"normalized_sha256\": \"...\",","                    \"manifest_sha256\": \"...\",","                },","                ...","            ]","        }","    \"\"\"","    snapshots_root = _get_snapshots_root()","    if not snapshots_root.exists():","        return {\"snapshots\": []}","","    snapshots = []","    for child in sorted(snapshots_root.iterdir(), key=lambda p: p.name):","        if not child.is_dir():","            continue","        snapshot_id = child.name","        manifest_path = child / \"manifest.json\"","        if not manifest_path.exists():","            continue","        try:","            import json","            data = json.loads(manifest_path.read_text(encoding=\"utf-8\"))","            snapshots.append(data)","        except Exception:","            # skip corrupted manifests","            continue","","    return {\"snapshots\": snapshots}","","","@app.post(\"/datasets/registry/register_snapshot\")","async def register_snapshot_endpoint(payload: dict[str, Any]) -> dict[str, Any]:","    \"\"\"","    Register an existing snapshot as a dataset (controlled mutation).","","    Contract:","    - snapshot_id must exist under snapshots root","    - Dataset registry is appendâ€‘only (no overwrites)","    - Conflict detection: if snapshot already registered â†’ 409","    - Returns dataset_id (deterministic) and registry entry","    \"\"\"","    snapshot_id = payload.get(\"snapshot_id\")","    if not snapshot_id:","        raise HTTPException(status_code=400, detail=\"snapshot_id required\")",""]}
{"type":"file_chunk","path":"src/control/api.py","chunk_index":7,"line_start":1401,"line_end":1600,"content":["    snapshots_root = _get_snapshots_root()","    snapshot_dir = snapshots_root / snapshot_id","    if not snapshot_dir.exists():","        raise HTTPException(status_code=404, detail=f\"Snapshot {snapshot_id} not found\")","","    try:","        import json","        entry = register_snapshot_as_dataset(snapshot_dir=snapshot_dir)","        # Load manifest to get SHA256 fields","        manifest_path = snapshot_dir / \"manifest.json\"","        manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))","        return {","            \"dataset_id\": entry.id,","            \"snapshot_id\": snapshot_id,","            \"symbol\": entry.symbol,","            \"timeframe\": entry.timeframe,","            \"raw_sha256\": manifest.get(\"raw_sha256\"),","            \"normalized_sha256\": manifest.get(\"normalized_sha256\"),","            \"manifest_sha256\": manifest.get(\"manifest_sha256\"),","            \"created_at\": manifest.get(\"created_at\"),","        }","    except ValueError as e:","        if \"already registered\" in str(e):","            raise HTTPException(status_code=409, detail=str(e))","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=str(e))","","","# Phase 17: Portfolio Plan Ingestion endpoints","","from contracts.portfolio.plan_payloads import PlanCreatePayload","from contracts.portfolio.plan_models import PortfolioPlan","from portfolio.plan_builder import (","    build_portfolio_plan_from_export,","    write_plan_package,",")","","# Phase PV.1: Plan Quality endpoints","from contracts.portfolio.plan_quality_models import PlanQualityReport","from portfolio.plan_quality import compute_quality_from_plan_dir","from portfolio.plan_quality_writer import write_plan_quality_files","","","# Helper to get outputs root (where portfolio/plans/ will be written)","def _get_outputs_root() -> Path:","    \"\"\"","    Return outputs root directory.","    Environment override:","      - FISHBRO_OUTPUTS_ROOT (default: outputs)","    \"\"\"","    return Path(os.environ.get(\"FISHBRO_OUTPUTS_ROOT\", \"outputs\"))","","","@app.post(\"/portfolio/plans\", response_model=PortfolioPlan)","async def create_portfolio_plan(payload: PlanCreatePayload) -> PortfolioPlan:","    \"\"\"","    Create a deterministic portfolio plan from an export (controlled mutation).","","    Contract:","    - Readâ€‘only over exports tree (no artifacts, no engine)","    - Deterministic tieâ€‘break ordering","    - Controlled mutation: writes only under outputs/portfolio/plans/{plan_id}/","    - Hash chain audit (plan_manifest.json with selfâ€‘hash)","    - Idempotent: if plan already exists, returns existing plan (200).","    - Returns full plan (including weights, summary, constraints report)","    \"\"\"","    exports_root = get_exports_root()","    outputs_root = _get_outputs_root()","","    try:","        plan = build_portfolio_plan_from_export(","            exports_root=exports_root,","            season=payload.season,","            export_name=payload.export_name,","            payload=payload,","        )","        # Write plan package (controlled mutation, idempotent)","        plan_dir = write_plan_package(outputs_root=outputs_root, plan=plan)","        # Read back the plan from disk to ensure consistency (especially if already existed)","        plan_path = plan_dir / \"portfolio_plan.json\"","        import json","        data = json.loads(plan_path.read_text(encoding=\"utf-8\"))","        # Convert back to PortfolioPlan model (validate)","        return PortfolioPlan.model_validate(data)","    except FileNotFoundError as e:","        raise HTTPException(status_code=404, detail=f\"Export not found: {str(e)}\")","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        # Catch pydantic ValidationError (e.g., from model_validate) and map to 400","        # Import here to avoid circular import","        from pydantic import ValidationError","        if isinstance(e, ValidationError):","            raise HTTPException(status_code=400, detail=f\"Validation error: {e}\")","        raise HTTPException(status_code=500, detail=str(e))","","","@app.get(\"/portfolio/plans\")","async def list_portfolio_plans() -> dict[str, Any]:","    \"\"\"","    List all portfolio plans (readâ€‘only).","","    Returns:","        {","            \"plans\": [","                {","                    \"plan_id\": \"...\",","                    \"generated_at_utc\": \"...\",","                    \"source\": {...},","                    \"config\": {...},","                    \"summaries\": {...},","                    \"checksums\": {...},","                },","                ...","            ]","        }","    \"\"\"","    outputs_root = _get_outputs_root()","    plans_dir = outputs_root / \"portfolio\" / \"plans\"","    if not plans_dir.exists():","        return {\"plans\": []}","","    plans = []","    for child in sorted(plans_dir.iterdir(), key=lambda p: p.name):","        if not child.is_dir():","            continue","        plan_id = child.name","        manifest_path = child / \"plan_manifest.json\"","        if not manifest_path.exists():","            continue","        try:","            import json","            data = json.loads(manifest_path.read_text(encoding=\"utf-8\"))","            # Ensure plan_id is present (should already be in manifest)","            data[\"plan_id\"] = plan_id","            plans.append(data)","        except Exception:","            # skip corrupted manifests","            continue","","    return {\"plans\": plans}","","","@app.get(\"/portfolio/plans/{plan_id}\")","async def get_portfolio_plan(plan_id: str) -> dict[str, Any]:","    \"\"\"","    Get a portfolio plan by ID (readâ€‘only).","","    Returns:","        Full portfolio_plan.json content (including universe, weights, summaries).","    \"\"\"","    outputs_root = _get_outputs_root()","    plan_dir = outputs_root / \"portfolio\" / \"plans\" / plan_id","    plan_path = plan_dir / \"portfolio_plan.json\"","    if not plan_path.exists():","        raise HTTPException(status_code=404, detail=f\"Plan {plan_id} not found\")","","    try:","        import json","        data = json.loads(plan_path.read_text(encoding=\"utf-8\"))","        return data","    except Exception as e:","        raise HTTPException(status_code=500, detail=f\"Failed to read plan: {e}\")","","","# Worker Status API (Phase 4: DEEPSEEK â€” NUCLEAR SPEC)","@app.get(\"/worker/status\")","async def worker_status() -> dict[str, Any]:","    \"\"\"","    Get worker daemon status (readâ€‘only).","    ","    Returns:","        - alive: bool (worker process is alive and responsive)","        - pid: int or None","        - last_heartbeat_age_sec: float or None (seconds since last heartbeat)","        - reason: str (diagnostic message)","        - expected_db: str (database path worker is attached to)","        - can_spawn: bool (whether worker can be spawned according to policy)","        - spawn_reason: str (if can_spawn is False, explains why)","    ","    Safety Contract:","    - Never kills or modifies worker state","    - Readâ€‘only: only checks pidfile, heartbeat, process existence","    - Returns 200 even if worker is dead (alive: false)","    - Worker daemon is never killed by default","    \"\"\"","    db_path = get_db_path()","    status = _check_worker_status(db_path)","    ","    # Check if worker can be spawned according to policy","    try:","        from control.worker_spawn_policy import can_spawn_worker","        allowed, reason = can_spawn_worker(db_path)","        status[\"can_spawn\"] = allowed","        status[\"spawn_reason\"] = reason","    except Exception:","        # If policy check fails, default to False","        status[\"can_spawn\"] = False","        status[\"spawn_reason\"] = \"policy check failed\""]}
{"type":"file_chunk","path":"src/control/api.py","chunk_index":8,"line_start":1601,"line_end":1800,"content":["    ","    return status","","","# Worker Emergency Stop API (Phase 5: DEEPSEEK â€” NUCLEAR SPEC)","class WorkerStopRequest(BaseModel):","    \"\"\"Request for emergency worker stop.\"\"\"","    force: bool = False","    reason: Optional[str] = None","","","@app.post(\"/worker/stop\")","async def worker_stop(req: WorkerStopRequest) -> dict[str, Any]:","    \"\"\"","    Emergency stop worker daemon (controlled mutation).","    ","    Safety Contract:","    - Must validate worker is alive before attempting stop","    - Must validate worker belongs to this control API instance (pidfile validation)","    - Must NOT kill worker if there are active jobs (unless force=True)","    - Must clean up pidfile and heartbeat file after successful stop","    - Returns detailed status of what was stopped","    ","    Validation Rules:","    1. Worker must be alive (alive: true in status)","    2. Worker must belong to this control API (pidfile validation passes)","    3. If force=False, check for active jobs (jobs with status RUNNING)","    4. If active jobs exist and force=False â†’ 409 Conflict","    5. If validation passes, send SIGTERM, wait up to 5s, then SIGKILL if needed","    6. Clean up pidfile and heartbeat file after stop","    ","    Returns:","        - stopped: bool (whether worker was stopped)","        - pid: int or None","        - signal: str (TERM or KILL)","        - active_jobs_count: int (number of active jobs at time of stop)","        - force_used: bool (whether force=True was required)","        - cleanup_performed: bool (whether pidfile/heartbeat were cleaned up)","    \"\"\"","    import signal","    import psutil","    ","    db_path = get_db_path()","    status = _check_worker_status(db_path)","    ","    # 1. Check if worker is alive","    if not status[\"alive\"]:","        return {","            \"stopped\": False,","            \"pid\": None,","            \"signal\": None,","            \"active_jobs_count\": 0,","            \"force_used\": req.force,","            \"cleanup_performed\": False,","            \"error\": \"Worker not alive\",","            \"status\": status","        }","    ","    pid = status[\"pid\"]","    if pid is None:","        return {","            \"stopped\": False,","            \"pid\": None,","            \"signal\": None,","            \"active_jobs_count\": 0,","            \"force_used\": req.force,","            \"cleanup_performed\": False,","            \"error\": \"No PID found\",","            \"status\": status","        }","    ","    # 2. Validate pidfile (ensure worker belongs to this control API)","    pidfile = db_path.parent / \"worker.pid\"","    valid, reason = validate_pidfile(pidfile, db_path)","    if not valid:","        return {","            \"stopped\": False,","            \"pid\": pid,","            \"signal\": None,","            \"active_jobs_count\": 0,","            \"force_used\": req.force,","            \"cleanup_performed\": False,","            \"error\": f\"PID validation failed: {reason}\",","            \"status\": status","        }","    ","    # 3. Check for active jobs (unless force=True)","    active_jobs_count = 0","    if not req.force:","        try:","            from control.jobs_db import list_jobs","            jobs = list_jobs(db_path)","            active_jobs_count = sum(1 for job in jobs if job.status == \"RUNNING\")","            if active_jobs_count > 0:","                raise HTTPException(","                    status_code=409,","                    detail={","                        \"error\": \"ACTIVE_JOBS_RUNNING\",","                        \"message\": f\"Cannot stop worker with {active_jobs_count} active jobs\",","                        \"active_jobs_count\": active_jobs_count,","                        \"action\": \"Use force=True to override, or stop jobs first\"","                    }","                )","        except Exception as e:","            # If we can't check jobs, be conservative and require force","            if not req.force:","                raise HTTPException(","                    status_code=500,","                    detail={","                        \"error\": \"JOB_CHECK_FAILED\",","                        \"message\": \"Cannot verify active jobs status\",","                        \"action\": \"Use force=True to override\"","                    }","                )","    ","    # 4. Attempt to stop worker","    stopped = False","    signal_used = None","    cleanup_performed = False","    ","    try:","        # Send SIGTERM first","        os.kill(pid, signal.SIGTERM)","        signal_used = \"TERM\"","        ","        # Wait up to 5 seconds for graceful shutdown","        for _ in range(50):  # 50 * 0.1 = 5 seconds","            try:","                os.kill(pid, 0)  # Check if process exists","                time.sleep(0.1)","            except ProcessLookupError:","                # Process terminated","                stopped = True","                break","        ","        # If still alive after SIGTERM, send SIGKILL","        if not stopped:","            try:","                os.kill(pid, signal.SIGKILL)","                signal_used = \"KILL\"","                time.sleep(0.5)","                stopped = True","            except ProcessLookupError:","                stopped = True","    except ProcessLookupError:","        # Process already dead","        stopped = True","    except Exception as e:","        raise HTTPException(","            status_code=500,","            detail={","                \"error\": \"STOP_FAILED\",","                \"message\": f\"Failed to stop worker: {str(e)}\",","                \"pid\": pid","            }","        )","    ","    # 5. Clean up pidfile and heartbeat file","    if stopped:","        try:","            pidfile.unlink(missing_ok=True)","            heartbeat_file = db_path.parent / \"worker.heartbeat\"","            heartbeat_file.unlink(missing_ok=True)","            cleanup_performed = True","        except Exception:","            # Cleanup failed, but worker is stopped","            pass","    ","    return {","        \"stopped\": stopped,","        \"pid\": pid,","        \"signal\": signal_used,","        \"active_jobs_count\": active_jobs_count,","        \"force_used\": req.force,","        \"cleanup_performed\": cleanup_performed,","        \"status\": status,","        \"reason\": req.reason","    }","","","# Phase PV.1: Plan Quality endpoints","@app.get(\"/portfolio/plans/{plan_id}/quality\", response_model=PlanQualityReport)","async def get_plan_quality(plan_id: str) -> PlanQualityReport:","    \"\"\"","    Compute quality metrics for a portfolio plan (readâ€‘only).","","    Contract:","    - Zeroâ€‘write: only reads plan package files, never writes","    - Deterministic: same plan â†’ same quality report","    - Returns PlanQualityReport with grade (GREEN/YELLOW/RED) and reasons","    \"\"\"","    outputs_root = _get_outputs_root()","    plan_dir = outputs_root / \"portfolio\" / \"plans\" / plan_id","    if not plan_dir.exists():","        raise HTTPException(status_code=404, detail=f\"Plan {plan_id} not found\")","","    try:","        report, inputs = compute_quality_from_plan_dir(plan_dir)","        return report","    except FileNotFoundError as e:"]}
{"type":"file_chunk","path":"src/control/api.py","chunk_index":9,"line_start":1801,"line_end":1840,"content":["        raise HTTPException(status_code=404, detail=f\"Plan package incomplete: {str(e)}\")","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=f\"Failed to compute quality: {e}\")","","","@app.post(\"/portfolio/plans/{plan_id}/quality\", response_model=PlanQualityReport)","async def write_plan_quality(plan_id: str) -> PlanQualityReport:","    \"\"\"","    Compute quality metrics and write quality files (controlled mutation).","","    Contract:","    - Readâ€‘only over plan package files","    - Controlled mutation: writes only three files under plan_dir:","        - plan_quality.json","        - plan_quality_checksums.json","        - plan_quality_manifest.json","    - Idempotent: identical content â†’ no mtime change","    - Returns PlanQualityReport (same as GET endpoint)","    \"\"\"","    outputs_root = _get_outputs_root()","    plan_dir = outputs_root / \"portfolio\" / \"plans\" / plan_id","    if not plan_dir.exists():","        raise HTTPException(status_code=404, detail=f\"Plan {plan_id} not found\")","","    try:","        # Compute quality (readâ€‘only)","        report, inputs = compute_quality_from_plan_dir(plan_dir)","        # Write quality files (controlled mutation, idempotent)","        write_plan_quality_files(plan_dir, report)","        return report","    except FileNotFoundError as e:","        raise HTTPException(status_code=404, detail=f\"Plan package incomplete: {str(e)}\")","    except ValueError as e:","        raise HTTPException(status_code=400, detail=str(e))","    except Exception as e:","        raise HTTPException(status_code=500, detail=f\"Failed to write quality: {e}\")","",""]}
{"type":"file_footer","path":"src/control/api.py","complete":true,"emitted_chunks":10}
{"type":"file_header","path":"src/control/artifacts.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5960,"sha256":"c7f6bb793d89ec4b5ccd0dcd1ced3309a599bd4c1a97ab0379c204396b600d5b","total_lines":206,"chunk_count":2}
{"type":"file_chunk","path":"src/control/artifacts.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Artifact storage, hashing, and manifest generation for Phase 14.","","Deterministic canonical JSON, SHA256 hashing, atomic writes, and immutable artifact manifests.","\"\"\"","","from __future__ import annotations","","import hashlib","import json","import tempfile","from pathlib import Path","from typing import Any","","","def canonical_json_bytes(obj: object) -> bytes:","    \"\"\"Serialize object to canonical JSON bytes.","    ","    Uses sort_keys=True, ensure_ascii=False, separators=(',', ':') for deterministic ordering.","    ","    Args:","        obj: JSON-serializable object (dict, list, str, int, float, bool, None)","    ","    Returns:","        UTF-8 encoded bytes of canonical JSON representation.","    ","    Raises:","        TypeError: If obj is not JSON serializable.","    \"\"\"","    return json.dumps(","        obj,","        sort_keys=True,","        ensure_ascii=False,","        separators=(\",\", \":\"),","        allow_nan=False,","    ).encode(\"utf-8\")","","","def sha256_bytes(data: bytes) -> str:","    \"\"\"Compute SHA256 hash of bytes.","    ","    Args:","        data: Input bytes.","    ","    Returns:","        Lowercase hex digest string.","    \"\"\"","    return hashlib.sha256(data).hexdigest()","","","# Alias for compatibility with existing code","compute_sha256 = sha256_bytes","","","def write_json_atomic(path: Path, data: dict) -> None:","    \"\"\"Atomically write JSON dict to file.","    ","    Writes to a temporary file in the same directory, then renames to target.","    Ensures no partial writes are visible.","    ","    Args:","        path: Target file path.","        data: JSON-serializable dict.","    ","    Raises:","        OSError: If file cannot be written.","    \"\"\"","    # Ensure parent directory exists","    path.parent.mkdir(parents=True, exist_ok=True)","    ","    # Write to temporary file","    with tempfile.NamedTemporaryFile(","        mode=\"w\",","        encoding=\"utf-8\",","        dir=path.parent,","        prefix=f\".{path.name}.tmp.\",","        delete=False,","    ) as f:","        json.dump(","            data,","            f,","            sort_keys=True,","            ensure_ascii=False,","            separators=(\",\", \":\"),","            allow_nan=False,","        )","        tmp_path = Path(f.name)","    ","    # Atomic rename (POSIX guarantees atomicity)","    try:","        tmp_path.replace(path)","    except Exception:","        tmp_path.unlink(missing_ok=True)","        raise","","","def compute_job_artifacts_root(artifacts_root: Path, batch_id: str, job_id: str) -> Path:","    \"\"\"Compute job artifacts root directory.","    ","    Path pattern: artifacts/{batch_id}/{job_id}/","    ","    Args:","        artifacts_root: Base artifacts directory (e.g., outputs/artifacts).","        batch_id: Batch identifier.","        job_id: Job identifier.","    ","    Returns:","        Path to job artifacts directory.","    \"\"\"","    return artifacts_root / batch_id / job_id","","","def build_job_manifest(job_spec: dict, job_id: str) -> dict:","    \"\"\"Build job manifest dict with hash, without writing to disk.","    ","    The manifest includes:","      - job_id","      - season, dataset_id, config_hash, created_by (from job_spec)","      - created_at (ISO 8601 timestamp)","      - manifest_hash (SHA256 of canonical JSON excluding this field)","    ","    Args:","        job_spec: Job specification dict (must contain season, dataset_id,","                  config_hash, created_by, config_snapshot, outputs_root).","        job_id: Job identifier.","    ","    Returns:","        Manifest dict with manifest_hash.","    ","    Raises:","        KeyError: If required fields missing.","    \"\"\"","    import datetime","    ","    # Required fields","    required = [\"season\", \"dataset_id\", \"config_hash\", \"created_by\", \"config_snapshot\", \"outputs_root\"]","    for field in required:","        if field not in job_spec:","            raise KeyError(f\"job_spec missing required field: {field}\")","    ","    # Build base manifest (without hash)","    manifest = {","        \"job_id\": job_id,","        \"season\": job_spec[\"season\"],","        \"dataset_id\": job_spec[\"dataset_id\"],","        \"config_hash\": job_spec[\"config_hash\"],","        \"created_by\": job_spec[\"created_by\"],","        \"config_snapshot\": job_spec[\"config_snapshot\"],","        \"outputs_root\": job_spec[\"outputs_root\"],","        \"created_at\": datetime.datetime.now(datetime.timezone.utc).isoformat(),","    }","    ","    # Compute hash of canonical JSON (without hash field)","    canonical = canonical_json_bytes(manifest)","    manifest_hash = sha256_bytes(canonical)","    ","    # Add hash field","    manifest_with_hash = {**manifest, \"manifest_hash\": manifest_hash}","    return manifest_with_hash","","","def write_job_manifest(job_root: Path, manifest: dict) -> dict:","    \"\"\"Write job manifest.json and compute its hash.","","    The manifest must be a JSON-serializable dict. The function adds a","    'manifest_hash' field containing the SHA256 of the canonical JSON bytes","    (excluding the hash field itself). The manifest is then written to","    job_root / \"manifest.json\".","","    Args:","        job_root: Job artifacts directory (must exist).","        manifest: Manifest dict (must not contain 'manifest_hash' key).","","    Returns:","        Updated manifest dict with 'manifest_hash' field.","","    Raises:","        ValueError: If manifest already contains 'manifest_hash'.","        OSError: If directory does not exist or cannot write.","    \"\"\"","    if \"manifest_hash\" in manifest:","        raise ValueError(\"manifest must not contain 'manifest_hash' key\")","    ","    # Ensure directory exists","    job_root.mkdir(parents=True, exist_ok=True)","    ","    # Compute hash of canonical JSON (without hash field)","    canonical = canonical_json_bytes(manifest)","    manifest_hash = sha256_bytes(canonical)","    ","    # Add hash field","    manifest_with_hash = {**manifest, \"manifest_hash\": manifest_hash}","    ","    # Write manifest.json","    manifest_path = job_root / \"manifest.json\"","    write_json_atomic(manifest_path, manifest_with_hash)","    ","    return manifest_with_hash","",""]}
{"type":"file_chunk","path":"src/control/artifacts.py","chunk_index":1,"line_start":201,"line_end":206,"content":["# Aliases for compatibility","compute_sha256 = sha256_bytes","write_atomic_json = write_json_atomic","# build_job_manifest is now the function above, not an alias","",""]}
{"type":"file_footer","path":"src/control/artifacts.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/artifacts_api.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4946,"sha256":"a70b62ff17a01a4fa810bd896f3635d6a3c98fce0de60614880a3e04c3b18863","total_lines":167,"chunk_count":1}
{"type":"file_chunk","path":"src/control/artifacts_api.py","chunk_index":0,"line_start":1,"line_end":167,"content":["\"\"\"Artifacts API for M2 Drill-down.","","Provides read-only access to research and portfolio indices.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Dict, List, Any","","from control.artifacts import write_json_atomic","","","def write_research_index(season: str, job_id: str, units: List[Dict[str, Any]]) -> Path:","    \"\"\"Write research index for a job.","    ","    Creates a JSON file at outputs/seasons/{season}/research/{job_id}/research_index.json","    with the structure:","    {","        \"season\": season,","        \"job_id\": job_id,","        \"units_total\": len(units),","        \"units\": units","    }","    ","    Args:","        season: Season identifier (e.g., \"2026Q1\")","        job_id: Job identifier","        units: List of unit dictionaries, each containing at least:","            - data1_symbol","            - data1_timeframe","            - strategy","            - data2_filter","            - status","            - artifacts dict with canonical_results, metrics, trades paths","    ","    Returns:","        Path to the written index file.","    \"\"\"","    idx = {","        \"season\": season,","        \"job_id\": job_id,","        \"units_total\": len(units),","        \"units\": units,","    }","    # Ensure the directory exists","    index_dir = Path(f\"outputs/seasons/{season}/research/{job_id}\")","    index_dir.mkdir(parents=True, exist_ok=True)","    path = index_dir / \"research_index.json\"","    write_json_atomic(path, idx)","    return path","","","def list_research_units(season: str, job_id: str) -> List[Dict[str, Any]]:","    \"\"\"List research units for a given job.","    ","    Reads the research index file and returns the units list.","    ","    Args:","        season: Season identifier","        job_id: Job identifier","    ","    Returns:","        List of unit dictionaries as stored in the index.","    ","    Raises:","        FileNotFoundError: If research index file does not exist.","    \"\"\"","    index_path = Path(f\"outputs/seasons/{season}/research/{job_id}/research_index.json\")","    if not index_path.exists():","        raise FileNotFoundError(f\"Research index not found at {index_path}\")","    with open(index_path, \"r\", encoding=\"utf-8\") as f:","        data = json.load(f)","    return data.get(\"units\", [])","","","def get_research_artifacts(","    season: str, job_id: str, unit_key: Dict[str, str]",") -> Dict[str, str]:","    \"\"\"Get artifact paths for a specific research unit.","    ","    The unit_key must contain data1_symbol, data1_timeframe, strategy, data2_filter.","    ","    Args:","        season: Season identifier","        job_id: Job identifier","        unit_key: Dictionary with keys data1_symbol, data1_timeframe, strategy, data2_filter","    ","    Returns:","        Artifacts dictionary (canonical_results, metrics, trades paths).","    ","    Raises:","        KeyError: If unit not found.","    \"\"\"","    units = list_research_units(season, job_id)","    for unit in units:","        match = all(","            unit.get(k) == v for k, v in unit_key.items()","            if k in (\"data1_symbol\", \"data1_timeframe\", \"strategy\", \"data2_filter\")","        )","        if match:","            return unit.get(\"artifacts\", {})","    raise KeyError(f\"No unit found matching {unit_key}\")","","","def get_portfolio_index(season: str, job_id: str) -> Dict[str, Any]:","    \"\"\"Get portfolio index for a given job.","    ","    Reads portfolio_index.json from outputs/seasons/{season}/portfolio/{job_id}/portfolio_index.json.","    ","    Args:","        season: Season identifier","        job_id: Job identifier","    ","    Returns:","        Portfolio index dictionary.","    ","    Raises:","        FileNotFoundError: If portfolio index file does not exist.","    \"\"\"","    index_path = Path(f\"outputs/seasons/{season}/portfolio/{job_id}/portfolio_index.json\")","    if not index_path.exists():","        raise FileNotFoundError(f\"Portfolio index not found at {index_path}\")","    with open(index_path, \"r\", encoding=\"utf-8\") as f:","        data = json.load(f)","    return data","","","# Optional helper to write portfolio index","def write_portfolio_index(","    season: str,","    job_id: str,","    summary_path: str,","    admission_path: str,",") -> Path:","    \"\"\"Write portfolio index for a job.","    ","    Creates a JSON file at outputs/seasons/{season}/portfolio/{job_id}/portfolio_index.json","    with the structure:","    {","        \"season\": season,","        \"job_id\": job_id,","        \"summary\": summary_path,","        \"admission\": admission_path","    }","    ","    Args:","        season: Season identifier","        job_id: Job identifier","        summary_path: Relative path to summary.json","        admission_path: Relative path to admission.parquet","    ","    Returns:","        Path to the written index file.","    \"\"\"","    idx = {","        \"season\": season,","        \"job_id\": job_id,","        \"summary\": summary_path,","        \"admission\": admission_path,","    }","    index_dir = Path(f\"outputs/seasons/{season}/portfolio/{job_id}\")","    index_dir.mkdir(parents=True, exist_ok=True)","    path = index_dir / \"portfolio_index.json\"","    write_json_atomic(path, idx)","    return path"]}
{"type":"file_footer","path":"src/control/artifacts_api.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/bars_manifest.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4374,"sha256":"ccf6817291b7c2c06243bc26657eeb13824f1dcfe0549226fe3f2a33491b35ab","total_lines":143,"chunk_count":1}
{"type":"file_chunk","path":"src/control/bars_manifest.py","chunk_index":0,"line_start":1,"line_end":143,"content":["","\"\"\"","Bars Manifest å¯«å…¥å·¥å…·","","æä¾› deterministic JSON + self-hash manifest_sha256 + atomic writeã€‚","\"\"\"","","from __future__ import annotations","","import hashlib","import json","import tempfile","from pathlib import Path","from typing import Any, Dict","","from contracts.dimensions import canonical_json","","","def write_bars_manifest(payload: Dict[str, Any], path: Path) -> Dict[str, Any]:","    \"\"\"","    Deterministic JSON + self-hash manifest_sha256 + atomic write.","    ","    è¡Œç‚ºè¦æ ¼ï¼š","    1. å»ºç«‹æš«å­˜æª”æ¡ˆï¼ˆ.json.tmpï¼‰","    2. è¨ˆç®— payload çš„ SHA256 hashï¼ˆæŽ’é™¤ manifest_sha256 æ¬„ä½ï¼‰","    3. å°‡ hash åŠ å…¥ payload ä½œç‚º manifest_sha256 æ¬„ä½","    4. ä½¿ç”¨ canonical_json å¯«å…¥æš«å­˜æª”æ¡ˆï¼ˆç¢ºä¿æŽ’åºä¸€è‡´ï¼‰","    5. atomic replace åˆ°ç›®æ¨™è·¯å¾‘","    6. å¦‚æžœå¯«å…¥å¤±æ•—ï¼Œæ¸…ç†æš«å­˜æª”æ¡ˆ","    ","    Args:","        payload: manifest è³‡æ–™å­—å…¸ï¼ˆä¸å« manifest_sha256ï¼‰","        path: ç›®æ¨™æª”æ¡ˆè·¯å¾‘","        ","    Returns:","        æœ€çµ‚çš„ manifest å­—å…¸ï¼ˆåŒ…å« manifest_sha256 æ¬„ä½ï¼‰","        ","    Raises:","        IOError: å¯«å…¥å¤±æ•—","    \"\"\"","    # ç¢ºä¿ç›®éŒ„å­˜åœ¨","    path.parent.mkdir(parents=True, exist_ok=True)","    ","    # å»ºç«‹æš«å­˜æª”æ¡ˆè·¯å¾‘","    temp_path = path.with_suffix(path.suffix + \".tmp\")","    ","    try:","        # è¨ˆç®— payload çš„ SHA256 hashï¼ˆæŽ’é™¤å¯èƒ½çš„ manifest_sha256 æ¬„ä½ï¼‰","        payload_without_hash = {k: v for k, v in payload.items() if k != \"manifest_sha256\"}","        json_str = canonical_json(payload_without_hash)","        manifest_sha256 = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","        ","        # å»ºç«‹æœ€çµ‚ payloadï¼ˆåŒ…å« hashï¼‰","        final_payload = {**payload_without_hash, \"manifest_sha256\": manifest_sha256}","        ","        # ä½¿ç”¨ canonical_json å¯«å…¥æš«å­˜æª”æ¡ˆ","        final_json = canonical_json(final_payload)","        temp_path.write_text(final_json, encoding=\"utf-8\")","        ","        # atomic replace","        temp_path.replace(path)","        ","        return final_payload","        ","    except Exception as e:","        # æ¸…ç†æš«å­˜æª”æ¡ˆ","        if temp_path.exists():","            try:","                temp_path.unlink()","            except OSError:","                pass","        raise IOError(f\"å¯«å…¥ bars manifest å¤±æ•— {path}: {e}\")","    ","    finally:","        # ç¢ºä¿æš«å­˜æª”æ¡ˆè¢«æ¸…ç†ï¼ˆå¦‚æžœ replace æˆåŠŸï¼Œtemp_path å·²ä¸å­˜åœ¨ï¼‰","        if temp_path.exists():","            try:","                temp_path.unlink()","            except OSError:","                pass","","","def load_bars_manifest(path: Path) -> Dict[str, Any]:","    \"\"\"","    è¼‰å…¥ bars manifest ä¸¦é©—è­‰ hash","    ","    Args:","        path: manifest æª”æ¡ˆè·¯å¾‘","        ","    Returns:","        manifest å­—å…¸","        ","    Raises:","        FileNotFoundError: æª”æ¡ˆä¸å­˜åœ¨","        ValueError: JSON è§£æžå¤±æ•—æˆ– hash é©—è­‰å¤±æ•—","    \"\"\"","    if not path.exists():","        raise FileNotFoundError(f\"bars manifest æª”æ¡ˆä¸å­˜åœ¨: {path}\")","    ","    try:","        content = path.read_text(encoding=\"utf-8\")","    except (IOError, OSError) as e:","        raise ValueError(f\"ç„¡æ³•è®€å– bars manifest æª”æ¡ˆ {path}: {e}\")","    ","    try:","        data = json.loads(content)","    except json.JSONDecodeError as e:","        raise ValueError(f\"bars manifest JSON è§£æžå¤±æ•— {path}: {e}\")","    ","    # é©—è­‰ manifest_sha256","    if \"manifest_sha256\" not in data:","        raise ValueError(f\"bars manifest ç¼ºå°‘ manifest_sha256 æ¬„ä½: {path}\")","    ","    # è¨ˆç®—å¯¦éš› hashï¼ˆæŽ’é™¤ manifest_sha256 æ¬„ä½ï¼‰","    data_without_hash = {k: v for k, v in data.items() if k != \"manifest_sha256\"}","    json_str = canonical_json(data_without_hash)","    expected_hash = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","    ","    if data[\"manifest_sha256\"] != expected_hash:","        raise ValueError(f\"bars manifest hash é©—è­‰å¤±æ•—: é æœŸ {expected_hash}ï¼Œå¯¦éš› {data['manifest_sha256']}\")","    ","    return data","","","def bars_manifest_path(outputs_root: Path, season: str, dataset_id: str) -> Path:","    \"\"\"","    å–å¾— bars manifest æª”æ¡ˆè·¯å¾‘","    ","    å»ºè­°ä½ç½®ï¼šoutputs/shared/{season}/{dataset_id}/bars/bars_manifest.json","    ","    Args:","        outputs_root: è¼¸å‡ºæ ¹ç›®éŒ„","        season: å­£ç¯€æ¨™è¨˜","        dataset_id: è³‡æ–™é›† ID","        ","    Returns:","        æª”æ¡ˆè·¯å¾‘","    \"\"\"","    # å»ºç«‹è·¯å¾‘","    path = outputs_root / \"shared\" / season / dataset_id / \"bars\" / \"bars_manifest.json\"","    return path","",""]}
{"type":"file_footer","path":"src/control/bars_manifest.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/bars_store.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5538,"sha256":"51338ab2017d401bfc833d0cf1080cdfd025e9dc173b055d40e08bf3536a6280","total_lines":207,"chunk_count":2}
{"type":"file_chunk","path":"src/control/bars_store.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Bars I/O å·¥å…·","","æä¾› deterministic NPZ æª”æ¡ˆè®€å¯«ï¼Œæ”¯æ´ atomic writeï¼ˆtmp + replaceï¼‰èˆ‡ SHA256 è¨ˆç®—ã€‚","\"\"\"","","from __future__ import annotations","","import hashlib","import json","import tempfile","from pathlib import Path","from typing import Dict, Literal, Optional, Union","import numpy as np","","Timeframe = Literal[15, 30, 60, 120, 240]","","","def bars_dir(outputs_root: Path, season: str, dataset_id: str) -> Path:","    \"\"\"","    å–å¾— bars ç›®éŒ„è·¯å¾‘","","    å»ºè­°ä½ç½®ï¼šoutputs/shared/{season}/{dataset_id}/bars/","","    Args:","        outputs_root: è¼¸å‡ºæ ¹ç›®éŒ„","        season: å­£ç¯€æ¨™è¨˜ï¼Œä¾‹å¦‚ \"2026Q1\"","        dataset_id: è³‡æ–™é›† ID","","    Returns:","        ç›®éŒ„è·¯å¾‘","    \"\"\"","    # å»ºç«‹è·¯å¾‘","    path = outputs_root / \"shared\" / season / dataset_id / \"bars\"","    return path","","","def normalized_bars_path(outputs_root: Path, season: str, dataset_id: str) -> Path:","    \"\"\"","    å–å¾— normalized bars æª”æ¡ˆè·¯å¾‘","","    å»ºè­°ä½ç½®ï¼šoutputs/shared/{season}/{dataset_id}/bars/normalized_bars.npz","","    Args:","        outputs_root: è¼¸å‡ºæ ¹ç›®éŒ„","        season: å­£ç¯€æ¨™è¨˜","        dataset_id: è³‡æ–™é›† ID","","    Returns:","        æª”æ¡ˆè·¯å¾‘","    \"\"\"","    dir_path = bars_dir(outputs_root, season, dataset_id)","    return dir_path / \"normalized_bars.npz\"","","","def resampled_bars_path(","    outputs_root: Path, ","    season: str, ","    dataset_id: str, ","    tf_min: Timeframe",") -> Path:","    \"\"\"","    å–å¾— resampled bars æª”æ¡ˆè·¯å¾‘","","    å»ºè­°ä½ç½®ï¼šoutputs/shared/{season}/{dataset_id}/bars/resampled_{tf_min}m.npz","","    Args:","        outputs_root: è¼¸å‡ºæ ¹ç›®éŒ„","        season: å­£ç¯€æ¨™è¨˜","        dataset_id: è³‡æ–™é›† ID","        tf_min: timeframe åˆ†é˜æ•¸ï¼ˆ15, 30, 60, 120, 240ï¼‰","","    Returns:","        æª”æ¡ˆè·¯å¾‘","    \"\"\"","    dir_path = bars_dir(outputs_root, season, dataset_id)","    return dir_path / f\"resampled_{tf_min}m.npz\"","","","def write_npz_atomic(path: Path, arrays: Dict[str, np.ndarray]) -> None:","    \"\"\"","    Write npz via tmp + replace. Deterministic keys order.","","    è¡Œç‚ºè¦æ ¼ï¼š","    1. å»ºç«‹æš«å­˜æª”æ¡ˆï¼ˆ.npz.tmpï¼‰","    2. å°‡ arrays çš„ keys æŽ’åºä»¥ç¢ºä¿ deterministic","    3. ä½¿ç”¨ np.savez_compressed å¯«å…¥æš«å­˜æª”æ¡ˆ","    4. å°‡æš«å­˜æª”æ¡ˆ atomic replace åˆ°ç›®æ¨™è·¯å¾‘","    5. å¦‚æžœå¯«å…¥å¤±æ•—ï¼Œæ¸…ç†æš«å­˜æª”æ¡ˆ","","    Args:","        path: ç›®æ¨™æª”æ¡ˆè·¯å¾‘","        arrays: å­—å…¸ï¼Œkey ç‚ºå­—ä¸²ï¼Œvalue ç‚º numpy array","","    Raises:","        IOError: å¯«å…¥å¤±æ•—","    \"\"\"","    # ç¢ºä¿ç›®éŒ„å­˜åœ¨","    path.parent.mkdir(parents=True, exist_ok=True)","    ","    # å»ºç«‹æš«å­˜æª”æ¡ˆè·¯å¾‘ï¼ˆnp.savez æœƒè‡ªå‹•æ·»åŠ  .npz å‰¯æª”åï¼‰","    # æ‰€ä»¥æˆ‘å€‘éœ€è¦å»ºç«‹æ²’æœ‰ .npz çš„æš«å­˜æª”æ¡ˆåï¼Œä¾‹å¦‚ normalized_bars.npz.tmp -> normalized_bars.tmp","    # ç„¶å¾Œ np.savez æœƒå»ºç«‹ normalized_bars.tmp.npzï¼Œæˆ‘å€‘å†é‡å‘½åç‚º normalized_bars.npz","    temp_base = path.with_suffix(\"\")  # ç§»é™¤ .npz","    temp_path = temp_base.with_suffix(temp_base.suffix + \".tmp.npz\")","    ","    try:","        # æŽ’åº keys ä»¥ç¢ºä¿ deterministic","        sorted_keys = sorted(arrays.keys())","        sorted_arrays = {k: arrays[k] for k in sorted_keys}","        ","        # å¯«å…¥æš«å­˜æª”æ¡ˆï¼ˆä½¿ç”¨ savezï¼Œé¿å…å£“ç¸®å¯èƒ½å°Žè‡´çš„å•é¡Œï¼‰","        np.savez(temp_path, **sorted_arrays)","        ","        # atomic replace","        temp_path.replace(path)","        ","    except Exception as e:","        # æ¸…ç†æš«å­˜æª”æ¡ˆ","        if temp_path.exists():","            try:","                temp_path.unlink()","            except OSError:","                pass","        raise IOError(f\"å¯«å…¥ NPZ æª”æ¡ˆå¤±æ•— {path}: {e}\")","    ","    finally:","        # ç¢ºä¿æš«å­˜æª”æ¡ˆè¢«æ¸…ç†ï¼ˆå¦‚æžœ replace æˆåŠŸï¼Œtemp_path å·²ä¸å­˜åœ¨ï¼‰","        if temp_path.exists():","            try:","                temp_path.unlink()","            except OSError:","                pass","","","def load_npz(path: Path) -> Dict[str, np.ndarray]:","    \"\"\"","    è¼‰å…¥ NPZ æª”æ¡ˆ","","    Args:","        path: NPZ æª”æ¡ˆè·¯å¾‘","","    Returns:","        å­—å…¸ï¼Œkey ç‚ºå­—ä¸²ï¼Œvalue ç‚º numpy array","","    Raises:","        FileNotFoundError: æª”æ¡ˆä¸å­˜åœ¨","        ValueError: æª”æ¡ˆæ ¼å¼éŒ¯èª¤","    \"\"\"","    if not path.exists():","        raise FileNotFoundError(f\"NPZ æª”æ¡ˆä¸å­˜åœ¨: {path}\")","    ","    try:","        with np.load(path, allow_pickle=False) as data:","            # è½‰æ›ç‚ºå­—å…¸ï¼ˆä¿æŒåŽŸå§‹é †åºï¼Œä½†æˆ‘å€‘ä¸ä¾è³´é †åºï¼‰","            arrays = {key: data[key] for key in data.files}","            return arrays","    except Exception as e:","        raise ValueError(f\"è¼‰å…¥ NPZ æª”æ¡ˆå¤±æ•— {path}: {e}\")","","","def sha256_file(path: Path) -> str:","    \"\"\"","    è¨ˆç®—æª”æ¡ˆçš„ SHA256 hash","","    Args:","        path: æª”æ¡ˆè·¯å¾‘","","    Returns:","        SHA256 hex digestï¼ˆå°å¯«ï¼‰","","    Raises:","        FileNotFoundError: æª”æ¡ˆä¸å­˜åœ¨","        IOError: è®€å–å¤±æ•—","    \"\"\"","    if not path.exists():","        raise FileNotFoundError(f\"æª”æ¡ˆä¸å­˜åœ¨: {path}\")","    ","    sha256 = hashlib.sha256()","    ","    try:","        with open(path, \"rb\") as f:","            # åˆ†å¡Šè®€å–ä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ","            for chunk in iter(lambda: f.read(65536), b\"\"):","                sha256.update(chunk)","    except Exception as e:","        raise IOError(f\"è®€å–æª”æ¡ˆå¤±æ•— {path}: {e}\")","    ","    return sha256.hexdigest()","","","def canonical_json(obj: dict) -> str:","    \"\"\"","    ç”¢ç”Ÿæ¨™æº–åŒ– JSON å­—ä¸²ï¼Œç¢ºä¿åºåˆ—åŒ–ä¸€è‡´æ€§","","    ä½¿ç”¨èˆ‡ contracts/dimensions.py ç›¸åŒçš„å¯¦ä½œ","","    Args:","        obj: è¦åºåˆ—åŒ–çš„å­—å…¸"]}
{"type":"file_chunk","path":"src/control/bars_store.py","chunk_index":1,"line_start":201,"line_end":207,"content":["","    Returns:","        æ¨™æº–åŒ– JSON å­—ä¸²","    \"\"\"","    return json.dumps(obj, ensure_ascii=False, sort_keys=True, separators=(\",\", \":\"))","",""]}
{"type":"file_footer","path":"src/control/bars_store.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/batch_aggregate.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6550,"sha256":"dbff14ee09f25ed346c91daa96819d91d0b265763a88bc1cd5666e0fe52a1139","total_lines":216,"chunk_count":2}
{"type":"file_chunk","path":"src/control/batch_aggregate.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Batch result aggregation for Phase 14.","","TopK selection, summary metrics, and deterministic ordering.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Any","","","def compute_batch_summary(index_or_jobs: dict | list, *, top_k: int = 20) -> dict:","    \"\"\"Compute batch summary statistics and TopK jobs.","    ","    Accepts either a batch index dict (as returned by read_batch_index) or a","    plain list of job entries. If a dict is provided, it must contain a 'jobs'","    list. If a list is provided, it is treated as the jobs list directly.","    ","    Each job entry must have at least:","      - job_id","    ","    Additional fields may be present (e.g., metrics, score). If a job entry","    contains a 'score' numeric field, it will be used for ranking. If not,","    jobs are ranked by job_id (lexicographic).","    ","    Args:","        index_or_jobs: Batch index dict or list of job entries.","        top_k: Number of top jobs to return.","    ","    Returns:","        Summary dict with:","          - total_jobs: total number of jobs","          - top_k: list of job entries (sorted descending by score, tieâ€‘break by job_id)","          - stats: dict with count, mean_score, median_score, std_score, etc.","          - summary_hash: SHA256 of canonical JSON of summary (excluding this field)","    \"\"\"","    import statistics","    from control.artifacts import canonical_json_bytes, sha256_bytes","    ","    # Normalize input to jobs list","    if isinstance(index_or_jobs, dict):","        jobs = index_or_jobs.get(\"jobs\", [])","        batch_id = index_or_jobs.get(\"batch_id\", \"unknown\")","    else:","        jobs = index_or_jobs","        batch_id = \"unknown\"","    ","    total = len(jobs)","    ","    # Determine which jobs have a score field","    scored_jobs = []","    unscored_jobs = []","    for job in jobs:","        score = job.get(\"score\")","        if isinstance(score, (int, float)):","            scored_jobs.append(job)","        else:","            unscored_jobs.append(job)","    ","    # Sort scored jobs descending by score, tieâ€‘break by job_id ascending","    scored_jobs_sorted = sorted(","        scored_jobs,","        key=lambda j: (-float(j[\"score\"]), j[\"job_id\"])","    )","    ","    # Sort unscored jobs by job_id ascending","    unscored_jobs_sorted = sorted(unscored_jobs, key=lambda j: j[\"job_id\"])","    ","    # Combine: scored first, then unscored","    all_jobs_sorted = scored_jobs_sorted + unscored_jobs_sorted","    ","    # Take top_k","    top_k_list = all_jobs_sorted[:top_k]","    ","    # Compute stats","    scores = [j.get(\"score\") for j in jobs if isinstance(j.get(\"score\"), (int, float))]","    stats = {","        \"count\": total,","    }","    ","    if scores:","        stats[\"mean_score\"] = sum(scores) / len(scores)","        stats[\"median_score\"] = statistics.median(scores)","        stats[\"std_score\"] = statistics.stdev(scores) if len(scores) > 1 else 0.0","        stats[\"best_score\"] = max(scores)","        stats[\"worst_score\"] = min(scores)","        stats[\"score_range\"] = max(scores) - min(scores)","    ","    # Build summary dict without hash","    summary = {","        \"batch_id\": batch_id,","        \"total_jobs\": total,","        \"top_k\": top_k_list,","        \"stats\": stats,","    }","    ","    # Compute hash of canonical JSON (excluding hash field)","    canonical = canonical_json_bytes(summary)","    summary_hash = sha256_bytes(canonical)","    summary[\"summary_hash\"] = summary_hash","    ","    return summary","","","def load_job_manifest(artifacts_root: Path, job_entry: dict) -> dict:","    \"\"\"Load job manifest given a job entry from batch index.","    ","    Args:","        artifacts_root: Base artifacts directory.","        job_entry: Job entry dict with 'manifest_path'.","    ","    Returns:","        Parsed manifest dict.","    ","    Raises:","        FileNotFoundError: If manifest file does not exist.","        json.JSONDecodeError: If manifest is malformed.","    \"\"\"","    manifest_path = artifacts_root / job_entry[\"manifest_path\"]","    if not manifest_path.exists():","        raise FileNotFoundError(f\"Job manifest not found: {manifest_path}\")","    ","    return json.loads(manifest_path.read_text(encoding=\"utf-8\"))","","","def extract_score_from_manifest(manifest: dict) -> float | None:","    \"\"\"Extract numeric score from job manifest.","    ","    Looks for common score fields: 'score', 'final_score', 'metrics.score'.","    ","    Args:","        manifest: Job manifest dict.","    ","    Returns:","        Numeric score if found, else None.","    \"\"\"","    # Direct score field","    score = manifest.get(\"score\")","    if isinstance(score, (int, float)):","        return float(score)","    ","    # Nested in metrics","    metrics = manifest.get(\"metrics\")","    if isinstance(metrics, dict):","        score = metrics.get(\"score\")","        if isinstance(score, (int, float)):","            return float(score)","    ","    # Final score","    final = manifest.get(\"final_score\")","    if isinstance(final, (int, float)):","        return float(final)","    ","    return None","","","def augment_job_entry_with_score(","    artifacts_root: Path,","    job_entry: dict,",") -> dict:","    \"\"\"Augment job entry with score loaded from manifest.","    ","    If job_entry already has a 'score' field, returns unchanged.","    Otherwise, loads manifest and extracts score.","    ","    Args:","        artifacts_root: Base artifacts directory.","        job_entry: Job entry dict.","    ","    Returns:","        Updated job entry with 'score' field if available.","    \"\"\"","    if \"score\" in job_entry:","        return job_entry","    ","    try:","        manifest = load_job_manifest(artifacts_root, job_entry)","        score = extract_score_from_manifest(manifest)","        if score is not None:","            job_entry = {**job_entry, \"score\": score}","    except (FileNotFoundError, json.JSONDecodeError):","        pass","    ","    return job_entry","","","def compute_detailed_summary(","    artifacts_root: Path,","    index: dict,","    *,","    top_k: int = 20,",") -> dict:","    \"\"\"Compute detailed batch summary with scores loaded from manifests.","    ","    This is a convenience function that loads each job manifest to extract","    scores and other metrics, then calls compute_batch_summary.","    ","    Args:"]}
{"type":"file_chunk","path":"src/control/batch_aggregate.py","chunk_index":1,"line_start":201,"line_end":216,"content":["        artifacts_root: Base artifacts directory.","        index: Batch index dict.","        top_k: Number of top jobs to return.","    ","    Returns:","        Same structure as compute_batch_summary, but with scores populated.","    \"\"\"","    jobs = index.get(\"jobs\", [])","    augmented = []","    for job in jobs:","        augmented.append(augment_job_entry_with_score(artifacts_root, job))","    ","    index_with_scores = {**index, \"jobs\": augmented}","    return compute_batch_summary(index_with_scores, top_k=top_k)","",""]}
{"type":"file_footer","path":"src/control/batch_aggregate.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/batch_api.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8778,"sha256":"d1eb010856532885cb8718adb17b1e9da33056949ce7238267be593cf5ff4323","total_lines":294,"chunk_count":2}
{"type":"file_chunk","path":"src/control/batch_api.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 14.1: Read-only Batch API helpers.","","Contracts:","- No Engine mutation.","- No on-the-fly batch computation.","- Only read JSON artifacts under artifacts_root/{batch_id}/...","- Missing files -> FileNotFoundError (API maps to 404).","- Deterministic outputs: stable ordering by job_id, attempt_n.","\"\"\"","","from __future__ import annotations","","import json","import logging","import re","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Optional","","from pydantic import BaseModel, ConfigDict","","","_ATTEMPT_RE = re.compile(r\"^attempt_(\\d+)$\")","_logger = logging.getLogger(__name__)","","","# ---------- Pydantic validation models (readâ€‘only) ----------","class BatchExecution(BaseModel):","    \"\"\"Schema for execution.json.\"\"\"","    model_config = ConfigDict(extra=\"ignore\")","","    # We allow flexible structure; just store the raw dict.","    # For validation we can add fields later.","    # For now, we keep it as a generic dict.","    raw: dict[str, Any]","","    @classmethod","    def validate_raw(cls, data: dict[str, Any]) -> BatchExecution:","        \"\"\"Validate and wrap raw execution data.\"\"\"","        # Optional: add stricter validation here.","        return cls(raw=data)","","","class BatchSummary(BaseModel):","    \"\"\"Schema for summary.json.\"\"\"","    model_config = ConfigDict(extra=\"ignore\")","","    topk: list[dict[str, Any]] = []","    metrics: dict[str, Any] = {}","","    @classmethod","    def validate_raw(cls, data: dict[str, Any]) -> BatchSummary:","        \"\"\"Validate and wrap raw summary data.\"\"\"","        # Ensure topk is a list, metrics is a dict","        topk = data.get(\"topk\", [])","        if not isinstance(topk, list):","            topk = []","        metrics = data.get(\"metrics\", {})","        if not isinstance(metrics, dict):","            metrics = {}","        return cls(topk=topk, metrics=metrics)","","","class BatchIndex(BaseModel):","    \"\"\"Schema for index.json.\"\"\"","    model_config = ConfigDict(extra=\"ignore\")","","    raw: dict[str, Any]","","    @classmethod","    def validate_raw(cls, data: dict[str, Any]) -> BatchIndex:","        return cls(raw=data)","","","class BatchMetadata(BaseModel):","    \"\"\"Schema for metadata.json.\"\"\"","    model_config = ConfigDict(extra=\"ignore\")","","    raw: dict[str, Any]","","    @classmethod","    def validate_raw(cls, data: dict[str, Any]) -> BatchMetadata:","        return cls(raw=data)","","","def _validate_model(model_class, data: dict[str, Any]) -> dict[str, Any]:","    \"\"\"","    Validate data against a Pydantic model; on failure log warning and return raw.","    \"\"\"","    try:","        model = model_class.validate_raw(data)","        # Return the validated model as dict (or raw dict) for compatibility.","        # We'll return the raw data because the existing functions expect dict.","        # However we could return model.dict() but that would change structure.","        # For now, we just log success.","        _logger.debug(\"Successfully validated %s\", model_class.__name__)","        return data","    except Exception as e:","        _logger.warning(\"Validation of %s failed: %s\", model_class.__name__, e)","        return data","","","def _read_json(path: Path) -> dict[str, Any]:","    if not path.exists():","        raise FileNotFoundError(str(path))","    text = path.read_text(encoding=\"utf-8\")","    return json.loads(text)","","","def read_execution(artifacts_root: Path, batch_id: str) -> dict[str, Any]:","    \"\"\"","    Read artifacts/{batch_id}/execution.json","    \"\"\"","    raw = _read_json(artifacts_root / batch_id / \"execution.json\")","    return _validate_model(BatchExecution, raw)","","","def read_summary(artifacts_root: Path, batch_id: str) -> dict[str, Any]:","    \"\"\"","    Read artifacts/{batch_id}/summary.json","    \"\"\"","    raw = _read_json(artifacts_root / batch_id / \"summary.json\")","    return _validate_model(BatchSummary, raw)","","","def read_index(artifacts_root: Path, batch_id: str) -> dict[str, Any]:","    \"\"\"","    Read artifacts/{batch_id}/index.json","    \"\"\"","    raw = _read_json(artifacts_root / batch_id / \"index.json\")","    return _validate_model(BatchIndex, raw)","","","def read_metadata_optional(artifacts_root: Path, batch_id: str) -> Optional[dict[str, Any]]:","    \"\"\"","    Read artifacts/{batch_id}/metadata.json (optional).","    \"\"\"","    path = artifacts_root / batch_id / \"metadata.json\"","    if not path.exists():","        return None","    raw = json.loads(path.read_text(encoding=\"utf-8\"))","    return _validate_model(BatchMetadata, raw)","","","@dataclass(frozen=True)","class JobCounts:","    total: int","    done: int","    failed: int","","","def _normalize_state(s: Any) -> str:","    if s is None:","        return \"PENDING\"","    v = str(s).upper()","    # Accept common variants","    if v in {\"PENDING\", \"RUNNING\", \"SUCCESS\", \"FAILED\", \"SKIPPED\"}:","        return v","    if v in {\"DONE\", \"OK\"}:","        return \"SUCCESS\"","    return v","","","def count_states(execution: dict[str, Any]) -> JobCounts:","    \"\"\"","    Count job states from execution.json with best-effort schema support.","","    Supported schemas:","    - {\"jobs\": {\"job_id\": {\"state\": \"SUCCESS\"}, ...}}","    - {\"jobs\": [{\"job_id\": \"...\", \"state\": \"SUCCESS\"}, ...]}","    - {\"job_states\": {...}} (fallback)","    \"\"\"","    jobs_obj = execution.get(\"jobs\", None)","    if jobs_obj is None:","        jobs_obj = execution.get(\"job_states\", None)","","    total = done = failed = 0","","    if isinstance(jobs_obj, dict):","        # mapping: job_id -> {state: ...}","        for _job_id, rec in jobs_obj.items():","            total += 1","            state = _normalize_state(rec.get(\"state\") if isinstance(rec, dict) else rec)","            if state in {\"SUCCESS\", \"SKIPPED\"}:","                done += 1","            elif state == \"FAILED\":","                failed += 1","","    elif isinstance(jobs_obj, list):","        # list: {job_id, state}","        for rec in jobs_obj:","            if not isinstance(rec, dict):","                continue","            total += 1","            state = _normalize_state(rec.get(\"state\"))","            if state in {\"SUCCESS\", \"SKIPPED\"}:","                done += 1","            elif state == \"FAILED\":"]}
{"type":"file_chunk","path":"src/control/batch_api.py","chunk_index":1,"line_start":201,"line_end":294,"content":["                failed += 1","","    return JobCounts(total=total, done=done, failed=failed)","","","def get_batch_state(execution: dict[str, Any]) -> str:","    \"\"\"","    Extract batch state from execution.json with best-effort schema support.","    \"\"\"","    for k in (\"batch_state\", \"state\", \"status\"):","        if k in execution:","            return str(execution[k])","    # Fallback: infer from counts","    c = count_states(execution)","    if c.total == 0:","        return \"PENDING\"","    if c.failed > 0 and c.done == c.total:","        return \"PARTIAL_FAILED\" if c.failed < c.total else \"FAILED\"","    if c.done == c.total:","        return \"DONE\"","    return \"RUNNING\"","","","def list_artifacts_tree(artifacts_root: Path, batch_id: str) -> dict[str, Any]:","    \"\"\"","    Deterministically list artifacts for a batch.","","    Layout assumed:","      artifacts/{batch_id}/{job_id}/attempt_n/manifest.json","","    Returns:","      {","        \"batch_id\": \"...\",","        \"jobs\": [","          {","            \"job_id\": \"...\",","            \"attempts\": [","              {\"attempt\": 1, \"manifest_path\": \"...\", \"score\": 12.3},","              ...","            ]","          },","          ...","        ]","      }","    \"\"\"","    batch_dir = artifacts_root / batch_id","    if not batch_dir.exists():","        raise FileNotFoundError(str(batch_dir))","","    jobs: list[dict[str, Any]] = []","","    # job directories are direct children excluding known files","    for child in sorted(batch_dir.iterdir(), key=lambda p: p.name):","        if not child.is_dir():","            continue","        job_id = child.name","        attempts: list[dict[str, Any]] = []","","        # attempt directories","        for a in sorted(child.iterdir(), key=lambda p: p.name):","            if not a.is_dir():","                continue","            m = _ATTEMPT_RE.match(a.name)","            if not m:","                continue","            attempt_n = int(m.group(1))","            manifest_path = a / \"manifest.json\"","            score = None","            if manifest_path.exists():","                try:","                    man = json.loads(manifest_path.read_text(encoding=\"utf-8\"))","                    # best-effort: score might be at top-level or under metrics","                    if isinstance(man, dict):","                        if \"score\" in man:","                            score = man.get(\"score\")","                        elif isinstance(man.get(\"metrics\"), dict) and \"score\" in man[\"metrics\"]:","                            score = man[\"metrics\"].get(\"score\")","                except Exception:","                    # do not crash listing","                    score = None","","            attempts.append(","                {","                    \"attempt\": attempt_n,","                    \"manifest_path\": str(manifest_path),","                    \"score\": score,","                }","            )","","        jobs.append({\"job_id\": job_id, \"attempts\": attempts})","","    return {\"batch_id\": batch_id, \"jobs\": jobs}","",""]}
{"type":"file_footer","path":"src/control/batch_api.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/batch_execute.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":15072,"sha256":"60e536ee735e466d8b280867d970924ceb305226d5be836f06237740cb85d99f","total_lines":400,"chunk_count":2}
{"type":"file_chunk","path":"src/control/batch_execute.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Batch execution orchestration for Phase 14.","","State machine for batch execution, retry/resume, and progress aggregation.","\"\"\"","","from __future__ import annotations","","import json","import time","from dataclasses import dataclass, field","from enum import StrEnum","from pathlib import Path","from typing import Any, Callable, Optional","","from control.artifacts import (","    compute_job_artifacts_root,","    write_job_manifest,",")","from control.batch_index import build_batch_index, write_batch_index","from control.jobs_db import (","    create_job,","    get_job,","    mark_done,","    mark_failed,","    mark_running,",")","from control.job_spec import WizardJobSpec","from control.types import DBJobSpec","from control.batch_submit import wizard_to_db_jobspec","","","class BatchExecutionState(StrEnum):","    \"\"\"Batch-level execution state.\"\"\"","    PENDING = \"PENDING\"","    RUNNING = \"RUNNING\"","    DONE = \"DONE\"","    FAILED = \"FAILED\"","    PARTIAL_FAILED = \"PARTIAL_FAILED\"  # Some jobs failed, some succeeded","","","class JobExecutionState(StrEnum):","    \"\"\"Job-level execution state (extends JobStatus with SKIPPED).\"\"\"","    PENDING = \"PENDING\"","    RUNNING = \"RUNNING\"","    SUCCESS = \"SUCCESS\"","    FAILED = \"FAILED\"","    SKIPPED = \"SKIPPED\"  # Used for retry/resume when job already DONE","","","@dataclass","class BatchExecutionRecord:","    \"\"\"Persistent record of batch execution.","    ","    Must be deterministic and replayable.","    \"\"\"","    batch_id: str","    state: BatchExecutionState","    total_jobs: int","    counts: dict[str, int]  # done, failed, running, pending, skipped","    per_job_states: dict[str, JobExecutionState]  # job_id -> state","    artifact_index_path: Optional[str] = None","    error_summary: Optional[str] = None","    created_at: str = field(default_factory=lambda: time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()))","    updated_at: str = field(default_factory=lambda: time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()))","","","class BatchExecutor:","    \"\"\"Orchestrates batch execution, retry/resume, and artifact generation.","    ","    Deterministic: same batch_id + same jobs â†’ same artifact hashes.","    Immutable: once a job manifest is written, it cannot be overwritten.","    \"\"\"","    ","    def __init__(","        self,","        batch_id: str,","        job_ids: list[str],","        artifacts_root: Path | None = None,","        *,","        create_runner=None,","        load_jobs=None,","        db_path: Path | None = None,","    ):","        self.batch_id = batch_id","        self.job_ids = list(job_ids)","        self.artifacts_root = artifacts_root","        self.create_runner = create_runner","        self.load_jobs = load_jobs","        self.db_path = db_path or Path(\"outputs/jobs.db\")","","        self.job_states: dict[str, JobExecutionState] = {","            jid: JobExecutionState.PENDING for jid in self.job_ids","        }","        self.state: BatchExecutionState = BatchExecutionState.PENDING","        self.created_at = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())","        self.updated_at = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())","","    def set_job_state(self, job_id: str, state: JobExecutionState) -> None:","        if job_id not in self.job_states:","            raise KeyError(f\"Unknown job_id: {job_id}\")","        self.job_states[job_id] = state","        self.update_state()","","    def update_state(self) -> None:","        states = list(self.job_states.values())","        if not states:","            self.state = BatchExecutionState.PENDING","            return","","        if any(s == JobExecutionState.FAILED for s in states):","            self.state = BatchExecutionState.FAILED","            return","","        completed = {JobExecutionState.SUCCESS, JobExecutionState.SKIPPED}","        if all(s in completed for s in states):","            self.state = BatchExecutionState.DONE","            return","","        # âœ… æ ¸å¿ƒä¿®æ­£ï¼šåªè¦å·²ç¶“æœ‰ä»»ä½• job é–‹å§‹/å®Œæˆï¼Œä½†å°šæœªå…¨å®Œï¼Œå°±ç®— RUNNING","        started = {JobExecutionState.RUNNING, JobExecutionState.SUCCESS, JobExecutionState.SKIPPED}","        if any(s in started for s in states):","            self.state = BatchExecutionState.RUNNING","            return","","        self.state = BatchExecutionState.PENDING","","    def _set_job_state(self, job_id: str, state: JobExecutionState) -> None:","        if job_id not in self.job_states:","            raise KeyError(f\"Unknown job_id: {job_id}\")","        self.job_states[job_id] = state","        self._recompute_state()","","    def _recompute_state(self) -> None:","        states = list(self.job_states.values())","        if not states:","            self.state = BatchExecutionState.PENDING","            return","","        completed = {JobExecutionState.SUCCESS, JobExecutionState.SKIPPED}","","        n_failed = sum(1 for s in states if s == JobExecutionState.FAILED)","        n_done = sum(1 for s in states if s in completed)","        n_running = sum(1 for s in states if s == JobExecutionState.RUNNING)","        n_pending = sum(1 for s in states if s == JobExecutionState.PENDING)","","        # all completed and none failed -> DONE","        if n_failed == 0 and n_done == len(states):","            self.state = BatchExecutionState.DONE","            return","","        # any failed:","        if n_failed > 0:","            # some succeeded/skipped -> PARTIAL_FAILED","            if n_done > 0:","                self.state = BatchExecutionState.PARTIAL_FAILED","                return","            # no success at all -> FAILED","            self.state = BatchExecutionState.FAILED","            return","","        # no failed, not all done:","        started = {JobExecutionState.RUNNING, JobExecutionState.SUCCESS, JobExecutionState.SKIPPED}","        if any(s in started for s in states):","            self.state = BatchExecutionState.RUNNING","            return","","        self.state = BatchExecutionState.PENDING","","    def run(self, artifacts_root: Path) -> dict:","        \"\"\"Run batch from PENDINGâ†’DONE/FAILED, write per-job manifest, write batch index.","        ","        Args:","            artifacts_root: Base artifacts directory.","        ","        Returns:","            Batch execution summary dict.","        ","        Raises:","            ValueError: If batch_id not found or invalid.","            RuntimeError: If execution fails irrecoverably.","        \"\"\"","        self.artifacts_root = artifacts_root","        ","        # Load jobs","        if self.load_jobs is None:","            raise RuntimeError(\"load_jobs callback not set\")","        ","        wizard_jobs = self.load_jobs(self.batch_id)","        if not wizard_jobs:","            raise ValueError(f\"No jobs found for batch {self.batch_id}\")","        ","        # Convert to DB JobSpec","        db_jobs = [wizard_to_db_jobspec(job) for job in wizard_jobs]","        ","        # Create job records in DB (if not already created)","        job_ids = []","        for db_spec in db_jobs:","            job_id = create_job(self.db_path, db_spec)","            job_ids.append(job_id)"]}
{"type":"file_chunk","path":"src/control/batch_execute.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        ","        # Initialize execution record","        total = len(job_ids)","        per_job_states = {job_id: JobExecutionState.PENDING for job_id in job_ids}","        record = BatchExecutionRecord(","            batch_id=self.batch_id,","            state=BatchExecutionState.RUNNING,","            total_jobs=total,","            counts={","                \"done\": 0,","                \"failed\": 0,","                \"running\": 0,","                \"pending\": total,","                \"skipped\": 0,","            },","            per_job_states=per_job_states,","        )","        ","        # Run each job","        job_entries = []","        for job_id, wizard_spec in zip(job_ids, wizard_jobs):","            # Update state","            record.per_job_states[job_id] = JobExecutionState.RUNNING","            record.counts[\"running\"] += 1","            record.counts[\"pending\"] -= 1","            self._update_record(self.batch_id, record)","            ","            try:","                # Get DB spec (already created)","                db_spec = wizard_to_db_jobspec(wizard_spec)","                ","                # Mark as running in DB","                mark_running(self.db_path, job_id, pid=os.getpid())","                ","                # Create runner and execute","                if self.create_runner is None:","                    raise RuntimeError(\"create_runner callback not set\")","                runner = self.create_runner(db_spec)","                result = runner.run()","                ","                # Write job manifest","                job_root = compute_job_artifacts_root(self.artifacts_root, self.batch_id, job_id)","                manifest = self._build_job_manifest(job_id, wizard_spec, result)","                manifest_with_hash = write_job_manifest(job_root, manifest)","                ","                # Mark as done in DB","                mark_done(self.db_path, job_id)","                ","                # Update record","                record.per_job_states[job_id] = JobExecutionState.SUCCESS","                record.counts[\"running\"] -= 1","                record.counts[\"done\"] += 1","                ","                # Collect job entry for batch index","                job_entries.append({","                    \"job_id\": job_id,","                    \"manifest_hash\": manifest_with_hash[\"manifest_hash\"],","                    \"manifest_path\": str((job_root / \"manifest.json\").relative_to(self.artifacts_root)),","                })","                ","            except Exception as e:","                # Mark as failed","                mark_failed(self.db_path, job_id, error=str(e))","                record.per_job_states[job_id] = JobExecutionState.FAILED","                record.counts[\"running\"] -= 1","                record.counts[\"failed\"] += 1","                # Still create a minimal manifest for failed job","                job_root = compute_job_artifacts_root(self.artifacts_root, self.batch_id, job_id)","                manifest = self._build_failed_job_manifest(job_id, wizard_spec, str(e))","                manifest_with_hash = write_job_manifest(job_root, manifest)","                job_entries.append({","                    \"job_id\": job_id,","                    \"manifest_hash\": manifest_with_hash[\"manifest_hash\"],","                    \"manifest_path\": str((job_root / \"manifest.json\").relative_to(self.artifacts_root)),","                    \"error\": str(e),","                })","            ","            self._update_record(self.batch_id, record)","        ","        # Determine final batch state","        if record.counts[\"failed\"] == 0:","            record.state = BatchExecutionState.DONE","        elif record.counts[\"done\"] > 0:","            record.state = BatchExecutionState.PARTIAL_FAILED","        else:","            record.state = BatchExecutionState.FAILED","        ","        # Build and write batch index","        batch_root = self.artifacts_root / self.batch_id","        index = build_batch_index(self.artifacts_root, self.batch_id, job_entries)","        index_with_hash = write_batch_index(batch_root, index)","        ","        record.artifact_index_path = str(batch_root / \"index.json\")","        record.updated_at = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())","        self._update_record(self.batch_id, record)","        ","        # Write final record","        self._write_execution_record(self.batch_id, record)","        ","        return {","            \"batch_id\": self.batch_id,","            \"state\": record.state,","            \"counts\": record.counts,","            \"artifact_index_path\": record.artifact_index_path,","            \"index_hash\": index_with_hash.get(\"index_hash\"),","        }","    ","    def retry_failed(self, artifacts_root: Path) -> None:","        \"\"\"Only rerun FAILED jobs, skip DONE, update state+index; forbidden if frozen.","        ","        Args:","            artifacts_root: Base artifacts directory.","        \"\"\"","        self.artifacts_root = artifacts_root","        # Minimal implementation for testing","    ","    def _build_job_manifest(self, job_id: str, wizard_spec: WizardJobSpec, result: dict) -> dict:","        \"\"\"Build job manifest from execution result.\"\"\"","        return {","            \"job_id\": job_id,","            \"spec\": wizard_spec.model_dump(mode=\"json\"),","            \"result\": result,","            \"created_at\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),","        }","    ","    def _build_failed_job_manifest(self, job_id: str, wizard_spec: WizardJobSpec, error: str) -> dict:","        \"\"\"Build job manifest for failed job.\"\"\"","        return {","            \"job_id\": job_id,","            \"spec\": wizard_spec.model_dump(mode=\"json\"),","            \"error\": error,","            \"created_at\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),","        }","    ","    def _update_record(self, batch_id: str, record: BatchExecutionRecord) -> None:","        \"\"\"Update execution record (in-memory).\"\"\"","        record.updated_at = time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())","        # In a real implementation, would persist to disk/db","    ","    def _write_execution_record(self, batch_id: str, record: BatchExecutionRecord) -> None:","        \"\"\"Write execution record to file.\"\"\"","        if self.artifacts_root is None:","            return  # No artifacts root, skip writing","        record_path = self.artifacts_root / batch_id / \"execution.json\"","        record_path.parent.mkdir(parents=True, exist_ok=True)","        data = {","            \"batch_id\": record.batch_id,","            \"state\": record.state,","            \"total_jobs\": record.total_jobs,","            \"counts\": record.counts,","            \"per_job_states\": record.per_job_states,","            \"artifact_index_path\": record.artifact_index_path,","            \"error_summary\": record.error_summary,","            \"created_at\": record.created_at,","            \"updated_at\": record.updated_at,","        }","        with open(record_path, \"w\", encoding=\"utf-8\") as f:","            json.dump(data, f, indent=2)","    ","    def _load_execution_record(self, batch_id: str) -> Optional[BatchExecutionRecord]:","        \"\"\"Load execution record from file.\"\"\"","        if self.artifacts_root is None:","            return None","        record_path = self.artifacts_root / batch_id / \"execution.json\"","        if not record_path.exists():","            return None","        with open(record_path, \"r\", encoding=\"utf-8\") as f:","            data = json.load(f)","        ","        return BatchExecutionRecord(","            batch_id=data[\"batch_id\"],","            state=BatchExecutionState(data[\"state\"]),","            total_jobs=data[\"total_jobs\"],","            counts=data[\"counts\"],","            per_job_states={k: JobExecutionState(v) for k, v in data[\"per_job_states\"].items()},","            artifact_index_path=data.get(\"artifact_index_path\"),","            error_summary=data.get(\"error_summary\"),","            created_at=data[\"created_at\"],","            updated_at=data[\"updated_at\"],","        )","","","# Import os for pid","import os","","","# Simplified top-level functions for testing and simple use cases","","def run_batch(batch_id: str, job_ids: list[str], artifacts_root: Path) -> BatchExecutor:","    executor = BatchExecutor(batch_id, job_ids)","    executor.run(artifacts_root)","    return executor","","","def retry_failed(batch_id: str, artifacts_root: Path) -> BatchExecutor:","    executor = BatchExecutor(batch_id, [])","    executor.retry_failed(artifacts_root)","    return executor","",""]}
{"type":"file_footer","path":"src/control/batch_execute.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/batch_index.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5583,"sha256":"ecb59f7d690273c0f4a95558522535105d83bee9449704b95c8319394fbc4a0d","total_lines":179,"chunk_count":1}
{"type":"file_chunk","path":"src/control/batch_index.py","chunk_index":0,"line_start":1,"line_end":179,"content":["","\"\"\"Batch-level index generation for Phase 14.","","Deterministic batch index that references job manifests and provides immutable artifact references.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Any","","from control.artifacts import canonical_json_bytes, sha256_bytes, write_json_atomic","","","def build_batch_index(","    artifacts_root: Path,","    batch_id: str,","    job_entries: list[dict],","    *,","    write: bool = True,",") -> dict:","    \"\"\"Build batch index dict from job entries and optionally write to disk.","    ","    The index contains:","      - batch_id","      - job_count","      - jobs: sorted list of job entries (by job_id)","      - index_hash: SHA256 of canonical JSON (excluding this field)","    ","    Each job entry must contain at least:","      - job_id","      - manifest_hash (SHA256 of job manifest)","      - manifest_path: relative path from artifacts_root to manifest.json","    ","    Args:","        artifacts_root: Base artifacts directory (e.g., outputs/artifacts).","        batch_id: Batch identifier.","        job_entries: List of job entry dicts (must contain job_id).","        write: If True (default), write index.json to artifacts_root / batch_id.","    ","    Returns:","        Batch index dict with index_hash.","    ","    Raises:","        ValueError: If duplicate job_id or missing required fields.","        OSError: If write fails.","    \"\"\"","    # Validate job entries","    seen = set()","    for entry in job_entries:","        job_id = entry.get(\"job_id\")","        if job_id is None:","            raise ValueError(\"job entry missing 'job_id'\")","        if job_id in seen:","            raise ValueError(f\"duplicate job_id in batch: {job_id}\")","        seen.add(job_id)","        ","        if \"manifest_hash\" not in entry:","            raise ValueError(f\"job entry {job_id} missing 'manifest_hash'\")","        if \"manifest_path\" not in entry:","            raise ValueError(f\"job entry {job_id} missing 'manifest_path'\")","    ","    # Sort entries by job_id for deterministic ordering","    sorted_entries = sorted(job_entries, key=lambda e: e[\"job_id\"])","    ","    # Build index dict (without hash)","    index_without_hash = {","        \"batch_id\": batch_id,","        \"job_count\": len(sorted_entries),","        \"jobs\": sorted_entries,","        \"schema_version\": \"1.0\",","    }","    ","    # Compute hash of canonical JSON (without hash field)","    canonical = canonical_json_bytes(index_without_hash)","    index_hash = sha256_bytes(canonical)","    ","    # Add hash field","    index = {**index_without_hash, \"index_hash\": index_hash}","    ","    # Write to disk if requested","    if write:","        batch_root = artifacts_root / batch_id","        write_batch_index(batch_root, index)","    ","    return index","","","def write_batch_index(batch_root: Path, index: dict) -> dict:","    \"\"\"Write batch index.json, ensuring it has a valid index_hash.","","    If the index already contains an 'index_hash' field, it is kept (but validated).","    Otherwise, the function computes the SHA256 of the canonical JSON bytes","    (excluding the hash field itself) and adds it. The index is then written to","    batch_root / \"index.json\".","","    Args:","        batch_root: Batch artifacts directory (must exist).","        index: Batch index dict (may contain 'index_hash').","","    Returns:","        Updated index dict with 'index_hash' field.","","    Raises:","        ValueError: If existing index_hash does not match computed hash.","        OSError: If directory does not exist or cannot write.","    \"\"\"","    # Ensure directory exists","    batch_root.mkdir(parents=True, exist_ok=True)","    ","    # Compute hash of canonical JSON (without hash field)","    index_without_hash = {k: v for k, v in index.items() if k != \"index_hash\"}","    canonical = canonical_json_bytes(index_without_hash)","    computed_hash = sha256_bytes(canonical)","    ","    # Determine final hash","    if \"index_hash\" in index:","        if index[\"index_hash\"] != computed_hash:","            raise ValueError(\"existing index_hash does not match computed hash\")","        index_hash = index[\"index_hash\"]","    else:","        index_hash = computed_hash","    ","    # Ensure index contains hash","    index_with_hash = {**index_without_hash, \"index_hash\": index_hash}","    ","    # Write index.json","    index_path = batch_root / \"index.json\"","    write_json_atomic(index_path, index_with_hash)","    ","    return index_with_hash","","","def read_batch_index(batch_root: Path) -> dict:","    \"\"\"Read batch index.json.","    ","    Args:","        batch_root: Batch artifacts directory.","    ","    Returns:","        Parsed index dict (including index_hash).","    ","    Raises:","        FileNotFoundError: If index.json does not exist.","        json.JSONDecodeError: If file is malformed.","    \"\"\"","    index_path = batch_root / \"index.json\"","    if not index_path.exists():","        raise FileNotFoundError(f\"batch index not found: {index_path}\")","    ","    data = json.loads(index_path.read_text(encoding=\"utf-8\"))","    return data","","","def validate_batch_index(index: dict) -> bool:","    \"\"\"Validate batch index integrity.","    ","    Checks that index_hash matches the SHA256 of the rest of the index.","    ","    Args:","        index: Batch index dict (must contain 'index_hash').","    ","    Returns:","        True if hash matches, False otherwise.","    \"\"\"","    if \"index_hash\" not in index:","        return False","    ","    # Extract hash and compute from rest","    provided_hash = index[\"index_hash\"]","    index_without_hash = {k: v for k, v in index.items() if k != \"index_hash\"}","    ","    canonical = canonical_json_bytes(index_without_hash)","    computed_hash = sha256_bytes(canonical)","    ","    return provided_hash == computed_hash","",""]}
{"type":"file_footer","path":"src/control/batch_index.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/batch_submit.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6755,"sha256":"c6d3d7ef1c2b82ab74c930154719842bb4e2fa123585ac6cdcc09f4c012cfb9c","total_lines":214,"chunk_count":2}
{"type":"file_chunk","path":"src/control/batch_submit.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Batch Job Submission for Phase 13.","","Deterministic batch_id computation and batch submission.","\"\"\"","","from __future__ import annotations","","import hashlib","import json","from pathlib import Path","from typing import Any","","from pydantic import BaseModel, ConfigDict, Field","","from control.job_spec import WizardJobSpec","from control.types import DBJobSpec","","# Import create_job for monkeypatching by tests","from control.jobs_db import create_job","","","class BatchSubmitRequest(BaseModel):","    \"\"\"Request body for batch job submission.\"\"\"","    ","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","    ","    jobs: list[WizardJobSpec] = Field(","        ...,","        description=\"List of JobSpec to submit\"","    )","","","class BatchSubmitResponse(BaseModel):","    \"\"\"Response for batch job submission.\"\"\"","    ","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","    ","    batch_id: str = Field(","        ...,","        description=\"Deterministic hash of normalized job list\"","    )","    ","    total_jobs: int = Field(","        ...,","        description=\"Number of jobs in batch\"","    )","    ","    job_ids: list[str] = Field(","        ...,","        description=\"Job IDs in same order as input jobs\"","    )","","","def compute_batch_id(jobs: list[WizardJobSpec]) -> str:","    \"\"\"Compute deterministic batch ID from list of JobSpec.","    ","    Args:","        jobs: List of JobSpec (order does not matter)","    ","    Returns:","        batch_id string with format \"batch-\" + sha1[:12]","    \"\"\"","    # Normalize each job to JSON-safe dict with sorted keys","    normalized = []","    for job in jobs:","        # Use model_dump with mode=\"json\" to handle dates","        d = job.model_dump(mode=\"json\", exclude_none=True)","        # Ensure params dict keys are sorted","        if \"params\" in d and isinstance(d[\"params\"], dict):","            d[\"params\"] = {k: d[\"params\"][k] for k in sorted(d[\"params\"])}","        normalized.append(d)","    ","    # Sort normalized list by its JSON representation to make order irrelevant","    normalized_sorted = sorted(","        normalized,","        key=lambda d: json.dumps(d, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=False)","    )","    ","    # Serialize with deterministic JSON","    data = json.dumps(","        normalized_sorted,","        sort_keys=True,","        separators=(\",\", \":\"),","        ensure_ascii=False,","    )","    ","    # Compute SHA1 hash","    sha1 = hashlib.sha1(data.encode(\"utf-8\")).hexdigest()","    return f\"batch-{sha1[:12]}\"","","","def wizard_to_db_jobspec(wizard_spec: WizardJobSpec, dataset_record: dict) -> DBJobSpec:","    \"\"\"Convert Wizard JobSpec to DB JobSpec.","    ","    Args:","        wizard_spec: Wizard JobSpec (config-only wizard output)","        dataset_record: Dataset registry record containing fingerprint","        ","    Returns:","        DBJobSpec for DB/worker runtime","        ","    Raises:","        ValueError: if data_fingerprint_sha256_40 is missing (DIRTY jobs are forbidden)","    \"\"\"","    # Use data1.dataset_id as dataset_id","    dataset_id = wizard_spec.data1.dataset_id","    ","    # Use season as outputs_root subdirectory (must match test expectation)","    outputs_root = f\"outputs/seasons/{wizard_spec.season}/runs\"","    ","    # Create config_snapshot that includes all wizard fields (JSON-safe)","    # Convert params from MappingProxyType to dict for JSON serialization","    params_dict = dict(wizard_spec.params)","    config_snapshot = {","        \"season\": wizard_spec.season,","        \"data1\": wizard_spec.data1.model_dump(mode=\"json\"),","        \"data2\": wizard_spec.data2.model_dump(mode=\"json\") if wizard_spec.data2 else None,","        \"strategy_id\": wizard_spec.strategy_id,","        \"params\": params_dict,","        \"wfs\": wizard_spec.wfs.model_dump(mode=\"json\"),","    }","    ","    # Compute config_hash from snapshot (deterministic)","    config_hash = hashlib.sha1(","        json.dumps(config_snapshot, sort_keys=True, separators=(\",\", \":\")).encode(\"utf-8\")","    ).hexdigest()[:16]","    ","    # Get fingerprint from dataset registry","    # Try fingerprint_sha256_40 first, then normalized_sha256_40","    fp = dataset_record.get(\"fingerprint_sha256_40\") or dataset_record.get(\"normalized_sha256_40\")","    if not fp:","        raise ValueError(\"data_fingerprint_sha256_40 is required; DIRTY jobs are forbidden\")","    ","    return DBJobSpec(","        season=wizard_spec.season,","        dataset_id=dataset_id,","        outputs_root=outputs_root,","        config_snapshot=config_snapshot,","        config_hash=config_hash,","        data_fingerprint_sha256_40=fp,","        created_by=\"wizard_batch\",","    )","","","def submit_batch(","    db_path: Path,","    req: BatchSubmitRequest,","    dataset_index: dict | None = None",") -> BatchSubmitResponse:","    \"\"\"Submit a batch of jobs.","    ","    Args:","        db_path: Path to SQLite database","        req: Batch submit request","        dataset_index: Optional dataset index dict mapping dataset_id to record.","                      If not provided, will attempt to load from cache.","    ","    Returns:","        BatchSubmitResponse with batch_id and job_ids","    ","    Raises:","        ValueError: if any job fails validation or fingerprint missing","        RuntimeError: if DB submission fails","    \"\"\"","    # Validate jobs list not empty","    if len(req.jobs) == 0:","        raise ValueError(\"jobs list cannot be empty\")","    ","    # Cap at 1000 jobs (default cap)","    cap = 1000","    if len(req.jobs) > cap:","        raise ValueError(f\"jobs list exceeds maximum allowed ({cap})\")","    ","    # Compute batch_id","    batch_id = compute_batch_id(req.jobs)","    ","    # Convert each job to DB JobSpec and submit","    job_ids = []","    for job in req.jobs:","        # Get dataset record for fingerprint","        dataset_id = job.data1.dataset_id","        dataset_record = None","        ","        if dataset_index and dataset_id in dataset_index:","            dataset_record = dataset_index[dataset_id]","        else:","            # Try to load from cache","            try:","                from control.api import load_dataset_index","                idx = load_dataset_index()","                # Find dataset by id","                for ds in idx.datasets:","                    if ds.id == dataset_id:","                        dataset_record = ds.model_dump(mode=\"json\")","                        break","            except Exception:","                # If cannot load dataset index, raise error","                raise ValueError(f\"Cannot load dataset record for {dataset_id}; fingerprint required\")","        "]}
{"type":"file_chunk","path":"src/control/batch_submit.py","chunk_index":1,"line_start":201,"line_end":214,"content":["        if not dataset_record:","            raise ValueError(f\"Dataset {dataset_id} not found in registry; fingerprint required\")","        ","        db_spec = wizard_to_db_jobspec(job, dataset_record)","        job_id = create_job(db_path, db_spec)","        job_ids.append(job_id)","    ","    return BatchSubmitResponse(","        batch_id=batch_id,","        total_jobs=len(job_ids),","        job_ids=job_ids","    )","",""]}
{"type":"file_footer","path":"src/control/batch_submit.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/build_context.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1898,"sha256":"261bdf4f18bbd4cc8bf3c71de00635a8474dac2999888fe6ad4a5ec0453ff0d7","total_lines":63,"chunk_count":1}
{"type":"file_chunk","path":"src/control/build_context.py","chunk_index":0,"line_start":1,"line_end":63,"content":["from __future__ import annotations","","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Optional, Literal","","","BuildMode = Literal[\"FULL\", \"INCREMENTAL\"]","","","@dataclass(frozen=True, slots=True)","class BuildContext:","    \"\"\"","    Contract-only build context.","","    Rules:","    - resolver / runner ä¸å¾—è‡ªè¡Œå°‹æ‰¾ txt","    - txt_path å¿…é ˆç”± caller æä¾›","    - ä¸åšä»»ä½• filesystem æŽƒæ","    \"\"\"","","    txt_path: Path","    mode: BuildMode","    outputs_root: Path","    build_bars_if_missing: bool = False","","    season: str = \"\"","    dataset_id: str = \"\"","    strategy_id: str = \"\"","    config_snapshot: Optional[dict[str, Any]] = None","    config_hash: str = \"\"","    created_by: str = \"b5c\"","    data_fingerprint_sha1: str = \"\"","","    def __post_init__(self) -> None:","        object.__setattr__(self, \"txt_path\", Path(self.txt_path))","        object.__setattr__(self, \"outputs_root\", Path(self.outputs_root))","","        if self.mode not in (\"FULL\", \"INCREMENTAL\"):","            raise ValueError(f\"Invalid mode: {self.mode}\")","","        if not self.txt_path.exists():","            raise FileNotFoundError(f\"txt_path ä¸å­˜åœ¨: {self.txt_path}\")","","        if self.txt_path.suffix.lower() != \".txt\":","            raise ValueError(\"txt_path must be a .txt file\")","","    def ensure_config_snapshot(self) -> dict[str, Any]:","        return self.config_snapshot or {}","","    def to_build_shared_kwargs(self) -> dict[str, Any]:","        \"\"\"Return kwargs suitable for build_shared.\"\"\"","        return {","            \"txt_path\": self.txt_path,","            \"mode\": self.mode,","            \"outputs_root\": self.outputs_root,","            \"save_fingerprint\": True,","            \"generated_at_utc\": None,","            \"build_bars\": self.build_bars_if_missing,","            \"build_features\": False,  # will be overridden by caller","            \"feature_registry\": None,","            \"tfs\": [15, 30, 60, 120, 240],","        }"]}
{"type":"file_footer","path":"src/control/build_context.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/data_build.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12077,"sha256":"a4cef41080167d189ddaed6ce294a079bacae46e4e60ceab309d47c327373b7f","total_lines":348,"chunk_count":2}
{"type":"file_chunk","path":"src/control/data_build.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"TXT to Parquet Build Pipeline.","","Provides deterministic conversion of raw TXT files to Parquet format","for backtest performance and schema stability.","\"\"\"","","from __future__ import annotations","","import hashlib","import json","import shutil","import tempfile","import time","from dataclasses import dataclass","from datetime import datetime, timezone","from pathlib import Path","from typing import List, Optional, Dict, Any","import pandas as pd","","from data.raw_ingest import ingest_raw_txt, RawIngestResult","","","@dataclass(frozen=True)","class BuildParquetRequest:","    \"\"\"Request to build Parquet from TXT.\"\"\"","    dataset_id: str","    force: bool               # rebuild even if up-to-date","    deep_validate: bool       # optional schema validation after build","    reason: str               # for audit/logging","","","@dataclass(frozen=True)","class BuildParquetResult:","    \"\"\"Result of Parquet build operation.\"\"\"","    ok: bool","    dataset_id: str","    started_utc: str","    finished_utc: str","    txt_signature: str","    parquet_signature: str","    parquet_paths: List[str]","    rows_written: Optional[int]","    notes: List[str]","    error: Optional[str]","","","def _compute_file_signature(file_path: Path, max_size_mb: int = 50) -> str:","    \"\"\"Compute signature for a file.","    ","    For small files (< max_size_mb): compute sha256","    For large files: use stat-hash (path + size + mtime)","    \"\"\"","    try:","        if not file_path.exists():","            return \"missing\"","        ","        stat = file_path.stat()","        file_size_mb = stat.st_size / (1024 * 1024)","        ","        if file_size_mb < max_size_mb:","            # Small file: compute actual hash","            hasher = hashlib.sha256()","            with open(file_path, 'rb') as f:","                # Read in chunks to handle large files","                chunk_size = 8192","                while chunk := f.read(chunk_size):","                    hasher.update(chunk)","            return f\"sha256:{hasher.hexdigest()[:16]}\"","        else:","            # Large file: use stat-hash","            return f\"stat:{file_path.name}:{stat.st_size}:{stat.st_mtime}\"","    except Exception as e:","        return f\"error:{str(e)[:50]}\"","","","def _get_txt_files_for_dataset(dataset_id: str) -> List[Path]:","    \"\"\"Get TXT files required for a dataset.","    ","    This is a placeholder implementation. In a real system, this would","    look up the dataset descriptor to find TXT source paths.","    ","    For now, we'll use a simple mapping based on dataset ID pattern.","    \"\"\"","    # Simple mapping: dataset_id -> txt file pattern","    # In a real implementation, this would come from dataset registry","    base_dir = Path(\"data/raw\")","    ","    # Extract symbol from dataset_id (simplified)","    parts = dataset_id.split('_')","    if len(parts) >= 2 and '.' in parts[0]:","        symbol = parts[0].split('.')[1]  # e.g., \"CME.MNQ\" -> \"MNQ\"","    else:","        symbol = \"unknown\"","    ","    # Look for TXT files","    txt_files = []","    if base_dir.exists():","        for txt_path in base_dir.glob(f\"**/*{symbol}*.txt\"):","            txt_files.append(txt_path)","    ","    # If no files found, create a dummy path for testing","    if not txt_files:","        dummy_path = base_dir / f\"{dataset_id}.txt\"","        txt_files.append(dummy_path)","    ","    return txt_files","","","def _get_parquet_output_path(dataset_id: str) -> Path:","    \"\"\"Get output path for Parquet files.","    ","    Deterministic output paths inside dataset-managed folder.","    \"\"\"","    # Create parquet directory structure","    parquet_root = Path(\"outputs/parquet\")","    ","    # Clean dataset_id for filesystem","    safe_id = dataset_id.replace('/', '_').replace('\\\\', '_').replace(':', '_')","    ","    # Create partitioned structure: parquet/<dataset_id>/data.parquet","    output_dir = parquet_root / safe_id","    output_dir.mkdir(parents=True, exist_ok=True)","    ","    return output_dir / \"data.parquet\"","","","def _build_parquet_from_txt_impl(","    txt_files: List[Path],","    parquet_path: Path,","    force: bool,","    deep_validate: bool",") -> BuildParquetResult:","    \"\"\"Core implementation of TXT to Parquet conversion.\"\"\"","    started_utc = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","    notes = []","    ","    try:","        # 1. Check if TXT files exist","        missing_txt = [str(p) for p in txt_files if not p.exists()]","        if missing_txt:","            return BuildParquetResult(","                ok=False,","                dataset_id=\"unknown\",","                started_utc=started_utc,","                finished_utc=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","                txt_signature=\"\",","                parquet_signature=\"\",","                parquet_paths=[],","                rows_written=None,","                notes=notes,","                error=f\"Missing TXT files: {missing_txt}\"","            )","        ","        # 2. Compute TXT signature","        txt_signatures = []","        for txt_file in txt_files:","            sig = _compute_file_signature(txt_file)","            txt_signatures.append(f\"{txt_file.name}:{sig}\")","        txt_signature = \"|\".join(txt_signatures)","        ","        # 3. Check if Parquet already exists and is up-to-date","        parquet_exists = parquet_path.exists()","        parquet_signature = \"\"","        ","        if parquet_exists:","            parquet_signature = _compute_file_signature(parquet_path)","            # Simple up-to-date check: compare signatures","            # In a real implementation, this would compare metadata","            if not force:","                # Check if we should skip rebuild","                notes.append(f\"Parquet exists at {parquet_path}\")","                # For now, we'll always rebuild if force=False but parquet exists","                # In a real system, we'd compare content hashes","        ","        # 4. Ingest TXT files","        all_dfs = []","        for txt_file in txt_files:","            try:","                result: RawIngestResult = ingest_raw_txt(txt_file)","                df = result.df","                ","                # Convert ts_str to datetime","                df['timestamp'] = pd.to_datetime(df['ts_str'], format='%Y/%m/%d %H:%M:%S', errors='coerce')","                df = df.drop(columns=['ts_str'])","                ","                # Reorder columns","                df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]","                ","                all_dfs.append(df)","                notes.append(f\"Ingested {txt_file.name}: {len(df)} rows\")","            except Exception as e:","                return BuildParquetResult(","                    ok=False,","                    dataset_id=\"unknown\",","                    started_utc=started_utc,","                    finished_utc=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","                    txt_signature=txt_signature,","                    parquet_signature=parquet_signature,","                    parquet_paths=[],","                    rows_written=None,"]}
{"type":"file_chunk","path":"src/control/data_build.py","chunk_index":1,"line_start":201,"line_end":348,"content":["                    notes=notes,","                    error=f\"Failed to ingest {txt_file}: {e}\"","                )","        ","        # 5. Combine DataFrames","        if not all_dfs:","            return BuildParquetResult(","                ok=False,","                dataset_id=\"unknown\",","                started_utc=started_utc,","                finished_utc=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","                txt_signature=txt_signature,","                parquet_signature=parquet_signature,","                parquet_paths=[],","                rows_written=None,","                notes=notes,","                error=\"No data ingested from TXT files\"","            )","        ","        combined_df = pd.concat(all_dfs, ignore_index=True)","        ","        # 6. Sort by timestamp","        combined_df = combined_df.sort_values('timestamp')","        ","        # 7. Write to Parquet with atomic safety","        temp_dir = tempfile.mkdtemp(prefix=\"parquet_build_\")","        try:","            temp_path = Path(temp_dir) / \"temp.parquet\"","            combined_df.to_parquet(","                temp_path,","                engine='pyarrow',","                compression='snappy',","                index=False","            )","            ","            # Atomic rename","            parquet_path.parent.mkdir(parents=True, exist_ok=True)","            shutil.move(str(temp_path), str(parquet_path))","            ","            notes.append(f\"Written Parquet to {parquet_path}\")","        finally:","            shutil.rmtree(temp_dir, ignore_errors=True)","        ","        # 8. Compute new Parquet signature","        new_parquet_signature = _compute_file_signature(parquet_path)","        ","        # 9. Deep validation if requested","        if deep_validate:","            try:","                # Read back and validate schema","                validate_df = pd.read_parquet(parquet_path)","                expected_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']","                if list(validate_df.columns) != expected_cols:","                    notes.append(f\"Warning: Schema mismatch. Expected {expected_cols}, got {list(validate_df.columns)}\")","                else:","                    notes.append(\"Deep validation passed\")","            except Exception as e:","                notes.append(f\"Deep validation warning: {e}\")","        ","        finished_utc = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","        ","        return BuildParquetResult(","            ok=True,","            dataset_id=\"unknown\",","            started_utc=started_utc,","            finished_utc=finished_utc,","            txt_signature=txt_signature,","            parquet_signature=new_parquet_signature,","            parquet_paths=[str(parquet_path)],","            rows_written=len(combined_df),","            notes=notes,","            error=None","        )","        ","    except Exception as e:","        finished_utc = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","        return BuildParquetResult(","            ok=False,","            dataset_id=\"unknown\",","            started_utc=started_utc,","            finished_utc=finished_utc,","            txt_signature=\"\",","            parquet_signature=\"\",","            parquet_paths=[],","            rows_written=None,","            notes=notes,","            error=f\"Build failed: {e}\"","        )","","","def build_parquet_from_txt(req: BuildParquetRequest) -> BuildParquetResult:","    \"\"\"Convert raw TXT to Parquet for the given dataset_id.","    ","    Requirements:","    - Deterministic output paths inside dataset-managed folder","    - Safe atomic writes: write to temp then rename","    - Up-to-date logic:","        - compute txt_signature (stat-hash or partial hash) from required TXT files","        - compute existing parquet_signature (from parquet files or metadata)","        - if not force and signatures match => no-op but ok=True","    - Must never mutate season artifacts.","    \"\"\"","    # Get TXT files for dataset","    txt_files = _get_txt_files_for_dataset(req.dataset_id)","    ","    # Get output path","    parquet_path = _get_parquet_output_path(req.dataset_id)","    ","    # Update result with actual dataset_id","    result = _build_parquet_from_txt_impl(txt_files, parquet_path, req.force, req.deep_validate)","    ","    # Create a new result with the correct dataset_id","    return BuildParquetResult(","        ok=result.ok,","        dataset_id=req.dataset_id,","        started_utc=result.started_utc,","        finished_utc=result.finished_utc,","        txt_signature=result.txt_signature,","        parquet_signature=result.parquet_signature,","        parquet_paths=result.parquet_paths,","        rows_written=result.rows_written,","        notes=result.notes,","        error=result.error","    )","","","# Simple test function","def test_build_parquet() -> None:","    \"\"\"Test the build_parquet_from_txt function.\"\"\"","    print(\"Testing build_parquet_from_txt...\")","    ","    # Create a dummy request","    req = BuildParquetRequest(","        dataset_id=\"test_dataset\",","        force=True,","        deep_validate=False,","        reason=\"test\"","    )","    ","    result = build_parquet_from_txt(req)","    print(f\"Result: {result.ok}\")","    print(f\"Notes: {result.notes}\")","    if result.error:","        print(f\"Error: {result.error}\")","","","if __name__ == \"__main__\":","    test_build_parquet()"]}
{"type":"file_footer","path":"src/control/data_build.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/data_snapshot.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7520,"sha256":"652fedab102ddc2aa8fdcfeb482f3cf21266baa2f7acb98110da2a193a12bd49","total_lines":239,"chunk_count":2}
{"type":"file_chunk","path":"src/control/data_snapshot.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 16.5: Data Snapshot Core (controlled mutation, deterministic).","","Contracts:","- Writes only under outputs/datasets/snapshots/{snapshot_id}/","- Deterministic normalization & checksums","- Immutable snapshots (never overwrite)","- Timezoneâ€‘aware UTC timestamps","\"\"\"","","from __future__ import annotations","","import hashlib","import json","import shutil","import tempfile","from datetime import datetime, timezone","from pathlib import Path","from typing import Any","","from contracts.data.snapshot_models import SnapshotMetadata, SnapshotStats","from control.artifacts import canonical_json_bytes, compute_sha256, write_atomic_json","","","def write_json_atomic_any(path: Path, obj: Any) -> None:","    \"\"\"","    Atomically write any JSONâ€‘serializable object to file.","","    Uses the same atomic rename technique as write_atomic_json.","    \"\"\"","    path.parent.mkdir(parents=True, exist_ok=True)","    with tempfile.NamedTemporaryFile(","        mode=\"w\",","        encoding=\"utf-8\",","        dir=path.parent,","        prefix=f\".{path.name}.tmp.\",","        delete=False,","    ) as f:","        json.dump(","            obj,","            f,","            sort_keys=True,","            ensure_ascii=False,","            separators=(\",\", \":\"),","            allow_nan=False,","        )","        tmp_path = Path(f.name)","    try:","        tmp_path.replace(path)","    except Exception:","        tmp_path.unlink(missing_ok=True)","        raise","","","def compute_snapshot_id(","    raw_bars: list[dict[str, Any]],","    symbol: str,","    timeframe: str,","    transform_version: str = \"v1\",",") -> str:","    \"\"\"","    Deterministic snapshot identifier.","","    Format: {symbol}_{timeframe}_{raw_sha256[:12]}_{transform_version}","    \"\"\"","    # Compute raw SHA256 from canonical JSON of raw_bars","    raw_canonical = canonical_json_bytes(raw_bars)","    raw_sha256 = compute_sha256(raw_canonical)","    raw_prefix = raw_sha256[:12]","","    # Normalize symbol and timeframe (remove special chars)","    symbol_norm = symbol.replace(\"/\", \"_\").upper()","    tf_norm = timeframe.replace(\"/\", \"_\").lower()","    return f\"{symbol_norm}_{tf_norm}_{raw_prefix}_{transform_version}\"","","","def normalize_bars(","    raw_bars: list[dict[str, Any]],","    transform_version: str = \"v1\",",") -> tuple[list[dict[str, Any]], str]:","    \"\"\"","    Normalize raw bars to canonical form (deterministic).","","    Returns:","        (normalized_bars, normalized_sha256)","    \"\"\"","    # Ensure each bar has required fields","    required = {\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"}","    normalized = []","    for bar in raw_bars:","        # Validate types","        ts = bar[\"timestamp\"]","        # Ensure timestamp is ISO 8601 string; if not, attempt conversion","        if isinstance(ts, datetime):","            ts = ts.isoformat().replace(\"+00:00\", \"Z\")","        elif not isinstance(ts, str):","            raise ValueError(f\"Invalid timestamp type: {type(ts)}\")","","        # Ensure numeric fields are float","        open_ = float(bar[\"open\"])","        high = float(bar[\"high\"])","        low = float(bar[\"low\"])","        close = float(bar[\"close\"])","        volume = float(bar[\"volume\"]) if isinstance(bar[\"volume\"], (int, float)) else 0.0","","        # Build canonical dict with fixed key order","        canonical = {","            \"timestamp\": ts,","            \"open\": open_,","            \"high\": high,","            \"low\": low,","            \"close\": close,","            \"volume\": volume,","        }","        normalized.append(canonical)","","    # Sort by timestamp ascending","    normalized.sort(key=lambda b: b[\"timestamp\"])","","    # Compute SHA256 of canonical JSON","    canonical_bytes = canonical_json_bytes(normalized)","    sha = compute_sha256(canonical_bytes)","    return normalized, sha","","","def compute_stats(normalized_bars: list[dict[str, Any]]) -> SnapshotStats:","    \"\"\"Compute basic statistics from normalized bars.\"\"\"","    if not normalized_bars:","        raise ValueError(\"normalized_bars cannot be empty\")","","    timestamps = [b[\"timestamp\"] for b in normalized_bars]","    lows = [b[\"low\"] for b in normalized_bars]","    highs = [b[\"high\"] for b in normalized_bars]","    volumes = [b[\"volume\"] for b in normalized_bars]","","    return SnapshotStats(","        count=len(normalized_bars),","        min_timestamp=min(timestamps),","        max_timestamp=max(timestamps),","        min_price=min(lows),","        max_price=max(highs),","        total_volume=sum(volumes),","    )","","","def create_snapshot(","    snapshots_root: Path,","    raw_bars: list[dict[str, Any]],","    symbol: str,","    timeframe: str,","    transform_version: str = \"v1\",",") -> SnapshotMetadata:","    \"\"\"","    Controlledâ€‘mutation: create a data snapshot.","","    Writes only under snapshots_root/{snapshot_id}/","    Deterministic normalization & checksums.","    \"\"\"","    if not raw_bars:","        raise ValueError(\"raw_bars cannot be empty\")","","    # 1. Compute raw SHA256","    raw_canonical = canonical_json_bytes(raw_bars)","    raw_sha256 = compute_sha256(raw_canonical)","","    # 2. Normalize bars","    normalized_bars, normalized_sha256 = normalize_bars(raw_bars, transform_version)","","    # 3. Compute snapshot ID","    snapshot_id = compute_snapshot_id(raw_bars, symbol, timeframe, transform_version)","","    # 4. Create snapshot directory (atomic)","    snapshot_dir = snapshots_root / snapshot_id","    if snapshot_dir.exists():","        raise FileExistsError(","            f\"Snapshot {snapshot_id} already exists; immutable rule violated\"","        )","","    # Write files via temporary directory to ensure atomicity","    with tempfile.TemporaryDirectory(prefix=f\"snapshot_{snapshot_id}_\") as tmp:","        tmp_path = Path(tmp)","","        # raw.json","        raw_path = tmp_path / \"raw.json\"","        write_json_atomic_any(raw_path, raw_bars)","","        # normalized.json","        norm_path = tmp_path / \"normalized.json\"","        write_json_atomic_any(norm_path, normalized_bars)","","        # Compute stats","        stats = compute_stats(normalized_bars)","","        # manifest.json (without manifest_sha256 field)","        manifest = {","            \"snapshot_id\": snapshot_id,","            \"symbol\": symbol,","            \"timeframe\": timeframe,","            \"transform_version\": transform_version,"]}
{"type":"file_chunk","path":"src/control/data_snapshot.py","chunk_index":1,"line_start":201,"line_end":239,"content":["            \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","            \"raw_sha256\": raw_sha256,","            \"normalized_sha256\": normalized_sha256,","            \"stats\": stats.model_dump(mode=\"json\"),","        }","        manifest_path = tmp_path / \"manifest.json\"","        write_json_atomic_any(manifest_path, manifest)","","        # Compute manifest SHA256 (hash of manifest without manifest_sha256)","        manifest_canonical = canonical_json_bytes(manifest)","        manifest_sha256 = compute_sha256(manifest_canonical)","","        # Add manifest_sha256 to manifest","        manifest[\"manifest_sha256\"] = manifest_sha256","        write_json_atomic_any(manifest_path, manifest)","","        # Create snapshot directory","        snapshot_dir.mkdir(parents=True, exist_ok=False)","","        # Move files into place (atomic rename)","        shutil.move(str(raw_path), str(snapshot_dir / \"raw.json\"))","        shutil.move(str(norm_path), str(snapshot_dir / \"normalized.json\"))","        shutil.move(str(manifest_path), str(snapshot_dir / \"manifest.json\"))","","    # Build metadata","    meta = SnapshotMetadata(","        snapshot_id=snapshot_id,","        symbol=symbol,","        timeframe=timeframe,","        transform_version=transform_version,","        created_at=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        raw_sha256=raw_sha256,","        normalized_sha256=normalized_sha256,","        manifest_sha256=manifest_sha256,","        stats=stats,","    )","    return meta","",""]}
{"type":"file_footer","path":"src/control/data_snapshot.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/dataset_catalog.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5273,"sha256":"0672985364c7013b25c7bb91ae24be9e275e1887e31e663c3985e19a02d70fac","total_lines":166,"chunk_count":1}
{"type":"file_chunk","path":"src/control/dataset_catalog.py","chunk_index":0,"line_start":1,"line_end":166,"content":["\"\"\"Dataset Catalog for M1 Wizard.","","Provides dataset listing and filtering capabilities for the wizard UI.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import List, Optional","","from data.dataset_registry import DatasetIndex, DatasetRecord","","","class DatasetCatalog:","    \"\"\"Catalog for available datasets.\"\"\"","    ","    def __init__(self, index_path: Optional[Path] = None):","        \"\"\"Initialize catalog with dataset index.","        ","        Args:","            index_path: Path to dataset index JSON file. If None, uses default.","        \"\"\"","        self.index_path = index_path or Path(\"outputs/datasets/datasets_index.json\")","        self._index: Optional[DatasetIndex] = None","    ","    def load_index(self) -> DatasetIndex:","        \"\"\"Load dataset index from file.\"\"\"","        if not self.index_path.exists():","            raise FileNotFoundError(","                f\"Dataset index not found at {self.index_path}. \"","                \"Please run: python scripts/build_dataset_registry.py\"","            )","        ","        data = json.loads(self.index_path.read_text(encoding=\"utf-8\"))","        self._index = DatasetIndex.model_validate(data)","        return self._index","    ","    @property","    def index(self) -> DatasetIndex:","        \"\"\"Get dataset index (loads if not already loaded).\"\"\"","        if self._index is None:","            self.load_index()","        return self._index","    ","    def list_datasets(self) -> List[DatasetRecord]:","        \"\"\"List all available datasets.\"\"\"","        return self.index.datasets","    ","    def get_dataset(self, dataset_id: str) -> Optional[DatasetRecord]:","        \"\"\"Get dataset by ID.\"\"\"","        for dataset in self.index.datasets:","            if dataset.id == dataset_id:","                return dataset","        return None","    ","    def filter_by_symbol(self, symbol: str) -> List[DatasetRecord]:","        \"\"\"Filter datasets by symbol.\"\"\"","        return [d for d in self.index.datasets if d.symbol == symbol]","    ","    def filter_by_timeframe(self, timeframe: str) -> List[DatasetRecord]:","        \"\"\"Filter datasets by timeframe.\"\"\"","        return [d for d in self.index.datasets if d.timeframe == timeframe]","    ","    def filter_by_exchange(self, exchange: str) -> List[DatasetRecord]:","        \"\"\"Filter datasets by exchange.\"\"\"","        return [d for d in self.index.datasets if d.exchange == exchange]","    ","    def get_unique_symbols(self) -> List[str]:","        \"\"\"Get list of unique symbols.\"\"\"","        return sorted({d.symbol for d in self.index.datasets})","    ","    def get_unique_timeframes(self) -> List[str]:","        \"\"\"Get list of unique timeframes.\"\"\"","        return sorted({d.timeframe for d in self.index.datasets})","    ","    def get_unique_exchanges(self) -> List[str]:","        \"\"\"Get list of unique exchanges.\"\"\"","        return sorted({d.exchange for d in self.index.datasets})","    ","    def validate_dataset_selection(","        self,","        dataset_id: str,","        start_date: Optional[str] = None,","        end_date: Optional[str] = None","    ) -> bool:","        \"\"\"Validate dataset selection with optional date range.","        ","        Args:","            dataset_id: Dataset ID to validate","            start_date: Optional start date (YYYY-MM-DD)","            end_date: Optional end date (YYYY-MM-DD)","            ","        Returns:","            True if valid, False otherwise","        \"\"\"","        dataset = self.get_dataset(dataset_id)","        if dataset is None:","            return False","        ","        # TODO: Add date range validation if needed","        return True","    ","    def list_dataset_ids(self) -> List[str]:","        \"\"\"Get list of all dataset IDs.","        ","        Returns:","            List of dataset IDs sorted alphabetically","        \"\"\"","        return sorted([d.id for d in self.index.datasets])","    ","    def describe_dataset(self, dataset_id: str) -> Optional[DatasetRecord]:","        \"\"\"Get dataset descriptor by ID.","        ","        Args:","            dataset_id: Dataset ID to describe","            ","        Returns:","            DatasetRecord if found, None otherwise","        \"\"\"","        return self.get_dataset(dataset_id)","","","# Singleton instance for easy access","_catalog_instance: Optional[DatasetCatalog] = None","","def get_dataset_catalog() -> DatasetCatalog:","    \"\"\"Get singleton dataset catalog instance.\"\"\"","    global _catalog_instance","    if _catalog_instance is None:","        _catalog_instance = DatasetCatalog()","    return _catalog_instance","","","# Public API functions for registry access","def list_dataset_ids() -> List[str]:","    \"\"\"Public API: Get list of all dataset IDs.","    ","    Returns:","        List of dataset IDs sorted alphabetically","    \"\"\"","    catalog = get_dataset_catalog()","    return catalog.list_dataset_ids()","","","def list_datasets() -> List[DatasetRecord]:","    \"\"\"Public API: Get list of all dataset records.","    ","    Returns:","        List of DatasetRecord objects","    \"\"\"","    catalog = get_dataset_catalog()","    return catalog.list_datasets()","","","def describe_dataset(dataset_id: str) -> Optional[DatasetRecord]:","    \"\"\"Public API: Get dataset descriptor by ID.","    ","    Args:","        dataset_id: Dataset ID to describe","        ","    Returns:","        DatasetRecord if found, None otherwise","    \"\"\"","    catalog = get_dataset_catalog()","    return catalog.describe_dataset(dataset_id)"]}
{"type":"file_footer","path":"src/control/dataset_catalog.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/dataset_descriptor.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5003,"sha256":"e28a3aac75bb1fa6cd8fc6fc9ab4d44fe798b4003673cf9aee9d3ffc0df12031","total_lines":176,"chunk_count":1}
{"type":"file_chunk","path":"src/control/dataset_descriptor.py","chunk_index":0,"line_start":1,"line_end":176,"content":["\"\"\"Dataset Descriptor with TXT and Parquet locations.","","Extends the basic DatasetRecord with information about","raw TXT sources and derived Parquet outputs.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, field","from pathlib import Path","from typing import List, Optional, Dict, Any","","from data.dataset_registry import DatasetRecord","","","@dataclass(frozen=True)","class DatasetDescriptor:","    \"\"\"Extended dataset descriptor with TXT and Parquet information.\"\"\"","    ","    # Core dataset info","    dataset_id: str","    base_record: DatasetRecord","    ","    # TXT source information","    txt_root: str","    txt_required_paths: List[str]","    ","    # Parquet output information","    parquet_root: str","    parquet_expected_paths: List[str]","    ","    # Metadata","    kind: str = \"unknown\"","    notes: List[str] = field(default_factory=list)","    ","    @property","    def symbol(self) -> str:","        \"\"\"Get symbol from base record.\"\"\"","        return self.base_record.symbol","    ","    @property","    def exchange(self) -> str:","        \"\"\"Get exchange from base record.\"\"\"","        return self.base_record.exchange","    ","    @property","    def timeframe(self) -> str:","        \"\"\"Get timeframe from base record.\"\"\"","        return self.base_record.timeframe","    ","    @property","    def path(self) -> str:","        \"\"\"Get path from base record.\"\"\"","        return self.base_record.path","    ","    @property","    def start_date(self) -> str:","        \"\"\"Get start date from base record.\"\"\"","        return self.base_record.start_date.isoformat()","    ","    @property","    def end_date(self) -> str:","        \"\"\"Get end date from base record.\"\"\"","        return self.base_record.end_date.isoformat()","","","def create_descriptor_from_record(record: DatasetRecord) -> DatasetDescriptor:","    \"\"\"Create a DatasetDescriptor from a DatasetRecord.","    ","    This is a placeholder implementation that infers TXT and Parquet","    paths based on the dataset ID and record information.","    ","    In a real system, this would come from a configuration file or","    database lookup.","    \"\"\"","    dataset_id = record.id","    ","    # Infer TXT root and paths based on dataset ID pattern","    # Example: \"CME.MNQ.60m.2020-2024\" -> data/raw/CME/MNQ/*.txt","    parts = dataset_id.split('.')","    if len(parts) >= 2:","        exchange = parts[0]","        symbol = parts[1]","        txt_root = f\"data/raw/{exchange}/{symbol}\"","        txt_required_paths = [","            f\"{txt_root}/daily.txt\",","            f\"{txt_root}/intraday.txt\"","        ]","    else:","        txt_root = f\"data/raw/{dataset_id}\"","        txt_required_paths = [f\"{txt_root}/data.txt\"]","    ","    # Parquet output paths","    # Use outputs/parquet/<dataset_id>/data.parquet","    safe_id = dataset_id.replace('/', '_').replace('\\\\', '_').replace(':', '_')","    parquet_root = f\"outputs/parquet/{safe_id}\"","    parquet_expected_paths = [","        f\"{parquet_root}/data.parquet\"","    ]","    ","    # Determine kind based on timeframe","    timeframe = record.timeframe","    if timeframe.endswith('m'):","        kind = \"intraday\"","    elif timeframe.endswith('D'):","        kind = \"daily\"","    else:","        kind = \"unknown\"","    ","    return DatasetDescriptor(","        dataset_id=dataset_id,","        base_record=record,","        txt_root=txt_root,","        txt_required_paths=txt_required_paths,","        parquet_root=parquet_root,","        parquet_expected_paths=parquet_expected_paths,","        kind=kind,","        notes=[\"Auto-generated descriptor\"]","    )","","","def get_descriptor(dataset_id: str) -> Optional[DatasetDescriptor]:","    \"\"\"Get dataset descriptor by ID.","    ","    Args:","        dataset_id: Dataset ID to look up","        ","    Returns:","        DatasetDescriptor if found, None otherwise","    \"\"\"","    from control.dataset_catalog import describe_dataset","    ","    record = describe_dataset(dataset_id)","    if record is None:","        return None","    ","    return create_descriptor_from_record(record)","","","def list_descriptors() -> List[DatasetDescriptor]:","    \"\"\"List all dataset descriptors.","    ","    Returns:","        List of all DatasetDescriptor objects","    \"\"\"","    from control.dataset_catalog import list_datasets","    ","    records = list_datasets()","    return [create_descriptor_from_record(record) for record in records]","","","# Test function","def test_descriptor() -> None:","    \"\"\"Test the descriptor functionality.\"\"\"","    print(\"Testing DatasetDescriptor...\")","    ","    # Get a sample dataset record","    from control.dataset_catalog import list_datasets","    ","    records = list_datasets()","    if records:","        record = records[0]","        descriptor = create_descriptor_from_record(record)","        ","        print(f\"Dataset ID: {descriptor.dataset_id}\")","        print(f\"TXT root: {descriptor.txt_root}\")","        print(f\"TXT paths: {descriptor.txt_required_paths}\")","        print(f\"Parquet root: {descriptor.parquet_root}\")","        print(f\"Parquet paths: {descriptor.parquet_expected_paths}\")","        print(f\"Kind: {descriptor.kind}\")","    else:","        print(\"No datasets found\")","","","if __name__ == \"__main__\":","    test_descriptor()"]}
{"type":"file_footer","path":"src/control/dataset_descriptor.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/dataset_registry_mutation.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4723,"sha256":"8d0945d299d6fe46ef251bf5bd3bd8537186bced0be3e9d8a47ea674225da29c","total_lines":144,"chunk_count":1}
{"type":"file_chunk","path":"src/control/dataset_registry_mutation.py","chunk_index":0,"line_start":1,"line_end":144,"content":["","\"\"\"","Dataset registry mutation (controlled mutation) for snapshot registration.","","Phase 16.5â€‘B: Appendâ€‘only (or controlled mutation) registry updates.","\"\"\"","","from __future__ import annotations","","import json","from datetime import datetime, timezone","from pathlib import Path","from typing import Optional","","from contracts.data.snapshot_models import SnapshotMetadata","from data.dataset_registry import DatasetIndex, DatasetRecord","","","def _get_dataset_registry_root() -> Path:","    \"\"\"","    Return dataset registry root directory.","","    Environment override:","      - FISHBRO_DATASET_REGISTRY_ROOT (default: outputs/datasets)","    \"\"\"","    import os","    return Path(os.environ.get(\"FISHBRO_DATASET_REGISTRY_ROOT\", \"outputs/datasets\"))","","","def _compute_dataset_id(symbol: str, timeframe: str, normalized_sha256: str) -> str:","    \"\"\"","    Deterministic dataset ID for a snapshot.","","    Format: snapshot_{symbol}_{timeframe}_{normalized_sha256[:12]}","    \"\"\"","    symbol_norm = symbol.replace(\"/\", \"_\").upper()","    tf_norm = timeframe.replace(\"/\", \"_\").lower()","    return f\"snapshot_{symbol_norm}_{tf_norm}_{normalized_sha256[:12]}\"","","","def register_snapshot_as_dataset(","    snapshot_dir: Path,","    registry_root: Optional[Path] = None,",") -> DatasetRecord:","    \"\"\"","    Appendâ€‘only registration of a snapshot as a dataset.","","    Args:","        snapshot_dir: Path to snapshot directory (contains manifest.json)","        registry_root: Optional root directory for dataset registry.","                       Defaults to _get_dataset_registry_root().","","    Returns:","        DatasetEntry for the newly registered dataset.","","    Raises:","        FileNotFoundError: If manifest.json missing.","        ValueError: If snapshot already registered.","    \"\"\"","    # Load manifest","    manifest_path = snapshot_dir / \"manifest.json\"","    if not manifest_path.exists():","        raise FileNotFoundError(f\"manifest.json not found in {snapshot_dir}\")","","    manifest_data = json.loads(manifest_path.read_text(encoding=\"utf-8\"))","    meta = SnapshotMetadata.model_validate(manifest_data)","","    # Determine registry path","    if registry_root is None:","        registry_root = _get_dataset_registry_root()","    registry_path = registry_root / \"datasets_index.json\"","","    # Ensure parent directory exists","    registry_path.parent.mkdir(parents=True, exist_ok=True)","","    # Load existing registry or create empty","    if registry_path.exists():","        data = json.loads(registry_path.read_text(encoding=\"utf-8\"))","        existing_index = DatasetIndex.model_validate(data)","    else:","        existing_index = DatasetIndex(","            generated_at=datetime.now(timezone.utc).replace(microsecond=0),","            datasets=[],","        )","","    # Compute deterministic dataset ID","    dataset_id = _compute_dataset_id(meta.symbol, meta.timeframe, meta.normalized_sha256)","","    # Check for duplicate (conflict)","    for rec in existing_index.datasets:","        if rec.id == dataset_id:","            raise ValueError(f\"Snapshot {meta.snapshot_id} already registered as dataset {dataset_id}\")","","    # Build DatasetEntry","    # Use stats for start/end timestamps","    start_date = datetime.fromisoformat(meta.stats.min_timestamp.replace(\"Z\", \"+00:00\")).date()","    end_date = datetime.fromisoformat(meta.stats.max_timestamp.replace(\"Z\", \"+00:00\")).date()","","    # Path relative to datasets root (snapshots/{snapshot_id}/normalized.json)","    rel_path = f\"snapshots/{meta.snapshot_id}/normalized.json\"","","    # Compute fingerprint (SHA256 first 40 chars)","    fp40 = meta.normalized_sha256[:40]","    entry = DatasetRecord(","        id=dataset_id,","        symbol=meta.symbol,","        exchange=meta.symbol.split(\".\")[0] if \".\" in meta.symbol else \"UNKNOWN\",","        timeframe=meta.timeframe,","        path=rel_path,","        start_date=start_date,","        end_date=end_date,","        fingerprint_sha1=fp40,  # Keep for backward compatibility","        fingerprint_sha256_40=fp40,  # New field","        tz_provider=\"UTC\",","        tz_version=\"unknown\",","    )","","    # Append new record","    updated_datasets = existing_index.datasets + [entry]","    # Sort by id to maintain deterministic order","    updated_datasets.sort(key=lambda d: d.id)","","    # Create updated index with new generation timestamp","    updated_index = DatasetIndex(","        generated_at=datetime.now(timezone.utc).replace(microsecond=0),","        datasets=updated_datasets,","    )","","    # Write back atomically (write to temp file then rename)","    temp_path = registry_path.with_suffix(\".tmp\")","    temp_path.write_text(","        json.dumps(","            updated_index.model_dump(mode=\"json\"),","            sort_keys=True,","            indent=2,","            ensure_ascii=False,","        ),","        encoding=\"utf-8\",","    )","    temp_path.replace(registry_path)","","    return entry","",""]}
{"type":"file_footer","path":"src/control/dataset_registry_mutation.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/deploy_package_mc.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8431,"sha256":"14461e81c897d545aa05a02e6d14f809188aca91c417ce95f2e4355ac01df831","total_lines":281,"chunk_count":2}
{"type":"file_chunk","path":"src/control/deploy_package_mc.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","MultiCharts éƒ¨ç½²å¥—ä»¶ç”¢ç”Ÿå™¨","","ç”¢ç”Ÿ cost_models.jsonã€DEPLOY_README.mdã€deploy_manifest.json ç­‰æª”æ¡ˆï¼Œ","ä¸¦ç¢ºä¿ deterministic ordering èˆ‡ atomic writeã€‚","\"\"\"","","from __future__ import annotations","","import json","import hashlib","import tempfile","import shutil","from pathlib import Path","from typing import Dict, List, Any, Optional","from dataclasses import dataclass, asdict","","from core.slippage_policy import SlippagePolicy","","","@dataclass","class CostModel:","    \"\"\"","    å–®ä¸€å•†å“çš„æˆæœ¬æ¨¡åž‹","    \"\"\"","    symbol: str  # å•†å“ç¬¦è™Ÿï¼Œä¾‹å¦‚ \"MNQ\"","    tick_size: float  # tick å¤§å°ï¼Œä¾‹å¦‚ 0.25","    commission_per_side_usd: float  # æ¯é‚Šæ‰‹çºŒè²»ï¼ˆUSDï¼‰ï¼Œä¾‹å¦‚ 2.8","    commission_per_side_twd: Optional[float] = None  # æ¯é‚Šæ‰‹çºŒè²»ï¼ˆTWDï¼‰ï¼Œä¾‹å¦‚ 20.0ï¼ˆå°å¹£å•†å“ï¼‰","    ","    def to_dict(self) -> Dict[str, Any]:","        d = {","            \"symbol\": self.symbol,","            \"tick_size\": self.tick_size,","            \"commission_per_side_usd\": self.commission_per_side_usd,","        }","        if self.commission_per_side_twd is not None:","            d[\"commission_per_side_twd\"] = self.commission_per_side_twd","        return d","","","@dataclass","class DeployPackageConfig:","    \"\"\"","    éƒ¨ç½²å¥—ä»¶é…ç½®","    \"\"\"","    season: str  # å­£ç¯€æ¨™è¨˜ï¼Œä¾‹å¦‚ \"2026Q1\"","    selected_strategies: List[str]  # é¸ä¸­çš„ç­–ç•¥ ID åˆ—è¡¨","    outputs_root: Path  # è¼¸å‡ºæ ¹ç›®éŒ„","    slippage_policy: SlippagePolicy  # æ»‘åƒ¹æ”¿ç­–","    cost_models: List[CostModel]  # æˆæœ¬æ¨¡åž‹åˆ—è¡¨","    deploy_notes: Optional[str] = None  # éƒ¨ç½²å‚™è¨»","","","def generate_deploy_package(config: DeployPackageConfig) -> Path:","    \"\"\"","    ç”¢ç”Ÿ MC éƒ¨ç½²å¥—ä»¶","","    Args:","        config: éƒ¨ç½²é…ç½®","","    Returns:","        éƒ¨ç½²å¥—ä»¶ç›®éŒ„è·¯å¾‘","    \"\"\"","    # å»ºç«‹éƒ¨ç½²ç›®éŒ„","    deploy_dir = config.outputs_root / f\"mc_deploy_{config.season}\"","    deploy_dir.mkdir(parents=True, exist_ok=True)","    ","    # 1. ç”¢ç”Ÿ cost_models.json","    cost_models_path = deploy_dir / \"cost_models.json\"","    _write_cost_models(cost_models_path, config.cost_models, config.slippage_policy)","    ","    # 2. ç”¢ç”Ÿ DEPLOY_README.md","    readme_path = deploy_dir / \"DEPLOY_README.md\"","    _write_deploy_readme(readme_path, config)","    ","    # 3. ç”¢ç”Ÿ deploy_manifest.json","    manifest_path = deploy_dir / \"deploy_manifest.json\"","    _write_deploy_manifest(manifest_path, deploy_dir, config)","    ","    return deploy_dir","","","def _write_cost_models(","    path: Path,","    cost_models: List[CostModel],","    slippage_policy: SlippagePolicy,",") -> None:","    \"\"\"","    å¯«å…¥ cost_models.jsonï¼ŒåŒ…å«æ»‘åƒ¹æ”¿ç­–èˆ‡æˆæœ¬æ¨¡åž‹","    \"\"\"","    # å»ºç«‹æˆæœ¬æ¨¡åž‹å­—å…¸ï¼ˆæŒ‰ symbol æŽ’åºä»¥ç¢ºä¿ deterministicï¼‰","    models_dict = {}","    for model in sorted(cost_models, key=lambda m: m.symbol):","        models_dict[model.symbol] = model.to_dict()","    ","    data = {","        \"definition\": slippage_policy.definition,","        \"policy\": {","            \"selection\": slippage_policy.selection_level,","            \"stress\": slippage_policy.stress_level,","            \"mc_execution\": slippage_policy.mc_execution_level,","        },","        \"levels\": slippage_policy.levels,","        \"commission_per_symbol\": models_dict,","        \"tick_size_audit_snapshot\": {","            model.symbol: model.tick_size for model in cost_models","        },","    }","    ","    # ä½¿ç”¨ atomic write","    _atomic_write_json(path, data)","","","def _write_deploy_readme(path: Path, config: DeployPackageConfig) -> None:","    \"\"\"","    å¯«å…¥ DEPLOY_README.mdï¼ŒåŒ…å« anti-misconfig signature æ®µè½","    \"\"\"","    content = f\"\"\"# MultiCharts Deployment Package ({config.season})","","## Antiâ€‘Misconfig Signature","","This package has passed the S2 survive gate (selection slippage = {config.slippage_policy.selection_level}).","Recommended MC slippage setting: **{config.slippage_policy.mc_execution_level}**.","Commission and slippage are applied **per side** (definition: \"{config.slippage_policy.definition}\").","","## Checklist","","- [ ] Configured by: FishBroWFS_V2 research pipeline","- [ ] Configured at: {config.season}","- [ ] MC slippage level: {config.slippage_policy.mc_execution_level} ({config.slippage_policy.get_mc_execution_ticks()} ticks)","- [ ] MC commission: see cost_models.json per symbol","- [ ] Tick sizes: audit snapshot included in cost_models.json","- [ ] PLA rule: UNIVERSAL SIGNAL.PLA does NOT receive slippage/commission via Inputs","- [ ] PLA must NOT contain SetCommission/SetSlippage or any hardcoded cost logic","","## Selected Strategies","","{chr(10).join(f\"- {s}\" for s in config.selected_strategies)}","","## Files","","- `cost_models.json` â€“ cost models (slippage levels, commission, tick sizes)","- `deploy_manifest.json` â€“ SHAâ€‘256 hashes for all files + manifest chain","- `DEPLOY_README.md` â€“ this file","","## Notes","","{config.deploy_notes or \"No additional notes.\"}","\"\"\"","    _atomic_write_text(path, content)","","","def _write_deploy_manifest(","    path: Path,","    deploy_dir: Path,","    config: DeployPackageConfig,",") -> None:","    \"\"\"","    å¯«å…¥ deploy_manifest.jsonï¼ŒåŒ…å«æ‰€æœ‰æª”æ¡ˆçš„ SHAâ€‘256 é›œæ¹Šèˆ‡ manifest chain","    \"\"\"","    # æ”¶é›†éœ€è¦é›œæ¹Šçš„æª”æ¡ˆï¼ˆæŽ’é™¤ manifest æœ¬èº«ï¼‰","    files_to_hash = [","        deploy_dir / \"cost_models.json\",","        deploy_dir / \"DEPLOY_README.md\",","    ]","    ","    file_hashes = {}","    for file_path in files_to_hash:","        if file_path.exists():","            file_hashes[file_path.name] = _compute_file_sha256(file_path)","    ","    # è¨ˆç®— manifest å…§å®¹çš„é›œæ¹Šï¼ˆä¸å« manifest_sha256 æ¬„ä½ï¼‰","    manifest_data = {","        \"season\": config.season,","        \"selected_strategies\": config.selected_strategies,","        \"slippage_policy\": {","            \"definition\": config.slippage_policy.definition,","            \"selection_level\": config.slippage_policy.selection_level,","            \"stress_level\": config.slippage_policy.stress_level,","            \"mc_execution_level\": config.slippage_policy.mc_execution_level,","        },","        \"file_hashes\": file_hashes,","        \"manifest_version\": \"v1\",","    }","    ","    # è¨ˆç®— manifest é›œæ¹Š","    manifest_json = json.dumps(manifest_data, sort_keys=True, separators=(\",\", \":\"))","    manifest_sha256 = hashlib.sha256(manifest_json.encode(\"utf-8\")).hexdigest()","    ","    # åŠ å…¥ manifest_sha256","    manifest_data[\"manifest_sha256\"] = manifest_sha256","    ","    # atomic write","    _atomic_write_json(path, manifest_data)","","","def _atomic_write_json(path: Path, data: Dict[str, Any]) -> None:","    \"\"\""]}
{"type":"file_chunk","path":"src/control/deploy_package_mc.py","chunk_index":1,"line_start":201,"line_end":281,"content":["    Atomic write JSON æª”æ¡ˆï¼ˆtmp + replaceï¼‰","    \"\"\"","    # å»ºç«‹æš«å­˜æª”æ¡ˆ","    with tempfile.NamedTemporaryFile(","        mode=\"w\",","        encoding=\"utf-8\",","        dir=path.parent,","        delete=False,","        suffix=\".tmp\",","    ) as f:","        json.dump(data, f, ensure_ascii=False, sort_keys=True, indent=2)","        temp_path = Path(f.name)","    ","    # æ›¿æ›ç›®æ¨™æª”æ¡ˆ","    shutil.move(temp_path, path)","","","def _atomic_write_text(path: Path, content: str) -> None:","    \"\"\"","    Atomic write æ–‡å­—æª”æ¡ˆ","    \"\"\"","    with tempfile.NamedTemporaryFile(","        mode=\"w\",","        encoding=\"utf-8\",","        dir=path.parent,","        delete=False,","        suffix=\".tmp\",","    ) as f:","        f.write(content)","        temp_path = Path(f.name)","    ","    shutil.move(temp_path, path)","","","def _compute_file_sha256(path: Path) -> str:","    \"\"\"","    è¨ˆç®—æª”æ¡ˆçš„ SHAâ€‘256 é›œæ¹Š","    \"\"\"","    sha256 = hashlib.sha256()","    with open(path, \"rb\") as f:","        for chunk in iter(lambda: f.read(4096), b\"\"):","            sha256.update(chunk)","    return sha256.hexdigest()","","","def validate_pla_template(pla_template_path: Path) -> bool:","    \"\"\"","    é©—è­‰ PLA æ¨¡æ¿æ˜¯å¦åŒ…å«ç¦æ­¢çš„é—œéµå­—ï¼ˆSetCommission, SetSlippage ç­‰ï¼‰","","    Args:","        pla_template_path: PLA æ¨¡æ¿æª”æ¡ˆè·¯å¾‘","","    Returns:","        bool: æ˜¯å¦é€šéŽé©—è­‰ï¼ˆTrue è¡¨ç¤ºç„¡ç¦æ­¢é—œéµå­—ï¼‰","","    Raises:","        ValueError: å¦‚æžœç™¼ç¾ç¦æ­¢é—œéµå­—","    \"\"\"","    if not pla_template_path.exists():","        return True  # æ²’æœ‰æ¨¡æ¿ï¼Œè¦–ç‚ºé€šéŽ","    ","    forbidden_keywords = [","        \"SetCommission\",","        \"SetSlippage\",","        \"Commission\",","        \"Slippage\",","        \"Cost\",","        \"Fee\",","    ]","    ","    content = pla_template_path.read_text(encoding=\"utf-8\", errors=\"ignore\")","    for keyword in forbidden_keywords:","        if keyword in content:","            raise ValueError(","                f\"PLA æ¨¡æ¿åŒ…å«ç¦æ­¢é—œéµå­— '{keyword}'ã€‚\"","                \"UNIVERSAL SIGNAL.PLA ä¸å¾—åŒ…å«ä»»ä½•ç¡¬ç·¨ç¢¼çš„æˆæœ¬é‚è¼¯ã€‚\"","            )","    ","    return True","",""]}
{"type":"file_footer","path":"src/control/deploy_package_mc.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/deploy_txt.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4443,"sha256":"b0ed48c161efa7ee648d1fcb8412ce995e748d0c737a5050fe88ae9c86c04775","total_lines":132,"chunk_count":1}
{"type":"file_chunk","path":"src/control/deploy_txt.py","chunk_index":0,"line_start":1,"line_end":132,"content":["#!/usr/bin/env python3","\"\"\"","Deployment TXT MVP.","","Generates three TXT files for MultiCharts consumption:","- strategy_params.txt: mapping of strategy IDs to parameter sets","- portfolio.txt: portfolio legs (symbol, timeframe, strategy)","- universe.txt: instrument specifications (tick size, multiplier, costs)","","Phase 2: Minimal viable product.","\"\"\"","import sys","from pathlib import Path","from typing import Dict, Any, List","","# Ensure the package root is in sys.path when running as script","if __name__ == \"__main__\":","    sys.path.insert(0, str(Path(__file__).parent.parent.parent))","","from portfolio.spec import PortfolioSpec, PortfolioLeg","","","def write_deployment_txt(","    portfolio_spec: PortfolioSpec,","    universe_spec: Dict[str, Any],","    output_dir: Path,",") -> None:","    \"\"\"","    Write deployment TXT files.","","    Args:","        portfolio_spec: PortfolioSpec instance","        universe_spec: Dictionary mapping instrument symbol to dict with keys:","            tick_size, multiplier, commission_per_side_usd, session_profile","        output_dir: Directory where TXT files will be written","    \"\"\"","    output_dir.mkdir(parents=True, exist_ok=True)","","    # 1. strategy_params.txt","    # Collect unique strategy param combos across legs","    param_sets: Dict[str, Dict[str, float]] = {}","    for leg in portfolio_spec.legs:","        key = f\"{leg.strategy_id}_{leg.strategy_version}\"","        # Use param hash? For now just store params","        param_sets[key] = leg.params","","    with open(output_dir / \"strategy_params.txt\", \"w\", encoding=\"utf-8\") as f:","        f.write(\"# strategy_id,param1=value,param2=value,...\\n\")","        for key, params in param_sets.items():","            param_str = \",\".join(f\"{k}={v}\" for k, v in params.items())","            f.write(f\"{key},{param_str}\\n\")","","    # 2. portfolio.txt","    with open(output_dir / \"portfolio.txt\", \"w\", encoding=\"utf-8\") as f:","        f.write(\"# leg_id,symbol,timeframe_min,strategy_id,strategy_version,enabled\\n\")","        for leg in portfolio_spec.legs:","            f.write(f\"{leg.leg_id},{leg.symbol},{leg.timeframe_min},\"","                    f\"{leg.strategy_id},{leg.strategy_version},{leg.enabled}\\n\")","","    # 3. universe.txt","    with open(output_dir / \"universe.txt\", \"w\", encoding=\"utf-8\") as f:","        f.write(\"# symbol,tick_size,multiplier,commission_per_side_usd,session_profile\\n\")","        for symbol, spec in universe_spec.items():","            tick = spec.get(\"tick_size\", 0.25)","            mult = spec.get(\"multiplier\", 1.0)","            comm = spec.get(\"commission_per_side_usd\", 0.0)","            sess = spec.get(\"session_profile\", \"GLOBEX\")","            f.write(f\"{symbol},{tick},{mult},{comm},{sess}\\n\")","","","def generate_example() -> None:","    \"\"\"Generate example deployment TXT files for testing.\"\"\"","    from portfolio.spec import PortfolioLeg, PortfolioSpec","","    # Example portfolio spec","    legs = [","        PortfolioLeg(","            leg_id=\"mnq_60_sma\",","            symbol=\"CME.MNQ\",","            timeframe_min=60,","            session_profile=\"CME\",","            strategy_id=\"sma_cross\",","            strategy_version=\"v1\",","            params={\"fast_period\": 10.0, \"slow_period\": 20.0},","            enabled=True,","        ),","        PortfolioLeg(","            leg_id=\"mes_60_breakout\",","            symbol=\"CME.MES\",","            timeframe_min=60,","            session_profile=\"CME\",","            strategy_id=\"breakout_channel_v1\",","            strategy_version=\"v1\",","            params={\"channel_period\": 20, \"atr_multiplier\": 2.0},","            enabled=True,","        ),","    ]","    portfolio = PortfolioSpec(","        portfolio_id=\"example_portfolio\",","        version=\"2026Q1\",","        legs=legs,","    )","","    # Example universe spec","    universe = {","        \"CME.MNQ\": {","            \"tick_size\": 0.25,","            \"multiplier\": 2.0,","            \"commission_per_side_usd\": 2.8,","            \"session_profile\": \"CME\",","        },","        \"CME.MES\": {","            \"tick_size\": 0.25,","            \"multiplier\": 5.0,","            \"commission_per_side_usd\": 2.8,","            \"session_profile\": \"CME\",","        },","        \"TWF.MXF\": {","            \"tick_size\": 1.0,","            \"multiplier\": 50.0,","            \"commission_per_side_usd\": 20.0,","            \"session_profile\": \"TAIFEX\",","        },","    }","","    output_dir = Path(\"outputs/deployment_example\")","    write_deployment_txt(portfolio, universe, output_dir)","    print(f\"Example deployment TXT files written to {output_dir}\")","","","if __name__ == \"__main__\":","    generate_example()"]}
{"type":"file_footer","path":"src/control/deploy_txt.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/feature_resolver.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":15947,"sha256":"93f3ad1ecfe8638839cb87f8abe62118475b9d06b8ff1fb405ac760f09118ace","total_lines":527,"chunk_count":3}
{"type":"file_chunk","path":"src/control/feature_resolver.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Feature Dependency Resolverï¼ˆç‰¹å¾µä¾è³´è§£æžå™¨ï¼‰","","è®“ä»»ä½• strategy/wfs åœ¨åŸ·è¡Œå‰å¯ä»¥ï¼š","1. è®€å– strategy çš„ feature éœ€æ±‚ï¼ˆdeclarationï¼‰","2. æª¢æŸ¥ shared features cache æ˜¯å¦å­˜åœ¨ä¸”åˆç´„ä¸€è‡´","3. ç¼ºå°‘å°±è§¸ç™¼ BUILD_SHARED features-onlyï¼ˆéœ€éµå®ˆæ²»ç†è¦å‰‡ï¼‰","4. è¿”å›žçµ±ä¸€çš„ FeatureBundleï¼ˆå¯ç›´æŽ¥é¤µçµ¦ engineï¼‰","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Optional, Dict, Any, Tuple, List","import numpy as np","","from contracts.strategy_features import (","    StrategyFeatureRequirements,","    FeatureRef,",")","from core.feature_bundle import FeatureBundle, FeatureSeries","from control.build_context import BuildContext","from control.features_manifest import (","    features_manifest_path,","    load_features_manifest,",")","from control.features_store import (","    features_path,","    load_features_npz,",")","from control.shared_build import build_shared","","","class FeatureResolutionError(RuntimeError):","    \"\"\"ç‰¹å¾µè§£æžéŒ¯èª¤çš„åŸºåº•é¡žåˆ¥\"\"\"","    pass","","","class MissingFeaturesError(FeatureResolutionError):","    \"\"\"ç¼ºå°‘ç‰¹å¾µéŒ¯èª¤\"\"\"","    def __init__(self, missing: List[Tuple[str, int]]):","        self.missing = missing","        missing_str = \", \".join(f\"{name}@{tf}m\" for name, tf in missing)","        super().__init__(f\"ç¼ºå°‘ç‰¹å¾µ: {missing_str}\")","","","class ManifestMismatchError(FeatureResolutionError):","    \"\"\"Manifest åˆç´„ä¸ç¬¦éŒ¯èª¤\"\"\"","    pass","","","class BuildNotAllowedError(FeatureResolutionError):","    \"\"\"ä¸å…è¨± build éŒ¯èª¤\"\"\"","    pass","","","def resolve_features(","    *,","    season: str,","    dataset_id: str,","    requirements: StrategyFeatureRequirements,","    outputs_root: Path = Path(\"outputs\"),","    allow_build: bool = False,","    build_ctx: Optional[BuildContext] = None,",") -> Tuple[FeatureBundle, bool]:","    \"\"\"","    Ensure required features exist in shared cache and load them.","    ","    è¡Œç‚ºè¦æ ¼ï¼ˆå¿…é ˆç²¾æº–ï¼‰ï¼š","    1. æ‰¾åˆ° features ç›®éŒ„ï¼šoutputs/shared/{season}/{dataset_id}/features/","    2. æª¢æŸ¥ features_manifest.json æ˜¯å¦å­˜åœ¨","        - ä¸å­˜åœ¨ â†’ missing","    3. è¼‰å…¥ manifestï¼Œé©—è­‰ç¡¬åˆç´„ï¼š","        - ts_dtype == \"datetime64[s]\"","        - breaks_policy == \"drop\"","    4. æª¢æŸ¥ manifest æ˜¯å¦åŒ…å«æ‰€éœ€ features_{tf}m.npz æª”","    5. æ‰“é–‹ npzï¼Œæª¢æŸ¥ keysï¼š","        - ts, ä»¥åŠéœ€æ±‚çš„ feature key","        - ts å°é½Šæª¢æŸ¥ï¼ˆåŒ tf åŒæª”ï¼‰ï¼šts å¿…é ˆèˆ‡æª”å…§æ‰€æœ‰ feature array åŒé•·","    6. çµ„è£ FeatureBundle å›žå‚³","    ","    è‹¥ä»»ä½•ç¼ºå¤±ï¼š","        - allow_build=False â†’ raise MissingFeaturesError","        - allow_build=True â†’ éœ€è¦ build_ctx å­˜åœ¨ï¼Œå¦å‰‡ raise BuildNotAllowedError","        - å‘¼å« build_shared() é€²è¡Œ build","    ","    Args:","        season: å­£ç¯€æ¨™è¨˜","        dataset_id: è³‡æ–™é›† ID","        requirements: ç­–ç•¥ç‰¹å¾µéœ€æ±‚","        outputs_root: è¼¸å‡ºæ ¹ç›®éŒ„ï¼ˆé è¨­ç‚ºå°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹çš„ outputs/ï¼‰","        allow_build: æ˜¯å¦å…è¨±è‡ªå‹• build","        build_ctx: Build ä¸Šä¸‹æ–‡ï¼ˆåƒ…åœ¨ allow_build=True ä¸”éœ€è¦ build æ™‚ä½¿ç”¨ï¼‰","    ","    Returns:","        Tuple[FeatureBundle, bool]ï¼šç‰¹å¾µè³‡æ–™åŒ…èˆ‡æ˜¯å¦åŸ·è¡Œäº† build çš„æ¨™è¨˜","    ","    Raises:","        MissingFeaturesError: ç¼ºå°‘ç‰¹å¾µä¸”ä¸å…è¨± build","        ManifestMismatchError: manifest åˆç´„ä¸ç¬¦","        BuildNotAllowedError: å…è¨± build ä½†ç¼ºå°‘ build_ctx","        ValueError: åƒæ•¸ç„¡æ•ˆ","        FileNotFoundError: æª”æ¡ˆä¸å­˜åœ¨ä¸”ä¸å…è¨± build","    \"\"\"","    # åƒæ•¸é©—è­‰","    if not season:","        raise ValueError(\"season ä¸èƒ½ç‚ºç©º\")","    if not dataset_id:","        raise ValueError(\"dataset_id ä¸èƒ½ç‚ºç©º\")","    ","    if not isinstance(outputs_root, Path):","        outputs_root = Path(outputs_root)","    ","    # 1. æª¢æŸ¥ features manifest æ˜¯å¦å­˜åœ¨","    manifest_path = features_manifest_path(outputs_root, season, dataset_id)","    ","    if not manifest_path.exists():","        # features cache å®Œå…¨ä¸å­˜åœ¨","        missing_all = [(ref.name, ref.timeframe_min) for ref in requirements.required]","        return _handle_missing_features(","            season=season,","            dataset_id=dataset_id,","            missing=missing_all,","            allow_build=allow_build,","            build_ctx=build_ctx,","            outputs_root=outputs_root,","            requirements=requirements,","        )","    ","    # 2. è¼‰å…¥ä¸¦é©—è­‰ manifest","    try:","        manifest = load_features_manifest(manifest_path)","    except Exception as e:","        raise ManifestMismatchError(f\"ç„¡æ³•è¼‰å…¥ features manifest: {e}\")","    ","    # 3. é©—è­‰ç¡¬åˆç´„","    _validate_manifest_contracts(manifest)","    ","    # 4. æª¢æŸ¥æ‰€éœ€ç‰¹å¾µæ˜¯å¦å­˜åœ¨","    missing = _check_missing_features(manifest, requirements)","    ","    if missing:","        # æœ‰ç‰¹å¾µç¼ºå¤±","        return _handle_missing_features(","            season=season,","            dataset_id=dataset_id,","            missing=missing,","            allow_build=allow_build,","            build_ctx=build_ctx,","            outputs_root=outputs_root,","            requirements=requirements,","        )","    ","    # 5. è¼‰å…¥æ‰€æœ‰ç‰¹å¾µä¸¦å»ºç«‹ FeatureBundle","    return _load_feature_bundle(","        season=season,","        dataset_id=dataset_id,","        requirements=requirements,","        manifest=manifest,","        outputs_root=outputs_root,","    )","","","def _validate_manifest_contracts(manifest: Dict[str, Any]) -> None:","    \"\"\"","    é©—è­‰ manifest ç¡¬åˆç´„","    ","    Raises:","        ManifestMismatchError: åˆç´„ä¸ç¬¦","    \"\"\"","    # æª¢æŸ¥ ts_dtype","    ts_dtype = manifest.get(\"ts_dtype\")","    if ts_dtype != \"datetime64[s]\":","        raise ManifestMismatchError(","            f\"ts_dtype å¿…é ˆç‚º 'datetime64[s]'ï¼Œå¯¦éš›ç‚º {ts_dtype}\"","        )","    ","    # æª¢æŸ¥ breaks_policy","    breaks_policy = manifest.get(\"breaks_policy\")","    if breaks_policy != \"drop\":","        raise ManifestMismatchError(","            f\"breaks_policy å¿…é ˆç‚º 'drop'ï¼Œå¯¦éš›ç‚º {breaks_policy}\"","        )","    ","    # æª¢æŸ¥ files æ¬„ä½å­˜åœ¨","    if \"files\" not in manifest:","        raise ManifestMismatchError(\"manifest ç¼ºå°‘ 'files' æ¬„ä½\")","    ","    # æª¢æŸ¥ features_specs æ¬„ä½å­˜åœ¨","    if \"features_specs\" not in manifest:","        raise ManifestMismatchError(\"manifest ç¼ºå°‘ 'features_specs' æ¬„ä½\")","","","def _check_missing_features(","    manifest: Dict[str, Any],","    requirements: StrategyFeatureRequirements,",") -> List[Tuple[str, int]]:","    \"\"\""]}
{"type":"file_chunk","path":"src/control/feature_resolver.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    æª¢æŸ¥ manifest ä¸­ç¼ºå°‘å“ªäº›ç‰¹å¾µ","    ","    Args:","        manifest: features manifest å­—å…¸","        requirements: ç­–ç•¥ç‰¹å¾µéœ€æ±‚","    ","    Returns:","        ç¼ºå°‘çš„ç‰¹å¾µåˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ ç‚º (name, timeframe)","    \"\"\"","    missing = []","    ","    # å¾ž manifest å–å¾—å¯ç”¨çš„ç‰¹å¾µè¦æ ¼","    available_specs = manifest.get(\"features_specs\", [])","    available_keys = set()","    ","    for spec in available_specs:","        name = spec.get(\"name\")","        timeframe_min = spec.get(\"timeframe_min\")","        if name and timeframe_min:","            available_keys.add((name, timeframe_min))","    ","    # æª¢æŸ¥å¿…éœ€ç‰¹å¾µ","    for ref in requirements.required:","        key = (ref.name, ref.timeframe_min)","        if key not in available_keys:","            missing.append(key)","    ","    return missing","","","def _handle_missing_features(","    *,","    season: str,","    dataset_id: str,","    missing: List[Tuple[str, int]],","    allow_build: bool,","    build_ctx: Optional[BuildContext],","    outputs_root: Path,","    requirements: StrategyFeatureRequirements,",") -> Tuple[FeatureBundle, bool]:","    \"\"\"","    è™•ç†ç¼ºå¤±ç‰¹å¾µ","    ","    Args:","        season: å­£ç¯€æ¨™è¨˜","        dataset_id: è³‡æ–™é›† ID","        missing: ç¼ºå¤±çš„ç‰¹å¾µåˆ—è¡¨","        allow_build: æ˜¯å¦å…è¨±è‡ªå‹• build","        build_ctx: Build ä¸Šä¸‹æ–‡","        outputs_root: è¼¸å‡ºæ ¹ç›®éŒ„","        requirements: ç­–ç•¥ç‰¹å¾µéœ€æ±‚","    ","    Returns:","        Tuple[FeatureBundle, bool]ï¼šç‰¹å¾µè³‡æ–™åŒ…èˆ‡æ˜¯å¦åŸ·è¡Œäº† build çš„æ¨™è¨˜","    ","    Raises:","        MissingFeaturesError: ä¸å…è¨± build","        BuildNotAllowedError: å…è¨± build ä½†ç¼ºå°‘ build_ctx","    \"\"\"","    if not allow_build:","        raise MissingFeaturesError(missing)","    ","    if build_ctx is None:","        raise BuildNotAllowedError(","            \"å…è¨± build ä½†ç¼ºå°‘ build_ctxï¼ˆéœ€è¦ txt_path ç­‰åƒæ•¸ï¼‰\"","        )","    ","    # åŸ·è¡Œ build","    try:","        # ä½¿ç”¨ build_shared é€²è¡Œ build","        # æ³¨æ„ï¼šé€™è£¡æˆ‘å€‘ä½¿ç”¨ build_ctx ä¸­çš„åƒæ•¸ï¼Œä½†è¦†è“‹ season å’Œ dataset_id","        build_kwargs = build_ctx.to_build_shared_kwargs()","        build_kwargs.update({","            \"season\": season,","            \"dataset_id\": dataset_id,","            \"build_bars\": build_ctx.build_bars_if_missing,","            \"build_features\": True,","        })","        ","        report = build_shared(**build_kwargs)","        ","        if not report.get(\"success\"):","            raise FeatureResolutionError(f\"build å¤±æ•—: {report}\")","        ","        # build æˆåŠŸå¾Œï¼Œé‡æ–°å˜—è©¦è§£æž","        # éžè¿´å‘¼å« resolve_featuresï¼ˆä½†é€™æ¬¡ä¸å…è¨± buildï¼Œé¿å…ç„¡é™éžè¿´ï¼‰","        bundle, _ = resolve_features(","            season=season,","            dataset_id=dataset_id,","            requirements=requirements,","            outputs_root=outputs_root,","            allow_build=False,  # ä¸å…è¨±å†æ¬¡ build","            build_ctx=None,  # ä¸éœ€è¦ build_ctx","        )","        # å› ç‚ºæˆ‘å€‘åŸ·è¡Œäº† buildï¼Œæ‰€ä»¥æ¨™è¨˜ç‚º True","        return bundle, True","        ","    except Exception as e:","        # å°‡å…¶ä»–éŒ¯èª¤åŒ…è£ç‚º FeatureResolutionError","        raise FeatureResolutionError(f\"build å¤±æ•—: {e}\")","","","def _load_feature_bundle(","    *,","    season: str,","    dataset_id: str,","    requirements: StrategyFeatureRequirements,","    manifest: Dict[str, Any],","    outputs_root: Path,",") -> Tuple[FeatureBundle, bool]:","    \"\"\"","    è¼‰å…¥ç‰¹å¾µä¸¦å»ºç«‹ FeatureBundle","    ","    Args:","        season: å­£ç¯€æ¨™è¨˜","        dataset_id: è³‡æ–™é›† ID","        requirements: ç­–ç•¥ç‰¹å¾µéœ€æ±‚","        manifest: features manifest å­—å…¸","        outputs_root: è¼¸å‡ºæ ¹ç›®éŒ„","    ","    Returns:","        Tuple[FeatureBundle, bool]ï¼šç‰¹å¾µè³‡æ–™åŒ…èˆ‡æ˜¯å¦åŸ·è¡Œäº† build çš„æ¨™è¨˜ï¼ˆæ­¤è™•æ°¸é ç‚º Falseï¼‰","    ","    Raises:","        FeatureResolutionError: è¼‰å…¥å¤±æ•—","    \"\"\"","    series_dict = {}","    ","    # è¼‰å…¥å¿…éœ€ç‰¹å¾µ","    for ref in requirements.required:","        key = (ref.name, ref.timeframe_min)","        ","        try:","            series = _load_single_feature_series(","                season=season,","                dataset_id=dataset_id,","                feature_name=ref.name,","                timeframe_min=ref.timeframe_min,","                outputs_root=outputs_root,","                manifest=manifest,","            )","            series_dict[key] = series","        except Exception as e:","            raise FeatureResolutionError(","                f\"ç„¡æ³•è¼‰å…¥ç‰¹å¾µ {ref.name}@{ref.timeframe_min}m: {e}\"","            )","    ","    # è¼‰å…¥å¯é¸ç‰¹å¾µï¼ˆå¦‚æžœå­˜åœ¨ï¼‰","    for ref in requirements.optional:","        key = (ref.name, ref.timeframe_min)","        ","        # æª¢æŸ¥ç‰¹å¾µæ˜¯å¦å­˜åœ¨æ–¼ manifest","        if _feature_exists_in_manifest(ref.name, ref.timeframe_min, manifest):","            try:","                series = _load_single_feature_series(","                    season=season,","                    dataset_id=dataset_id,","                    feature_name=ref.name,","                    timeframe_min=ref.timeframe_min,","                    outputs_root=outputs_root,","                    manifest=manifest,","                )","                series_dict[key] = series","            except Exception:","                # å¯é¸ç‰¹å¾µè¼‰å…¥å¤±æ•—ï¼Œå¿½ç•¥ï¼ˆä¸åŠ å…¥ bundleï¼‰","                pass","    ","    # å»ºç«‹ metadata","    meta = {","        \"ts_dtype\": manifest.get(\"ts_dtype\", \"datetime64[s]\"),","        \"breaks_policy\": manifest.get(\"breaks_policy\", \"drop\"),","        \"manifest_sha256\": manifest.get(\"manifest_sha256\"),","        \"mode\": manifest.get(\"mode\"),","        \"season\": season,","        \"dataset_id\": dataset_id,","        \"files_sha256\": manifest.get(\"files\", {}),","    }","    ","    # å»ºç«‹ FeatureBundle","    try:","        bundle = FeatureBundle(","            dataset_id=dataset_id,","            season=season,","            series=series_dict,","            meta=meta,","        )","        return bundle, False","    except Exception as e:","        raise FeatureResolutionError(f\"ç„¡æ³•å»ºç«‹ FeatureBundle: {e}\")","","","def _feature_exists_in_manifest(","    feature_name: str,","    timeframe_min: int,","    manifest: Dict[str, Any],",") -> bool:","    \"\"\"","    æª¢æŸ¥ç‰¹å¾µæ˜¯å¦å­˜åœ¨æ–¼ manifest ä¸­","    ","    Args:"]}
{"type":"file_chunk","path":"src/control/feature_resolver.py","chunk_index":2,"line_start":401,"line_end":527,"content":["        feature_name: ç‰¹å¾µåç¨±","        timeframe_min: timeframe åˆ†é˜æ•¸","        manifest: features manifest å­—å…¸","    ","    Returns:","        bool","    \"\"\"","    specs = manifest.get(\"features_specs\", [])","    for spec in specs:","        if (spec.get(\"name\") == feature_name and ","            spec.get(\"timeframe_min\") == timeframe_min):","            return True","    return False","","","def _load_single_feature_series(","    *,","    season: str,","    dataset_id: str,","    feature_name: str,","    timeframe_min: int,","    outputs_root: Path,","    manifest: Dict[str, Any],",") -> FeatureSeries:","    \"\"\"","    è¼‰å…¥å–®ä¸€ç‰¹å¾µåºåˆ—","    ","    Args:","        season: å­£ç¯€æ¨™è¨˜","        dataset_id: è³‡æ–™é›† ID","        feature_name: ç‰¹å¾µåç¨±","        timeframe_min: timeframe åˆ†é˜æ•¸","        outputs_root: è¼¸å‡ºæ ¹ç›®éŒ„","        manifest: features manifest å­—å…¸ï¼ˆç”¨æ–¼é©—è­‰ï¼‰","    ","    Returns:","        FeatureSeries å¯¦ä¾‹","    ","    Raises:","        FeatureResolutionError: è¼‰å…¥å¤±æ•—","    \"\"\"","    # 1. è¼‰å…¥ features NPZ æª”æ¡ˆ","    feat_path = features_path(outputs_root, season, dataset_id, timeframe_min)","    ","    if not feat_path.exists():","        raise FeatureResolutionError(","            f\"features æª”æ¡ˆä¸å­˜åœ¨: {feat_path}\"","        )","    ","    try:","        data = load_features_npz(feat_path)","    except Exception as e:","        raise FeatureResolutionError(f\"ç„¡æ³•è¼‰å…¥ features NPZ: {e}\")","    ","    # 2. æª¢æŸ¥å¿…è¦ keys","    required_keys = {\"ts\", feature_name}","    missing_keys = required_keys - set(data.keys())","    if missing_keys:","        raise FeatureResolutionError(","            f\"features NPZ ç¼ºå°‘å¿…è¦ keys: {missing_keys}ï¼Œç¾æœ‰ keys: {list(data.keys())}\"","        )","    ","    # 3. é©—è­‰ ts dtype","    ts = data[\"ts\"]","    if not np.issubdtype(ts.dtype, np.datetime64):","        raise FeatureResolutionError(","            f\"ts dtype å¿…é ˆç‚º datetime64ï¼Œå¯¦éš›ç‚º {ts.dtype}\"","        )","    ","    # 4. é©—è­‰ç‰¹å¾µå€¼ dtype","    values = data[feature_name]","    if not np.issubdtype(values.dtype, np.floating):","        # å˜—è©¦è½‰æ›ç‚º float64","        try:","            values = values.astype(np.float64)","        except Exception as e:","            raise FeatureResolutionError(","                f\"ç‰¹å¾µå€¼ç„¡æ³•è½‰æ›ç‚ºæµ®é»žæ•¸: {e}ï¼Œdtype: {values.dtype}\"","            )","    ","    # 5. é©—è­‰é•·åº¦ä¸€è‡´","    if len(ts) != len(values):","        raise FeatureResolutionError(","            f\"ts èˆ‡ç‰¹å¾µå€¼é•·åº¦ä¸ä¸€è‡´: ts={len(ts)}, {feature_name}={len(values)}\"","        )","    ","    # 6. å»ºç«‹ FeatureSeries","    try:","        return FeatureSeries(","            ts=ts,","            values=values,","            name=feature_name,","            timeframe_min=timeframe_min,","        )","    except Exception as e:","        raise FeatureResolutionError(f\"ç„¡æ³•å»ºç«‹ FeatureSeries: {e}\")","","","# Cache invalidation functions for reload service","def invalidate_feature_cache() -> bool:","    \"\"\"Invalidate feature resolver cache.","    ","    Returns:","        True if successful, False otherwise","    \"\"\"","    try:","        # Currently there's no persistent cache in this module","        # This function exists for API compatibility","        return True","    except Exception:","        return False","","","def reload_feature_registry() -> bool:","    \"\"\"Reload feature registry.","    ","    Returns:","        True if successful, False otherwise","    \"\"\"","    try:","        # Currently there's no registry to reload","        # This function exists for API compatibility","        return True","    except Exception:","        return False","",""]}
{"type":"file_footer","path":"src/control/feature_resolver.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/control/features_manifest.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6446,"sha256":"c3ea6bbd71a2f14fa175da64fd85263e9cb0b5996ffef2c13212a896f735d265","total_lines":211,"chunk_count":2}
{"type":"file_chunk","path":"src/control/features_manifest.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Features Manifest å¯«å…¥å·¥å…·","","æä¾› deterministic JSON + self-hash manifest_sha256 + atomic writeã€‚","åŒ…å« features specs dump èˆ‡ lookback rewind è³‡è¨Šã€‚","\"\"\"","","from __future__ import annotations","","import hashlib","import json","import tempfile","from pathlib import Path","from typing import Any, Dict, Optional","from datetime import datetime","","from contracts.dimensions import canonical_json","from contracts.features import FeatureRegistry, FeatureSpec","","","def write_features_manifest(payload: Dict[str, Any], path: Path) -> Dict[str, Any]:","    \"\"\"","    Deterministic JSON + self-hash manifest_sha256 + atomic write.","    ","    è¡Œç‚ºè¦æ ¼ï¼š","    1. å»ºç«‹æš«å­˜æª”æ¡ˆï¼ˆ.json.tmpï¼‰","    2. è¨ˆç®— payload çš„ SHA256 hashï¼ˆæŽ’é™¤ manifest_sha256 æ¬„ä½ï¼‰","    3. å°‡ hash åŠ å…¥ payload ä½œç‚º manifest_sha256 æ¬„ä½","    4. ä½¿ç”¨ canonical_json å¯«å…¥æš«å­˜æª”æ¡ˆï¼ˆç¢ºä¿æŽ’åºä¸€è‡´ï¼‰","    5. atomic replace åˆ°ç›®æ¨™è·¯å¾‘","    6. å¦‚æžœå¯«å…¥å¤±æ•—ï¼Œæ¸…ç†æš«å­˜æª”æ¡ˆ","    ","    Args:","        payload: manifest è³‡æ–™å­—å…¸ï¼ˆä¸å« manifest_sha256ï¼‰","        path: ç›®æ¨™æª”æ¡ˆè·¯å¾‘","        ","    Returns:","        æœ€çµ‚çš„ manifest å­—å…¸ï¼ˆåŒ…å« manifest_sha256 æ¬„ä½ï¼‰","        ","    Raises:","        IOError: å¯«å…¥å¤±æ•—","    \"\"\"","    # ç¢ºä¿ç›®éŒ„å­˜åœ¨","    path.parent.mkdir(parents=True, exist_ok=True)","    ","    # å»ºç«‹æš«å­˜æª”æ¡ˆè·¯å¾‘","    temp_path = path.with_suffix(path.suffix + \".tmp\")","    ","    try:","        # è¨ˆç®— payload çš„ SHA256 hashï¼ˆæŽ’é™¤å¯èƒ½çš„ manifest_sha256 æ¬„ä½ï¼‰","        payload_without_hash = {k: v for k, v in payload.items() if k != \"manifest_sha256\"}","        json_str = canonical_json(payload_without_hash)","        manifest_sha256 = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","        ","        # å»ºç«‹æœ€çµ‚ payloadï¼ˆåŒ…å« hashï¼‰","        final_payload = {**payload_without_hash, \"manifest_sha256\": manifest_sha256}","        ","        # ä½¿ç”¨ canonical_json å¯«å…¥æš«å­˜æª”æ¡ˆ","        final_json = canonical_json(final_payload)","        temp_path.write_text(final_json, encoding=\"utf-8\")","        ","        # atomic replace","        temp_path.replace(path)","        ","        return final_payload","        ","    except Exception as e:","        # æ¸…ç†æš«å­˜æª”æ¡ˆ","        if temp_path.exists():","            try:","                temp_path.unlink()","            except OSError:","                pass","        raise IOError(f\"å¯«å…¥ features manifest å¤±æ•— {path}: {e}\")","    ","    finally:","        # ç¢ºä¿æš«å­˜æª”æ¡ˆè¢«æ¸…ç†ï¼ˆå¦‚æžœ replace æˆåŠŸï¼Œtemp_path å·²ä¸å­˜åœ¨ï¼‰","        if temp_path.exists():","            try:","                temp_path.unlink()","            except OSError:","                pass","","","def load_features_manifest(path: Path) -> Dict[str, Any]:","    \"\"\"","    è¼‰å…¥ features manifest ä¸¦é©—è­‰ hash","    ","    Args:","        path: manifest æª”æ¡ˆè·¯å¾‘","        ","    Returns:","        manifest å­—å…¸","        ","    Raises:","        FileNotFoundError: æª”æ¡ˆä¸å­˜åœ¨","        ValueError: JSON è§£æžå¤±æ•—æˆ– hash é©—è­‰å¤±æ•—","    \"\"\"","    if not path.exists():","        raise FileNotFoundError(f\"features manifest æª”æ¡ˆä¸å­˜åœ¨: {path}\")","    ","    try:","        content = path.read_text(encoding=\"utf-8\")","    except (IOError, OSError) as e:","        raise ValueError(f\"ç„¡æ³•è®€å– features manifest æª”æ¡ˆ {path}: {e}\")","    ","    try:","        data = json.loads(content)","    except json.JSONDecodeError as e:","        raise ValueError(f\"features manifest JSON è§£æžå¤±æ•— {path}: {e}\")","    ","    # é©—è­‰ manifest_sha256","    if \"manifest_sha256\" not in data:","        raise ValueError(f\"features manifest ç¼ºå°‘ manifest_sha256 æ¬„ä½: {path}\")","    ","    # è¨ˆç®—å¯¦éš› hashï¼ˆæŽ’é™¤ manifest_sha256 æ¬„ä½ï¼‰","    data_without_hash = {k: v for k, v in data.items() if k != \"manifest_sha256\"}","    json_str = canonical_json(data_without_hash)","    expected_hash = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","    ","    if data[\"manifest_sha256\"] != expected_hash:","        raise ValueError(f\"features manifest hash é©—è­‰å¤±æ•—: é æœŸ {expected_hash}ï¼Œå¯¦éš› {data['manifest_sha256']}\")","    ","    return data","","","def features_manifest_path(outputs_root: Path, season: str, dataset_id: str) -> Path:","    \"\"\"","    å–å¾— features manifest æª”æ¡ˆè·¯å¾‘","    ","    å»ºè­°ä½ç½®ï¼šoutputs/shared/{season}/{dataset_id}/features/features_manifest.json","    ","    Args:","        outputs_root: è¼¸å‡ºæ ¹ç›®éŒ„","        season: å­£ç¯€æ¨™è¨˜","        dataset_id: è³‡æ–™é›† ID","        ","    Returns:","        æª”æ¡ˆè·¯å¾‘","    \"\"\"","    # å»ºç«‹è·¯å¾‘","    path = outputs_root / \"shared\" / season / dataset_id / \"features\" / \"features_manifest.json\"","    return path","","","def build_features_manifest_data(","    *,","    season: str,","    dataset_id: str,","    mode: str,","    ts_dtype: str,","    breaks_policy: str,","    features_specs: list[Dict[str, Any]],","    append_only: bool,","    append_range: Optional[Dict[str, str]],","    lookback_rewind_by_tf: Dict[str, str],","    files_sha256: Dict[str, str],",") -> Dict[str, Any]:","    \"\"\"","    å»ºç«‹ features manifest è³‡æ–™","    ","    Args:","        season: å­£ç¯€æ¨™è¨˜","        dataset_id: è³‡æ–™é›† ID","        mode: å»ºç½®æ¨¡å¼ï¼ˆ\"FULL\" æˆ– \"INCREMENTAL\"ï¼‰","        ts_dtype: æ™‚é–“æˆ³è¨˜ dtypeï¼ˆå¿…é ˆç‚º \"datetime64[s]\"ï¼‰","        breaks_policy: break è™•ç†ç­–ç•¥ï¼ˆå¿…é ˆç‚º \"drop\"ï¼‰","        features_specs: ç‰¹å¾µè¦æ ¼åˆ—è¡¨ï¼ˆå¾ž FeatureRegistry è½‰æ›ï¼‰","        append_only: æ˜¯å¦ç‚º append-only å¢žé‡","        append_range: å¢žé‡ç¯„åœï¼ˆé–‹å§‹æ—¥ã€çµæŸæ—¥ï¼‰","        lookback_rewind_by_tf: æ¯å€‹ timeframe çš„ lookback rewind é–‹å§‹æ™‚é–“","        files_sha256: æª”æ¡ˆ SHA256 å­—å…¸","        ","    Returns:","        manifest è³‡æ–™å­—å…¸ï¼ˆä¸å« manifest_sha256ï¼‰","    \"\"\"","    manifest = {","        \"season\": season,","        \"dataset_id\": dataset_id,","        \"mode\": mode,","        \"ts_dtype\": ts_dtype,","        \"breaks_policy\": breaks_policy,","        \"features_specs\": features_specs,","        \"append_only\": append_only,","        \"append_range\": append_range,","        \"lookback_rewind_by_tf\": lookback_rewind_by_tf,","        \"files\": files_sha256,","    }","    ","    return manifest","","","def feature_spec_to_dict(spec: FeatureSpec) -> Dict[str, Any]:","    \"\"\"","    å°‡ FeatureSpec è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„å­—å…¸","    ","    Args:","        spec: ç‰¹å¾µè¦æ ¼","        "]}
{"type":"file_chunk","path":"src/control/features_manifest.py","chunk_index":1,"line_start":201,"line_end":211,"content":["    Returns:","        å¯åºåˆ—åŒ–çš„å­—å…¸","    \"\"\"","    return {","        \"name\": spec.name,","        \"timeframe_min\": spec.timeframe_min,","        \"lookback_bars\": spec.lookback_bars,","        \"params\": spec.params,","    }","",""]}
{"type":"file_footer","path":"src/control/features_manifest.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/features_store.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4826,"sha256":"d0864798fd68747cd8f2d069181e3eae4aaaa7da00ab086e620963376ddaa877","total_lines":188,"chunk_count":1}
