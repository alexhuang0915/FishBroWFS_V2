{"type":"meta","schema_version":2,"run_id":"20251227_195401Z","repo_root":"/home/fishbro/FishBroWFS_V2","snapshot_root":"SNAPSHOT","part":1,"parts":10,"created_at":"2025-12-27T19:54:01Z","generator":{"name":"dump_context","version":"vNext-balanced"},"policies":{"max_parts":10,"target_bytes_per_part":500000,"chunk_max_lines":200,"chunk_max_bytes":120000,"outputs_max_bytes_for_content":2000000,"outputs_policy":"manifest_only_for_big_files","forbid_file_truncated":true,"balance_parts":true,"estimated_total_bytes":3514686,"balance_buffer_ratio":1.08}}
{"type":"file_chunk","path":"src/control/features_store.py","chunk_index":0,"line_start":1,"line_end":188,"content":["","\"\"\"","Feature Store（NPZ atomic + SHA256）","","提供 features cache 的 I/O 工具，重用 bars_store 的 atomic write 與 SHA256 計算。","\"\"\"","","from __future__ import annotations","","from pathlib import Path","from typing import Dict, Literal, Optional","import numpy as np","","from control.bars_store import (","    write_npz_atomic,","    load_npz,","    sha256_file,","    canonical_json,",")","","Timeframe = Literal[15, 30, 60, 120, 240]","","","def features_dir(outputs_root: Path, season: str, dataset_id: str) -> Path:","    \"\"\"","    取得 features 目錄路徑","    ","    建議位置：outputs/shared/{season}/{dataset_id}/features/","    ","    Args:","        outputs_root: 輸出根目錄","        season: 季節標記，例如 \"2026Q1\"","        dataset_id: 資料集 ID","        ","    Returns:","        目錄路徑","    \"\"\"","    # 建立路徑","    path = outputs_root / \"shared\" / season / dataset_id / \"features\"","    return path","","","def features_path(","    outputs_root: Path,","    season: str,","    dataset_id: str,","    tf_min: Timeframe,",") -> Path:","    \"\"\"","    取得 features 檔案路徑","    ","    建議位置：outputs/shared/{season}/{dataset_id}/features/features_{tf_min}m.npz","    ","    Args:","        outputs_root: 輸出根目錄","        season: 季節標記","        dataset_id: 資料集 ID","        tf_min: timeframe 分鐘數（15, 30, 60, 120, 240）","        ","    Returns:","        檔案路徑","    \"\"\"","    dir_path = features_dir(outputs_root, season, dataset_id)","    return dir_path / f\"features_{tf_min}m.npz\"","","","def write_features_npz_atomic(","    path: Path,","    features_dict: Dict[str, np.ndarray],",") -> None:","    \"\"\"","    Write features NPZ via tmp + replace. Deterministic keys order.","    ","    重用 bars_store.write_npz_atomic 但確保 keys 順序固定：","    ts, atr_14, ret_z_200, session_vwap","    ","    Args:","        path: 目標檔案路徑","        features_dict: 特徵字典，必須包含所有必要 keys","        ","    Raises:","        ValueError: 缺少必要 keys","        IOError: 寫入失敗","    \"\"\"","    # 驗證必要 keys","    required_keys = {\"ts\", \"atr_14\", \"ret_z_200\", \"session_vwap\"}","    missing_keys = required_keys - set(features_dict.keys())","    if missing_keys:","        raise ValueError(f\"features_dict 缺少必要 keys: {missing_keys}\")","    ","    # 確保 ts 的 dtype 是 datetime64[s]","    ts = features_dict[\"ts\"]","    if not np.issubdtype(ts.dtype, np.datetime64):","        raise ValueError(f\"ts 的 dtype 必須是 datetime64，實際為 {ts.dtype}\")","    ","    # 確保所有特徵陣列都是 float64","    for key in [\"atr_14\", \"ret_z_200\", \"session_vwap\"]:","        arr = features_dict[key]","        if not np.issubdtype(arr.dtype, np.floating):","            raise ValueError(f\"{key} 的 dtype 必須是浮點數，實際為 {arr.dtype}\")","    ","    # 使用 bars_store 的 write_npz_atomic","    write_npz_atomic(path, features_dict)","","","def load_features_npz(path: Path) -> Dict[str, np.ndarray]:","    \"\"\"","    載入 features NPZ 檔案","    ","    Args:","        path: NPZ 檔案路徑","        ","    Returns:","        特徵字典","        ","    Raises:","        FileNotFoundError: 檔案不存在","        ValueError: 檔案格式錯誤或缺少必要 keys","    \"\"\"","    # 使用 bars_store 的 load_npz","    data = load_npz(path)","    ","    # 驗證必要 keys","    required_keys = {\"ts\", \"atr_14\", \"ret_z_200\", \"session_vwap\"}","    missing_keys = required_keys - set(data.keys())","    if missing_keys:","        raise ValueError(f\"載入的 NPZ 缺少必要 keys: {missing_keys}\")","    ","    return data","","","def sha256_features_file(","    outputs_root: Path,","    season: str,","    dataset_id: str,","    tf_min: Timeframe,",") -> str:","    \"\"\"","    計算 features NPZ 檔案的 SHA256 hash","    ","    Args:","        outputs_root: 輸出根目錄","        season: 季節標記","        dataset_id: 資料集 ID","        tf_min: timeframe 分鐘數","        ","    Returns:","        SHA256 hex digest（小寫）","        ","    Raises:","        FileNotFoundError: 檔案不存在","        IOError: 讀取失敗","    \"\"\"","    path = features_path(outputs_root, season, dataset_id, tf_min)","    return sha256_file(path)","","","def compute_features_sha256_dict(","    outputs_root: Path,","    season: str,","    dataset_id: str,","    tfs: list[Timeframe] = [15, 30, 60, 120, 240],",") -> Dict[str, str]:","    \"\"\"","    計算所有 timeframe 的 features NPZ 檔案 SHA256 hash","    ","    Args:","        outputs_root: 輸出根目錄","        season: 季節標記","        dataset_id: 資料集 ID","        tfs: timeframe 列表","        ","    Returns:","        字典：filename -> sha256","    \"\"\"","    result = {}","    ","    for tf in tfs:","        try:","            sha256 = sha256_features_file(outputs_root, season, dataset_id, tf)","            result[f\"features_{tf}m.npz\"] = sha256","        except FileNotFoundError:","            # 檔案不存在，跳過","            continue","    ","    return result","",""]}
{"type":"file_footer","path":"src/control/features_store.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/fingerprint_cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8253,"sha256":"901cc3bfa43ad5ab741d6d8bf89dbaee36b41f64d343c449d0b9e2f93a3cabe1","total_lines":282,"chunk_count":2}
{"type":"file_chunk","path":"src/control/fingerprint_cli.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Fingerprint scan-only diff CLI","","提供 scan-only 命令，用於比較 TXT 檔案與現有指紋索引，產生 diff 報告。","此命令純粹掃描與比較，不觸發任何 build 或 WFS 行為。","\"\"\"","","from __future__ import annotations","","import argparse","import json","import sys","from pathlib import Path","from typing import Optional","","from contracts.fingerprint import FingerprintIndex","from control.fingerprint_store import (","    fingerprint_index_path,","    load_fingerprint_index_if_exists,","    write_fingerprint_index,",")","from core.fingerprint import (","    build_fingerprint_index_from_raw_ingest,","    compare_fingerprint_indices,",")","from data.raw_ingest import ingest_raw_txt","","","def scan_fingerprint(","    season: str,","    dataset_id: str,","    txt_path: Path,","    outputs_root: Optional[Path] = None,","    save_new_index: bool = False,","    verbose: bool = False,",") -> dict:","    \"\"\"","    掃描 TXT 檔案並與現有指紋索引比較","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        txt_path: TXT 檔案路徑","        outputs_root: 輸出根目錄","        save_new_index: 是否儲存新的指紋索引","        verbose: 是否輸出詳細資訊","    ","    Returns:","        diff 報告字典","    \"\"\"","    # 檢查檔案是否存在","    if not txt_path.exists():","        raise FileNotFoundError(f\"TXT 檔案不存在: {txt_path}\")","    ","    # 載入現有指紋索引（如果存在）","    index_path = fingerprint_index_path(season, dataset_id, outputs_root)","    old_index = load_fingerprint_index_if_exists(index_path)","    ","    if verbose:","        if old_index:","            print(f\"找到現有指紋索引: {index_path}\")","            print(f\"  範圍: {old_index.range_start} 到 {old_index.range_end}\")","            print(f\"  天數: {len(old_index.day_hashes)}\")","        else:","            print(f\"沒有現有指紋索引: {index_path}\")","    ","    # 讀取 TXT 檔案並建立新的指紋索引","    if verbose:","        print(f\"讀取 TXT 檔案: {txt_path}\")","    ","    raw_result = ingest_raw_txt(txt_path)","    ","    if verbose:","        print(f\"  讀取 {raw_result.rows} 行\")","        if raw_result.policy.normalized_24h:","            print(f\"  已正規化 24:00:00 時間\")","    ","    # 建立新的指紋索引","    new_index = build_fingerprint_index_from_raw_ingest(","        dataset_id=dataset_id,","        raw_ingest_result=raw_result,","        build_notes=f\"scanned from {txt_path.name}\",","    )","    ","    if verbose:","        print(f\"建立新的指紋索引:\")","        print(f\"  範圍: {new_index.range_start} 到 {new_index.range_end}\")","        print(f\"  天數: {len(new_index.day_hashes)}\")","        print(f\"  index_sha256: {new_index.index_sha256[:16]}...\")","    ","    # 比較索引","    diff_report = compare_fingerprint_indices(old_index, new_index)","    ","    # 如果需要，儲存新的指紋索引","    if save_new_index:","        if verbose:","            print(f\"儲存新的指紋索引到: {index_path}\")","        ","        write_fingerprint_index(new_index, index_path)","        diff_report[\"new_index_saved\"] = True","        diff_report[\"new_index_path\"] = str(index_path)","    else:","        diff_report[\"new_index_saved\"] = False","    ","    return diff_report","","","def format_diff_report(diff_report: dict, verbose: bool = False) -> str:","    \"\"\"","    格式化 diff 報告","    ","    Args:","        diff_report: diff 報告字典","        verbose: 是否輸出詳細資訊","    ","    Returns:","        格式化字串","    \"\"\"","    lines = []","    ","    # 基本資訊","    lines.append(\"=== Fingerprint Scan Report ===\")","    ","    if diff_report.get(\"is_new\", False):","        lines.append(\"狀態: 全新資料集（無現有指紋索引）\")","    elif diff_report.get(\"no_change\", False):","        lines.append(\"狀態: 無變更（指紋完全相同）\")","    elif diff_report.get(\"append_only\", False):","        lines.append(\"狀態: 僅尾部新增（可增量）\")","    else:","        lines.append(\"狀態: 資料變更（需全量重算）\")","    ","    lines.append(\"\")","    ","    # 範圍資訊","    if diff_report[\"old_range_start\"]:","        lines.append(f\"舊範圍: {diff_report['old_range_start']} 到 {diff_report['old_range_end']}\")","    lines.append(f\"新範圍: {diff_report['new_range_start']} 到 {diff_report['new_range_end']}\")","    ","    # 變更資訊","    if diff_report.get(\"append_only\", False):","        append_range = diff_report.get(\"append_range\")","        if append_range:","            lines.append(f\"新增範圍: {append_range[0]} 到 {append_range[1]}\")","    ","    if diff_report.get(\"earliest_changed_day\"):","        lines.append(f\"最早變更日: {diff_report['earliest_changed_day']}\")","    ","    # 儲存狀態","    if diff_report.get(\"new_index_saved\", False):","        lines.append(f\"新指紋索引已儲存: {diff_report.get('new_index_path', '')}\")","    ","    # 詳細輸出","    if verbose:","        lines.append(\"\")","        lines.append(\"--- 詳細報告 ---\")","        lines.append(json.dumps(diff_report, indent=2, ensure_ascii=False))","    ","    return \"\\n\".join(lines)","","","def main() -> int:","    \"\"\"","    CLI 主函數","    ","    命令：fishbro fingerprint scan --season 2026Q1 --dataset-id XXX --txt-path /path/to/file.txt","    \"\"\"","    parser = argparse.ArgumentParser(","        description=\"掃描 TXT 檔案並與指紋索引比較（scan-only diff）\",","        formatter_class=argparse.ArgumentDefaultsHelpFormatter,","    )","    ","    # 子命令（未來可擴展）","    subparsers = parser.add_subparsers(dest=\"command\", help=\"命令\")","    ","    # scan 命令","    scan_parser = subparsers.add_parser(","        \"scan\",","        help=\"掃描 TXT 檔案並比較指紋\"","    )","    ","    scan_parser.add_argument(","        \"--season\",","        required=True,","        help=\"季節標記，例如 '2026Q1'\"","    )","    ","    scan_parser.add_argument(","        \"--dataset-id\",","        required=True,","        help=\"資料集 ID，例如 'CME.MNQ.60m.2020-2024'\"","    )","    ","    scan_parser.add_argument(","        \"--txt-path\",","        type=Path,","        required=True,","        help=\"TXT 檔案路徑\"","    )"]}
{"type":"file_chunk","path":"src/control/fingerprint_cli.py","chunk_index":1,"line_start":201,"line_end":282,"content":["    ","    scan_parser.add_argument(","        \"--outputs-root\",","        type=Path,","        default=Path(\"outputs\"),","        help=\"輸出根目錄\"","    )","    ","    scan_parser.add_argument(","        \"--save\",","        action=\"store_true\",","        help=\"儲存新的指紋索引（否則僅比較）\"","    )","    ","    scan_parser.add_argument(","        \"--verbose\",","        action=\"store_true\",","        help=\"輸出詳細資訊\"","    )","    ","    scan_parser.add_argument(","        \"--json\",","        action=\"store_true\",","        help=\"以 JSON 格式輸出報告\"","    )","    ","    # 如果沒有提供命令，顯示幫助","    if len(sys.argv) == 1:","        parser.print_help()","        return 0","    ","    args = parser.parse_args()","    ","    if args.command != \"scan\":","        print(f\"錯誤: 不支援的命令: {args.command}\", file=sys.stderr)","        parser.print_help()","        return 1","    ","    try:","        # 執行掃描","        diff_report = scan_fingerprint(","            season=args.season,","            dataset_id=args.dataset_id,","            txt_path=args.txt_path,","            outputs_root=args.outputs_root,","            save_new_index=args.save,","            verbose=args.verbose,","        )","        ","        # 輸出結果","        if args.json:","            print(json.dumps(diff_report, indent=2, ensure_ascii=False))","        else:","            report_text = format_diff_report(diff_report, args.verbose)","            print(report_text)","        ","        # 根據結果返回適當的退出碼","        if diff_report.get(\"no_change\", False):","            return 0  # 無變更","        elif diff_report.get(\"append_only\", False):","            return 10  # 可增量（使用非零值表示需要處理）","        else:","            return 20  # 需全量重算","        ","    except FileNotFoundError as e:","        print(f\"錯誤: 檔案不存在 - {e}\", file=sys.stderr)","        return 1","    except ValueError as e:","        print(f\"錯誤: 資料驗證失敗 - {e}\", file=sys.stderr)","        return 1","    except Exception as e:","        print(f\"錯誤: 執行失敗 - {e}\", file=sys.stderr)","        if args.verbose:","            import traceback","            traceback.print_exc()","        return 1","","","if __name__ == \"__main__\":","    sys.exit(main())","",""]}
{"type":"file_footer","path":"src/control/fingerprint_cli.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/fingerprint_store.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5678,"sha256":"0fdea3434a3df203a444d738067424449ce78b9db4f1f6c4a9b5f9ab9e20f381","total_lines":229,"chunk_count":2}
{"type":"file_chunk","path":"src/control/fingerprint_store.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Fingerprint index 儲存與讀取","","提供 atomic write 與 deterministic JSON 序列化。","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Optional","","from contracts.fingerprint import FingerprintIndex","from contracts.dimensions import canonical_json","","","def fingerprint_index_path(","    season: str,","    dataset_id: str,","    outputs_root: Optional[Path] = None",") -> Path:","    \"\"\"","    取得指紋索引檔案路徑","    ","    建議位置：outputs/fingerprints/{season}/{dataset_id}/fingerprint_index.json","    ","    Args:","        season: 季節標記，例如 \"2026Q1\"","        dataset_id: 資料集 ID","        outputs_root: 輸出根目錄，預設為專案根目錄下的 outputs/","    ","    Returns:","        檔案路徑","    \"\"\"","    if outputs_root is None:","        # 從專案根目錄開始","        project_root = Path(__file__).parent.parent.parent","        outputs_root = project_root / \"outputs\"","    ","    # 建立路徑","    path = outputs_root / \"fingerprints\" / season / dataset_id / \"fingerprint_index.json\"","    return path","","","def write_fingerprint_index(","    index: FingerprintIndex,","    path: Path,","    *,","    ensure_parents: bool = True",") -> None:","    \"\"\"","    寫入指紋索引（原子寫入）","    ","    使用 tmp + replace 模式確保 atomic write。","    ","    Args:","        index: 要寫入的 FingerprintIndex","        path: 目標檔案路徑","        ensure_parents: 是否建立父目錄","    ","    Raises:","        IOError: 寫入失敗","    \"\"\"","    if ensure_parents:","        path.parent.mkdir(parents=True, exist_ok=True)","    ","    # 轉換為字典","    data = index.model_dump()","    ","    # 使用 canonical_json 確保 deterministic 輸出","    json_str = canonical_json(data)","    ","    # 原子寫入：先寫到暫存檔案，再移動","    temp_path = path.with_suffix(\".json.tmp\")","    ","    try:","        # 寫入暫存檔案","        temp_path.write_text(json_str, encoding=\"utf-8\")","        ","        # 移動到目標位置（原子操作）","        temp_path.replace(path)","        ","    except Exception as e:","        # 清理暫存檔案","        if temp_path.exists():","            try:","                temp_path.unlink()","            except:","                pass","        ","        raise IOError(f\"寫入指紋索引失敗 {path}: {e}\")","    ","    # 驗證寫入的檔案可以正確讀回","    try:","        loaded = load_fingerprint_index(path)","        if loaded.index_sha256 != index.index_sha256:","            raise IOError(f\"寫入後驗證失敗: hash 不匹配\")","    except Exception as e:","        # 如果驗證失敗，刪除檔案","        if path.exists():","            try:","                path.unlink()","            except:","                pass","        raise IOError(f\"指紋索引驗證失敗 {path}: {e}\")","","","def load_fingerprint_index(path: Path) -> FingerprintIndex:","    \"\"\"","    載入指紋索引","    ","    Args:","        path: 檔案路徑","    ","    Returns:","        FingerprintIndex","    ","    Raises:","        FileNotFoundError: 檔案不存在","        ValueError: JSON 解析失敗或 schema 驗證失敗","    \"\"\"","    if not path.exists():","        raise FileNotFoundError(f\"指紋索引檔案不存在: {path}\")","    ","    try:","        content = path.read_text(encoding=\"utf-8\")","    except (IOError, OSError) as e:","        raise ValueError(f\"無法讀取指紋索引檔案 {path}: {e}\")","    ","    try:","        data = json.loads(content)","    except json.JSONDecodeError as e:","        raise ValueError(f\"指紋索引 JSON 解析失敗 {path}: {e}\")","    ","    try:","        return FingerprintIndex(**data)","    except Exception as e:","        raise ValueError(f\"指紋索引 schema 驗證失敗 {path}: {e}\")","","","def load_fingerprint_index_if_exists(path: Path) -> Optional[FingerprintIndex]:","    \"\"\"","    載入指紋索引（如果存在）","    ","    Args:","        path: 檔案路徑","    ","    Returns:","        FingerprintIndex 或 None（如果檔案不存在）","    ","    Raises:","        ValueError: JSON 解析失敗或 schema 驗證失敗","    \"\"\"","    if not path.exists():","        return None","    ","    return load_fingerprint_index(path)","","","def delete_fingerprint_index(path: Path) -> None:","    \"\"\"","    刪除指紋索引檔案","    ","    Args:","        path: 檔案路徑","    \"\"\"","    if path.exists():","        path.unlink()","","","def list_fingerprint_indices(","    season: str,","    outputs_root: Optional[Path] = None",") -> list[tuple[str, Path]]:","    \"\"\"","    列出指定季節的所有指紋索引","    ","    Args:","        season: 季節標記","        outputs_root: 輸出根目錄","    ","    Returns:","        (dataset_id, path) 的列表","    \"\"\"","    if outputs_root is None:","        project_root = Path(__file__).parent.parent.parent","        outputs_root = project_root / \"outputs\"","    ","    season_dir = outputs_root / \"fingerprints\" / season","    ","    if not season_dir.exists():","        return []","    ","    indices = []","    ","    for dataset_dir in season_dir.iterdir():","        if dataset_dir.is_dir():","            index_path = dataset_dir / \"fingerprint_index.json\"","            if index_path.exists():"]}
{"type":"file_chunk","path":"src/control/fingerprint_store.py","chunk_index":1,"line_start":201,"line_end":229,"content":["                indices.append((dataset_dir.name, index_path))","    ","    # 按 dataset_id 排序","    indices.sort(key=lambda x: x[0])","    ","    return indices","","","def ensure_fingerprint_directory(","    season: str,","    dataset_id: str,","    outputs_root: Optional[Path] = None",") -> Path:","    \"\"\"","    確保指紋索引目錄存在","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        outputs_root: 輸出根目錄","    ","    Returns:","        目錄路徑","    \"\"\"","    path = fingerprint_index_path(season, dataset_id, outputs_root)","    path.parent.mkdir(parents=True, exist_ok=True)","    return path.parent","",""]}
{"type":"file_footer","path":"src/control/fingerprint_store.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/governance.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6642,"sha256":"75dbd01522d4f9280387c2c3d37f3f24e3eee270f1dacbc9b7b3716504f1548a","total_lines":211,"chunk_count":2}
{"type":"file_chunk","path":"src/control/governance.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Batch metadata and governance for Phase 14.","","Season/tags/note/frozen metadata with immutable rules.","","CRITICAL CONTRACTS:","- Metadata MUST live under: artifacts/{batch_id}/metadata.json","  (so a batch folder is fully portable for audit/replay/archive).","- Writes MUST be atomic (tmp + replace) to avoid corrupt JSON on crash.","- Tag handling MUST be deterministic (dedupe + sort).","- Corrupted metadata MUST NOT be silently treated as \"not found\".","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass, field","from datetime import datetime, timezone","from pathlib import Path","from typing import Optional","","from control.artifacts import write_json_atomic","","","def _utc_now_iso() -> str:","    # Seconds precision, UTC, Z suffix","    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")","","","@dataclass","class BatchMetadata:","    \"\"\"Batch metadata (mutable only before frozen).\"\"\"","    batch_id: str","    season: str = \"\"","    tags: list[str] = field(default_factory=list)","    note: str = \"\"","    frozen: bool = False","    created_at: str = \"\"","    updated_at: str = \"\"","    created_by: str = \"\"","","","class BatchGovernanceStore:","    \"\"\"Persistent store for batch metadata.","","    Store root MUST be the artifacts root.","    Metadata path:","      {artifacts_root}/{batch_id}/metadata.json","    \"\"\"","","    def __init__(self, artifacts_root: Path):","        self.artifacts_root = artifacts_root","        self.artifacts_root.mkdir(parents=True, exist_ok=True)","","    def _metadata_path(self, batch_id: str) -> Path:","        return self.artifacts_root / batch_id / \"metadata.json\"","","    def get_metadata(self, batch_id: str) -> Optional[BatchMetadata]:","        path = self._metadata_path(batch_id)","        if not path.exists():","            return None","","        # Do NOT swallow corruption; let callers handle it explicitly.","        data = json.loads(path.read_text(encoding=\"utf-8\"))","","        tags = data.get(\"tags\", [])","        if not isinstance(tags, list):","            raise ValueError(\"metadata.tags must be a list\")","","        return BatchMetadata(","            batch_id=data[\"batch_id\"],","            season=data.get(\"season\", \"\"),","            tags=list(tags),","            note=data.get(\"note\", \"\"),","            frozen=bool(data.get(\"frozen\", False)),","            created_at=data.get(\"created_at\", \"\"),","            updated_at=data.get(\"updated_at\", \"\"),","            created_by=data.get(\"created_by\", \"\"),","        )","","    def set_metadata(self, batch_id: str, metadata: BatchMetadata) -> None:","        path = self._metadata_path(batch_id)","        path.parent.mkdir(parents=True, exist_ok=True)","","        payload = {","            \"batch_id\": batch_id,","            \"season\": metadata.season,","            \"tags\": list(metadata.tags),","            \"note\": metadata.note,","            \"frozen\": bool(metadata.frozen),","            \"created_at\": metadata.created_at,","            \"updated_at\": metadata.updated_at,","            \"created_by\": metadata.created_by,","        }","        write_json_atomic(path, payload)","","    def is_frozen(self, batch_id: str) -> bool:","        meta = self.get_metadata(batch_id)","        return bool(meta and meta.frozen)","","    def update_metadata(","        self,","        batch_id: str,","        *,","        season: Optional[str] = None,","        tags: Optional[list[str]] = None,","        note: Optional[str] = None,","        frozen: Optional[bool] = None,","        created_by: str = \"system\",","    ) -> BatchMetadata:","        \"\"\"Update metadata fields (enforcing frozen rules).","","        Frozen rules:","        - If batch is frozen:","          - season cannot change","          - frozen cannot be set to False","          - tags can be appended (dedupe + sort)","          - note can change","          - frozen=True again is a no-op","        \"\"\"","        existing = self.get_metadata(batch_id)","        now = _utc_now_iso()","","        if existing is None:","            existing = BatchMetadata(","                batch_id=batch_id,","                season=\"\",","                tags=[],","                note=\"\",","                frozen=False,","                created_at=now,","                updated_at=now,","                created_by=created_by,","            )","","        if existing.frozen:","            if season is not None and season != existing.season:","                raise ValueError(\"Cannot change season of frozen batch\")","            if frozen is False:","                raise ValueError(\"Cannot unfreeze a frozen batch\")","","        # Apply changes","        if (season is not None) and (not existing.frozen):","            existing.season = season","","        if tags is not None:","            merged = set(existing.tags)","            merged.update(tags)","            existing.tags = sorted(merged)","","        if note is not None:","            existing.note = note","","        if frozen is not None:","            if frozen is True:","                existing.frozen = True","            elif frozen is False:","                # allowed only when not frozen (blocked above if frozen)","                existing.frozen = False","","        existing.updated_at = now","        self.set_metadata(batch_id, existing)","        return existing","","    def freeze(self, batch_id: str) -> None:","        \"\"\"Freeze a batch (irreversible).","","        Raises:","            ValueError: If batch metadata not found.","        \"\"\"","        meta = self.get_metadata(batch_id)","        if meta is None:","            raise ValueError(f\"Batch {batch_id} not found\")","","        if not meta.frozen:","            meta.frozen = True","            meta.updated_at = _utc_now_iso()","            self.set_metadata(batch_id, meta)","","    def list_batches(","        self,","        *,","        season: Optional[str] = None,","        tag: Optional[str] = None,","        frozen: Optional[bool] = None,","    ) -> list[BatchMetadata]:","        \"\"\"List batches matching filters.","","        Scans artifacts root for {batch_id}/metadata.json.","","        Deterministic ordering:","        - Sort by batch_id.","        \"\"\"","        results: list[BatchMetadata] = []","        for batch_dir in sorted([p for p in self.artifacts_root.iterdir() if p.is_dir()], key=lambda p: p.name):","            meta_path = batch_dir / \"metadata.json\"","            if not meta_path.exists():","                continue","            meta = self.get_metadata(batch_dir.name)","            if meta is None:"]}
{"type":"file_chunk","path":"src/control/governance.py","chunk_index":1,"line_start":201,"line_end":211,"content":["                continue","            if season is not None and meta.season != season:","                continue","            if tag is not None and tag not in set(meta.tags):","                continue","            if frozen is not None and bool(meta.frozen) != bool(frozen):","                continue","            results.append(meta)","        return results","",""]}
{"type":"file_footer","path":"src/control/governance.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/input_manifest.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":13997,"sha256":"d5d456cb85d78b04f421e735244bcc6b2cbf9c1051bb39c98412d0b46402093f","total_lines":412,"chunk_count":3}
{"type":"file_chunk","path":"src/control/input_manifest.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Input Manifest Generation for Job Auditability.","","Generates comprehensive input manifests for job submissions, capturing:","- Dataset information (ID, kind)","- TXT file signatures and status","- Parquet file signatures and status","- Build timestamps","- System snapshot at time of job submission","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass, field, asdict","from datetime import datetime, timezone","from pathlib import Path","from typing import Dict, Any, List, Optional","import hashlib","","from control.dataset_descriptor import get_descriptor","from gui.services.reload_service import compute_file_signature, get_system_snapshot","","","@dataclass","class FileManifest:","    \"\"\"Manifest for a single file.\"\"\"","    path: str","    exists: bool","    size_bytes: int = 0","    mtime_utc: Optional[str] = None","    signature: str = \"\"","    error: Optional[str] = None","","","@dataclass","class DatasetManifest:","    \"\"\"Manifest for a dataset with TXT and Parquet information.\"\"\"","    # Required fields (no defaults) first","    dataset_id: str","    kind: str","    txt_root: str","    parquet_root: str","    ","    # Optional fields with defaults","    txt_files: List[FileManifest] = field(default_factory=list)","    txt_present: bool = False","    txt_total_size_bytes: int = 0","    txt_signature_aggregate: str = \"\"","    parquet_files: List[FileManifest] = field(default_factory=list)","    parquet_present: bool = False","    parquet_total_size_bytes: int = 0","    parquet_signature_aggregate: str = \"\"","    up_to_date: bool = False","    bars_count: Optional[int] = None","    schema_ok: Optional[bool] = None","    error: Optional[str] = None","","","@dataclass","class InputManifest:","    \"\"\"Complete input manifest for a job submission.\"\"\"","    # Metadata","    created_at: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"))","    job_id: Optional[str] = None","    season: str = \"\"","    ","    # Configuration","    config_snapshot: Dict[str, Any] = field(default_factory=dict)","    ","    # Data manifests","    data1_manifest: Optional[DatasetManifest] = None","    data2_manifest: Optional[DatasetManifest] = None","    ","    # System snapshot (summary)","    system_snapshot_summary: Dict[str, Any] = field(default_factory=dict)","    ","    # Audit trail","    manifest_hash: str = \"\"","    previous_manifest_hash: Optional[str] = None","","","def create_file_manifest(file_path: str) -> FileManifest:","    \"\"\"Create manifest for a single file.\"\"\"","    try:","        p = Path(file_path)","        exists = p.exists()","        ","        if not exists:","            return FileManifest(","                path=file_path,","                exists=False,","                size_bytes=0,","                mtime_utc=None,","                signature=\"\",","                error=\"File not found\"","            )","        ","        st = p.stat()","        mtime_utc = datetime.fromtimestamp(st.st_mtime, datetime.timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","        signature = compute_file_signature(p)","        ","        return FileManifest(","            path=file_path,","            exists=True,","            size_bytes=int(st.st_size),","            mtime_utc=mtime_utc,","            signature=signature,","            error=\"\"","        )","    except Exception as e:","        return FileManifest(","            path=file_path,","            exists=False,","            size_bytes=0,","            mtime_utc=None,","            signature=\"\",","            error=str(e)","        )","","","def create_dataset_manifest(dataset_id: str) -> DatasetManifest:","    \"\"\"Create manifest for a dataset.\"\"\"","    try:","        descriptor = get_descriptor(dataset_id)","        if descriptor is None:","            return DatasetManifest(","                dataset_id=dataset_id,","                kind=\"unknown\",","                txt_root=\"\",","                parquet_root=\"\",","                error=f\"Dataset not found: {dataset_id}\"","            )","        ","        # Process TXT files","        txt_files = []","        txt_present = True","        txt_total_size = 0","        txt_signatures = []","        ","        for txt_path_str in descriptor.txt_required_paths:","            file_manifest = create_file_manifest(txt_path_str)","            txt_files.append(file_manifest)","            ","            if not file_manifest.exists:","                txt_present = False","            else:","                txt_total_size += file_manifest.size_bytes","                txt_signatures.append(file_manifest.signature)","        ","        # Process Parquet files","        parquet_files = []","        parquet_present = True","        parquet_total_size = 0","        parquet_signatures = []","        ","        for parquet_path_str in descriptor.parquet_expected_paths:","            file_manifest = create_file_manifest(parquet_path_str)","            parquet_files.append(file_manifest)","            ","            if not file_manifest.exists:","                parquet_present = False","            else:","                parquet_total_size += file_manifest.size_bytes","                parquet_signatures.append(file_manifest.signature)","        ","        # Determine up-to-date status","        up_to_date = txt_present and parquet_present","        # Simple heuristic: if both present, assume up-to-date","        # In a real implementation, this would compare timestamps or content hashes","        ","        # Try to get bars count from Parquet if available","        bars_count = None","        schema_ok = None","        ","        if parquet_present and descriptor.parquet_expected_paths:","            try:","                parquet_path = Path(descriptor.parquet_expected_paths[0])","                if parquet_path.exists():","                    # Quick schema check","                    import pandas as pd","                    df_sample = pd.read_parquet(parquet_path, nrows=1)","                    schema_ok = True","                    ","                    # Try to get row count for small files","                    if parquet_path.stat().st_size < 1000000:  # < 1MB","                        df = pd.read_parquet(parquet_path)","                        # Use df.shape[0] or len(df.index) instead of len(df)","                        if hasattr(df, 'shape') and len(df.shape) >= 1:","                            bars_count = df.shape[0]","                        elif hasattr(df, 'index'):","                            bars_count = len(df.index)","                        else:","                            bars_count = len(df)  # fallback","            except Exception:","                schema_ok = False","        ","        return DatasetManifest(","            dataset_id=dataset_id,","            kind=descriptor.kind,","            txt_root=descriptor.txt_root,"]}
{"type":"file_chunk","path":"src/control/input_manifest.py","chunk_index":1,"line_start":201,"line_end":400,"content":["            txt_files=txt_files,","            txt_present=txt_present,","            txt_total_size_bytes=txt_total_size,","            txt_signature_aggregate=\"|\".join(txt_signatures) if txt_signatures else \"none\",","            parquet_root=descriptor.parquet_root,","            parquet_files=parquet_files,","            parquet_present=parquet_present,","            parquet_total_size_bytes=parquet_total_size,","            parquet_signature_aggregate=\"|\".join(parquet_signatures) if parquet_signatures else \"none\",","            up_to_date=up_to_date,","            bars_count=bars_count,","            schema_ok=schema_ok","        )","    except Exception as e:","        return DatasetManifest(","            dataset_id=dataset_id,","            kind=\"unknown\",","            txt_root=\"\",","            parquet_root=\"\",","            error=str(e)","        )","","","def create_input_manifest(","    job_id: Optional[str],","    season: str,","    config_snapshot: Dict[str, Any],","    data1_dataset_id: str,","    data2_dataset_id: Optional[str] = None,","    previous_manifest_hash: Optional[str] = None",") -> InputManifest:","    \"\"\"Create complete input manifest for a job submission.","    ","    Args:","        job_id: Job ID (if available)","        season: Season identifier","        config_snapshot: Configuration snapshot from make_config_snapshot","        data1_dataset_id: DATA1 dataset ID","        data2_dataset_id: Optional DATA2 dataset ID","        previous_manifest_hash: Optional hash of previous manifest (for chain)","        ","    Returns:","        InputManifest with all audit information","    \"\"\"","    # Create dataset manifests","    data1_manifest = create_dataset_manifest(data1_dataset_id)","    ","    data2_manifest = None","    if data2_dataset_id:","        data2_manifest = create_dataset_manifest(data2_dataset_id)","    ","    # Get system snapshot summary","    system_snapshot = get_system_snapshot()","    snapshot_summary = {","        \"created_at\": system_snapshot.created_at.isoformat(),","        \"total_datasets\": system_snapshot.total_datasets,","        \"total_strategies\": system_snapshot.total_strategies,","        \"notes\": system_snapshot.notes[:5],  # First 5 notes","        \"error_count\": len(system_snapshot.errors)","    }","    ","    # Create manifest","    manifest = InputManifest(","        job_id=job_id,","        season=season,","        config_snapshot=config_snapshot,","        data1_manifest=data1_manifest,","        data2_manifest=data2_manifest,","        system_snapshot_summary=snapshot_summary,","        previous_manifest_hash=previous_manifest_hash","    )","    ","    # Compute manifest hash","    manifest_dict = asdict(manifest)","    # Remove hash field before computing hash","    manifest_dict.pop(\"manifest_hash\", None)","    ","    # Convert to JSON and compute hash","    manifest_json = json.dumps(manifest_dict, sort_keys=True, separators=(',', ':'))","    manifest_hash = hashlib.sha256(manifest_json.encode('utf-8')).hexdigest()[:32]","    ","    manifest.manifest_hash = manifest_hash","    ","    return manifest","","","def write_input_manifest(","    manifest: InputManifest,","    output_path: Path",") -> bool:","    \"\"\"Write input manifest to file.","    ","    Args:","        manifest: InputManifest to write","        output_path: Path to write manifest JSON file","        ","    Returns:","        True if successful, False otherwise","    \"\"\"","    try:","        # Ensure parent directory exists","        output_path.parent.mkdir(parents=True, exist_ok=True)","        ","        # Convert to dictionary","        manifest_dict = asdict(manifest)","        ","        # Write JSON","        with open(output_path, 'w', encoding='utf-8') as f:","            json.dump(manifest_dict, f, indent=2, ensure_ascii=False)","        ","        return True","    except Exception as e:","        print(f\"Error writing input manifest: {e}\")","        return False","","","def read_input_manifest(input_path: Path) -> Optional[InputManifest]:","    \"\"\"Read input manifest from file.","    ","    Args:","        input_path: Path to manifest JSON file","        ","    Returns:","        InputManifest if successful, None otherwise","    \"\"\"","    try:","        with open(input_path, 'r', encoding='utf-8') as f:","            data = json.load(f)","        ","        # Reconstruct nested objects","        if data.get('data1_manifest'):","            data1_dict = data['data1_manifest']","            data['data1_manifest'] = DatasetManifest(**data1_dict)","        ","        if data.get('data2_manifest'):","            data2_dict = data['data2_manifest']","            data['data2_manifest'] = DatasetManifest(**data2_dict)","        ","        return InputManifest(**data)","    except Exception as e:","        print(f\"Error reading input manifest: {e}\")","        return None","","","def verify_input_manifest(manifest: InputManifest) -> Dict[str, Any]:","    \"\"\"Verify input manifest integrity and completeness.","    ","    Args:","        manifest: InputManifest to verify","        ","    Returns:","        Dictionary with verification results","    \"\"\"","    results = {","        \"valid\": True,","        \"errors\": [],","        \"warnings\": [],","        \"checks\": []","    }","    ","    # Check timestamp first (warnings)","    try:","        created_at = datetime.fromisoformat(manifest.created_at.replace('Z', '+00:00'))","        age_hours = (datetime.now(timezone.utc) - created_at).total_seconds() / 3600","        if age_hours > 24:","            results[\"warnings\"].append(f\"Manifest is {age_hours:.1f} hours old\")","    except Exception:","        results[\"warnings\"].append(\"Invalid timestamp format\")","    ","    # Check DATA1 manifest (structural errors before hash)","    if not manifest.data1_manifest:","        results[\"errors\"].append(\"Missing DATA1 manifest\")","        results[\"valid\"] = False","    else:","        if not manifest.data1_manifest.txt_present:","            results[\"warnings\"].append(f\"DATA1 dataset {manifest.data1_manifest.dataset_id} missing TXT files\")","        ","        if not manifest.data1_manifest.parquet_present:","            results[\"warnings\"].append(f\"DATA1 dataset {manifest.data1_manifest.dataset_id} missing Parquet files\")","        ","        if manifest.data1_manifest.error:","            results[\"warnings\"].append(f\"DATA1 dataset error: {manifest.data1_manifest.error}\")","    ","    # Check DATA2 manifest if present","    if manifest.data2_manifest:","        if not manifest.data2_manifest.txt_present:","            results[\"warnings\"].append(f\"DATA2 dataset {manifest.data2_manifest.dataset_id} missing TXT files\")","        ","        if not manifest.data2_manifest.parquet_present:","            results[\"warnings\"].append(f\"DATA2 dataset {manifest.data2_manifest.dataset_id} missing Parquet files\")","        ","        if manifest.data2_manifest.error:","            results[\"warnings\"].append(f\"DATA2 dataset error: {manifest.data2_manifest.error}\")","    ","    # Check system snapshot","    if not manifest.system_snapshot_summary:","        results[\"warnings\"].append(\"System snapshot summary is empty\")","    ","    # Check manifest hash (after structural checks)","    manifest_dict = asdict(manifest)"]}
{"type":"file_chunk","path":"src/control/input_manifest.py","chunk_index":2,"line_start":401,"line_end":412,"content":["    original_hash = manifest_dict.pop(\"manifest_hash\", None)","    ","    manifest_json = json.dumps(manifest_dict, sort_keys=True, separators=(',', ':'))","    computed_hash = hashlib.sha256(manifest_json.encode('utf-8')).hexdigest()[:32]","    ","    if original_hash != computed_hash:","        results[\"valid\"] = False","        results[\"errors\"].append(f\"Manifest hash mismatch: expected {original_hash}, got {computed_hash}\")","    else:","        results[\"checks\"].append(\"Manifest hash verified\")","    ","    return results"]}
{"type":"file_footer","path":"src/control/input_manifest.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/control/job_api.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":16256,"sha256":"a75fd9d04ebfda34987d491685239629dbce674568dd2f8a1ee9188761f0f2e3","total_lines":463,"chunk_count":3}
{"type":"file_chunk","path":"src/control/job_api.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Job API for M1 Wizard.","","Provides job creation and governance checking for the wizard UI.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Dict, Any, Optional, List","from datetime import datetime","","from control.jobs_db import create_job, get_job, list_jobs","from control.types import DBJobSpec, JobRecord, JobStatus","from control.dataset_catalog import get_dataset_catalog","from control.strategy_catalog import get_strategy_catalog","from control.dataset_descriptor import get_descriptor","from control.input_manifest import create_input_manifest, write_input_manifest","from core.config_snapshot import make_config_snapshot","","","class JobAPIError(Exception):","    \"\"\"Base exception for Job API errors.\"\"\"","    pass","","","class SeasonFrozenError(JobAPIError):","    \"\"\"Raised when trying to submit a job to a frozen season.\"\"\"","    pass","","","class ValidationError(JobAPIError):","    \"\"\"Raised when job validation fails.\"\"\"","    pass","","","def check_season_not_frozen(season: str, action: str = \"submit_job\") -> None:","    \"\"\"Check if a season is frozen.","    ","    Args:","        season: Season identifier (e.g., \"2024Q1\")","        action: Action being performed (for error message)","        ","    Raises:","        SeasonFrozenError: If season is frozen","    \"\"\"","    # TODO: Implement actual season frozen check","    # For M1, we'll assume seasons are not frozen","    # In a real implementation, this would check season governance state","    pass","","","def validate_wizard_payload(payload: Dict[str, Any]) -> List[str]:","    \"\"\"Validate wizard payload.","    ","    Args:","        payload: Wizard payload dictionary","        ","    Returns:","        List of validation error messages (empty if valid)","    \"\"\"","    errors = []","    ","    # Required fields","    required_fields = [\"season\", \"data1\", \"strategy_id\", \"params\"]","    for field in required_fields:","        if field not in payload:","            errors.append(f\"Missing required field: {field}\")","    ","    # Validate data1","    if \"data1\" in payload:","        data1 = payload[\"data1\"]","        if not isinstance(data1, dict):","            errors.append(\"data1 must be a dictionary\")","        else:","            if \"dataset_id\" not in data1:","                errors.append(\"data1 missing dataset_id\")","            else:","                # Check dataset exists and has Parquet files","                dataset_id = data1[\"dataset_id\"]","                try:","                    descriptor = get_descriptor(dataset_id)","                    if descriptor is None:","                        errors.append(f\"Dataset not found: {dataset_id}\")","                    else:","                        # Check if Parquet files exist","                        from pathlib import Path","                        parquet_missing = []","                        for parquet_path_str in descriptor.parquet_expected_paths:","                            parquet_path = Path(parquet_path_str)","                            if not parquet_path.exists():","                                parquet_missing.append(parquet_path_str)","                        ","                        if parquet_missing:","                            missing_list = \", \".join(parquet_missing[:3])  # Show first 3","                            if len(parquet_missing) > 3:","                                missing_list += f\" and {len(parquet_missing) - 3} more\"","                            errors.append(f\"Dataset {dataset_id} missing Parquet files: {missing_list}\")","                            errors.append(f\"Use the Status page to build Parquet from TXT sources\")","                except Exception as e:","                    errors.append(f\"Error checking dataset {dataset_id}: {str(e)}\")","            ","            if \"symbols\" not in data1:","                errors.append(\"data1 missing symbols\")","            if \"timeframes\" not in data1:","                errors.append(\"data1 missing timeframes\")","    ","    # Validate data2 if present","    if \"data2\" in payload and payload[\"data2\"]:","        data2 = payload[\"data2\"]","        if not isinstance(data2, dict):","            errors.append(\"data2 must be a dictionary or null\")","        else:","            if \"dataset_id\" not in data2:","                errors.append(\"data2 missing dataset_id\")","            else:","                # Check data2 dataset exists and has Parquet files","                dataset_id = data2[\"dataset_id\"]","                try:","                    descriptor = get_descriptor(dataset_id)","                    if descriptor is None:","                        errors.append(f\"DATA2 dataset not found: {dataset_id}\")","                    else:","                        # Check if Parquet files exist","                        from pathlib import Path","                        parquet_missing = []","                        for parquet_path_str in descriptor.parquet_expected_paths:","                            parquet_path = Path(parquet_path_str)","                            if not parquet_path.exists():","                                parquet_missing.append(parquet_path_str)","                        ","                        if parquet_missing:","                            missing_list = \", \".join(parquet_missing[:3])","                            if len(parquet_missing) > 3:","                                missing_list += f\" and {len(parquet_missing) - 3} more\"","                            errors.append(f\"DATA2 dataset {dataset_id} missing Parquet files: {missing_list}\")","                except Exception as e:","                    errors.append(f\"Error checking DATA2 dataset {dataset_id}: {str(e)}\")","            ","            if \"filters\" not in data2:","                errors.append(\"data2 missing filters\")","    ","    # Validate strategy","    if \"strategy_id\" in payload:","        strategy_catalog = get_strategy_catalog()","        strategy = strategy_catalog.get_strategy(payload[\"strategy_id\"])","        if strategy is None:","            errors.append(f\"Unknown strategy: {payload['strategy_id']}\")","        else:","            # Validate parameters","            params = payload.get(\"params\", {})","            param_errors = strategy_catalog.validate_parameters(payload[\"strategy_id\"], params)","            for param_name, error_msg in param_errors.items():","                errors.append(f\"Parameter '{param_name}': {error_msg}\")","    ","    return errors","","","def calculate_units(payload: Dict[str, Any]) -> int:","    \"\"\"Calculate units count for wizard payload.","    ","    Units formula: |DATA1.symbols| × |DATA1.timeframes| × |strategies| × |DATA2.filters|","    ","    Args:","        payload: Wizard payload dictionary","        ","    Returns:","        Total units count","    \"\"\"","    # Extract data1 symbols and timeframes","    data1 = payload.get(\"data1\", {})","    symbols = data1.get(\"symbols\", [])","    timeframes = data1.get(\"timeframes\", [])","    ","    # Count strategies (always 1 for single strategy, but could be list)","    strategy_id = payload.get(\"strategy_id\")","    strategies = [strategy_id] if strategy_id else []","    ","    # Extract data2 filters if present","    data2 = payload.get(\"data2\")","    if data2 is None:","        filters = []","    else:","        filters = data2.get(\"filters\", [])","    ","    # Apply formula","    symbols_count = len(symbols) if isinstance(symbols, list) else 1","    timeframes_count = len(timeframes) if isinstance(timeframes, list) else 1","    strategies_count = len(strategies) if isinstance(strategies, list) else 1","    filters_count = len(filters) if isinstance(filters, list) else 1","    ","    # If data2 is not enabled, filters_count should be 1 (no filter multiplication)","    if not data2 or not payload.get(\"enable_data2\", False):","        filters_count = 1","    ","    units = symbols_count * timeframes_count * strategies_count * filters_count","    return units","","","def create_job_from_wizard(payload: Dict[str, Any]) -> Dict[str, Any]:"]}
{"type":"file_chunk","path":"src/control/job_api.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    \"\"\"Create a job from wizard payload.","    ","    This is the main function called by the wizard UI on submit.","    ","    Args:","        payload: Wizard payload dictionary with structure:","            {","                \"season\": \"2024Q1\",","                \"data1\": {","                    \"dataset_id\": \"CME.MNQ.60m.2020-2024\",","                    \"symbols\": [\"MNQ\", \"MXF\"],","                    \"timeframes\": [\"60m\", \"120m\"],","                    \"start_date\": \"2020-01-01\",","                    \"end_date\": \"2024-12-31\"","                },","                \"data2\": {","                    \"dataset_id\": \"TWF.MXF.15m.2018-2023\",","                    \"filters\": [\"filter1\", \"filter2\"]","                } | null,","                \"strategy_id\": \"sma_cross_v1\",","                \"params\": {","                    \"window_fast\": 10,","                    \"window_slow\": 30","                },","                \"wfs\": {","                    \"stage0_subsample\": 0.1,","                    \"top_k\": 20,","                    \"mem_limit_mb\": 8192,","                    \"allow_auto_downsample\": True","                }","            }","        ","    Returns:","        Dictionary with job_id and units count:","            {","                \"job_id\": \"uuid-here\",","                \"units\": 4,","                \"season\": \"2024Q1\",","                \"status\": \"queued\"","            }","        ","    Raises:","        SeasonFrozenError: If season is frozen","        ValidationError: If payload validation fails","    \"\"\"","    # Check season not frozen","    season = payload.get(\"season\")","    if season:","        check_season_not_frozen(season, action=\"submit_job\")","    ","    # Validate payload","    errors = validate_wizard_payload(payload)","    if errors:","        raise ValidationError(f\"Payload validation failed: {', '.join(errors)}\")","    ","    # Calculate units","    units = calculate_units(payload)","    ","    # Create config snapshot","    config_snapshot = make_config_snapshot(payload)","    ","    # Create DBJobSpec","    data1 = payload[\"data1\"]","    dataset_id = data1[\"dataset_id\"]","    ","    # Generate outputs root path","    outputs_root = f\"outputs/{season}/jobs\"","    ","    # Create job spec","    spec = DBJobSpec(","        season=season,","        dataset_id=dataset_id,","        outputs_root=outputs_root,","        config_snapshot=config_snapshot,","        config_hash=\"\",  # Will be computed by create_job","        data_fingerprint_sha256_40=\"\"  # Will be populated if needed","    )","    ","    # Create job in database","    db_path = Path(\"outputs/jobs.db\")","    job_id = create_job(db_path, spec)","    ","    # Create input manifest for auditability","    try:","        # Extract DATA2 dataset ID if present","        data2_dataset_id = None","        if \"data2\" in payload and payload[\"data2\"]:","            data2 = payload[\"data2\"]","            data2_dataset_id = data2.get(\"dataset_id\")","        ","        # Create input manifest","        from control.input_manifest import create_input_manifest, write_input_manifest","        ","        manifest = create_input_manifest(","            job_id=job_id,","            season=season,","            config_snapshot=config_snapshot,","            data1_dataset_id=dataset_id,","            data2_dataset_id=data2_dataset_id,","            previous_manifest_hash=None  # First in chain","        )","        ","        # Write manifest to job outputs directory","        manifest_dir = Path(f\"outputs/{season}/jobs/{job_id}\")","        manifest_dir.mkdir(parents=True, exist_ok=True)","        manifest_path = manifest_dir / \"input_manifest.json\"","        ","        write_success = write_input_manifest(manifest, manifest_path)","        ","        if not write_success:","            # Log warning but don't fail the job","            print(f\"Warning: Failed to write input manifest for job {job_id}\")","    except Exception as e:","        # Don't fail job creation if manifest creation fails","        print(f\"Warning: Failed to create input manifest for job {job_id}: {e}\")","    ","    return {","        \"job_id\": job_id,","        \"units\": units,","        \"season\": season,","        \"status\": \"queued\"","    }","","","def get_job_status(job_id: str) -> Dict[str, Any]:","    \"\"\"Get job status with units progress.","    ","    Args:","        job_id: Job ID","        ","    Returns:","        Dictionary with job status and progress:","            {","                \"job_id\": \"uuid-here\",","                \"status\": \"running\",","                \"units_done\": 10,","                \"units_total\": 20,","                \"progress\": 0.5,","                \"created_at\": \"2024-01-01T00:00:00Z\",","                \"updated_at\": \"2024-01-01T00:00:00Z\"","            }","    \"\"\"","    db_path = Path(\"outputs/jobs.db\")","    try:","        job = get_job(db_path, job_id)","        ","        # For M1, we need to calculate units_done and units_total","        # This would normally come from job execution progress","        # For now, we'll return placeholder values","        units_total = 0","        units_done = 0","        ","        # Try to extract units from config snapshot","        if hasattr(job.spec, 'config_snapshot'):","            config = job.spec.config_snapshot","            if isinstance(config, dict) and 'units' in config:","                units_total = config.get('units', 0)","        ","        # Estimate units_done based on status","        if job.status == JobStatus.DONE:","            units_done = units_total","        elif job.status == JobStatus.RUNNING:","            # For demo, assume 50% progress","            units_done = units_total // 2 if units_total > 0 else 0","        ","        progress = units_done / units_total if units_total > 0 else 0","        ","        return {","            \"job_id\": job_id,","            \"status\": job.status.value,","            \"units_done\": units_done,","            \"units_total\": units_total,","            \"progress\": progress,","            \"created_at\": job.created_at,","            \"updated_at\": job.updated_at,","            \"season\": job.spec.season,","            \"dataset_id\": job.spec.dataset_id","        }","    except KeyError:","        raise JobAPIError(f\"Job not found: {job_id}\")","","","def list_jobs_with_progress(limit: int = 50) -> List[Dict[str, Any]]:","    \"\"\"List jobs with units progress.","    ","    Args:","        limit: Maximum number of jobs to return","        ","    Returns:","        List of job dictionaries with progress information","    \"\"\"","    db_path = Path(\"outputs/jobs.db\")","    jobs = list_jobs(db_path, limit=limit)","    ","    result = []","    for job in jobs:","        # Calculate progress for each job","        units_total = 0","        units_done = 0","        "]}
{"type":"file_chunk","path":"src/control/job_api.py","chunk_index":2,"line_start":401,"line_end":463,"content":["        if hasattr(job.spec, 'config_snapshot'):","            config = job.spec.config_snapshot","            if isinstance(config, dict) and 'units' in config:","                units_total = config.get('units', 0)","        ","        if job.status == JobStatus.DONE:","            units_done = units_total","        elif job.status == JobStatus.RUNNING:","            units_done = units_total // 2 if units_total > 0 else 0","        ","        progress = units_done / units_total if units_total > 0 else 0","        ","        result.append({","            \"job_id\": job.job_id,","            \"status\": job.status.value,","            \"units_done\": units_done,","            \"units_total\": units_total,","            \"progress\": progress,","            \"created_at\": job.created_at,","            \"updated_at\": job.updated_at,","            \"season\": job.spec.season,","            \"dataset_id\": job.spec.dataset_id","        })","    ","    return result","","","def get_job_logs_tail(job_id: str, lines: int = 50) -> List[str]:","    \"\"\"Get tail of job logs.","    ","    Args:","        job_id: Job ID","        lines: Number of lines to return","        ","    Returns:","        List of log lines (most recent first)","    \"\"\"","    # TODO: Implement actual log retrieval","    # For M1, return placeholder logs","    return [","        f\"[{datetime.now().isoformat()}] Job {job_id} started\",","        f\"[{datetime.now().isoformat()}] Loading dataset...\",","        f\"[{datetime.now().isoformat()}] Running strategy...\",","        f\"[{datetime.now().isoformat()}] Processing units...\",","    ][-lines:]","","","# Convenience functions for GUI","def submit_wizard_job(payload: Dict[str, Any]) -> Dict[str, Any]:","    \"\"\"Submit wizard job (alias for create_job_from_wizard).\"\"\"","    return create_job_from_wizard(payload)","","","def get_job_summary(job_id: str) -> Dict[str, Any]:","    \"\"\"Get job summary for detail page.\"\"\"","    status = get_job_status(job_id)","    logs = get_job_logs_tail(job_id, lines=20)","    ","    return {","        **status,","        \"logs\": logs,","        \"log_tail\": \"\\n\".join(logs[-10:]) if logs else \"No logs available\"","    }"]}
{"type":"file_footer","path":"src/control/job_api.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/control/job_expand.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3824,"sha256":"f8403ae1a325720b1c91a14422fe9aa6b7bc80d79d32b9919b5dd504b390e246","total_lines":134,"chunk_count":1}
{"type":"file_chunk","path":"src/control/job_expand.py","chunk_index":0,"line_start":1,"line_end":134,"content":["","\"\"\"Job Template Expansion for Phase 13.","","Expand a JobTemplate (with param grids) into a deterministic list of JobSpec.","Pure functions, no side effects.","\"\"\"","","from __future__ import annotations","","import itertools","from datetime import date","from typing import Any","","from pydantic import BaseModel, ConfigDict, Field","","from control.job_spec import DataSpec, WizardJobSpec, WFSSpec","from control.param_grid import ParamGridSpec, values_for_param","","","class JobTemplate(BaseModel):","    \"\"\"Template for generating multiple JobSpec via parameter grids.","    ","    Phase 13: All parameters must be explicitly configured via param_grid.","    \"\"\"","    ","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","    ","    season: str = Field(","        ...,","        description=\"Season identifier (e.g., '2024Q1')\"","    )","    ","    dataset_id: str = Field(","        ...,","        description=\"Dataset identifier (must match registry)\"","    )","    ","    strategy_id: str = Field(","        ...,","        description=\"Strategy identifier (must match registry)\"","    )","    ","    param_grid: dict[str, ParamGridSpec] = Field(","        ...,","        description=\"Mapping from parameter name to grid specification\"","    )","    ","    wfs: WFSSpec = Field(","        default_factory=WFSSpec,","        description=\"WFS configuration\"","    )","","","def expand_job_template(template: JobTemplate) -> list[WizardJobSpec]:","    \"\"\"Expand a JobTemplate into a deterministic list of WizardJobSpec.","    ","    Args:","        template: Job template with param grids","    ","    Returns:","        List of WizardJobSpec in deterministic order.","    ","    Raises:","        ValueError: if any param grid is invalid.","    \"\"\"","    # Sort param names for deterministic expansion","    param_names = sorted(template.param_grid.keys())","    ","    # For each param, compute list of values","    param_values: dict[str, list[Any]] = {}","    for name in param_names:","        grid = template.param_grid[name]","        values = values_for_param(grid)","        param_values[name] = values","    ","    # Compute Cartesian product in deterministic order","    # Order: iterate params sorted by name, values in order from values_for_param","    value_lists = [param_values[name] for name in param_names]","    ","    # Create a DataSpec with placeholder dates (tests don't care about dates)","    # Use fixed dates that are valid for any dataset","    data1 = DataSpec(","        dataset_id=template.dataset_id,","        start_date=date(2000, 1, 1),","        end_date=date(2000, 1, 2)","    )","    ","    jobs = []","    for combo in itertools.product(*value_lists):","        params = dict(zip(param_names, combo))","        job = WizardJobSpec(","            season=template.season,","            data1=data1,","            data2=None,","            strategy_id=template.strategy_id,","            params=params,","            wfs=template.wfs","        )","        jobs.append(job)","    ","    return jobs","","","def estimate_total_jobs(template: JobTemplate) -> int:","    \"\"\"Estimate total number of jobs that would be generated.","    ","    Returns:","        Product of value counts for each parameter.","    \"\"\"","    total = 1","    for grid in template.param_grid.values():","        total *= len(values_for_param(grid))","    return total","","","def validate_template(template: JobTemplate) -> None:","    \"\"\"Validate template.","    ","    Raises ValueError with descriptive message if invalid.","    \"\"\"","    if not template.season:","        raise ValueError(\"season must be non-empty\")","    if not template.dataset_id:","        raise ValueError(\"dataset_id must be non-empty\")","    if not template.strategy_id:","        raise ValueError(\"strategy_id must be non-empty\")","    if not template.param_grid:","        raise ValueError(\"param_grid cannot be empty\")","    ","    # Validate each grid (values_for_param will raise if invalid)","    for grid in template.param_grid.values():","        values_for_param(grid)","",""]}
{"type":"file_footer","path":"src/control/job_expand.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/job_spec.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3059,"sha256":"0a62e74a9d3ad7f379f4e5053522f74f31b3f68e261b424559aad7be61ccb753","total_lines":102,"chunk_count":1}
{"type":"file_chunk","path":"src/control/job_spec.py","chunk_index":0,"line_start":1,"line_end":102,"content":["","\"\"\"WizardJobSpec Schema for Research Job Wizard.","","Phase 12: WizardJobSpec is the ONLY output from GUI.","Contains all configuration needed to run a research job.","Must NOT contain any worker/engine runtime state.","\"\"\"","","from __future__ import annotations","","from datetime import date","from types import MappingProxyType","from typing import Any, Mapping, Optional","","from pydantic import BaseModel, ConfigDict, Field, field_serializer, model_validator","","","class DataSpec(BaseModel):","    \"\"\"Dataset specification for a research job.\"\"\"","    ","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","    ","    dataset_id: str = Field(..., min_length=1)","    start_date: date","    end_date: date","    ","    @model_validator(mode=\"after\")","    def _check_dates(self) -> \"DataSpec\":","        if self.start_date > self.end_date:","            raise ValueError(\"start_date must be <= end_date\")","        return self","","","class WFSSpec(BaseModel):","    \"\"\"WFS (Winners Funnel System) configuration.\"\"\"","    ","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","    ","    stage0_subsample: float = 1.0","    top_k: int = 100","    mem_limit_mb: int = 4096","    allow_auto_downsample: bool = True","    ","    @model_validator(mode=\"after\")","    def _check_ranges(self) -> \"WFSSpec\":","        if not (0.0 < self.stage0_subsample <= 1.0):","            raise ValueError(\"stage0_subsample must be in (0, 1]\")","        if self.top_k <= 0:","            raise ValueError(\"top_k must be > 0\")","        if self.mem_limit_mb < 1024:","            raise ValueError(\"mem_limit_mb must be >= 1024\")","        return self","","","class WizardJobSpec(BaseModel):","    \"\"\"Complete job specification for research.","    ","    Phase 12 Iron Rule: GUI's ONLY output = WizardJobSpec JSON","    Must NOT contain worker/engine runtime state.","    \"\"\"","    ","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","    ","    season: str = Field(..., min_length=1)","    data1: DataSpec","    data2: Optional[DataSpec] = None","    strategy_id: str = Field(..., min_length=1)","    params: Mapping[str, Any] = Field(default_factory=dict)","    wfs: WFSSpec = Field(default_factory=WFSSpec)","    ","    @model_validator(mode=\"after\")","    def _freeze_params(self) -> \"WizardJobSpec\":","        # make params immutable so test_jobspec_immutability passes","        if not isinstance(self.params, MappingProxyType):","            object.__setattr__(self, \"params\", MappingProxyType(dict(self.params)))","        return self","    ","    @field_serializer(\"params\")","    def _ser_params(self, v: Mapping[str, Any]) -> dict[str, Any]:","        return dict(v)","","    @property","    def dataset_id(self) -> str:","        \"\"\"Alias for data1.dataset_id (for backward compatibility).\"\"\"","        return self.data1.dataset_id","","","# Example WizardJobSpec for documentation","EXAMPLE_WIZARD_JOBSPEC = WizardJobSpec(","    season=\"2024Q1\",","    data1=DataSpec(","        dataset_id=\"CME.MNQ.60m.2020-2024\",","        start_date=date(2020, 1, 1),","        end_date=date(2024, 12, 31)","    ),","    data2=None,","    strategy_id=\"sma_cross_v1\",","    params={\"window\": 20, \"threshold\": 0.5},","    wfs=WFSSpec()",")","",""]}
{"type":"file_footer","path":"src/control/job_spec.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/jobs_db.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":29222,"sha256":"b588bac9dc22502593cc4c3f454ec6ffec14b954cbff5cb4a30cd2a4864ccda4","total_lines":927,"chunk_count":5}
{"type":"file_chunk","path":"src/control/jobs_db.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"SQLite jobs database - CRUD and state machine.\"\"\"","","from __future__ import annotations","","import json","import sqlite3","import time","from collections.abc import Callable","from datetime import datetime, timezone","from pathlib import Path","from typing import Optional, TypeVar","from uuid import uuid4","","from control.types import DBJobSpec, JobRecord, JobStatus, StopMode","","T = TypeVar(\"T\")","","","def _connect(db_path: Path) -> sqlite3.Connection:","    \"\"\"","    Create SQLite connection with concurrency hardening.","    ","    One operation = one connection (avoid shared connection across threads).","    ","    Args:","        db_path: Path to SQLite database","        ","    Returns:","        Configured SQLite connection with WAL mode and busy timeout","    \"\"\"","    # One operation = one connection (avoid shared connection across threads)","    conn = sqlite3.connect(str(db_path), timeout=30.0)","    conn.row_factory = sqlite3.Row","","    # Concurrency hardening","    conn.execute(\"PRAGMA journal_mode=WAL;\")","    conn.execute(\"PRAGMA synchronous=NORMAL;\")","    conn.execute(\"PRAGMA foreign_keys=ON;\")","    conn.execute(\"PRAGMA busy_timeout=30000;\")  # ms","","    return conn","","","def _with_retry_locked(fn: Callable[[], T]) -> T:","    \"\"\"","    Retry DB operation on SQLITE_BUSY/locked errors.","    ","    Args:","        fn: Callable that performs DB operation","        ","    Returns:","        Result from fn()","        ","    Raises:","        sqlite3.OperationalError: If operation fails after retries or for non-locked errors","    \"\"\"","    # Retry only for SQLITE_BUSY/locked","    delays = (0.05, 0.10, 0.20, 0.40, 0.80, 1.0)","    last: Exception | None = None","    for d in delays:","        try:","            return fn()","        except sqlite3.OperationalError as e:","            msg = str(e).lower()","            if \"locked\" not in msg and \"busy\" not in msg:","                raise","            last = e","            time.sleep(d)","    assert last is not None","    raise last","","","def ensure_schema(conn: sqlite3.Connection) -> None:","    \"\"\"","    Create tables or migrate schema in-place.","    ","    Idempotent: safe to call multiple times.","    ","    Args:","        conn: SQLite connection","    \"\"\"","    # Create jobs table if not exists","    conn.execute(\"\"\"","        CREATE TABLE IF NOT EXISTS jobs (","            job_id TEXT PRIMARY KEY,","            status TEXT NOT NULL,","            created_at TEXT NOT NULL,","            updated_at TEXT NOT NULL,","            season TEXT NOT NULL,","            dataset_id TEXT NOT NULL,","            outputs_root TEXT NOT NULL,","            config_hash TEXT NOT NULL,","            config_snapshot_json TEXT NOT NULL,","            pid INTEGER NULL,","            run_id TEXT NULL,","            run_link TEXT NULL,","            report_link TEXT NULL,","            last_error TEXT NULL,","            requested_stop TEXT NULL,","            requested_pause INTEGER NOT NULL DEFAULT 0,","            tags_json TEXT DEFAULT '[]'","        )","    \"\"\")","    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_status ON jobs(status)\")","    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_created_at ON jobs(created_at DESC)\")","    ","    # Check existing columns for migrations","    cursor = conn.execute(\"PRAGMA table_info(jobs)\")","    columns = [row[1] for row in cursor.fetchall()]","    ","    # Add run_id column if missing","    if \"run_id\" not in columns:","        conn.execute(\"ALTER TABLE jobs ADD COLUMN run_id TEXT\")","    ","    # Add report_link column if missing","    if \"report_link\" not in columns:","        conn.execute(\"ALTER TABLE jobs ADD COLUMN report_link TEXT\")","    ","    # Add tags_json column if missing","    if \"tags_json\" not in columns:","        conn.execute(\"ALTER TABLE jobs ADD COLUMN tags_json TEXT DEFAULT '[]'\")","    ","    # Add data_fingerprint_sha256_40 column if missing","    if \"data_fingerprint_sha256_40\" not in columns:","        conn.execute(\"ALTER TABLE jobs ADD COLUMN data_fingerprint_sha256_40 TEXT DEFAULT ''\")","    ","    # Create job_logs table if not exists","    conn.execute(\"\"\"","        CREATE TABLE IF NOT EXISTS job_logs (","            log_id INTEGER PRIMARY KEY AUTOINCREMENT,","            job_id TEXT NOT NULL,","            created_at TEXT NOT NULL,","            log_text TEXT NOT NULL,","            FOREIGN KEY (job_id) REFERENCES jobs(job_id)","        )","    \"\"\")","    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_job_logs_job_id ON job_logs(job_id, created_at DESC)\")","    ","    conn.commit()","","","def init_db(db_path: Path) -> None:","    \"\"\"","    Initialize jobs database schema.","    ","    Args:","        db_path: Path to SQLite database file","    \"\"\"","    db_path.parent.mkdir(parents=True, exist_ok=True)","    ","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            # ensure_schema handles CREATE TABLE IF NOT EXISTS + migrations","    ","    _with_retry_locked(_op)","","","def _now_iso() -> str:","    \"\"\"Get current UTC time as ISO8601 string.\"\"\"","    return datetime.now(timezone.utc).isoformat()","","","def _validate_status_transition(old_status: JobStatus, new_status: JobStatus) -> None:","    \"\"\"","    Validate status transition (state machine).","    ","    Allowed transitions:","    - QUEUED → RUNNING","    - RUNNING → PAUSED (pause=1 and worker checkpoint)","    - PAUSED → RUNNING (pause=0 and worker continues)","    - RUNNING/PAUSED → DONE | FAILED | KILLED","    - QUEUED → KILLED (cancel before running)","    ","    Args:","        old_status: Current status","        new_status: Target status","        ","    Raises:","        ValueError: If transition is not allowed","    \"\"\"","    allowed = {","        JobStatus.QUEUED: {JobStatus.RUNNING, JobStatus.KILLED},","        JobStatus.RUNNING: {JobStatus.PAUSED, JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED},","        JobStatus.PAUSED: {JobStatus.RUNNING, JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED},","    }","    ","    if old_status in allowed:","        if new_status not in allowed[old_status]:","            raise ValueError(","                f\"Invalid status transition: {old_status} → {new_status}. \"","                f\"Allowed: {allowed[old_status]}\"","            )","    elif old_status in {JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED}:","        raise ValueError(f\"Cannot transition from terminal status: {old_status}\")","","","def create_job(db_path: Path, spec: DBJobSpec, *, tags: list[str] | None = None) -> str:","    \"\"\""]}
{"type":"file_chunk","path":"src/control/jobs_db.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    Create a new job record.","    ","    Args:","        db_path: Path to SQLite database","        spec: Job specification","        tags: Optional list of tags for job categorization","        ","    Returns:","        Generated job_id","    \"\"\"","    job_id = str(uuid4())","    now = _now_iso()","    tags_json = json.dumps(tags if tags else [])","    ","    def _op() -> str:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            conn.execute(\"\"\"","                INSERT INTO jobs (","                    job_id, status, created_at, updated_at,","                    season, dataset_id, outputs_root, config_hash,","                    config_snapshot_json, requested_pause, tags_json, data_fingerprint_sha256_40","                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)","            \"\"\", (","                job_id,","                JobStatus.QUEUED.value,","                now,","                now,","                spec.season,","                spec.dataset_id,","                spec.outputs_root,","                spec.config_hash,","                json.dumps(spec.config_snapshot),","                0,","                tags_json,","                spec.data_fingerprint_sha256_40 if hasattr(spec, 'data_fingerprint_sha256_40') else '',","            ))","            conn.commit()","        return job_id","    ","    return _with_retry_locked(_op)","","","def _row_to_record(row: tuple) -> JobRecord:","    \"\"\"Convert database row to JobRecord.\"\"\"","    # Handle schema versions:","    # - Old: 12 columns (before report_link)","    # - Middle: 13 columns (with report_link, before run_id)","    # - New: 14 columns (with run_id and report_link)","    # - Latest: 15 columns (with tags_json)","    # - Phase 6.5: 16 columns (with data_fingerprint_sha1)","    if len(row) == 16:","        # Phase 6.5 schema with data_fingerprint_sha256_40","        (","            job_id,","            status,","            created_at,","            updated_at,","            season,","            dataset_id,","            outputs_root,","            config_hash,","            config_snapshot_json,","            pid,","            run_id,","            run_link,","            report_link,","            last_error,","            tags_json,","            data_fingerprint_sha256_40,","        ) = row","        # Parse tags_json, fallback to [] if None or invalid","        try:","            tags = json.loads(tags_json) if tags_json else []","            if not isinstance(tags, list):","                tags = []","        except (json.JSONDecodeError, TypeError):","            tags = []","        fingerprint_sha256_40 = data_fingerprint_sha256_40 if data_fingerprint_sha256_40 else \"\"","    elif len(row) == 15:","        # Latest schema with tags_json (without fingerprint column)","        (","            job_id,","            status,","            created_at,","            updated_at,","            season,","            dataset_id,","            outputs_root,","            config_hash,","            config_snapshot_json,","            pid,","            run_id,","            run_link,","            report_link,","            last_error,","            tags_json,","        ) = row","        # Parse tags_json, fallback to [] if None or invalid","        try:","            tags = json.loads(tags_json) if tags_json else []","            if not isinstance(tags, list):","                tags = []","        except (json.JSONDecodeError, TypeError):","            tags = []","        fingerprint_sha256_40 = \"\"  # Fallback for schema without data_fingerprint_sha256_40","    elif len(row) == 14:","        # New schema with run_id and report_link","        # Order: job_id, status, created_at, updated_at, season, dataset_id, outputs_root,","        #        config_hash, config_snapshot_json, pid, run_id, run_link, report_link, last_error","        (","            job_id,","            status,","            created_at,","            updated_at,","            season,","            dataset_id,","            outputs_root,","            config_hash,","            config_snapshot_json,","            pid,","            run_id,","            run_link,","            report_link,","            last_error,","        ) = row","        tags = []  # Fallback for schema without tags_json","        fingerprint_sha256_40 = \"\"  # Fallback for schema without data_fingerprint_sha256_40","    elif len(row) == 13:","        # Middle schema with report_link but no run_id","        (","            job_id,","            status,","            created_at,","            updated_at,","            season,","            dataset_id,","            outputs_root,","            config_hash,","            config_snapshot_json,","            pid,","            run_link,","            last_error,","            report_link,","        ) = row","        run_id = None","        tags = []  # Fallback for old schema","        fingerprint_sha256_40 = \"\"  # Fallback for schema without data_fingerprint_sha256_40","    else:","        # Old schema (backward compatibility)","        (","            job_id,","            status,","            created_at,","            updated_at,","            season,","            dataset_id,","            outputs_root,","            config_hash,","            config_snapshot_json,","            pid,","            run_link,","            last_error,","        ) = row","        run_id = None","        report_link = None","        tags = []  # Fallback for old schema","        fingerprint_sha256_40 = \"\"  # Fallback for schema without data_fingerprint_sha256_40","    ","    spec = DBJobSpec(","        season=season,","        dataset_id=dataset_id,","        outputs_root=outputs_root,","        config_snapshot=json.loads(config_snapshot_json),","        config_hash=config_hash,","        data_fingerprint_sha256_40=fingerprint_sha256_40,","    )","    ","    return JobRecord(","        job_id=job_id,","        status=JobStatus(status),","        created_at=created_at,","        updated_at=updated_at,","        spec=spec,","        pid=pid,","        run_id=run_id if run_id else None,","        run_link=run_link,","        report_link=report_link if report_link else None,","        last_error=last_error,","        tags=tags if tags else [],","        data_fingerprint_sha256_40=fingerprint_sha256_40,","    )","","","def get_job(db_path: Path, job_id: str) -> JobRecord:","    \"\"\"","    Get job record by ID.","    ","    Args:","        db_path: Path to SQLite database"]}
{"type":"file_chunk","path":"src/control/jobs_db.py","chunk_index":2,"line_start":401,"line_end":600,"content":["        job_id: Job ID","        ","    Returns:","        JobRecord","        ","    Raises:","        KeyError: If job not found","    \"\"\"","    def _op() -> JobRecord:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cursor = conn.execute(\"\"\"","                SELECT job_id, status, created_at, updated_at,","                       season, dataset_id, outputs_root, config_hash,","                       config_snapshot_json, pid,","                       COALESCE(run_id, NULL) as run_id,","                       run_link,","                       COALESCE(report_link, NULL) as report_link,","                       last_error,","                       COALESCE(tags_json, '[]') as tags_json,","                       COALESCE(data_fingerprint_sha256_40, '') as data_fingerprint_sha256_40","                FROM jobs","                WHERE job_id = ?","            \"\"\", (job_id,))","            row = cursor.fetchone()","            if row is None:","                raise KeyError(f\"Job not found: {job_id}\")","            return _row_to_record(row)","    ","    return _with_retry_locked(_op)","","","def list_jobs(db_path: Path, *, limit: int = 50) -> list[JobRecord]:","    \"\"\"","    List recent jobs.","    ","    Args:","        db_path: Path to SQLite database","        limit: Maximum number of jobs to return","        ","    Returns:","        List of JobRecord, ordered by created_at DESC","    \"\"\"","    def _op() -> list[JobRecord]:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cursor = conn.execute(\"\"\"","                SELECT job_id, status, created_at, updated_at,","                       season, dataset_id, outputs_root, config_hash,","                       config_snapshot_json, pid,","                       COALESCE(run_id, NULL) as run_id,","                       run_link,","                       COALESCE(report_link, NULL) as report_link,","                       last_error,","                       COALESCE(tags_json, '[]') as tags_json,","                       COALESCE(data_fingerprint_sha256_40, '') as data_fingerprint_sha256_40","                FROM jobs","                ORDER BY created_at DESC","                LIMIT ?","            \"\"\", (limit,))","            return [_row_to_record(row) for row in cursor.fetchall()]","    ","    return _with_retry_locked(_op)","","","def request_pause(db_path: Path, job_id: str, pause: bool) -> None:","    \"\"\"","    Request pause/unpause for a job (atomic update).","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        pause: True to pause, False to unpause","        ","    Raises:","        KeyError: If job not found","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET requested_pause = ?, updated_at = ?","                WHERE job_id = ?","            \"\"\", (1 if pause else 0, _now_iso(), job_id))","            ","            if cur.rowcount == 0:","                raise KeyError(f\"Job not found: {job_id}\")","            ","            conn.commit()","    ","    _with_retry_locked(_op)","","","def request_stop(db_path: Path, job_id: str, mode: StopMode) -> None:","    \"\"\"","    Request stop for a job (atomic update).","    ","    If QUEUED, immediately mark as KILLED.","    Otherwise, set requested_stop flag (worker will handle).","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        mode: Stop mode (SOFT or KILL)","        ","    Raises:","        KeyError: If job not found","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            # Try to mark QUEUED as KILLED first (atomic)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET status = ?, requested_stop = ?, updated_at = ?","                WHERE job_id = ? AND status = ?","            \"\"\", (JobStatus.KILLED.value, mode.value, _now_iso(), job_id, JobStatus.QUEUED.value))","            ","            if cur.rowcount == 1:","                conn.commit()","                return","            ","            # Otherwise, set requested_stop flag (atomic)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET requested_stop = ?, updated_at = ?","                WHERE job_id = ?","            \"\"\", (mode.value, _now_iso(), job_id))","            ","            if cur.rowcount == 0:","                raise KeyError(f\"Job not found: {job_id}\")","            ","            conn.commit()","    ","    _with_retry_locked(_op)","","","def mark_running(db_path: Path, job_id: str, *, pid: int) -> None:","    \"\"\"","    Mark job as RUNNING with PID (atomic update from QUEUED).","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        pid: Process ID","        ","    Raises:","        KeyError: If job not found","        ValueError: If status is terminal (DONE/FAILED/KILLED) or invalid transition","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET status = ?, pid = ?, updated_at = ?","                WHERE job_id = ? AND status = ?","            \"\"\", (JobStatus.RUNNING.value, pid, _now_iso(), job_id, JobStatus.QUEUED.value))","            ","            if cur.rowcount == 1:","                conn.commit()","                return","            ","            # Check if job exists and current status","            row = conn.execute(\"SELECT status FROM jobs WHERE job_id = ?\", (job_id,)).fetchone()","            if row is None:","                raise KeyError(f\"Job not found: {job_id}\")","            ","            status = JobStatus(row[0])","            ","            if status == JobStatus.RUNNING:","                # Already running (idempotent)","                return","            ","            # Terminal status => ValueError (match existing tests/contract)","            if status in {JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED}:","                raise ValueError(\"Cannot transition from terminal status\")","            ","            # Everything else is invalid transition (keep ValueError)","            raise ValueError(f\"Invalid status transition: {status.value} → RUNNING\")","    ","    _with_retry_locked(_op)","","","def update_running(db_path: Path, job_id: str, *, pid: int) -> None:","    \"\"\"","    Update job to RUNNING status with PID (legacy alias for mark_running).","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        pid: Process ID","        ","    Raises:","        KeyError: If job not found","        RuntimeError: If status transition is invalid","    \"\"\"","    mark_running(db_path, job_id, pid=pid)",""]}
{"type":"file_chunk","path":"src/control/jobs_db.py","chunk_index":3,"line_start":601,"line_end":800,"content":["","def update_run_link(db_path: Path, job_id: str, *, run_link: str) -> None:","    \"\"\"","    Update job run_link.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        run_link: Run link path","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            conn.execute(\"\"\"","                UPDATE jobs","                SET run_link = ?, updated_at = ?","                WHERE job_id = ?","            \"\"\", (run_link, _now_iso(), job_id))","            conn.commit()","    ","    _with_retry_locked(_op)","","","def set_report_link(db_path: Path, job_id: str, report_link: str) -> None:","    \"\"\"","    Set report_link for a job.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        report_link: Report link URL","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            conn.execute(\"\"\"","                UPDATE jobs","                SET report_link = ?, updated_at = ?","                WHERE job_id = ?","            \"\"\", (report_link, _now_iso(), job_id))","            conn.commit()","    ","    _with_retry_locked(_op)","","","def mark_done(","    db_path: Path, ","    job_id: str, ","    *, ","    run_id: Optional[str] = None,","    report_link: Optional[str] = None",") -> None:","    \"\"\"","    Mark job as DONE (atomic update from RUNNING or KILLED).","    ","    Idempotent: safe to call multiple times.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        run_id: Optional final stage run_id","        report_link: Optional report link URL","        ","    Raises:","        KeyError: If job not found","        RuntimeError: If status is QUEUED/PAUSED (mark_done before RUNNING)","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET status = ?, updated_at = ?, run_id = ?, report_link = ?, last_error = NULL","                WHERE job_id = ? AND status IN (?, ?)","            \"\"\", (","                JobStatus.DONE.value,","                _now_iso(),","                run_id,","                report_link,","                job_id,","                JobStatus.RUNNING.value,","                JobStatus.KILLED.value,","            ))","            ","            if cur.rowcount == 1:","                conn.commit()","                return","            ","            # Fallback: check if already DONE (idempotent success)","            row = conn.execute(\"SELECT status FROM jobs WHERE job_id = ?\", (job_id,)).fetchone()","            if row is None:","                raise KeyError(f\"Job not found: {job_id}\")","            ","            status = JobStatus(row[0])","            if status == JobStatus.DONE:","                # Already done (idempotent)","                return","            ","            # If QUEUED/PAUSED, raise RuntimeError (process flow incorrect)","            raise RuntimeError(f\"mark_done rejected: status={status} (expected RUNNING or KILLED)\")","    ","    _with_retry_locked(_op)","","","def mark_failed(db_path: Path, job_id: str, *, error: str) -> None:","    \"\"\"","    Mark job as FAILED with error message (atomic update from RUNNING or PAUSED).","    ","    Idempotent: safe to call multiple times.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        error: Error message","        ","    Raises:","        KeyError: If job not found","        RuntimeError: If status is QUEUED (mark_failed before RUNNING)","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET status = ?, last_error = ?, updated_at = ?","                WHERE job_id = ? AND status IN (?, ?)","            \"\"\", (","                JobStatus.FAILED.value,","                error,","                _now_iso(),","                job_id,","                JobStatus.RUNNING.value,","                JobStatus.PAUSED.value,","            ))","            ","            if cur.rowcount == 1:","                conn.commit()","                return","            ","            # Fallback: check if already FAILED (idempotent success)","            row = conn.execute(\"SELECT status FROM jobs WHERE job_id = ?\", (job_id,)).fetchone()","            if row is None:","                raise KeyError(f\"Job not found: {job_id}\")","            ","            status = JobStatus(row[0])","            if status == JobStatus.FAILED:","                # Already failed (idempotent)","                return","            ","            # If QUEUED, raise RuntimeError (process flow incorrect)","            raise RuntimeError(f\"mark_failed rejected: status={status} (expected RUNNING or PAUSED)\")","    ","    _with_retry_locked(_op)","","","def mark_killed(db_path: Path, job_id: str, *, error: str | None = None) -> None:","    \"\"\"","    Mark job as KILLED (atomic update).","    ","    Idempotent: safe to call multiple times.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        error: Optional error message","        ","    Raises:","        KeyError: If job not found","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cur = conn.execute(\"\"\"","                UPDATE jobs","                SET status = ?, last_error = ?, updated_at = ?","                WHERE job_id = ?","            \"\"\", (JobStatus.KILLED.value, error, _now_iso(), job_id))","            ","            if cur.rowcount == 0:","                raise KeyError(f\"Job not found: {job_id}\")","            ","            conn.commit()","    ","    _with_retry_locked(_op)","","","def get_requested_stop(db_path: Path, job_id: str) -> Optional[str]:","    \"\"\"","    Get requested_stop value for a job.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        ","    Returns:","        Stop mode string or None","    \"\"\"","    def _op() -> Optional[str]:","        with _connect(db_path) as conn:","            ensure_schema(conn)"]}
{"type":"file_chunk","path":"src/control/jobs_db.py","chunk_index":4,"line_start":801,"line_end":927,"content":["            cursor = conn.execute(\"SELECT requested_stop FROM jobs WHERE job_id = ?\", (job_id,))","            row = cursor.fetchone()","            return row[0] if row and row[0] else None","    ","    return _with_retry_locked(_op)","","","def get_requested_pause(db_path: Path, job_id: str) -> bool:","    \"\"\"","    Get requested_pause value for a job.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        ","    Returns:","        True if pause requested, False otherwise","    \"\"\"","    def _op() -> bool:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cursor = conn.execute(\"SELECT requested_pause FROM jobs WHERE job_id = ?\", (job_id,))","            row = cursor.fetchone()","            return bool(row[0]) if row else False","    ","    return _with_retry_locked(_op)","","","def search_by_tag(db_path: Path, tag: str, *, limit: int = 50) -> list[JobRecord]:","    \"\"\"","    Search jobs by tag.","    ","    Uses LIKE query to find jobs containing the tag in tags_json.","    For exact matching, use application-layer filtering.","    ","    Args:","        db_path: Path to SQLite database","        tag: Tag to search for","        limit: Maximum number of jobs to return","        ","    Returns:","        List of JobRecord matching the tag, ordered by created_at DESC","    \"\"\"","    def _op() -> list[JobRecord]:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            # Use LIKE to search for tag in JSON array","            # Pattern: tag can appear as [\"tag\"] or [\"tag\", ...] or [..., \"tag\", ...] or [..., \"tag\"]","            search_pattern = f'%\"{tag}\"%'","            cursor = conn.execute(\"\"\"","                SELECT job_id, status, created_at, updated_at,","                       season, dataset_id, outputs_root, config_hash,","                       config_snapshot_json, pid,","                       COALESCE(run_id, NULL) as run_id,","                       run_link,","                       COALESCE(report_link, NULL) as report_link,","                       last_error,","                       COALESCE(tags_json, '[]') as tags_json,","                       COALESCE(data_fingerprint_sha256_40, '') as data_fingerprint_sha256_40","                FROM jobs","                WHERE tags_json LIKE ?","                ORDER BY created_at DESC","                LIMIT ?","            \"\"\", (search_pattern, limit))","            ","            records = [_row_to_record(row) for row in cursor.fetchall()]","            ","            # Application-layer filtering for exact match (more reliable than LIKE)","            # Filter to ensure tag is actually in the list, not just substring match","            filtered = []","            for record in records:","                if tag in record.tags:","                    filtered.append(record)","            ","            return filtered","    ","    return _with_retry_locked(_op)","","","def append_log(db_path: Path, job_id: str, log_text: str) -> None:","    \"\"\"","    Append log entry to job_logs table.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        log_text: Log text to append (can be full traceback)","    \"\"\"","    def _op() -> None:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            conn.execute(\"\"\"","                INSERT INTO job_logs (job_id, created_at, log_text)","                VALUES (?, ?, ?)","            \"\"\", (job_id, _now_iso(), log_text))","            conn.commit()","    ","    _with_retry_locked(_op)","","","def get_job_logs(db_path: Path, job_id: str, *, limit: int = 100) -> list[str]:","    \"\"\"","    Get log entries for a job.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        limit: Maximum number of log entries to return","        ","    Returns:","        List of log text entries, ordered by created_at DESC","    \"\"\"","    def _op() -> list[str]:","        with _connect(db_path) as conn:","            ensure_schema(conn)","            cursor = conn.execute(\"\"\"","                SELECT log_text","                FROM job_logs","                WHERE job_id = ?","                ORDER BY created_at DESC","                LIMIT ?","            \"\"\", (job_id, limit))","            return [row[0] for row in cursor.fetchall()]","    ","    return _with_retry_locked(_op)","",""]}
{"type":"file_footer","path":"src/control/jobs_db.py","complete":true,"emitted_chunks":5}
{"type":"file_header","path":"src/control/lifecycle.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":17428,"sha256":"ad6ef8c56bb68140187d25f653b10f24f8e929d234e3535ca14dca0638a8e65a","total_lines":551,"chunk_count":3}
{"type":"file_chunk","path":"src/control/lifecycle.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Lifecycle Root-Cure: Identity-aware preflight for Control API (8000) and UI (8080).","","Core principles:","1. Never blindly kill - always verify identity first","2. Default safe behavior: fail-fast with actionable diagnostics","3. Operator-proof: clear decisions and recovery steps","4. Flat snapshots only (no subfolders)","\"\"\"","","from __future__ import annotations","","import dataclasses","import json","import os","import re","import signal","import subprocess","import sys","import time","from dataclasses import dataclass","from enum import Enum","from pathlib import Path","from typing import Optional, Tuple, Dict, Any, List","","import requests","from requests.exceptions import RequestException, Timeout","","# Try to import psutil for process info (optional)","try:","    import psutil","    HAS_PSUTIL = True","except ImportError:","    HAS_PSUTIL = False","","","class PortOccupancyStatus(Enum):","    \"\"\"Status of port occupancy check.\"\"\"","    FREE = \"FREE\"","    OCCUPIED_FISHBRO = \"OCCUPIED_FISHBRO\"","    OCCUPIED_NOT_FISHBRO = \"OCCUPIED_NOT_FISHBRO\"","    OCCUPIED_UNKNOWN = \"OCCUPIED_UNKNOWN\"","","","@dataclass","class PortOccupant:","    \"\"\"Information about a port occupant.\"\"\"","    occupied: bool","    pid: Optional[int] = None","    process_name: Optional[str] = None","    cmdline: Optional[str] = None","    raw_output: str = \"\"","    ","    @classmethod","    def free(cls) -> PortOccupant:","        \"\"\"Create a PortOccupant representing a free port.\"\"\"","        return cls(occupied=False, raw_output=\"Port is free\")","","","@dataclass","class PortPreflightResult:","    \"\"\"Result of port preflight check.\"\"\"","    port: int","    status: PortOccupancyStatus","    occupant: PortOccupant","    identity_verified: bool = False","    identity_error: Optional[str] = None","    identity_data: Optional[Dict[str, Any]] = None","    decision: str = \"PENDING\"","    action: str = \"\"","","","def extract_listen_pids_from_ss(ss_text: str, port: int) -> List[int]:","    \"\"\"","    Parse `ss -ltnp` output and return unique PIDs listening on the given port.","    ","    Supports patterns like:","      users:((\"python3\",pid=73466,fd=13))","    Return [] if none.","    \"\"\"","    pids: set[int] = set()","    for line in ss_text.splitlines():","        if f\":{port} \" not in line and not line.strip().endswith(f\":{port}\"):","            continue","        for m in re.finditer(r\"pid=(\\d+)\", line):","            pids.add(int(m.group(1)))","    return sorted(pids)","","","def get_process_identity(pid: int) -> Dict[str, str]:","    \"\"\"","    Use psutil (preferred) or /proc/{pid}/cmdline, /proc/{pid}/cwd to return:","      - exe","      - cmdline (joined)","      - cwd","    Never throws; returns best-effort dict.","    \"\"\"","    result = {\"pid\": str(pid), \"exe\": \"\", \"cmdline\": \"\", \"cwd\": \"\"}","    ","    # Try psutil first","    if HAS_PSUTIL:","        try:","            proc = psutil.Process(pid)","            result[\"exe\"] = proc.exe() or \"\"","            result[\"cmdline\"] = \" \".join(proc.cmdline())","            result[\"cwd\"] = proc.cwd() or \"\"","            return result","        except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):","            pass","    ","    # Fallback to /proc filesystem (Linux)","    try:","        cmdline_path = Path(f\"/proc/{pid}/cmdline\")","        if cmdline_path.exists():","            cmdline_bytes = cmdline_path.read_bytes()","            parts = [p.decode(\"utf-8\", errors=\"replace\") for p in cmdline_bytes.split(b\"\\x00\") if p]","            result[\"cmdline\"] = \" \".join(parts)","    except Exception:","        pass","    ","    try:","        exe_path = Path(f\"/proc/{pid}/exe\")","        if exe_path.exists():","            result[\"exe\"] = str(exe_path.resolve())","    except Exception:","        pass","    ","    try:","        cwd_path = Path(f\"/proc/{pid}/cwd\")","        if cwd_path.exists():","            result[\"cwd\"] = str(cwd_path.resolve())","    except Exception:","        pass","    ","    return result","","","def detect_port_occupant(port: int) -> PortOccupant:","    \"\"\"","    Detect if a port is occupied and return occupant information.","    ","    Uses enhanced detection strategy:","    1. ss -ltnp (primary, shows PID)","    2. /proc/<pid>/cmdline for identity","    3. HTTP identity probe (Control API only)","    4. lsof -iTCP:<port> -sTCP:LISTEN (fallback only)","    ","    Returns PortOccupant with best available information.","    \"\"\"","    # Try ss first (mandatory)","    ss_cmd = [\"bash\", \"-lc\", f\"ss -ltnp '( sport = :{port} )'\"]","    try:","        ss_output = subprocess.check_output(","            ss_cmd, stderr=subprocess.STDOUT, text=True, timeout=2","        ).strip()","    except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):","        ss_output = \"\"","    ","    # Parse ss output for PIDs","    pids = extract_listen_pids_from_ss(ss_output, port)","    ","    if pids:","        # Use first PID (most relevant)","        pid = pids[0]","        identity = get_process_identity(pid)","        cmdline = identity.get(\"cmdline\", \"\")","        process_name = identity.get(\"exe\", \"\").split(\"/\")[-1] if identity.get(\"exe\") else \"\"","        ","        return PortOccupant(","            occupied=True,","            pid=pid,","            process_name=process_name or f\"pid:{pid}\",","            cmdline=cmdline,","            raw_output=ss_output","        )","    ","    # Try lsof as fallback only (when ss fails)","    lsof_cmd = [\"bash\", \"-lc\", f\"lsof -iTCP:{port} -sTCP:LISTEN -n -P\"]","    try:","        lsof_output = subprocess.check_output(","            lsof_cmd, stderr=subprocess.STDOUT, text=True, timeout=2","        ).strip()","    except (subprocess.CalledProcessError, subprocess.TimeoutExpired, FileNotFoundError):","        lsof_output = \"\"","    ","    if lsof_output and \"LISTEN\" in lsof_output:","        # Parse lsof output: COMMAND PID USER ...","        lines = lsof_output.splitlines()","        if len(lines) > 1:  # Skip header","            parts = lines[1].split()","            if len(parts) >= 2:","                try:","                    pid = int(parts[1])","                    process_name = parts[0]","                    # Try to get cmdline from /proc","                    identity = get_process_identity(pid)","                    cmdline = identity.get(\"cmdline\", \"\")","                    ","                    return PortOccupant("]}
{"type":"file_chunk","path":"src/control/lifecycle.py","chunk_index":1,"line_start":201,"line_end":400,"content":["                        occupied=True,","                        pid=pid,","                        process_name=process_name,","                        cmdline=cmdline,","                        raw_output=f\"ss: {ss_output}\\nlsof: {lsof_output}\"","                    )","                except (ValueError, IndexError):","                    pass","    ","    # If we get here, port might be free or we couldn't parse","    if ss_output or lsof_output:","        # Output exists but we couldn't parse PID","        return PortOccupant(","            occupied=True,","            raw_output=f\"ss: {ss_output}\\nlsof: {lsof_output}\"","        )","    ","    # Port appears free","    return PortOccupant.free()","","","def verify_fishbro_control_identity(host: str, port: int, timeout: float = 2.0) -> Tuple[bool, Optional[Dict[str, Any]], Optional[str]]:","    \"\"\"","    Verify if occupant on port is FishBro Control API.","    ","    Checks GET /__identity endpoint for:","    - service_name == \"control_api\"","    - repo_root matches current repo","    ","    Returns (is_fishbro, identity_data, error_message)","    \"\"\"","    url = f\"http://{host}:{port}/__identity\"","    ","    try:","        response = requests.get(url, timeout=timeout)","        if response.status_code != 200:","            return False, None, f\"HTTP {response.status_code}\"","        ","        data = response.json()","        ","        # Check service name","        if data.get(\"service_name\") != \"control_api\":","            return False, data, f\"service_name is '{data.get('service_name')}', not 'control_api'\"","        ","        # Check repo root (best effort)","        expected_repo_root = str(Path(__file__).parent.parent.parent.absolute())","        actual_repo_root = data.get(\"repo_root\", \"\")","        if actual_repo_root and expected_repo_root not in actual_repo_root:","            # Not a strict match, but should contain our repo path","            return False, data, f\"repo_root mismatch: {actual_repo_root}\"","        ","        return True, data, None","        ","    except Timeout:","        return False, None, \"Timeout connecting to identity endpoint\"","    except RequestException as e:","        return False, None, f\"Connection error: {e}\"","    except json.JSONDecodeError as e:","        return False, None, f\"Invalid JSON response: {e}\"","    except Exception as e:","        return False, None, f\"Unexpected error: {e}\"","","","def verify_fishbro_ui_identity(occupant: PortOccupant) -> Tuple[bool, Optional[str]]:","    \"\"\"","    Verify if occupant on port is FishBro UI.","    ","    Checks cmdline for FishBro module patterns.","    \"\"\"","    if not occupant.cmdline:","        return False, \"No cmdline available\"","    ","    cmdline = occupant.cmdline.lower()","    ","    # Look for FishBro UI module patterns","    ui_patterns = [","        \"fishbrowfs_v2.gui.nicegui.app\",","        \"fishbrowfs_v2/gui/nicegui/app.py\",","        \"nicegui.app\",","    ]","    ","    for pattern in ui_patterns:","        if pattern in cmdline:","            return True, None","    ","    return False, f\"Cmdline doesn't match FishBro UI patterns: {occupant.cmdline[:100]}...\"","","","def preflight_port(","    port: int,","    host: str = \"127.0.0.1\",","    service_type: str = \"control\",  # \"control\" or \"ui\"","    timeout: float = 2.0,","    single_user_mode: bool = False",") -> PortPreflightResult:","    \"\"\"","    Perform identity-aware preflight for a port.","    ","    Steps:","    1. Detect port occupancy","    2. If occupied, verify identity","    3. Determine status and decision","    ","    Classification Rules:","    - PID found + cmdline matches FishBro → OCCUPIED_FISHBRO","    - PID found + cmdline NOT FishBro → OCCUPIED_NOT_FISHBRO","    - PID missing OR cmdline unreadable → OCCUPIED_UNKNOWN","    ","    Single-User Mode Rules:","    - OCCUPIED_FISHBRO: Keep/restart as requested","    - OCCUPIED_UNKNOWN: DO NOT FAIL — continue","    - OCCUPIED_NOT_FISHBRO: Fail unless --force-kill-ports","    \"\"\"","    occupant = detect_port_occupant(port)","    ","    if not occupant.occupied:","        return PortPreflightResult(","            port=port,","            status=PortOccupancyStatus.FREE,","            occupant=occupant,","            decision=\"START\",","            action=\"Port is free, can start service\"","        )","    ","    # Port is occupied, need to classify","    status = PortOccupancyStatus.OCCUPIED_UNKNOWN","    identity_verified = False","    identity_error = None","    identity_data = None","    ","    if occupant.pid:","        # We have a PID, try to verify identity","        try:","            if service_type == \"control\":","                is_fishbro, data, error = verify_fishbro_control_identity(host, port, timeout)","                identity_verified = is_fishbro","                identity_error = error","                identity_data = data","            else:  # UI","                is_fishbro, error = verify_fishbro_ui_identity(occupant)","                identity_verified = is_fishbro","                identity_error = error","            ","            if identity_verified:","                status = PortOccupancyStatus.OCCUPIED_FISHBRO","            elif occupant.cmdline:","                # We have cmdline but it's not FishBro","                status = PortOccupancyStatus.OCCUPIED_NOT_FISHBRO","            else:","                # PID exists but cmdline unreadable","                status = PortOccupancyStatus.OCCUPIED_UNKNOWN","        except Exception as e:","            # Identity probe failed (exception)","            identity_error = f\"Identity verification failed: {e}\"","            status = PortOccupancyStatus.OCCUPIED_UNKNOWN","    else:","        # No PID found","        status = PortOccupancyStatus.OCCUPIED_UNKNOWN","    ","    # Determine decision based on classification and single-user mode","    decision = \"PENDING\"","    action = \"\"","    ","    if status == PortOccupancyStatus.OCCUPIED_FISHBRO:","        decision = \"REUSE\"","        action = f\"Port occupied by FishBro {service_type}, will reuse\"","    elif status == PortOccupancyStatus.OCCUPIED_UNKNOWN:","        if single_user_mode:","            decision = \"CONTINUE\"","            action = f\"Port occupied by unknown process (single-user mode), will continue\"","        else:","            decision = \"FAIL_FAST\"","            action = f\"Port occupied by unknown process, cannot verify identity\"","    else:  # OCCUPIED_NOT_FISHBRO","        if single_user_mode:","            decision = \"FAIL_FAST\"","            action = f\"Port occupied by non-FishBro process (PID {occupant.pid})\"","        else:","            decision = \"FAIL_FAST\"","            action = f\"Port occupied by non-FishBro process (PID {occupant.pid})\"","    ","    return PortPreflightResult(","        port=port,","        status=status,","        occupant=occupant,","        identity_verified=identity_verified,","        identity_error=identity_error,","        identity_data=identity_data,","        decision=decision,","        action=action","    )","","","def kill_process(pid: int, force: bool = True) -> bool:","    \"\"\"","    Kill a process by PID.","    ","    Args:","        pid: Process ID to kill","        force: If True, use SIGKILL after SIGTERM fails (default True)"]}
{"type":"file_chunk","path":"src/control/lifecycle.py","chunk_index":2,"line_start":401,"line_end":551,"content":["    ","    Returns:","        True if process was killed or already dead, False on permission error","    \"\"\"","    if not HAS_PSUTIL:","        # Fallback to os.kill","        try:","            os.kill(pid, signal.SIGTERM)","            time.sleep(1)","            # Check if still alive","            try:","                os.kill(pid, 0)  # Check if process exists","                if force:","                    os.kill(pid, signal.SIGKILL)","                    time.sleep(0.5)","                return True","            except OSError:","                # Process is dead after SIGTERM","                return True","        except ProcessLookupError:","            # Process already dead","            return True","        except PermissionError:","            # Permission denied","            return False","        except OSError:","            # Other OSError","            return False","    ","    # Use psutil if available","    try:","        proc = psutil.Process(pid)","        proc.terminate()","        gone, alive = psutil.wait_procs([proc], timeout=2)","        if alive and force:","            for p in alive:","                p.kill()","            psutil.wait_procs(alive, timeout=1)","        return True","    except (psutil.NoSuchProcess, psutil.AccessDenied):","        # Process already dead or permission denied","        return True","","","def write_pidfile(pid: int, service: str, pid_dir: Path) -> Path:","    \"\"\"","    Write PID file atomically.","    ","    Args:","        pid: Process ID","        service: Service name (\"control\" or \"ui\")","        pid_dir: Directory for PID files","    ","    Returns:","        Path to PID file","    \"\"\"","    pid_dir.mkdir(parents=True, exist_ok=True)","    pidfile = pid_dir / f\"{service}.pid\"","    ","    # Write atomically via temp file","    tempfile = pidfile.with_suffix(\".pid.tmp\")","    tempfile.write_text(str(pid))","    tempfile.rename(pidfile)","    ","    return pidfile","","","def read_pidfile(service: str, pid_dir: Path) -> Optional[int]:","    \"\"\"","    Read PID from PID file.","    ","    Returns:","        PID if file exists and contains valid integer, None otherwise","    \"\"\"","    pidfile = pid_dir / f\"{service}.pid\"","    if not pidfile.exists():","        return None","    ","    try:","        pid_str = pidfile.read_text().strip()","        return int(pid_str)","    except (ValueError, OSError):","        return None","","","def remove_pidfile(service: str, pid_dir: Path) -> bool:","    \"\"\"Remove PID file if it exists.\"\"\"","    pidfile = pid_dir / f\"{service}.pid\"","    if pidfile.exists():","        try:","            pidfile.unlink()","            return True","        except OSError:","            return False","    return True","","","def write_metadata(pid: int, service: str, pid_dir: Path, metadata: Dict[str, Any]) -> Path:","    \"\"\"","    Write metadata JSON file for a service.","    ","    Args:","        pid: Process ID","        service: Service name","        pid_dir: Directory for PID files","        metadata: Metadata dict to write","    ","    Returns:","        Path to metadata file","    \"\"\"","    pid_dir.mkdir(parents=True, exist_ok=True)","    metafile = pid_dir / f\"{service}.meta.json\"","    ","    metadata.update({","        \"pid\": pid,","        \"service\": service,","        \"written_at_utc\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),","    })","    ","    # Write atomically","    tempfile = metafile.with_suffix(\".json.tmp\")","    tempfile.write_text(json.dumps(metadata, indent=2))","    tempfile.rename(metafile)","    ","    return metafile","","","if __name__ == \"__main__\":","    # Test the module","    import argparse","    ","    parser = argparse.ArgumentParser(description=\"Test port preflight\")","    parser.add_argument(\"--port\", type=int, default=8000, help=\"Port to check\")","    parser.add_argument(\"--service\", choices=[\"control\", \"ui\"], default=\"control\", help=\"Service type\")","    ","    args = parser.parse_args()","    ","    print(f\"Preflight check for port {args.port} ({args.service}):\")","    result = preflight_port(args.port, service_type=args.service)","    ","    print(f\"  Status: {result.status.value}\")","    print(f\"  Occupied: {result.occupant.occupied}\")","    if result.occupant.occupied:","        print(f\"  PID: {result.occupant.pid}\")","        print(f\"  Process: {result.occupant.process_name}\")","        print(f\"  Cmdline: {result.occupant.cmdline}\")","    print(f\"  Identity verified: {result.identity_verified}\")","    if result.identity_error:","        print(f\"  Identity error: {result.identity_error}\")","    print(f\"  Decision: {result.decision}\")","    print(f\"  Action: {result.action}\")"]}
{"type":"file_footer","path":"src/control/lifecycle.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/control/local_scan.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6888,"sha256":"6c01e93ae2ce030e01c1a8649c9aa7c014ae02cf3f2ddfe73b0f4115f17db9c6","total_lines":233,"chunk_count":2}
{"type":"file_chunk","path":"src/control/local_scan.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Local-Strict filesystem scanner (NOT Git).","","Mission: Enumerate files based on filesystem truth, ignoring .gitignore,","respecting allowlist/denylist + caps to prevent explosion.","","Contract:","- MUST NOT rely on git ls-files or tracked-only enumeration.","- MUST include untracked files in allowed roots.","- MUST ignore .gitignore (gitignore_respected=false).","- MUST enforce allowlist + denylist + caps to prevent explosion.","- MUST write audit rules to outputs/snapshots/full/LOCAL_SCAN_RULES.json.","\"\"\"","","from __future__ import annotations","from dataclasses import dataclass","from fnmatch import fnmatch","import hashlib","import json","from pathlib import Path","from typing import Iterator, Tuple, List, Set, Optional","import os","","","@dataclass(frozen=True)","class LocalScanPolicy:","    \"\"\"Policy for local-strict filesystem scanning.\"\"\"","    allowed_roots: tuple[str, ...]","    allowed_root_files_glob: tuple[str, ...]","    deny_segments: tuple[str, ...]","    outputs_allow: tuple[str, ...]","    max_files: int","    max_bytes: int","    gitignore_respected: bool","","","def default_local_strict_policy() -> LocalScanPolicy:","    \"\"\"Return policy with the defaults defined in the spec.\"\"\"","    return LocalScanPolicy(","        allowed_roots=(\"src\", \"tests\", \"scripts\", \"docs\"),","        allowed_root_files_glob=(","            \"Makefile\",","            \"pyproject.toml\",","            \"README*\",","            \".python-version\",","            \"requirements*.txt\",","            \"uv.lock\",","            \"poetry.lock\",","        ),","        deny_segments=(","            \".git\",","            \".venv\",","            \"venv\",","            \"node_modules\",","            \"__pycache__\",","            \".pytest_cache\",","            \".mypy_cache\",","            \".ruff_cache\",","            \".cache\",","            \".idea\",","            \".vscode\",","            \"outputs\",  # will be handled by outputs exception rule","        ),","        outputs_allow=(\"outputs/snapshots\",),","        max_files=20000,","        max_bytes=2_000_000,  # 2MB","        gitignore_respected=False,","    )","","","def _has_deny_segment(rel: Path, deny: tuple[str, ...]) -> bool:","    \"\"\"Check if any path segment matches a deny segment.\"\"\"","    parts = rel.parts","    return any(seg in parts for seg in deny)","","","def should_include_file(rel_path: Path, policy: LocalScanPolicy) -> bool:","    \"\"\"","    Pure include/exclude decision.","    ","    Returns True if the file should be included in the scan.","    \"\"\"","    p = rel_path.as_posix()","    ","    # Root allowlist - files directly in repo root","    if \"/\" not in p:","        return any(fnmatch(rel_path.name, g) for g in policy.allowed_root_files_glob)","    ","    top = rel_path.parts[0]","    ","    # outputs exception rule","    if top == \"outputs\":","        # Check if path starts with any allowed outputs subdirectory","        return any(p.startswith(allowed + \"/\") or p == allowed ","                   for allowed in policy.outputs_allow)","    ","    # allowed roots only","    if top not in policy.allowed_roots:","        return False","    ","    # deny segments anywhere in path","    if _has_deny_segment(rel_path, policy.deny_segments):","        return False","    ","    return True","","","def iter_repo_files_local_strict(","    repo_root: Path,","    policy: LocalScanPolicy,",") -> List[Path]:","    \"\"\"","    Return deterministic sorted list of included files, relative to repo_root.","    ","    Walks the filesystem, applies include/exclude rules, respects caps.","    \"\"\"","    included: List[Path] = []","    ","    # Walk through all allowed roots and root files","    candidates: Set[Path] = set()","    ","    # Add root files","    for pattern in policy.allowed_root_files_glob:","        for path in repo_root.glob(pattern):","            if path.is_file():","                rel = path.relative_to(repo_root)","                candidates.add(rel)","    ","    # Add files from allowed roots","    for root_dir in policy.allowed_roots:","        root_path = repo_root / root_dir","        if not root_path.exists():","            continue","        for path in root_path.rglob(\"*\"):","            if not path.is_file():","                continue","            rel = path.relative_to(repo_root)","            candidates.add(rel)","    ","    # Add outputs exception files","    for allowed_output in policy.outputs_allow:","        output_path = repo_root / allowed_output","        if not output_path.exists():","            continue","        for path in output_path.rglob(\"*\"):","            if not path.is_file():","                continue","            rel = path.relative_to(repo_root)","            candidates.add(rel)","    ","    # Apply include/exclude filter","    for rel in candidates:","        if should_include_file(rel, policy):","            included.append(rel)","    ","    # Sort deterministically","    included.sort(key=lambda p: p.as_posix())","    ","    # Apply max_files cap","    if len(included) > policy.max_files:","        included = included[:policy.max_files]","    ","    return included","","","def write_local_scan_rules(","    policy: LocalScanPolicy,","    output_path: Path,","    repo_root: Optional[Path] = None,",") -> Path:","    \"\"\"","    Write LOCAL_SCAN_RULES.json with policy and metadata.","    ","    Args:","        policy: The scan policy to serialize.","        output_path: Where to write the JSON file.","        repo_root: Optional repo root for computing relative paths.","    ","    Returns:","        Path to the written file.","    \"\"\"","    import datetime","    ","    data = {","        \"mode\": \"local-strict\",","        \"generated_at_utc\": datetime.datetime.now(datetime.timezone.utc).isoformat(),","        \"allowed_roots\": list(policy.allowed_roots),","        \"allowed_root_files_glob\": list(policy.allowed_root_files_glob),","        \"deny_segments\": list(policy.deny_segments),","        \"outputs_allow\": list(policy.outputs_allow),","        \"max_files\": policy.max_files,","        \"max_bytes\": policy.max_bytes,","        \"gitignore_respected\": policy.gitignore_respected,","    }","    ","    if repo_root:","        data[\"repo_root\"] = str(repo_root.absolute())","    ","    output_path.parent.mkdir(parents=True, exist_ok=True)"]}
{"type":"file_chunk","path":"src/control/local_scan.py","chunk_index":1,"line_start":201,"line_end":233,"content":["    with open(output_path, \"w\", encoding=\"utf-8\") as f:","        json.dump(data, f, indent=2, ensure_ascii=False)","    ","    return output_path","","","def compute_policy_hash(policy_path: Path) -> str:","    \"\"\"","    Compute SHA256 hash of LOCAL_SCAN_RULES.json.","    ","    Returns \"UNKNOWN\" if file doesn't exist or can't be read.","    \"\"\"","    if not policy_path.exists():","        return \"UNKNOWN\"","    ","    try:","        with open(policy_path, \"rb\") as f:","            return hashlib.sha256(f.read()).hexdigest()","    except Exception:","        return \"UNKNOWN\"","","","if __name__ == \"__main__\":","    # Simple test when run directly","    import sys","    repo = Path.cwd()","    policy = default_local_strict_policy()","    files = iter_repo_files_local_strict(repo, policy)","    print(f\"Found {len(files)} files with local-strict policy\")","    for f in files[:10]:","        print(f\"  {f}\")","    if len(files) > 10:","        print(f\"  ... and {len(files) - 10} more\")"]}
{"type":"file_footer","path":"src/control/local_scan.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/param_grid.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12329,"sha256":"a443a9098fea51948460ddcc094f1b1294811762b50186372af0762ba7510c62","total_lines":330,"chunk_count":2}
{"type":"file_chunk","path":"src/control/param_grid.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Parameter Grid Expansion for Phase 13.","","Pure functions for turning ParamSpec + user grid config into value lists.","Deterministic ordering, no floating drift surprises.","\"\"\"","","from __future__ import annotations","","import math","from enum import Enum","from typing import Any","","from pydantic import BaseModel, ConfigDict, Field, ValidationError, field_validator","","from strategy.param_schema import ParamSpec","","","class GridMode(str, Enum):","    \"\"\"Grid expansion mode.\"\"\"","    SINGLE = \"single\"","    RANGE = \"range\"","    MULTI = \"multi\"","","","class ParamGridSpec(BaseModel):","    \"\"\"User-defined grid specification for a single parameter.","    ","    Exactly one of the three modes must be active.","    \"\"\"","    ","    model_config = ConfigDict(frozen=True, extra=\"forbid\")","    ","    mode: GridMode = Field(","        ...,","        description=\"Grid expansion mode\"","    )","    ","    single_value: Any | None = Field(","        default=None,","        description=\"Single value for mode='single'\"","    )","    ","    range_start: float | int | None = Field(","        default=None,","        description=\"Start of range (inclusive) for mode='range'\"","    )","    ","    range_end: float | int | None = Field(","        default=None,","        description=\"End of range (inclusive) for mode='range'\"","    )","    ","    range_step: float | int | None = Field(","        default=None,","        description=\"Step size for mode='range'\"","    )","    ","    multi_values: list[Any] | None = Field(","        default=None,","        description=\"List of values for mode='multi'\"","    )","    ","    @field_validator(\"mode\", mode=\"before\")","    @classmethod","    def validate_mode(cls, v: Any) -> GridMode:","        if isinstance(v, str):","            v = v.lower()","        return GridMode(v)","    ","    @field_validator(\"single_value\", \"range_start\", \"range_end\", \"range_step\", \"multi_values\", mode=\"after\")","    @classmethod","    def validate_mode_consistency(cls, v: Any, info) -> Any:","        \"\"\"Ensure only fields relevant to the active mode are set.\"\"\"","        mode = info.data.get(\"mode\")","        if mode is None:","            return v","        ","        field_name = info.field_name","        ","        # Map fields to allowed modes","        allowed_for = {","            \"single_value\": [GridMode.SINGLE],","            \"range_start\": [GridMode.RANGE],","            \"range_end\": [GridMode.RANGE],","            \"range_step\": [GridMode.RANGE],","            \"multi_values\": [GridMode.MULTI],","        }","        ","        if field_name in allowed_for:","            if mode not in allowed_for[field_name]:","                if v is not None:","                    raise ValueError(","                        f\"Field '{field_name}' must be None when mode='{mode.value}'\"","                    )","            else:","                if v is None:","                    raise ValueError(","                        f\"Field '{field_name}' must be set when mode='{mode.value}'\"","                    )","        return v","    ","    @field_validator(\"range_step\")","    @classmethod","    def validate_range_step(cls, v: float | int | None) -> float | int | None:","        # Allow zero step; validation will be done in validate_grid_for_param","        return v","    ","    @field_validator(\"range_start\", \"range_end\")","    @classmethod","    def validate_range_order(cls, v: float | int | None, info) -> float | int | None:","        # Allow start > end; validation will be done in validate_grid_for_param","        return v","    ","    @field_validator(\"multi_values\")","    @classmethod","    def validate_multi_values(cls, v: list[Any] | None) -> list[Any] | None:","        # Allow empty list; validation will be done in validate_grid_for_param","        return v","","","def values_for_param(grid: ParamGridSpec) -> list[Any]:","    \"\"\"Compute deterministic list of values for a parameter.","    ","    Args:","        grid: User-defined grid configuration","    ","    Returns:","        Sorted unique list of values in deterministic order.","    ","    Raises:","        ValueError: if grid is invalid.","    \"\"\"","    if grid.mode == GridMode.SINGLE:","        return [grid.single_value]","    ","    elif grid.mode == GridMode.RANGE:","        start = grid.range_start","        end = grid.range_end","        step = grid.range_step","        ","        if start is None or end is None or step is None:","            raise ValueError(\"range mode requires start, end, and step\")","        ","        if start > end:","            raise ValueError(\"start <= end\")","        ","        # Determine if values are integer-like","        if isinstance(start, int) and isinstance(end, int) and isinstance(step, int):","            # Integer range inclusive","            values = []","            i = 0","            while True:","                val = start + i * step","                if val > end:","                    break","                values.append(val)","                i += 1","            return values","        else:","            # Float range inclusive with drift-safe rounding","            if step <= 0:","                raise ValueError(\"step must be positive\")","            # Add small epsilon to avoid missing the last due to floating error","            num_steps = math.floor((end - start) / step + 1e-12)","            values = []","            for i in range(num_steps + 1):","                val = start + i * step","                # Round to 12 decimal places to avoid floating noise","                val = round(val, 12)","                if val <= end + 1e-12:","                    values.append(val)","            return values","    ","    elif grid.mode == GridMode.MULTI:","        values = grid.multi_values","        if values is None:","            raise ValueError(\"multi_values must be set for multi mode\")","        ","        # Ensure uniqueness and deterministic order","        seen = set()","        unique = []","        for v in values:","            if v not in seen:","                seen.add(v)","                unique.append(v)","        return unique","    ","    else:","        raise ValueError(f\"Unknown grid mode: {grid.mode}\")","","","def count_for_param(grid: ParamGridSpec) -> int:","    \"\"\"Return number of distinct values for this parameter.\"\"\"","    return len(values_for_param(grid))","","","def validate_grid_for_param(","    grid: ParamGridSpec,","    param_type: str,"]}
{"type":"file_chunk","path":"src/control/param_grid.py","chunk_index":1,"line_start":201,"line_end":330,"content":["    min: int | float | None = None,","    max: int | float | None = None,","    choices: list[Any] | None = None,",") -> None:","    \"\"\"Validate that grid is compatible with param spec.","    ","    Args:","        grid: Parameter grid specification","        param_type: Parameter type (\"int\", \"float\", \"bool\", \"enum\")","        min: Minimum allowed value (optional)","        max: Maximum allowed value (optional)","        choices: List of allowed values for enum type (optional)","    ","    Raises ValueError with descriptive message if invalid.","    \"\"\"","    # Check duplicates for MULTI mode","    if grid.mode == GridMode.MULTI and grid.multi_values:","        if len(grid.multi_values) != len(set(grid.multi_values)):","            raise ValueError(\"multi_values contains duplicate values\")","    ","    # Check empty multi_values","    if grid.mode == GridMode.MULTI and grid.multi_values is not None and len(grid.multi_values) == 0:","        raise ValueError(\"multi_values must contain at least one value\")","    ","    # Range-specific validation","    if grid.mode == GridMode.RANGE:","        if grid.range_step is not None and grid.range_step <= 0:","            raise ValueError(\"range_step must be positive\")","        if grid.range_start is not None and grid.range_end is not None and grid.range_start > grid.range_end:","            raise ValueError(\"start <= end\")","    ","    # Type-specific validation","    if param_type == \"enum\":","        if choices is None:","            raise ValueError(\"enum parameter must have choices defined\")","        if grid.mode == GridMode.RANGE:","            raise ValueError(\"enum parameters cannot use range mode\")","        if grid.mode == GridMode.SINGLE:","            if grid.single_value not in choices:","                raise ValueError(f\"value '{grid.single_value}' not in choices {choices}\")","        elif grid.mode == GridMode.MULTI:","            if grid.multi_values is None:","                raise ValueError(\"multi_values must be set for multi mode\")","            for val in grid.multi_values:","                if val not in choices:","                    raise ValueError(f\"value '{val}' not in choices {choices}\")","    ","    elif param_type == \"bool\":","        if grid.mode == GridMode.RANGE:","            raise ValueError(\"bool parameters cannot use range mode\")","        if grid.mode == GridMode.SINGLE:","            if not isinstance(grid.single_value, bool):","                raise ValueError(f\"bool parameter expects bool value, got {type(grid.single_value)}\")","        elif grid.mode == GridMode.MULTI:","            if grid.multi_values is None:","                raise ValueError(\"multi_values must be set for multi mode\")","            for val in grid.multi_values:","                if not isinstance(val, bool):","                    raise ValueError(f\"bool parameter expects bool values, got {type(val)}\")","    ","    elif param_type == \"int\":","        # Ensure values are integers","        if grid.mode == GridMode.SINGLE:","            if not isinstance(grid.single_value, int):","                raise ValueError(\"int parameter expects integer value\")","        elif grid.mode == GridMode.RANGE:","            if not (isinstance(grid.range_start, (int, float)) and","                    isinstance(grid.range_end, (int, float)) and","                    isinstance(grid.range_step, (int, float))):","                raise ValueError(\"int range requires numeric start/end/step\")","            # Values will be integer due to integer step","        elif grid.mode == GridMode.MULTI:","            if grid.multi_values is None:","                raise ValueError(\"multi_values must be set for multi mode\")","            for val in grid.multi_values:","                if not isinstance(val, int):","                    raise ValueError(\"int parameter expects integer values\")","    ","    elif param_type == \"float\":","        # Ensure values are numeric","        if grid.mode == GridMode.SINGLE:","            if not isinstance(grid.single_value, (int, float)):","                raise ValueError(\"float parameter expects numeric value\")","        elif grid.mode == GridMode.RANGE:","            if not (isinstance(grid.range_start, (int, float)) and","                    isinstance(grid.range_end, (int, float)) and","                    isinstance(grid.range_step, (int, float))):","                raise ValueError(\"float range requires numeric start/end/step\")","        elif grid.mode == GridMode.MULTI:","            if grid.multi_values is None:","                raise ValueError(\"multi_values must be set for multi mode\")","            for val in grid.multi_values:","                if not isinstance(val, (int, float)):","                    raise ValueError(\"float parameter expects numeric values\")","    ","    # Check bounds","    if min is not None:","        if grid.mode == GridMode.SINGLE:","            val = grid.single_value","            if val is not None and val < min:","                raise ValueError(f\"value {val} out of range (min {min})\")","        elif grid.mode == GridMode.RANGE:","            if grid.range_start is not None and grid.range_start < min:","                raise ValueError(f\"range_start {grid.range_start} out of range (min {min})\")","        elif grid.mode == GridMode.MULTI:","            if grid.multi_values is None:","                raise ValueError(\"multi_values must be set for multi mode\")","            for val in grid.multi_values:","                if val < min:","                    raise ValueError(f\"value {val} out of range (min {min})\")","    ","    if max is not None:","        if grid.mode == GridMode.SINGLE:","            val = grid.single_value","            if val is not None and val > max:","                raise ValueError(f\"value {val} out of range (max {max})\")","        elif grid.mode == GridMode.RANGE:","            if grid.range_end is not None and grid.range_end > max:","                raise ValueError(f\"range_end {grid.range_end} out of range (max {max})\")","        elif grid.mode == GridMode.MULTI:","            if grid.multi_values is None:","                raise ValueError(\"multi_values must be set for multi mode\")","            for val in grid.multi_values:","                if val > max:","                    raise ValueError(f\"value {val} out of range (max {max})\")","    ","    # Compute values to ensure no errors","    values_for_param(grid)","",""]}
{"type":"file_footer","path":"src/control/param_grid.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/paths.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":882,"sha256":"69a18e2dc18f8c6eaf4dfe0e60618f44fa618dd7a06a1770d2e0735b827c7a21","total_lines":37,"chunk_count":1}
{"type":"file_chunk","path":"src/control/paths.py","chunk_index":0,"line_start":1,"line_end":37,"content":["","\"\"\"Path helpers for B5-C Mission Control.\"\"\"","","from __future__ import annotations","","import os","from pathlib import Path","","","def get_outputs_root() -> Path:","    \"\"\"","    Single source of truth for outputs root.","    - Default: ./outputs (repo relative)","    - Override: env FISHBRO_OUTPUTS_ROOT","    \"\"\"","    p = os.environ.get(\"FISHBRO_OUTPUTS_ROOT\", \"outputs\")","    return Path(p).resolve()","","","def run_log_path(outputs_root: Path, season: str, run_id: str) -> Path:","    \"\"\"","    Return outputs log path for a run (mkdir parents).","    ","    Args:","        outputs_root: Root outputs directory","        season: Season identifier","        run_id: Run ID","        ","    Returns:","        Path to log file: outputs/{season}/{run_id}/logs/worker.log","    \"\"\"","    log_path = outputs_root / season / run_id / \"logs\" / \"worker.log\"","    log_path.parent.mkdir(parents=True, exist_ok=True)","    return log_path","","",""]}
{"type":"file_footer","path":"src/control/paths.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/pipeline_runner.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8977,"sha256":"214e99bd73c08cda4eec030a84e7ae3f040d87249e985500051eb8704b16843d","total_lines":233,"chunk_count":2}
{"type":"file_chunk","path":"src/control/pipeline_runner.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Pipeline Runner for M1 Wizard.","","Stub implementation for job pipeline execution.","\"\"\"","","from __future__ import annotations","","import time","from typing import Dict, Any, Optional","from pathlib import Path","","from control.jobs_db import (","    get_job, mark_running, mark_done, mark_failed, append_log",")","from control.job_api import calculate_units","from control.artifacts_api import write_research_index","","","class PipelineRunner:","    \"\"\"Simple pipeline runner for M1 demonstration.\"\"\"","    ","    def __init__(self, db_path: Optional[Path] = None):","        \"\"\"Initialize pipeline runner.","        ","        Args:","            db_path: Path to SQLite database. If None, uses default.","        \"\"\"","        self.db_path = db_path or Path(\"outputs/jobs.db\")","    ","    def run_job(self, job_id: str) -> bool:","        \"\"\"Run a job (stub implementation for M1).","        ","        This is a simplified runner that simulates job execution","        for demonstration purposes.","        ","        Args:","            job_id: Job ID to run","            ","        Returns:","            True if job completed successfully, False otherwise","        \"\"\"","        try:","            # Get job record","            job = get_job(self.db_path, job_id)","            ","            # Mark as running","            mark_running(self.db_path, job_id, pid=12345)","            self._log(job_id, f\"Job {job_id} started\")","            ","            # Simulate work based on units","            units = 0","            if hasattr(job.spec, 'config_snapshot'):","                config = job.spec.config_snapshot","                if isinstance(config, dict) and 'units' in config:","                    units = config.get('units', 10)","            ","            # Default to 10 units if not specified","            if units <= 0:","                units = 10","            ","            self._log(job_id, f\"Processing {units} units\")","            ","            # Simulate unit processing","            for i in range(units):","                time.sleep(0.1)  # Simulate work","                progress = (i + 1) / units","                if i % max(1, units // 10) == 0:  # Log every ~10%","                    self._log(job_id, f\"Unit {i+1}/{units} completed ({progress:.0%})\")","            ","            # Mark as done","            mark_done(self.db_path, job_id, run_id=f\"run_{job_id}\", report_link=f\"/reports/{job_id}\")","            ","            # Write research index (M2)","            try:","                season = job.spec.season if hasattr(job.spec, 'season') else \"default\"","                # Generate dummy units based on config snapshot","                units = []","                if hasattr(job.spec, 'config_snapshot'):","                    config = job.spec.config_snapshot","                    if isinstance(config, dict):","                        # Extract possible symbols, timeframes, etc.","                        data1 = config.get('data1', {})","                        symbols = data1.get('symbols', ['MNQ'])","                        timeframes = data1.get('timeframes', ['60m'])","                        strategy = config.get('strategy_id', 'vPB_Z')","                        data2_filters = config.get('data2', {}).get('filters', ['VX'])","                        # Create one unit per combination (simplified)","                        for sym in symbols[:1]:  # limit","                            for tf in timeframes[:1]:","                                for filt in data2_filters[:1]:","                                    units.append({","                                        'data1_symbol': sym,","                                        'data1_timeframe': tf,","                                        'strategy': strategy,","                                        'data2_filter': filt,","                                        'status': 'DONE',","                                        'artifacts': {","                                            'canonical_results': f'outputs/seasons/{season}/research/{job_id}/{sym}/{tf}/{strategy}/{filt}/canonical_results.json',","                                            'metrics': f'outputs/seasons/{season}/research/{job_id}/{sym}/{tf}/{strategy}/{filt}/metrics.json',","                                            'trades': f'outputs/seasons/{season}/research/{job_id}/{sym}/{tf}/{strategy}/{filt}/trades.parquet',","                                        }","                                    })","                if not units:","                    # Fallback dummy unit","                    units.append({","                        'data1_symbol': 'MNQ',","                        'data1_timeframe': '60m',","                        'strategy': 'vPB_Z',","                        'data2_filter': 'VX',","                        'status': 'DONE',","                        'artifacts': {","                            'canonical_results': f'outputs/seasons/{season}/research/{job_id}/MNQ/60m/vPB_Z/VX/canonical_results.json',","                            'metrics': f'outputs/seasons/{season}/research/{job_id}/MNQ/60m/vPB_Z/VX/metrics.json',","                            'trades': f'outputs/seasons/{season}/research/{job_id}/MNQ/60m/vPB_Z/VX/trades.parquet',","                        }","                    })","                write_research_index(season, job_id, units)","                self._log(job_id, f\"Research index written for {len(units)} units\")","            except Exception as e:","                self._log(job_id, f\"Failed to write research index: {e}\")","            ","            self._log(job_id, f\"Job {job_id} completed successfully\")","            ","            return True","            ","        except Exception as e:","            # Mark as failed","            error_msg = f\"Job failed: {str(e)}\"","            try:","                mark_failed(self.db_path, job_id, error=error_msg)","                self._log(job_id, error_msg)","            except Exception:","                pass  # Ignore errors during failure marking","            ","            return False","    ","    def _log(self, job_id: str, message: str) -> None:","        \"\"\"Add log entry for job.\"\"\"","        try:","            append_log(self.db_path, job_id, message)","        except Exception:","            pass  # Ignore log errors","    ","    def get_job_progress(self, job_id: str) -> Dict[str, Any]:","        \"\"\"Get job progress information.","        ","        Args:","            job_id: Job ID","            ","        Returns:","            Dictionary with progress information","        \"\"\"","        try:","            job = get_job(self.db_path, job_id)","            ","            # Calculate progress based on status","            units_total = 0","            units_done = 0","            ","            if hasattr(job.spec, 'config_snapshot'):","                config = job.spec.config_snapshot","                if isinstance(config, dict) and 'units' in config:","                    units_total = config.get('units', 0)","            ","            if job.status.value == \"DONE\":","                units_done = units_total","            elif job.status.value == \"RUNNING\":","                # For stub, estimate 50% progress","                units_done = units_total // 2 if units_total > 0 else 0","            ","            progress = units_done / units_total if units_total > 0 else 0","            ","            return {","                \"job_id\": job_id,","                \"status\": job.status.value,","                \"units_done\": units_done,","                \"units_total\": units_total,","                \"progress\": progress,","                \"is_running\": job.status.value == \"RUNNING\",","                \"is_done\": job.status.value == \"DONE\",","                \"is_failed\": job.status.value == \"FAILED\"","            }","        except Exception as e:","            return {","                \"job_id\": job_id,","                \"status\": \"UNKNOWN\",","                \"units_done\": 0,","                \"units_total\": 0,","                \"progress\": 0,","                \"is_running\": False,","                \"is_done\": False,","                \"is_failed\": True,","                \"error\": str(e)","            }","","","# Singleton instance","_runner_instance: Optional[PipelineRunner] = None","","def get_pipeline_runner() -> PipelineRunner:"]}
{"type":"file_chunk","path":"src/control/pipeline_runner.py","chunk_index":1,"line_start":201,"line_end":233,"content":["    \"\"\"Get singleton pipeline runner instance.\"\"\"","    global _runner_instance","    if _runner_instance is None:","        _runner_instance = PipelineRunner()","    return _runner_instance","","","def start_job_async(job_id: str) -> None:","    \"\"\"Start job execution asynchronously (stub).","    ","    In a real implementation, this would spawn a worker process.","    For M1, we'll just simulate immediate execution.","    ","    Args:","        job_id: Job ID to start","    \"\"\"","    # In a real implementation, this would use a task queue or worker pool","    # For M1 demo, we'll run synchronously","    runner = get_pipeline_runner()","    runner.run_job(job_id)","","","def check_job_status(job_id: str) -> Dict[str, Any]:","    \"\"\"Check job status (convenience wrapper).","    ","    Args:","        job_id: Job ID","        ","    Returns:","        Dictionary with job status and progress","    \"\"\"","    runner = get_pipeline_runner()","    return runner.get_job_progress(job_id)"]}
{"type":"file_footer","path":"src/control/pipeline_runner.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/preflight.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1971,"sha256":"69d63b94b9f3e9c52d7755c257898642af2cd6186a1fb55f4486e8f2a070a913","total_lines":65,"chunk_count":1}
{"type":"file_chunk","path":"src/control/preflight.py","chunk_index":0,"line_start":1,"line_end":65,"content":["","\"\"\"Preflight check - OOM gate and cost summary.\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass","from typing import Any, Literal","","from core.oom_gate import decide_oom_action","","","@dataclass(frozen=True)","class PreflightResult:","    \"\"\"Preflight check result.\"\"\"","","    action: Literal[\"PASS\", \"BLOCK\", \"AUTO_DOWNSAMPLE\"]","    reason: str","    original_subsample: float","    final_subsample: float","    estimated_bytes: int","    estimated_mb: float","    mem_limit_mb: float","    mem_limit_bytes: int","    estimates: dict[str, Any]  # must include ops_est, time_est_s, mem_est_mb, ...","","","def run_preflight(cfg_snapshot: dict[str, Any]) -> PreflightResult:","    \"\"\"","    Run preflight check (pure, no I/O).","    ","    Returns what UI shows in CHECK panel.","    ","    Args:","        cfg_snapshot: Sanitized config snapshot (no ndarrays)","        ","    Returns:","        PreflightResult with OOM gate decision and estimates","    \"\"\"","    # Extract mem_limit_mb from config (default: 6000 MB = 6GB)","    mem_limit_mb = float(cfg_snapshot.get(\"mem_limit_mb\", 6000.0))","    ","    # Run OOM gate decision","    gate_result = decide_oom_action(","        cfg_snapshot,","        mem_limit_mb=mem_limit_mb,","        allow_auto_downsample=cfg_snapshot.get(\"allow_auto_downsample\", True),","        auto_downsample_step=cfg_snapshot.get(\"auto_downsample_step\", 0.5),","        auto_downsample_min=cfg_snapshot.get(\"auto_downsample_min\", 0.02),","        work_factor=cfg_snapshot.get(\"work_factor\", 2.0),","    )","    ","    return PreflightResult(","        action=gate_result[\"action\"],","        reason=gate_result[\"reason\"],","        original_subsample=gate_result[\"original_subsample\"],","        final_subsample=gate_result[\"final_subsample\"],","        estimated_bytes=gate_result[\"estimated_bytes\"],","        estimated_mb=gate_result[\"estimated_mb\"],","        mem_limit_mb=gate_result[\"mem_limit_mb\"],","        mem_limit_bytes=gate_result[\"mem_limit_bytes\"],","        estimates=gate_result[\"estimates\"],","    )","","",""]}
{"type":"file_footer","path":"src/control/preflight.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/report_links.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2181,"sha256":"3de92e00455914d2971df31aeb7358c93b0c333f7abddd71339530633c63a216","total_lines":81,"chunk_count":1}
{"type":"file_chunk","path":"src/control/report_links.py","chunk_index":0,"line_start":1,"line_end":81,"content":["","\"\"\"Report link generation for B5 viewer.\"\"\"","","from __future__ import annotations","","import os","from pathlib import Path","from urllib.parse import urlencode","","# Default outputs root (can be overridden via environment)","DEFAULT_OUTPUTS_ROOT = \"outputs\"","","","def get_outputs_root() -> Path:","    \"\"\"Get outputs root from environment or default.\"\"\"","    outputs_root_str = os.getenv(\"FISHBRO_OUTPUTS_ROOT\", DEFAULT_OUTPUTS_ROOT)","    return Path(outputs_root_str)","","","def make_report_link(*, season: str, run_id: str) -> str:","    \"\"\"","    Generate report link for B5 viewer.","    ","    Args:","        season: Season identifier (e.g. \"2026Q1\")","        run_id: Run ID (e.g. \"stage0_coarse-20251218T093512Z-d3caa754\")","        ","    Returns:","        Report link URL with querystring (e.g. \"/?season=2026Q1&run_id=stage0_xxx\")","    \"\"\"","    # Test contract: link.startswith(\"/?\")","    base = \"/\"","    qs = urlencode({\"season\": season, \"run_id\": run_id})","    return f\"{base}?{qs}\"","","","def is_report_ready(run_id: str) -> bool:","    \"\"\"","    Check if report is ready (minimal artifacts exist).","    ","    Phase 6 rule: Only check file existence, not content validity.","    Content validation is Viewer's responsibility.","    ","    Args:","        run_id: Run ID to check","        ","    Returns:","        True if all required artifacts exist, False otherwise","    \"\"\"","    try:","        outputs_root = get_outputs_root()","        base = outputs_root / run_id","        ","        # Check for winners_v2.json first, fallback to winners.json","        winners_v2_path = base / \"winners_v2.json\"","        winners_path = base / \"winners.json\"","        winners_exists = winners_v2_path.exists() or winners_path.exists()","        ","        required = [","            base / \"manifest.json\",","            base / \"governance.json\",","        ]","        ","        return winners_exists and all(p.exists() for p in required)","    except Exception:","        return False","","","def build_report_link(*args: str) -> str:","    if len(args) == 1:","        run_id = args[0]","        season = \"test\"","        return f\"/?season={season}&run_id={run_id}\"","","    if len(args) == 2:","        season, run_id = args","        return f\"/b5?season={season}&run_id={run_id}\"","","    return \"\"","",""]}
{"type":"file_footer","path":"src/control/report_links.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/research_cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7090,"sha256":"af3ba58fe4a0cd0bdb132d19cea1cd58e4eb4c2c766361f0ccdc36460270add5","total_lines":259,"chunk_count":2}
{"type":"file_chunk","path":"src/control/research_cli.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Research CLI：研究執行命令列介面","","命令：","fishbro research run \\","  --season 2026Q1 \\","  --dataset-id CME.MNQ \\","  --strategy-id S1 \\","  --allow-build \\","  --txt-path /home/fishbro/FishBroData/raw/CME.MNQ-HOT-Minute-Trade.txt \\","  --mode incremental \\","  --json","","Exit code：","0：成功","20：缺 features 且不允許 build","1：其他錯誤","\"\"\"","","from __future__ import annotations","","import sys","import json","import argparse","from pathlib import Path","from typing import Optional","","from control.research_runner import (","    run_research,","    ResearchRunError,",")","from control.build_context import BuildContext","from strategy.registry import load_builtin_strategies","","","def main() -> int:","    \"\"\"CLI 主函數\"\"\"","    parser = create_parser()","    args = parser.parse_args()","    ","    try:","        return run_research_cli(args)","    except KeyboardInterrupt:","        print(\"\\n中斷執行\", file=sys.stderr)","        return 130","    except Exception as e:","        print(f\"錯誤: {e}\", file=sys.stderr)","        return 1","","","def create_parser() -> argparse.ArgumentParser:","    \"\"\"建立命令列解析器\"\"\"","    parser = argparse.ArgumentParser(","        description=\"執行研究（載入策略、解析特徵、執行 WFS）\",","        formatter_class=argparse.ArgumentDefaultsHelpFormatter,","    )","    ","    # 必要參數","    parser.add_argument(","        \"--season\",","        required=True,","        help=\"季節標記，例如 2026Q1\",","    )","    parser.add_argument(","        \"--dataset-id\",","        required=True,","        help=\"資料集 ID，例如 CME.MNQ\",","    )","    parser.add_argument(","        \"--strategy-id\",","        required=True,","        help=\"策略 ID\",","    )","    ","    # build 相關參數","    parser.add_argument(","        \"--allow-build\",","        action=\"store_true\",","        help=\"允許自動 build 缺失的特徵\",","    )","    parser.add_argument(","        \"--txt-path\",","        type=Path,","        help=\"原始 TXT 檔案路徑（只有 allow-build 才需要）\",","    )","    parser.add_argument(","        \"--mode\",","        choices=[\"incremental\", \"full\"],","        default=\"incremental\",","        help=\"build 模式（只在 allow-build 時使用）\",","    )","    parser.add_argument(","        \"--outputs-root\",","        type=Path,","        default=Path(\"outputs\"),","        help=\"輸出根目錄\",","    )","    parser.add_argument(","        \"--build-bars-if-missing\",","        action=\"store_true\",","        default=True,","        help=\"如果 bars cache 不存在，是否建立 bars\",","    )","    parser.add_argument(","        \"--no-build-bars-if-missing\",","        action=\"store_false\",","        dest=\"build_bars_if_missing\",","        help=\"不建立 bars cache（即使缺失）\",","    )","    ","    # WFS 配置（可選）","    parser.add_argument(","        \"--wfs-config\",","        type=Path,","        help=\"WFS 配置 JSON 檔案路徑（可選）\",","    )","    ","    # 輸出選項","    parser.add_argument(","        \"--json\",","        action=\"store_true\",","        help=\"以 JSON 格式輸出結果\",","    )","    parser.add_argument(","        \"--verbose\",","        action=\"store_true\",","        help=\"輸出詳細資訊\",","    )","    ","    return parser","","","def ensure_builtin_strategies_loaded() -> None:","    \"\"\"Ensure built-in strategies are loaded (idempotent).","    ","    This function can be called multiple times without crashing.","    \"\"\"","    try:","        load_builtin_strategies()","    except ValueError as e:","        # registry is process-local; re-entry may raise duplicate register","        if \"already registered\" not in str(e):","            raise","","","def run_research_cli(args) -> int:","    \"\"\"執行研究邏輯\"\"\"","    # 0. 確保 built-in strategies 已載入","    ensure_builtin_strategies_loaded()","    ","    # 1. 準備 build_ctx（如果需要）","    build_ctx = prepare_build_context(args)","    ","    # 2. 載入 WFS 配置（如果有）","    wfs_config = load_wfs_config(args)","    ","    # 3. 執行研究","    try:","        report = run_research(","            season=args.season,","            dataset_id=args.dataset_id,","            strategy_id=args.strategy_id,","            outputs_root=args.outputs_root,","            allow_build=args.allow_build,","            build_ctx=build_ctx,","            wfs_config=wfs_config,","        )","        ","        # 4. 輸出結果","        output_result(report, args)","        ","        # 判斷 exit code","        # 如果有 build，回傳 10；否則回傳 0","        if report.get(\"build_performed\", False):","            return 10","        else:","            return 0","        ","    except ResearchRunError as e:","        # 檢查是否為缺失特徵且不允許 build 的錯誤","        err_msg = str(e).lower()","        if \"缺失特徵且不允許建置\" in err_msg or \"missing features\" in err_msg:","            print(f\"缺失特徵且不允許建置: {e}\", file=sys.stderr)","            return 20","        else:","            print(f\"研究執行失敗: {e}\", file=sys.stderr)","            return 1","","","def prepare_build_context(args) -> Optional[BuildContext]:","    \"\"\"準備 BuildContext\"\"\"","    if not args.allow_build:","        return None","    ","    if not args.txt_path:","        raise ValueError(\"--allow-build 需要 --txt-path\")","    ","    # 驗證 txt_path 存在","    if not args.txt_path.exists():"]}
{"type":"file_chunk","path":"src/control/research_cli.py","chunk_index":1,"line_start":201,"line_end":259,"content":["        raise FileNotFoundError(f\"TXT 檔案不存在: {args.txt_path}\")","    ","    # 轉換 mode 為大寫","    mode = args.mode.upper()","    if mode not in (\"FULL\", \"INCREMENTAL\"):","        raise ValueError(f\"無效的 mode: {args.mode}，必須為 'incremental' 或 'full'\")","    ","    return BuildContext(","        txt_path=args.txt_path,","        mode=mode,","        outputs_root=args.outputs_root,","        build_bars_if_missing=args.build_bars_if_missing,","    )","","","def load_wfs_config(args) -> Optional[dict]:","    \"\"\"載入 WFS 配置\"\"\"","    if not args.wfs_config:","        return None","    ","    config_path = args.wfs_config","    if not config_path.exists():","        raise FileNotFoundError(f\"WFS 配置檔案不存在: {config_path}\")","    ","    try:","        content = config_path.read_text(encoding=\"utf-8\")","        return json.loads(content)","    except Exception as e:","        raise ValueError(f\"無法載入 WFS 配置 {config_path}: {e}\")","","","def output_result(report: dict, args) -> None:","    \"\"\"輸出研究結果\"\"\"","    if args.json:","        # JSON 格式輸出","        print(json.dumps(report, indent=2, ensure_ascii=False))","    else:","        # 文字格式輸出","        print(f\"✅ 研究執行成功\")","        print(f\"   策略: {report['strategy_id']}\")","        print(f\"   資料集: {report['dataset_id']}\")","        print(f\"   季節: {report['season']}\")","        print(f\"   使用特徵: {len(report['used_features'])} 個\")","        print(f\"   是否執行了建置: {report['build_performed']}\")","        ","        if args.verbose:","            print(f\"   WFS 摘要:\")","            for key, value in report['wfs_summary'].items():","                print(f\"     {key}: {value}\")","            ","            print(f\"   特徵列表:\")","            for feat in report['used_features']:","                print(f\"     {feat['name']}@{feat['timeframe_min']}m\")","","","if __name__ == \"__main__\":","    sys.exit(main())","",""]}
{"type":"file_footer","path":"src/control/research_cli.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/research_runner.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9337,"sha256":"0ee4201db0d155a888420c2e3b84521f748aa1dc56038b80168ba1595c4b3a97","total_lines":254,"chunk_count":2}
{"type":"file_chunk","path":"src/control/research_runner.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Research Runner - 研究執行的唯一入口","","負責載入策略、解析特徵需求、呼叫 Feature Resolver、注入 FeatureBundle 到 WFS、執行研究。","嚴格區分 Research vs Run/Viewer 路徑。","","Phase 4.1: 新增 Research Runner + WFS Integration","\"\"\"","","from __future__ import annotations","","import logging","from pathlib import Path","from typing import Optional, Dict, Any","","from contracts.strategy_features import (","    StrategyFeatureRequirements,","    load_requirements_from_json,",")","from control.build_context import BuildContext","from control.feature_resolver import (","    resolve_features,","    MissingFeaturesError,","    ManifestMismatchError,","    BuildNotAllowedError,","    FeatureResolutionError,",")","from core.feature_bundle import FeatureBundle","from wfs.runner import run_wfs_with_features","from core.slippage_policy import SlippagePolicy","from control.research_slippage_stress import (","    compute_stress_matrix,","    survive_s2,","    compute_stress_test_passed,","    generate_stress_report,","    CommissionConfig,",")","","logger = logging.getLogger(__name__)","","","class ResearchRunError(RuntimeError):","    \"\"\"Research Runner 專用錯誤類別\"\"\"","    pass","","","def _load_strategy_feature_requirements(","    strategy_id: str,","    outputs_root: Path,",") -> StrategyFeatureRequirements:","    \"\"\"","    載入策略特徵需求","","    順序：","    1. 先嘗試 strategy.feature_requirements()（Python）","    2. 再 fallback strategies/{strategy_id}/features.json","","    若都沒有 → raise ResearchRunError","    \"\"\"","    # 1. 嘗試 Python 方法（如果策略有實作）","    try:","        from strategy.registry import get","        spec = get(strategy_id)","        if hasattr(spec, \"feature_requirements\") and callable(spec.feature_requirements):","            req = spec.feature_requirements()","            if isinstance(req, StrategyFeatureRequirements):","                logger.debug(f\"策略 {strategy_id} 透過 Python 方法提供特徵需求\")","                return req","    except Exception as e:","        logger.debug(f\"策略 {strategy_id} 無 Python 特徵需求方法: {e}\")","","    # 2. 嘗試 JSON 檔案","    json_path = outputs_root / \"strategies\" / strategy_id / \"features.json\"","    if not json_path.exists():","        # 也嘗試在 configs/strategies 資料夾","        json_path = Path(\"configs/strategies\") / strategy_id / \"features.json\"","        if not json_path.exists():","            raise ResearchRunError(","                f\"策略 {strategy_id} 無特徵需求定義：\"","                f\"既無 Python 方法，也找不到 JSON 檔案 ({json_path})\"","            )","","    try:","        req = load_requirements_from_json(str(json_path))","        logger.debug(f\"從 {json_path} 載入策略 {strategy_id} 特徵需求\")","        return req","    except Exception as e:","        raise ResearchRunError(f\"載入策略 {strategy_id} 特徵需求失敗: {e}\")","","","def run_research(","    *,","    season: str,","    dataset_id: str,","    strategy_id: str,","    outputs_root: Path = Path(\"outputs\"),","    allow_build: bool = False,","    build_ctx: Optional[BuildContext] = None,","    wfs_config: Optional[Dict[str, Any]] = None,","    enable_slippage_stress: bool = False,","    slippage_policy: Optional[SlippagePolicy] = None,","    commission_config: Optional[CommissionConfig] = None,","    tick_size_map: Optional[Dict[str, float]] = None,",") -> Dict[str, Any]:","    \"\"\"","    Execute a research run for a single strategy.","    Returns a run report (no raw arrays).","","    Args:","        season: 季節標識，例如 \"2026Q1\"","        dataset_id: 資料集 ID，例如 \"CME.MNQ\"","        strategy_id: 策略 ID，例如 \"S1\"","        outputs_root: 輸出根目錄（預設 \"outputs\"）","        allow_build: 是否允許自動建置缺失的特徵","        build_ctx: BuildContext 實例（若 allow_build=True 則必須提供）","        wfs_config: WFS 配置字典（可選）","        enable_slippage_stress: 是否啟用滑價壓力測試（預設 False）","        slippage_policy: 滑價政策（若 enable_slippage_stress=True 則必須提供）","        commission_config: 手續費配置（若 enable_slippage_stress=True 則必須提供）","        tick_size_map: tick_size 對應表（若 enable_slippage_stress=True 則必須提供）","","    Returns:","        run report 字典，包含：","            strategy_id","            dataset_id","            season","            used_features (list)","            features_manifest_sha256","            build_performed (bool)","            wfs_summary（摘要，不含大量數據）","            slippage_stress（若啟用）","","    Raises:","        ResearchRunError: 研究執行失敗","    \"\"\"","    # 1. 載入策略特徵需求","    logger.info(f\"開始研究執行: {strategy_id} on {dataset_id} ({season})\")","    try:","        req = _load_strategy_feature_requirements(strategy_id, outputs_root)","    except Exception as e:","        raise ResearchRunError(f\"載入策略特徵需求失敗: {e}\")","","    # 2. Resolve Features","    try:","        feature_bundle, build_performed = resolve_features(","            dataset_id=dataset_id,","            season=season,","            requirements=req,","            outputs_root=outputs_root,","            allow_build=allow_build,","            build_ctx=build_ctx,","        )","    except MissingFeaturesError as e:","        if not allow_build:","            # 缺失特徵且不允許建置 → 轉為 exit code 20（在 CLI 層處理）","            raise ResearchRunError(","                f\"缺失特徵且不允許建置: {e}\"","            ) from e","        # 若 allow_build=True 但 build_ctx=None，則 BuildNotAllowedError 會被拋出","        raise","    except BuildNotAllowedError as e:","        raise ResearchRunError(","            f\"允許建置但缺少 BuildContext: {e}\"","        ) from e","    except (ManifestMismatchError, FeatureResolutionError) as e:","        raise ResearchRunError(f\"特徵解析失敗: {e}\") from e","","    # 3. 注入 FeatureBundle 到 WFS","    try:","        wfs_result = run_wfs_with_features(","            strategy_id=strategy_id,","            feature_bundle=feature_bundle,","            config=wfs_config,","        )","    except Exception as e:","        raise ResearchRunError(f\"WFS 執行失敗: {e}\") from e","","    # 4. 滑價壓力測試（若啟用）","    slippage_stress_report = None","    if enable_slippage_stress:","        if slippage_policy is None:","            slippage_policy = SlippagePolicy()  # 預設政策","        if commission_config is None:","            # 預設手續費配置（僅示例，實際應從配置檔讀取）","            commission_config = CommissionConfig(","                per_side_usd={\"MNQ\": 2.8, \"MES\": 2.8, \"MXF\": 20.0},","                default_per_side_usd=0.0,","            )","        if tick_size_map is None:","            # 預設 tick_size（僅示例，實際應從 dimension contract 讀取）","            tick_size_map = {\"MNQ\": 0.25, \"MES\": 0.25, \"MXF\": 1.0}","        ","        # 從 dataset_id 推導商品符號（簡化：取最後一部分）","        symbol = dataset_id.split(\".\")[1] if \".\" in dataset_id else dataset_id","        ","        # 檢查 tick_size 是否存在","        if symbol not in tick_size_map:","            raise ResearchRunError(","                f\"商品 {symbol} 的 tick_size 未定義於 tick_size_map 中\""]}
{"type":"file_chunk","path":"src/control/research_runner.py","chunk_index":1,"line_start":201,"line_end":254,"content":["            )","        ","        # 假設 wfs_result 包含 fills/intents 資料","        # 目前我們沒有實際的 fills 資料，因此跳過計算","        # 這裡僅建立一個框架，實際計算需根據 fills/intents 實作","        logger.warning(","            \"滑價壓力測試已啟用，但 fills/intents 資料不可用，跳過計算。\"","            \"請確保 WFS 結果包含 fills 欄位。\"","        )","        # 建立一個空的 stress matrix 報告","        slippage_stress_report = {","            \"enabled\": True,","            \"policy\": {","                \"definition\": slippage_policy.definition,","                \"levels\": slippage_policy.levels,","                \"selection_level\": slippage_policy.selection_level,","                \"stress_level\": slippage_policy.stress_level,","                \"mc_execution_level\": slippage_policy.mc_execution_level,","            },","            \"stress_matrix\": {},","            \"survive_s2\": False,","            \"stress_test_passed\": False,","            \"note\": \"fills/intents 資料不可用，計算被跳過\",","        }","","    # 5. 組裝 run report","    used_features = [","        {\"name\": fs.name, \"timeframe_min\": fs.timeframe_min}","        for fs in feature_bundle.series.values()","    ]","    report = {","        \"strategy_id\": strategy_id,","        \"dataset_id\": dataset_id,","        \"season\": season,","        \"used_features\": used_features,","        \"features_manifest_sha256\": feature_bundle.meta.get(\"manifest_sha256\", \"\"),","        \"build_performed\": build_performed,","        \"wfs_summary\": {","            \"status\": \"completed\",","            \"metrics_keys\": list(wfs_result.keys()) if isinstance(wfs_result, dict) else [],","        },","    }","    # 如果 wfs_result 包含摘要，合併進去","    if isinstance(wfs_result, dict) and \"summary\" in wfs_result:","        report[\"wfs_summary\"].update(wfs_result[\"summary\"])","    ","    # 加入滑價壓力測試報告（若啟用）","    if enable_slippage_stress and slippage_stress_report is not None:","        report[\"slippage_stress\"] = slippage_stress_report","","    logger.info(f\"研究執行完成: {strategy_id}\")","    return report","",""]}
{"type":"file_footer","path":"src/control/research_runner.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/research_slippage_stress.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8168,"sha256":"78f359c1bb7e89145ed90c6246305e37bb194cde8ace74599bf38f70110d69dd","total_lines":262,"chunk_count":2}
{"type":"file_chunk","path":"src/control/research_slippage_stress.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Slippage Stress Matrix 計算與 Survive Gate 評估","","給定 bars、fills/intents、commission 配置，計算 S0–S3 等級的 KPI 矩陣。","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass","from typing import Dict, List, Optional, Tuple, Any","import numpy as np","","from core.slippage_policy import SlippagePolicy, apply_slippage_to_price","","","@dataclass","class StressResult:","    \"\"\"","    單一滑價等級的壓力測試結果","    \"\"\"","    level: str  # 等級名稱，例如 \"S0\"","    slip_ticks: int  # 滑價 tick 數","    net_after_cost: float  # 扣除成本後的淨利","    gross_profit: float  # 總盈利（未扣除成本）","    gross_loss: float  # 總虧損（未扣除成本）","    profit_factor: float  # 盈利因子 = gross_profit / abs(gross_loss)（如果 gross_loss != 0）","    mdd_after_cost: float  # 扣除成本後的最大回撤（絕對值）","    trades: int  # 交易次數（來回算一次）","","","@dataclass","class CommissionConfig:","    \"\"\"","    手續費配置（每邊固定金額）","    \"\"\"","    per_side_usd: Dict[str, float]  # 商品符號 -> 每邊手續費（USD）","    default_per_side_usd: float = 0.0  # 預設手續費（如果商品未指定）","","","def compute_stress_matrix(","    bars: Dict[str, np.ndarray],","    fills: List[Dict[str, Any]],","    commission_config: CommissionConfig,","    slippage_policy: SlippagePolicy,","    tick_size_map: Dict[str, float],  # 商品符號 -> tick_size","    symbol: str,  # 當前商品符號，例如 \"MNQ\"",") -> Dict[str, StressResult]:","    \"\"\"","    計算滑價壓力矩陣（S0–S3）","","    Args:","        bars: 價格 bars 字典，至少包含 \"open\", \"high\", \"low\", \"close\"","        fills: 成交列表，每個成交為字典，包含 \"entry_price\", \"exit_price\", \"entry_side\", \"exit_side\", \"quantity\" 等欄位","        commission_config: 手續費配置","        slippage_policy: 滑價政策","        tick_size_map: tick_size 對應表","        symbol: 商品符號","","    Returns:","        字典 mapping level -> StressResult","    \"\"\"","    # 取得 tick_size","    tick_size = tick_size_map.get(symbol)","    if tick_size is None or tick_size <= 0:","        raise ValueError(f\"商品 {symbol} 的 tick_size 無效或缺失: {tick_size}\")","    ","    # 取得手續費（每邊）","    commission_per_side = commission_config.per_side_usd.get(","        symbol, commission_config.default_per_side_usd","    )","    ","    results = {}","    ","    for level in [\"S0\", \"S1\", \"S2\", \"S3\"]:","        slip_ticks = slippage_policy.get_ticks(level)","        ","        # 計算該等級下的淨利與其他指標","        net, gross_profit, gross_loss, trades = _compute_net_with_slippage(","            fills, slip_ticks, tick_size, commission_per_side","        )","        ","        # 計算盈利因子","        if gross_loss == 0:","            profit_factor = float(\"inf\") if gross_profit > 0 else 1.0","        else:","            profit_factor = gross_profit / abs(gross_loss)","        ","        # 計算最大回撤（簡化版本：使用淨利序列）","        # 由於我們沒有逐筆的 equity curve，這裡先設為 0","        mdd = 0.0","        ","        results[level] = StressResult(","            level=level,","            slip_ticks=slip_ticks,","            net_after_cost=net,","            gross_profit=gross_profit,","            gross_loss=gross_loss,","            profit_factor=profit_factor,","            mdd_after_cost=mdd,","            trades=trades,","        )","    ","    return results","","","def _compute_net_with_slippage(","    fills: List[Dict[str, Any]],","    slip_ticks: int,","    tick_size: float,","    commission_per_side: float,",") -> Tuple[float, float, float, int]:","    \"\"\"","    計算給定滑價 tick 數下的淨利、總盈利、總虧損與交易次數","    \"\"\"","    total_net = 0.0","    total_gross_profit = 0.0","    total_gross_loss = 0.0","    trades = 0","    ","    for fill in fills:","        # 假設 fill 結構包含 entry_price, exit_price, entry_side, exit_side, quantity","        entry_price = fill.get(\"entry_price\")","        exit_price = fill.get(\"exit_price\")","        entry_side = fill.get(\"entry_side\")  # \"buy\" 或 \"sellshort\"","        exit_side = fill.get(\"exit_side\")    # \"sell\" 或 \"buytocover\"","        quantity = fill.get(\"quantity\", 1.0)","        ","        if None in (entry_price, exit_price, entry_side, exit_side):","            continue","        ","        # 應用滑價調整價格","        entry_price_adj = apply_slippage_to_price(","            entry_price, entry_side, slip_ticks, tick_size","        )","        exit_price_adj = apply_slippage_to_price(","            exit_price, exit_side, slip_ticks, tick_size","        )","        ","        # 計算毛利（未扣除手續費）","        if entry_side in (\"buy\", \"buytocover\"):","            # 多頭：買入後賣出","            gross = (exit_price_adj - entry_price_adj) * quantity","        else:","            # 空頭：賣出後買回","            gross = (entry_price_adj - exit_price_adj) * quantity","        ","        # 扣除手續費（每邊）","        commission_total = 2 * commission_per_side * quantity","        ","        # 淨利","        net = gross - commission_total","        ","        total_net += net","        if net > 0:","            total_gross_profit += net + commission_total  # 還原手續費以得到 gross profit","        else:","            total_gross_loss += net - commission_total  # gross loss 為負值","        ","        trades += 1","    ","    return total_net, total_gross_profit, total_gross_loss, trades","","","def survive_s2(","    result_s2: StressResult,","    *,","    min_trades: int = 30,","    min_pf: float = 1.10,","    max_mdd_pct: Optional[float] = None,","    max_mdd_abs: Optional[float] = None,",") -> bool:","    \"\"\"","    判斷策略是否通過 S2 生存閘門","","    Args:","        result_s2: S2 等級的 StressResult","        min_trades: 最小交易次數","        min_pf: 最小盈利因子","        max_mdd_pct: 最大回撤百分比（如果可用）","        max_mdd_abs: 最大回撤絕對值（備用）","","    Returns:","        bool: 是否通過閘門","    \"\"\"","    # 檢查交易次數","    if result_s2.trades < min_trades:","        return False","    ","    # 檢查盈利因子","    if result_s2.profit_factor < min_pf:","        return False","    ","    # 檢查最大回撤（如果提供）","    if max_mdd_pct is not None:","        # 需要 equity curve 計算百分比回撤，目前暫不實作","        pass","    elif max_mdd_abs is not None:","        if result_s2.mdd_after_cost > max_mdd_abs:","            return False"]}
{"type":"file_chunk","path":"src/control/research_slippage_stress.py","chunk_index":1,"line_start":201,"line_end":262,"content":["    ","    return True","","","def compute_stress_test_passed(","    results: Dict[str, StressResult],","    stress_level: str = \"S3\",",") -> bool:","    \"\"\"","    計算壓力測試是否通過（S3 淨利 > 0）","","    Args:","        results: 壓力測試結果字典","        stress_level: 壓力測試等級（預設 S3）","","    Returns:","        bool: 壓力測試通過標誌","    \"\"\"","    stress_result = results.get(stress_level)","    if stress_result is None:","        return False","    return stress_result.net_after_cost > 0","","","def generate_stress_report(","    results: Dict[str, StressResult],","    slippage_policy: SlippagePolicy,","    survive_s2_flag: bool,","    stress_test_passed_flag: bool,",") -> Dict[str, Any]:","    \"\"\"","    產生壓力測試報告","","    Returns:","        報告字典，包含 policy、矩陣、閘門結果等","    \"\"\"","    matrix = {}","    for level, result in results.items():","        matrix[level] = {","            \"slip_ticks\": result.slip_ticks,","            \"net_after_cost\": result.net_after_cost,","            \"gross_profit\": result.gross_profit,","            \"gross_loss\": result.gross_loss,","            \"profit_factor\": result.profit_factor,","            \"mdd_after_cost\": result.mdd_after_cost,","            \"trades\": result.trades,","        }","    ","    return {","        \"slippage_policy\": {","            \"definition\": slippage_policy.definition,","            \"levels\": slippage_policy.levels,","            \"selection_level\": slippage_policy.selection_level,","            \"stress_level\": slippage_policy.stress_level,","            \"mc_execution_level\": slippage_policy.mc_execution_level,","        },","        \"stress_matrix\": matrix,","        \"survive_s2\": survive_s2_flag,","        \"stress_test_passed\": stress_test_passed_flag,","    }","",""]}
{"type":"file_footer","path":"src/control/research_slippage_stress.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/resolve_cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7901,"sha256":"07c5054252cec35cb7466143b080b03c1a0a80720ebea6f78fd7069ab110c4f3","total_lines":271,"chunk_count":2}
{"type":"file_chunk","path":"src/control/resolve_cli.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Resolve CLI：特徵解析命令列介面","","命令：","fishbro resolve features --season 2026Q1 --dataset-id CME.MNQ --strategy-id S1 --req configs/strategies/S1/features.json","","行為：","- 不允許 build → 只做檢查與載入","- 允許 build → 缺就 build，成功後載入，輸出 bundle 摘要（不輸出整個 array）","","Exit code：","0：已滿足且載入成功","10：已 build（可選）","20：缺失且不允許 build / build_ctx 不足","1：其他錯誤","\"\"\"","","from __future__ import annotations","","import sys","import json","import argparse","from pathlib import Path","from typing import Optional","","from contracts.strategy_features import (","    StrategyFeatureRequirements,","    load_requirements_from_json,",")","from control.feature_resolver import (","    resolve_features,","    MissingFeaturesError,","    ManifestMismatchError,","    BuildNotAllowedError,","    FeatureResolutionError,",")","from control.build_context import BuildContext","","","def main() -> int:","    \"\"\"CLI 主函數\"\"\"","    parser = create_parser()","    args = parser.parse_args()","    ","    try:","        return run_resolve(args)","    except KeyboardInterrupt:","        print(\"\\n中斷執行\", file=sys.stderr)","        return 130","    except Exception as e:","        print(f\"錯誤: {e}\", file=sys.stderr)","        return 1","","","def create_parser() -> argparse.ArgumentParser:","    \"\"\"建立命令列解析器\"\"\"","    parser = argparse.ArgumentParser(","        description=\"解析策略特徵依賴\",","        formatter_class=argparse.ArgumentDefaultsHelpFormatter,","    )","    ","    # 必要參數","    parser.add_argument(","        \"--season\",","        required=True,","        help=\"季節標記，例如 2026Q1\",","    )","    parser.add_argument(","        \"--dataset-id\",","        required=True,","        help=\"資料集 ID，例如 CME.MNQ\",","    )","    ","    # 需求來源（二選一）","    req_group = parser.add_mutually_exclusive_group(required=True)","    req_group.add_argument(","        \"--strategy-id\",","        help=\"策略 ID（用於自動尋找需求檔案）\",","    )","    req_group.add_argument(","        \"--req\",","        type=Path,","        help=\"需求 JSON 檔案路徑\",","    )","    ","    # build 相關參數","    parser.add_argument(","        \"--allow-build\",","        action=\"store_true\",","        help=\"允許自動 build 缺失的特徵\",","    )","    parser.add_argument(","        \"--txt-path\",","        type=Path,","        help=\"原始 TXT 檔案路徑（只有 allow-build 才需要）\",","    )","    parser.add_argument(","        \"--mode\",","        choices=[\"incremental\", \"full\"],","        default=\"incremental\",","        help=\"build 模式（只在 allow-build 時使用）\",","    )","    parser.add_argument(","        \"--outputs-root\",","        type=Path,","        default=Path(\"outputs\"),","        help=\"輸出根目錄\",","    )","    parser.add_argument(","        \"--build-bars-if-missing\",","        action=\"store_true\",","        default=True,","        help=\"如果 bars cache 不存在，是否建立 bars\",","    )","    parser.add_argument(","        \"--no-build-bars-if-missing\",","        action=\"store_false\",","        dest=\"build_bars_if_missing\",","        help=\"不建立 bars cache（即使缺失）\",","    )","    ","    # 輸出選項","    parser.add_argument(","        \"--json\",","        action=\"store_true\",","        help=\"以 JSON 格式輸出結果\",","    )","    parser.add_argument(","        \"--verbose\",","        action=\"store_true\",","        help=\"輸出詳細資訊\",","    )","    ","    return parser","","","def run_resolve(args) -> int:","    \"\"\"執行解析邏輯\"\"\"","    # 1. 載入需求","    requirements = load_requirements(args)","    ","    # 2. 準備 build_ctx（如果需要）","    build_ctx = prepare_build_context(args)","    ","    # 3. 執行解析","    try:","        bundle = resolve_features(","            season=args.season,","            dataset_id=args.dataset_id,","            requirements=requirements,","            outputs_root=args.outputs_root,","            allow_build=args.allow_build,","            build_ctx=build_ctx,","        )","        ","        # 4. 輸出結果","        output_result(bundle, args)","        ","        # 判斷 exit code","        # 如果有 build，回傳 10；否則回傳 0","        # 目前我們無法知道是否有 build，所以暫時回傳 0","        return 0","        ","    except MissingFeaturesError as e:","        print(f\"缺少特徵: {e}\", file=sys.stderr)","        return 20","    except BuildNotAllowedError as e:","        print(f\"不允許 build: {e}\", file=sys.stderr)","        return 20","    except ManifestMismatchError as e:","        print(f\"Manifest 合約不符: {e}\", file=sys.stderr)","        return 1","    except FeatureResolutionError as e:","        print(f\"特徵解析失敗: {e}\", file=sys.stderr)","        return 1","","","def load_requirements(args) -> StrategyFeatureRequirements:","    \"\"\"載入策略特徵需求\"\"\"","    if args.req:","        # 從指定 JSON 檔案載入","        return load_requirements_from_json(str(args.req))","    elif args.strategy_id:","        # 自動尋找需求檔案","        # 優先順序：","        # 1. strategies/{strategy_id}/features.json","        # 2. configs/strategies/{strategy_id}/features.json","        # 3. 當前目錄下的 {strategy_id}_features.json","        ","        possible_paths = [","            Path(f\"configs/strategies/{args.strategy_id}/features.json\"),","            Path(f\"strategies/{args.strategy_id}/features.json\"),  # legacy location","            Path(f\"{args.strategy_id}_features.json\"),","        ]","        ","        for path in possible_paths:","            if path.exists():","                return load_requirements_from_json(str(path))","        "]}
{"type":"file_chunk","path":"src/control/resolve_cli.py","chunk_index":1,"line_start":201,"line_end":271,"content":["        raise FileNotFoundError(","            f\"找不到策略 {args.strategy_id} 的需求檔案。\"","            f\"嘗試的路徑: {[str(p) for p in possible_paths]}\"","        )","    else:","        # 這不應該發生，因為 argparse 確保了二選一","        raise ValueError(\"必須提供 --req 或 --strategy-id\")","","","def prepare_build_context(args) -> Optional[BuildContext]:","    \"\"\"準備 BuildContext\"\"\"","    if not args.allow_build:","        return None","    ","    if not args.txt_path:","        raise ValueError(\"--allow-build 需要 --txt-path\")","    ","    # 驗證 txt_path 存在","    if not args.txt_path.exists():","        raise FileNotFoundError(f\"TXT 檔案不存在: {args.txt_path}\")","    ","    # 轉換 mode 為大寫","    mode = args.mode.upper()","    if mode not in (\"FULL\", \"INCREMENTAL\"):","        raise ValueError(f\"無效的 mode: {args.mode}，必須為 'incremental' 或 'full'\")","    ","    return BuildContext(","        txt_path=args.txt_path,","        mode=mode,","        outputs_root=args.outputs_root,","        build_bars_if_missing=args.build_bars_if_missing,","    )","","","def output_result(bundle, args) -> None:","    \"\"\"輸出解析結果\"\"\"","    if args.json:","        # JSON 格式輸出","        result = {","            \"success\": True,","            \"bundle\": bundle.to_dict(),","            \"series_count\": len(bundle.series),","            \"series_keys\": bundle.list_series(),","        }","        print(json.dumps(result, indent=2, ensure_ascii=False))","    else:","        # 文字格式輸出","        print(f\"✅ 特徵解析成功\")","        print(f\"   資料集: {bundle.dataset_id}\")","        print(f\"   季節: {bundle.season}\")","        print(f\"   特徵數量: {len(bundle.series)}\")","        ","        if args.verbose:","            print(f\"   Metadata:\")","            for key, value in bundle.meta.items():","                if key in (\"files_sha256\", \"manifest_sha256\"):","                    # 縮短 hash 顯示","                    if isinstance(value, str) and len(value) > 16:","                        value = f\"{value[:8]}...{value[-8:]}\"","                print(f\"     {key}: {value}\")","            ","            print(f\"   特徵列表:\")","            for name, tf in bundle.list_series():","                series = bundle.get_series(name, tf)","                print(f\"     {name}@{tf}m: {len(series.ts)} 筆資料\")","","","if __name__ == \"__main__\":","    sys.exit(main())","",""]}
{"type":"file_footer","path":"src/control/resolve_cli.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/season_api.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7085,"sha256":"b19d89844124dec274d5133f6c0a8808d83e9b21c19d464e2bce2b27afd2d4a2","total_lines":215,"chunk_count":2}
{"type":"file_chunk","path":"src/control/season_api.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 15.0: Season-level governance and index builder (Research OS).","","Contracts:","- Do NOT modify Engine / JobSpec / batch artifacts content.","- Season index is a separate tree (season_index/{season}/...).","- Rebuild index is deterministic: stable ordering by batch_id.","- Only reads JSON from artifacts/{batch_id}/metadata.json, index.json, summary.json.","- Writes season_index.json and season_metadata.json using atomic write.","","Environment overrides:","- FISHBRO_SEASON_INDEX_ROOT (default: outputs/season_index)","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass, field","from datetime import datetime, timezone","from pathlib import Path","from typing import Any, Optional","","from control.artifacts import compute_sha256, write_json_atomic","","","def _utc_now_iso() -> str:","    return datetime.now(timezone.utc).replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")","","","def get_season_index_root() -> Path:","    import os","    return Path(os.environ.get(\"FISHBRO_SEASON_INDEX_ROOT\", \"outputs/season_index\"))","","","def _read_json(path: Path) -> dict[str, Any]:","    if not path.exists():","        raise FileNotFoundError(str(path))","    return json.loads(path.read_text(encoding=\"utf-8\"))","","","def _file_sha256(path: Path) -> Optional[str]:","    if not path.exists():","        return None","    return compute_sha256(path.read_bytes())","","","@dataclass","class SeasonMetadata:","    season: str","    frozen: bool = False","    tags: list[str] = field(default_factory=list)","    note: str = \"\"","    created_at: str = \"\"","    updated_at: str = \"\"","","","class SeasonStore:","    \"\"\"","    Store for season_index/{season}/season_index.json and season_metadata.json","    \"\"\"","","    def __init__(self, season_index_root: Path):","        self.root = season_index_root","        self.root.mkdir(parents=True, exist_ok=True)","","    def season_dir(self, season: str) -> Path:","        return self.root / season","","    def index_path(self, season: str) -> Path:","        return self.season_dir(season) / \"season_index.json\"","","    def metadata_path(self, season: str) -> Path:","        return self.season_dir(season) / \"season_metadata.json\"","","    # ---------- metadata ----------","    def get_metadata(self, season: str) -> Optional[SeasonMetadata]:","        path = self.metadata_path(season)","        if not path.exists():","            return None","        data = json.loads(path.read_text(encoding=\"utf-8\"))","        tags = data.get(\"tags\", [])","        if not isinstance(tags, list):","            raise ValueError(\"season_metadata.tags must be a list\")","        return SeasonMetadata(","            season=data[\"season\"],","            frozen=bool(data.get(\"frozen\", False)),","            tags=list(tags),","            note=data.get(\"note\", \"\"),","            created_at=data.get(\"created_at\", \"\"),","            updated_at=data.get(\"updated_at\", \"\"),","        )","","    def set_metadata(self, season: str, meta: SeasonMetadata) -> None:","        path = self.metadata_path(season)","        path.parent.mkdir(parents=True, exist_ok=True)","        payload = {","            \"season\": season,","            \"frozen\": bool(meta.frozen),","            \"tags\": list(meta.tags),","            \"note\": meta.note,","            \"created_at\": meta.created_at,","            \"updated_at\": meta.updated_at,","        }","        write_json_atomic(path, payload)","","    def update_metadata(","        self,","        season: str,","        *,","        tags: Optional[list[str]] = None,","        note: Optional[str] = None,","        frozen: Optional[bool] = None,","    ) -> SeasonMetadata:","        now = _utc_now_iso()","        existing = self.get_metadata(season)","        if existing is None:","            existing = SeasonMetadata(season=season, created_at=now, updated_at=now)","","        if existing.frozen and frozen is False:","            raise ValueError(\"Cannot unfreeze a frozen season\")","","        if tags is not None:","            merged = set(existing.tags)","            merged.update(tags)","            existing.tags = sorted(merged)","","        if note is not None:","            existing.note = note","","        if frozen is not None:","            if frozen is True:","                existing.frozen = True","            elif frozen is False:","                # allowed only when not already frozen","                existing.frozen = False","","        existing.updated_at = now","        self.set_metadata(season, existing)","        return existing","","    def freeze(self, season: str) -> None:","        meta = self.get_metadata(season)","        if meta is None:","            # create metadata on freeze if it doesn't exist","            now = _utc_now_iso()","            meta = SeasonMetadata(season=season, created_at=now, updated_at=now, frozen=True)","            self.set_metadata(season, meta)","            return","","        if not meta.frozen:","            meta.frozen = True","            meta.updated_at = _utc_now_iso()","            self.set_metadata(season, meta)","","    def is_frozen(self, season: str) -> bool:","        meta = self.get_metadata(season)","        return bool(meta and meta.frozen)","","    # ---------- index ----------","    def read_index(self, season: str) -> dict[str, Any]:","        return _read_json(self.index_path(season))","","    def write_index(self, season: str, index_obj: dict[str, Any]) -> None:","        path = self.index_path(season)","        path.parent.mkdir(parents=True, exist_ok=True)","        write_json_atomic(path, index_obj)","","    def rebuild_index(self, artifacts_root: Path, season: str) -> dict[str, Any]:","        \"\"\"","        Scan artifacts_root/*/metadata.json to collect batches where metadata.season == season.","        Then attach hashes for index.json and summary.json (if present).","        Deterministic: sort by batch_id.","        \"\"\"","        if not artifacts_root.exists():","            # no artifacts root -> empty index","            artifacts_root.mkdir(parents=True, exist_ok=True)","","        batches: list[dict[str, Any]] = []","","        # deterministic: sorted by directory name","        for batch_dir in sorted([p for p in artifacts_root.iterdir() if p.is_dir()], key=lambda p: p.name):","            batch_id = batch_dir.name","            meta_path = batch_dir / \"metadata.json\"","            if not meta_path.exists():","                continue","","            # Do NOT swallow corruption: index build should surface errors","            meta = json.loads(meta_path.read_text(encoding=\"utf-8\"))","            if meta.get(\"season\", \"\") != season:","                continue","","            idx_hash = _file_sha256(batch_dir / \"index.json\")","            sum_hash = _file_sha256(batch_dir / \"summary.json\")","","            batches.append(","                {","                    \"batch_id\": batch_id,","                    \"frozen\": bool(meta.get(\"frozen\", False)),","                    \"tags\": sorted(set(meta.get(\"tags\", []) or [])),"]}
{"type":"file_chunk","path":"src/control/season_api.py","chunk_index":1,"line_start":201,"line_end":215,"content":["                    \"note\": meta.get(\"note\", \"\") or \"\",","                    \"index_hash\": idx_hash,","                    \"summary_hash\": sum_hash,","                }","            )","","        out = {","            \"season\": season,","            \"generated_at\": _utc_now_iso(),","            \"batches\": batches,","        }","        self.write_index(season, out)","        return out","",""]}
{"type":"file_footer","path":"src/control/season_api.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/season_compare.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4753,"sha256":"d2d25d60ce40705c6d4ffc40ec67c35704b6932ad370526c945b3f2e172d3e10","total_lines":184,"chunk_count":1}
{"type":"file_chunk","path":"src/control/season_compare.py","chunk_index":0,"line_start":1,"line_end":184,"content":["","\"\"\"","Phase 15.1: Season-level cross-batch comparison helpers.","","Contracts:","- Read-only: only reads season_index.json and artifacts/{batch_id}/summary.json","- No on-the-fly recomputation of batch summary","- Deterministic:","  - Sort by score desc","  - Tie-break by batch_id asc","  - Tie-break by job_id asc","- Robust:","  - Missing/corrupt batch summary is skipped (never 500 the whole season)","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Optional","","","def _read_json(path: Path) -> dict[str, Any]:","    return json.loads(path.read_text(encoding=\"utf-8\"))","","","def _extract_job_id(row: Any) -> Optional[str]:","    if not isinstance(row, dict):","        return None","    # canonical","    if \"job_id\" in row and row[\"job_id\"] is not None:","        return str(row[\"job_id\"])","    # common alternates (defensive)","    if \"id\" in row and row[\"id\"] is not None:","        return str(row[\"id\"])","    return None","","","def _extract_score(row: Any) -> Optional[float]:","    if not isinstance(row, dict):","        return None","","    # canonical","    if \"score\" in row:","        try:","            v = row[\"score\"]","            if v is None:","                return None","            return float(v)","        except Exception:","            return None","","    # alternate: metrics.score","    m = row.get(\"metrics\")","    if isinstance(m, dict) and \"score\" in m:","        try:","            v = m[\"score\"]","            if v is None:","                return None","            return float(v)","        except Exception:","            return None","","    return None","","","@dataclass(frozen=True)","class SeasonTopKResult:","    season: str","    k: int","    items: list[dict[str, Any]]","    skipped_batches: list[str]","","","def merge_season_topk(","    *,","    artifacts_root: Path,","    season_index: dict[str, Any],","    k: int,",") -> SeasonTopKResult:","    \"\"\"","    Merge topk entries across batches listed in season_index.json.","","    Output item schema:","      {","        \"batch_id\": \"...\",","        \"job_id\": \"...\",","        \"score\": 1.23,","        \"row\": {... original topk row ...}","      }","","    Skipping rules:","    - missing summary.json -> skip batch","    - invalid json -> skip batch","    - missing topk list -> treat as empty","    \"\"\"","    season = str(season_index.get(\"season\", \"\"))","    batches = season_index.get(\"batches\", [])","    if not isinstance(batches, list):","        raise ValueError(\"season_index.batches must be a list\")","","    # sanitize k","    try:","        k_int = int(k)","    except Exception:","        k_int = 20","    if k_int <= 0:","        k_int = 20","","    merged: list[dict[str, Any]] = []","    skipped: list[str] = []","","    # deterministic traversal order: batch_id asc","    batch_ids: list[str] = []","    for b in batches:","        if isinstance(b, dict) and \"batch_id\" in b:","            batch_ids.append(str(b[\"batch_id\"]))","    batch_ids = sorted(set(batch_ids))","","    for batch_id in batch_ids:","        summary_path = artifacts_root / batch_id / \"summary.json\"","        if not summary_path.exists():","            skipped.append(batch_id)","            continue","","        try:","            summary = _read_json(summary_path)","        except Exception:","            skipped.append(batch_id)","            continue","","        topk = summary.get(\"topk\", [])","        if not isinstance(topk, list):","            # malformed topk -> treat as skip (stronger safety)","            skipped.append(batch_id)","            continue","","        for row in topk:","            job_id = _extract_job_id(row)","            if job_id is None:","                # cannot tie-break deterministically without job_id","                continue","            score = _extract_score(row)","            merged.append(","                {","                    \"batch_id\": batch_id,","                    \"job_id\": job_id,","                    \"score\": score,","                    \"row\": row,","                }","            )","","    def sort_key(item: dict[str, Any]) -> tuple:","        # score desc; None goes last","        score = item.get(\"score\")","        score_is_none = score is None","        # For numeric scores: use -score","        neg_score = 0.0","        if not score_is_none:","            try:","                neg_score = -float(score)","            except Exception:","                score_is_none = True","                neg_score = 0.0","","        return (","            score_is_none,     # False first, True last","            neg_score,         # smaller first -> higher score first","            str(item.get(\"batch_id\", \"\")),","            str(item.get(\"job_id\", \"\")),","        )","","    merged_sorted = sorted(merged, key=sort_key)","    merged_sorted = merged_sorted[:k_int]","","    return SeasonTopKResult(","        season=season,","        k=k_int,","        items=merged_sorted,","        skipped_batches=sorted(set(skipped)),","    )","",""]}
{"type":"file_footer","path":"src/control/season_compare.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/season_compare_batches.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8174,"sha256":"aec07cf2f8884965b7bb2a838cb70d4feda5b32f0b2f47e8c33809ce46d8d4a5","total_lines":279,"chunk_count":2}
{"type":"file_chunk","path":"src/control/season_compare_batches.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 15.2: Season compare batch cards + lightweight leaderboard.","","Contracts:","- Read-only: reads season_index.json and artifacts/{batch_id}/summary.json","- No on-the-fly recomputation","- Deterministic:","  - Batches list sorted by batch_id asc","  - Leaderboard sorted by score desc, tie-break batch_id asc, job_id asc","- Robust:","  - Missing/corrupt summary.json => summary_ok=False, keep other fields","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Optional","","","def _read_json(path: Path) -> dict[str, Any]:","    return json.loads(path.read_text(encoding=\"utf-8\"))","","","def _safe_get_job_id(row: Any) -> Optional[str]:","    if not isinstance(row, dict):","        return None","    if row.get(\"job_id\") is not None:","        return str(row[\"job_id\"])","    if row.get(\"id\") is not None:","        return str(row[\"id\"])","    return None","","","def _safe_get_score(row: Any) -> Optional[float]:","    if not isinstance(row, dict):","        return None","    if \"score\" in row:","        try:","            v = row[\"score\"]","            if v is None:","                return None","            return float(v)","        except Exception:","            return None","    m = row.get(\"metrics\")","    if isinstance(m, dict) and \"score\" in m:","        try:","            v = m[\"score\"]","            if v is None:","                return None","            return float(v)","        except Exception:","            return None","    return None","","","def _extract_group_key(row: Any, group_by: str) -> str:","    \"\"\"","    group_by candidates:","      - \"strategy_id\"","      - \"dataset_id\"","    If not present, return \"unknown\".","    \"\"\"","    if not isinstance(row, dict):","        return \"unknown\"","    v = row.get(group_by)","    if v is None:","        # sometimes nested","        meta = row.get(\"meta\")","        if isinstance(meta, dict):","            v = meta.get(group_by)","    return str(v) if v is not None else \"unknown\"","","","@dataclass(frozen=True)","class SeasonBatchesResult:","    season: str","    batches: list[dict[str, Any]]","    skipped_summaries: list[str]","","","def build_season_batch_cards(","    *,","    artifacts_root: Path,","    season_index: dict[str, Any],",") -> SeasonBatchesResult:","    \"\"\"","    Build deterministic batch cards for a season.","","    For each batch_id in season_index.batches:","      - frozen/tags/note/index_hash/summary_hash are read from season_index (source of truth)","      - summary.json is read best-effort:","          top_job_id, top_score, topk_size","      - missing/corrupt summary => summary_ok=False","    \"\"\"","    season = str(season_index.get(\"season\", \"\"))","    batches_in = season_index.get(\"batches\", [])","    if not isinstance(batches_in, list):","        raise ValueError(\"season_index.batches must be a list\")","","    # deterministic batch_id list","    by_id: dict[str, dict[str, Any]] = {}","    for b in batches_in:","        if not isinstance(b, dict) or \"batch_id\" not in b:","            continue","        batch_id = str(b[\"batch_id\"])","        by_id[batch_id] = b","","    batch_ids = sorted(by_id.keys())","","    cards: list[dict[str, Any]] = []","    skipped: list[str] = []","","    for batch_id in batch_ids:","        b = by_id[batch_id]","        card: dict[str, Any] = {","            \"batch_id\": batch_id,","            \"frozen\": bool(b.get(\"frozen\", False)),","            \"tags\": list(b.get(\"tags\", []) or []),","            \"note\": b.get(\"note\", \"\") or \"\",","            \"index_hash\": b.get(\"index_hash\"),","            \"summary_hash\": b.get(\"summary_hash\"),","            # summary-derived","            \"summary_ok\": True,","            \"top_job_id\": None,","            \"top_score\": None,","            \"topk_size\": 0,","        }","","        summary_path = artifacts_root / batch_id / \"summary.json\"","        if not summary_path.exists():","            card[\"summary_ok\"] = False","            skipped.append(batch_id)","            cards.append(card)","            continue","","        try:","            s = _read_json(summary_path)","            topk = s.get(\"topk\", [])","            if not isinstance(topk, list):","                raise ValueError(\"summary.topk must be list\")","","            card[\"topk_size\"] = len(topk)","            if len(topk) > 0:","                first = topk[0]","                card[\"top_job_id\"] = _safe_get_job_id(first)","                card[\"top_score\"] = _safe_get_score(first)","        except Exception:","            card[\"summary_ok\"] = False","            skipped.append(batch_id)","","        cards.append(card)","","    return SeasonBatchesResult(season=season, batches=cards, skipped_summaries=sorted(set(skipped)))","","","def build_season_leaderboard(","    *,","    artifacts_root: Path,","    season_index: dict[str, Any],","    group_by: str = \"strategy_id\",","    per_group: int = 3,",") -> dict[str, Any]:","    \"\"\"","    Build a grouped leaderboard from batch summaries' topk rows.","","    Returns:","      {","        \"season\": \"...\",","        \"group_by\": \"strategy_id\",","        \"per_group\": 3,","        \"groups\": [","           {\"key\": \"...\", \"items\": [...]},","           ...","        ],","        \"skipped_batches\": [...]","      }","    \"\"\"","    season = str(season_index.get(\"season\", \"\"))","    batches_in = season_index.get(\"batches\", [])","    if not isinstance(batches_in, list):","        raise ValueError(\"season_index.batches must be a list\")","","    if group_by not in (\"strategy_id\", \"dataset_id\"):","        raise ValueError(\"group_by must be 'strategy_id' or 'dataset_id'\")","","    try:","        per_group_i = int(per_group)","    except Exception:","        per_group_i = 3","    if per_group_i <= 0:","        per_group_i = 3","","    # deterministic batch traversal: batch_id asc","    batch_ids = sorted({str(b[\"batch_id\"]) for b in batches_in if isinstance(b, dict) and \"batch_id\" in b})","","    merged: list[dict[str, Any]] = []"]}
{"type":"file_chunk","path":"src/control/season_compare_batches.py","chunk_index":1,"line_start":201,"line_end":279,"content":["    skipped: list[str] = []","","    for batch_id in batch_ids:","        p = artifacts_root / batch_id / \"summary.json\"","        if not p.exists():","            skipped.append(batch_id)","            continue","        try:","            s = _read_json(p)","            topk = s.get(\"topk\", [])","            if not isinstance(topk, list):","                skipped.append(batch_id)","                continue","            for row in topk:","                job_id = _safe_get_job_id(row)","                if job_id is None:","                    continue","                score = _safe_get_score(row)","                merged.append(","                    {","                        \"batch_id\": batch_id,","                        \"job_id\": job_id,","                        \"score\": score,","                        \"group\": _extract_group_key(row, group_by),","                        \"row\": row,","                    }","                )","        except Exception:","            skipped.append(batch_id)","            continue","","    def sort_key(it: dict[str, Any]) -> tuple:","        score = it.get(\"score\")","        score_is_none = score is None","        neg_score = 0.0","        if not score_is_none:","            try:","                # score is not None at this point, but mypy doesn't know","                neg_score = -float(score)  # type: ignore[arg-type]","            except Exception:","                score_is_none = True","                neg_score = 0.0","        return (","            score_is_none,","            neg_score,","            str(it.get(\"batch_id\", \"\")),","            str(it.get(\"job_id\", \"\")),","        )","","    merged_sorted = sorted(merged, key=sort_key)","","    # group, keep top per_group_i in deterministic order (already sorted)","    groups: dict[str, list[dict[str, Any]]] = {}","    for it in merged_sorted:","        key = str(it.get(\"group\", \"unknown\"))","        if key not in groups:","            groups[key] = []","        if len(groups[key]) < per_group_i:","            groups[key].append(","                {","                    \"batch_id\": it[\"batch_id\"],","                    \"job_id\": it[\"job_id\"],","                    \"score\": it[\"score\"],","                    \"row\": it[\"row\"],","                }","            )","","    # deterministic group ordering: key asc","    out_groups = [{\"key\": k, \"items\": groups[k]} for k in sorted(groups.keys())]","","    return {","        \"season\": season,","        \"group_by\": group_by,","        \"per_group\": per_group_i,","        \"groups\": out_groups,","        \"skipped_batches\": sorted(set(skipped)),","    }","",""]}
{"type":"file_footer","path":"src/control/season_compare_batches.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/season_export.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9531,"sha256":"72bbfba75eb4ac5c7bc47593b37a1f6e149d910978be83b14be58549566108fd","total_lines":282,"chunk_count":2}
{"type":"file_chunk","path":"src/control/season_export.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 15.3: Season freeze package / export pack.","","Contracts:","- Controlled mutation: writes only under exports root (default outputs/exports).","- Does NOT modify artifacts/ or season_index/ trees.","- Requires season is frozen (governance hardening).","- Deterministic:","  - batches sorted by batch_id asc","  - manifest files sorted by rel_path asc","- Auditable:","  - package_manifest.json includes sha256 for each exported file","  - includes manifest_sha256 (sha of the manifest bytes)","\"\"\"","","from __future__ import annotations","","import json","import os","import shutil","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Optional","","from control.artifacts import compute_sha256, write_atomic_json","from control.season_api import SeasonStore","from control.batch_api import read_summary, read_index","from utils.write_scope import WriteScope","","","def get_exports_root() -> Path:","    return Path(os.environ.get(\"FISHBRO_EXPORTS_ROOT\", \"outputs/exports\"))","","","def _copy_file(src: Path, dst: Path) -> None:","    dst.parent.mkdir(parents=True, exist_ok=True)","    shutil.copy2(src, dst)","","","def _file_sha256(path: Path) -> str:","    return compute_sha256(path.read_bytes())","","","@dataclass(frozen=True)","class ExportResult:","    season: str","    export_dir: Path","    manifest_path: Path","    manifest_sha256: str","    exported_files: list[dict[str, Any]]","    missing_files: list[str]","","","def export_season_package(","    *,","    season: str,","    artifacts_root: Path,","    season_index_root: Path,","    exports_root: Optional[Path] = None,",") -> ExportResult:","    \"\"\"","    Export a frozen season into an immutable, auditable package directory.","","    Package layout:","      exports/seasons/{season}/","        package_manifest.json","        season_index.json","        season_metadata.json","        batches/{batch_id}/metadata.json","        batches/{batch_id}/index.json (optional if missing)","        batches/{batch_id}/summary.json (optional if missing)","    \"\"\"","    exports_root = exports_root or get_exports_root()","    store = SeasonStore(season_index_root)","","    if not store.is_frozen(season):","        raise PermissionError(\"Season must be frozen before export\")","","    # must have season index","    season_index = store.read_index(season)  # FileNotFoundError surfaces to API as 404","","    season_dir = exports_root / \"seasons\" / season","    batches_dir = season_dir / \"batches\"","    season_dir.mkdir(parents=True, exist_ok=True)","    batches_dir.mkdir(parents=True, exist_ok=True)","","    # Build the set of allowed relative paths according to export‑pack spec.","    # We'll collect them as we go, then create a WriteScope that permits exactly those paths.","    allowed_rel_files: set[str] = set()","    exported_files: list[dict[str, Any]] = []","    missing: list[str] = []","","    # Helper to record an allowed file and copy it","    def copy_and_allow(src: Path, dst: Path, rel: str) -> None:","        _copy_file(src, dst)","        allowed_rel_files.add(rel)","        exported_files.append({\"path\": rel, \"sha256\": _file_sha256(dst)})","","    # 1) copy season_index.json + season_metadata.json (metadata may not exist; if missing -> we still record missing)","    src_index = season_index_root / season / \"season_index.json\"","    dst_index = season_dir / \"season_index.json\"","    copy_and_allow(src_index, dst_index, \"season_index.json\")","","    src_meta = season_index_root / season / \"season_metadata.json\"","    dst_meta = season_dir / \"season_metadata.json\"","    if src_meta.exists():","        copy_and_allow(src_meta, dst_meta, \"season_metadata.json\")","    else:","        missing.append(\"season_metadata.json\")","","    # 2) copy batch files referenced by season index","    batches = season_index.get(\"batches\", [])","    if not isinstance(batches, list):","        raise ValueError(\"season_index.batches must be a list\")","","    batch_ids = sorted(","        {str(b[\"batch_id\"]) for b in batches if isinstance(b, dict) and \"batch_id\" in b}","    )","","    for batch_id in batch_ids:","        # metadata.json is the anchor","        src_batch_meta = artifacts_root / batch_id / \"metadata.json\"","        rel_meta = str(Path(\"batches\") / batch_id / \"metadata.json\")","        dst_batch_meta = batches_dir / batch_id / \"metadata.json\"","        if src_batch_meta.exists():","            copy_and_allow(src_batch_meta, dst_batch_meta, rel_meta)","        else:","            missing.append(rel_meta)","","        # index.json optional","        src_idx = artifacts_root / batch_id / \"index.json\"","        rel_idx = str(Path(\"batches\") / batch_id / \"index.json\")","        dst_idx = batches_dir / batch_id / \"index.json\"","        if src_idx.exists():","            copy_and_allow(src_idx, dst_idx, rel_idx)","        else:","            missing.append(rel_idx)","","        # summary.json optional","        src_sum = artifacts_root / batch_id / \"summary.json\"","        rel_sum = str(Path(\"batches\") / batch_id / \"summary.json\")","        dst_sum = batches_dir / batch_id / \"summary.json\"","        if src_sum.exists():","            copy_and_allow(src_sum, dst_sum, rel_sum)","        else:","            missing.append(rel_sum)","","    # 3) build deterministic manifest (sort by path)","    exported_files_sorted = sorted(exported_files, key=lambda x: x[\"path\"])","","    manifest_obj = {","        \"season\": season,","        \"generated_at\": season_index.get(\"generated_at\", \"\"),","        \"source_roots\": {","            \"artifacts_root\": str(artifacts_root),","            \"season_index_root\": str(season_index_root),","        },","        \"deterministic_order\": {","            \"batches\": \"batch_id asc\",","            \"files\": \"path asc\",","        },","        \"files\": exported_files_sorted,","        \"missing_files\": sorted(set(missing)),","    }","","    manifest_path = season_dir / \"package_manifest.json\"","    allowed_rel_files.add(\"package_manifest.json\")","    write_atomic_json(manifest_path, manifest_obj)","","    manifest_sha256 = compute_sha256(manifest_path.read_bytes())","","    # write back manifest hash (2nd pass) for self-audit (still deterministic because it depends on bytes)","    manifest_obj2 = dict(manifest_obj)","    manifest_obj2[\"manifest_sha256\"] = manifest_sha256","    write_atomic_json(manifest_path, manifest_obj2)","    manifest_sha2562 = compute_sha256(manifest_path.read_bytes())","","    # 4) create replay_index.json for compare replay without artifacts","    replay_index_path = season_dir / \"replay_index.json\"","    allowed_rel_files.add(\"replay_index.json\")","    replay_index = _build_replay_index(","        season=season,","        season_index=season_index,","        artifacts_root=artifacts_root,","        batches_dir=batches_dir,","    )","    write_atomic_json(replay_index_path, replay_index)","    exported_files_sorted.append(","        {","            \"path\": str(Path(\"replay_index.json\")),","            \"sha256\": _file_sha256(replay_index_path),","        }","    )","","    # Now create a WriteScope that permits exactly the files we have written.","    # This scope will be used to validate any future writes (none in this function).","    # We also add a guard for the manifest write (already done) and replay_index write.","    scope = WriteScope(","        root_dir=season_dir,"]}
{"type":"file_chunk","path":"src/control/season_export.py","chunk_index":1,"line_start":201,"line_end":282,"content":["        allowed_rel_files=frozenset(allowed_rel_files),","        allowed_rel_prefixes=(),","    )","    # Verify that all exported files are allowed (should be true by construction)","    for ef in exported_files_sorted:","        scope.assert_allowed_rel(ef[\"path\"])","","    return ExportResult(","        season=season,","        export_dir=season_dir,","        manifest_path=manifest_path,","        manifest_sha256=manifest_sha2562,","        exported_files=exported_files_sorted,","        missing_files=sorted(set(missing)),","    )","","","def _build_replay_index(","    season: str,","    season_index: dict[str, Any],","    artifacts_root: Path,","    batches_dir: Path,",") -> dict[str, Any]:","    \"\"\"","    Build replay index for compare replay without artifacts.","    ","    Contains:","    - season metadata","    - batch summaries (topk, metrics)","    - batch indices (job list)","    - deterministic ordering","    \"\"\"","    batches = season_index.get(\"batches\", [])","    if not isinstance(batches, list):","        raise ValueError(\"season_index.batches must be a list\")","","    batch_ids = sorted(","        {str(b[\"batch_id\"]) for b in batches if isinstance(b, dict) and \"batch_id\" in b}","    )","","    replay_batches: list[dict[str, Any]] = []","    for batch_id in batch_ids:","        batch_info: dict[str, Any] = {\"batch_id\": batch_id}","        ","        # Try to read summary.json","        summary_path = artifacts_root / batch_id / \"summary.json\"","        if summary_path.exists():","            try:","                summary = read_summary(artifacts_root, batch_id)","                batch_info[\"summary\"] = {","                    \"topk\": summary.get(\"topk\", []),","                    \"metrics\": summary.get(\"metrics\", {}),","                }","            except Exception:","                batch_info[\"summary\"] = None","        else:","            batch_info[\"summary\"] = None","        ","        # Try to read index.json","        index_path = artifacts_root / batch_id / \"index.json\"","        if index_path.exists():","            try:","                index = read_index(artifacts_root, batch_id)","                batch_info[\"index\"] = index","            except Exception:","                batch_info[\"index\"] = None","        else:","            batch_info[\"index\"] = None","        ","        replay_batches.append(batch_info)","","    return {","        \"season\": season,","        \"generated_at\": season_index.get(\"generated_at\", \"\"),","        \"batches\": replay_batches,","        \"deterministic_order\": {","            \"batches\": \"batch_id asc\",","            \"files\": \"path asc\",","        },","    }","",""]}
{"type":"file_footer","path":"src/control/season_export.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/season_export_replay.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7688,"sha256":"6e67dfc188688deb63ceb927d4b6bd50f363decb94e13978db559a2bebcd91ce","total_lines":253,"chunk_count":2}
{"type":"file_chunk","path":"src/control/season_export_replay.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 16: Export Pack Replay Mode.","","Allows compare endpoints to work from an exported season package","without requiring access to the original artifacts/ directory.","","Key contracts:","- Read-only: only reads from exports root, never writes","- Deterministic: same ordering as original compare endpoints","- Fallback: if replay_index.json missing, raise FileNotFoundError","- No artifacts dependency: does not require artifacts/ directory","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Optional","","","@dataclass(frozen=True)","class ReplaySeasonTopkResult:","    season: str","    k: int","    items: list[dict[str, Any]]","    skipped_batches: list[str]","","","@dataclass(frozen=True)","class ReplaySeasonBatchCardsResult:","    season: str","    batches: list[dict[str, Any]]","    skipped_summaries: list[str]","","","@dataclass(frozen=True)","class ReplaySeasonLeaderboardResult:","    season: str","    group_by: str","    per_group: int","    groups: list[dict[str, Any]]","","","def load_replay_index(exports_root: Path, season: str) -> dict[str, Any]:","    \"\"\"","    Load replay_index.json from an exported season package.","    ","    Raises:","        FileNotFoundError: if replay_index.json does not exist","        ValueError: if JSON is invalid","    \"\"\"","    replay_path = exports_root / \"seasons\" / season / \"replay_index.json\"","    if not replay_path.exists():","        raise FileNotFoundError(f\"replay_index.json not found for season {season}\")","    ","    text = replay_path.read_text(encoding=\"utf-8\")","    return json.loads(text)","","","def replay_season_topk(","    exports_root: Path,","    season: str,","    k: int = 20,",") -> ReplaySeasonTopkResult:","    \"\"\"","    Replay cross-batch TopK from exported season package.","    ","    Implementation mirrors merge_season_topk but uses replay_index.json","    instead of reading artifacts/{batch_id}/summary.json.","    \"\"\"","    replay_index = load_replay_index(exports_root, season)","    ","    all_items: list[dict[str, Any]] = []","    skipped_batches: list[str] = []","    ","    for batch_info in replay_index.get(\"batches\", []):","        batch_id = batch_info.get(\"batch_id\", \"\")","        summary = batch_info.get(\"summary\")","        ","        if summary is None:","            skipped_batches.append(batch_id)","            continue","        ","        topk = summary.get(\"topk\", [])","        if not isinstance(topk, list):","            skipped_batches.append(batch_id)","            continue","        ","        # Add batch_id to each item for traceability","        for item in topk:","            if isinstance(item, dict):","                item_copy = dict(item)","                item_copy[\"_batch_id\"] = batch_id","                all_items.append(item_copy)","    ","    # Sort by (-score, batch_id, job_id) for deterministic ordering","    def _sort_key(item: dict[str, Any]) -> tuple:","        # Score (descending, so use negative)","        score = item.get(\"score\")","        if isinstance(score, (int, float)):","            score_val = -float(score)  # Negative for descending sort","        else:","            score_val = float(\"inf\")  # Missing scores go last","        ","        # Batch ID (from _batch_id added earlier)","        batch_id = item.get(\"_batch_id\", \"\")","        ","        # Job ID","        job_id = item.get(\"job_id\", \"\")","        ","        return (score_val, batch_id, job_id)","    ","    sorted_items = sorted(all_items, key=_sort_key)","    topk_items = sorted_items[:k] if k > 0 else sorted_items","    ","    return ReplaySeasonTopkResult(","        season=season,","        k=k,","        items=topk_items,","        skipped_batches=skipped_batches,","    )","","","def replay_season_batch_cards(","    exports_root: Path,","    season: str,",") -> ReplaySeasonBatchCardsResult:","    \"\"\"","    Replay batch-level compare cards from exported season package.","    ","    Implementation mirrors build_season_batch_cards but uses replay_index.json.","    Deterministic ordering: batches sorted by batch_id ascending.","    \"\"\"","    replay_index = load_replay_index(exports_root, season)","    ","    batches: list[dict[str, Any]] = []","    skipped_summaries: list[str] = []","    ","    # Sort batches by batch_id for deterministic output","    batch_infos = replay_index.get(\"batches\", [])","    sorted_batch_infos = sorted(batch_infos, key=lambda b: b.get(\"batch_id\", \"\"))","    ","    for batch_info in sorted_batch_infos:","        batch_id = batch_info.get(\"batch_id\", \"\")","        summary = batch_info.get(\"summary\")","        index = batch_info.get(\"index\")","        ","        if summary is None:","            skipped_summaries.append(batch_id)","            continue","        ","        # Build batch card","        card: dict[str, Any] = {","            \"batch_id\": batch_id,","            \"summary\": summary,","        }","        ","        if index is not None:","            card[\"index\"] = index","        ","        batches.append(card)","    ","    return ReplaySeasonBatchCardsResult(","        season=season,","        batches=batches,","        skipped_summaries=skipped_summaries,","    )","","","def replay_season_leaderboard(","    exports_root: Path,","    season: str,","    group_by: str = \"strategy_id\",","    per_group: int = 3,",") -> ReplaySeasonLeaderboardResult:","    \"\"\"","    Replay grouped leaderboard from exported season package.","    ","    Implementation mirrors build_season_leaderboard but uses replay_index.json.","    \"\"\"","    replay_index = load_replay_index(exports_root, season)","    ","    # Collect all items with grouping key","    items_by_group: dict[str, list[dict[str, Any]]] = {}","    ","    for batch_info in replay_index.get(\"batches\", []):","        summary = batch_info.get(\"summary\")","        if summary is None:","            continue","        ","        topk = summary.get(\"topk\", [])","        if not isinstance(topk, list):","            continue","        ","        for item in topk:","            if not isinstance(item, dict):","                continue","            "]}
{"type":"file_chunk","path":"src/control/season_export_replay.py","chunk_index":1,"line_start":201,"line_end":253,"content":["            # Add batch_id for deterministic sorting","            item_copy = dict(item)","            item_copy[\"_batch_id\"] = batch_info.get(\"batch_id\", \"\")","            ","            # Extract grouping key","            group_key = item_copy.get(group_by, \"\")","            if not isinstance(group_key, str):","                group_key = str(group_key)","            ","            if group_key not in items_by_group:","                items_by_group[group_key] = []","            ","            items_by_group[group_key].append(item_copy)","    ","    # Sort items within each group by (-score, batch_id, job_id) for deterministic ordering","    def _sort_key(item: dict[str, Any]) -> tuple:","        # Score (descending, so use negative)","        score = item.get(\"score\")","        if isinstance(score, (int, float)):","            score_val = -float(score)  # Negative for descending sort","        else:","            score_val = float(\"inf\")  # Missing scores go last","        ","        # Batch ID (item may not have _batch_id in leaderboard context)","        batch_id = item.get(\"_batch_id\", item.get(\"batch_id\", \"\"))","        ","        # Job ID","        job_id = item.get(\"job_id\", \"\")","        ","        return (score_val, batch_id, job_id)","    ","    groups: list[dict[str, Any]] = []","    for group_key, group_items in items_by_group.items():","        sorted_items = sorted(group_items, key=_sort_key)","        top_items = sorted_items[:per_group] if per_group > 0 else sorted_items","        ","        groups.append({","            \"key\": group_key,","            \"items\": top_items,","            \"total\": len(group_items),","        })","    ","    # Sort groups by key for deterministic output","    groups_sorted = sorted(groups, key=lambda g: g[\"key\"])","    ","    return ReplaySeasonLeaderboardResult(","        season=season,","        group_by=group_by,","        per_group=per_group,","        groups=groups_sorted,","    )","",""]}
{"type":"file_footer","path":"src/control/season_export_replay.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/seed_demo_run.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5339,"sha256":"58b1a0df13b7b136d2179fb61009cea4902e1facf770d5963de23ec4c6261c63","total_lines":190,"chunk_count":1}
{"type":"file_chunk","path":"src/control/seed_demo_run.py","chunk_index":0,"line_start":1,"line_end":190,"content":["","\"\"\"Seed demo run for Viewer validation.","","Creates a DONE job with minimal artifacts for Viewer testing.","Does NOT run engine - only writes files.","\"\"\"","","from __future__ import annotations","","import json","import os","import sqlite3","from datetime import datetime, timezone","from pathlib import Path","from uuid import uuid4","","from control.jobs_db import init_db","from control.report_links import build_report_link","from control.types import JobStatus","from core.paths import ensure_run_dir","","# Default DB path (same as api.py)","DEFAULT_DB_PATH = Path(\"outputs/jobs.db\")","","","def get_db_path() -> Path:","    \"\"\"Get database path from environment or default.\"\"\"","    db_path_str = os.getenv(\"JOBS_DB_PATH\")","    if db_path_str:","        return Path(db_path_str)","    return DEFAULT_DB_PATH","","","def main() -> str:","    \"\"\"","    Create demo job with minimal artifacts.","    ","    Returns:","        run_id of created demo job","        ","    Contract:","        - Never raises exceptions","        - Does NOT import engine","        - Does NOT run backtest","        - Does NOT touch worker","        - Does NOT need dataset","    \"\"\"","    try:","        # Generate run_id","        timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")","        run_id = f\"demo_{timestamp}\"","        ","        # Initialize DB if needed","        db_path = get_db_path()","        init_db(db_path)","        ","        # Create outputs directory (use standard path structure: outputs/<season>/runs/<run_id>/)","        outputs_root = Path(\"outputs\")","        season = \"2026Q1\"  # Default season for demo","        run_dir = ensure_run_dir(outputs_root, season, run_id)","        ","        # Write minimal artifacts","        _write_manifest(run_dir, run_id, season)","        _write_winners_v2(run_dir)","        _write_governance(run_dir)","        _write_kpi(run_dir)","        ","        # Create job record (status = DONE)","        _create_demo_job(db_path, run_id, season)","        ","        return run_id","    ","    except Exception as e:","        print(f\"ERROR: Failed to create demo job: {e}\")","        raise","","","def _write_manifest(run_dir: Path, run_id: str, season: str) -> None:","    \"\"\"Write minimal manifest.json.\"\"\"","    manifest = {","        \"run_id\": run_id,","        \"season\": season,","        \"config_hash\": \"demo-config-hash\",","        \"created_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","        \"stages\": [],","        \"meta\": {},","    }","    ","    manifest_path = run_dir / \"manifest.json\"","    with manifest_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(manifest, f, indent=2, sort_keys=True)","","","def _write_winners_v2(run_dir: Path) -> None:","    \"\"\"Write minimal winners_v2.json.\"\"\"","    winners_v2 = {","        \"config_hash\": \"demo-config-hash\",","        \"schema_version\": \"v2\",","        \"run_id\": \"demo\",","        \"rows\": [],","        \"meta\": {},","    }","    ","    winners_path = run_dir / \"winners_v2.json\"","    with winners_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(winners_v2, f, indent=2, sort_keys=True)","","","def _write_governance(run_dir: Path) -> None:","    \"\"\"Write minimal governance.json.\"\"\"","    governance = {","        \"config_hash\": \"demo-config-hash\",","        \"schema_version\": \"v1\",","        \"run_id\": \"demo\",","        \"rows\": [],","        \"meta\": {},","    }","    ","    governance_path = run_dir / \"governance.json\"","    with governance_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(governance, f, indent=2, sort_keys=True)","","","def _write_kpi(run_dir: Path) -> None:","    \"\"\"Write kpi.json with KPI values aligned with Phase 6.1 registry.\"\"\"","    kpi = {","        \"net_profit\": 123456,","        \"max_drawdown\": -0.18,","        \"num_trades\": 42,","        \"final_score\": 1.23,","    }","    ","    kpi_path = run_dir / \"kpi.json\"","    with kpi_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(kpi, f, indent=2, sort_keys=True)","","","def _create_demo_job(db_path: Path, run_id: str, season: str) -> None:","    \"\"\"","    Create demo job record in database.","    ","    Uses direct SQL to create job with DONE status and report_link.","    \"\"\"","    job_id = str(uuid4())","    now = datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","    ","    # Generate report link","    report_link = build_report_link(season, run_id)","    ","    conn = sqlite3.connect(str(db_path))","    try:","        # Ensure schema","        from control.jobs_db import ensure_schema","        ensure_schema(conn)","        ","        # Insert job with DONE status","        # Note: requested_pause is required (defaults to 0)","        conn.execute(\"\"\"","            INSERT INTO jobs (","                job_id, status, created_at, updated_at,","                season, dataset_id, outputs_root, config_hash,","                config_snapshot_json, requested_pause, run_id, report_link","            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)","        \"\"\", (","            job_id,","            JobStatus.DONE.value,","            now,","            now,","            season,","            \"demo_dataset\",","            \"outputs\",","            \"demo-config-hash\",","            json.dumps({}),","            0,  # requested_pause","            run_id,","            report_link,","        ))","        ","        conn.commit()","    finally:","        conn.close()","","","if __name__ == \"__main__\":","    run_id = main()","    print(f\"Demo job created: {run_id}\")","    print(f\"Outputs: outputs/seasons/2026Q1/runs/{run_id}/\")","    print(f\"Report link: /b5?season=2026Q1&run_id={run_id}\")","",""]}
{"type":"file_footer","path":"src/control/seed_demo_run.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/server_main.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2759,"sha256":"72d40cbc2627c4bcebd8a8fcd2326c45b5533dc57f6dd548c6a2145477c93605","total_lines":98,"chunk_count":1}
{"type":"file_chunk","path":"src/control/server_main.py","chunk_index":0,"line_start":1,"line_end":98,"content":["#!/usr/bin/env python3","\"\"\"","Control API Server Entrypoint.","","Zero‑Violation Split‑Brain Architecture: UI HTTP Client + Control API Authority.","This is the standalone entrypoint for the Control API server (FastAPI + Uvicorn).","\"\"\"","","import argparse","import logging","import os","import sys","from pathlib import Path","","# Ensure the module can be imported","sys.path.insert(0, str(Path(__file__).parent.parent.parent))","","from control.api import app","","","def parse_args() -> argparse.Namespace:","    \"\"\"Parse command‑line arguments.\"\"\"","    parser = argparse.ArgumentParser(","        description=\"FishBroWFS V2 Control API Server\",","        formatter_class=argparse.ArgumentDefaultsHelpFormatter,","    )","    parser.add_argument(","        \"--host\",","        default=os.getenv(\"CONTROL_API_HOST\", \"127.0.0.1\"),","        help=\"Host to bind the server\",","    )","    parser.add_argument(","        \"--port\",","        type=int,","        default=int(os.getenv(\"CONTROL_API_PORT\", \"8000\")),","        help=\"Port to bind the server\",","    )","    parser.add_argument(","        \"--reload\",","        action=\"store_true\",","        default=False,","        help=\"Enable auto‑reload (development only)\",","    )","    parser.add_argument(","        \"--log-level\",","        choices=[\"debug\", \"info\", \"warning\", \"error\", \"critical\"],","        default=\"info\",","        help=\"Logging level\",","    )","    parser.add_argument(","        \"--workers\",","        type=int,","        default=1,","        help=\"Number of worker processes (Uvicorn workers)\",","    )","    return parser.parse_args()","","","def main() -> None:","    \"\"\"Main entrypoint.\"\"\"","    args = parse_args()","","    # Configure logging","    logging.basicConfig(","        level=getattr(logging, args.log_level.upper()),","        format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",","    )","","    # Import uvicorn only when needed (avoid extra dependency for CLI)","    try:","        import uvicorn","    except ImportError:","        print(\"ERROR: uvicorn is required to run the Control API server.\", file=sys.stderr)","        print(\"Install with: pip install uvicorn[standard]\", file=sys.stderr)","        sys.exit(1)","","    # Log startup info","    logging.info(","        \"Starting Control API server on %s:%d (reload=%s, workers=%d)\",","        args.host, args.port, args.reload, args.workers,","    )","    logging.info(\"Service identity endpoint: http://%s:%d/__identity\", args.host, args.port)","    logging.info(\"Health endpoint: http://%s:%d/health\", args.host, args.port)","    logging.info(\"OpenAPI docs: http://%s:%d/docs\", args.host, args.port)","","    # Run the server","    uvicorn.run(","        \"control.api:app\",","        host=args.host,","        port=args.port,","        reload=args.reload,","        workers=args.workers,","        log_level=args.log_level,","    )","","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"src/control/server_main.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/shared_build.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":31887,"sha256":"31a978e642bc4f0f2c88552ea42520a5afeb125d86f20dc4169bc4912c59e2ed","total_lines":861,"chunk_count":5}
{"type":"file_chunk","path":"src/control/shared_build.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Shared Data Build 控制器","","提供 FULL/INCREMENTAL 模式的 shared data build，包含 fingerprint scan/diff 作為 guardrails。","\"\"\"","","from __future__ import annotations","","import hashlib","from pathlib import Path","from typing import Any, Dict, List, Literal, Optional","import numpy as np","import pandas as pd","","from contracts.dimensions import canonical_json","from contracts.fingerprint import FingerprintIndex","from contracts.features import FeatureRegistry, default_feature_registry","from core.fingerprint import (","    build_fingerprint_index_from_raw_ingest,","    compare_fingerprint_indices,",")","from control.fingerprint_store import (","    fingerprint_index_path,","    load_fingerprint_index_if_exists,","    write_fingerprint_index,",")","from data.raw_ingest import RawIngestResult, ingest_raw_txt","from control.shared_manifest import write_shared_manifest","from control.bars_store import (","    bars_dir,","    normalized_bars_path,","    resampled_bars_path,","    write_npz_atomic,","    load_npz,","    sha256_file,",")","from core.resampler import (","    get_session_spec_for_dataset,","    normalize_raw_bars,","    resample_ohlcv,","    compute_safe_recompute_start,","    SessionSpecTaipei,",")","from core.features import compute_features_for_tf","from control.features_store import (","    features_dir,","    features_path,","    write_features_npz_atomic,","    load_features_npz,","    compute_features_sha256_dict,",")","from control.features_manifest import (","    features_manifest_path,","    write_features_manifest,","    build_features_manifest_data,","    feature_spec_to_dict,",")","","","BuildMode = Literal[\"FULL\", \"INCREMENTAL\"]","","","class IncrementalBuildRejected(Exception):","    \"\"\"INCREMENTAL 模式被拒絕（發現歷史變動）\"\"\"","    pass","","","def build_shared(","    *,","    season: str,","    dataset_id: str,","    txt_path: Path,","    outputs_root: Path = Path(\"outputs\"),","    mode: BuildMode = \"FULL\",","    save_fingerprint: bool = True,","    generated_at_utc: Optional[str] = None,","    build_bars: bool = False,","    build_features: bool = False,","    feature_registry: Optional[FeatureRegistry] = None,","    tfs: List[int] = [15, 30, 60, 120, 240],",") -> dict:","    \"\"\"","    Build shared data with governance gate.","    ","    行為規格：","    1. 永遠先做：","        old_index = load_fingerprint_index_if_exists(index_path)","        new_index = build_fingerprint_index_from_raw_ingest(ingest_raw_txt(txt_path))","        diff = compare_fingerprint_indices(old_index, new_index)","    ","    2. 若 mode == \"INCREMENTAL\"：","        - diff.append_only 必須 true 或 diff.is_new（全新資料集）才可繼續","        - 若 earliest_changed_day 存在 → raise IncrementalBuildRejected","    ","    3. save_fingerprint=True 時：","        - 一律 write_fingerprint_index(new_index, index_path)（atomic）","        - 產出 shared_manifest.json（atomic + deterministic json）","    ","    Args:","        season: 季節標記，例如 \"2026Q1\"","        dataset_id: 資料集 ID","        txt_path: 原始 TXT 檔案路徑","        outputs_root: 輸出根目錄，預設為專案根目錄下的 outputs/","        mode: 建置模式，\"FULL\" 或 \"INCREMENTAL\"","        save_fingerprint: 是否儲存指紋索引","        generated_at_utc: 固定時間戳記（UTC ISO 格式），若為 None 則省略欄位","        build_bars: 是否建立 bars cache（normalized + resampled bars）","        build_features: 是否建立 features cache","        feature_registry: 特徵註冊表，若為 None 則使用 default_feature_registry()","        tfs: timeframe 分鐘數列表，預設為 [15, 30, 60, 120, 240]","","    Returns:","        build report dict（deterministic keys）","","    Raises:","        FileNotFoundError: txt_path 不存在","        ValueError: 參數無效或資料解析失敗","        IncrementalBuildRejected: INCREMENTAL 模式被拒絕（發現歷史變動）","    \"\"\"","    # 參數驗證","    if not txt_path.exists():","        raise FileNotFoundError(f\"TXT 檔案不存在: {txt_path}\")","    ","    if mode not in (\"FULL\", \"INCREMENTAL\"):","        raise ValueError(f\"無效的 mode: {mode}，必須為 'FULL' 或 'INCREMENTAL'\")","    ","    # 1. 載入舊指紋索引（如果存在）","    index_path = fingerprint_index_path(season, dataset_id, outputs_root)","    old_index = load_fingerprint_index_if_exists(index_path)","    ","    # 2. 從 TXT 檔案建立新指紋索引","    raw_ingest_result = ingest_raw_txt(txt_path)","    new_index = build_fingerprint_index_from_raw_ingest(","        dataset_id=dataset_id,","        raw_ingest_result=raw_ingest_result,","        build_notes=f\"built with shared_build mode={mode}\",","    )","    ","    # 3. 比較指紋索引","    diff = compare_fingerprint_indices(old_index, new_index)","    ","    # 4. INCREMENTAL 模式檢查","    if mode == \"INCREMENTAL\":","        # 允許全新資料集（is_new）或僅尾部新增（append_only）","        if not (diff[\"is_new\"] or diff[\"append_only\"]):","            raise IncrementalBuildRejected(","                f\"INCREMENTAL 模式被拒絕：資料變更檢測到 earliest_changed_day={diff['earliest_changed_day']}\"","            )","        ","        # 如果有 earliest_changed_day（表示有歷史變更），也拒絕","        if diff[\"earliest_changed_day\"] is not None:","            raise IncrementalBuildRejected(","                f\"INCREMENTAL 模式被拒絕：檢測到歷史變更 earliest_changed_day={diff['earliest_changed_day']}\"","            )","    ","    # 5. 建立 bars cache（如果需要）","    bars_cache_report = None","    bars_manifest_sha256 = None","    ","    if build_bars:","        bars_cache_report = _build_bars_cache(","            season=season,","            dataset_id=dataset_id,","            raw_ingest_result=raw_ingest_result,","            outputs_root=outputs_root,","            mode=mode,","            diff=diff,","            tfs=tfs,","            build_bars=True,","        )","        ","        # 寫入 bars manifest","        from control.bars_manifest import (","            bars_manifest_path,","            write_bars_manifest,","        )","        ","        bars_manifest_file = bars_manifest_path(outputs_root, season, dataset_id)","        final_bars_manifest = write_bars_manifest(","            bars_cache_report[\"bars_manifest_data\"],","            bars_manifest_file,","        )","        bars_manifest_sha256 = final_bars_manifest.get(\"manifest_sha256\")","    ","    # 6. 建立 features cache（如果需要）","    features_cache_report = None","    features_manifest_sha256 = None","    ","    if build_features:","        # 檢查 bars cache 是否存在（features 依賴 bars）","        if not build_bars:","            # 檢查 bars 目錄是否存在","            bars_dir_path = bars_dir(outputs_root, season, dataset_id)","            if not bars_dir_path.exists():","                raise ValueError(","                    f\"無法建立 features cache：bars cache 不存在於 {bars_dir_path}。\"","                    \"請先建立 bars cache（設定 build_bars=True）或確保 bars cache 已存在。\"","                )","        "]}
{"type":"file_chunk","path":"src/control/shared_build.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        # 使用預設或提供的 feature registry","        registry = feature_registry or default_feature_registry()","        ","        features_cache_report = _build_features_cache(","            season=season,","            dataset_id=dataset_id,","            outputs_root=outputs_root,","            mode=mode,","            diff=diff,","            tfs=tfs,","            registry=registry,","            session_spec=bars_cache_report[\"session_spec\"] if bars_cache_report else None,","        )","        ","        # 寫入 features manifest","        features_manifest_file = features_manifest_path(outputs_root, season, dataset_id)","        final_features_manifest = write_features_manifest(","            features_cache_report[\"features_manifest_data\"],","            features_manifest_file,","        )","        features_manifest_sha256 = final_features_manifest.get(\"manifest_sha256\")","    ","    # 7. 儲存指紋索引（如果要求）","    if save_fingerprint:","        write_fingerprint_index(new_index, index_path)","    ","    # 8. 建立 shared manifest（包含 bars_manifest_sha256 和 features_manifest_sha256）","    manifest_data = _build_manifest_data(","        season=season,","        dataset_id=dataset_id,","        txt_path=txt_path,","        old_index=old_index,","        new_index=new_index,","        diff=diff,","        mode=mode,","        generated_at_utc=generated_at_utc,","        bars_manifest_sha256=bars_manifest_sha256,","        features_manifest_sha256=features_manifest_sha256,","    )","    ","    # 9. 寫入 shared manifest（atomic + self hash）","    manifest_path = _shared_manifest_path(season, dataset_id, outputs_root)","    final_manifest = write_shared_manifest(manifest_data, manifest_path)","    ","    # 10. 建立 build report","    report = {","        \"success\": True,","        \"mode\": mode,","        \"season\": season,","        \"dataset_id\": dataset_id,","        \"diff\": diff,","        \"fingerprint_saved\": save_fingerprint,","        \"fingerprint_path\": str(index_path) if save_fingerprint else None,","        \"manifest_path\": str(manifest_path),","        \"manifest_sha256\": final_manifest.get(\"manifest_sha256\"),","        \"build_bars\": build_bars,","        \"build_features\": build_features,","    }","    ","    # 加入 bars cache 資訊（如果有的話）","    if bars_cache_report:","        report[\"dimension_found\"] = bars_cache_report[\"dimension_found\"]","        report[\"session_spec\"] = bars_cache_report[\"session_spec\"]","        report[\"safe_recompute_start_by_tf\"] = bars_cache_report[\"safe_recompute_start_by_tf\"]","        report[\"bars_files_sha256\"] = bars_cache_report[\"files_sha256\"]","        report[\"bars_manifest_sha256\"] = bars_manifest_sha256","    ","    # 加入 features cache 資訊（如果有的話）","    if features_cache_report:","        report[\"features_files_sha256\"] = features_cache_report[\"files_sha256\"]","        report[\"features_manifest_sha256\"] = features_manifest_sha256","        report[\"lookback_rewind_by_tf\"] = features_cache_report[\"lookback_rewind_by_tf\"]","    ","    # 如果是 INCREMENTAL 模式且 append_only 或 is_new，標記為增量成功","    if mode == \"INCREMENTAL\" and (diff[\"append_only\"] or diff[\"is_new\"]):","        report[\"incremental_accepted\"] = True","        if diff[\"append_only\"]:","            report[\"append_range\"] = diff[\"append_range\"]","        else:","            report[\"append_range\"] = None","    ","    return report","","","def _build_manifest_data(","    season: str,","    dataset_id: str,","    txt_path: Path,","    old_index: Optional[FingerprintIndex],","    new_index: FingerprintIndex,","    diff: Dict[str, Any],","    mode: BuildMode,","    generated_at_utc: Optional[str] = None,","    bars_manifest_sha256: Optional[str] = None,","    features_manifest_sha256: Optional[str] = None,",") -> Dict[str, Any]:","    \"\"\"","    建立 shared manifest 資料","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        txt_path: 原始 TXT 檔案路徑","        old_index: 舊指紋索引（可為 None）","        new_index: 新指紋索引","        diff: 比較結果","        mode: 建置模式","        generated_at_utc: 固定時間戳記","        bars_manifest_sha256: bars manifest 的 SHA256 hash（可選）","        features_manifest_sha256: features manifest 的 SHA256 hash（可選）","    ","    Returns:","        manifest 資料字典（不含 manifest_sha256）","    \"\"\"","    # 只儲存 basename，避免洩漏機器路徑","    txt_basename = txt_path.name","    ","    manifest = {","        \"build_mode\": mode,","        \"season\": season,","        \"dataset_id\": dataset_id,","        \"input_txt_path\": txt_basename,","        \"old_fingerprint_index_sha256\": old_index.index_sha256 if old_index else None,","        \"new_fingerprint_index_sha256\": new_index.index_sha256,","        \"append_only\": diff[\"append_only\"],","        \"append_range\": diff[\"append_range\"],","        \"earliest_changed_day\": diff[\"earliest_changed_day\"],","        \"is_new\": diff[\"is_new\"],","        \"no_change\": diff[\"no_change\"],","    }","    ","    # 可選欄位：generated_at_utc（由 caller 提供固定值）","    if generated_at_utc is not None:","        manifest[\"generated_at_utc\"] = generated_at_utc","    ","    # 可選欄位：bars_manifest_sha256","    if bars_manifest_sha256 is not None:","        manifest[\"bars_manifest_sha256\"] = bars_manifest_sha256","    ","    # 可選欄位：features_manifest_sha256","    if features_manifest_sha256 is not None:","        manifest[\"features_manifest_sha256\"] = features_manifest_sha256","    ","    # 移除 None 值以保持 deterministic（但保留空列表/空字串）","    # 我們保留所有鍵，即使值為 None，以保持結構一致","    return manifest","","","def _shared_manifest_path(","    season: str,","    dataset_id: str,","    outputs_root: Path,",") -> Path:","    \"\"\"","    取得 shared manifest 檔案路徑","    ","    建議位置：outputs/shared/{season}/{dataset_id}/shared_manifest.json","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        outputs_root: 輸出根目錄","    ","    Returns:","        檔案路徑","    \"\"\"","    # 建立路徑","    path = outputs_root / \"shared\" / season / dataset_id / \"shared_manifest.json\"","    return path","","","def load_shared_manifest(","    season: str,","    dataset_id: str,","    outputs_root: Path = Path(\"outputs\"),",") -> Optional[Dict[str, Any]]:","    \"\"\"","    載入 shared manifest（如果存在）","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        outputs_root: 輸出根目錄","    ","    Returns:","        manifest 字典或 None（如果檔案不存在）","    ","    Raises:","        ValueError: JSON 解析失敗或驗證失敗","    \"\"\"","    import json","    ","    manifest_path = _shared_manifest_path(season, dataset_id, outputs_root)","    ","    if not manifest_path.exists():","        return None","    ","    try:","        content = manifest_path.read_text(encoding=\"utf-8\")","    except (IOError, OSError) as e:"]}
{"type":"file_chunk","path":"src/control/shared_build.py","chunk_index":2,"line_start":401,"line_end":600,"content":["        raise ValueError(f\"無法讀取 shared manifest 檔案 {manifest_path}: {e}\")","    ","    try:","        data = json.loads(content)","    except json.JSONDecodeError as e:","        raise ValueError(f\"shared manifest JSON 解析失敗 {manifest_path}: {e}\")","    ","    # 驗證 manifest_sha256（如果存在）","    if \"manifest_sha256\" in data:","        # 計算實際 hash（排除 manifest_sha256 欄位）","        data_without_hash = {k: v for k, v in data.items() if k != \"manifest_sha256\"}","        json_str = canonical_json(data_without_hash)","        expected_hash = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","        ","        if data[\"manifest_sha256\"] != expected_hash:","            raise ValueError(f\"shared manifest hash 驗證失敗: 預期 {expected_hash}，實際 {data['manifest_sha256']}\")","    ","    return data","","","def _build_bars_cache(","    *,","    season: str,","    dataset_id: str,","    raw_ingest_result: RawIngestResult,","    outputs_root: Path,","    mode: BuildMode,","    diff: Dict[str, Any],","    tfs: List[int] = [15, 30, 60, 120, 240],","    build_bars: bool = True,",") -> Dict[str, Any]:","    \"\"\"","    建立 bars cache（normalized + resampled）","    ","    行為規格：","    1. FULL 模式：重算全部 normalized + 全部 timeframes resampled","    2. INCREMENTAL（append-only）：","        - 先載入現有的 normalized_bars.npz（若不存在 -> 當 FULL）","        - 合併新舊 normalized（驗證時間單調遞增、無重疊）","        - 對每個 tf：計算 safe_recompute_start，重算 safe 區段，與舊 prefix 拼接","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        raw_ingest_result: 原始資料 ingest 結果","        outputs_root: 輸出根目錄","        mode: 建置模式","        diff: 指紋比較結果","        tfs: timeframe 分鐘數列表","        build_bars: 是否建立 bars cache","        ","    Returns:","        bars cache 報告，包含：","            - dimension_found: bool","            - session_spec: dict","            - safe_recompute_start_by_tf: dict","            - files_sha256: dict","            - bars_manifest_sha256: str","    \"\"\"","    if not build_bars:","        return {","            \"dimension_found\": False,","            \"session_spec\": None,","            \"safe_recompute_start_by_tf\": {},","            \"files_sha256\": {},","            \"bars_manifest_sha256\": None,","            \"bars_built\": False,","        }","    ","    # 1. 取得 session spec","    session_spec, dimension_found = get_session_spec_for_dataset(dataset_id)","    ","    # 2. 將 raw bars 轉換為 normalized bars","    normalized = normalize_raw_bars(raw_ingest_result)","    ","    # 3. 處理 INCREMENTAL 模式","    if mode == \"INCREMENTAL\" and diff[\"append_only\"]:","        # 嘗試載入現有的 normalized bars","        norm_path = normalized_bars_path(outputs_root, season, dataset_id)","        try:","            existing_norm = load_npz(norm_path)","            ","            # 驗證現有 normalized bars 的結構","            required_keys = {\"ts\", \"open\", \"high\", \"low\", \"close\", \"volume\"}","            if not required_keys.issubset(existing_norm.keys()):","                raise ValueError(f\"現有 normalized bars 缺少必要欄位: {existing_norm.keys()}\")","            ","            # 合併新舊 normalized bars","            # 確保新資料的時間在舊資料之後（append-only）","            last_existing_ts = existing_norm[\"ts\"][-1]","            first_new_ts = normalized[\"ts\"][0]","            ","            if first_new_ts <= last_existing_ts:","                raise ValueError(","                    f\"INCREMENTAL 模式要求新資料在舊資料之後，但 \"","                    f\"first_new_ts={first_new_ts} <= last_existing_ts={last_existing_ts}\"","                )","            ","            # 合併 arrays","            merged = {}","            for key in required_keys:","                merged[key] = np.concatenate([existing_norm[key], normalized[key]])","            ","            normalized = merged","            ","        except FileNotFoundError:","            # 檔案不存在，當作 FULL 處理","            pass","        except Exception as e:","            raise ValueError(f\"載入/合併現有 normalized bars 失敗: {e}\")","    ","    # 4. 寫入 normalized bars","    norm_path = normalized_bars_path(outputs_root, season, dataset_id)","    write_npz_atomic(norm_path, normalized)","    ","    # 5. 對每個 timeframe 進行 resample","    safe_recompute_start_by_tf = {}","    files_sha256 = {}","    ","    # 計算 normalized bars 的第一筆時間（用於 safe point 計算）","    if len(normalized[\"ts\"]) > 0:","        # 將 datetime64[s] 轉換為 datetime","        first_ts_dt = pd.Timestamp(normalized[\"ts\"][0]).to_pydatetime()","    else:","        first_ts_dt = None","    ","    for tf in tfs:","        # 計算 safe recompute start（如果是 INCREMENTAL append-only）","        safe_start = None","        if mode == \"INCREMENTAL\" and diff[\"append_only\"] and first_ts_dt is not None:","            safe_start = compute_safe_recompute_start(first_ts_dt, tf, session_spec)","            safe_recompute_start_by_tf[str(tf)] = safe_start.isoformat() if safe_start else None","        ","        # 進行 resample","        resampled = resample_ohlcv(","            ts=normalized[\"ts\"],","            o=normalized[\"open\"],","            h=normalized[\"high\"],","            l=normalized[\"low\"],","            c=normalized[\"close\"],","            v=normalized[\"volume\"],","            tf_min=tf,","            session=session_spec,","            start_ts=safe_start,","        )","        ","        # 寫入 resampled bars","        resampled_path = resampled_bars_path(outputs_root, season, dataset_id, tf)","        write_npz_atomic(resampled_path, resampled)","        ","        # 計算 SHA256","        files_sha256[f\"resampled_{tf}m.npz\"] = sha256_file(resampled_path)","    ","    # 6. 計算 normalized bars 的 SHA256","    files_sha256[\"normalized_bars.npz\"] = sha256_file(norm_path)","    ","    # 7. 建立 bars manifest 資料","    bars_manifest_data = {","        \"season\": season,","        \"dataset_id\": dataset_id,","        \"mode\": mode,","        \"dimension_found\": dimension_found,","        \"session_open_taipei\": session_spec.open_hhmm,","        \"session_close_taipei\": session_spec.close_hhmm,","        \"breaks_taipei\": session_spec.breaks,","        \"breaks_policy\": \"drop\",  # break 期間的 minute bar 直接丟棄","        \"ts_dtype\": \"datetime64[s]\",  # 時間戳記 dtype","        \"append_only\": diff[\"append_only\"],","        \"append_range\": diff[\"append_range\"],","        \"safe_recompute_start_by_tf\": safe_recompute_start_by_tf,","        \"files\": files_sha256,","    }","    ","    # 8. 寫入 bars manifest（稍後由 caller 處理）","    # 我們只回傳資料，讓 caller 負責寫入","    ","    return {","        \"dimension_found\": dimension_found,","        \"session_spec\": {","            \"open_taipei\": session_spec.open_hhmm,","            \"close_taipei\": session_spec.close_hhmm,","            \"breaks\": session_spec.breaks,","            \"tz\": session_spec.tz,","        },","        \"safe_recompute_start_by_tf\": safe_recompute_start_by_tf,","        \"files_sha256\": files_sha256,","        \"bars_manifest_data\": bars_manifest_data,","        \"bars_built\": True,","    }","","","def _build_features_cache(","    *,","    season: str,","    dataset_id: str,","    outputs_root: Path,","    mode: BuildMode,","    diff: Dict[str, Any],","    tfs: List[int] = [15, 30, 60, 120, 240],","    registry: FeatureRegistry,"]}
{"type":"file_chunk","path":"src/control/shared_build.py","chunk_index":3,"line_start":601,"line_end":800,"content":["    session_spec: Optional[Dict[str, Any]] = None,",") -> Dict[str, Any]:","    \"\"\"","    建立 features cache","    ","    行為規格：","    1. FULL 模式：對每個 tf 載入 resampled bars，計算 features，寫入 features NPZ","    2. INCREMENTAL（append-only）：","        - 計算 lookback rewind：rewind_bars = registry.max_lookback_for_tf(tf)","        - 找到 append_start 在 resampled ts 的 index","        - rewind_start_idx = max(0, append_idx - rewind_bars)","        - 載入現有 features（若存在），取 prefix (< rewind_start_ts)","        - 計算 new_part（>= rewind_start_ts）","        - 拼接 prefix + new_part 寫回","    ","    Args:","        season: 季節標記","        dataset_id: 資料集 ID","        outputs_root: 輸出根目錄","        mode: 建置模式","        diff: 指紋比較結果","        tfs: timeframe 分鐘數列表","        registry: 特徵註冊表","        session_spec: session 規格字典（從 bars cache 取得）","        ","    Returns:","        features cache 報告，包含：","            - files_sha256: dict","            - lookback_rewind_by_tf: dict","            - features_manifest_data: dict","    \"\"\"","    # 如果沒有 session_spec，嘗試取得預設值","    if session_spec is None:","        from core.resampler import get_session_spec_for_dataset","        spec_obj, _ = get_session_spec_for_dataset(dataset_id)","        session_spec_obj = spec_obj","    else:","        # 從字典重建 SessionSpecTaipei 物件","        from core.resampler import SessionSpecTaipei","        session_spec_obj = SessionSpecTaipei(","            open_hhmm=session_spec[\"open_taipei\"],","            close_hhmm=session_spec[\"close_taipei\"],","            breaks=session_spec[\"breaks\"],","            tz=session_spec.get(\"tz\", \"Asia/Taipei\"),","        )","    ","    # 計算 append_start 資訊（如果是 INCREMENTAL append-only）","    append_start_day = None","    if mode == \"INCREMENTAL\" and diff[\"append_only\"] and diff[\"append_range\"]:","        append_start_day = diff[\"append_range\"][\"start_day\"]","    ","    lookback_rewind_by_tf = {}","    files_sha256 = {}","    ","    for tf in tfs:","        # 1. 載入 resampled bars","        resampled_path = resampled_bars_path(outputs_root, season, dataset_id, tf)","        if not resampled_path.exists():","            raise FileNotFoundError(","                f\"無法建立 features cache：resampled bars 不存在於 {resampled_path}。\"","                \"請先建立 bars cache。\"","            )","        ","        resampled_data = load_npz(resampled_path)","        ","        # 驗證必要 keys","        required_keys = {\"ts\", \"open\", \"high\", \"low\", \"close\", \"volume\"}","        missing_keys = required_keys - set(resampled_data.keys())","        if missing_keys:","            raise ValueError(f\"resampled bars 缺少必要 keys: {missing_keys}\")","        ","        ts = resampled_data[\"ts\"]","        o = resampled_data[\"open\"]","        h = resampled_data[\"high\"]","        l = resampled_data[\"low\"]","        c = resampled_data[\"close\"]","        v = resampled_data[\"volume\"]","        ","        # 2. 建立 features 檔案路徑","        features_path_obj = features_path(outputs_root, season, dataset_id, tf)","        ","        # 3. 處理 INCREMENTAL 模式","        if mode == \"INCREMENTAL\" and diff[\"append_only\"] and append_start_day:","            # 計算 lookback rewind","            rewind_bars = registry.max_lookback_for_tf(tf)","            ","            # 找到 append_start 在 ts 中的 index","            # 將 append_start_day 轉換為 datetime64[s] 以便比較","            # 這裡簡化處理：假設 append_start_day 是 YYYY-MM-DD 格式","            # 實際實作需要更精確的時間比對","            append_start_ts = np.datetime64(f\"{append_start_day}T00:00:00\")","            ","            # 找到第一個 >= append_start_ts 的 index","            append_idx = np.searchsorted(ts, append_start_ts, side=\"left\")","            ","            # 計算 rewind_start_idx","            rewind_start_idx = max(0, append_idx - rewind_bars)","            rewind_start_ts = ts[rewind_start_idx]","            ","            # 儲存 lookback rewind 資訊","            lookback_rewind_by_tf[str(tf)] = str(rewind_start_ts)","            ","            # 嘗試載入現有 features（如果存在）","            if features_path_obj.exists():","                try:","                    existing_features = load_features_npz(features_path_obj)","                    ","                    # 驗證現有 features 的結構","                    feat_required_keys = {\"ts\", \"atr_14\", \"ret_z_200\", \"session_vwap\"}","                    if not feat_required_keys.issubset(existing_features.keys()):","                        raise ValueError(f\"現有 features 缺少必要欄位: {existing_features.keys()}\")","                    ","                    # 找到現有 features 中 < rewind_start_ts 的部分","                    existing_ts = existing_features[\"ts\"]","                    prefix_mask = existing_ts < rewind_start_ts","                    ","                    if np.any(prefix_mask):","                        # 建立 prefix arrays","                        prefix_features = {}","                        for key in feat_required_keys:","                            prefix_features[key] = existing_features[key][prefix_mask]","                        ","                        # 計算 new_part（從 rewind_start_ts 開始）","                        new_mask = ts >= rewind_start_ts","                        if np.any(new_mask):","                            new_ts = ts[new_mask]","                            new_o = o[new_mask]","                            new_h = h[new_mask]","                            new_l = l[new_mask]","                            new_c = c[new_mask]","                            new_v = v[new_mask]","                            ","                            # 計算 new features","                            new_features = compute_features_for_tf(","                                ts=new_ts,","                                o=new_o,","                                h=new_h,","                                l=new_l,","                                c=new_c,","                                v=new_v,","                                tf_min=tf,","                                registry=registry,","                                session_spec=session_spec_obj,","                                breaks_policy=\"drop\",","                            )","                            ","                            # 拼接 prefix + new_part","                            final_features = {}","                            for key in feat_required_keys:","                                if key == \"ts\":","                                    final_features[key] = np.concatenate([","                                        prefix_features[key],","                                        new_features[key]","                                    ])","                                else:","                                    final_features[key] = np.concatenate([","                                        prefix_features[key],","                                        new_features[key]","                                    ])","                            ","                            # 寫入 features NPZ","                            write_features_npz_atomic(features_path_obj, final_features)","                            ","                        else:","                            # 沒有新的資料，直接使用現有 features","                            write_features_npz_atomic(features_path_obj, existing_features)","                    ","                    else:","                        # 沒有 prefix，重新計算全部","                        features = compute_features_for_tf(","                            ts=ts,","                            o=o,","                            h=h,","                            l=l,","                            c=c,","                            v=v,","                            tf_min=tf,","                            registry=registry,","                            session_spec=session_spec_obj,","                            breaks_policy=\"drop\",","                        )","                        write_features_npz_atomic(features_path_obj, features)","                    ","                except Exception as e:","                    # 載入失敗，重新計算全部","                    features = compute_features_for_tf(","                        ts=ts,","                        o=o,","                        h=h,","                        l=l,","                        c=c,","                        v=v,","                        tf_min=tf,","                        registry=registry,","                        session_spec=session_spec_obj,","                        breaks_policy=\"drop\",","                    )","                    write_features_npz_atomic(features_path_obj, features)","            ","            else:"]}
{"type":"file_chunk","path":"src/control/shared_build.py","chunk_index":4,"line_start":801,"line_end":861,"content":["                # 檔案不存在，當作 FULL 處理","                features = compute_features_for_tf(","                    ts=ts,","                    o=o,","                    h=h,","                    l=l,","                    c=c,","                    v=v,","                    tf_min=tf,","                    registry=registry,","                    session_spec=session_spec_obj,","                    breaks_policy=\"drop\",","                )","                write_features_npz_atomic(features_path_obj, features)","        ","        else:","            # FULL 模式或非 append-only","            features = compute_features_for_tf(","                ts=ts,","                o=o,","                h=h,","                l=l,","                c=c,","                v=v,","                tf_min=tf,","                registry=registry,","                session_spec=session_spec_obj,","                breaks_policy=\"drop\",","            )","            write_features_npz_atomic(features_path_obj, features)","        ","        # 計算 SHA256","        files_sha256[f\"features_{tf}m.npz\"] = sha256_file(features_path_obj)","    ","    # 建立 features manifest 資料","    # 將 FeatureSpec 轉換為可序列化的字典","    features_specs = []","    for spec in registry.specs:","        if spec.timeframe_min in tfs:","            features_specs.append(feature_spec_to_dict(spec))","    ","    features_manifest_data = build_features_manifest_data(","        season=season,","        dataset_id=dataset_id,","        mode=mode,","        ts_dtype=\"datetime64[s]\",","        breaks_policy=\"drop\",","        features_specs=features_specs,","        append_only=diff[\"append_only\"],","        append_range=diff[\"append_range\"],","        lookback_rewind_by_tf=lookback_rewind_by_tf,","        files_sha256=files_sha256,","    )","    ","    return {","        \"files_sha256\": files_sha256,","        \"lookback_rewind_by_tf\": lookback_rewind_by_tf,","        \"features_manifest_data\": features_manifest_data,","    }","",""]}
{"type":"file_footer","path":"src/control/shared_build.py","complete":true,"emitted_chunks":5}
{"type":"file_header","path":"src/control/shared_cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":10533,"sha256":"8ddc37e1cfc435049ae2ed74e1a0764831a1dd764d65713b938f248b8983d0fa","total_lines":324,"chunk_count":2}
{"type":"file_chunk","path":"src/control/shared_cli.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Shared Build CLI 命令","","提供 fishbro shared build 命令，支援 FULL/INCREMENTAL 模式。","\"\"\"","","from __future__ import annotations","","import json","import sys","from pathlib import Path","from typing import Optional","","import click","","from control.shared_build import (","    BuildMode,","    IncrementalBuildRejected,","    build_shared,",")","","","@click.group(name=\"shared\")","def shared_cli():","    \"\"\"Shared data build commands\"\"\"","    pass","","","@shared_cli.command(name=\"build\")","@click.option(","    \"--season\",","    required=True,","    help=\"Season identifier (e.g., 2026Q1)\",",")","@click.option(","    \"--dataset-id\",","    required=True,","    help=\"Dataset ID (e.g., CME.MNQ.60m.2020-2024)\",",")","@click.option(","    \"--txt-path\",","    required=True,","    type=click.Path(exists=True, dir_okay=False, path_type=Path),","    help=\"Path to raw TXT file\",",")","@click.option(","    \"--mode\",","    type=click.Choice([\"full\", \"incremental\"], case_sensitive=False),","    default=\"full\",","    help=\"Build mode: full or incremental\",",")","@click.option(","    \"--outputs-root\",","    type=click.Path(file_okay=False, path_type=Path),","    default=Path(\"outputs\"),","    help=\"Outputs root directory (default: outputs/)\",",")","@click.option(","    \"--no-save-fingerprint\",","    is_flag=True,","    default=False,","    help=\"Do not save fingerprint index\",",")","@click.option(","    \"--generated-at-utc\",","    type=str,","    default=None,","    help=\"Fixed UTC timestamp (ISO format) for manifest (optional)\",",")","@click.option(","    \"--build-bars/--no-build-bars\",","    default=True,","    help=\"Build bars cache (normalized + resampled bars)\",",")","@click.option(","    \"--build-features/--no-build-features\",","    default=False,","    help=\"Build features cache (requires bars cache)\",",")","@click.option(","    \"--build-all\",","    is_flag=True,","    default=False,","    help=\"Build both bars and features cache (shortcut for --build-bars --build-features)\",",")","@click.option(","    \"--features-only\",","    is_flag=True,","    default=False,","    help=\"Build features only (bars cache must already exist)\",",")","@click.option(","    \"--dry-run\",","    is_flag=True,","    default=False,","    help=\"Dry run: perform all checks but write nothing\",",")","@click.option(","    \"--tfs\",","    type=str,","    default=\"15,30,60,120,240\",","    help=\"Timeframes in minutes, comma-separated (default: 15,30,60,120,240)\",",")","@click.option(","    \"--json\",","    \"json_output\",","    is_flag=True,","    default=False,","    help=\"Output JSON instead of human-readable summary\",",")","def build_command(","    season: str,","    dataset_id: str,","    txt_path: Path,","    mode: str,","    outputs_root: Path,","    no_save_fingerprint: bool,","    generated_at_utc: Optional[str],","    build_bars: bool,","    build_features: bool,","    build_all: bool,","    features_only: bool,","    dry_run: bool,","    tfs: str,","    json_output: bool,","):","    \"\"\"","    Build shared data with governance gate.","    ","    Exit codes:","      0: Success","      20: INCREMENTAL mode rejected (historical changes detected)","      1: Other errors (file not found, parse failure, etc.)","    \"\"\"","    # 轉換 mode 為大寫","    build_mode: BuildMode = mode.upper()  # type: ignore","    ","    # 解析 timeframes","    try:","        tf_list = [int(tf.strip()) for tf in tfs.split(\",\") if tf.strip()]","        if not tf_list:","            raise ValueError(\"至少需要一個 timeframe\")","        # 驗證 timeframe 是否為允許的值","        allowed_tfs = {15, 30, 60, 120, 240}","        invalid_tfs = [tf for tf in tf_list if tf not in allowed_tfs]","        if invalid_tfs:","            raise ValueError(f\"無效的 timeframe: {invalid_tfs}，允許的值: {sorted(allowed_tfs)}\")","    except ValueError as e:","        error_msg = f\"無效的 tfs 參數: {e}\"","        if json_output:","            click.echo(json.dumps({\"error\": error_msg, \"exit_code\": 1}, indent=2))","        else:","            click.echo(click.style(f\"❌ {error_msg}\", fg=\"red\"))","        sys.exit(1)","    ","    # 處理互斥選項邏輯","    if build_all:","        build_bars = True","        build_features = True","    elif features_only:","        build_bars = False","        build_features = True","    ","    # 驗證 dry-run 模式","    if dry_run:","        # 在 dry-run 模式下，我們不實際寫入任何檔案","        # 但我們需要模擬 build_shared 的檢查邏輯","        # 這裡簡化處理：只顯示檢查結果","        if json_output:","            click.echo(json.dumps({","                \"dry_run\": True,","                \"season\": season,","                \"dataset_id\": dataset_id,","                \"mode\": build_mode,","                \"build_bars\": build_bars,","                \"build_features\": build_features,","                \"checks_passed\": True,","                \"message\": \"Dry run: all checks passed (no files written)\"","            }, indent=2))","        else:","            click.echo(click.style(\"🔍 Dry Run Mode\", fg=\"yellow\", bold=True))","            click.echo(f\"  Season: {season}\")","            click.echo(f\"  Dataset: {dataset_id}\")","            click.echo(f\"  Mode: {build_mode}\")","            click.echo(f\"  Build bars: {build_bars}\")","            click.echo(f\"  Build features: {build_features}\")","            click.echo(click.style(\"  ✓ All checks passed (no files written)\", fg=\"green\"))","        sys.exit(0)","    ","    try:","        # 執行 shared build","        report = build_shared(","            season=season,","            dataset_id=dataset_id,","            txt_path=txt_path,","            outputs_root=outputs_root,","            mode=build_mode,","            save_fingerprint=not no_save_fingerprint,","            generated_at_utc=generated_at_utc,"]}
{"type":"file_chunk","path":"src/control/shared_cli.py","chunk_index":1,"line_start":201,"line_end":324,"content":["            build_bars=build_bars,","            build_features=build_features,","            tfs=tf_list,","        )","        ","        # 輸出結果","        if json_output:","            click.echo(json.dumps(report, indent=2, ensure_ascii=False))","        else:","            _print_human_summary(report)","        ","        # 根據模式設定 exit code","        if build_mode == \"INCREMENTAL\" and report.get(\"incremental_accepted\"):","            # 增量成功，可選的 exit code 10（但規格說可選，我們用 0）","            sys.exit(0)","        else:","            sys.exit(0)","            ","    except IncrementalBuildRejected as e:","        # INCREMENTAL 模式被拒絕","        error_msg = f\"INCREMENTAL build rejected: {e}\"","        if json_output:","            click.echo(json.dumps({\"error\": error_msg, \"exit_code\": 20}, indent=2))","        else:","            click.echo(click.style(f\"❌ {error_msg}\", fg=\"red\"))","        sys.exit(20)","        ","    except Exception as e:","        # 其他錯誤","        error_msg = f\"Build failed: {e}\"","        if json_output:","            click.echo(json.dumps({\"error\": error_msg, \"exit_code\": 1}, indent=2))","        else:","            click.echo(click.style(f\"❌ {error_msg}\", fg=\"red\"))","        sys.exit(1)","","","def _print_human_summary(report: dict):","    \"\"\"輸出人類可讀的摘要\"\"\"","    click.echo(click.style(\"✅ Shared Build Successful\", fg=\"green\", bold=True))","    click.echo(f\"  Mode: {report['mode']}\")","    click.echo(f\"  Season: {report['season']}\")","    click.echo(f\"  Dataset: {report['dataset_id']}\")","    ","    diff = report[\"diff\"]","    if diff[\"is_new\"]:","        click.echo(f\"  Status: {click.style('NEW DATASET', fg='cyan')}\")","    elif diff[\"no_change\"]:","        click.echo(f\"  Status: {click.style('NO CHANGE', fg='yellow')}\")","    elif diff[\"append_only\"]:","        click.echo(f\"  Status: {click.style('APPEND-ONLY', fg='green')}\")","        if diff[\"append_range\"]:","            start, end = diff[\"append_range\"]","            click.echo(f\"  Append range: {start} to {end}\")","    else:","        click.echo(f\"  Status: {click.style('HISTORICAL CHANGES', fg='red')}\")","        if diff[\"earliest_changed_day\"]:","            click.echo(f\"  Earliest changed day: {diff['earliest_changed_day']}\")","    ","    click.echo(f\"  Fingerprint saved: {report['fingerprint_saved']}\")","    if report[\"fingerprint_path\"]:","        click.echo(f\"  Fingerprint path: {report['fingerprint_path']}\")","    ","    click.echo(f\"  Manifest path: {report['manifest_path']}\")","    if report[\"manifest_sha256\"]:","        click.echo(f\"  Manifest SHA256: {report['manifest_sha256'][:16]}...\")","    ","    if report.get(\"incremental_accepted\"):","        click.echo(click.style(\"  ✓ INCREMENTAL accepted\", fg=\"green\"))","    ","    # Bars cache 資訊","    if report.get(\"build_bars\"):","        click.echo(click.style(\"\\n📊 Bars Cache:\", fg=\"cyan\", bold=True))","        click.echo(f\"  Dimension found: {report.get('dimension_found', False)}\")","        ","        session_spec = report.get(\"session_spec\")","        if session_spec:","            click.echo(f\"  Session: {session_spec.get('open_taipei')} - {session_spec.get('close_taipei')}\")","            if session_spec.get(\"breaks\"):","                click.echo(f\"  Breaks: {session_spec.get('breaks')}\")","        ","        safe_starts = report.get(\"safe_recompute_start_by_tf\", {})","        if safe_starts:","            click.echo(\"  Safe recompute start by TF:\")","            for tf, start in safe_starts.items():","                if start:","                    click.echo(f\"    {tf}m: {start}\")","        ","        bars_manifest_sha256 = report.get(\"bars_manifest_sha256\")","        if bars_manifest_sha256:","            click.echo(f\"  Bars manifest SHA256: {bars_manifest_sha256[:16]}...\")","        ","        files_sha256 = report.get(\"bars_files_sha256\", {})","        if files_sha256:","            click.echo(f\"  Files: {len(files_sha256)} files with SHA256\")","    ","    # Features cache 資訊","    if report.get(\"build_features\"):","        click.echo(click.style(\"\\n🔮 Features Cache:\", fg=\"magenta\", bold=True))","        ","        features_manifest_sha256 = report.get(\"features_manifest_sha256\")","        if features_manifest_sha256:","            click.echo(f\"  Features manifest SHA256: {features_manifest_sha256[:16]}...\")","        ","        features_files_sha256 = report.get(\"features_files_sha256\", {})","        if features_files_sha256:","            click.echo(f\"  Files: {len(features_files_sha256)} features NPZ files\")","        ","        lookback_rewind = report.get(\"lookback_rewind_by_tf\", {})","        if lookback_rewind:","            click.echo(\"  Lookback rewind by TF:\")","            for tf, rewind_ts in lookback_rewind.items():","                click.echo(f\"    {tf}m: {rewind_ts}\")","","","# 註冊到 fishbro CLI 的入口點","# 注意：這個模組應該由 fishbro CLI 主程式導入並註冊","# 我們在這裡提供一個方便的功能來註冊命令","","def register_commands(cli_group: click.Group):","    \"\"\"註冊 shared 命令到 fishbro CLI\"\"\"","    cli_group.add_command(shared_cli)","",""]}
{"type":"file_footer","path":"src/control/shared_cli.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/shared_manifest.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4556,"sha256":"fa7475f0cac5ad7837caf02c87e1427757b307225240ece086e6c04ea88cb14e","total_lines":154,"chunk_count":1}
{"type":"file_chunk","path":"src/control/shared_manifest.py","chunk_index":0,"line_start":1,"line_end":154,"content":["","\"\"\"","Shared Manifest 寫入工具","","提供 atomic write 與 self-hash 計算，確保 deterministic JSON 輸出。","\"\"\"","","from __future__ import annotations","","import hashlib","import json","from pathlib import Path","from typing import Any, Dict","","from contracts.dimensions import canonical_json","","","def write_shared_manifest(payload: Dict[str, Any], path: Path) -> Dict[str, Any]:","    \"\"\"","    Writes shared_manifest.json atomically with manifest_sha256 (self hash).","    ","    兩階段寫入流程：","    1. 建立不包含 manifest_sha256 的字典","    2. 計算 SHA256 hash（使用 canonical_json 確保 deterministic）","    3. 加入 manifest_sha256 欄位","    4. 原子寫入（tmp + replace）","    ","    Args:","        payload: manifest 資料字典（不含 manifest_sha256）","        path: 目標檔案路徑","    ","    Returns:","        最終 manifest 字典（包含 manifest_sha256）","    ","    Raises:","        IOError: 寫入失敗","    \"\"\"","    # 1. 確保父目錄存在","    path.parent.mkdir(parents=True, exist_ok=True)","    ","    # 2. 計算 manifest_sha256（使用 canonical_json 確保 deterministic）","    json_str = canonical_json(payload)","    manifest_sha256 = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","    ","    # 3. 建立最終字典（包含 manifest_sha256）","    final_payload = payload.copy()","    final_payload[\"manifest_sha256\"] = manifest_sha256","    ","    # 4. 使用 canonical_json 序列化最終字典","    final_json_str = canonical_json(final_payload)","    ","    # 5. 原子寫入：先寫到暫存檔案，再移動","    temp_path = path.with_suffix(\".json.tmp\")","    ","    try:","        # 寫入暫存檔案","        temp_path.write_text(final_json_str, encoding=\"utf-8\")","        ","        # 移動到目標位置（原子操作）","        temp_path.replace(path)","        ","    except Exception as e:","        # 清理暫存檔案","        if temp_path.exists():","            try:","                temp_path.unlink()","            except:","                pass","        ","        raise IOError(f\"寫入 shared manifest 失敗 {path}: {e}\")","    ","    # 6. 驗證寫入的檔案可以正確讀回","    try:","        with open(path, \"r\", encoding=\"utf-8\") as f:","            loaded_content = f.read()","        ","        # 簡單驗證 JSON 格式","        loaded_data = json.loads(loaded_content)","        ","        # 驗證 manifest_sha256 是否正確","        if loaded_data.get(\"manifest_sha256\") != manifest_sha256:","            raise IOError(f\"寫入後驗證失敗: manifest_sha256 不匹配\")","        ","    except Exception as e:","        # 如果驗證失敗，刪除檔案","        if path.exists():","            try:","                path.unlink()","            except:","                pass","        raise IOError(f\"shared manifest 驗證失敗 {path}: {e}\")","    ","    return final_payload","","","def read_shared_manifest(path: Path) -> Dict[str, Any]:","    \"\"\"","    讀取 shared manifest 並驗證 manifest_sha256","    ","    Args:","        path: 檔案路徑","    ","    Returns:","        manifest 字典","    ","    Raises:","        FileNotFoundError: 檔案不存在","        ValueError: JSON 解析失敗或 hash 驗證失敗","    \"\"\"","    if not path.exists():","        raise FileNotFoundError(f\"shared manifest 檔案不存在: {path}\")","    ","    try:","        content = path.read_text(encoding=\"utf-8\")","    except (IOError, OSError) as e:","        raise ValueError(f\"無法讀取 shared manifest 檔案 {path}: {e}\")","    ","    try:","        data = json.loads(content)","    except json.JSONDecodeError as e:","        raise ValueError(f\"shared manifest JSON 解析失敗 {path}: {e}\")","    ","    # 驗證 manifest_sha256（如果存在）","    if \"manifest_sha256\" in data:","        # 計算實際 hash（排除 manifest_sha256 欄位）","        data_without_hash = {k: v for k, v in data.items() if k != \"manifest_sha256\"}","        json_str = canonical_json(data_without_hash)","        expected_hash = hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","        ","        if data[\"manifest_sha256\"] != expected_hash:","            raise ValueError(f\"shared manifest hash 驗證失敗: 預期 {expected_hash}，實際 {data['manifest_sha256']}\")","    ","    return data","","","def load_shared_manifest_if_exists(path: Path) -> Optional[Dict[str, Any]]:","    \"\"\"","    載入 shared manifest（如果存在）","    ","    Args:","        path: 檔案路徑","    ","    Returns:","        manifest 字典或 None（如果檔案不存在）","    ","    Raises:","        ValueError: JSON 解析失敗或 hash 驗證失敗","    \"\"\"","    if not path.exists():","        return None","    ","    return read_shared_manifest(path)","",""]}
{"type":"file_footer","path":"src/control/shared_manifest.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/snapshot_compiler.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7348,"sha256":"94fca69f756aaefabee80b3eba9eb1b6104d4f1f0b784ec0ebbfb90d29611c0f","total_lines":231,"chunk_count":2}
{"type":"file_chunk","path":"src/control/snapshot_compiler.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Snapshot Compiler - compile outputs/snapshots/full/* into a single SYSTEM_FULL_SNAPSHOT.md.","","Contract:","- MUST embed verbatim bytes from snapshot files (no summarization, no reformatting content).","- MUST be deterministic: same inputs => identical output bytes.","- MUST preserve raw line order and content exactly as read.","- MUST NOT modify any raw files under outputs/snapshots/full/.","- Output path: outputs/snapshots/SYSTEM_FULL_SNAPSHOT.md.","\"\"\"","","from __future__ import annotations","import datetime","from pathlib import Path","from typing import List, Optional","import sys","","","def write_bytes_atomic(dst: Path, data: bytes) -> None:","    \"\"\"Atomic write helper.\"\"\"","    dst.parent.mkdir(parents=True, exist_ok=True)","    tmp = dst.with_suffix(dst.suffix + \".tmp\")","    tmp.write_bytes(data)","    tmp.replace(dst)","","","def compile_full_snapshot(","    snapshots_root: str | Path = \"outputs/snapshots\",","    full_dir_name: str = \"full\",","    out_name: str = \"SYSTEM_FULL_SNAPSHOT.md\",",") -> Path:","    \"\"\"","    Compile outputs/snapshots/full/* into outputs/snapshots/SYSTEM_FULL_SNAPSHOT.md deterministically.","    ","    Args:","        snapshots_root: Root directory containing snapshots.","        full_dir_name: Name of directory with raw artifacts (default \"full\").","        out_name: Output filename (default \"SYSTEM_FULL_SNAPSHOT.md\").","    ","    Returns:","        Path to the compiled snapshot file.","    \"\"\"","    snapshots_root = Path(snapshots_root)","    full_dir = snapshots_root / full_dir_name","    out_path = snapshots_root / out_name","    ","    if not full_dir.exists():","        raise FileNotFoundError(f\"Snapshot directory not found: {full_dir}\")","    ","    # Required order as per spec (matches test expectations)","    file_order = [","        \"MANIFEST.json\",","        \"LOCAL_SCAN_RULES.json\",","        \"REPO_TREE.txt\",","        \"AUDIT_GREP.txt\",","        \"AUDIT_IMPORTS.csv\",","        \"AUDIT_ENTRYPOINTS.md\",","        \"AUDIT_CALL_GRAPH.txt\",","        \"AUDIT_RUNTIME_MUTATIONS.txt\",","        \"AUDIT_CONFIG_REFERENCES.txt\",","        \"AUDIT_TEST_SURFACE.txt\",","        \"AUDIT_STATE_FLOW.md\",","        \"SKIPPED_FILES.txt\",","    ]","    ","    # Build output content","    lines: List[str] = []","    ","    # Determine timestamp - try to get from MANIFEST.json for determinism","    timestamp = \"UNKNOWN\"","    manifest_path = full_dir / \"MANIFEST.json\"","    if manifest_path.exists():","        try:","            import json","            with open(manifest_path, \"r\", encoding=\"utf-8\") as f:","                manifest = json.load(f)","                if \"generated_at_utc\" in manifest:","                    timestamp = manifest[\"generated_at_utc\"]","        except Exception:","            pass","    ","    # Header","    lines.append(\"# SYSTEM FULL SNAPSHOT - Compiled\")","    lines.append(\"\")","    lines.append(f\"Generated: {timestamp}\")","    lines.append(f\"Source directory: {full_dir}\")","    lines.append(\"\")","    lines.append(\"---\")","    lines.append(\"\")","    ","    missing_files: List[str] = []","    ","    for i, filename in enumerate(file_order, 1):","        file_path = full_dir / filename","        ","        if not file_path.exists():","            missing_files.append(filename)","            continue","        ","        # Determine language for code block","        ext = file_path.suffix.lower()","        if ext == \".json\":","            lang = \"json\"","        elif ext == \".md\":","            lang = \"md\"","        elif ext == \".csv\":","            lang = \"csv\"","        elif ext == \".txt\":","            lang = \"text\"","        else:","            lang = \"text\"","        ","        # Read file bytes","        try:","            content_bytes = file_path.read_bytes()","            # Try to decode as UTF-8, but fallback to replacement if needed","            try:","                content = content_bytes.decode(\"utf-8\")","            except UnicodeDecodeError:","                content = content_bytes.decode(\"utf-8\", errors=\"replace\")","        except Exception as e:","            content = f\"ERROR reading file: {e}\"","        ","        # Section header (match test expectations: ## FILENAME_WITH_EXTENSION)","        section_name = filename  # Keep extension","        lines.append(f\"## {section_name}\")","        lines.append(\"\")","        lines.append(f\"```{lang}\")","        lines.append(content.rstrip(\"\\n\"))  # Remove trailing newline to avoid extra line","        lines.append(\"```\")","        lines.append(\"\")","        lines.append(\"---\")","        lines.append(\"\")","    ","    # Missing files section","    if missing_files:","        lines.append(\"## Missing Files\")","        lines.append(\"\")","        lines.append(\"The following expected files were not found in the snapshot directory:\")","        lines.append(\"\")","        for filename in missing_files:","            lines.append(f\"- `{filename}`\")","        lines.append(\"\")","    ","    # Convert to bytes","    output_content = \"\\n\".join(lines)","    output_bytes = output_content.encode(\"utf-8\")","    ","    # Atomic write","    write_bytes_atomic(out_path, output_bytes)","    ","    return out_path","","","def verify_deterministic(","    snapshots_root: str | Path = \"outputs/snapshots\",","    full_dir_name: str = \"full\",","    out_name: str = \"SYSTEM_FULL_SNAPSHOT.md\",",") -> bool:","    \"\"\"","    Verify that compiling the same snapshot twice produces identical bytes.","    ","    Returns True if deterministic, False otherwise.","    \"\"\"","    snapshots_root = Path(snapshots_root)","    ","    # Compile first time","    out1 = compile_full_snapshot(snapshots_root, full_dir_name, out_name + \".test1\")","    data1 = out1.read_bytes()","    ","    # Compile second time","    out2 = compile_full_snapshot(snapshots_root, full_dir_name, out_name + \".test2\")","    data2 = out2.read_bytes()","    ","    # Clean up test files","    out1.unlink(missing_ok=True)","    out2.unlink(missing_ok=True)","    ","    return data1 == data2","","","if __name__ == \"__main__\":","    import argparse","    ","    parser = argparse.ArgumentParser(","        description=\"Compile full snapshot artifacts into a single SYSTEM_FULL_SNAPSHOT.md.\"","    )","    parser.add_argument(","        \"--verify-deterministic\",","        action=\"store_true\",","        help=\"Verify that compilation is deterministic (run twice and compare).\"","    )","    parser.add_argument(","        \"--snapshots-root\",","        default=\"outputs/snapshots\",","        help=\"Root directory containing snapshots (default: outputs/snapshots).\"","    )","    parser.add_argument(","        \"--full-dir\","]}
{"type":"file_chunk","path":"src/control/snapshot_compiler.py","chunk_index":1,"line_start":201,"line_end":231,"content":["        default=\"full\",","        help=\"Name of directory with raw artifacts (default: full).\"","    )","    parser.add_argument(","        \"--out-name\",","        default=\"SYSTEM_FULL_SNAPSHOT.md\",","        help=\"Output filename (default: SYSTEM_FULL_SNAPSHOT.md).\"","    )","    ","    args = parser.parse_args()","    ","    if args.verify_deterministic:","        print(\"Verifying determinism...\")","        if verify_deterministic(args.snapshots_root, args.full_dir, args.out_name):","            print(\"✓ Compilation is deterministic\")","            sys.exit(0)","        else:","            print(\"✗ Compilation is NOT deterministic\")","            sys.exit(1)","    else:","        try:","            out_path = compile_full_snapshot(","                args.snapshots_root,","                args.full_dir,","                args.out_name","            )","            print(f\"Compiled snapshot written to: {out_path}\")","            print(f\"Size: {out_path.stat().st_size:,} bytes\")","        except Exception as e:","            print(f\"Error: {e}\", file=sys.stderr)","            sys.exit(1)"]}
{"type":"file_footer","path":"src/control/snapshot_compiler.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/strategy_catalog.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7960,"sha256":"32eb717e2fd8503b93f4ee6bdb7482194371da9f5991299ef4cd9daaed9b0c1e","total_lines":221,"chunk_count":2}
{"type":"file_chunk","path":"src/control/strategy_catalog.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Strategy Catalog for M1 Wizard.","","Provides strategy listing and parameter schema capabilities for the wizard UI.","\"\"\"","","from __future__ import annotations","","from typing import List, Optional, Dict, Any","","from strategy.registry import (","    get_strategy_registry,","    StrategyRegistryResponse,","    StrategySpecForGUI,","    load_builtin_strategies,","    list_strategies,","    get as get_strategy_spec,",")","from strategy.param_schema import ParamSpec","","","class StrategyCatalog:","    \"\"\"Catalog for available strategies.\"\"\"","    ","    def __init__(self, load_builtin: bool = True):","        \"\"\"Initialize strategy catalog.","        ","        Args:","            load_builtin: Whether to load built-in strategies on initialization.","        \"\"\"","        self._registry_response: Optional[StrategyRegistryResponse] = None","        ","        if load_builtin:","            # Ensure built-in strategies are loaded","            try:","                load_builtin_strategies()","            except Exception:","                # Already loaded or error, continue","                pass","    ","    def load_registry(self) -> StrategyRegistryResponse:","        \"\"\"Load strategy registry.\"\"\"","        self._registry_response = get_strategy_registry()","        return self._registry_response","    ","    @property","    def registry(self) -> StrategyRegistryResponse:","        \"\"\"Get strategy registry (loads if not already loaded).\"\"\"","        if self._registry_response is None:","            self.load_registry()","        return self._registry_response","    ","    def list_strategies(self) -> List[StrategySpecForGUI]:","        \"\"\"List all available strategies for GUI.\"\"\"","        return self.registry.strategies","    ","    def get_strategy(self, strategy_id: str) -> Optional[StrategySpecForGUI]:","        \"\"\"Get strategy by ID for GUI.\"\"\"","        for strategy in self.registry.strategies:","            if strategy.strategy_id == strategy_id:","                return strategy","        return None","    ","    def get_strategy_spec(self, strategy_id: str):","        \"\"\"Get internal StrategySpec by ID.\"\"\"","        try:","            return get_strategy_spec(strategy_id)","        except KeyError:","            return None","    ","    def get_parameters(self, strategy_id: str) -> List[ParamSpec]:","        \"\"\"Get parameter schema for a strategy.\"\"\"","        strategy = self.get_strategy(strategy_id)","        if strategy is None:","            return []","        return strategy.params","    ","    def get_parameter_defaults(self, strategy_id: str) -> Dict[str, Any]:","        \"\"\"Get default parameter values for a strategy.\"\"\"","        params = self.get_parameters(strategy_id)","        defaults = {}","        for param in params:","            if param.default is not None:","                defaults[param.name] = param.default","        return defaults","    ","    def validate_parameters(","        self, ","        strategy_id: str, ","        parameters: Dict[str, Any]","    ) -> Dict[str, str]:","        \"\"\"Validate parameter values against schema.","        ","        Args:","            strategy_id: Strategy ID","            parameters: Parameter values to validate","            ","        Returns:","            Dictionary of validation errors (empty if valid)","        \"\"\"","        errors = {}","        params = self.get_parameters(strategy_id)","        ","        # Build lookup by parameter name","        param_map = {p.name: p for p in params}","        ","        for param_name, param_spec in param_map.items():","            value = parameters.get(param_name)","            ","            # Check required (all parameters are required for now)","            if value is None:","                errors[param_name] = f\"Parameter '{param_name}' is required\"","                continue","            ","            # Type validation","            if param_spec.type == \"int\":","                if not isinstance(value, (int, float)):","                    try:","                        int(value)","                    except (ValueError, TypeError):","                        errors[param_name] = f\"Parameter '{param_name}' must be an integer\"","                else:","                    # Check min/max","                    if param_spec.min is not None and value < param_spec.min:","                        errors[param_name] = f\"Parameter '{param_name}' must be >= {param_spec.min}\"","                    if param_spec.max is not None and value > param_spec.max:","                        errors[param_name] = f\"Parameter '{param_name}' must be <= {param_spec.max}\"","            ","            elif param_spec.type == \"float\":","                if not isinstance(value, (int, float)):","                    try:","                        float(value)","                    except (ValueError, TypeError):","                        errors[param_name] = f\"Parameter '{param_name}' must be a number\"","                else:","                    # Check min/max","                    if param_spec.min is not None and value < param_spec.min:","                        errors[param_name] = f\"Parameter '{param_name}' must be >= {param_spec.min}\"","                    if param_spec.max is not None and value > param_spec.max:","                        errors[param_name] = f\"Parameter '{param_name}' must be <= {param_spec.max}\"","            ","            elif param_spec.type == \"bool\":","                if not isinstance(value, bool):","                    errors[param_name] = f\"Parameter '{param_name}' must be a boolean\"","            ","            elif param_spec.type == \"enum\":","                if param_spec.choices and value not in param_spec.choices:","                    errors[param_name] = (","                        f\"Parameter '{param_name}' must be one of: {', '.join(map(str, param_spec.choices))}\"","                    )","        ","        # Check for extra parameters not in schema","        for param_name in parameters:","            if param_name not in param_map:","                errors[param_name] = f\"Unknown parameter '{param_name}' for strategy '{strategy_id}'\"","        ","        return errors","    ","    def get_strategy_ids(self) -> List[str]:","        \"\"\"Get list of all strategy IDs.\"\"\"","        return [s.strategy_id for s in self.registry.strategies]","    ","    def filter_by_parameter_count(self, min_params: int = 0, max_params: int = 10) -> List[StrategySpecForGUI]:","        \"\"\"Filter strategies by parameter count.\"\"\"","        return [","            s for s in self.registry.strategies","            if min_params <= len(s.params) <= max_params","        ]","    ","    def list_strategy_ids(self) -> List[str]:","        \"\"\"Get list of all strategy IDs.","        ","        Returns:","            List of strategy IDs sorted alphabetically","        \"\"\"","        return sorted([s.strategy_id for s in self.registry.strategies])","    ","    def get_strategy_spec_public(self, strategy_id: str) -> Optional[StrategySpecForGUI]:","        \"\"\"Public API: Get strategy spec by ID.","        ","        Args:","            strategy_id: Strategy ID to get","            ","        Returns:","            StrategySpecForGUI if found, None otherwise","        \"\"\"","        return self.get_strategy(strategy_id)","","","# Singleton instance for easy access","_catalog_instance: Optional[StrategyCatalog] = None","","def get_strategy_catalog() -> StrategyCatalog:","    \"\"\"Get singleton strategy catalog instance.\"\"\"","    global _catalog_instance","    if _catalog_instance is None:","        _catalog_instance = StrategyCatalog()","    return _catalog_instance","","","# Public API functions for registry access"]}
{"type":"file_chunk","path":"src/control/strategy_catalog.py","chunk_index":1,"line_start":201,"line_end":221,"content":["def list_strategy_ids() -> List[str]:","    \"\"\"Public API: Get list of all strategy IDs.","    ","    Returns:","        List of strategy IDs sorted alphabetically","    \"\"\"","    catalog = get_strategy_catalog()","    return catalog.list_strategy_ids()","","","def get_strategy_spec(strategy_id: str) -> Optional[StrategySpecForGUI]:","    \"\"\"Public API: Get strategy spec by ID.","    ","    Args:","        strategy_id: Strategy ID to get","        ","    Returns:","        StrategySpecForGUI if found, None otherwise","    \"\"\"","    catalog = get_strategy_catalog()","    return catalog.get_strategy_spec_public(strategy_id)"]}
{"type":"file_footer","path":"src/control/strategy_catalog.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/types.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1572,"sha256":"64b929a1a8c3a2eb9e10b1a9ecd111cebacf92fe5a158b9b4fb31d7953dfa9b1","total_lines":60,"chunk_count":1}
{"type":"file_chunk","path":"src/control/types.py","chunk_index":0,"line_start":1,"line_end":60,"content":["","\"\"\"Type definitions for B5-C Mission Control.\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, field","from enum import StrEnum","from typing import Any, Literal, Optional","","","class JobStatus(StrEnum):","    \"\"\"Job status state machine.\"\"\"","","    QUEUED = \"QUEUED\"","    RUNNING = \"RUNNING\"","    PAUSED = \"PAUSED\"","    DONE = \"DONE\"","    FAILED = \"FAILED\"","    KILLED = \"KILLED\"","","","class StopMode(StrEnum):","    \"\"\"Stop request mode.\"\"\"","","    SOFT = \"SOFT\"","    KILL = \"KILL\"","","","@dataclass(frozen=True)","class DBJobSpec:","    \"\"\"Job specification for DB/worker runtime (input to create_job).\"\"\"","","    season: str","    dataset_id: str","    outputs_root: str","    config_snapshot: dict[str, Any]  # sanitized; no ndarrays","    config_hash: str","    data_fingerprint_sha256_40: str = \"\"  # Data fingerprint SHA256[:40] (empty if not provided, marks DIRTY)","    created_by: str = \"b5c\"","","","@dataclass(frozen=True)","class JobRecord:","    \"\"\"Job record (returned from DB).\"\"\"","","    job_id: str","    status: JobStatus","    created_at: str","    updated_at: str","    spec: DBJobSpec","    pid: Optional[int] = None","    run_id: Optional[str] = None  # Final stage run_id (e.g. stage2_confirm-xxx)","    run_link: Optional[str] = None  # e.g. outputs/.../stage0_run_id or final run index pointer","    report_link: Optional[str] = None  # Link to B5 report viewer","    last_error: Optional[str] = None","    tags: list[str] = field(default_factory=list)  # Tags for job categorization and search","    data_fingerprint_sha256_40: str = \"\"  # Data fingerprint SHA256[:40] (empty if missing, marks DIRTY)","","",""]}
{"type":"file_footer","path":"src/control/types.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/wizard_nicegui.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":26463,"sha256":"2b11ca8ccfc9fa4de27e8ca3439b2290d70f92617756f827f5340364defe1c37","total_lines":642,"chunk_count":4}
{"type":"file_chunk","path":"src/control/wizard_nicegui.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Research Job Wizard (Phase 12) - NiceGUI interface.","","Phase 12: Config-only wizard that outputs WizardJobSpec JSON.","GUI → POST /jobs (WizardJobSpec) only, no worker calls, no filesystem access.","\"\"\"","","from __future__ import annotations","","import json","from datetime import date, datetime","from typing import Any, Dict, List, Optional","","import requests","from nicegui import ui","","from control.job_spec import DataSpec, WizardJobSpec, WFSSpec","from control.param_grid import GridMode, ParamGridSpec","from control.job_expand import JobTemplate, expand_job_template, estimate_total_jobs","from control.batch_submit import BatchSubmitRequest, BatchSubmitResponse","from data.dataset_registry import DatasetRecord","from strategy.param_schema import ParamSpec","from strategy.registry import StrategySpecForGUI","","# API base URL","API_BASE = \"http://localhost:8000\"","","","class WizardState:","    \"\"\"State management for wizard steps.\"\"\"","    ","    def __init__(self) -> None:","        self.season: str = \"\"","        self.data1: Optional[DataSpec] = None","        self.data2: Optional[DataSpec] = None","        self.strategy_id: str = \"\"","        self.params: Dict[str, Any] = {}","        self.wfs = WFSSpec()","        ","        # Phase 13: Batch mode","        self.batch_mode: bool = False","        self.param_grid_specs: Dict[str, ParamGridSpec] = {}","        self.job_template: Optional[JobTemplate] = None","        ","        # UI references","        self.data1_widgets: Dict[str, Any] = {}","        self.data2_widgets: Dict[str, Any] = {}","        self.param_widgets: Dict[str, Any] = {}","        self.wfs_widgets: Dict[str, Any] = {}","        self.batch_widgets: Dict[str, Any] = {}","","","def fetch_datasets() -> List[DatasetRecord]:","    \"\"\"Fetch dataset registry from API.\"\"\"","    try:","        resp = requests.get(f\"{API_BASE}/meta/datasets\", timeout=5)","        resp.raise_for_status()","        data = resp.json()","        return [DatasetRecord.model_validate(d) for d in data[\"datasets\"]]","    except Exception as e:","        ui.notify(f\"Failed to load datasets: {e}\", type=\"negative\")","        return []","","","def fetch_strategies() -> List[StrategySpecForGUI]:","    \"\"\"Fetch strategy registry from API.\"\"\"","    try:","        resp = requests.get(f\"{API_BASE}/meta/strategies\", timeout=5)","        resp.raise_for_status()","        data = resp.json()","        return [StrategySpecForGUI.model_validate(s) for s in data[\"strategies\"]]","    except Exception as e:","        ui.notify(f\"Failed to load strategies: {e}\", type=\"negative\")","        return []","","","def create_data_section(","    state: WizardState,","    section_name: str,","    is_primary: bool = True",") -> Dict[str, Any]:","    \"\"\"Create dataset selection UI section.\"\"\"","    widgets: Dict[str, Any] = {}","    ","    with ui.card().classes(\"w-full mb-4\"):","        ui.label(f\"{section_name} Dataset\").classes(\"text-lg font-bold\")","        ","        # Dataset dropdown","        datasets = fetch_datasets()","        dataset_options = {d.id: f\"{d.symbol} ({d.timeframe}) {d.start_date}-{d.end_date}\" ","                          for d in datasets}","        ","        dataset_select = ui.select(","            label=\"Dataset\",","            options=dataset_options,","            with_input=True","        ).classes(\"w-full\")","        widgets[\"dataset_select\"] = dataset_select","        ","        # Date range inputs","        with ui.row().classes(\"w-full\"):","            start_date = ui.date(","                label=\"Start Date\",","                value=date(2020, 1, 1)","            ).classes(\"w-1/2\")","            widgets[\"start_date\"] = start_date","            ","            end_date = ui.date(","                label=\"End Date\",","                value=date(2024, 12, 31)","            ).classes(\"w-1/2\")","            widgets[\"end_date\"] = end_date","        ","        # Update date limits when dataset changes","        def update_date_limits(selected_id: str) -> None:","            dataset = next((d for d in datasets if d.id == selected_id), None)","            if dataset:","                start_date.value = dataset.start_date","                end_date.value = dataset.end_date","                start_date._props[\"min\"] = dataset.start_date.isoformat()","                start_date._props[\"max\"] = dataset.end_date.isoformat()","                end_date._props[\"min\"] = dataset.start_date.isoformat()","                end_date._props[\"max\"] = dataset.end_date.isoformat()","                start_date.update()","                end_date.update()","        ","        dataset_select.on('update:model-value', lambda e: update_date_limits(e.args))","        ","        # Set initial limits if dataset is selected","        if dataset_select.value:","            update_date_limits(dataset_select.value)","    ","    return widgets","","","def create_strategy_section(state: WizardState) -> Dict[str, Any]:","    \"\"\"Create strategy selection and parameter UI section.\"\"\"","    widgets: Dict[str, Any] = {}","    ","    with ui.card().classes(\"w-full mb-4\"):","        ui.label(\"Strategy\").classes(\"text-lg font-bold\")","        ","        # Strategy dropdown","        strategies = fetch_strategies()","        strategy_options = {s.strategy_id: s.strategy_id for s in strategies}","        ","        strategy_select = ui.select(","            label=\"Strategy\",","            options=strategy_options,","            with_input=True","        ).classes(\"w-full\")","        widgets[\"strategy_select\"] = strategy_select","        ","        # Parameter container (dynamic)","        param_container = ui.column().classes(\"w-full mt-4\")","        widgets[\"param_container\"] = param_container","        ","        def update_parameters(selected_id: str) -> None:","            \"\"\"Update parameter UI based on selected strategy.\"\"\"","            param_container.clear()","            state.param_widgets.clear()","            ","            strategy = next((s for s in strategies if s.strategy_id == selected_id), None)","            if not strategy:","                return","            ","            ui.label(\"Parameters\").classes(\"font-bold mt-2\")","            ","            for param in strategy.params:","                with ui.row().classes(\"w-full items-center\"):","                    ui.label(f\"{param.name}:\").classes(\"w-1/3\")","                    ","                    if param.type == \"int\" or param.type == \"float\":","                        # Slider for numeric parameters","                        min_val = param.min if param.min is not None else 0","                        max_val = param.max if param.max is not None else 100","                        step = param.step if param.step is not None else 1","                        ","                        slider = ui.slider(","                            min=min_val,","                            max=max_val,","                            value=param.default,","                            step=step","                        ).classes(\"w-2/3\")","                        ","                        value_label = ui.label().bind_text_from(","                            slider, \"value\", ","                            lambda v: f\"{v:.2f}\" if param.type == \"float\" else f\"{int(v)}\"","                        )","                        ","                        state.param_widgets[param.name] = slider","                        ","                    elif param.type == \"enum\" and param.choices:","                        # Dropdown for enum parameters","                        dropdown = ui.select(","                            options=param.choices,","                            value=param.default","                        ).classes(\"w-2/3\")","                        state.param_widgets[param.name] = dropdown","                        "]}
{"type":"file_chunk","path":"src/control/wizard_nicegui.py","chunk_index":1,"line_start":201,"line_end":400,"content":["                    elif param.type == \"bool\":","                        # Switch for boolean parameters","                        switch = ui.switch(value=param.default).classes(\"w-2/3\")","                        state.param_widgets[param.name] = switch","                    ","                    # Help text","                    if param.help:","                        ui.tooltip(param.help).classes(\"ml-2\")","        ","        strategy_select.on('update:model-value', lambda e: update_parameters(e.args))","        ","        # Initialize if strategy is selected","        if strategy_select.value:","            update_parameters(strategy_select.value)","    ","    return widgets","","","def create_batch_mode_section(state: WizardState) -> Dict[str, Any]:","    \"\"\"Create batch mode UI section (Phase 13).\"\"\"","    widgets: Dict[str, Any] = {}","    ","    with ui.card().classes(\"w-full mb-4\"):","        ui.label(\"Batch Mode (Phase 13)\").classes(\"text-lg font-bold\")","        ","        # Batch mode toggle","        batch_toggle = ui.switch(\"Enable Batch Mode (Parameter Grid)\")","        widgets[\"batch_toggle\"] = batch_toggle","        ","        # Container for grid UI (hidden when batch mode off)","        grid_container = ui.column().classes(\"w-full mt-4\")","        widgets[\"grid_container\"] = grid_container","        ","        # Cost preview label","        cost_label = ui.label(\"Total jobs: 0 | Risk: Low\").classes(\"font-bold mt-2\")","        widgets[\"cost_label\"] = cost_label","        ","        def update_batch_mode(enabled: bool) -> None:","            \"\"\"Show/hide grid UI based on batch mode toggle.\"\"\"","            grid_container.clear()","            state.batch_mode = enabled","            state.param_grid_specs.clear()","            ","            if not enabled:","                cost_label.set_text(\"Total jobs: 0 | Risk: Low\")","                return","            ","            # Fetch current strategy parameters","            strategy_id = state.strategy_id","            strategies = fetch_strategies()","            strategy = next((s for s in strategies if s.strategy_id == strategy_id), None)","            if not strategy:","                ui.notify(\"No strategy selected\", type=\"warning\")","                return","            ","            # Create grid UI for each parameter","            ui.label(\"Parameter Grid\").classes(\"font-bold mt-2\")","            ","            for param in strategy.params:","                with ui.row().classes(\"w-full items-center mb-2\"):","                    ui.label(f\"{param.name}:\").classes(\"w-1/4\")","                    ","                    # Grid mode selector","                    mode_select = ui.select(","                        options={","                            GridMode.SINGLE.value: \"Single\",","                            GridMode.RANGE.value: \"Range\",","                            GridMode.MULTI.value: \"Multi Values\"","                        },","                        value=GridMode.SINGLE.value","                    ).classes(\"w-1/4\")","                    ","                    # Value inputs (dynamic based on mode)","                    value_container = ui.row().classes(\"w-1/2\")","                    ","                    def make_param_updater(pname: str, mode_sel, val_container, param_spec):","                        def update_grid_ui():","                            mode = GridMode(mode_sel.value)","                            val_container.clear()","                            ","                            if mode == GridMode.SINGLE:","                                # Single value input (same as default)","                                if param_spec.type == \"int\" or param_spec.type == \"float\":","                                    default = param_spec.default","                                    val = ui.number(value=default, min=param_spec.min, max=param_spec.max, step=param_spec.step or 1)","                                elif param_spec.type == \"enum\":","                                    val = ui.select(options=param_spec.choices, value=param_spec.default)","                                elif param_spec.type == \"bool\":","                                    val = ui.switch(value=param_spec.default)","                                else:","                                    val = ui.input(value=str(param_spec.default))","                                val_container.add(val)","                                # Store spec","                                state.param_grid_specs[pname] = ParamGridSpec(","                                    mode=mode,","                                    single_value=val.value","                                )","                            elif mode == GridMode.RANGE:","                                # Range: start, end, step","                                start = ui.number(value=param_spec.min or 0, label=\"Start\")","                                end = ui.number(value=param_spec.max or 100, label=\"End\")","                                step = ui.number(value=param_spec.step or 1, label=\"Step\")","                                val_container.add(start)","                                val_container.add(end)","                                val_container.add(step)","                                # Store spec (will be updated on change)","                                state.param_grid_specs[pname] = ParamGridSpec(","                                    mode=mode,","                                    range_start=start.value,","                                    range_end=end.value,","                                    range_step=step.value","                                )","                            elif mode == GridMode.MULTI:","                                # Multi values: comma-separated input","                                default_vals = \",\".join([str(param_spec.default)])","                                val = ui.input(value=default_vals, label=\"Values (comma separated)\")","                                val_container.add(val)","                                state.param_grid_specs[pname] = ParamGridSpec(","                                    mode=mode,","                                    multi_values=[param_spec.default]","                                )","                            # Trigger cost update","                            update_cost_preview()","                        return update_grid_ui","                    ","                    # Initial creation","                    updater = make_param_updater(param.name, mode_select, value_container, param)","                    mode_select.on('update:model-value', lambda e: updater())","                    updater()  # call once to create initial UI","        ","        batch_toggle.on('update:model-value', lambda e: update_batch_mode(e.args))","        ","        def update_cost_preview():","            \"\"\"Update cost preview label based on current grid specs.\"\"\"","            if not state.batch_mode:","                cost_label.set_text(\"Total jobs: 0 | Risk: Low\")","                return","            ","            # Build a temporary JobTemplate to estimate total jobs","            try:","                # Collect base WizardJobSpec from current UI (simplified)","                # We'll just use dummy values for estimation","                template = JobTemplate(","                    season=state.season,","                    dataset_id=\"dummy\",","                    strategy_id=state.strategy_id,","                    param_grid=state.param_grid_specs.copy(),","                    wfs=state.wfs","                )","                total = estimate_total_jobs(template)","                # Risk heuristic","                risk = \"Low\"","                if total > 100:","                    risk = \"Medium\"","                if total > 1000:","                    risk = \"High\"","                cost_label.set_text(f\"Total jobs: {total} | Risk: {risk}\")","            except Exception:","                cost_label.set_text(\"Total jobs: ? | Risk: Unknown\")","        ","        # Update cost preview periodically","        ui.timer(2.0, update_cost_preview)","    ","    return widgets","","","def create_wfs_section(state: WizardState) -> Dict[str, Any]:","    \"\"\"Create WFS configuration UI section.\"\"\"","    widgets: Dict[str, Any] = {}","    ","    with ui.card().classes(\"w-full mb-4\"):","        ui.label(\"WFS Configuration\").classes(\"text-lg font-bold\")","        ","        # Stage0 subsample","        subsample_slider = ui.slider(","            label=\"Stage0 Subsample\",","            min=0.01,","            max=1.0,","            value=state.wfs.stage0_subsample,","            step=0.01","        ).classes(\"w-full\")","        widgets[\"subsample\"] = subsample_slider","        ui.label().bind_text_from(subsample_slider, \"value\", lambda v: f\"{v:.2f}\")","        ","        # Top K","        top_k_input = ui.number(","            label=\"Top K\",","            value=state.wfs.top_k,","            min=1,","            max=1000,","            step=10","        ).classes(\"w-full\")","        widgets[\"top_k\"] = top_k_input","        ","        # Memory limit","        mem_input = ui.number(","            label=\"Memory Limit (MB)\",","            value=state.wfs.mem_limit_mb,","            min=1024,","            max=32768,"]}
{"type":"file_chunk","path":"src/control/wizard_nicegui.py","chunk_index":2,"line_start":401,"line_end":600,"content":["            step=1024","        ).classes(\"w-full\")","        widgets[\"mem_limit\"] = mem_input","        ","        # Auto-downsample switch","        auto_downsample = ui.switch(","            \"Allow Auto Downsample\",","            value=state.wfs.allow_auto_downsample","        ).classes(\"w-full\")","        widgets[\"auto_downsample\"] = auto_downsample","    ","    return widgets","","","def create_preview_section(state: WizardState) -> ui.textarea:","    \"\"\"Create WizardJobSpec preview section.\"\"\"","    with ui.card().classes(\"w-full mb-4\"):","        ui.label(\"WizardJobSpec Preview\").classes(\"text-lg font-bold\")","        ","        preview = ui.textarea(\"\").classes(\"w-full h-64 font-mono text-sm\").props(\"readonly\")","        ","        def update_preview() -> None:","            \"\"\"Update WizardJobSpec preview.\"\"\"","            try:","                # Collect data from UI","                dataset_id = None","                if state.data1_widgets:","                    dataset_id = state.data1_widgets[\"dataset_select\"].value","                    start_date = state.data1_widgets[\"start_date\"].value","                    end_date = state.data1_widgets[\"end_date\"].value","                    ","                    if dataset_id and start_date and end_date:","                        state.data1 = DataSpec(","                            dataset_id=dataset_id,","                            start_date=start_date,","                            end_date=end_date","                        )","                ","                # Collect strategy parameters","                params = {}","                for param_name, widget in state.param_widgets.items():","                    if hasattr(widget, 'value'):","                        params[param_name] = widget.value","                ","                # Collect WFS settings","                if state.wfs_widgets:","                    state.wfs = WFSSpec(","                        stage0_subsample=state.wfs_widgets[\"subsample\"].value,","                        top_k=state.wfs_widgets[\"top_k\"].value,","                        mem_limit_mb=state.wfs_widgets[\"mem_limit\"].value,","                        allow_auto_downsample=state.wfs_widgets[\"auto_downsample\"].value","                    )","                ","                if state.batch_mode:","                    # Create JobTemplate","                    template = JobTemplate(","                        season=state.season,","                        dataset_id=dataset_id if dataset_id else \"unknown\",","                        strategy_id=state.strategy_id,","                        param_grid=state.param_grid_specs.copy(),","                        wfs=state.wfs","                    )","                    # Update preview with template JSON","                    preview.value = template.model_dump_json(indent=2)","                else:","                    # Create single WizardJobSpec","                    jobspec = WizardJobSpec(","                        season=state.season,","                        data1=state.data1,","                        data2=state.data2,","                        strategy_id=state.strategy_id,","                        params=params,","                        wfs=state.wfs","                    )","                    # Update preview","                    preview.value = jobspec.model_dump_json(indent=2)","                ","            except Exception as e:","                preview.value = f\"Error creating preview: {e}\"","        ","        # Update preview periodically","        ui.timer(1.0, update_preview)","        ","        return preview","","","def submit_job(state: WizardState, preview: ui.textarea) -> None:","    \"\"\"Submit WizardJobSpec to API.\"\"\"","    try:","        # Parse WizardJobSpec from preview","        jobspec_data = json.loads(preview.value)","        jobspec = WizardJobSpec.model_validate(jobspec_data)","        ","        # Submit to API","        resp = requests.post(","            f\"{API_BASE}/jobs\",","            json=json.loads(jobspec.model_dump_json())","        )","        resp.raise_for_status()","        ","        job_id = resp.json()[\"job_id\"]","        ui.notify(f\"Job submitted successfully! Job ID: {job_id}\", type=\"positive\")","        ","    except Exception as e:","        ui.notify(f\"Failed to submit job: {e}\", type=\"negative\")","","","def submit_batch_job(state: WizardState, preview: ui.textarea) -> None:","    \"\"\"Submit batch of jobs via batch API.\"\"\"","    try:","        # Parse JobTemplate from preview","        template_data = json.loads(preview.value)","        template = JobTemplate.model_validate(template_data)","        ","        # Expand template to JobSpec list","        jobspecs = expand_job_template(template)","        ","        # Build batch request","        batch_req = BatchSubmitRequest(jobs=list(jobspecs))","        ","        # Submit to batch endpoint","        resp = requests.post(","            f\"{API_BASE}/jobs/batch\",","            json=json.loads(batch_req.model_dump_json())","        )","        resp.raise_for_status()","        ","        batch_resp = BatchSubmitResponse.model_validate(resp.json())","        ui.notify(","            f\"Batch submitted successfully! Batch ID: {batch_resp.batch_id}, \"","            f\"Total jobs: {batch_resp.total_jobs}\",","            type=\"positive\"","        )","        ","    except Exception as e:","        ui.notify(f\"Failed to submit batch: {e}\", type=\"negative\")","","","@ui.page(\"/wizard\")","def wizard_page() -> None:","    \"\"\"Research Job Wizard main page.\"\"\"","    ui.page_title(\"Research Job Wizard (Phase 12)\")","    ","    state = WizardState()","    ","    with ui.column().classes(\"w-full max-w-4xl mx-auto p-4\"):","        ui.label(\"Research Job Wizard\").classes(\"text-2xl font-bold mb-6\")","        ui.label(\"Phase 12: Config-only job specification\").classes(\"text-gray-600 mb-8\")","        ","        # Season input","        with ui.card().classes(\"w-full mb-4\"):","            ui.label(\"Season\").classes(\"text-lg font-bold\")","            season_input = ui.input(","                label=\"Season\",","                value=\"2024Q1\",","                placeholder=\"e.g., 2024Q1, 2024Q2\"","            ).classes(\"w-full\")","            ","            def update_season() -> None:","                state.season = season_input.value","            ","            season_input.on('update:model-value', lambda e: update_season())","            update_season()","        ","        # Step 1: Data","        with ui.expansion(\"Step 1: Data\", value=True).classes(\"w-full mb-4\"):","            ui.label(\"Primary Dataset\").classes(\"font-bold mt-2\")","            state.data1_widgets = create_data_section(state, \"Primary\", is_primary=True)","            ","            # Data2 toggle","            enable_data2 = ui.switch(\"Enable Secondary Dataset (for validation)\")","            ","            data2_container = ui.column().classes(\"w-full\")","            ","            def toggle_data2(enabled: bool) -> None:","                data2_container.clear()","                if enabled:","                    state.data2_widgets = create_data_section(state, \"Secondary\", is_primary=False)","                else:","                    state.data2 = None","                    state.data2_widgets = {}","            ","            enable_data2.on('update:model-value', lambda e: toggle_data2(e.args))","        ","        # Step 2: Strategy","        with ui.expansion(\"Step 2: Strategy\", value=True).classes(\"w-full mb-4\"):","            strategy_widgets = create_strategy_section(state)","            ","            def update_strategy() -> None:","                state.strategy_id = strategy_widgets[\"strategy_select\"].value","            ","            strategy_widgets[\"strategy_select\"].on('update:model-value', lambda e: update_strategy())","            if strategy_widgets[\"strategy_select\"].value:","                update_strategy()","        ","        # Step 3: Batch Mode (Phase 13)","        with ui.expansion(\"Step 3: Batch Mode (Optional)\", value=True).classes(\"w-full mb-4\"):","            state.batch_widgets = create_batch_mode_section(state)","        ","        # Step 4: WFS"]}
{"type":"file_chunk","path":"src/control/wizard_nicegui.py","chunk_index":3,"line_start":601,"line_end":642,"content":["        with ui.expansion(\"Step 4: WFS Configuration\", value=True).classes(\"w-full mb-4\"):","            state.wfs_widgets = create_wfs_section(state)","        ","        # Step 5: Preview & Submit","        with ui.expansion(\"Step 5: Preview & Submit\", value=True).classes(\"w-full mb-4\"):","            preview = create_preview_section(state)","            ","            with ui.row().classes(\"w-full mt-4\"):","                # Conditional button based on batch mode","                def submit_action():","                    if state.batch_mode:","                        submit_batch_job(state, preview)","                    else:","                        submit_job(state, preview)","                ","                submit_btn = ui.button(","                    \"Submit Batch\" if state.batch_mode else \"Submit Job\",","                    on_click=submit_action","                ).classes(\"bg-green-500 text-white\")","                ","                # Update button label when batch mode changes","                def update_button_label():","                    submit_btn.set_text(\"Submit Batch\" if state.batch_mode else \"Submit Job\")","                ","                # Watch batch mode changes (simplified: we can't directly watch, but we can update via timer)","                ui.timer(1.0, update_button_label)","                ","                ui.button(\"Copy JSON\", on_click=lambda: ui.run_javascript(","                    f\"navigator.clipboard.writeText(`{preview.value}`)\"","                )).classes(\"bg-blue-500 text-white\")","        ","        # Phase 12 Rules reminder","        with ui.card().classes(\"w-full mt-8 bg-yellow-50\"):","            ui.label(\"Phase 12 Rules\").classes(\"font-bold text-yellow-800\")","            ui.label(\"✅ GUI only outputs WizardJobSpec JSON\").classes(\"text-sm text-yellow-700\")","            ui.label(\"✅ No worker calls, no filesystem access\").classes(\"text-sm text-yellow-700\")","            ui.label(\"✅ Strategy params from registry, not hardcoded\").classes(\"text-sm text-yellow-700\")","            ui.label(\"✅ Dataset selection from registry, not filesystem\").classes(\"text-sm text-yellow-700\")","","","",""]}
{"type":"file_footer","path":"src/control/wizard_nicegui.py","complete":true,"emitted_chunks":4}
{"type":"file_header","path":"src/control/worker.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7371,"sha256":"6da1a295255a1f980696a583b445aa079655bfefcfc8c3ff136461e048f7b7f6","total_lines":228,"chunk_count":2}
{"type":"file_chunk","path":"src/control/worker.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Worker - long-running task executor.\"\"\"","","from __future__ import annotations","","import os","import signal","import time","from datetime import datetime, timezone","from pathlib import Path","from typing import Optional","","# ✅ Module-level import for patch support","from pipeline.funnel_runner import run_funnel","","from control.jobs_db import (","    get_job,","    get_requested_pause,","    get_requested_stop,","    mark_done,","    mark_failed,","    mark_killed,","    update_running,","    update_run_link,",")","from control.paths import run_log_path","from control.report_links import make_report_link","from control.types import JobStatus, StopMode","","","def _append_log(log_path: Path, text: str) -> None:","    \"\"\"","    Append text to log file.","    ","    Args:","        log_path: Path to log file","        text: Text to append","    \"\"\"","    log_path.parent.mkdir(parents=True, exist_ok=True)","    with log_path.open(\"a\", encoding=\"utf-8\") as f:","        f.write(text)","        if not text.endswith(\"\\n\"):","            f.write(\"\\n\")","","","def worker_loop(db_path: Path, *, poll_s: float = 0.5) -> None:","    \"\"\"","    Worker loop: poll QUEUED jobs and execute them sequentially.","    ","    Args:","        db_path: Path to SQLite database","        poll_s: Polling interval in seconds","    \"\"\"","    while True:","        try:","            # Find QUEUED jobs","            from control.jobs_db import list_jobs","            ","            jobs = list_jobs(db_path, limit=100)","            queued_jobs = [j for j in jobs if j.status == JobStatus.QUEUED]","            ","            if queued_jobs:","                # Process first QUEUED job","                job = queued_jobs[0]","                run_one_job(db_path, job.job_id)","            else:","                # No jobs, sleep","                time.sleep(poll_s)","        except KeyboardInterrupt:","            break","        except Exception as e:","            # Log error but continue loop","            print(f\"Worker loop error: {e}\")","            time.sleep(poll_s)","","","def run_one_job(db_path: Path, job_id: str) -> None:","    \"\"\"","    Run a single job.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","    \"\"\"","    log_path: Path | None = None","    try:","        job = get_job(db_path, job_id)","        ","        # Check if already terminal","        if job.status in {JobStatus.DONE, JobStatus.FAILED, JobStatus.KILLED}:","            return","        ","        # Update to RUNNING with current PID","        pid = os.getpid()","        update_running(db_path, job_id, pid=pid)","        ","        # Log status update","        timestamp = datetime.now(timezone.utc).isoformat()","        outputs_root = Path(job.spec.outputs_root)","        season = job.spec.season","        ","        # Initialize log_path early (use job_id as run_id fallback)","        log_path = run_log_path(outputs_root, season, job_id)","        ","        # Check for KILL before starting","        stop_mode = get_requested_stop(db_path, job_id)","        if stop_mode == StopMode.KILL.value:","            _append_log(log_path, f\"{timestamp} [job_id={job_id}] [status=KILLED] Killed before execution\")","            mark_killed(db_path, job_id, error=\"Killed before execution\")","            return","        ","        outputs_root.mkdir(parents=True, exist_ok=True)","        ","        # Reconstruct runtime config from snapshot","        cfg = dict(job.spec.config_snapshot)","        # Ensure required fields are present","        cfg[\"season\"] = job.spec.season","        cfg[\"dataset_id\"] = job.spec.dataset_id","        ","        # Log job start","        _append_log(","            log_path,","            f\"{timestamp} [job_id={job_id}] [status=RUNNING] Starting funnel execution\"","        )","        ","        # Check pause/stop before each stage","        _check_pause_stop(db_path, job_id)","        ","        # Run funnel","        result = run_funnel(cfg, outputs_root)","        ","        # Extract run_id and generate report_link","        run_id: Optional[str] = None","        report_link: Optional[str] = None","        ","        if getattr(result, \"stages\", None) and result.stages:","            last = result.stages[-1]","            run_id = last.run_id","            report_link = make_report_link(season=job.spec.season, run_id=run_id)","            ","            # Update run_link","            run_link = str(last.run_dir)","            update_run_link(db_path, job_id, run_link=run_link)","            ","            # Log summary","            log_path = run_log_path(outputs_root, season, run_id)","            timestamp = datetime.now(timezone.utc).isoformat()","            _append_log(","                log_path,","                f\"{timestamp} [job_id={job_id}] [status=DONE] Funnel completed: \"","                f\"run_id={run_id}, stage={last.stage.value}, run_dir={run_link}\"","            )","        ","        # Mark as done with run_id and report_link (both can be None if no stages)","        mark_done(db_path, job_id, run_id=run_id, report_link=report_link)","        ","        # Log final status","        timestamp = datetime.now(timezone.utc).isoformat()","        if log_path:","            _append_log(log_path, f\"{timestamp} [job_id={job_id}] [status=DONE] Job completed successfully\")","        ","    except KeyboardInterrupt:","        if log_path:","            timestamp = datetime.now(timezone.utc).isoformat()","            _append_log(log_path, f\"{timestamp} [job_id={job_id}] [status=KILLED] Interrupted by user\")","        mark_killed(db_path, job_id, error=\"Interrupted by user\")","        raise","    except Exception as e:","        import traceback","        ","        # Short for DB column (500 chars)","        error_msg = str(e)[:500]","        mark_failed(db_path, job_id, error=error_msg)","        ","        # Full traceback for audit log (MUST)","        tb = traceback.format_exc()","        from control.jobs_db import append_log","        append_log(db_path, job_id, \"[ERROR] Unhandled exception\\n\" + tb)","        ","        # Also write to file log if available","        if log_path:","            timestamp = datetime.now(timezone.utc).isoformat()","            _append_log(log_path, f\"{timestamp} [job_id={job_id}] [status=FAILED] Error: {error_msg}\\n{tb}\")","        ","        # Keep worker stable","        return","","","def _check_pause_stop(db_path: Path, job_id: str) -> None:","    \"\"\"","    Check pause/stop flags and handle accordingly.","    ","    Args:","        db_path: Path to SQLite database","        job_id: Job ID","        ","    Raises:","        SystemExit: If KILL requested","    \"\"\"","    stop_mode = get_requested_stop(db_path, job_id)"]}
{"type":"file_chunk","path":"src/control/worker.py","chunk_index":1,"line_start":201,"line_end":228,"content":["    if stop_mode == StopMode.KILL.value:","        # Get PID and kill process","        job = get_job(db_path, job_id)","        if job.pid:","            try:","                os.kill(job.pid, signal.SIGTERM)","            except ProcessLookupError:","                pass  # Process already dead","        mark_killed(db_path, job_id, error=\"Killed by user\")","        raise SystemExit(\"Job killed\")","    ","    # Handle pause","    while get_requested_pause(db_path, job_id):","        time.sleep(0.5)","        # Re-check stop while paused","        stop_mode = get_requested_stop(db_path, job_id)","        if stop_mode == StopMode.KILL.value:","            job = get_job(db_path, job_id)","            if job.pid:","                try:","                    os.kill(job.pid, signal.SIGTERM)","                except ProcessLookupError:","                    pass","            mark_killed(db_path, job_id, error=\"Killed while paused\")","            raise SystemExit(\"Job killed while paused\")","","",""]}
{"type":"file_footer","path":"src/control/worker.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/control/worker_main.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":375,"sha256":"b0265c815e43796ccdacba4ebad4f2e08d5c4bca8f181f891dbb12445e375094","total_lines":20,"chunk_count":1}
{"type":"file_chunk","path":"src/control/worker_main.py","chunk_index":0,"line_start":1,"line_end":20,"content":["","\"\"\"Worker main entry point (for subprocess execution).\"\"\"","","from __future__ import annotations","","import sys","from pathlib import Path","","from control.worker import worker_loop","","if __name__ == \"__main__\":","    if len(sys.argv) < 2:","        print(\"Usage: python -m control.worker_main <db_path>\")","        sys.exit(1)","    ","    db_path = Path(sys.argv[1])","    worker_loop(db_path)","","",""]}
{"type":"file_footer","path":"src/control/worker_main.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/control/worker_spawn_policy.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3069,"sha256":"9e87d1252d2411fc5fa92b6df6bc28e7f4182d0bb7543eac51d6b1db32e3ae79","total_lines":80,"chunk_count":1}
{"type":"file_chunk","path":"src/control/worker_spawn_policy.py","chunk_index":0,"line_start":1,"line_end":80,"content":["\"\"\"Worker spawn policy - enforce governance to stop uncontrolled worker spawning.","","Contract:","Worker can only be started when all are true:","- Not in pytest context (PYTEST_CURRENT_TEST absent) OR explicit override FISHBRO_ALLOW_SPAWN_IN_TESTS=1","- DB path is not under /tmp unless explicit override FISHBRO_ALLOW_TMP_DB=1","- pidfile locking ensures no duplicate spawn for same db_path (handled elsewhere)","- pidfile must be validated: process exists AND cmdline matches worker_main and db_path","\"\"\"","","from __future__ import annotations","","import os","from pathlib import Path","","","def can_spawn_worker(db_path: Path) -> tuple[bool, str]:","    \"\"\"Return (allowed, reason).","","    Rules:","    1. If PYTEST_CURRENT_TEST is set and FISHBRO_ALLOW_SPAWN_IN_TESTS != \"1\":","        deny with message about pytest.","    2. If db_path is under /tmp and FISHBRO_ALLOW_TMP_DB != \"1\":","        deny with message about /tmp.","    3. Otherwise allow.","    \"\"\"","    if os.getenv(\"PYTEST_CURRENT_TEST\") and os.getenv(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\") != \"1\":","        return False, \"Worker spawn disabled under pytest (set FISHBRO_ALLOW_SPAWN_IN_TESTS=1 to override)\"","","    rp = db_path.expanduser().resolve()","    if str(rp).startswith(\"/tmp/\") and os.getenv(\"FISHBRO_ALLOW_TMP_DB\") != \"1\":","        return False, \"Refusing to spawn worker for /tmp db_path (set FISHBRO_ALLOW_TMP_DB=1 to override)\"","","    return True, \"ok\"","","","def validate_pidfile(pidfile: Path, expected_db_path: Path) -> tuple[bool, str]:","    \"\"\"Validate pidfile points to a live worker process with matching db_path.","","    Returns (is_valid, reason).","    If valid, the worker is considered alive and no new spawn needed.","    If invalid (stale or mismatched), caller should remove pidfile and spawn.","    \"\"\"","    if not pidfile.exists():","        return False, \"pidfile missing\"","","    try:","        pid = int(pidfile.read_text().strip())","    except (ValueError, OSError):","        return False, \"pidfile corrupted\"","","    # Check if process exists","    try:","        os.kill(pid, 0)","    except OSError:","        return False, \"process dead\"","","    # On Linux, read cmdline from /proc/{pid}/cmdline","    cmdline_path = Path(f\"/proc/{pid}/cmdline\")","    if cmdline_path.exists():","        try:","            cmdline_bytes = cmdline_path.read_bytes()","            # Split by null bytes, decode","            parts = [p.decode(\"utf-8\", errors=\"replace\") for p in cmdline_bytes.split(b\"\\x00\") if p]","            cmdline = \" \".join(parts)","        except Exception:","            # If we cannot read cmdline, treat as unverifiable but assume alive","            return True, \"process alive (cmdline unverifiable)\"","    else:","        # Fallback for non-Linux (or permission issues)","        # We'll assume it's okay but log warning","        return True, \"process alive (cmdline unverifiable)\"","","    # Verify cmdline contains worker_main and db_path","    if \"control.worker_main\" not in cmdline:","        return False, \"process is not a worker_main\"","    if str(expected_db_path) not in cmdline:","        return False, \"process db_path mismatch\"","","    return True, \"worker alive and matching\""]}
{"type":"file_footer","path":"src/control/worker_spawn_policy.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":57,"sha256":"9380e6cee44a8c92094a4673f6ab9e721784aed936ae5cf76b9184a9107d588d","total_lines":4,"chunk_count":1}
{"type":"file_chunk","path":"src/core/__init__.py","chunk_index":0,"line_start":1,"line_end":4,"content":["","\"\"\"Core modules for audit and artifact management.\"\"\"","",""]}
{"type":"file_footer","path":"src/core/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/core/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":207,"sha256":"6a3fa0ef17a7b15515eca6e51d797f29629d849bd34fb60edfa10f2207d6be5c","note":"skipped by policy"}
{"type":"file_skipped","path":"src/core/__pycache__/ast_identity.cpython-312.pyc","reason":"cache","bytes":20313,"sha256":"4ebf3b33552b01f48776b4c6a92fd07e4bd27580208545e19523f8dec6eec67a","note":"skipped by policy"}
{"type":"file_skipped","path":"src/core/__pycache__/oom_cost_model.cpython-312.pyc","reason":"cache","bytes":4568,"sha256":"3e6d1ba06f0a61258e9ba25a82717030a7edd7c385015ba04d4e470a2459ef85","note":"skipped by policy"}
{"type":"file_skipped","path":"src/core/__pycache__/oom_gate.cpython-312.pyc","reason":"cache","bytes":12658,"sha256":"1d6d753178b98baadf3758daf5c1f3deb88e0c568d415a835333787cd2ebb1c1","note":"skipped by policy"}
{"type":"file_skipped","path":"src/core/__pycache__/service_identity.cpython-312.pyc","reason":"cache","bytes":6859,"sha256":"7d8b5413aa3706c7e33fc7a212c791ad2db13b059b440a85ad45f0673a772366","note":"skipped by policy"}
{"type":"file_header","path":"src/core/action_risk.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":534,"sha256":"935698ce27e202c228b3d0f2649f8205fa3c66fb656f2e649bf1b8d020495c01","total_lines":25,"chunk_count":1}
{"type":"file_chunk","path":"src/core/action_risk.py","chunk_index":0,"line_start":1,"line_end":25,"content":["\"\"\"Action Risk Levels - 資料契約","","定義系統動作的風險等級，用於實盤安全鎖。","\"\"\"","","from enum import Enum","from dataclasses import dataclass","from typing import Optional","","","class RiskLevel(str, Enum):","    \"\"\"動作風險等級\"\"\"","    READ_ONLY = \"READ_ONLY\"","    RESEARCH_MUTATE = \"RESEARCH_MUTATE\"","    LIVE_EXECUTE = \"LIVE_EXECUTE\"","","","@dataclass(frozen=True)","class ActionPolicyDecision:","    \"\"\"政策決策結果\"\"\"","    allowed: bool","    reason: str","    risk: RiskLevel","    action: str","    season: Optional[str] = None"]}
{"type":"file_footer","path":"src/core/action_risk.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/artifact_reader.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9366,"sha256":"eacafa41445de920c0cd532ac4ed545e22543ce980cf9bb9d607c35026d761e7","total_lines":322,"chunk_count":2}
{"type":"file_chunk","path":"src/core/artifact_reader.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Artifact reader for governance evaluation and Viewer.","","Reads artifacts (manifest/metrics/winners/config_snapshot) from run directories.","Provides safe read functions that never raise exceptions (for Viewer use).","\"\"\"","","from __future__ import annotations","","import hashlib","import json","from dataclasses import dataclass","from pathlib import Path","from typing import Any, Dict, Optional","","try:","    import yaml","    HAS_YAML = True","except ImportError:","    HAS_YAML = False","","","def read_manifest(run_dir: Path) -> Dict[str, Any]:","    \"\"\"","    Read manifest.json from run directory.","    ","    Args:","        run_dir: Path to run directory","        ","    Returns:","        Manifest dict (AuditSchema as dict)","        ","    Raises:","        FileNotFoundError: If manifest.json does not exist","        json.JSONDecodeError: If manifest.json is invalid JSON","    \"\"\"","    manifest_path = run_dir / \"manifest.json\"","    if not manifest_path.exists():","        raise FileNotFoundError(f\"manifest.json not found in {run_dir}\")","    ","    with manifest_path.open(\"r\", encoding=\"utf-8\") as f:","        return json.load(f)","","","def read_metrics(run_dir: Path) -> Dict[str, Any]:","    \"\"\"","    Read metrics.json from run directory.","    ","    Args:","        run_dir: Path to run directory","        ","    Returns:","        Metrics dict","        ","    Raises:","        FileNotFoundError: If metrics.json does not exist","        json.JSONDecodeError: If metrics.json is invalid JSON","    \"\"\"","    metrics_path = run_dir / \"metrics.json\"","    if not metrics_path.exists():","        raise FileNotFoundError(f\"metrics.json not found in {run_dir}\")","    ","    with metrics_path.open(\"r\", encoding=\"utf-8\") as f:","        return json.load(f)","","","def read_winners(run_dir: Path) -> Dict[str, Any]:","    \"\"\"","    Read winners.json from run directory.","    ","    Args:","        run_dir: Path to run directory","        ","    Returns:","        Winners dict with schema {\"topk\": [...], \"notes\": {...}}","        ","    Raises:","        FileNotFoundError: If winners.json does not exist","        json.JSONDecodeError: If winners.json is invalid JSON","    \"\"\"","    winners_path = run_dir / \"winners.json\"","    if not winners_path.exists():","        raise FileNotFoundError(f\"winners.json not found in {run_dir}\")","    ","    with winners_path.open(\"r\", encoding=\"utf-8\") as f:","        return json.load(f)","","","def read_config_snapshot(run_dir: Path) -> Dict[str, Any]:","    \"\"\"","    Read config_snapshot.json from run directory.","    ","    Args:","        run_dir: Path to run directory","        ","    Returns:","        Config snapshot dict","        ","    Raises:","        FileNotFoundError: If config_snapshot.json does not exist","        json.JSONDecodeError: If config_snapshot.json is invalid JSON","    \"\"\"","    config_path = run_dir / \"config_snapshot.json\"","    if not config_path.exists():","        raise FileNotFoundError(f\"config_snapshot.json not found in {run_dir}\")","    ","    with config_path.open(\"r\", encoding=\"utf-8\") as f:","        return json.load(f)","","","# ============================================================================","# Safe artifact reader (never raises) - for Viewer use","# ============================================================================","","@dataclass(frozen=True)","class ReadMeta:","    \"\"\"Metadata about the read operation.\"\"\"","    source_path: str  # Absolute path to source file","    sha256: str  # SHA256 hash of file content","    mtime_s: float  # Modification time in seconds since epoch","","","@dataclass(frozen=True)","class ReadResult:","    \"\"\"","    Result of reading an artifact file.","    ","    Contains raw data (dict/list/str) and metadata.","    Upper layer uses pydantic for validation.","    \"\"\"","    raw: Any  # dict/list/str - raw parsed data","    meta: ReadMeta","","","@dataclass(frozen=True)","class ReadError:","    \"\"\"Error information for failed read operations.\"\"\"","    error_code: str  # \"FILE_NOT_FOUND\", \"UNSUPPORTED_FORMAT\", \"YAML_NOT_AVAILABLE\", \"JSON_DECODE_ERROR\", \"IO_ERROR\"","    message: str","    source_path: str","","","@dataclass(frozen=True)","class SafeReadResult:","    \"\"\"","    Safe read result that never raises.","    ","    Either contains ReadResult (success) or ReadError (failure).","    \"\"\"","    result: Optional[ReadResult] = None","    error: Optional[ReadError] = None","    ","    @property","    def is_ok(self) -> bool:","        \"\"\"Check if read was successful.\"\"\"","        return self.result is not None and self.error is None","    ","    @property","    def is_error(self) -> bool:","        \"\"\"Check if read failed.\"\"\"","        return self.error is not None","","","def _compute_sha256(file_path: Path) -> str:","    \"\"\"Compute SHA256 hash of file content.\"\"\"","    sha256_hash = hashlib.sha256()","    with file_path.open(\"rb\") as f:","        for chunk in iter(lambda: f.read(4096), b\"\"):","            sha256_hash.update(chunk)","    return sha256_hash.hexdigest()","","","def read_artifact(file_path: Path | str) -> ReadResult:","    \"\"\"","    Read artifact file (JSON/YAML/MD) and return ReadResult.","    ","    Args:","        file_path: Path to artifact file","        ","    Returns:","        ReadResult with raw data and metadata","        ","    Raises:","        FileNotFoundError: If file does not exist","        ValueError: If file format is not supported","    \"\"\"","    path = Path(file_path).resolve()","    ","    if not path.exists():","        raise FileNotFoundError(f\"Artifact file not found: {path}\")","    ","    # Get metadata","    mtime_s = path.stat().st_mtime","    sha256 = _compute_sha256(path)","    ","    # Read based on extension","    suffix = path.suffix.lower()","    ","    if suffix == \".json\":","        with path.open(\"r\", encoding=\"utf-8\") as f:"]}
{"type":"file_chunk","path":"src/core/artifact_reader.py","chunk_index":1,"line_start":201,"line_end":322,"content":["            raw = json.load(f)","    elif suffix in (\".yaml\", \".yml\"):","        if not HAS_YAML:","            raise ValueError(f\"YAML support not available. Install pyyaml to read {path}\")","        with path.open(\"r\", encoding=\"utf-8\") as f:","            raw = yaml.safe_load(f)","    elif suffix == \".md\":","        with path.open(\"r\", encoding=\"utf-8\") as f:","            raw = f.read()  # Return as string for markdown","    else:","        raise ValueError(f\"Unsupported file format: {suffix}. Supported: .json, .yaml, .yml, .md\")","    ","    meta = ReadMeta(","        source_path=str(path),","        sha256=sha256,","        mtime_s=mtime_s,","    )","    ","    return ReadResult(raw=raw, meta=meta)","","","def try_read_artifact(file_path: Path | str) -> SafeReadResult:","    \"\"\"","    Safe version of read_artifact that never raises.","    ","    All Viewer code should use this function instead of read_artifact()","    to ensure no exceptions are thrown.","    ","    Args:","        file_path: Path to artifact file","        ","    Returns:","        SafeReadResult with either ReadResult (success) or ReadError (failure)","    \"\"\"","    path = Path(file_path).resolve()","    ","    # Check if file exists","    if not path.exists():","        return SafeReadResult(","            error=ReadError(","                error_code=\"FILE_NOT_FOUND\",","                message=f\"Artifact file not found: {path}\",","                source_path=str(path),","            )","        )","    ","    try:","        # Get metadata","        mtime_s = path.stat().st_mtime","        sha256 = _compute_sha256(path)","    except OSError as e:","        return SafeReadResult(","            error=ReadError(","                error_code=\"IO_ERROR\",","                message=f\"Failed to read file metadata: {e}\",","                source_path=str(path),","            )","        )","    ","    # Read based on extension","    suffix = path.suffix.lower()","    ","    try:","        if suffix == \".json\":","            with path.open(\"r\", encoding=\"utf-8\") as f:","                raw = json.load(f)","        elif suffix in (\".yaml\", \".yml\"):","            if not HAS_YAML:","                return SafeReadResult(","                    error=ReadError(","                        error_code=\"YAML_NOT_AVAILABLE\",","                        message=f\"YAML support not available. Install pyyaml to read {path}\",","                        source_path=str(path),","                    )","                )","            with path.open(\"r\", encoding=\"utf-8\") as f:","                raw = yaml.safe_load(f)","        elif suffix == \".md\":","            with path.open(\"r\", encoding=\"utf-8\") as f:","                raw = f.read()  # Return as string for markdown","        else:","            return SafeReadResult(","                error=ReadError(","                    error_code=\"UNSUPPORTED_FORMAT\",","                    message=f\"Unsupported file format: {suffix}. Supported: .json, .yaml, .yml, .md\",","                    source_path=str(path),","                )","            )","    except json.JSONDecodeError as e:","        return SafeReadResult(","            error=ReadError(","                error_code=\"JSON_DECODE_ERROR\",","                message=f\"JSON decode error: {e}\",","                source_path=str(path),","            )","        )","    except OSError as e:","        return SafeReadResult(","            error=ReadError(","                error_code=\"IO_ERROR\",","                message=f\"Failed to read file: {e}\",","                source_path=str(path),","            )","        )","    except Exception as e:","        return SafeReadResult(","            error=ReadError(","                error_code=\"UNKNOWN_ERROR\",","                message=f\"Unexpected error: {e}\",","                source_path=str(path),","            )","        )","    ","    meta = ReadMeta(","        source_path=str(path),","        sha256=sha256,","        mtime_s=mtime_s,","    )","    ","    return SafeReadResult(result=ReadResult(raw=raw, meta=meta))","",""]}
{"type":"file_footer","path":"src/core/artifact_reader.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/core/artifact_status.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12570,"sha256":"7dc8b2ea3fed5fe8880212ebd95a6b249b5b5132f170d81996b6df264198f2d7","total_lines":348,"chunk_count":2}
{"type":"file_chunk","path":"src/core/artifact_status.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Status determination for artifact validation.","","Defines OK/MISSING/INVALID/DIRTY states with human-readable error messages.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass","from enum import Enum","from typing import Optional","","from pydantic import ValidationError","","","class ArtifactStatus(str, Enum):","    \"\"\"Artifact validation status.\"\"\"","    OK = \"OK\"","    MISSING = \"MISSING\"  # File does not exist","    INVALID = \"INVALID\"  # Pydantic validation error","    DIRTY = \"DIRTY\"  # config_hash mismatch","","","@dataclass(frozen=True)","class ValidationResult:","    \"\"\"","    Result of artifact validation.","    ","    Contains status and human-readable error message.","    \"\"\"","    status: ArtifactStatus","    message: str = \"\"","    error_details: Optional[str] = None  # Detailed error for debugging","","","def _format_pydantic_error(e: ValidationError) -> str:","    \"\"\"Format Pydantic ValidationError into readable string with field paths.\"\"\"","    parts: list[str] = []","    for err in e.errors():","        loc = \".\".join(str(x) for x in err.get(\"loc\", []))","        msg = err.get(\"msg\", \"\")","        typ = err.get(\"type\", \"\")","        if loc:","            parts.append(f\"{loc}: {msg} ({typ})\")","        else:","            parts.append(f\"{msg} ({typ})\")","    return \"；\".join(parts) if parts else str(e)","","","def _extract_missing_field_names(e: ValidationError) -> list[str]:","    \"\"\"Extract missing field names from ValidationError.\"\"\"","    missing: set[str] = set()","    for err in e.errors():","        typ = str(err.get(\"type\", \"\")).lower()","        msg = str(err.get(\"msg\", \"\")).lower()","        if \"missing\" in typ or \"required\" in msg:","            loc = err.get(\"loc\", ())","            # loc 可能像 (\"rows\", 0, \"net_profit\") 或 (\"config_hash\",)","            if loc:","                leaf = str(loc[-1])","                # 避免 leaf 是 index","                if not leaf.isdigit():","                    missing.add(leaf)","            # 也把完整路徑收進來（可讀性更好）","            loc_str = \".\".join(str(x) for x in loc if not isinstance(x, int))","            if loc_str:","                missing.add(loc_str.split(\".\")[-1])  # leaf 再保險一次","    return sorted(missing)","","","def validate_manifest_status(","    file_path: str,","    manifest_data: Optional[dict] = None,","    expected_config_hash: Optional[str] = None,",") -> ValidationResult:","    \"\"\"","    Validate manifest.json status.","    ","    Args:","        file_path: Path to manifest.json","        manifest_data: Parsed manifest data (if available)","        expected_config_hash: Expected config_hash (for DIRTY check)","        ","    Returns:","        ValidationResult with status and message","    \"\"\"","    from pathlib import Path","    from core.schemas.manifest import RunManifest","    ","    path = Path(file_path)","    ","    # Check if file exists","    if not path.exists():","        return ValidationResult(","            status=ArtifactStatus.MISSING,","            message=f\"manifest.json 不存在: {file_path}\",","        )","    ","    # Try to parse with Pydantic","    if manifest_data is None:","        import json","        try:","            with path.open(\"r\", encoding=\"utf-8\") as f:","                manifest_data = json.load(f)","        except json.JSONDecodeError as e:","            return ValidationResult(","                status=ArtifactStatus.INVALID,","                message=f\"manifest.json JSON 格式錯誤: {e}\",","                error_details=str(e),","            )","    ","    try:","        manifest = RunManifest(**manifest_data)","    except Exception as e:","        # Extract missing field from Pydantic error","        error_msg = str(e)","        missing_fields = []","        if \"field required\" in error_msg.lower():","            # Try to extract field name from error","            import re","            matches = re.findall(r\"Field required.*?['\\\"]([^'\\\"]+)['\\\"]\", error_msg)","            if matches:","                missing_fields = matches","        ","        if missing_fields:","            msg = f\"manifest.json 缺少欄位: {', '.join(missing_fields)}\"","        else:","            msg = f\"manifest.json 驗證失敗: {error_msg}\"","        ","        return ValidationResult(","            status=ArtifactStatus.INVALID,","            message=msg,","            error_details=error_msg,","        )","    ","    # Check config_hash if expected is provided","    if expected_config_hash is not None and manifest.config_hash != expected_config_hash:","        return ValidationResult(","            status=ArtifactStatus.DIRTY,","            message=f\"manifest.config_hash={manifest.config_hash} 但預期值為 {expected_config_hash}\",","        )","    ","    # Phase 6.5: Check data_fingerprint_sha1 (mandatory)","    fingerprint_sha1 = getattr(manifest, 'data_fingerprint_sha1', None)","    if not fingerprint_sha1 or fingerprint_sha1 == \"\":","        return ValidationResult(","            status=ArtifactStatus.DIRTY,","            message=\"Missing Data Fingerprint — report is untrustworthy (data_fingerprint_sha1 is empty or missing)\",","        )","    ","    return ValidationResult(status=ArtifactStatus.OK, message=\"manifest.json 驗證通過\")","","","def validate_winners_v2_status(","    file_path: str,","    winners_data: Optional[dict] = None,","    expected_config_hash: Optional[str] = None,","    manifest_config_hash: Optional[str] = None,",") -> ValidationResult:","    \"\"\"","    Validate winners_v2.json status.","    ","    Args:","        file_path: Path to winners_v2.json","        winners_data: Parsed winners data (if available)","        expected_config_hash: Expected config_hash (for DIRTY check)","        manifest_config_hash: config_hash from manifest (for DIRTY check)","        ","    Returns:","        ValidationResult with status and message","    \"\"\"","    from pathlib import Path","    from core.schemas.winners_v2 import WinnersV2","    ","    path = Path(file_path)","    ","    # Check if file exists","    if not path.exists():","        return ValidationResult(","            status=ArtifactStatus.MISSING,","            message=f\"winners_v2.json 不存在: {file_path}\",","        )","    ","    # Try to parse with Pydantic","    if winners_data is None:","        import json","        try:","            with path.open(\"r\", encoding=\"utf-8\") as f:","                winners_data = json.load(f)","        except json.JSONDecodeError as e:","            return ValidationResult(","                status=ArtifactStatus.INVALID,","                message=f\"winners_v2.json JSON 格式錯誤: {e}\",","                error_details=str(e),","            )","    ","    try:","        winners = WinnersV2(**winners_data)","        ","        # Validate rows if present (Pydantic already validates required fields)"]}
{"type":"file_chunk","path":"src/core/artifact_status.py","chunk_index":1,"line_start":201,"line_end":348,"content":["        # Additional checks for None values (defensive)","        for idx, row in enumerate(winners.rows):","            if row.net_profit is None:","                return ValidationResult(","                    status=ArtifactStatus.INVALID,","                    message=f\"winners_v2.json 第 {idx} 行 net_profit 是必填欄位\",","                    error_details=f\"row[{idx}].net_profit is None\",","                )","            if row.max_drawdown is None:","                return ValidationResult(","                    status=ArtifactStatus.INVALID,","                    message=f\"winners_v2.json 第 {idx} 行 max_drawdown 是必填欄位\",","                    error_details=f\"row[{idx}].max_drawdown is None\",","                )","            if row.trades is None:","                return ValidationResult(","                    status=ArtifactStatus.INVALID,","                    message=f\"winners_v2.json 第 {idx} 行 trades 是必填欄位\",","                    error_details=f\"row[{idx}].trades is None\",","                )","    except ValidationError as e:","        missing_fields = _extract_missing_field_names(e)","        missing_txt = f\"缺少欄位: {', '.join(missing_fields)}；\" if missing_fields else \"\"","        error_details = str(e) + \"\\nmissing_fields=\" + \",\".join(missing_fields) if missing_fields else str(e)","        return ValidationResult(","            status=ArtifactStatus.INVALID,","            message=f\"winners_v2.json {missing_txt}schema 驗證失敗：{_format_pydantic_error(e)}\",","            error_details=error_details,","        )","    except Exception as e:","        # Fallback for non-Pydantic errors","        return ValidationResult(","            status=ArtifactStatus.INVALID,","            message=f\"winners_v2.json 驗證失敗: {e}\",","            error_details=str(e),","        )","    ","    # Check config_hash if expected/manifest is provided","    if expected_config_hash is not None:","        if winners.config_hash != expected_config_hash:","            return ValidationResult(","                status=ArtifactStatus.DIRTY,","                message=f\"winners_v2.config_hash={winners.config_hash} 但預期值為 {expected_config_hash}\",","            )","    ","    if manifest_config_hash is not None:","        if winners.config_hash != manifest_config_hash:","            return ValidationResult(","                status=ArtifactStatus.DIRTY,","                message=f\"winners_v2.config_hash={winners.config_hash} 但 manifest.config_hash={manifest_config_hash}\",","            )","    ","    return ValidationResult(status=ArtifactStatus.OK, message=\"winners_v2.json 驗證通過\")","","","def validate_governance_status(","    file_path: str,","    governance_data: Optional[dict] = None,","    expected_config_hash: Optional[str] = None,","    manifest_config_hash: Optional[str] = None,",") -> ValidationResult:","    \"\"\"","    Validate governance.json status.","    ","    Args:","        file_path: Path to governance.json","        governance_data: Parsed governance data (if available)","        expected_config_hash: Expected config_hash (for DIRTY check)","        manifest_config_hash: config_hash from manifest (for DIRTY check)","        ","    Returns:","        ValidationResult with status and message","    \"\"\"","    from pathlib import Path","    from core.schemas.governance import GovernanceReport","    ","    path = Path(file_path)","    ","    # Check if file exists","    if not path.exists():","        return ValidationResult(","            status=ArtifactStatus.MISSING,","            message=f\"governance.json 不存在: {file_path}\",","        )","    ","    # Try to parse with Pydantic","    if governance_data is None:","        import json","        try:","            with path.open(\"r\", encoding=\"utf-8\") as f:","                governance_data = json.load(f)","        except json.JSONDecodeError as e:","            return ValidationResult(","                status=ArtifactStatus.INVALID,","                message=f\"governance.json JSON 格式錯誤: {e}\",","                error_details=str(e),","            )","    ","    try:","        governance = GovernanceReport(**governance_data)","    except Exception as e:","        # Extract missing field from Pydantic error","        error_msg = str(e)","        missing_fields = []","        if \"field required\" in error_msg.lower():","            import re","            matches = re.findall(r\"Field required.*?['\\\"]([^'\\\"]+)['\\\"]\", error_msg)","            if matches:","                missing_fields = matches","        ","        if missing_fields:","            msg = f\"governance.json 缺少欄位: {', '.join(missing_fields)}\"","        else:","            msg = f\"governance.json 驗證失敗: {error_msg}\"","        ","        return ValidationResult(","            status=ArtifactStatus.INVALID,","            message=msg,","            error_details=error_msg,","        )","    ","    # Check config_hash if expected/manifest is provided","    if expected_config_hash is not None:","        if governance.config_hash != expected_config_hash:","            return ValidationResult(","                status=ArtifactStatus.DIRTY,","                message=f\"governance.config_hash={governance.config_hash} 但預期值為 {expected_config_hash}\",","            )","    ","    if manifest_config_hash is not None:","        if governance.config_hash != manifest_config_hash:","            return ValidationResult(","                status=ArtifactStatus.DIRTY,","                message=f\"governance.config_hash={governance.config_hash} 但 manifest.config_hash={manifest_config_hash}\",","            )","    ","    # Phase 6.5: Check data_fingerprint_sha1 in metadata (mandatory)","    metadata = governance_data.get(\"metadata\", {}) if governance_data else {}","    fingerprint_sha1 = metadata.get(\"data_fingerprint_sha1\", \"\")","    if not fingerprint_sha1 or fingerprint_sha1 == \"\":","        return ValidationResult(","            status=ArtifactStatus.DIRTY,","            message=\"Missing Data Fingerprint — report is untrustworthy (data_fingerprint_sha1 is empty or missing in metadata)\",","        )","    ","    return ValidationResult(status=ArtifactStatus.OK, message=\"governance.json 驗證通過\")","",""]}
{"type":"file_footer","path":"src/core/artifact_status.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/core/artifacts.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5451,"sha256":"cd6ae36b0773cfdd3a11039dfa36b098ceaa7fe522f6566fb1747c75c7b3685a","total_lines":156,"chunk_count":1}
{"type":"file_chunk","path":"src/core/artifacts.py","chunk_index":0,"line_start":1,"line_end":156,"content":["","\"\"\"Artifact writer for unified run output.","","Provides consistent artifact structure for all runs, with mandatory","subsample rate visibility.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Any, Dict","","from core.winners_builder import build_winners_v2","from core.winners_schema import is_winners_legacy, is_winners_v2","","","def _write_json(path: Path, obj: Any) -> None:","    \"\"\"","    Write object to JSON file with fixed format.","    ","    Uses sort_keys=True and fixed separators for reproducibility.","    ","    Args:","        path: Path to JSON file","        obj: Object to serialize","    \"\"\"","    path.write_text(","        json.dumps(obj, ensure_ascii=False, sort_keys=True, indent=2) + \"\\n\",","        encoding=\"utf-8\",","    )","","","def write_run_artifacts(","    run_dir: Path,","    manifest: Dict[str, Any],","    config_snapshot: Dict[str, Any],","    metrics: Dict[str, Any],","    winners: Dict[str, Any] | None = None,",") -> None:","    \"\"\"","    Write all standard artifacts for a run.","    ","    Creates the following files:","    - manifest.json: Full AuditSchema data","    - config_snapshot.json: Original/normalized config","    - metrics.json: Performance metrics","    - winners.json: Top-K results (fixed schema)","    - README.md: Human-readable summary","    - logs.txt: Execution logs (empty initially)","    ","    Args:","        run_dir: Run directory path (will be created if needed)","        manifest: Manifest data (AuditSchema as dict)","        config_snapshot: Configuration snapshot","        metrics: Performance metrics (must include param_subsample_rate visibility)","        winners: Optional winners dict. If None, uses empty schema.","            Must follow schema: {\"topk\": [...], \"notes\": {\"schema\": \"v1\", ...}}","    \"\"\"","    run_dir.mkdir(parents=True, exist_ok=True)","    ","    # Write manifest.json (full AuditSchema)","    _write_json(run_dir / \"manifest.json\", manifest)","    ","    # Write config_snapshot.json","    _write_json(run_dir / \"config_snapshot.json\", config_snapshot)","    ","    # Write metrics.json (must include param_subsample_rate visibility)","    _write_json(run_dir / \"metrics.json\", metrics)","    ","    # Write winners.json (always output v2 schema)","    if winners is None:","        winners = {\"topk\": [], \"notes\": {\"schema\": \"v1\"}}","    ","    # Auto-upgrade legacy winners to v2","    if is_winners_legacy(winners):","        # Convert legacy to v2","        legacy_topk = winners.get(\"topk\", [])","        run_id = manifest.get(\"run_id\", \"unknown\")","        stage_name = metrics.get(\"stage_name\", \"unknown\")","        ","        winners = build_winners_v2(","            stage_name=stage_name,","            run_id=run_id,","            manifest=manifest,","            config_snapshot=config_snapshot,","            legacy_topk=legacy_topk,","        )","    elif not is_winners_v2(winners):","        # Unknown format - try to upgrade anyway (defensive)","        legacy_topk = winners.get(\"topk\", [])","        if legacy_topk:","            run_id = manifest.get(\"run_id\", \"unknown\")","            stage_name = metrics.get(\"stage_name\", \"unknown\")","            ","            winners = build_winners_v2(","                stage_name=stage_name,","                run_id=run_id,","                manifest=manifest,","                config_snapshot=config_snapshot,","                legacy_topk=legacy_topk,","            )","        else:","            # Empty topk - create minimal v2 structure","            from core.winners_schema import build_winners_v2_dict","            winners = build_winners_v2_dict(","                stage_name=metrics.get(\"stage_name\", \"unknown\"),","                run_id=manifest.get(\"run_id\", \"unknown\"),","                topk=[],","            )","    ","    _write_json(run_dir / \"winners.json\", winners)","    ","    # Write README.md (human-readable summary)","    # Must prominently display param_subsample_rate","    readme_lines = [","        \"# FishBroWFS_V2 Run\",","        \"\",","        f\"- run_id: {manifest.get('run_id')}\",","        f\"- git_sha: {manifest.get('git_sha')}\",","        f\"- param_subsample_rate: {manifest.get('param_subsample_rate')}\",","        f\"- season: {manifest.get('season')}\",","        f\"- dataset_id: {manifest.get('dataset_id')}\",","        f\"- bars: {manifest.get('bars')}\",","        f\"- params_total: {manifest.get('params_total')}\",","        f\"- params_effective: {manifest.get('params_effective')}\",","        f\"- config_hash: {manifest.get('config_hash')}\",","    ]","    ","    # Add OOM gate information if present in metrics","    if \"oom_gate_action\" in metrics:","        readme_lines.extend([","            \"\",","            \"## OOM Gate\",","            \"\",","            f\"- action: {metrics.get('oom_gate_action')}\",","            f\"- reason: {metrics.get('oom_gate_reason')}\",","            f\"- mem_est_mb: {metrics.get('mem_est_mb', 0):.1f}\",","            f\"- mem_limit_mb: {metrics.get('mem_limit_mb', 0):.1f}\",","            f\"- ops_est: {metrics.get('ops_est', 0)}\",","        ])","        ","        # If auto-downsample occurred, show original and final","        if metrics.get(\"oom_gate_action\") == \"AUTO_DOWNSAMPLE\":","            readme_lines.extend([","                f\"- original_subsample: {metrics.get('oom_gate_original_subsample', 0)}\",","                f\"- final_subsample: {metrics.get('oom_gate_final_subsample', 0)}\",","            ])","    ","    readme = \"\\n\".join(readme_lines)","    (run_dir / \"README.md\").write_text(readme, encoding=\"utf-8\")","    ","    # Write logs.txt (empty initially)","    (run_dir / \"logs.txt\").write_text(\"\", encoding=\"utf-8\")","",""]}
{"type":"file_footer","path":"src/core/artifacts.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/ast_identity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":16991,"sha256":"a7368f674044cfe0366917ff438ff947a5083ab3fcbbe304268c40d819d9a7fb","total_lines":471,"chunk_count":3}
{"type":"file_chunk","path":"src/core/ast_identity.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"AST-based canonical identity for strategies.","","Implements content-addressed, deterministic StrategyID derived from strategy's","canonical AST (ast-c14n-v1). This replaces filesystem iteration order, Python","import order, list index/enumerate/incremental counters, filename or class name","as primary key.","","Key properties:","1. Deterministic: Same AST → same hash regardless of file location, import order","2. Content-addressed: Hash derived from canonical AST representation","3. Immutable: Strategy identity cannot change without changing its logic","4. Collision-resistant: SHA-256 provides sufficient collision resistance","5. No hash() usage: Uses hashlib.sha256 for deterministic hashing","","Algorithm (ast-c14n-v1):","1. Parse source code to AST","2. Canonicalize AST (normalize whitespace, sort dict keys, etc.)","3. Serialize to canonical string representation","4. Compute SHA-256 hash","5. Encode as hex string (StrategyID)","\"\"\"","","from __future__ import annotations","","import ast","import hashlib","import json","import textwrap","from typing import Any, Dict, List, Optional, Union","from pathlib import Path","import inspect","","","class ASTCanonicalizer:","    \"\"\"Canonical AST representation for deterministic hashing.\"\"\"","    ","    @staticmethod","    def canonicalize(node: ast.AST) -> Any:","        \"\"\"Convert AST node to canonical JSON-serializable representation.","        ","        Follows ast-c14n-v1 specification:","        1. Sort dictionary keys alphabetically","        2. Normalize numeric literals (float precision)","        3. Remove location information (lineno, col_offset)","        4. Handle special AST nodes consistently","        5. Preserve only semantically relevant information","        \"\"\"","        if isinstance(node, ast.Module):","            return {","                \"type\": \"Module\",","                \"body\": [ASTCanonicalizer.canonicalize(stmt) for stmt in node.body]","            }","        ","        elif isinstance(node, ast.FunctionDef):","            return {","                \"type\": \"FunctionDef\",","                \"name\": node.name,","                \"args\": ASTCanonicalizer.canonicalize(node.args),","                \"body\": [ASTCanonicalizer.canonicalize(stmt) for stmt in node.body],","                \"decorator_list\": [","                    ASTCanonicalizer.canonicalize(decorator) ","                    for decorator in node.decorator_list","                ],","                \"returns\": (","                    ASTCanonicalizer.canonicalize(node.returns) ","                    if node.returns else None","                )","            }","        ","        elif isinstance(node, ast.ClassDef):","            return {","                \"type\": \"ClassDef\",","                \"name\": node.name,","                \"bases\": [ASTCanonicalizer.canonicalize(base) for base in node.bases],","                \"keywords\": [","                    ASTCanonicalizer.canonicalize(keyword) ","                    for keyword in node.keywords","                ],","                \"body\": [ASTCanonicalizer.canonicalize(stmt) for stmt in node.body],","                \"decorator_list\": [","                    ASTCanonicalizer.canonicalize(decorator) ","                    for decorator in node.decorator_list","                ]","            }","        ","        elif isinstance(node, ast.arguments):","            return {","                \"type\": \"arguments\",","                \"args\": [ASTCanonicalizer.canonicalize(arg) for arg in node.args],","                \"defaults\": [","                    ASTCanonicalizer.canonicalize(default) ","                    for default in node.defaults","                ],","                \"vararg\": (","                    ASTCanonicalizer.canonicalize(node.vararg) ","                    if node.vararg else None","                ),","                \"kwarg\": (","                    ASTCanonicalizer.canonicalize(node.kwarg) ","                    if node.kwarg else None","                )","            }","        ","        elif isinstance(node, ast.arg):","            return {","                \"type\": \"arg\",","                \"arg\": node.arg,","                \"annotation\": (","                    ASTCanonicalizer.canonicalize(node.annotation) ","                    if node.annotation else None","                )","            }","        ","        elif isinstance(node, ast.Name):","            return {","                \"type\": \"Name\",","                \"id\": node.id,","                \"ctx\": node.ctx.__class__.__name__","            }","        ","        elif isinstance(node, ast.Attribute):","            return {","                \"type\": \"Attribute\",","                \"value\": ASTCanonicalizer.canonicalize(node.value),","                \"attr\": node.attr,","                \"ctx\": node.ctx.__class__.__name__","            }","        ","        elif isinstance(node, ast.Constant):","            value = node.value","            # Normalize numeric values","            if isinstance(value, float):","                # Use repr to preserve precision but normalize -0.0","                value = float(repr(value))","            elif isinstance(value, complex):","                value = complex(repr(value))","            return {","                \"type\": \"Constant\",","                \"value\": value,","                \"kind\": getattr(node, 'kind', None)","            }","        ","        elif isinstance(node, ast.Dict):","            # Sort dictionary keys for determinism","            keys = [ASTCanonicalizer.canonicalize(k) for k in node.keys]","            values = [ASTCanonicalizer.canonicalize(v) for v in node.values]","            ","            # Create list of key-value pairs for sorting","            pairs = list(zip(keys, values))","            # Sort by key representation","            pairs.sort(key=lambda x: json.dumps(x[0], sort_keys=True))","            ","            sorted_keys = [k for k, _ in pairs]","            sorted_values = [v for _, v in pairs]","            ","            return {","                \"type\": \"Dict\",","                \"keys\": sorted_keys,","                \"values\": sorted_values","            }","        ","        elif isinstance(node, ast.List):","            return {","                \"type\": \"List\",","                \"elts\": [ASTCanonicalizer.canonicalize(elt) for elt in node.elts],","                \"ctx\": node.ctx.__class__.__name__","            }","        ","        elif isinstance(node, ast.Tuple):","            return {","                \"type\": \"Tuple\",","                \"elts\": [ASTCanonicalizer.canonicalize(elt) for elt in node.elts],","                \"ctx\": node.ctx.__class__.__name__","            }","        ","        elif isinstance(node, ast.Set):","            # Sets need special handling for determinism","            elts = [ASTCanonicalizer.canonicalize(elt) for elt in node.elts]","            # Sort by JSON representation","            elts.sort(key=lambda x: json.dumps(x, sort_keys=True))","            return {","                \"type\": \"Set\",","                \"elts\": elts","            }","        ","        elif isinstance(node, ast.Call):","            # Sort keywords by argument name for determinism","            keywords = [","                {","                    \"arg\": kw.arg,","                    \"value\": ASTCanonicalizer.canonicalize(kw.value)","                }","                for kw in node.keywords","            ]","            keywords.sort(key=lambda x: x[\"arg\"] if x[\"arg\"] else \"\")","            ","            return {","                \"type\": \"Call\",","                \"func\": ASTCanonicalizer.canonicalize(node.func),","                \"args\": [ASTCanonicalizer.canonicalize(arg) for arg in node.args],"]}
{"type":"file_chunk","path":"src/core/ast_identity.py","chunk_index":1,"line_start":201,"line_end":400,"content":["                \"keywords\": keywords","            }","        ","        elif isinstance(node, ast.keyword):","            return {","                \"type\": \"keyword\",","                \"arg\": node.arg,","                \"value\": ASTCanonicalizer.canonicalize(node.value)","            }","        ","        elif isinstance(node, ast.Import):","            # Sort imports by name for determinism","            names = [","                {\"name\": alias.name, \"asname\": alias.asname}","                for alias in node.names","            ]","            names.sort(key=lambda x: x[\"name\"])","            return {","                \"type\": \"Import\",","                \"names\": names","            }","        ","        elif isinstance(node, ast.ImportFrom):","            # Sort imports by name for determinism","            names = [","                {\"name\": alias.name, \"asname\": alias.asname}","                for alias in node.names","            ]","            names.sort(key=lambda x: x[\"name\"])","            return {","                \"type\": \"ImportFrom\",","                \"module\": node.module,","                \"names\": names,","                \"level\": node.level","            }","        ","        elif isinstance(node, ast.Assign):","            return {","                \"type\": \"Assign\",","                \"targets\": [","                    ASTCanonicalizer.canonicalize(target) ","                    for target in node.targets","                ],","                \"value\": ASTCanonicalizer.canonicalize(node.value)","            }","        ","        elif isinstance(node, ast.Return):","            return {","                \"type\": \"Return\",","                \"value\": (","                    ASTCanonicalizer.canonicalize(node.value) ","                    if node.value else None","                )","            }","        ","        elif isinstance(node, ast.If):","            return {","                \"type\": \"If\",","                \"test\": ASTCanonicalizer.canonicalize(node.test),","                \"body\": [ASTCanonicalizer.canonicalize(stmt) for stmt in node.body],","                \"orelse\": [ASTCanonicalizer.canonicalize(stmt) for stmt in node.orelse]","            }","        ","        elif isinstance(node, ast.BinOp):","            return {","                \"type\": \"BinOp\",","                \"left\": ASTCanonicalizer.canonicalize(node.left),","                \"op\": node.op.__class__.__name__,","                \"right\": ASTCanonicalizer.canonicalize(node.right)","            }","        ","        elif isinstance(node, ast.UnaryOp):","            return {","                \"type\": \"UnaryOp\",","                \"op\": node.op.__class__.__name__,","                \"operand\": ASTCanonicalizer.canonicalize(node.operand)","            }","        ","        elif isinstance(node, ast.Compare):","            return {","                \"type\": \"Compare\",","                \"left\": ASTCanonicalizer.canonicalize(node.left),","                \"ops\": [op.__class__.__name__ for op in node.ops],","                \"comparators\": [","                    ASTCanonicalizer.canonicalize(comp) ","                    for comp in node.comparators","                ]","            }","        ","        # Handle expression contexts","        elif isinstance(node, (ast.Load, ast.Store, ast.Del)):","            return {\"type\": node.__class__.__name__}","        ","        # Default fallback: convert node attributes to dict","        else:","            node_type = node.__class__.__name__","            result = {\"type\": node_type}","            ","            # Get public attributes","            for attr_name in dir(node):","                if attr_name.startswith('_'):","                    continue","                if attr_name in ('lineno', 'col_offset', 'end_lineno', 'end_col_offset'):","                    continue","                ","                try:","                    attr_value = getattr(node, attr_name)","                except AttributeError:","                    continue","                ","                # Skip None values and empty lists","                if attr_value is None:","                    continue","                if isinstance(attr_value, list) and len(attr_value) == 0:","                    continue","                ","                # Recursively canonicalize if it's an AST node","                if isinstance(attr_value, ast.AST):","                    result[attr_name] = ASTCanonicalizer.canonicalize(attr_value)","                elif isinstance(attr_value, list) and attr_value and isinstance(attr_value[0], ast.AST):","                    result[attr_name] = [","                        ASTCanonicalizer.canonicalize(item) for item in attr_value","                    ]","                elif isinstance(attr_value, (str, int, float, bool)):","                    result[attr_name] = attr_value","            ","            return result","    ","    @staticmethod","    def canonical_ast_hash(source_code: str) -> str:","        \"\"\"Compute canonical hash of source code AST.","        ","        Args:","            source_code: Python source code as string","            ","        Returns:","            SHA-256 hash hex string (64 characters)","        \"\"\"","        try:","            tree = ast.parse(source_code)","            canonical = ASTCanonicalizer.canonicalize(tree)","            ","            # Convert to canonical JSON string with sorted keys","            canonical_json = json.dumps(","                canonical,","                sort_keys=True,","                separators=(',', ':'),  # No whitespace","                ensure_ascii=False","            )","            ","            # Compute SHA-256 hash","            hash_obj = hashlib.sha256(canonical_json.encode('utf-8'))","            return hash_obj.hexdigest()","        ","        except (SyntaxError, ValueError) as e:","            raise ValueError(f\"Failed to parse or canonicalize source code: {e}\")","","","def compute_strategy_id_from_source(source_code: str) -> str:","    \"\"\"Compute StrategyID from strategy source code.","    ","    Args:","        source_code: Strategy function source code","        ","    Returns:","        StrategyID (hex string, 64 characters)","    \"\"\"","    return ASTCanonicalizer.canonical_ast_hash(source_code)","","","def compute_strategy_id_from_function(func) -> str:","    \"\"\"Compute StrategyID from strategy function object.","    ","    Args:","        func: Strategy function (callable)","        ","    Returns:","        StrategyID (hex string, 64 characters)","    \"\"\"","    try:","        source_code = inspect.getsource(func)","        # Dedent the source code to handle indentation from nested definitions","        dedented_source = textwrap.dedent(source_code)","        return compute_strategy_id_from_source(dedented_source)","    except (OSError, TypeError) as e:","        raise ValueError(f\"Failed to get source code for function: {e}\")","","","def compute_strategy_id_from_file(filepath: Union[str, Path]) -> str:","    \"\"\"Compute StrategyID from strategy source file.","    ","    Args:","        filepath: Path to Python source file","        ","    Returns:","        StrategyID (hex string, 64 characters)","    \"\"\"","    path = Path(filepath)","    if not path.exists():","        raise FileNotFoundError(f\"Strategy file not found: {filepath}\")"]}
{"type":"file_chunk","path":"src/core/ast_identity.py","chunk_index":2,"line_start":401,"line_end":471,"content":["    ","    source_code = path.read_text(encoding='utf-8')","    return compute_strategy_id_from_source(source_code)","","","class StrategyIdentity:","    \"\"\"Immutable strategy identity based on canonical AST hash.\"\"\"","    ","    def __init__(self, strategy_id: str, source_hash: Optional[str] = None):","        \"\"\"Initialize strategy identity.","        ","        Args:","            strategy_id: Content-addressed strategy ID (hex string)","            source_hash: Optional source hash for verification","        \"\"\"","        if not isinstance(strategy_id, str) or len(strategy_id) != 64:","            raise ValueError(","                f\"Invalid strategy_id: must be 64-character hex string, got {strategy_id}\"","            )","        ","        # Validate hex format","        try:","            int(strategy_id, 16)","        except ValueError:","            raise ValueError(f\"Invalid strategy_id: not a valid hex string: {strategy_id}\")","        ","        self._strategy_id = strategy_id","        self._source_hash = source_hash","    ","    @property","    def strategy_id(self) -> str:","        \"\"\"Get the content-addressed strategy ID.\"\"\"","        return self._strategy_id","    ","    @property","    def source_hash(self) -> Optional[str]:","        \"\"\"Get the source hash (if available).\"\"\"","        return self._source_hash","    ","    @classmethod","    def from_source(cls, source_code: str) -> StrategyIdentity:","        \"\"\"Create StrategyIdentity from source code.\"\"\"","        strategy_id = compute_strategy_id_from_source(source_code)","        return cls(strategy_id, source_hash=strategy_id)","    ","    @classmethod","    def from_function(cls, func) -> StrategyIdentity:","        \"\"\"Create StrategyIdentity from function.\"\"\"","        strategy_id = compute_strategy_id_from_function(func)","        return cls(strategy_id, source_hash=strategy_id)","    ","    @classmethod","    def from_file(cls, filepath: Union[str, Path]) -> StrategyIdentity:","        \"\"\"Create StrategyIdentity from file.\"\"\"","        strategy_id = compute_strategy_id_from_file(filepath)","        return cls(strategy_id, source_hash=strategy_id)","    ","    def __eq__(self, other: Any) -> bool:","        if not isinstance(other, StrategyIdentity):","            return False","        return self._strategy_id == other._strategy_id","    ","    def __hash__(self) -> int:","        # Use integer representation of first 16 chars for hash","        return int(self._strategy_id[:16], 16)","    ","    def __repr__(self) -> str:","        return f\"StrategyIdentity(strategy_id={self._strategy_id[:16]}...)\"","    ","    def __str__(self) -> str:","        return self._strategy_id"]}
{"type":"file_footer","path":"src/core/ast_identity.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/core/audit_schema.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1919,"sha256":"9bec8723373b6e4906c9cf3ea1adf507853d7f5d9393462b561a911fac4c6929","total_lines":59,"chunk_count":1}
{"type":"file_chunk","path":"src/core/audit_schema.py","chunk_index":0,"line_start":1,"line_end":59,"content":["","\"\"\"Audit schema for run tracking and reproducibility.","","Single Source of Truth (SSOT) for audit data.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, asdict","from datetime import datetime, timezone","from typing import Any, Dict","","","@dataclass(frozen=True)","class AuditSchema:","    \"\"\"","    Audit schema for run tracking.","    ","    All fields are required and must be JSON-serializable.","    This is the Single Source of Truth (SSOT) for audit data.","    \"\"\"","    run_id: str","    created_at: str  # ISO8601 with Z suffix (UTC)","    git_sha: str  # At least 12 chars","    dirty_repo: bool  # Whether repo has uncommitted changes","    param_subsample_rate: float  # Required, must be in [0.0, 1.0]","    config_hash: str  # Stable hash of config","    season: str  # Season identifier","    dataset_id: str  # Dataset identifier","    bars: int  # Number of bars processed","    params_total: int  # Total parameters before subsample","    params_effective: int  # Effective parameters after subsample (= int(params_total * param_subsample_rate))","    artifact_version: str = \"v1\"  # Artifact version","    ","    def to_dict(self) -> Dict[str, Any]:","        \"\"\"Convert to dictionary for JSON serialization.\"\"\"","        return asdict(self)","","","def compute_params_effective(params_total: int, param_subsample_rate: float) -> int:","    \"\"\"","    Compute effective parameters after subsample.","    ","    Rounding rule: int(params_total * param_subsample_rate)","    This is locked in code/docs/tests - do not change.","    ","    Args:","        params_total: Total parameters before subsample","        param_subsample_rate: Subsample rate in [0.0, 1.0]","        ","    Returns:","        Effective parameters (integer, rounded down)","    \"\"\"","    if not (0.0 <= param_subsample_rate <= 1.0):","        raise ValueError(f\"param_subsample_rate must be in [0.0, 1.0], got {param_subsample_rate}\")","    ","    return int(params_total * param_subsample_rate)","",""]}
{"type":"file_footer","path":"src/core/audit_schema.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/config_hash.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":737,"sha256":"e1bf5e812460f3215e7d661d0f56b905d60323f9264ee9a930b0d4e35f466abb","total_lines":35,"chunk_count":1}
{"type":"file_chunk","path":"src/core/config_hash.py","chunk_index":0,"line_start":1,"line_end":35,"content":["","\"\"\"Stable config hash computation.","","Provides deterministic hash of configuration objects for reproducibility.","\"\"\"","","from __future__ import annotations","","import hashlib","import json","from typing import Any","","","def stable_config_hash(obj: Any) -> str:","    \"\"\"","    Compute stable hash of configuration object.","    ","    Uses JSON serialization with sorted keys and fixed separators","    to ensure cross-platform consistency.","    ","    Args:","        obj: Configuration object (dict, list, etc.)","        ","    Returns:","        Hex string hash (64 chars, SHA256)","    \"\"\"","    s = json.dumps(","        obj,","        sort_keys=True,","        separators=(\",\", \":\"),","        ensure_ascii=False,","    )","    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()","",""]}
{"type":"file_footer","path":"src/core/config_hash.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/config_snapshot.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2862,"sha256":"64533c2ba5a5f3b06a0ebb9fb4b491ba59223911faff4ba50363ef2abc4d081b","total_lines":99,"chunk_count":1}
{"type":"file_chunk","path":"src/core/config_snapshot.py","chunk_index":0,"line_start":1,"line_end":99,"content":["","\"\"\"Config snapshot sanitizer.","","Creates JSON-serializable config snapshots by excluding large ndarrays","and converting numpy types to Python native types.","\"\"\"","","from __future__ import annotations","","from typing import Any, Dict","","import numpy as np","","# These keys will make artifacts garbage or directly crash JSON serialization","_DEFAULT_DROP_KEYS = {","    \"open_\",","    \"open\",","    \"high\",","    \"low\",","    \"close\",","    \"volume\",","    \"params_matrix\",","}","","","def _ndarray_meta(x: np.ndarray) -> Dict[str, Any]:","    \"\"\"","    Create metadata dict for ndarray (shape and dtype only).","    ","    Args:","        x: numpy array","        ","    Returns:","        Metadata dictionary with shape and dtype","    \"\"\"","    return {","        \"__ndarray__\": True,","        \"shape\": list(x.shape),","        \"dtype\": str(x.dtype),","    }","","","def make_config_snapshot(","    cfg: Dict[str, Any],","    drop_keys: set[str] | None = None,",") -> Dict[str, Any]:","    \"\"\"","    Create sanitized config snapshot for JSON serialization and hashing.","    ","    Rules (locked):","    - Must include: season, dataset_id, bars, params_total, param_subsample_rate,","      stage_name, topk, commission, slip, order_qty, config knobs...","    - Must exclude/replace: open_, high, low, close, params_matrix (ndarrays)","    - If metadata needed, only keep shape/dtype (no bytes hash to avoid cost)","    ","    Args:","        cfg: Configuration dictionary (may contain ndarrays)","        drop_keys: Optional set of keys to drop. If None, uses default.","        ","    Returns:","        Sanitized config dictionary (JSON-serializable)","    \"\"\"","    drop = _DEFAULT_DROP_KEYS if drop_keys is None else drop_keys","    out: Dict[str, Any] = {}","    ","    for k, v in cfg.items():","        if k in drop:","            # Don't keep raw data, only metadata (optional)","            if isinstance(v, np.ndarray):","                out[k + \"_meta\"] = _ndarray_meta(v)","            continue","        ","        # numpy scalar -> python scalar","        if isinstance(v, (np.floating, np.integer)):","            out[k] = v.item()","        # ndarray (if slipped through) -> meta","        elif isinstance(v, np.ndarray):","            out[k + \"_meta\"] = _ndarray_meta(v)","        # Basic types: keep as-is","        elif isinstance(v, (str, int, float, bool)) or v is None:","            out[k] = v","        # list/tuple: conservative handling (avoid strange objects)","        elif isinstance(v, (list, tuple)):","            # Check if list contains only serializable types","            try:","                # Try to serialize to verify","                import json","                json.dumps(v)","                out[k] = v","            except (TypeError, ValueError):","                # If not serializable, convert to string representation","                out[k] = str(v)","        # Other types: convert to string (avoid JSON crash)","        else:","            out[k] = str(v)","    ","    return out","",""]}
{"type":"file_footer","path":"src/core/config_snapshot.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/dimensions.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1613,"sha256":"69620ecdf2bad598b87e1bae34a80603ededcad70adb58ea14802c9c2877c584","total_lines":63,"chunk_count":1}
{"type":"file_chunk","path":"src/core/dimensions.py","chunk_index":0,"line_start":1,"line_end":63,"content":["","\"\"\"","穩定的維度查詢介面","","提供 get_dimension_for_dataset() 函數，用於查詢商品的維度定義（交易時段、交易所等）。","此模組使用 lazy loading 避免 import-time IO，並提供 deterministic 結果。","\"\"\"","","from __future__ import annotations","","from functools import lru_cache","from typing import Optional","","from contracts.dimensions import InstrumentDimension","from contracts.dimensions_loader import load_dimension_registry","","","@lru_cache(maxsize=1)","def _get_cached_registry():","    \"\"\"","    快取註冊表，避免重複讀取檔案","    ","    使用 lru_cache(maxsize=1) 確保：","    1. 第一次呼叫時讀取檔案","    2. 後續呼叫重用快取","    3. 避免 import-time IO","    \"\"\"","    return load_dimension_registry()","","","def get_dimension_for_dataset(","    dataset_id: str, ","    *, ","    symbol: str | None = None",") -> InstrumentDimension | None:","    \"\"\"","    查詢資料集的維度定義","    ","    Args:","        dataset_id: 資料集 ID，例如 \"CME.MNQ.60m.2020-2024\"","        symbol: 可選的商品符號，例如 \"CME.MNQ\"","    ","    Returns:","        InstrumentDimension 或 None（如果找不到）","    ","    Note:","        - 純讀取操作，無副作用（除了第一次呼叫時的檔案讀取）","        - 結果是 deterministic 的","        - 使用 lazy loading，避免 import-time IO","    \"\"\"","    registry = _get_cached_registry()","    return registry.get(dataset_id, symbol)","","","def clear_dimension_cache() -> None:","    \"\"\"","    清除維度快取","    ","    主要用於測試，或需要強制重新讀取註冊表的情況","    \"\"\"","    _get_cached_registry.cache_clear()","",""]}
{"type":"file_footer","path":"src/core/dimensions.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/feature_bundle.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5797,"sha256":"253374011c2febcd92c6498c2bd0e69147d594359f80471259e30b8a1777ddbf","total_lines":187,"chunk_count":1}
{"type":"file_chunk","path":"src/core/feature_bundle.py","chunk_index":0,"line_start":1,"line_end":187,"content":["","\"\"\"","FeatureBundle：engine/wfs 的統一輸入","","提供 frozen dataclass 結構，確保特徵資料的不可變性與型別安全。","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass","from typing import Dict, Tuple, Any","import numpy as np","","","@dataclass(frozen=True)","class FeatureSeries:","    \"\"\"","    單一特徵時間序列","    ","    Attributes:","        ts: 時間戳記陣列，dtype 必須是 datetime64[s]","        values: 特徵值陣列，dtype 必須是 float64","        name: 特徵名稱","        timeframe_min: timeframe 分鐘數","    \"\"\"","    ts: np.ndarray  # datetime64[s]","    values: np.ndarray  # float64","    name: str","    timeframe_min: int","    ","    def __post_init__(self):","        \"\"\"驗證資料型別與一致性\"\"\"","        # 驗證 ts dtype","        if not np.issubdtype(self.ts.dtype, np.datetime64):","            raise TypeError(f\"ts 必須是 datetime64，實際為 {self.ts.dtype}\")","        ","        # 驗證 values dtype","        if not np.issubdtype(self.values.dtype, np.floating):","            raise TypeError(f\"values 必須是浮點數，實際為 {self.values.dtype}\")","        ","        # 驗證長度一致","        if len(self.ts) != len(self.values):","            raise ValueError(","                f\"ts 與 values 長度不一致: ts={len(self.ts)}, values={len(self.values)}\"","            )","        ","        # 驗證 timeframe 為正整數","        if not isinstance(self.timeframe_min, int) or self.timeframe_min <= 0:","            raise ValueError(f\"timeframe_min 必須為正整數: {self.timeframe_min}\")","        ","        # 驗證名稱非空","        if not self.name:","            raise ValueError(\"name 不能為空\")","","","@dataclass(frozen=True)","class FeatureBundle:","    \"\"\"","    特徵資料包","    ","    包含一個資料集的所有特徵時間序列，以及相關 metadata。","    ","    Attributes:","        dataset_id: 資料集 ID","        season: 季節標記","        series: 特徵序列字典，key 為 (name, timeframe_min)","        meta: metadata 字典，包含 manifest hashes, breaks_policy, ts_dtype 等","    \"\"\"","    dataset_id: str","    season: str","    series: Dict[Tuple[str, int], FeatureSeries]","    meta: Dict[str, Any]","    ","    def __post_init__(self):","        \"\"\"驗證 bundle 一致性\"\"\"","        # 驗證 dataset_id 與 season 非空","        if not self.dataset_id:","            raise ValueError(\"dataset_id 不能為空\")","        if not self.season:","            raise ValueError(\"season 不能為空\")","        ","        # 驗證 meta 包含必要欄位","        required_meta_keys = {\"ts_dtype\", \"breaks_policy\"}","        missing_keys = required_meta_keys - set(self.meta.keys())","        if missing_keys:","            raise ValueError(f\"meta 缺少必要欄位: {missing_keys}\")","        ","        # 驗證 ts_dtype","        if self.meta[\"ts_dtype\"] != \"datetime64[s]\":","            raise ValueError(f\"ts_dtype 必須為 'datetime64[s]'，實際為 {self.meta['ts_dtype']}\")","        ","        # 驗證 breaks_policy","        if self.meta[\"breaks_policy\"] != \"drop\":","            raise ValueError(f\"breaks_policy 必須為 'drop'，實際為 {self.meta['breaks_policy']}\")","        ","        # 驗證所有 series 的 ts dtype 一致","        for (name, tf), series in self.series.items():","            if not np.issubdtype(series.ts.dtype, np.datetime64):","                raise TypeError(","                    f\"series ({name}, {tf}) 的 ts dtype 必須為 datetime64，實際為 {series.ts.dtype}\"","                )","    ","    def get_series(self, name: str, timeframe_min: int) -> FeatureSeries:","        \"\"\"","        取得特定特徵序列","        ","        Args:","            name: 特徵名稱","            timeframe_min: timeframe 分鐘數","        ","        Returns:","            FeatureSeries 實例","        ","        Raises:","            KeyError: 特徵不存在","        \"\"\"","        key = (name, timeframe_min)","        if key not in self.series:","            raise KeyError(f\"特徵不存在: {name}@{timeframe_min}m\")","        return self.series[key]","    ","    def has_series(self, name: str, timeframe_min: int) -> bool:","        \"\"\"","        檢查是否包含特定特徵序列","        ","        Args:","            name: 特徵名稱","            timeframe_min: timeframe 分鐘數","        ","        Returns:","            bool","        \"\"\"","        return (name, timeframe_min) in self.series","    ","    def list_series(self) -> list[Tuple[str, int]]:","        \"\"\"","        列出所有特徵序列的 (name, timeframe) 對","        ","        Returns:","            排序後的 (name, timeframe) 列表","        \"\"\"","        return sorted(self.series.keys())","    ","    def validate_against_requirements(","        self,","        required: list[Tuple[str, int]],","        optional: list[Tuple[str, int]] = None,","    ) -> bool:","        \"\"\"","        驗證 bundle 是否滿足需求","        ","        Args:","            required: 必需的特徵列表，每個元素為 (name, timeframe)","            optional: 可選的特徵列表（預設為空）","        ","        Returns:","            bool: 是否滿足所有必需特徵","        ","        Raises:","            ValueError: 參數無效","        \"\"\"","        if optional is None:","            optional = []","        ","        # 檢查必需特徵","        for name, tf in required:","            if not self.has_series(name, tf):","                return False","        ","        return True","    ","    def to_dict(self) -> Dict[str, Any]:","        \"\"\"","        轉換為字典表示（僅 metadata，不包含大型陣列）","        ","        Returns:","            字典包含 bundle 的基本資訊","        \"\"\"","        return {","            \"dataset_id\": self.dataset_id,","            \"season\": self.season,","            \"series_count\": len(self.series),","            \"series_keys\": self.list_series(),","            \"meta\": self.meta,","        }","",""]}
{"type":"file_footer","path":"src/core/feature_bundle.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/features.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8394,"sha256":"578766ed71ca9434eeb9d97e45e73da2331eb4a3f654d865f540ba11edcfa52a","total_lines":302,"chunk_count":2}
{"type":"file_chunk","path":"src/core/features.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Feature 計算核心","","提供 deterministic numpy 實作，禁止 pandas rolling。","所有計算必須與 FULL/INCREMENTAL 模式完全一致。","\"\"\"","","from __future__ import annotations","","import numpy as np","from typing import Dict, Literal, Optional","from datetime import datetime","","from contracts.features import FeatureRegistry, FeatureSpec","from core.resampler import SessionSpecTaipei","","","def compute_atr_14(","    o: np.ndarray,","    h: np.ndarray,","    l: np.ndarray,","    c: np.ndarray,",") -> np.ndarray:","    \"\"\"","    計算 ATR(14)（Average True Range）","    ","    公式：","    TR = max(high - low, abs(high - prev_close), abs(low - prev_close))","    ATR = rolling mean of TR with window=14 (population std, ddof=0)","    ","    前 13 根 bar 的 ATR 為 NaN（因為 window 不足）","    ","    Args:","        o: open 價格（未使用）","        h: high 價格","        l: low 價格","        c: close 價格","        ","    Returns:","        ATR(14) 陣列，與輸入長度相同","    \"\"\"","    n = len(c)","    if n == 0:","        return np.array([], dtype=np.float64)","    ","    # 計算 True Range","    tr = np.empty(n, dtype=np.float64)","    ","    # 第一根 bar 的 TR = high - low","    tr[0] = h[0] - l[0]","    ","    # 後續 bar 的 TR","    for i in range(1, n):","        hl = h[i] - l[i]","        hc = abs(h[i] - c[i-1])","        lc = abs(l[i] - c[i-1])","        tr[i] = max(hl, hc, lc)","    ","    # 計算 rolling mean with window=14 (population std, ddof=0)","    # 使用 cumulative sums 確保 deterministic","    atr = np.full(n, np.nan, dtype=np.float64)","    ","    if n >= 14:","        # 計算 cumulative sum of TR","        cumsum = np.cumsum(tr, dtype=np.float64)","        ","        # 計算 rolling mean","        for i in range(13, n):","            if i == 13:","                window_sum = cumsum[i]","            else:","                window_sum = cumsum[i] - cumsum[i-14]","            ","            atr[i] = window_sum / 14.0","    ","    return atr","","","def compute_returns(","    c: np.ndarray,","    method: str = \"log\",",") -> np.ndarray:","    \"\"\"","    計算 returns","    ","    公式：","    - log: r = log(close).diff()","    - simple: r = (close - prev_close) / prev_close","    ","    第一根 bar 的 return 為 NaN","    ","    Args:","        c: close 價格","        method: 計算方法，\"log\" 或 \"simple\"","        ","    Returns:","        returns 陣列，與輸入長度相同","    \"\"\"","    n = len(c)","    if n <= 1:","        return np.full(n, np.nan, dtype=np.float64)","    ","    ret = np.full(n, np.nan, dtype=np.float64)","    ","    if method == \"log\":","        # log returns: r = log(close).diff()","        log_c = np.log(c)","        ret[1:] = np.diff(log_c)","    else:","        # simple returns: r = (close - prev_close) / prev_close","        ret[1:] = (c[1:] - c[:-1]) / c[:-1]","    ","    return ret","","","def compute_rolling_z(","    x: np.ndarray,","    window: int,",") -> np.ndarray:","    \"\"\"","    計算 rolling z-score（population std, ddof=0）","    ","    公式：","    mean = (sum_x[i] - sum_x[i-window]) / window","    var = (sum_x2[i] - sum_x2[i-window]) / window - mean^2","    std = sqrt(max(var, 0))  # 防浮點負數","    z = (x - mean) / std","    ","    前 window-1 根 bar 的 z-score 為 NaN","    std == 0 時，z = NaN（而不是 0）","    ","    Args:","        x: 輸入數值陣列","        window: 滾動視窗大小","        ","    Returns:","        z-score 陣列，與輸入長度相同","    \"\"\"","    n = len(x)","    if n == 0 or window <= 1:","        return np.full(n, np.nan, dtype=np.float64)","    ","    # 初始化結果為 NaN","    z = np.full(n, np.nan, dtype=np.float64)","    ","    # 計算 cumulative sums","    cumsum = np.cumsum(x, dtype=np.float64)","    cumsum2 = np.cumsum(x * x, dtype=np.float64)","    ","    # 計算 rolling z-score","    for i in range(window - 1, n):","        # 計算視窗內的 sum 和 sum of squares","        if i == window - 1:","            sum_x = cumsum[i]","            sum_x2 = cumsum2[i]","        else:","            sum_x = cumsum[i] - cumsum[i - window]","            sum_x2 = cumsum2[i] - cumsum2[i - window]","        ","        # 計算 mean 和 variance","        mean = sum_x / window","        var = (sum_x2 / window) - (mean * mean)","        ","        # 防浮點負數","        if var < 0:","            var = 0.0","        ","        std = np.sqrt(var)","        ","        # 計算 z-score","        if std == 0:","            # std == 0 時，z = NaN（而不是 0）","            z[i] = np.nan","        else:","            z[i] = (x[i] - mean) / std","    ","    return z","","","def compute_session_vwap(","    ts: np.ndarray,","    c: np.ndarray,","    v: np.ndarray,","    session_spec: SessionSpecTaipei,","    breaks_policy: str = \"drop\",",") -> np.ndarray:","    \"\"\"","    計算 session VWAP（Volume Weighted Average Price）","    ","    每個 session 獨立計算 VWAP，並將該 session 內的所有 bar 賦予相同的 VWAP 值。","    ","    Args:","        ts: 時間戳記陣列（datetime64[s]）","        c: close 價格陣列","        v: volume 陣列","        session_spec: session 規格","        breaks_policy: break 處理策略（目前只支援 \"drop\"）","        ","    Returns:"]}
{"type":"file_chunk","path":"src/core/features.py","chunk_index":1,"line_start":201,"line_end":302,"content":["        session VWAP 陣列，與輸入長度相同","    \"\"\"","    n = len(ts)","    if n == 0:","        return np.array([], dtype=np.float64)","    ","    # 初始化結果為 NaN","    vwap = np.full(n, np.nan, dtype=np.float64)","    ","    # 將 datetime64[s] 轉換為 pandas Timestamp 以便進行日期時間操作","    # 我們需要判斷每個 bar 屬於哪個 session","    # 由於這是 MVP，我們先實作簡單版本：假設所有 bar 都在同一個 session","    # 實際實作需要根據 session_spec 進行 session 分類","    # 但根據 Phase 3B 要求，我們先提供固定實作","    ","    # 簡單實作：計算整個時間範圍的 VWAP（所有 bar 視為同一個 session）","    # 這不是正確的 session VWAP，但符合 MVP 要求","    total_volume = np.sum(v)","    if total_volume > 0:","        weighted_sum = np.sum(c * v)","        overall_vwap = weighted_sum / total_volume","        vwap[:] = overall_vwap","    else:","        vwap[:] = np.nan","    ","    return vwap","","","def compute_features_for_tf(","    ts: np.ndarray,","    o: np.ndarray,","    h: np.ndarray,","    l: np.ndarray,","    c: np.ndarray,","    v: np.ndarray,","    tf_min: int,","    registry: FeatureRegistry,","    session_spec: SessionSpecTaipei,","    breaks_policy: str = \"drop\",",") -> Dict[str, np.ndarray]:","    \"\"\"","    計算指定 timeframe 的所有特徵","    ","    Args:","        ts: 時間戳記陣列（datetime64[s]），必須與 resampled bars 完全一致","        o: open 價格陣列","        h: high 價格陣列","        l: low 價格陣列","        c: close 價格陣列","        v: volume 陣列","        tf_min: timeframe 分鐘數","        registry: 特徵註冊表","        session_spec: session 規格","        breaks_policy: break 處理策略","        ","    Returns:","        特徵字典，keys 必須為：","        - ts: 與輸入 ts 相同的物件/值（datetime64[s]）","        - atr_14: float64","        - ret_z_200: float64","        - session_vwap: float64","        ","    Raises:","        ValueError: 輸入陣列長度不一致或 registry 缺少必要特徵","    \"\"\"","    # 驗證輸入長度","    n = len(ts)","    for arr, name in [(o, \"open\"), (h, \"high\"), (l, \"low\"), (c, \"close\"), (v, \"volume\")]:","        if len(arr) != n:","            raise ValueError(f\"輸入陣列長度不一致: {name} 長度為 {len(arr)}，但 ts 長度為 {n}\")","    ","    # 取得該 timeframe 的特徵規格","    specs = registry.specs_for_tf(tf_min)","    ","    # 建立結果字典","    result = {\"ts\": ts}  # ts 必須是相同的物件/值","    ","    # 計算每個特徵","    for spec in specs:","        if spec.name == \"atr_14\":","            result[\"atr_14\"] = compute_atr_14(o, h, l, c)","        elif spec.name == \"ret_z_200\":","            # 先計算 returns","            returns = compute_returns(c, method=\"log\")","            # 再計算 z-score","            result[\"ret_z_200\"] = compute_rolling_z(returns, window=200)","        elif spec.name == \"session_vwap\":","            result[\"session_vwap\"] = compute_session_vwap(","                ts, c, v, session_spec, breaks_policy","            )","        else:","            raise ValueError(f\"不支援的特徵名稱: {spec.name}\")","    ","    # 確保所有必要特徵都存在","    required_features = [\"atr_14\", \"ret_z_200\", \"session_vwap\"]","    for feat in required_features:","        if feat not in result:","            raise ValueError(f\"registry 缺少必要特徵: {feat}\")","    ","    return result","",""]}
{"type":"file_footer","path":"src/core/features.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/core/fingerprint.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7693,"sha256":"fc59265986b214d1125c00a7bafc22830c587d045bcb2a72c4e8d36e1d7f8426","total_lines":295,"chunk_count":2}
{"type":"file_chunk","path":"src/core/fingerprint.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Fingerprint 計算核心","","提供 canonical bytes 規則與指紋計算函數，確保 deterministic 結果。","\"\"\"","","from __future__ import annotations","","import hashlib","from datetime import datetime","from typing import Any, Dict, Iterable, List, Tuple","","import numpy as np","import pandas as pd","","from contracts.fingerprint import FingerprintIndex","from data.raw_ingest import RawIngestResult","","","def canonical_bar_line(","    ts: datetime,","    o: float,","    h: float,","    l: float,","    c: float,","    v: float",") -> str:","    \"\"\"","    將單一 bar 轉換為標準化字串","    ","    格式固定：YYYY-MM-DDTHH:MM:SS|{o:.4f}|{h:.4f}|{l:.4f}|{c:.4f}|{v:.0f}","    ","    Args:","        ts: 時間戳記","        o: 開盤價","        h: 最高價","        l: 最低價","        c: 收盤價","        v: 成交量","    ","    Returns:","        標準化字串","    \"\"\"","    # 格式化時間戳記","    ts_str = ts.strftime(\"%Y-%m-%dT%H:%M:%S\")","    ","    # 格式化價格（固定小數位數）","    # 使用 round 確保 deterministic，避免浮點數表示差異","    o_fmt = f\"{o:.4f}\"","    h_fmt = f\"{h:.4f}\"","    l_fmt = f\"{l:.4f}\"","    c_fmt = f\"{c:.4f}\"","    ","    # 格式化成交量（整數）","    v_fmt = f\"{v:.0f}\"","    ","    return f\"{ts_str}|{o_fmt}|{h_fmt}|{l_fmt}|{c_fmt}|{v_fmt}\"","","","def compute_day_hash(lines: List[str]) -> str:","    \"\"\"","    計算一日的 hash","    ","    將該日所有 bar 的標準化字串排序後連接，計算 SHA256。","    ","    Args:","        lines: 該日所有 bar 的標準化字串列表","    ","    Returns:","        SHA256 hex 字串","    \"\"\"","    if not lines:","        # 空日的 hash（理論上不應該發生）","        return hashlib.sha256(b\"\").hexdigest()","    ","    # 排序確保 deterministic","    sorted_lines = sorted(lines)","    ","    # 連接所有字串，以換行分隔","    content = \"\\n\".join(sorted_lines)","    ","    # 計算 SHA256","    return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()","","","def _parse_ts_str(ts_str: str) -> datetime:","    \"\"\"","    解析時間戳記字串","    ","    支援多種格式：","    - \"YYYY-MM-DD HH:MM:SS\"","    - \"YYYY/MM/DD HH:MM:SS\"","    - \"YYYY-MM-DDTHH:MM:SS\"","    \"\"\"","    # 嘗試常見格式","    formats = [","        \"%Y-%m-%d %H:%M:%S\",","        \"%Y/%m/%d %H:%M:%S\",","        \"%Y-%m-%dT%H:%M:%S\",","        \"%Y/%m/%dT%H:%M:%S\",","    ]","    ","    for fmt in formats:","        try:","            return datetime.strptime(ts_str, fmt)","        except ValueError:","            continue","    ","    # 如果都不匹配，嘗試使用 pandas 解析","    try:","        return pd.to_datetime(ts_str).to_pydatetime()","    except Exception as e:","        raise ValueError(f\"無法解析時間戳記: {ts_str}\") from e","","","def _group_bars_by_day(","    bars: Iterable[Tuple[datetime, float, float, float, float, float]]",") -> Dict[str, List[str]]:","    \"\"\"","    將 bars 按日期分組","    ","    Args:","        bars: (ts, o, h, l, c, v) 的迭代器","    ","    Returns:","        字典：日期字串 (YYYY-MM-DD) -> 該日所有 bar 的標準化字串列表","    \"\"\"","    day_groups: Dict[str, List[str]] = {}","    ","    for ts, o, h, l, c, v in bars:","        # 取得日期字串","        day_str = ts.strftime(\"%Y-%m-%d\")","        ","        # 建立標準化字串","        line = canonical_bar_line(ts, o, h, l, c, v)","        ","        # 加入對應日期的群組","        if day_str not in day_groups:","            day_groups[day_str] = []","        day_groups[day_str].append(line)","    ","    return day_groups","","","def build_fingerprint_index_from_bars(","    dataset_id: str,","    bars: Iterable[Tuple[datetime, float, float, float, float, float]],","    dataset_timezone: str = \"Asia/Taipei\",","    build_notes: str = \"\"",") -> FingerprintIndex:","    \"\"\"","    從 bars 建立指紋索引","    ","    Args:","        dataset_id: 資料集 ID","        bars: (ts, o, h, l, c, v) 的迭代器","        dataset_timezone: 時區","        build_notes: 建置備註","    ","    Returns:","        FingerprintIndex","    \"\"\"","    # 按日期分組","    day_groups = _group_bars_by_day(bars)","    ","    if not day_groups:","        raise ValueError(\"沒有 bars 資料\")","    ","    # 計算每日 hash","    day_hashes: Dict[str, str] = {}","    for day_str, lines in day_groups.items():","        day_hashes[day_str] = compute_day_hash(lines)","    ","    # 找出日期範圍","    sorted_days = sorted(day_hashes.keys())","    range_start = sorted_days[0]","    range_end = sorted_days[-1]","    ","    # 建立指紋索引","    return FingerprintIndex.create(","        dataset_id=dataset_id,","        range_start=range_start,","        range_end=range_end,","        day_hashes=day_hashes,","        dataset_timezone=dataset_timezone,","        build_notes=build_notes","    )","","","def build_fingerprint_index_from_raw_ingest(","    dataset_id: str,","    raw_ingest_result: RawIngestResult,","    dataset_timezone: str = \"Asia/Taipei\",","    build_notes: str = \"\"",") -> FingerprintIndex:","    \"\"\"","    從 RawIngestResult 建立指紋索引（便利函數）","    ","    Args:"]}
{"type":"file_chunk","path":"src/core/fingerprint.py","chunk_index":1,"line_start":201,"line_end":295,"content":["        dataset_id: 資料集 ID","        raw_ingest_result: RawIngestResult","        dataset_timezone: 時區","        build_notes: 建置備註","    ","    Returns:","        FingerprintIndex","    \"\"\"","    df = raw_ingest_result.df","    ","    # 準備 bars 迭代器","    bars = []","    for _, row in df.iterrows():","        try:","            ts = _parse_ts_str(row[\"ts_str\"])","            bars.append((","                ts,","                float(row[\"open\"]),","                float(row[\"high\"]),","                float(row[\"low\"]),","                float(row[\"close\"]),","                float(row[\"volume\"])","            ))","        except Exception as e:","            raise ValueError(f\"解析 bar 資料失敗: {e}\") from e","    ","    return build_fingerprint_index_from_bars(","        dataset_id=dataset_id,","        bars=bars,","        dataset_timezone=dataset_timezone,","        build_notes=build_notes","    )","","","def compare_fingerprint_indices(","    old_index: FingerprintIndex | None,","    new_index: FingerprintIndex",") -> Dict[str, Any]:","    \"\"\"","    比較兩個指紋索引，產生 diff 報告","    ","    Args:","        old_index: 舊索引（可為 None）","        new_index: 新索引","    ","    Returns:","        diff 報告字典","    \"\"\"","    if old_index is None:","        return {","            \"old_range_start\": None,","            \"old_range_end\": None,","            \"new_range_start\": new_index.range_start,","            \"new_range_end\": new_index.range_end,","            \"append_only\": False,","            \"append_range\": None,","            \"earliest_changed_day\": None,","            \"no_change\": False,","            \"is_new\": True,","        }","    ","    # 檢查是否完全相同","    if old_index.index_sha256 == new_index.index_sha256:","        return {","            \"old_range_start\": old_index.range_start,","            \"old_range_end\": old_index.range_end,","            \"new_range_start\": new_index.range_start,","            \"new_range_end\": new_index.range_end,","            \"append_only\": False,","            \"append_range\": None,","            \"earliest_changed_day\": None,","            \"no_change\": True,","            \"is_new\": False,","        }","    ","    # 檢查是否為 append-only","    append_only = old_index.is_append_only(new_index)","    append_range = old_index.get_append_range(new_index) if append_only else None","    ","    # 找出最早變更的日期","    earliest_changed_day = old_index.get_earliest_changed_day(new_index)","    ","    return {","        \"old_range_start\": old_index.range_start,","        \"old_range_end\": old_index.range_end,","        \"new_range_start\": new_index.range_start,","        \"new_range_end\": new_index.range_end,","        \"append_only\": append_only,","        \"append_range\": append_range,","        \"earliest_changed_day\": earliest_changed_day,","        \"no_change\": False,","        \"is_new\": False,","    }","",""]}
{"type":"file_footer","path":"src/core/fingerprint.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/core/governance/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":52,"sha256":"1636f9e7dff514f1e8478f9e199228d850fcf060871f78cfbd8942c29d69cd58","total_lines":4,"chunk_count":1}
{"type":"file_chunk","path":"src/core/governance/__init__.py","chunk_index":0,"line_start":1,"line_end":4,"content":["","\"\"\"Governance lifecycle and transition logic.\"\"\"","",""]}
{"type":"file_footer","path":"src/core/governance/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/governance/transition.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1835,"sha256":"e44a56de1509ecad7dd9b084246713d8f844a251358699299d44ef690b0f45dc","total_lines":62,"chunk_count":1}
{"type":"file_chunk","path":"src/core/governance/transition.py","chunk_index":0,"line_start":1,"line_end":62,"content":["","\"\"\"Governance lifecycle state transition logic.","","Pure functions for state transitions based on decisions.","\"\"\"","","from __future__ import annotations","","from core.schemas.governance import Decision, LifecycleState","","","def governance_transition(","    prev_state: LifecycleState,","    decision: Decision,",") -> LifecycleState:","    \"\"\"","    Compute next lifecycle state based on previous state and decision.","    ","    Transition rules:","    - INCUBATION + KEEP → CANDIDATE","    - INCUBATION + DROP → RETIRED","    - INCUBATION + FREEZE → INCUBATION (no change)","    - CANDIDATE + KEEP → LIVE","    - CANDIDATE + DROP → RETIRED","    - CANDIDATE + FREEZE → CANDIDATE (no change)","    - LIVE + KEEP → LIVE (no change)","    - LIVE + DROP → RETIRED","    - LIVE + FREEZE → LIVE (no change)","    - RETIRED + any → RETIRED (terminal state, no transitions)","    ","    Args:","        prev_state: Previous lifecycle state","        decision: Governance decision (KEEP/DROP/FREEZE)","        ","    Returns:","        Next lifecycle state","    \"\"\"","    # RETIRED is terminal state","    if prev_state == \"RETIRED\":","        return \"RETIRED\"","    ","    # State transition matrix","    transitions: dict[tuple[LifecycleState, Decision], LifecycleState] = {","        # INCUBATION transitions","        (\"INCUBATION\", Decision.KEEP): \"CANDIDATE\",","        (\"INCUBATION\", Decision.DROP): \"RETIRED\",","        (\"INCUBATION\", Decision.FREEZE): \"INCUBATION\",","        ","        # CANDIDATE transitions","        (\"CANDIDATE\", Decision.KEEP): \"LIVE\",","        (\"CANDIDATE\", Decision.DROP): \"RETIRED\",","        (\"CANDIDATE\", Decision.FREEZE): \"CANDIDATE\",","        ","        # LIVE transitions","        (\"LIVE\", Decision.KEEP): \"LIVE\",","        (\"LIVE\", Decision.DROP): \"RETIRED\",","        (\"LIVE\", Decision.FREEZE): \"LIVE\",","    }","    ","    return transitions.get((prev_state, decision), prev_state)","",""]}
{"type":"file_footer","path":"src/core/governance/transition.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/governance_schema.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2615,"sha256":"b9bce7623e57b1740c55bb4f2acce499cbe04233f48d86cf2fb59d9df2ba4521","total_lines":82,"chunk_count":1}
{"type":"file_chunk","path":"src/core/governance_schema.py","chunk_index":0,"line_start":1,"line_end":82,"content":["","\"\"\"Governance schema for decision tracking and auditability.","","Single Source of Truth (SSOT) for governance decisions.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, asdict","from typing import Any, Dict, List","","from core.schemas.governance import Decision","","","@dataclass(frozen=True)","class EvidenceRef:","    \"\"\"","    Reference to evidence used in governance decision.","    ","    Points to specific artifacts (run_id, stage, artifact paths, key metrics)","    that support the decision.","    \"\"\"","    run_id: str","    stage_name: str","    artifact_paths: List[str]  # Relative paths to artifacts (manifest.json, metrics.json, etc.)","    key_metrics: Dict[str, Any]  # Key metrics extracted from artifacts","","","@dataclass(frozen=True)","class GovernanceItem:","    \"\"\"","    Governance decision for a single candidate.","    ","    Each item represents a decision (KEEP/FREEZE/DROP) for one candidate","    parameter set, with reasons and evidence chain.","    \"\"\"","    candidate_id: str  # Stable identifier: strategy_id:params_hash[:12]","    decision: Decision","    reasons: List[str]  # Human-readable reasons for decision","    evidence: List[EvidenceRef]  # Evidence chain supporting decision","    created_at: str  # ISO8601 with Z suffix (UTC)","    git_sha: str  # Git SHA at time of governance evaluation","","","@dataclass(frozen=True)","class GovernanceReport:","    \"\"\"","    Complete governance report for a set of candidates.","    ","    Contains:","    - items: List of governance decisions for each candidate","    - metadata: Report-level metadata (governance_id, season, etc.)","    \"\"\"","    items: List[GovernanceItem]","    metadata: Dict[str, Any]  # Report metadata (governance_id, season, created_at, etc.)","    ","    def to_dict(self) -> Dict[str, Any]:","        \"\"\"Convert to dictionary for JSON serialization.\"\"\"","        return {","            \"items\": [","                {","                    \"candidate_id\": item.candidate_id,","                    \"decision\": item.decision.value,","                    \"reasons\": item.reasons,","                    \"evidence\": [","                        {","                            \"run_id\": ev.run_id,","                            \"stage_name\": ev.stage_name,","                            \"artifact_paths\": ev.artifact_paths,","                            \"key_metrics\": ev.key_metrics,","                        }","                        for ev in item.evidence","                    ],","                    \"created_at\": item.created_at,","                    \"git_sha\": item.git_sha,","                }","                for item in self.items","            ],","            \"metadata\": self.metadata,","        }","",""]}
{"type":"file_footer","path":"src/core/governance_schema.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/governance_writer.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4768,"sha256":"9a4692ab16cece0df5074e47b600661475c0f1ac72fc2694ba9938d3c72e5c77","total_lines":144,"chunk_count":1}
{"type":"file_chunk","path":"src/core/governance_writer.py","chunk_index":0,"line_start":1,"line_end":144,"content":["","\"\"\"Governance writer for decision artifacts.","","Writes governance results to outputs directory with machine-readable JSON","and human-readable README.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Any, Dict","","from core.governance_schema import GovernanceReport","from core.schemas.governance import Decision","from core.run_id import make_run_id","","","def write_governance_artifacts(","    governance_dir: Path,","    report: GovernanceReport,",") -> None:","    \"\"\"","    Write governance artifacts to directory.","    ","    Creates:","    - governance.json: Machine-readable governance report","    - README.md: Human-readable summary","    - evidence_index.json: Optional evidence index (recommended)","    ","    Args:","        governance_dir: Path to governance directory (will be created if needed)","        report: GovernanceReport to write","    \"\"\"","    governance_dir.mkdir(parents=True, exist_ok=True)","    ","    # Write governance.json (machine-readable SSOT)","    governance_dict = report.to_dict()","    governance_path = governance_dir / \"governance.json\"","    with governance_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(","            governance_dict,","            f,","            ensure_ascii=False,","            sort_keys=True,","            indent=2,","        )","        f.write(\"\\n\")","    ","    # Write README.md (human-readable summary)","    readme_lines = [","        \"# Governance Report\",","        \"\",","        f\"- governance_id: {report.metadata.get('governance_id')}\",","        f\"- season: {report.metadata.get('season')}\",","        f\"- created_at: {report.metadata.get('created_at')}\",","        f\"- git_sha: {report.metadata.get('git_sha')}\",","        \"\",","        \"## Decision Summary\",","        \"\",","    ]","    ","    decisions = report.metadata.get(\"decisions\", {})","    readme_lines.extend([","        f\"- KEEP: {decisions.get('KEEP', 0)}\",","        f\"- FREEZE: {decisions.get('FREEZE', 0)}\",","        f\"- DROP: {decisions.get('DROP', 0)}\",","        \"\",","    ])","    ","    # List FREEZE reasons (concise)","    freeze_items = [item for item in report.items if item.decision is Decision.FREEZE]","    if freeze_items:","        readme_lines.extend([","            \"## FREEZE Reasons\",","            \"\",","        ])","        for item in freeze_items:","            reasons_str = \"; \".join(item.reasons)","            readme_lines.append(f\"- {item.candidate_id}: {reasons_str}\")","        readme_lines.append(\"\")","    ","    # Subsample/params_effective summary","    readme_lines.extend([","        \"## Subsample & Params Effective\",","        \"\",","    ])","    ","    # Extract subsample info from evidence","    subsample_info: Dict[str, Any] = {}","    for item in report.items:","        for ev in item.evidence:","            stage = ev.stage_name","            if stage not in subsample_info:","                subsample_info[stage] = {}","            metrics = ev.key_metrics","            if \"stage_planned_subsample\" in metrics:","                subsample_info[stage][\"stage_planned_subsample\"] = metrics[\"stage_planned_subsample\"]","            if \"param_subsample_rate\" in metrics:","                subsample_info[stage][\"param_subsample_rate\"] = metrics[\"param_subsample_rate\"]","            if \"params_effective\" in metrics:","                subsample_info[stage][\"params_effective\"] = metrics[\"params_effective\"]","    ","    for stage, info in subsample_info.items():","        readme_lines.append(f\"### {stage}\")","        if \"stage_planned_subsample\" in info:","            readme_lines.append(f\"- stage_planned_subsample: {info['stage_planned_subsample']}\")","        if \"param_subsample_rate\" in info:","            readme_lines.append(f\"- param_subsample_rate: {info['param_subsample_rate']}\")","        if \"params_effective\" in info:","            readme_lines.append(f\"- params_effective: {info['params_effective']}\")","        readme_lines.append(\"\")","    ","    readme = \"\\n\".join(readme_lines)","    readme_path = governance_dir / \"README.md\"","    readme_path.write_text(readme, encoding=\"utf-8\")","    ","    # Write evidence_index.json (optional but recommended)","    evidence_index = {","        \"governance_id\": report.metadata.get(\"governance_id\"),","        \"evidence_by_candidate\": {","            item.candidate_id: [","                {","                    \"run_id\": ev.run_id,","                    \"stage_name\": ev.stage_name,","                    \"artifact_paths\": ev.artifact_paths,","                }","                for ev in item.evidence","            ]","            for item in report.items","        },","    }","    evidence_index_path = governance_dir / \"evidence_index.json\"","    with evidence_index_path.open(\"w\", encoding=\"utf-8\") as f:","        json.dump(","            evidence_index,","            f,","            ensure_ascii=False,","            sort_keys=True,","            indent=2,","        )","        f.write(\"\\n\")","",""]}
{"type":"file_footer","path":"src/core/governance_writer.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/intents.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12266,"sha256":"3388690af563a3b85f83a4a4b2cf775e212da3ecc33d4dc9dfd5d834b32f1f19","total_lines":369,"chunk_count":2}
{"type":"file_chunk","path":"src/core/intents.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Intent-based state machine models for Attack #9 – Headless Intent-State Contract.","","Defines UserIntent objects that UI may create. All intents must go through a single","ActionQueue, backend execution must be single-consumer sequential, backend outputs","only read-only SystemState snapshots. All side effects must happen only inside StateProcessor.","\"\"\"","","from __future__ import annotations","","import uuid","from datetime import datetime, date","from enum import Enum","from typing import Any, Dict, List, Optional, Union","from pydantic import BaseModel, ConfigDict, Field, field_validator, model_validator","","","class IntentType(str, Enum):","    \"\"\"Types of user intents.\"\"\"","    CREATE_JOB = \"create_job\"","    CALCULATE_UNITS = \"calculate_units\"","    CHECK_SEASON = \"check_season\"","    GET_JOB_STATUS = \"get_job_status\"","    LIST_JOBS = \"list_jobs\"","    GET_JOB_LOGS = \"get_job_logs\"","    SUBMIT_BATCH = \"submit_batch\"","    VALIDATE_PAYLOAD = \"validate_payload\"","    BUILD_PARQUET = \"build_parquet\"","    FREEZE_SEASON = \"freeze_season\"","    EXPORT_SEASON = \"export_season\"","    COMPARE_SEASONS = \"compare_seasons\"","","","class IntentStatus(str, Enum):","    \"\"\"Status of an intent in the queue.\"\"\"","    PENDING = \"pending\"","    PROCESSING = \"processing\"","    COMPLETED = \"completed\"","    FAILED = \"failed\"","    DUPLICATE = \"duplicate\"  # Idempotency: duplicate intent detected","","","class UserIntent(BaseModel):","    \"\"\"Base class for all user intents.","    ","    UI may only create UserIntent objects. All intents must go through a single","    ActionQueue, backend execution must be single-consumer sequential.","    \"\"\"","    model_config = ConfigDict(frozen=False, extra=\"forbid\")","    ","    # Core intent metadata","    intent_id: str = Field(default_factory=lambda: str(uuid.uuid4()))","    intent_type: IntentType","    created_at: datetime = Field(default_factory=datetime.now)","    created_by: str = Field(default=\"ui\")  # Could be user_id, session_id, etc.","    ","    # Idempotency key: if two intents have same idempotency_key, second is duplicate","    idempotency_key: Optional[str] = Field(default=None)","    ","    # Processing metadata (set by ActionQueue/StateProcessor)","    status: IntentStatus = Field(default=IntentStatus.PENDING)","    processed_at: Optional[datetime] = None","    error_message: Optional[str] = None","    result: Optional[Dict[str, Any]] = None","    ","    @model_validator(mode=\"after\")","    def validate_idempotency_key(self) -> UserIntent:","        \"\"\"Generate idempotency key if not provided.","        ","        Subclasses should override with more specific logic.","        If no subclass sets idempotency_key, it remains None.","        \"\"\"","        # Base class does nothing; subclasses should set idempotency_key","        return self","","","# Concrete intent models for specific user actions","","class DataSpecIntent(BaseModel):","    \"\"\"Data specification for job creation intents.\"\"\"","    model_config = ConfigDict(frozen=True)","    ","    dataset_id: str","    symbols: List[str]","    timeframes: List[str]","    start_date: Optional[date] = None","    end_date: Optional[date] = None","    ","    @field_validator(\"symbols\", \"timeframes\")","    @classmethod","    def validate_non_empty_lists(cls, v: List[str]) -> List[str]:","        \"\"\"Ensure lists are not empty.\"\"\"","        if not v:","            raise ValueError(\"List cannot be empty\")","        return v","","","class CreateJobIntent(UserIntent):","    \"\"\"Intent to create a new job from wizard payload.\"\"\"","    model_config = ConfigDict(frozen=False)","    ","    intent_type: IntentType = Field(default=IntentType.CREATE_JOB)","    ","    # Job creation payload","    season: str","    data1: DataSpecIntent","    data2: Optional[DataSpecIntent] = None","    strategy_id: str","    params: Dict[str, Any]","    wfs: Dict[str, Any] = Field(default_factory=lambda: {","        \"stage0_subsample\": 0.1,","        \"top_k\": 20,","        \"mem_limit_mb\": 8192,","        \"allow_auto_downsample\": True","    })","    ","    @model_validator(mode=\"after\")","    def set_idempotency_key(self) -> CreateJobIntent:","        \"\"\"Set idempotency key based on job creation parameters.","        ","        Only sets idempotency_key if not already provided.","        \"\"\"","        if self.idempotency_key is None:","            # Create deterministic hash of job parameters","            import hashlib","            import json","            ","            key_data = {","                \"season\": self.season,","                \"data1_dataset\": self.data1.dataset_id,","                \"data1_symbols\": sorted(self.data1.symbols),","                \"data1_timeframes\": sorted(self.data1.timeframes),","                \"strategy_id\": self.strategy_id,","                \"params_hash\": hashlib.sha256(","                    json.dumps(self.params, sort_keys=True).encode()","                ).hexdigest()[:16]","            }","            ","            key_str = json.dumps(key_data, sort_keys=True)","            self.idempotency_key = f\"create_job:{hashlib.sha256(key_str.encode()).hexdigest()[:32]}\"","        return self","","","class CalculateUnitsIntent(UserIntent):","    \"\"\"Intent to calculate units for a wizard payload.\"\"\"","    model_config = ConfigDict(frozen=False)","    ","    intent_type: IntentType = Field(default=IntentType.CALCULATE_UNITS)","    ","    # Same payload as CreateJobIntent but without WFS","    season: str","    data1: DataSpecIntent","    data2: Optional[DataSpecIntent] = None","    strategy_id: str","    params: Dict[str, Any]","    ","    @model_validator(mode=\"after\")","    def set_idempotency_key(self) -> CalculateUnitsIntent:","        \"\"\"Set idempotency key based on calculation parameters.\"\"\"","        import hashlib","        import json","        ","        key_data = {","            \"type\": \"calculate_units\",","            \"season\": self.season,","            \"data1_dataset\": self.data1.dataset_id,","            \"data1_symbols\": sorted(self.data1.symbols),","            \"data1_timeframes\": sorted(self.data1.timeframes),","            \"strategy_id\": self.strategy_id,","            \"params_hash\": hashlib.sha256(","                json.dumps(self.params, sort_keys=True).encode()","            ).hexdigest()[:16]","        }","        ","        key_str = json.dumps(key_data, sort_keys=True)","        self.idempotency_key = f\"calculate_units:{hashlib.sha256(key_str.encode()).hexdigest()[:32]}\"","        return self","","","class CheckSeasonIntent(UserIntent):","    \"\"\"Intent to check if a season is frozen.\"\"\"","    model_config = ConfigDict(frozen=False)","    ","    intent_type: IntentType = Field(default=IntentType.CHECK_SEASON)","    ","    season: str","    action: str = Field(default=\"submit_job\")","    ","    @model_validator(mode=\"after\")","    def set_idempotency_key(self) -> CheckSeasonIntent:","        \"\"\"Set idempotency key based on season and action.\"\"\"","        self.idempotency_key = f\"check_season:{self.season}:{self.action}\"","        return self","","","class GetJobStatusIntent(UserIntent):","    \"\"\"Intent to get job status with units progress.\"\"\"","    model_config = ConfigDict(frozen=False)","    ","    intent_type: IntentType = Field(default=IntentType.GET_JOB_STATUS)","    "]}
{"type":"file_chunk","path":"src/core/intents.py","chunk_index":1,"line_start":201,"line_end":369,"content":["    job_id: str","    ","    @model_validator(mode=\"after\")","    def set_idempotency_key(self) -> GetJobStatusIntent:","        \"\"\"Set idempotency key based on job_id.\"\"\"","        self.idempotency_key = f\"get_job_status:{self.job_id}\"","        return self","","","class ListJobsIntent(UserIntent):","    \"\"\"Intent to list jobs with progress.\"\"\"","    model_config = ConfigDict(frozen=False)","    ","    intent_type: IntentType = Field(default=IntentType.LIST_JOBS)","    ","    limit: int = Field(default=50, ge=1, le=1000)","    ","    @model_validator(mode=\"after\")","    def set_idempotency_key(self) -> ListJobsIntent:","        \"\"\"Set idempotency key based on limit.\"\"\"","        self.idempotency_key = f\"list_jobs:limit={self.limit}\"","        return self","","","class GetJobLogsIntent(UserIntent):","    \"\"\"Intent to get tail of job logs.\"\"\"","    model_config = ConfigDict(frozen=False)","    ","    intent_type: IntentType = Field(default=IntentType.GET_JOB_LOGS)","    ","    job_id: str","    lines: int = Field(default=50, ge=1, le=1000)","    ","    @model_validator(mode=\"after\")","    def set_idempotency_key(self) -> GetJobLogsIntent:","        \"\"\"Set idempotency key based on job_id and lines.\"\"\"","        self.idempotency_key = f\"get_job_logs:{self.job_id}:lines={self.lines}\"","        return self","","","class SubmitBatchIntent(UserIntent):","    \"\"\"Intent to submit a batch of jobs.\"\"\"","    model_config = ConfigDict(frozen=False)","    ","    intent_type: IntentType = Field(default=IntentType.SUBMIT_BATCH)","    ","    # Batch specification","    season: str","    template: Dict[str, Any]  # JobTemplate serialized","    ","    @model_validator(mode=\"after\")","    def set_idempotency_key(self) -> SubmitBatchIntent:","        \"\"\"Set idempotency key based on batch parameters.\"\"\"","        import hashlib","        import json","        ","        key_data = {","            \"type\": \"submit_batch\",","            \"season\": self.season,","            \"template_hash\": hashlib.sha256(","                json.dumps(self.template, sort_keys=True).encode()","            ).hexdigest()[:16]","        }","        ","        key_str = json.dumps(key_data, sort_keys=True)","        self.idempotency_key = f\"submit_batch:{hashlib.sha256(key_str.encode()).hexdigest()[:32]}\"","        return self","","","class ValidatePayloadIntent(UserIntent):","    \"\"\"Intent to validate wizard payload.\"\"\"","    model_config = ConfigDict(frozen=False)","    ","    intent_type: IntentType = Field(default=IntentType.VALIDATE_PAYLOAD)","    ","    payload: Dict[str, Any]","    ","    @model_validator(mode=\"after\")","    def set_idempotency_key(self) -> ValidatePayloadIntent:","        \"\"\"Set idempotency key based on payload hash.\"\"\"","        import hashlib","        import json","        ","        payload_hash = hashlib.sha256(","            json.dumps(self.payload, sort_keys=True).encode()","        ).hexdigest()[:32]","        self.idempotency_key = f\"validate_payload:{payload_hash}\"","        return self","","","class BuildParquetIntent(UserIntent):","    \"\"\"Intent to build Parquet files for a dataset.\"\"\"","    model_config = ConfigDict(frozen=False)","    ","    intent_type: IntentType = Field(default=IntentType.BUILD_PARQUET)","    ","    dataset_id: str","    ","    @model_validator(mode=\"after\")","    def set_idempotency_key(self) -> BuildParquetIntent:","        \"\"\"Set idempotency key based on dataset_id.\"\"\"","        self.idempotency_key = f\"build_parquet:{self.dataset_id}\"","        return self","","","class FreezeSeasonIntent(UserIntent):","    \"\"\"Intent to freeze a season.\"\"\"","    model_config = ConfigDict(frozen=False)","    ","    intent_type: IntentType = Field(default=IntentType.FREEZE_SEASON)","    ","    season: str","    reason: Optional[str] = None","    ","    @model_validator(mode=\"after\")","    def set_idempotency_key(self) -> FreezeSeasonIntent:","        \"\"\"Set idempotency key based on season.\"\"\"","        self.idempotency_key = f\"freeze_season:{self.season}\"","        return self","","","class ExportSeasonIntent(UserIntent):","    \"\"\"Intent to export season data.\"\"\"","    model_config = ConfigDict(frozen=False)","    ","    intent_type: IntentType = Field(default=IntentType.EXPORT_SEASON)","    ","    season: str","    format: str = Field(default=\"json\")  # json, csv, parquet","    ","    @model_validator(mode=\"after\")","    def set_idempotency_key(self) -> ExportSeasonIntent:","        \"\"\"Set idempotency key based on season and format.\"\"\"","        self.idempotency_key = f\"export_season:{self.season}:{self.format}\"","        return self","","","class CompareSeasonsIntent(UserIntent):","    \"\"\"Intent to compare two seasons.\"\"\"","    model_config = ConfigDict(frozen=False)","    ","    intent_type: IntentType = Field(default=IntentType.COMPARE_SEASONS)","    ","    season_a: str","    season_b: str","    metrics: List[str] = Field(default_factory=lambda: [\"sharpe\", \"max_dd\", \"win_rate\"])","    ","    @model_validator(mode=\"after\")","    def set_idempotency_key(self) -> CompareSeasonsIntent:","        \"\"\"Set idempotency key based on seasons and metrics.\"\"\"","        self.idempotency_key = f\"compare_seasons:{self.season_a}:{self.season_b}:{','.join(sorted(self.metrics))}\"","        return self","","","# Type alias for all concrete intent types","Intent = Union[","    CreateJobIntent,","    CalculateUnitsIntent,","    CheckSeasonIntent,","    GetJobStatusIntent,","    ListJobsIntent,","    GetJobLogsIntent,","    SubmitBatchIntent,","    ValidatePayloadIntent,","    BuildParquetIntent,","    FreezeSeasonIntent,","    ExportSeasonIntent,","    CompareSeasonsIntent,","]"]}
{"type":"file_footer","path":"src/core/intents.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/core/oom_cost_model.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4239,"sha256":"ec1a5f399cb50fef6fc0cdf53ebabb626eff692c5a51224768edaf494d30dcec","total_lines":153,"chunk_count":1}
{"type":"file_chunk","path":"src/core/oom_cost_model.py","chunk_index":0,"line_start":1,"line_end":153,"content":["","\"\"\"OOM cost model for memory and computation estimation.","","Provides conservative estimates for memory usage and operations","to enable OOM gate decisions before stage execution.","\"\"\"","","from __future__ import annotations","","from typing import Any, Dict","","import numpy as np","","","def _bytes_of_array(a: Any) -> int:","    \"\"\"","    Get bytes of numpy array.","    ","    Args:","        a: Array-like object","        ","    Returns:","        Number of bytes (0 if not ndarray)","    \"\"\"","    if isinstance(a, np.ndarray):","        return int(a.nbytes)","    return 0","","","def estimate_memory_bytes(","    cfg: Dict[str, Any],","    work_factor: float = 2.0,",") -> int:","    \"\"\"","    Estimate memory usage in bytes (conservative upper bound).","    ","    Memory estimation includes:","    - Price arrays: open/high/low/close (if present)","    - Params matrix: params_total * param_dim * 8 bytes (if present)","    - Working buffers: conservative multiplier (work_factor)","    ","    Note: This is a conservative estimate. Actual usage may be lower,","    but gate uses this to prevent OOM failures.","    ","    Args:","        cfg: Configuration dictionary containing:","            - bars: Number of bars","            - params_total: Total parameters","            - param_subsample_rate: Subsample rate","            - open_, high, low, close: Optional OHLC arrays","            - params_matrix: Optional parameter matrix","        work_factor: Conservative multiplier for working buffers (default: 2.0)","        ","    Returns:","        Estimated memory in bytes","    \"\"\"","    mem = 0","    ","    # Price arrays (if present)","    for k in (\"open_\", \"open\", \"high\", \"low\", \"close\"):","        mem += _bytes_of_array(cfg.get(k))","    ","    # Params matrix","    mem += _bytes_of_array(cfg.get(\"params_matrix\"))","    ","    # Conservative working buffers","    # Note: This is a conservative multiplier to account for:","    # - Intermediate computation buffers","    # - Indicator arrays (donchian, ATR, etc.)","    # - Intent arrays","    # - Fill arrays","    mem = int(mem * float(work_factor))","    ","    # Note: We do NOT reduce mem by subsample_rate here because:","    # 1. Some allocations are per-bar (not per-param)","    # 2. Working buffers may scale differently","    # 3. Conservative estimate is safer for OOM prevention","    ","    return mem","","","def estimate_ops(cfg: Dict[str, Any]) -> int:","    \"\"\"","    Estimate operations count (coarse approximation).","    ","    Baseline: per-bar per-effective-param operations.","    This is a coarse estimate for cost tracking.","    ","    Args:","        cfg: Configuration dictionary containing:","            - bars: Number of bars","            - params_total: Total parameters","            - param_subsample_rate: Subsample rate","            ","    Returns:","        Estimated operations count","    \"\"\"","    bars = int(cfg.get(\"bars\", 0))","    params_total = int(cfg.get(\"params_total\", 0))","    subsample_rate = float(cfg.get(\"param_subsample_rate\", 1.0))","    ","    # Effective params after subsample (floor rule)","    params_effective = int(params_total * subsample_rate)","    ","    # Baseline: per-bar per-effective-param step (coarse)","    ops = int(bars * params_effective)","    ","    return ops","","","def estimate_time_s(cfg: Dict[str, Any]) -> float | None:","    \"\"\"","    Estimate execution time in seconds (optional).","    ","    This is a placeholder for future time estimation.","    Currently returns None.","    ","    Args:","        cfg: Configuration dictionary","        ","    Returns:","        Estimated time in seconds (None if not available)","    \"\"\"","    # Placeholder for future implementation","    return None","","","def summarize_estimates(cfg: Dict[str, Any]) -> Dict[str, Any]:","    \"\"\"","    Summarize all estimates in a JSON-serializable dict.","    ","    Args:","        cfg: Configuration dictionary","        ","    Returns:","        Dictionary with estimates:","        - mem_est_bytes: Memory estimate in bytes","        - mem_est_mb: Memory estimate in MB","        - ops_est: Operations estimate","        - time_est_s: Time estimate in seconds (None if not available)","    \"\"\"","    mem_b = estimate_memory_bytes(cfg)","    ops = estimate_ops(cfg)","    time_s = estimate_time_s(cfg)","    ","    return {","        \"mem_est_bytes\": mem_b,","        \"mem_est_mb\": mem_b / (1024.0 * 1024.0),","        \"ops_est\": ops,","        \"time_est_s\": time_s,","    }","",""]}
{"type":"file_footer","path":"src/core/oom_cost_model.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/core/oom_gate.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":14690,"sha256":"5e1e5e6e71fa74968cdc174738ac1fbd697536abf36e8b672346fa0b0144a8c0","total_lines":415,"chunk_count":3}
{"type":"file_chunk","path":"src/core/oom_gate.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"OOM gate decision maker.","","Pure functions for estimating memory usage and deciding PASS/BLOCK/AUTO_DOWNSAMPLE.","No engine dependencies, no file I/O - pure computation only.","","This module provides two APIs:","1. New API (for B5-C): estimate_bytes(), decide_gate() with Pydantic schemas","2. Legacy API (for pipeline/tests): decide_oom_action() with dict I/O","\"\"\"","","from __future__ import annotations","","from collections.abc import Mapping","from typing import Any, Dict, Literal, Optional","","import core.oom_cost_model as oom_cost_model","from core.schemas.oom_gate import OomGateDecision, OomGateInput","","OomAction = Literal[\"PASS\", \"BLOCK\", \"AUTO_DOWNSAMPLE\"]","","","def estimate_bytes(inp: OomGateInput) -> int:","    \"\"\"","    Estimate memory usage in bytes.","    ","    Formula (locked):","        estimated = bars * params * subsample * intents_per_bar * bytes_per_intent_est","    ","    Args:","        inp: OomGateInput with bars, params, param_subsample_rate, etc.","        ","    Returns:","        Estimated memory usage in bytes","    \"\"\"","    estimated = (","        inp.bars","        * inp.params","        * inp.param_subsample_rate","        * inp.intents_per_bar","        * inp.bytes_per_intent_est","    )","    return int(estimated)","","","def decide_gate(inp: OomGateInput) -> OomGateDecision:","    \"\"\"","    Decide OOM gate action: PASS, BLOCK, or AUTO_DOWNSAMPLE.","    ","    Rules (locked):","    - PASS: estimated <= ram_budget * 0.6","    - BLOCK: estimated > ram_budget * 0.9","    - AUTO_DOWNSAMPLE: otherwise, recommended_rate = (ram_budget * 0.6) / (bars * params * intents_per_bar * bytes_per_intent_est)","    ","    Args:","        inp: OomGateInput with configuration","        ","    Returns:","        OomGateDecision with decision and recommendations","    \"\"\"","    estimated = estimate_bytes(inp)","    ram_budget = inp.ram_budget_bytes","    ","    # Thresholds (locked)","    pass_threshold = ram_budget * 0.6","    block_threshold = ram_budget * 0.9","    ","    if estimated <= pass_threshold:","        return OomGateDecision(","            decision=\"PASS\",","            estimated_bytes=estimated,","            ram_budget_bytes=ram_budget,","            recommended_subsample_rate=None,","            notes=f\"Estimated {estimated:,} bytes <= {pass_threshold:,.0f} bytes (60% of budget)\",","        )","    ","    if estimated > block_threshold:","        return OomGateDecision(","            decision=\"BLOCK\",","            estimated_bytes=estimated,","            ram_budget_bytes=ram_budget,","            recommended_subsample_rate=None,","            notes=f\"Estimated {estimated:,} bytes > {block_threshold:,.0f} bytes (90% of budget) - BLOCKED\",","        )","    ","    # AUTO_DOWNSAMPLE: calculate recommended rate","    # recommended_rate = (ram_budget * 0.6) / (bars * params * intents_per_bar * bytes_per_intent_est)","    denominator = inp.bars * inp.params * inp.intents_per_bar * inp.bytes_per_intent_est","    if denominator > 0:","        recommended_rate = (ram_budget * 0.6) / denominator","        # Clamp to [0.0, 1.0]","        recommended_rate = max(0.0, min(1.0, recommended_rate))","    else:","        recommended_rate = 0.0","    ","    return OomGateDecision(","        decision=\"AUTO_DOWNSAMPLE\",","        estimated_bytes=estimated,","        ram_budget_bytes=ram_budget,","        recommended_subsample_rate=recommended_rate,","        notes=(","            f\"Estimated {estimated:,} bytes between {pass_threshold:,.0f} and {block_threshold:,.0f} \"","            f\"- recommended subsample rate: {recommended_rate:.4f}\"","        ),","    )","","","def _params_effective(params_total: int, rate: float) -> int:","    \"\"\"Calculate effective params with floor rule (at least 1).\"\"\"","    return max(1, int(params_total * rate))","","","def _estimate_bytes_legacy(cfg: Mapping[str, Any] | Dict[str, Any]) -> int:","    \"\"\"","    Estimate memory bytes using unified formula when keys are available.","    ","    Formula (locked): bars * params_total * param_subsample_rate * intents_per_bar * bytes_per_intent_est","    ","    Falls back to oom_cost_model.estimate_memory_bytes if keys are missing.","    ","    Args:","        cfg: Configuration dictionary","        ","    Returns:","        Estimated memory usage in bytes","    \"\"\"","    keys = (\"bars\", \"params_total\", \"param_subsample_rate\", \"intents_per_bar\", \"bytes_per_intent_est\")","    if all(k in cfg for k in keys):","        return int(","            int(cfg[\"bars\"])","            * int(cfg[\"params_total\"])","            * float(cfg[\"param_subsample_rate\"])","            * float(cfg[\"intents_per_bar\"])","            * int(cfg[\"bytes_per_intent_est\"])","        )","    # Fallback to cost model","    return int(oom_cost_model.estimate_memory_bytes(dict(cfg), work_factor=2.0))","","","def _estimate_ops(cfg: dict, *, params_effective: int) -> int:","    \"\"\"","    Safely estimate operations count.","    ","    Priority:","    1. Use oom_cost_model.estimate_ops if available (most consistent)","    2. Fallback to deterministic formula","    ","    Args:","        cfg: Configuration dictionary","        params_effective: Effective params count (already calculated)","        ","    Returns:","        Estimated operations count","    \"\"\"","    # If cost model has ops estimate, use it (most consistent)","    if hasattr(oom_cost_model, \"estimate_ops\"):","        return int(oom_cost_model.estimate_ops(cfg))","    if hasattr(oom_cost_model, \"estimate_ops_est\"):","        return int(oom_cost_model.estimate_ops_est(cfg))","    ","    # Fallback: at least stable and monotonic","    bars = int(cfg.get(\"bars\", 0))","    intents_per_bar = float(cfg.get(\"intents_per_bar\", 2.0))","    return int(bars * params_effective * intents_per_bar)","","","def decide_oom_action(","    cfg: Mapping[str, Any] | Dict[str, Any],","    *,","    mem_limit_mb: float,","    allow_auto_downsample: bool = True,","    auto_downsample_step: float = 0.5,","    auto_downsample_min: float = 0.02,","    work_factor: float = 2.0,",") -> Dict[str, Any]:","    \"\"\"","    Backward-compatible OOM gate used by funnel_runner + contract tests.","","    Returns a dict (schema-as-dict) consumed by pipeline and written to artifacts/README.","    This function NEVER mutates cfg - returns new_cfg in result dict.","    ","    Uses estimate_memory_bytes() from oom_cost_model (tests monkeypatch this).","    Must use module import (oom_cost_model.estimate_memory_bytes) for monkeypatch to work.","    ","    Algorithm: Monotonic step-based downsample search","    - If mem_est(original_subsample) <= limit → PASS","    - If over limit and allow_auto_downsample=False → BLOCK","    - If over limit and allow_auto_downsample=True:","      - Step-based search: cur * step (e.g., 0.5 → 0.25 → 0.125...)","      - Re-estimate mem_est at each candidate subsample","      - If mem_est <= limit → AUTO_DOWNSAMPLE with that subsample","      - If reach min_rate and still over limit → BLOCK","    ","    Args:","        cfg: Configuration dictionary with bars, params_total, param_subsample_rate, etc.","        mem_limit_mb: Memory limit in MB","        allow_auto_downsample: Whether to allow automatic downsample","        auto_downsample_step: Multiplier for each downsample step (default: 0.5, must be < 1.0)","        auto_downsample_min: Minimum subsample rate (default: 0.02)","        work_factor: Work factor for memory estimation (default: 2.0)"]}
