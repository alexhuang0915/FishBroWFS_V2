{"type":"meta","schema_version":2,"run_id":"20251227_195404Z","repo_root":"/home/fishbro/FishBroWFS_V2","snapshot_root":"SNAPSHOT","part":7,"parts":10,"created_at":"2025-12-27T19:54:04Z","generator":{"name":"dump_context","version":"vNext-balanced"},"policies":{"max_parts":10,"target_bytes_per_part":500000,"chunk_max_lines":200,"chunk_max_bytes":120000,"outputs_max_bytes_for_content":2000000,"outputs_policy":"manifest_only_for_big_files","forbid_file_truncated":true,"balance_parts":true,"estimated_total_bytes":3514686,"balance_buffer_ratio":1.08}}
{"type":"file_chunk","path":"tests/test_submit_returns_503_when_worker_missing.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    assert detail[\"error\"] == \"WORKER_UNAVAILABLE\"","    assert \"worker\" in detail","    assert \"action\" in detail","","","def test_batch_submit_returns_503_when_worker_missing(test_client_no_worker: TestClient) -> None:","    \"\"\"Test POST /jobs/batch returns 503 when worker is unavailable.\"\"\"","    from datetime import date","    ","    req = {","        \"jobs\": [","            {","                \"season\": \"test_season\",","                \"data1\": {","                    \"dataset_id\": \"test_dataset\",","                    \"start_date\": \"2020-01-01\",","                    \"end_date\": \"2024-12-31\"","                },","                \"data2\": None,","                \"strategy_id\": \"test_strategy\",","                \"params\": {},","                \"wfs\": {","                    \"stage0_subsample\": 1.0,","                    \"top_k\": 100,","                    \"mem_limit_mb\": 4096,","                    \"allow_auto_downsample\": True","                }","            }","        ]","    }","    ","    resp = test_client_no_worker.post(\"/jobs/batch\", json=req)","    ","    # Must return HTTP 503, not 500","    assert resp.status_code == 503, f\"Expected 503, got {resp.status_code}\"","    ","    # Must have proper JSON structure","    data = resp.json()","    assert \"detail\" in data","    detail = data[\"detail\"]","    ","    # Error message must mention worker (detail is a dict with \"message\" field)","    assert isinstance(detail, dict)","    assert \"message\" in detail","    assert \"worker\" in detail[\"message\"].lower(), f\"Error message should mention worker: {detail['message']}\"","    ","    # Should not be generic 500 error","    assert \"internal server error\" not in detail[\"message\"].lower()","    ","    # Check response structure matches our error format","    assert \"error\" in detail","    assert detail[\"error\"] == \"WORKER_UNAVAILABLE\"","","","def test_submit_job_succeeds_when_worker_running(test_client_with_worker: TestClient) -> None:","    \"\"\"Test POST /jobs succeeds when worker is available.\"\"\"","    req = {","        \"season\": \"test_season\",","        \"dataset_id\": \"test_dataset\",","        \"outputs_root\": \"outputs\",","        \"config_snapshot\": {\"bars\": 1000, \"params_total\": 100},","        \"config_hash\": \"abc123\",","        \"created_by\": \"b5c\",","    }","    ","    resp = test_client_with_worker.post(\"/jobs\", json=req)","    ","    # Should succeed (200 or 201)","    assert resp.status_code in (200, 201), f\"Expected success, got {resp.status_code}\"","    ","    data = resp.json()","    assert \"job_id\" in data","    assert isinstance(data[\"job_id\"], str)","","","def test_batch_submit_succeeds_when_worker_running(test_client_with_worker: TestClient) -> None:","    \"\"\"Test POST /jobs/batch succeeds when worker is available.\"\"\"","    from datetime import date","    ","    req = {","        \"jobs\": [","            {","                \"season\": \"test_season\",","                \"data1\": {","                    \"dataset_id\": \"test_dataset\",","                    \"start_date\": \"2020-01-01\",","                    \"end_date\": \"2024-12-31\"","                },","                \"data2\": None,","                \"strategy_id\": \"test_strategy\",","                \"params\": {},","                \"wfs\": {","                    \"stage0_subsample\": 1.0,","                    \"top_k\": 100,","                    \"mem_limit_mb\": 4096,","                    \"allow_auto_downsample\": True","                }","            }","        ]","    }","    ","    resp = test_client_with_worker.post(\"/jobs/batch\", json=req)","    ","    # Should succeed (200 or 201)","    assert resp.status_code in (200, 201), f\"Expected success, got {resp.status_code}\"","    ","    data = resp.json()","    assert \"batch_id\" in data","    assert isinstance(data[\"batch_id\"], str)","","","def test_worker_status_check_integration() -> None:","    \"\"\"Test that _check_worker_status function works correctly.\"\"\"","    from control.api import _check_worker_status","    from pathlib import Path","    ","    # Create a temporary DB path for testing","    db_path = Path(\"/tmp/test.db\")","    ","    # Mock the dependencies","    with patch('control.api.validate_pidfile') as mock_validate, \\","         patch('control.api.time.time') as mock_time, \\","         patch('control.api.Path.exists') as mock_exists, \\","         patch('control.api.Path.read_text') as mock_read_text, \\","         patch('control.api.Path.stat') as mock_stat:","        ","        # Test case 1: pidfile doesn't exist","        mock_exists.return_value = False","        result = _check_worker_status(db_path)","        assert not result[\"alive\"]","        assert result[\"reason\"] == \"pidfile missing\"","        ","        # Test case 2: pidfile exists but corrupted (read_text raises ValueError)","        mock_exists.return_value = True","        mock_validate.return_value = (True, \"\")  # pidfile is valid","        mock_read_text.side_effect = ValueError(\"invalid literal\")","        result = _check_worker_status(db_path)","        assert not result[\"alive\"]","        assert \"corrupted\" in result[\"reason\"]","        ","        # Test case 3: pidfile exists, validate_pidfile returns invalid","        mock_exists.return_value = True","        mock_validate.return_value = (False, \"pidfile stale\")","        # Clear any side effect from previous test","        mock_read_text.side_effect = None","        # read_text won't be called because validate_pidfile returns invalid","        result = _check_worker_status(db_path)","        assert not result[\"alive\"]","        assert result[\"reason\"] == \"pidfile stale\"","        ","        # Test case 4: pidfile exists, process alive, heartbeat stale","        mock_exists.return_value = True","        mock_read_text.side_effect = None  # Clear side effect","        mock_read_text.return_value = \"12345\"","        mock_validate.return_value = (True, \"\")","        # Mock stat for heartbeat file","        mock_stat_obj = MagicMock()","        mock_stat_obj.st_mtime = 1000.0","        mock_stat.return_value = mock_stat_obj","        mock_time.return_value = 2000.0  # Current time (1000 seconds later)","        result = _check_worker_status(db_path)","        assert result[\"alive\"]  # Process is alive","        assert result[\"last_heartbeat_age_sec\"] == 1000.0","        ","        # Test case 5: pidfile exists, process alive, heartbeat fresh","        mock_stat_obj = MagicMock()","        mock_stat_obj.st_mtime = 1995.0  # 5 seconds ago","        mock_stat.return_value = mock_stat_obj","        mock_time.return_value = 2000.0  # Current time","        result = _check_worker_status(db_path)","        assert result[\"alive\"]","        assert result[\"last_heartbeat_age_sec\"] == 5.0","","","def test_error_message_includes_diagnostics() -> None:","    \"\"\"Test that 503 error message includes diagnostic details.\"\"\"","    from control.api import require_worker_or_503","    from pathlib import Path","    import os","    ","    # Create a temporary DB path for testing","    db_path = Path(\"/tmp/test.db\")","    ","    # Mock _check_worker_status to return False with specific reason","    with patch('control.api._check_worker_status') as mock_check:","        mock_check.return_value = {","            \"alive\": False,","            \"pid\": None,","            \"last_heartbeat_age_sec\": None,","            \"reason\": \"pidfile missing\",","            \"expected_db\": str(db_path),","        }","        ","        # Ensure the environment variable does NOT allow spawn in tests","        original = os.environ.get(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\")","        os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = \"0\"","        try:","            # Should raise HTTPException with 503","            import fastapi","            try:"]}
{"type":"file_chunk","path":"tests/test_submit_returns_503_when_worker_missing.py","chunk_index":2,"line_start":401,"line_end":422,"content":["                require_worker_or_503(db_path)","                assert False, \"Should have raised HTTPException\"","            except fastapi.HTTPException as e:","                assert e.status_code == 503","                detail = e.detail","                # Check structure","                assert isinstance(detail, dict)","                assert \"error\" in detail","                assert detail[\"error\"] == \"WORKER_UNAVAILABLE\"","                assert \"worker\" in detail","                assert \"action\" in detail","                assert \"message\" in detail","                assert \"worker\" in detail[\"message\"].lower()","        finally:","            if original is not None:","                os.environ[\"FISHBRO_ALLOW_SPAWN_IN_TESTS\"] = original","            else:","                os.environ.pop(\"FISHBRO_ALLOW_SPAWN_IN_TESTS\", None)","","","if __name__ == \"__main__\":","    pytest.main([__file__, \"-v\"])"]}
{"type":"file_footer","path":"tests/test_submit_returns_503_when_worker_missing.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"tests/test_trigger_rate_param_subsample_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":14011,"sha256":"c65f24b8d49cc43ffbf10358f8152582df4105b0250e749d7a800027c5dd7a96","total_lines":347,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_trigger_rate_param_subsample_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Stage P2-3: Contract Tests for Param-subsample Trigger Rate","","Verifies that trigger_rate controls param subsampling:","- selected_params_count scales with trigger_rate","- intents_total scales approximately linearly with trigger_rate","- Workload reduction is effective","\"\"\"","from __future__ import annotations","","import numpy as np","import os","","from pipeline.runner_grid import run_grid","","","def test_selected_params_count_reasonable() -> None:","    \"\"\"","    Test that selected_params_count is reasonable for given trigger_rate.","    ","    With n_params=1000 and trigger_rate=0.05, we expect selected_params_count","    to be approximately 50 (allowing rounding error).","    \"\"\"","    # Ensure clean environment","    old_param_subsample_rate = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","    old_param_subsample_seed = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","    ","    try:","        n_bars = 500","        n_params = 1000","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)","            atr_len = 10 + (i % 5)","            stop_mult = 1.0 + (i % 3) * 0.5","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Set param_subsample_rate=0.05","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"0.05\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"","        ","        result = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify perf dict contains trigger rate info","        assert \"perf\" in result, \"perf must exist in run_grid result\"","        perf = result[\"perf\"]","        assert isinstance(perf, dict), \"perf must be a dict\"","        ","        selected_params_count = perf.get(\"selected_params_count\")","        param_subsample_rate_configured = perf.get(\"param_subsample_rate_configured\")","        selected_params_ratio = perf.get(\"selected_params_ratio\")","        ","        assert selected_params_count is not None, \"selected_params_count must exist\"","        assert param_subsample_rate_configured is not None, \"param_subsample_rate_configured must exist\"","        assert selected_params_ratio is not None, \"selected_params_ratio must exist\"","        ","        assert param_subsample_rate_configured == 0.05, f\"param_subsample_rate_configured should be 0.05, got {param_subsample_rate_configured}\"","        ","        # Contract: selected_params_count should be approximately 5% of n_params","        # Allow rounding error: [45, 55] for n_params=1000, rate=0.05","        assert 45 <= selected_params_count <= 55, (","            f\"selected_params_count ({selected_params_count}) should be approximately 50 \"","            f\"(5% of {n_params}), got {selected_params_count}\"","        )","        ","        # Contract: selected_params_ratio should match trigger_rate approximately","        expected_ratio = 0.05","        assert 0.04 <= selected_params_ratio <= 0.06, (","            f\"selected_params_ratio ({selected_params_ratio}) should be approximately \"","            f\"{expected_ratio}, got {selected_params_ratio}\"","        )","        ","        # Contract: metrics_rows_computed should equal selected_params_count","        metrics_rows_computed = perf.get(\"metrics_rows_computed\")","        assert metrics_rows_computed == selected_params_count, (","            f\"metrics_rows_computed ({metrics_rows_computed}) should equal \"","            f\"selected_params_count ({selected_params_count})\"","        )","        ","    finally:","        # Restore environment","        if old_param_subsample_rate is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = old_param_subsample_rate","        ","        if old_param_subsample_seed is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = old_param_subsample_seed","","","def test_intents_total_linear_scaling() -> None:","    \"\"\"","    Test that intents_total scales approximately linearly with trigger_rate.","    ","    This verifies workload reduction: when we run 5% of params, intents_total","    should be approximately 5% of baseline.","    \"\"\"","    # Ensure clean environment","    old_param_subsample_rate = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","    old_param_subsample_seed = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","    ","    try:","        n_bars = 500","        n_params = 200","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)","            atr_len = 10 + (i % 5)","            stop_mult = 1.0 + (i % 3) * 0.5","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Run A: param_subsample_rate=1.0 (baseline, all params)","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"1.0\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"","        ","        result_a = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Run B: param_subsample_rate=0.05 (5% of params)","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"0.05\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"  # Same seed for deterministic selection","        ","        result_b = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify perf dicts","        perf_a = result_a.get(\"perf\", {})","        perf_b = result_b.get(\"perf\", {})","        ","        assert isinstance(perf_a, dict), \"perf_a must be a dict\"","        assert isinstance(perf_b, dict), \"perf_b must be a dict\"","        ","        intents_total_a = perf_a.get(\"intents_total\")","        intents_total_b = perf_b.get(\"intents_total\")","        ","        assert intents_total_a is not None, \"intents_total_a must exist\"","        assert intents_total_b is not None, \"intents_total_b must exist\"","        ","        # Contract: intents_total_B should be <= intents_total_A * 0.07 (allowing overhead)","        # With 5% params, we expect approximately 5% workload, but allow up to 7% for overhead","        if intents_total_a > 0:"]}
{"type":"file_chunk","path":"tests/test_trigger_rate_param_subsample_contract.py","chunk_index":1,"line_start":201,"line_end":347,"content":["            ratio = intents_total_b / intents_total_a","            assert ratio <= 0.07, (","                f\"intents_total_B ({intents_total_b}) should be <= intents_total_A * 0.07 \"","                f\"({intents_total_a * 0.07}), got ratio {ratio:.4f}\"","            )","        ","        # Verify selected_params_count scaling","        selected_count_a = perf_a.get(\"selected_params_count\", n_params)","        selected_count_b = perf_b.get(\"selected_params_count\")","        ","        assert selected_count_b is not None, \"selected_params_count_B must exist\"","        assert selected_count_b < selected_count_a, (","            f\"selected_params_count_B ({selected_count_b}) should be < \"","            f\"selected_params_count_A ({selected_count_a})\"","        )","        ","    finally:","        # Restore environment","        if old_param_subsample_rate is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = old_param_subsample_rate","        ","        if old_param_subsample_seed is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = old_param_subsample_seed","","","def test_metrics_shape_preserved() -> None:","    \"\"\"","    Test that metrics shape is preserved (n_params, METRICS_N_COLUMNS) even with subsampling.","    ","    Only selected rows should be computed; unselected rows remain zeros.","    Uses metrics_computed_mask to verify which rows were computed.","    \"\"\"","    # Ensure clean environment","    old_trigger_rate = os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","    old_param_subsample_rate = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","    old_param_subsample_seed = os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","    ","    try:","        n_bars = 300","        n_params = 100","        ","        # Generate simple OHLC data","        rng = np.random.default_rng(42)","        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))","        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0","        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0","        open_ = (high + low) / 2","        ","        high = np.maximum(high, np.maximum(open_, close))","        low = np.minimum(low, np.minimum(open_, close))","        ","        # Generate params matrix","        params_list = []","        for i in range(n_params):","            ch_len = 20 + (i % 10)","            atr_len = 10 + (i % 5)","            stop_mult = 1.0","            params_list.append([ch_len, atr_len, stop_mult])","        ","        params_matrix = np.array(params_list, dtype=np.float64)","        ","        # Fix trigger_rate=1.0 (no intent-level sparsity) to test param subsample only","        os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = \"1.0\"","        # Set param_subsample_rate=0.1 (10% of params)","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = \"0.1\"","        os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = \"42\"","        ","        result = run_grid(","            open_=open_,","            high=high,","            low=low,","            close=close,","            params_matrix=params_matrix,","            commission=0.0,","            slip=0.0,","            order_qty=1,","            sort_params=True,","        )","        ","        # Verify metrics shape is preserved","        metrics = result.get(\"metrics\")","        assert metrics is not None, \"metrics must exist\"","        assert isinstance(metrics, np.ndarray), \"metrics must be np.ndarray\"","        assert metrics.shape == (n_params, 3), (","            f\"metrics shape should be ({n_params}, 3), got {metrics.shape}\"","        )","        ","        # Verify perf dict","        perf = result.get(\"perf\", {})","        metrics_rows_computed = perf.get(\"metrics_rows_computed\")","        selected_params_count = perf.get(\"selected_params_count\")","        metrics_computed_mask = perf.get(\"metrics_computed_mask\")","        ","        assert metrics_rows_computed == selected_params_count, (","            f\"metrics_rows_computed ({metrics_rows_computed}) should equal \"","            f\"selected_params_count ({selected_params_count})\"","        )","        ","        # Verify metrics_computed_mask exists and has correct shape","        assert metrics_computed_mask is not None, \"metrics_computed_mask must exist in perf\"","        assert isinstance(metrics_computed_mask, list), \"metrics_computed_mask must be a list\"","        assert len(metrics_computed_mask) == n_params, (","            f\"metrics_computed_mask length ({len(metrics_computed_mask)}) should equal n_params ({n_params})\"","        )","        ","        # Convert to numpy array for easier manipulation","        mask_array = np.array(metrics_computed_mask, dtype=bool)","        ","        # Verify that mask sum equals selected_params_count","        assert np.sum(mask_array) == selected_params_count, (","            f\"metrics_computed_mask sum ({np.sum(mask_array)}) should equal \"","            f\"selected_params_count ({selected_params_count})\"","        )","        ","        # Verify that uncomputed rows remain all zeros","        uncomputed_non_zero = np.sum(np.any(np.abs(metrics[~mask_array]) > 1e-10, axis=1))","        assert uncomputed_non_zero == 0, (","            f\"Uncomputed rows with non-zero metrics ({uncomputed_non_zero}) should be 0\"","        )","        ","        # NOTE: Do NOT require computed rows to be non-zero.","        # It's valid to have entry fills but no exits (trades=0), producing all-zero metrics.","        # Evidence of computation is provided by metrics_rows_computed == selected_params_count","        # and the metrics_computed_mask bookkeeping above.","        ","    finally:","        # Restore environment","        if old_trigger_rate is None:","            os.environ.pop(\"FISHBRO_PERF_TRIGGER_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_TRIGGER_RATE\"] = old_trigger_rate","        ","        if old_param_subsample_rate is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_RATE\"] = old_param_subsample_rate","        ","        if old_param_subsample_seed is None:","            os.environ.pop(\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\", None)","        else:","            os.environ[\"FISHBRO_PERF_PARAM_SUBSAMPLE_SEED\"] = old_param_subsample_seed","",""]}
{"type":"file_footer","path":"tests/test_trigger_rate_param_subsample_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_vectorization_parity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2630,"sha256":"3058382f60fc15b786485e67b9d84760e63ecba10a9df1ad2105f15242519b5a","total_lines":77,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_vectorization_parity.py","chunk_index":0,"line_start":1,"line_end":77,"content":["","from __future__ import annotations","","import numpy as np","","from data.layout import normalize_bars","from engine.engine_jit import simulate_arrays","from engine.types import Fill, OrderKind, OrderRole, Side","from strategy.kernel import DonchianAtrParams, run_kernel_arrays, run_kernel_object_mode","","","def _assert_fills_equal(a: list[Fill], b: list[Fill]) -> None:","    assert len(a) == len(b)","    for fa, fb in zip(a, b):","        assert fa.bar_index == fb.bar_index","        assert fa.role == fb.role","        assert fa.kind == fb.kind","        assert fa.side == fb.side","        assert fa.qty == fb.qty","        assert fa.order_id == fb.order_id","        assert abs(fa.price - fb.price) <= 1e-9","","","def test_strategy_object_vs_array_mode_parity() -> None:","    rng = np.random.default_rng(42)","    n = 300","    close = 100.0 + np.cumsum(rng.standard_normal(n)).astype(np.float64)","    high = close + 1.0","    low = close - 1.0","    open_ = (high + low) * 0.5","","    bars = normalize_bars(open_, high, low, close)","    params = DonchianAtrParams(channel_len=20, atr_len=14, stop_mult=2.0)","","    out_obj = run_kernel_object_mode(bars, params, commission=0.0, slip=0.0, order_qty=1)","    out_arr = run_kernel_arrays(bars, params, commission=0.0, slip=0.0, order_qty=1)","","    _assert_fills_equal(out_obj[\"fills\"], out_arr[\"fills\"])  # type: ignore[arg-type]","","","def test_simulate_arrays_same_bar_entry_exit_parity() -> None:","    # Construct a same-bar entry then exit scenario (created_bar=-1 activates on bar0).","    bars = normalize_bars(","        np.array([100.0], dtype=np.float64),","        np.array([120.0], dtype=np.float64),","        np.array([80.0], dtype=np.float64),","        np.array([110.0], dtype=np.float64),","    )","","    # ENTRY BUY STOP 105, EXIT SELL STOP 95, both active on bar0.","    order_id = np.array([1, 2], dtype=np.int64)","    created_bar = np.array([-1, -1], dtype=np.int64)","    role = np.array([1, 0], dtype=np.int8)  # ENTRY then EXIT (order_id tie-break handles)","    kind = np.array([0, 0], dtype=np.int8)  # STOP","    side = np.array([1, -1], dtype=np.int8)  # BUY, SELL","    price = np.array([105.0, 95.0], dtype=np.float64)","    qty = np.array([1, 1], dtype=np.int64)","","    fills = simulate_arrays(","        bars,","        order_id=order_id,","        created_bar=created_bar,","        role=role,","        kind=kind,","        side=side,","        price=price,","        qty=qty,","        ttl_bars=1,","    )","","    assert len(fills) == 2","    assert fills[0].role == OrderRole.ENTRY and fills[0].side == Side.BUY and fills[0].kind == OrderKind.STOP","    assert fills[1].role == OrderRole.EXIT and fills[1].side == Side.SELL and fills[1].kind == OrderKind.STOP","","","",""]}
{"type":"file_footer","path":"tests/test_vectorization_parity.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/test_winners_schema_v2_contract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6418,"sha256":"8dedb61cacdcdaf800cfa38d26e7c7746d7789f12129f4ed9e76a63ba3e5e765","total_lines":205,"chunk_count":2}
{"type":"file_chunk","path":"tests/test_winners_schema_v2_contract.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Contract tests for winners schema v2.","","Tests verify:","1. v2 schema structure (top-level fields)","2. WinnerItemV2 structure (required fields)","3. JSON serialization with sorted keys","4. Schema version detection","\"\"\"","","from __future__ import annotations","","import json","from datetime import datetime, timezone","","from core.winners_schema import (","    WinnerItemV2,","    build_winners_v2_dict,","    is_winners_legacy,","    is_winners_v2,","    WINNERS_SCHEMA_VERSION,",")","","","def test_winners_v2_top_level_schema() -> None:","    \"\"\"Test that v2 winners.json has required top-level fields.\"\"\"","    items = [","        WinnerItemV2(","            candidate_id=\"donchian_atr:123\",","            strategy_id=\"donchian_atr\",","            symbol=\"CME.MNQ\",","            timeframe=\"60m\",","            params={\"LE\": 8, \"LX\": 4, \"Z\": -0.4},","            score=1.234,","            metrics={\"net_profit\": 100.0, \"max_dd\": -10.0, \"trades\": 10, \"param_id\": 123},","            source={\"param_id\": 123, \"run_id\": \"test-123\", \"stage_name\": \"stage1_topk\"},","        ),","    ]","    ","    winners = build_winners_v2_dict(","        stage_name=\"stage1_topk\",","        run_id=\"test-123\",","        topk=items,","    )","    ","    # Verify top-level fields","    assert winners[\"schema\"] == WINNERS_SCHEMA_VERSION","    assert winners[\"stage_name\"] == \"stage1_topk\"","    assert \"generated_at\" in winners","    assert \"topk\" in winners","    assert \"notes\" in winners","    ","    # Verify notes schema","    assert winners[\"notes\"][\"schema\"] == WINNERS_SCHEMA_VERSION","","","def test_winner_item_v2_required_fields() -> None:","    \"\"\"Test that WinnerItemV2 has all required fields.\"\"\"","    item = WinnerItemV2(","        candidate_id=\"donchian_atr:c7bc8b64916c\",","        strategy_id=\"donchian_atr\",","        symbol=\"CME.MNQ\",","        timeframe=\"60m\",","        params={\"LE\": 8, \"LX\": 4, \"Z\": -0.4},","        score=1.234,","        metrics={\"net_profit\": 0.0, \"max_dd\": 0.0, \"trades\": 0, \"param_id\": 9},","        source={\"param_id\": 9, \"run_id\": \"stage1_topk-123\", \"stage_name\": \"stage1_topk\"},","    )","    ","    item_dict = item.to_dict()","    ","    # Verify all required fields exist","    assert \"candidate_id\" in item_dict","    assert \"strategy_id\" in item_dict","    assert \"symbol\" in item_dict","    assert \"timeframe\" in item_dict","    assert \"params\" in item_dict","    assert \"score\" in item_dict","    assert \"metrics\" in item_dict","    assert \"source\" in item_dict","    ","    # Verify field values","    assert item_dict[\"candidate_id\"] == \"donchian_atr:c7bc8b64916c\"","    assert item_dict[\"strategy_id\"] == \"donchian_atr\"","    assert item_dict[\"symbol\"] == \"CME.MNQ\"","    assert item_dict[\"timeframe\"] == \"60m\"","    assert isinstance(item_dict[\"params\"], dict)","    assert isinstance(item_dict[\"score\"], (int, float))","    assert isinstance(item_dict[\"metrics\"], dict)","    assert isinstance(item_dict[\"source\"], dict)","","","def test_winners_v2_json_serializable_sorted_keys() -> None:","    \"\"\"Test that v2 winners.json is JSON-serializable with sorted keys.\"\"\"","    items = [","        WinnerItemV2(","            candidate_id=\"donchian_atr:123\",","            strategy_id=\"donchian_atr\",","            symbol=\"CME.MNQ\",","            timeframe=\"60m\",","            params={\"LE\": 8},","            score=1.234,","            metrics={\"net_profit\": 100.0, \"max_dd\": -10.0, \"trades\": 10, \"param_id\": 123},","            source={\"param_id\": 123, \"run_id\": \"test-123\", \"stage_name\": \"stage1_topk\"},","        ),","    ]","    ","    winners = build_winners_v2_dict(","        stage_name=\"stage1_topk\",","        run_id=\"test-123\",","        topk=items,","    )","    ","    # Serialize to JSON with sorted keys","    json_str = json.dumps(winners, ensure_ascii=False, sort_keys=True, indent=2)","    ","    # Deserialize back","    winners_roundtrip = json.loads(json_str)","    ","    # Verify structure","    assert winners_roundtrip[\"schema\"] == WINNERS_SCHEMA_VERSION","    assert len(winners_roundtrip[\"topk\"]) == 1","    ","    item_dict = winners_roundtrip[\"topk\"][0]","    assert item_dict[\"candidate_id\"] == \"donchian_atr:123\"","    assert item_dict[\"strategy_id\"] == \"donchian_atr\"","    ","    # Verify JSON keys are sorted (check top-level)","    json_lines = json_str.split(\"\\n\")","    # Find line with \"generated_at\" and \"schema\" - should be in sorted order","    # (This is a simple check - full verification would require parsing)","    assert '\"generated_at\"' in json_str","    assert '\"schema\"' in json_str","","","def test_is_winners_v2_detection() -> None:","    \"\"\"Test schema version detection.\"\"\"","    # v2 format","    winners_v2 = {","        \"schema\": \"v2\",","        \"stage_name\": \"stage1_topk\",","        \"generated_at\": \"2025-12-18T00:00:00Z\",","        \"topk\": [],","        \"notes\": {\"schema\": \"v2\"},","    }","    assert is_winners_v2(winners_v2) is True","    assert is_winners_legacy(winners_v2) is False","    ","    # Legacy format","    winners_legacy = {","        \"topk\": [{\"param_id\": 0, \"net_profit\": 100.0, \"trades\": 10, \"max_dd\": -10.0}],","        \"notes\": {\"schema\": \"v1\"},","    }","    assert is_winners_v2(winners_legacy) is False","    assert is_winners_legacy(winners_legacy) is True","    ","    # Unknown format (no schema)","    winners_unknown = {","        \"topk\": [{\"param_id\": 0}],","    }","    assert is_winners_v2(winners_unknown) is False","    assert is_winners_legacy(winners_unknown) is True  # Falls back to legacy","","","def test_winner_item_v2_metrics_contains_legacy_fields() -> None:","    \"\"\"Test that metrics contains legacy fields for backward compatibility.\"\"\"","    item = WinnerItemV2(","        candidate_id=\"donchian_atr:123\",","        strategy_id=\"donchian_atr\",","        symbol=\"CME.MNQ\",","        timeframe=\"60m\",","        params={},","        score=1.234,","        metrics={","            \"net_profit\": 100.0,","            \"max_dd\": -10.0,","            \"trades\": 10,","            \"param_id\": 123,  # Legacy field","        },","        source={\"param_id\": 123, \"run_id\": \"test-123\", \"stage_name\": \"stage1_topk\"},","    )","    ","    item_dict = item.to_dict()","    metrics = item_dict[\"metrics\"]","    ","    # Verify legacy fields exist","    assert \"net_profit\" in metrics","    assert \"max_dd\" in metrics","    assert \"trades\" in metrics","    assert \"param_id\" in metrics","","","def test_winners_v2_empty_topk() -> None:","    \"\"\"Test that v2 schema handles empty topk correctly.\"\"\"","    winners = build_winners_v2_dict(","        stage_name=\"stage1_topk\",","        run_id=\"test-123\",","        topk=[],","    )","    "]}
{"type":"file_chunk","path":"tests/test_winners_schema_v2_contract.py","chunk_index":1,"line_start":201,"line_end":205,"content":["    assert winners[\"schema\"] == WINNERS_SCHEMA_VERSION","    assert winners[\"topk\"] == []","    assert isinstance(winners[\"topk\"], list)","",""]}
{"type":"file_footer","path":"tests/test_winners_schema_v2_contract.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/test_worker_writes_traceback_to_log.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2250,"sha256":"f31dd89e83f997deb0014268e492743da3dd030a0a7ee99739a2c1cd917c57fb","total_lines":69,"chunk_count":1}
{"type":"file_chunk","path":"tests/test_worker_writes_traceback_to_log.py","chunk_index":0,"line_start":1,"line_end":69,"content":["","\"\"\"Tests for worker writing full traceback to log.","","Tests that worker writes complete traceback.format_exc() to job_logs table","when job fails, while keeping last_error column short (500 chars).","\"\"\"","","from __future__ import annotations","","from pathlib import Path","from unittest.mock import Mock, patch","","import pytest","","from control.jobs_db import create_job, get_job, get_job_logs, init_db","from control.types import DBJobSpec, JobStatus","from control.worker import run_one_job","","","def test_worker_writes_traceback_to_log(tmp_path: Path) -> None:","    \"\"\"","    Test that worker writes full traceback to job_logs when job fails.","    ","    Verifies:","    - last_error is truncated to 500 chars","    - job_logs contains full traceback with \"Traceback (most recent call last):\"","    \"\"\"","    db = tmp_path / \"jobs.db\"","    init_db(db)","    ","    # Create a job","    spec = DBJobSpec(","        season=\"2026Q1\",","        dataset_id=\"test_dataset\",","        outputs_root=str(tmp_path / \"outputs\"),","        config_snapshot={\"test\": \"config\"},","        config_hash=\"test_hash\",","    )","    job_id = create_job(db, spec)","    ","    # Mock run_funnel to raise exception with traceback","    with patch(\"control.worker.run_funnel\", side_effect=ValueError(\"Test error with long message \" * 20)):","        # Run job (should catch exception and write traceback)","        run_one_job(db, job_id)","    ","    # Verify job is marked as FAILED","    job = get_job(db, job_id)","    assert job.status == JobStatus.FAILED","    assert job.last_error is not None","    assert len(job.last_error) <= 500  # Truncated","    ","    # Verify traceback is in job_logs","    logs = get_job_logs(db, job_id)","    assert len(logs) > 0, \"Should have at least one log entry\"","    ","    # Find error log entry","    error_logs = [log for log in logs if \"[ERROR]\" in log]","    assert len(error_logs) > 0, \"Should have error log entry\"","    ","    # Verify traceback format","    error_log = error_logs[0]","    assert \"Traceback (most recent call last):\" in error_log, \"Should contain full traceback\"","    assert \"ValueError\" in error_log, \"Should contain exception type\"","    assert \"Test error\" in error_log, \"Should contain error message\"","    ","    # Verify error message is in last_error (truncated)","    assert \"Test error\" in job.last_error","",""]}
{"type":"file_footer","path":"tests/test_worker_writes_traceback_to_log.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/wfs/test_wfs_no_io.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2816,"sha256":"4d3bfd2dd501f0e0484e73d9ed77bfdbc8889773785f467a9a408b51996915cf","total_lines":87,"chunk_count":1}
{"type":"file_chunk","path":"tests/wfs/test_wfs_no_io.py","chunk_index":0,"line_start":1,"line_end":87,"content":["","import builtins","from pathlib import Path","","import numpy as np","import pytest","","from core.feature_bundle import FeatureSeries, FeatureBundle","import wfs.runner as wfs_runner","","","class _DummySpec:","    \"\"\"","    Minimal strategy spec object for tests.","    Must provide:","      - defaults: dict","      - fn(strategy_input: dict, params: dict) -> dict with {\"intents\": [...]}","    \"\"\"","    def __init__(self):","        self.defaults = {}","","        def _fn(strategy_input, params):","            # Must not do IO; return valid structure for run_strategy().","            return {\"intents\": []}","","        self.fn = _fn","","","def test_run_wfs_with_features_disallows_file_io_without_real_strategy(monkeypatch):","    # 1) Hard deny all file IO primitives","    def _deny(*args, **kwargs):","        raise RuntimeError(\"IO is forbidden in run_wfs_with_features\")","","    monkeypatch.setattr(builtins, \"open\", _deny, raising=True)","    monkeypatch.setattr(Path, \"open\", _deny, raising=True)","    monkeypatch.setattr(Path, \"read_text\", _deny, raising=True)","    monkeypatch.setattr(Path, \"exists\", _deny, raising=True)","","    # 2) Inject dummy strategy spec so we don't rely on repo strategy registry/ids","    # Primary patch target: symbol referenced by wfs_runner module","    monkeypatch.setattr(wfs_runner, \"get_strategy_spec\", lambda strategy_id: _DummySpec(), raising=False)","","    # If get_strategy_spec isn't used in this repo layout, add fallback patches:","    # These should be kept harmless by raising=False.","    try:","        import strategy.registry as strat_registry","        monkeypatch.setattr(strat_registry, \"get\", lambda strategy_id: _DummySpec(), raising=False)","    except Exception:","        pass","","    try:","        import strategy.runner as strat_runner","        monkeypatch.setattr(strat_runner, \"get\", lambda strategy_id: _DummySpec(), raising=False)","    except Exception:","        pass","","    # 3) Build a minimal FeatureBundle","    ts = np.array(","        [\"2025-01-01T00:00:00\", \"2025-01-01T00:01:00\", \"2025-01-01T00:02:00\"],","        dtype=\"datetime64[s]\",","    )","    v = np.array([1.0, 2.0, 3.0], dtype=np.float64)","","    s1 = FeatureSeries(ts=ts, values=v, name=\"atr_14\", timeframe_min=60)","    s2 = FeatureSeries(ts=ts, values=v, name=\"ret_z_200\", timeframe_min=60)","    s3 = FeatureSeries(ts=ts, values=v, name=\"session_vwap\", timeframe_min=60)","","    # FeatureBundle requires meta dict with ts_dtype and breaks_policy","    meta = {","        \"ts_dtype\": \"datetime64[s]\",","        \"breaks_policy\": \"drop\",","    }","    bundle = FeatureBundle(","        dataset_id=\"D\",","        season=\"S\",","        series={(s.name, s.timeframe_min): s for s in [s1, s2, s3]},","        meta=meta,","    )","","    out = wfs_runner.run_wfs_with_features(","        strategy_id=\"__dummy__\",","        feature_bundle=bundle,","        config={\"params\": {}},","    )","","    assert isinstance(out, dict)",""]}
{"type":"file_footer","path":"tests/wfs/test_wfs_no_io.py","complete":true,"emitted_chunks":1}
