{"type":"meta","schema_version":2,"run_id":"20251227_195651Z","repo_root":"/home/fishbro/FishBroWFS_V2","snapshot_root":"SNAPSHOT","part":3,"parts":10,"created_at":"2025-12-27T19:56:51Z","generator":{"name":"dump_context","version":"vNext-balanced"},"policies":{"max_parts":10,"target_bytes_per_part":500000,"chunk_max_lines":200,"chunk_max_bytes":120000,"outputs_max_bytes_for_content":2000000,"outputs_policy":"manifest_only_for_big_files","forbid_file_truncated":true,"balance_parts":true,"estimated_total_bytes":3423207,"balance_buffer_ratio":1.08}}
{"type":"file_chunk","path":"src/portfolio/candidate_export.py","chunk_index":0,"line_start":1,"line_end":186,"content":["","\"\"\"","Phase Portfolio Bridge: Export candidates.json from Research OS.","","Exports CandidateSpecs to a deterministic, auditable JSON file","that can be consumed by Market OS without boundary violations.","\"\"\"","","from __future__ import annotations","","import hashlib","import json","from datetime import datetime, timezone","from pathlib import Path","from typing import List, Optional","","from portfolio.candidate_spec import CandidateSpec, CandidateExport","from portfolio.hash_utils import stable_json_dumps","","","def export_candidates(","    candidates: List[CandidateSpec],","    *,","    export_id: str,","    season: str,","    exports_root: Optional[Path] = None,",") -> Path:","    \"\"\"","    Export candidates to a deterministic JSON file.","    ","    File layout:","        exports/candidates/{season}/{export_id}/candidates.json","        exports/candidates/{season}/{export_id}/manifest.json","    ","    Returns:","        Path to the exported candidates.json file","    \"\"\"","    if exports_root is None:","        exports_root = Path(\"outputs/exports\")","    ","    # Create export directory","    export_dir = exports_root / \"candidates\" / season / export_id","    export_dir.mkdir(parents=True, exist_ok=True)","    ","    # Create CandidateExport with timezone-aware timestamp","    generated_at = datetime.now(timezone.utc).replace(microsecond=0).isoformat() + \"Z\"","    candidate_export = CandidateExport(","        export_id=export_id,","        generated_at=generated_at,","        season=season,","        candidates=sorted(candidates, key=lambda c: c.candidate_id),","        deterministic_order=\"candidate_id asc\",","    )","    ","    # Build base dict without hash fields","    base_dict = {","        \"export_id\": export_id,","        \"generated_at\": generated_at,","        \"season\": season,","        \"deterministic_order\": \"candidate_id asc\",","        \"candidates\": [_candidate_spec_to_dict(c) for c in candidate_export.candidates],","    }","    ","    # Compute candidates_sha256 (hash of base dict)","    candidates_sha256 = _compute_dict_sha256(base_dict)","    ","    # Add candidates_sha256 to dict (no manifest_sha256 in candidates.json)","    final_dict = dict(base_dict)","    final_dict[\"candidates_sha256\"] = candidates_sha256","    ","    # Write candidates.json","    candidates_path = export_dir / \"candidates.json\"","    candidates_path.write_text(","        stable_json_dumps(final_dict),","        encoding=\"utf-8\",","    )","    ","    # Compute file hash of candidates.json","    candidates_file_sha256 = _compute_file_sha256(candidates_path)","    ","    # Build manifest dict (without manifest_sha256)","    manifest_base = {","        \"export_id\": export_id,","        \"season\": season,","        \"generated_at\": generated_at,","        \"candidates_count\": len(candidates),","        \"candidates_file\": str(candidates_path.relative_to(export_dir)),","        \"deterministic_order\": \"candidate_id asc\",","        \"candidates_sha256\": candidates_sha256,","        \"candidates_file_sha256\": candidates_file_sha256,","    }","    ","    # Compute manifest_sha256 (hash of manifest_base)","    manifest_sha256 = _compute_dict_sha256(manifest_base)","    manifest_base[\"manifest_sha256\"] = manifest_sha256","    ","    # Write manifest.json","    manifest_path = export_dir / \"manifest.json\"","    manifest_path.write_text(","        stable_json_dumps(manifest_base),","        encoding=\"utf-8\",","    )","    ","    return candidates_path","","","def _candidate_export_to_dict(export: CandidateExport) -> dict:","    \"\"\"Convert CandidateExport to dict for JSON serialization.\"\"\"","    return {","        \"export_id\": export.export_id,","        \"generated_at\": export.generated_at,","        \"season\": export.season,","        \"deterministic_order\": export.deterministic_order,","        \"candidates\": [_candidate_spec_to_dict(c) for c in export.candidates],","    }","","","def _candidate_spec_to_dict(candidate: CandidateSpec) -> dict:","    \"\"\"Convert CandidateSpec to dict for JSON serialization.\"\"\"","    return {","        \"candidate_id\": candidate.candidate_id,","        \"strategy_id\": candidate.strategy_id,","        \"param_hash\": candidate.param_hash,","        \"research_score\": candidate.research_score,","        \"research_confidence\": candidate.research_confidence,","        \"season\": candidate.season,","        \"batch_id\": candidate.batch_id,","        \"job_id\": candidate.job_id,","        \"tags\": candidate.tags,","        \"metadata\": candidate.metadata,","    }","","","def _compute_file_sha256(path: Path) -> str:","    \"\"\"Compute SHA256 hash of a file.\"\"\"","    return hashlib.sha256(path.read_bytes()).hexdigest()","","","def _compute_dict_sha256(obj: dict) -> str:","    \"\"\"Compute SHA256 hash of a dict using stable JSON serialization.\"\"\"","    json_str = stable_json_dumps(obj)","    return hashlib.sha256(json_str.encode(\"utf-8\")).hexdigest()","","","def load_candidates(candidates_path: Path) -> CandidateExport:","    \"\"\"","    Load candidates from a candidates.json file.","    ","    Raises:","        FileNotFoundError: if file does not exist","        ValueError: if JSON is invalid","    \"\"\"","    if not candidates_path.exists():","        raise FileNotFoundError(f\"Candidates file not found: {candidates_path}\")","    ","    data = json.loads(candidates_path.read_text(encoding=\"utf-8\"))","    ","    # Remove hash fields if present (they are for audit only)","    data.pop(\"candidates_sha256\", None)","    ","    # Convert dicts back to CandidateSpec objects","    candidates = []","    for c_dict in data.get(\"candidates\", []):","        candidate = CandidateSpec(","            candidate_id=c_dict[\"candidate_id\"],","            strategy_id=c_dict[\"strategy_id\"],","            param_hash=c_dict[\"param_hash\"],","            research_score=c_dict[\"research_score\"],","            research_confidence=c_dict.get(\"research_confidence\", 1.0),","            season=c_dict.get(\"season\"),","            batch_id=c_dict.get(\"batch_id\"),","            job_id=c_dict.get(\"job_id\"),","            tags=c_dict.get(\"tags\", []),","            metadata=c_dict.get(\"metadata\", {}),","        )","        candidates.append(candidate)","    ","    return CandidateExport(","        export_id=data[\"export_id\"],","        generated_at=data[\"generated_at\"],","        season=data[\"season\"],","        candidates=candidates,","        deterministic_order=data.get(\"deterministic_order\", \"candidate_id asc\"),","    )","",""]}
{"type":"file_footer","path":"src/portfolio/candidate_export.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/candidate_spec.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5329,"sha256":"fe2edd87ff31eeb80609e597ee47f2f9ea1e21f8cd71072686e342d25c02c87b","total_lines":148,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/candidate_spec.py","chunk_index":0,"line_start":1,"line_end":148,"content":["","\"\"\"","Phase Portfolio Bridge: CandidateSpec for Research → Market boundary.","","Research OS can output CandidateSpecs (research candidates) that contain","only information allowed by the boundary contract:","- No trading details (symbol, timeframe, session_profile, etc.)","- No market-specific parameters","- Only research metrics and identifiers that can be mapped later by Market OS","","Boundary contract:","- Research OS MUST NOT know any trading details","- Market OS maps CandidateSpec to PortfolioLeg with trading details","- CandidateSpec is deterministic and auditable","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, field","from typing import Dict, List, Optional","","","@dataclass(frozen=True)","class CandidateSpec:","    \"\"\"","    Research candidate specification (boundary-safe).","    ","    Contains only information that Research OS is allowed to know:","    - Research identifiers (strategy_id, param_hash)","    - Research metrics (score, confidence, etc.)","    - Research metadata (season, batch_id, job_id)","    - No trading details (symbol, timeframe, session_profile, etc.)","    ","    Attributes:","        candidate_id: Unique candidate identifier (e.g., \"candidate_001\")","        strategy_id: Strategy identifier (e.g., \"sma_cross_v1\")","        param_hash: Hash of strategy parameters (deterministic)","        research_score: Research metric score (e.g., 1.5)","        research_confidence: Confidence metric (0.0-1.0)","        season: Season identifier (e.g., \"2026Q1\")","        batch_id: Batch identifier (e.g., \"batchA\")","        job_id: Job identifier (e.g., \"job1\")","        tags: Optional tags for categorization","        metadata: Optional additional research metadata (no trading details)","    \"\"\"","    candidate_id: str","    strategy_id: str","    param_hash: str","    research_score: float","    research_confidence: float = 1.0","    season: Optional[str] = None","    batch_id: Optional[str] = None","    job_id: Optional[str] = None","    tags: List[str] = field(default_factory=list)","    metadata: Dict[str, str] = field(default_factory=dict)","    ","    def __post_init__(self) -> None:","        \"\"\"Validate candidate spec.\"\"\"","        if not self.candidate_id:","            raise ValueError(\"candidate_id cannot be empty\")","        if not self.strategy_id:","            raise ValueError(\"strategy_id cannot be empty\")","        if not self.param_hash:","            raise ValueError(\"param_hash cannot be empty\")","        if not isinstance(self.research_score, (int, float)):","            raise ValueError(f\"research_score must be numeric, got {type(self.research_score)}\")","        if not 0.0 <= self.research_confidence <= 1.0:","            raise ValueError(f\"research_confidence must be between 0.0 and 1.0, got {self.research_confidence}\")","        ","        # Ensure metadata does not contain trading details","        forbidden_keys = {\"symbol\", \"timeframe\", \"session_profile\", \"market\", \"exchange\", \"trading\"}","        for key in self.metadata:","            if key.lower() in forbidden_keys:","                raise ValueError(f\"metadata key '{key}' contains trading details (boundary violation)\")","","","@dataclass(frozen=True)","class CandidateExport:","    \"\"\"","    Collection of CandidateSpecs for export.","    ","    Used to export research candidates from Research OS to Market OS.","    ","    Attributes:","        export_id: Unique export identifier (e.g., \"export_2026Q1_topk\")","        generated_at: ISO 8601 timestamp","        season: Season identifier","        candidates: List of CandidateSpecs","        deterministic_order: Ordering guarantee","    \"\"\"","    export_id: str","    generated_at: str","    season: str","    candidates: List[CandidateSpec]","    deterministic_order: str = \"candidate_id asc\"","    ","    def __post_init__(self) -> None:","        \"\"\"Validate candidate export.\"\"\"","        if not self.export_id:","            raise ValueError(\"export_id cannot be empty\")","        if not self.generated_at:","            raise ValueError(\"generated_at cannot be empty\")","        if not self.season:","            raise ValueError(\"season cannot be empty\")","        ","        # Check candidate_id uniqueness","        candidate_ids = [c.candidate_id for c in self.candidates]","        if len(candidate_ids) != len(set(candidate_ids)):","            duplicates = [cid for cid in candidate_ids if candidate_ids.count(cid) > 1]","            raise ValueError(f\"Duplicate candidate_id found: {set(duplicates)}\")","","","def create_candidate_from_research(","    *,","    candidate_id: str,","    strategy_id: str,","    params: Dict[str, float],","    research_score: float,","    research_confidence: float = 1.0,","    season: Optional[str] = None,","    batch_id: Optional[str] = None,","    job_id: Optional[str] = None,","    tags: Optional[List[str]] = None,","    metadata: Optional[Dict[str, str]] = None,",") -> CandidateSpec:","    \"\"\"","    Create a CandidateSpec from research results.","    ","    Computes param_hash from params dict (deterministic).","    \"\"\"","    from portfolio.hash_utils import hash_params","    ","    param_hash = hash_params(params)","    ","    return CandidateSpec(","        candidate_id=candidate_id,","        strategy_id=strategy_id,","        param_hash=param_hash,","        research_score=research_score,","        research_confidence=research_confidence,","        season=season,","        batch_id=batch_id,","        job_id=job_id,","        tags=tags or [],","        metadata=metadata or {},","    )","",""]}
{"type":"file_footer","path":"src/portfolio/candidate_spec.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":11194,"sha256":"25d8e34a892bcb3588324c47d392bb9b4c2730712bfccad3d9e28995ca8bda59","total_lines":289,"chunk_count":2}
{"type":"file_chunk","path":"src/portfolio/cli.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Portfolio CLI.\"\"\"","","import argparse","import json","import sys","import yaml","from pathlib import Path","from typing import Optional","","from core.schemas.portfolio_v1 import (","    PortfolioPolicyV1,","    PortfolioSpecV1,",")","from portfolio.runner_v1 import (","    run_portfolio_admission,","    validate_portfolio_spec,",")","from portfolio.artifacts_writer_v1 import (","    write_portfolio_artifacts,","    compute_spec_sha256,","    compute_policy_sha256,",")","","","def load_yaml_or_json(filepath: Path) -> dict:","    \"\"\"Load YAML or JSON file.\"\"\"","    content = filepath.read_text(encoding=\"utf-8\")","    if filepath.suffix.lower() in (\".yaml\", \".yml\"):","        return yaml.safe_load(content)","    else:","        return json.loads(content)","","","def save_yaml_or_json(filepath: Path, data: dict):","    \"\"\"Save data as YAML or JSON based on file extension.\"\"\"","    if filepath.suffix.lower() in (\".yaml\", \".yml\"):","        filepath.write_text(yaml.dump(data, default_flow_style=False), encoding=\"utf-8\")","    else:","        filepath.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")","","","def validate_command(args):","    \"\"\"Validate portfolio specification.\"\"\"","    try:","        # Load spec","        spec_data = load_yaml_or_json(args.spec)","        ","        # Load policy if provided separately","        policy_data = {}","        if args.policy:","            policy_data = load_yaml_or_json(args.policy)","            spec_data[\"policy\"] = policy_data","        ","        # Create spec object (without sha256 for now)","        if \"spec_sha256\" in spec_data:","            spec_data.pop(\"spec_sha256\")","        ","        spec = PortfolioSpecV1(**spec_data)","        ","        # Compute spec SHA256","        spec_sha256 = compute_spec_sha256(spec)","        print(f\"✓ Spec SHA256: {spec_sha256}\")","        ","        # Validate against outputs","        outputs_root = Path(args.outputs_root) if args.outputs_root else Path(\"outputs\")","        errors = validate_portfolio_spec(spec, outputs_root)","        ","        if errors:","            print(\"✗ Validation errors:\")","            for error in errors:","                print(f\"  - {error}\")","            sys.exit(1)","        ","        # Resource estimate","        total_estimate = len(spec.seasons) * len(spec.strategy_ids) * len(spec.instrument_ids) * 1000","        print(f\"✓ Resource estimate: ~{total_estimate} candidates\")","        ","        print(\"✓ Spec validation passed\")","        ","        # If --save flag, update spec with SHA256","        if args.save:","            spec_dict = spec.model_dump()","            spec_dict[\"spec_sha256\"] = spec_sha256","            save_yaml_or_json(args.spec, spec_dict)","            print(f\"✓ Updated {args.spec} with spec_sha256\")","        ","    except Exception as e:","        print(f\"✗ Validation failed: {e}\")","        sys.exit(1)","","","def run_command(args):","    \"\"\"Run portfolio admission.\"\"\"","    try:","        # Load spec","        spec_data = load_yaml_or_json(args.spec)","        spec = PortfolioSpecV1(**spec_data)","        ","        # Load policy (could be embedded in spec or separate)","        if \"policy\" in spec_data:","            policy_data = spec_data[\"policy\"]","        elif args.policy:","            policy_data = load_yaml_or_json(args.policy)","        else:","            raise ValueError(\"Policy not found in spec and --policy not provided\")","        ","        policy = PortfolioPolicyV1(**policy_data)","        ","        # Compute SHA256 for audit","        policy_sha256 = compute_policy_sha256(policy)","        spec_sha256 = spec.spec_sha256 if hasattr(spec, \"spec_sha256\") else compute_spec_sha256(spec)","        ","        print(f\"Policy SHA256: {policy_sha256}\")","        print(f\"Spec SHA256: {spec_sha256}\")","        ","        # Set equity","        equity_base = args.equity if args.equity else 1_000_000.0  # Default 1M TWD","        ","        # Output directory","        if args.output_dir:","            output_dir = Path(args.output_dir)","        else:","            # Create auto-generated directory","            from datetime import datetime","            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")","            output_dir = Path(\"outputs\") / \"portfolio\" / f\"run_{timestamp}\"","        ","        outputs_root = Path(args.outputs_root) if args.outputs_root else Path(\"outputs\")","        ","        # Run portfolio admission","        candidates, final_positions, results = run_portfolio_admission(","            policy=policy,","            spec=spec,","            equity_base=equity_base,","            outputs_root=outputs_root,","            replay_mode=False,","        )","        ","        # Update summary with SHA256","        summary = results[\"summary\"]","        summary.policy_sha256 = policy_sha256","        summary.spec_sha256 = spec_sha256","        ","        # Write artifacts","        hashes = write_portfolio_artifacts(","            output_dir=output_dir,","            decisions=results[\"decisions\"],","            bar_states=results[\"bar_states\"],","            summary=summary,","            policy=policy,","            spec=spec,","            replay_mode=False,","        )","        ","        print(f\"\\n✓ Portfolio admission completed\")","        print(f\"  Output directory: {output_dir}\")","        print(f\"  Candidates: {summary.total_candidates}\")","        print(f\"  Accepted: {summary.accepted_count}\")","        print(f\"  Rejected: {summary.rejected_count}\")","        print(f\"  Final slots used: {summary.final_slots_used}/{policy.max_slots_total}\")","        print(f\"  Final margin ratio: {summary.final_margin_ratio:.2%}\")","        ","        # Save run info","        run_info = {","            \"run_id\": output_dir.name,","            \"timestamp\": datetime.now().isoformat(),","            \"spec_sha256\": spec_sha256,","            \"policy_sha256\": policy_sha256,","            \"output_dir\": str(output_dir),","            \"summary\": summary.model_dump(),","        }","        run_info_path = output_dir / \"run_info.json\"","        run_info_path.write_text(json.dumps(run_info, indent=2), encoding=\"utf-8\")","        ","    except Exception as e:","        print(f\"✗ Run failed: {e}\")","        import traceback","        traceback.print_exc()","        sys.exit(1)","","","def replay_command(args):","    \"\"\"Replay portfolio admission (read-only).\"\"\"","    try:","        # Find run directory","        run_id = args.run_id","        runs_dir = Path(\"outputs\") / \"portfolio\"","        ","        run_dir = None","        for dir_path in runs_dir.glob(f\"*{run_id}*\"):","            if dir_path.is_dir():","                run_dir = dir_path","                break","        ","        if not run_dir or not run_dir.exists():","            print(f\"✗ Run directory not found for run_id: {run_id}\")","            sys.exit(1)","        ","        # Load spec and policy from run directory","        spec_path = run_dir / \"portfolio_spec.json\""]}
{"type":"file_chunk","path":"src/portfolio/cli.py","chunk_index":1,"line_start":201,"line_end":289,"content":["        policy_path = run_dir / \"portfolio_policy.json\"","        ","        if not spec_path.exists() or not policy_path.exists():","            print(f\"✗ Spec or policy not found in run directory\")","            sys.exit(1)","        ","        spec_data = json.loads(spec_path.read_text(encoding=\"utf-8\"))","        policy_data = json.loads(policy_path.read_text(encoding=\"utf-8\"))","        ","        spec = PortfolioSpecV1(**spec_data)","        policy = PortfolioPolicyV1(**policy_data)","        ","        print(f\"Replaying run: {run_dir.name}\")","        print(f\"Spec SHA256: {spec.spec_sha256 if hasattr(spec, 'spec_sha256') else 'N/A'}\")","        print(f\"Policy SHA256: {compute_policy_sha256(policy)}\")","        ","        # Run in replay mode (no writes)","        equity_base = args.equity if args.equity else 1_000_000.0","        outputs_root = Path(args.outputs_root) if args.outputs_root else Path(\"outputs\")","        ","        candidates, final_positions, results = run_portfolio_admission(","            policy=policy,","            spec=spec,","            equity_base=equity_base,","            outputs_root=outputs_root,","            replay_mode=True,","        )","        ","        summary = results[\"summary\"]","        print(f\"\\n✓ Replay completed (read-only)\")","        print(f\"  Candidates: {summary.total_candidates}\")","        print(f\"  Accepted: {summary.accepted_count}\")","        print(f\"  Rejected: {summary.rejected_count}\")","        print(f\"  Final slots used: {summary.final_slots_used}/{policy.max_slots_total}\")","        ","        # Compare with original results if available","        original_summary_path = run_dir / \"portfolio_summary.json\"","        if original_summary_path.exists():","            original_summary = json.loads(original_summary_path.read_text(encoding=\"utf-8\"))","            if (summary.accepted_count == original_summary[\"accepted_count\"] and","                summary.rejected_count == original_summary[\"rejected_count\"]):","                print(\"✓ Replay matches original results\")","            else:","                print(\"✗ Replay differs from original results!\")","                print(f\"  Original: {original_summary['accepted_count']} accepted, {original_summary['rejected_count']} rejected\")","                print(f\"  Replay: {summary.accepted_count} accepted, {summary.rejected_count} rejected\")","        ","    except Exception as e:","        print(f\"✗ Replay failed: {e}\")","        import traceback","        traceback.print_exc()","        sys.exit(1)","","","def main():","    \"\"\"Main CLI entry point.\"\"\"","    parser = argparse.ArgumentParser(description=\"Portfolio Engine CLI\")","    subparsers = parser.add_subparsers(dest=\"command\", required=True)","    ","    # Validate command","    validate_parser = subparsers.add_parser(\"validate\", help=\"Validate portfolio specification\")","    validate_parser.add_argument(\"--spec\", type=Path, required=True, help=\"Spec file (YAML/JSON)\")","    validate_parser.add_argument(\"--policy\", type=Path, help=\"Policy file (YAML/JSON, optional if embedded in spec)\")","    validate_parser.add_argument(\"--outputs-root\", type=Path, help=\"Outputs root directory (default: outputs)\")","    validate_parser.add_argument(\"--save\", action=\"store_true\", help=\"Save spec with computed SHA256\")","    validate_parser.set_defaults(func=validate_command)","    ","    # Run command","    run_parser = subparsers.add_parser(\"run\", help=\"Run portfolio admission\")","    run_parser.add_argument(\"--spec\", type=Path, required=True, help=\"Spec file (YAML/JSON)\")","    run_parser.add_argument(\"--policy\", type=Path, help=\"Policy file (YAML/JSON, optional if embedded in spec)\")","    run_parser.add_argument(\"--equity\", type=float, help=\"Equity in base currency (default: 1,000,000 TWD)\")","    run_parser.add_argument(\"--outputs-root\", type=Path, help=\"Outputs root directory (default: outputs)\")","    run_parser.add_argument(\"--output-dir\", type=Path, help=\"Output directory (default: auto-generated)\")","    run_parser.set_defaults(func=run_command)","    ","    # Replay command","    replay_parser = subparsers.add_parser(\"replay\", help=\"Replay portfolio admission (read-only)\")","    replay_parser.add_argument(\"--run-id\", type=str, required=True, help=\"Run ID or directory name\")","    replay_parser.add_argument(\"--equity\", type=float, help=\"Equity in base currency (default: 1,000,000 TWD)\")","    replay_parser.add_argument(\"--outputs-root\", type=Path, help=\"Outputs root directory (default: outputs)\")","    replay_parser.set_defaults(func=replay_command)","    ","    args = parser.parse_args()","    args.func(args)","","","if __name__ == \"__main__\":","    main()"]}
{"type":"file_footer","path":"src/portfolio/cli.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/portfolio/compiler.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1555,"sha256":"f63330d8d5a184e79e48c9e0ebc5ec5c03396facf78811625e4736208503f459","total_lines":58,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/compiler.py","chunk_index":0,"line_start":1,"line_end":58,"content":["","\"\"\"Portfolio compiler - compile PortfolioSpec to Funnel job configs.","","Phase 8: Convert portfolio specification to executable job configurations.","\"\"\"","","from __future__ import annotations","","from typing import Dict, List","","from portfolio.spec import PortfolioSpec","","","def compile_portfolio(spec: PortfolioSpec) -> List[Dict[str, any]]:","    \"\"\"Compile portfolio specification to job configurations.","    ","    Each enabled leg produces one job_cfg dict.","    ","    Args:","        spec: Portfolio specification","        ","    Returns:","        List of job configuration dicts (one per enabled leg)","    \"\"\"","    jobs = []","    ","    for leg in spec.legs:","        if not leg.enabled:","            continue","        ","        # Build job configuration","        job_cfg: Dict[str, any] = {","            # Portfolio metadata","            \"portfolio_id\": spec.portfolio_id,","            \"portfolio_version\": spec.version,","            ","            # Leg metadata","            \"leg_id\": leg.leg_id,","            \"symbol\": leg.symbol,","            \"timeframe_min\": leg.timeframe_min,","            \"session_profile\": leg.session_profile,  # Path, passed as-is to pipeline","            ","            # Strategy metadata","            \"strategy_id\": leg.strategy_id,","            \"strategy_version\": leg.strategy_version,","            ","            # Strategy parameters","            \"params\": dict(leg.params),  # Copy dict","            ","            # Optional: tags for categorization","            \"tags\": list(leg.tags),  # Copy list","        }","        ","        jobs.append(job_cfg)","    ","    return jobs","",""]}
{"type":"file_footer","path":"src/portfolio/compiler.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/decisions_reader.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3410,"sha256":"e9fcdecefab41a106f4717841504538b5c4bb14fdad9703626d95a885db2735f","total_lines":133,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/decisions_reader.py","chunk_index":0,"line_start":1,"line_end":133,"content":["","\"\"\"Decisions log parser for portfolio generation.","","Parses append-only decisions.log lines. Supports JSONL + pipe format.","Invalid lines are ignored.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Any","","","def _as_stripped_text(v: Any) -> str:","    \"\"\"Convert value to trimmed string. None -> ''.\"\"\"","    if v is None:","        return \"\"","    if isinstance(v, str):","        return v.strip()","    return str(v).strip()","","","def _parse_pipe_line(s: str) -> dict | None:","    \"\"\"","    Parse simple pipe-delimited lines:","      - run_id|DECISION","      - run_id|DECISION|note","      - run_id|DECISION|note|ts","    note may be empty. ts may be missing.","    \"\"\"","    parts = [p.strip() for p in s.split(\"|\")]","    if len(parts) < 2:","        return None","","    run_id = parts[0].strip()","    decision_raw = parts[1].strip()","    note = parts[2].strip() if len(parts) >= 3 else \"\"","    ts = parts[3].strip() if len(parts) >= 4 else \"\"","","    if not run_id:","        return None","    if not decision_raw:","        return None","","    out = {","        \"run_id\": run_id,","        \"decision\": decision_raw.upper(),","        \"note\": note,","    }","    if ts:","        out[\"ts\"] = ts","    return out","","","def parse_decisions_log_lines(lines: list[str]) -> list[dict]:","    \"\"\"Parse decisions.log lines. Supports JSONL + pipe format. Invalid lines ignored.","    ","    Required:","      - run_id (non-empty after strip)","      - decision (non-empty after strip; normalized to upper)","    Optional:","      - note (may be missing/empty)","      - ts   (kept if present)","    \"\"\"","    out: list[dict] = []","","    for raw in lines:","        if not isinstance(raw, str):","            continue","        s = raw.strip()","        if not s:","            continue","            ","        # 1) Try JSONL first","        parsed: dict | None = None","        try:","            obj = json.loads(s)","            if isinstance(obj, dict):","                run_id = _as_stripped_text(obj.get(\"run_id\"))","                decision_raw = _as_stripped_text(obj.get(\"decision\"))","                note = _as_stripped_text(obj.get(\"note\"))","                ts = _as_stripped_text(obj.get(\"ts\"))","","                if not run_id:","                    continue","                if not decision_raw:","                    continue","","                parsed = {","                    \"run_id\": run_id,","                    \"decision\": decision_raw.upper(),","                    \"note\": note,","                }","                if ts:","                    parsed[\"ts\"] = ts","        except Exception:","            # Not JSON -> try pipe","            parsed = None","","        # 2) Pipe fallback","        if parsed is None:","            parsed = _parse_pipe_line(s)","","        if parsed is None:","            continue","","        out.append(parsed)","","    return out","","","def read_decisions_log(decisions_log_path: Path) -> list[dict]:","    \"\"\"Read decisions.log file and parse its contents.","    ","    Args:","        decisions_log_path: Path to decisions.log file","        ","    Returns:","        List of parsed decision entries. Returns empty list if file doesn't exist.","    \"\"\"","    if not decisions_log_path.exists():","        return []","    ","    try:","        with open(decisions_log_path, 'r', encoding='utf-8') as f:","            lines = f.readlines()","        return parse_decisions_log_lines(lines)","    except Exception:","        # If any error occurs (permission, encoding, etc.), return empty list","        return []","",""]}
{"type":"file_footer","path":"src/portfolio/decisions_reader.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/engine_v1.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":10183,"sha256":"56896189baeb39fc53d251b52952ebdfcd056782fbacaf310b03b8b2ec1bf19c","total_lines":265,"chunk_count":2}
{"type":"file_chunk","path":"src/portfolio/engine_v1.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Portfolio admission engine V1.\"\"\"","","import logging","from typing import List, Tuple, Dict, Optional","from datetime import datetime","","from core.schemas.portfolio_v1 import (","    PortfolioPolicyV1,","    SignalCandidateV1,","    OpenPositionV1,","    AdmissionDecisionV1,","    PortfolioStateV1,","    PortfolioSummaryV1,",")","","logger = logging.getLogger(__name__)","","","class PortfolioEngineV1:","    \"\"\"Portfolio admission engine with deterministic decision making.\"\"\"","    ","    def __init__(self, policy: PortfolioPolicyV1, equity_base: float):","        \"\"\"","        Initialize portfolio engine.","        ","        Args:","            policy: Portfolio policy defining limits and behavior","            equity_base: Initial equity in base currency (TWD)","        \"\"\"","        self.policy = policy","        self.equity_base = equity_base","        ","        # Current state","        self.open_positions: List[OpenPositionV1] = []","        self.slots_used = 0","        self.margin_used_base = 0.0","        self.notional_used_base = 0.0","        ","        # Track decisions per bar","        self.decisions: List[AdmissionDecisionV1] = []","        self.bar_states: Dict[Tuple[int, datetime], PortfolioStateV1] = {}","        ","        # Statistics","        self.reject_count = 0","        ","    def _compute_sort_key(self, candidate: SignalCandidateV1) -> Tuple:","        \"\"\"","        Compute deterministic sort key for candidate.","        ","        Sort order (ascending):","        1. Higher priority first (lower priority number = higher priority)","        2. Higher candidate_score first (negative for descending)","        3. signal_series_sha256 lexicographically as final tie-break","        ","        Returns:","            Tuple for sorting","        \"\"\"","        priority = self.policy.strategy_priority.get(candidate.strategy_id, 9999)","        # Negative candidate_score for descending order (higher score first)","        score = -candidate.candidate_score","        # Use signal_series_sha256 as final deterministic tie-break","        # If not available, use strategy_id + instrument_id as fallback","        sha = candidate.signal_series_sha256 or f\"{candidate.strategy_id}:{candidate.instrument_id}\"","        ","        return (priority, score, sha)","    ","    def _get_sort_key_string(self, candidate: SignalCandidateV1) -> str:","        \"\"\"Generate human-readable sort key string for audit.\"\"\"","        priority = self.policy.strategy_priority.get(candidate.strategy_id, 9999)","        return f\"priority={priority},candidate_score={candidate.candidate_score:.4f},sha={candidate.signal_series_sha256 or 'N/A'}\"","    ","    def _check_instrument_cap(self, instrument_id: str) -> bool:","        \"\"\"Check if instrument has available slots.\"\"\"","        if not self.policy.max_slots_by_instrument:","            return True","        ","        max_slots = self.policy.max_slots_by_instrument.get(instrument_id)","        if max_slots is None:","            return True","        ","        # Count current slots for this instrument","        current_slots = sum(","            1 for pos in self.open_positions ","            if pos.instrument_id == instrument_id","        )","        return current_slots < max_slots","    ","    def _can_admit(self, candidate: SignalCandidateV1) -> Tuple[bool, str]:","        \"\"\"","        Check if candidate can be admitted.","        ","        Returns:","            Tuple of (can_admit, reason)","        \"\"\"","        # Check total slots","        if self.slots_used + candidate.required_slot > self.policy.max_slots_total:","            return False, \"REJECT_FULL\"","        ","        # Check instrument-specific cap","        if not self._check_instrument_cap(candidate.instrument_id):","            return False, \"REJECT_FULL\"  # Instrument-specific full","        ","        # Check margin ratio","        required_margin = candidate.required_margin_base","        new_margin_used = self.margin_used_base + required_margin","        max_allowed_margin = self.equity_base * self.policy.max_margin_ratio","        ","        if new_margin_used > max_allowed_margin:","            return False, \"REJECT_MARGIN\"","        ","        # Check notional ratio (optional)","        if self.policy.max_notional_ratio is not None:","            # Note: notional check not implemented in v1","            pass","        ","        return True, \"ACCEPT\"","    ","    def _add_position(self, candidate: SignalCandidateV1):","        \"\"\"Add new position to portfolio.\"\"\"","        position = OpenPositionV1(","            strategy_id=candidate.strategy_id,","            instrument_id=candidate.instrument_id,","            slots=candidate.required_slot,","            margin_base=candidate.required_margin_base,","            notional_base=0.0,  # Notional not tracked in v1","            entry_bar_index=candidate.bar_index,","            entry_bar_ts=candidate.bar_ts,","        )","        self.open_positions.append(position)","        self.slots_used += candidate.required_slot","        self.margin_used_base += candidate.required_margin_base","    ","    def admit_candidates(","        self,","        candidates: List[SignalCandidateV1],","        current_open_positions: Optional[List[OpenPositionV1]] = None,","    ) -> List[AdmissionDecisionV1]:","        \"\"\"","        Process admission for a list of candidates at the same bar.","        ","        Args:","            candidates: List of candidates for the same bar","            current_open_positions: Optional list of existing open positions","                (if None, uses engine's current state)","        ","        Returns:","            List of admission decisions","        \"\"\"","        # Reset to provided open positions if given","        if current_open_positions is not None:","            self.open_positions = current_open_positions.copy()","            self.slots_used = sum(pos.slots for pos in self.open_positions)","            self.margin_used_base = sum(pos.margin_base for pos in self.open_positions)","        ","        # Sort candidates deterministically","        sorted_candidates = sorted(candidates, key=self._compute_sort_key)","        ","        decisions = []","        for candidate in sorted_candidates:","            # Check if can admit","            can_admit, reason = self._can_admit(candidate)","            ","            # Create decision","            sort_key_str = self._get_sort_key_string(candidate)","            decision = AdmissionDecisionV1(","                strategy_id=candidate.strategy_id,","                instrument_id=candidate.instrument_id,","                bar_ts=candidate.bar_ts,","                bar_index=candidate.bar_index,","                signal_strength=candidate.signal_strength,","                candidate_score=candidate.candidate_score,","                signal_series_sha256=candidate.signal_series_sha256,","                accepted=can_admit,","                reason=reason,","                sort_key_used=sort_key_str,","                slots_after=self.slots_used + (candidate.required_slot if can_admit else 0),","                margin_after_base=self.margin_used_base + (candidate.required_margin_base if can_admit else 0),","            )","            ","            if can_admit:","                # Admit candidate","                self._add_position(candidate)","                logger.debug(","                    f\"Admitted {candidate.strategy_id}/{candidate.instrument_id} \"","                    f\"at bar {candidate.bar_index}, slots={self.slots_used}, \"","                    f\"margin={self.margin_used_base:.0f}\"","                )","            else:","                self.reject_count += 1","                logger.debug(","                    f\"Rejected {candidate.strategy_id}/{candidate.instrument_id} \"","                    f\"at bar {candidate.bar_index}: {reason}\"","                )","            ","            decisions.append(decision)","        ","        # Record bar state","        if candidates:","            bar_ts = candidates[0].bar_ts","            bar_index = candidates[0].bar_index"]}
{"type":"file_chunk","path":"src/portfolio/engine_v1.py","chunk_index":1,"line_start":201,"line_end":265,"content":["            self.bar_states[(bar_index, bar_ts)] = PortfolioStateV1(","                bar_ts=bar_ts,","                bar_index=bar_index,","                equity_base=self.equity_base,","                slots_used=self.slots_used,","                margin_used_base=self.margin_used_base,","                notional_used_base=self.notional_used_base,","                open_positions=self.open_positions.copy(),","                reject_count=self.reject_count,","            )","        ","        self.decisions.extend(decisions)","        return decisions","    ","    def get_summary(self) -> PortfolioSummaryV1:","        \"\"\"Generate summary of admission results.\"\"\"","        reject_reasons = {}","        for decision in self.decisions:","            if not decision.accepted:","                reject_reasons[decision.reason] = reject_reasons.get(decision.reason, 0) + 1","        ","        total = len(self.decisions)","        accepted = sum(1 for d in self.decisions if d.accepted)","        rejected = total - accepted","        ","        return PortfolioSummaryV1(","            total_candidates=total,","            accepted_count=accepted,","            rejected_count=rejected,","            reject_reasons=reject_reasons,","            final_slots_used=self.slots_used,","            final_margin_used_base=self.margin_used_base,","            final_margin_ratio=self.margin_used_base / self.equity_base if self.equity_base > 0 else 0.0,","            policy_sha256=\"\",  # To be filled by caller","            spec_sha256=\"\",  # To be filled by caller","        )","    ","    def reset(self):","        \"\"\"Reset engine to initial state.\"\"\"","        self.open_positions.clear()","        self.slots_used = 0","        self.margin_used_base = 0.0","        self.notional_used_base = 0.0","        self.decisions.clear()","        self.bar_states.clear()","        self.reject_count = 0","","","# Convenience function","def admit_candidates(","    policy: PortfolioPolicyV1,","    equity_base: float,","    candidates: List[SignalCandidateV1],","    current_open_positions: Optional[List[OpenPositionV1]] = None,",") -> Tuple[List[AdmissionDecisionV1], PortfolioSummaryV1]:","    \"\"\"","    Convenience function for one-shot admission.","    ","    Returns:","        Tuple of (decisions, summary)","    \"\"\"","    engine = PortfolioEngineV1(policy, equity_base)","    decisions = engine.admit_candidates(candidates, current_open_positions)","    summary = engine.get_summary()","    return decisions, summary"]}
{"type":"file_footer","path":"src/portfolio/engine_v1.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/portfolio/examples/portfolio_mvp_2026Q1.yaml","kind":"text","encoding":"utf-8","newline":"lf","bytes":592,"sha256":"8aa374296c97ed56b4c61d55e90592b8190fbab3d2aa5aae5d62c98f3dec637b","total_lines":26,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/examples/portfolio_mvp_2026Q1.yaml","chunk_index":0,"line_start":1,"line_end":26,"content":["portfolio_id: \"mvp\"","version: \"2026Q1\"","data_tz: \"Asia/Taipei\"","legs:","  - leg_id: \"mnq_60_sma\"","    symbol: \"CME.MNQ\"","    timeframe_min: 60","    session_profile: \"CME_MNQ_v2.yaml\"","    strategy_id: \"sma_cross\"","    strategy_version: \"v1\"","    params:","      fast_period: 10.0","      slow_period: 40.0","    enabled: true","    tags: [\"mvp\", \"cme\"]","","  - leg_id: \"mxf_60_mrz\"","    symbol: \"TWF.MXF\"","    timeframe_min: 60","    session_profile: \"TWF_MXF_v2.yaml\"","    strategy_id: \"mean_revert_zscore\"","    strategy_version: \"v1\"","    params:","      zscore_threshold: -2.0","    enabled: true","    tags: [\"mvp\", \"twf\"]"]}
{"type":"file_footer","path":"src/portfolio/examples/portfolio_mvp_2026Q1.yaml","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/hash_utils.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":810,"sha256":"536d3ac940db2659117933fd8ba24fb044975628d0671590198b0251388dde3d","total_lines":35,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/hash_utils.py","chunk_index":0,"line_start":1,"line_end":35,"content":["","\"\"\"Hash utilities for deterministic portfolio ID generation.\"\"\"","","import hashlib","import json","from typing import Any","","","def stable_json_dumps(obj: Any) -> str:","    \"\"\"Deterministic JSON dumps: sort_keys=True, separators=(',', ':'), ensure_ascii=False\"\"\"","    return json.dumps(","        obj,","        sort_keys=True,","        separators=(',', ':'),","        ensure_ascii=False,","        default=str  # Handle non-serializable types","    )","","","def sha1_text(s: str) -> str:","    \"\"\"SHA1 hex digest for text.\"\"\"","    return hashlib.sha1(s.encode('utf-8')).hexdigest()","","","def hash_params(params: dict[str, float]) -> str:","    \"\"\"","    Deterministic hash of strategy parameters.","    ","    Uses stable JSON serialization and SHA1.","    \"\"\"","    if not params:","        return \"empty\"","    return sha1_text(stable_json_dumps(params))","",""]}
{"type":"file_footer","path":"src/portfolio/hash_utils.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/instruments.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3680,"sha256":"e751f522278d6b0f8b9c1cab34503cdca42544094173c6e9356d4c27ddcb7c4e","total_lines":109,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/instruments.py","chunk_index":0,"line_start":1,"line_end":109,"content":["\"\"\"Instrument configuration loader with deterministic SHA256 hashing.\"\"\"","","from pathlib import Path","from dataclasses import dataclass","import hashlib","from typing import Dict","","import yaml","","","@dataclass(frozen=True)","class InstrumentSpec:","    \"\"\"Specification for a single instrument.\"\"\"","    instrument: str","    currency: str","    multiplier: float","    initial_margin_per_contract: float","    maintenance_margin_per_contract: float","    margin_basis: str = \"\"  # optional: exchange_maintenance, conservative_over_exchange, broker_day","","","@dataclass(frozen=True)","class InstrumentsConfig:","    \"\"\"Loaded instruments configuration with SHA256 hash.\"\"\"","    version: int","    base_currency: str","    fx_rates: Dict[str, float]","    instruments: Dict[str, InstrumentSpec]","    sha256: str","","","def load_instruments_config(path: Path) -> InstrumentsConfig:","    \"\"\"","    Load instruments configuration from YAML file.","    ","    Args:","        path: Path to instruments.yaml","        ","    Returns:","        InstrumentsConfig with SHA256 hash of canonical YAML bytes.","        ","    Raises:","        FileNotFoundError: if file does not exist","        yaml.YAMLError: if YAML is malformed","        KeyError: if required fields are missing","        ValueError: if validation fails (e.g., base_currency not in fx_rates)","    \"\"\"","    # Read raw bytes for deterministic SHA256","    raw_bytes = path.read_bytes()","    sha256 = hashlib.sha256(raw_bytes).hexdigest()","    ","    # Parse YAML","    data = yaml.safe_load(raw_bytes)","    ","    # Validate version","    version = data.get(\"version\")","    if version != 1:","        raise ValueError(f\"Unsupported version: {version}, expected 1\")","    ","    # Validate base_currency","    base_currency = data.get(\"base_currency\")","    if not base_currency:","        raise KeyError(\"Missing 'base_currency'\")","    ","    # Validate fx_rates","    fx_rates = data.get(\"fx_rates\", {})","    if not isinstance(fx_rates, dict):","        raise ValueError(\"'fx_rates' must be a dict\")","    if base_currency not in fx_rates:","        raise ValueError(f\"base_currency '{base_currency}' must be present in fx_rates\")","    if fx_rates.get(base_currency) != 1.0:","        raise ValueError(f\"fx_rates[{base_currency}] must be 1.0\")","    ","    # Validate instruments","    instruments_raw = data.get(\"instruments\", {})","    if not isinstance(instruments_raw, dict):","        raise ValueError(\"'instruments' must be a dict\")","    ","    instruments = {}","    for instrument_key, spec_dict in instruments_raw.items():","        # Validate required fields","        required = [\"currency\", \"multiplier\", \"initial_margin_per_contract\", \"maintenance_margin_per_contract\"]","        for field in required:","            if field not in spec_dict:","                raise KeyError(f\"Instrument '{instrument_key}' missing field '{field}'\")","        ","        # Validate currency exists in fx_rates","        currency = spec_dict[\"currency\"]","        if currency not in fx_rates:","            raise ValueError(f\"Instrument '{instrument_key}' currency '{currency}' not in fx_rates\")","        ","        # Create InstrumentSpec","        spec = InstrumentSpec(","            instrument=instrument_key,","            currency=currency,","            multiplier=float(spec_dict[\"multiplier\"]),","            initial_margin_per_contract=float(spec_dict[\"initial_margin_per_contract\"]),","            maintenance_margin_per_contract=float(spec_dict[\"maintenance_margin_per_contract\"]),","            margin_basis=spec_dict.get(\"margin_basis\", \"\"),","        )","        instruments[instrument_key] = spec","    ","    return InstrumentsConfig(","        version=version,","        base_currency=base_currency,","        fx_rates=fx_rates,","        instruments=instruments,","        sha256=sha256,","    )"]}
{"type":"file_footer","path":"src/portfolio/instruments.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/loader.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4174,"sha256":"30d065566b7c7deb9fdff240a5606cc8acbbaba1884cd14032eacf977c6b7251","total_lines":126,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/loader.py","chunk_index":0,"line_start":1,"line_end":126,"content":["","\"\"\"Portfolio specification loader.","","Phase 8: Load portfolio specs from YAML/JSON files.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Any, Dict, List","","import yaml","","from portfolio.spec import PortfolioLeg, PortfolioSpec","","","def load_portfolio_spec(path: Path) -> PortfolioSpec:","    \"\"\"Load portfolio specification from YAML or JSON file.","    ","    Args:","        path: Path to portfolio spec file (.yaml, .yml, or .json)","        ","    Returns:","        PortfolioSpec loaded from file","        ","    Raises:","        FileNotFoundError: If file does not exist","        ValueError: If file format is invalid","    \"\"\"","    if not path.exists():","        raise FileNotFoundError(f\"Portfolio spec not found: {path}\")","    ","    # Load based on file extension","    suffix = path.suffix.lower()","    if suffix in [\".yaml\", \".yml\"]:","        with path.open(\"r\", encoding=\"utf-8\") as f:","            data = yaml.safe_load(f)","    elif suffix == \".json\":","        with path.open(\"r\", encoding=\"utf-8\") as f:","            data = json.load(f)","    else:","        raise ValueError(f\"Unsupported file format: {suffix}. Must be .yaml, .yml, or .json\")","    ","    if not isinstance(data, dict):","        raise ValueError(f\"Invalid portfolio format: expected dict, got {type(data)}\")","    ","    # Extract fields","    portfolio_id = data.get(\"portfolio_id\")","    version = data.get(\"version\")","    data_tz = data.get(\"data_tz\", \"Asia/Taipei\")","    legs_data = data.get(\"legs\", [])","    ","    if not portfolio_id:","        raise ValueError(\"Portfolio spec missing 'portfolio_id' field\")","    if not version:","        raise ValueError(\"Portfolio spec missing 'version' field\")","    ","    # Load legs","    legs = []","    for leg_data in legs_data:","        if not isinstance(leg_data, dict):","            raise ValueError(f\"Leg must be dict, got {type(leg_data)}\")","        ","        leg_id = leg_data.get(\"leg_id\")","        symbol = leg_data.get(\"symbol\")","        timeframe_min = leg_data.get(\"timeframe_min\")","        session_profile = leg_data.get(\"session_profile\")","        strategy_id = leg_data.get(\"strategy_id\")","        strategy_version = leg_data.get(\"strategy_version\")","        params = leg_data.get(\"params\", {})","        enabled = leg_data.get(\"enabled\", True)","        tags = leg_data.get(\"tags\", [])","        ","        # Validate required fields","        if not leg_id:","            raise ValueError(\"Leg missing 'leg_id' field\")","        if not symbol:","            raise ValueError(f\"Leg '{leg_id}' missing 'symbol' field\")","        if timeframe_min is None:","            raise ValueError(f\"Leg '{leg_id}' missing 'timeframe_min' field\")","        if not session_profile:","            raise ValueError(f\"Leg '{leg_id}' missing 'session_profile' field\")","        if not strategy_id:","            raise ValueError(f\"Leg '{leg_id}' missing 'strategy_id' field\")","        if not strategy_version:","            raise ValueError(f\"Leg '{leg_id}' missing 'strategy_version' field\")","        ","        # Convert params values to float","        if not isinstance(params, dict):","            raise ValueError(f\"Leg '{leg_id}' params must be dict, got {type(params)}\")","        ","        params_float = {}","        for key, value in params.items():","            try:","                params_float[key] = float(value)","            except (ValueError, TypeError) as e:","                raise ValueError(","                    f\"Leg '{leg_id}' param '{key}' must be numeric, got {type(value)}: {e}\"","                )","        ","        # Convert tags to list","        if not isinstance(tags, list):","            raise ValueError(f\"Leg '{leg_id}' tags must be list, got {type(tags)}\")","        ","        leg = PortfolioLeg(","            leg_id=leg_id,","            symbol=symbol,","            timeframe_min=int(timeframe_min),","            session_profile=session_profile,","            strategy_id=strategy_id,","            strategy_version=strategy_version,","            params=params_float,","            enabled=bool(enabled),","            tags=list(tags),","        )","        legs.append(leg)","    ","    return PortfolioSpec(","        portfolio_id=portfolio_id,","        version=version,","        data_tz=data_tz,","        legs=legs,","    )","",""]}
{"type":"file_footer","path":"src/portfolio/loader.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/plan_builder.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":24787,"sha256":"e065e0e2a6098b7a0e50703386fc155c3c5e6fe73693f376b0cceac22b1273b4","total_lines":714,"chunk_count":4}
{"type":"file_chunk","path":"src/portfolio/plan_builder.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 17 rev2: Portfolio Plan Builder (deterministic, read‑only over exports).","","Contracts:","- Only reads from exports tree (no artifacts, no engine).","- Deterministic tie‑break ordering.","- Controlled mutation: writes only under outputs/portfolio/plans/{plan_id}/","- Hash chain audit (plan_manifest.json with self‑hash).","- Enrichment via batch_api (optional, best‑effort).","\"\"\"","","from __future__ import annotations","","import json","import os","from dataclasses import dataclass","from datetime import datetime, timezone","from decimal import Decimal, ROUND_HALF_UP, getcontext","from pathlib import Path","from typing import Any, Dict, List, Optional, Tuple","","# pydantic ValidationError not used; removed to avoid import error","","from contracts.portfolio.plan_payloads import PlanCreatePayload","from contracts.portfolio.plan_models import (","    ConstraintsReport,","    PlannedCandidate,","    PlannedWeight,","    PlanSummary,","    PortfolioPlan,","    SourceRef,",")","","# LEGAL gateway for artifacts reads","from control import batch_api  # Phase 14.1 read-only gateway","","# Use existing repo utilities","from control.artifacts import (","    canonical_json_bytes,","    compute_sha256,","    write_atomic_json,",")","","# Write‑scope guard","from utils.write_scope import create_plan_scope","","getcontext().prec = 40","","","# -----------------------------","# Helpers: canonical json + sha256","# -----------------------------","def canonical_json(obj: Any) -> str:","    # Use repo standard canonical_json_bytes and decode to string","    return canonical_json_bytes(obj).decode(\"utf-8\")","","","def sha256_bytes(b: bytes) -> str:","    return compute_sha256(b)","","","def sha256_text(s: str) -> str:","    return sha256_bytes(s.encode(\"utf-8\"))","","","def read_json(path: Path) -> Any:","    return json.loads(path.read_text(encoding=\"utf-8\"))","","","def write_text_atomic(path: Path, text: str) -> None:","    # deterministic-ish atomic write","    tmp = path.with_suffix(path.suffix + \".tmp\")","    tmp.write_text(text, encoding=\"utf-8\")","    os.replace(tmp, path)","","","def ensure_dir(p: Path) -> None:","    p.mkdir(parents=True, exist_ok=True)","","","# -----------------------------","# Candidate input model (loose)","# -----------------------------","@dataclass(frozen=True)","class CandidateIn:","    candidate_id: str","    strategy_id: str","    dataset_id: str","    params: Dict[str, Any]","    score: float","    season: str","    source_batch: str","    source_export: str","","","def _candidate_sort_key(c: CandidateIn) -> Tuple:","    # score DESC => use negative","    params_canon = canonical_json(c.params)","    return (-float(c.score), c.strategy_id, c.dataset_id, c.source_batch, params_canon, c.candidate_id)","","","def _candidate_id(c: CandidateIn) -> str:","    # Deterministic candidate_id from core fields","    # NOTE: do not include export_name here; source_export stored separately.","    payload = {","        \"strategy_id\": c.strategy_id,","        \"dataset_id\": c.dataset_id,","        \"params\": c.params,","        \"source_batch\": c.source_batch,","        \"season\": c.season,","    }","    return \"cand_\" + sha256_text(canonical_json(payload))[:16]","","","# -----------------------------","# Selection constraints","# -----------------------------","@dataclass","class SelectionReport:","    max_per_strategy_truncated: Dict[str, int] = None  # type: ignore","    max_per_dataset_truncated: Dict[str, int] = None   # type: ignore","","    def __post_init__(self):","        if self.max_per_strategy_truncated is None:","            self.max_per_strategy_truncated = {}","        if self.max_per_dataset_truncated is None:","            self.max_per_dataset_truncated = {}","","","def apply_selection_constraints(","    candidates_sorted: List[CandidateIn],","    top_n: int,","    max_per_strategy: int,","    max_per_dataset: int,",") -> Tuple[List[CandidateIn], SelectionReport]:","    limited = candidates_sorted[:top_n]","    per_strat: Dict[str, int] = {}","    per_ds: Dict[str, int] = {}","    selected: List[CandidateIn] = []","    rep = SelectionReport()","","    for c in limited:","        s_ok = per_strat.get(c.strategy_id, 0) < max_per_strategy","        d_ok = per_ds.get(c.dataset_id, 0) < max_per_dataset","","        if not s_ok:","            rep.max_per_strategy_truncated[c.strategy_id] = rep.max_per_strategy_truncated.get(c.strategy_id, 0) + 1","        if not d_ok:","            rep.max_per_dataset_truncated[c.dataset_id] = rep.max_per_dataset_truncated.get(c.dataset_id, 0) + 1","","        if s_ok and d_ok:","            selected.append(c)","            per_strat[c.strategy_id] = per_strat.get(c.strategy_id, 0) + 1","            per_ds[c.dataset_id] = per_ds.get(c.dataset_id, 0) + 1","","    return selected, rep","","","# -----------------------------","# Weighting + clip + renorm","# -----------------------------","@dataclass(frozen=True)","class WeightItem:","    candidate_id: str","    weight: float","","","def _to_dec(x: float) -> Decimal:","    return Decimal(str(x))","","","def _round_dec(x: Decimal, places: int = 12) -> Decimal:","    q = Decimal(\"1.\" + (\"0\" * places))","    return x.quantize(q, rounding=ROUND_HALF_UP)","","","def clip_and_renormalize_deterministic(","    items: List[WeightItem],","    min_w: float,","    max_w: float,","    *,","    places: int = 12,","    tol: float = 1e-9,",") -> Tuple[List[WeightItem], Dict[str, Any]]:","    if not items:","        return [], {","            \"max_weight_clipped\": [],","            \"min_weight_clipped\": [],","            \"renormalization_applied\": False,","            \"renormalization_factor\": None,","        }","","    min_d = _to_dec(min_w)","    max_d = _to_dec(max_w)","    max_clipped_ids: set[str] = set()","    min_clipped_ids: set[str] = set()","","    clipped: List[Tuple[str, Decimal]] = []","    for it in items:"]}
{"type":"file_chunk","path":"src/portfolio/plan_builder.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        w = _to_dec(it.weight)","        if w > max_d:","            w = max_d","            max_clipped_ids.add(it.candidate_id)","        if w < min_d:","            w = min_d","            min_clipped_ids.add(it.candidate_id)","        clipped.append((it.candidate_id, w))","","    total = sum(w for _, w in clipped)","    if total == Decimal(\"0\"):","        # deterministic fallback: equal","        n = Decimal(len(clipped))","        eq = Decimal(\"1\") / n","        clipped = [(cid, eq) for cid, _ in clipped]","        total = sum(w for _, w in clipped)","","    scaled = [(cid, (w / total)) for cid, w in clipped]","    rounded = [(cid, _round_dec(w, places)) for cid, w in scaled]","    rounded_total = sum(w for _, w in rounded)","","    one = Decimal(\"1\")","    unit = Decimal(\"1\") / (Decimal(10) ** places)","    residual = one - rounded_total","","    ticks = int((residual / unit).to_integral_value(rounding=ROUND_HALF_UP))","    order = sorted(range(len(rounded)), key=lambda i: rounded[i][0])  # cid asc","    updated = [(cid, w) for cid, w in rounded]  # keep as tuple","","    if ticks != 0:","        step = unit if ticks > 0 else -unit","        ticks_abs = abs(ticks)","        idx = 0","        while ticks_abs > 0:","            i = order[idx % len(order)]","            cid, w = updated[i]","            new_w = w + step","            if Decimal(\"0\") <= new_w <= Decimal(\"1\"):","                updated[i] = (cid, new_w)","                ticks_abs -= 1","            idx += 1","","    final_total = sum(w for _, w in updated)","    # Convert to floats","    out_map = {cid: float(w) for cid, w in updated}","    out_items = [WeightItem(it.candidate_id, out_map[it.candidate_id]) for it in items]","","    renormalization_applied = bool(max_clipped_ids or min_clipped_ids or (abs(float(rounded_total) - 1.0) > tol))","    renormalization_factor = float(Decimal(\"1\") / total) if total != Decimal(\"0\") and renormalization_applied else None","","    report = {","        \"max_weight_clipped\": sorted(list(max_clipped_ids)),","        \"min_weight_clipped\": sorted(list(min_clipped_ids)),","        \"renormalization_applied\": renormalization_applied,","        \"renormalization_factor\": renormalization_factor,","        \"final_total\": float(final_total),","    }","    return out_items, report","","","def assign_weights_equal(selected: List[CandidateIn], min_w: float, max_w: float) -> Tuple[List[WeightItem], Dict[str, Any]]:","    n = len(selected)","    base = 1.0 / n","    items = [WeightItem(c.candidate_id, base) for c in selected]","    return clip_and_renormalize_deterministic(items, min_w, max_w)","","","def assign_weights_bucket_equal(","    selected: List[CandidateIn],","    bucket_by: List[str],","    min_w: float,","    max_w: float,",") -> Tuple[List[WeightItem], Dict[str, Any]]:","    # Build buckets","    def bucket_key(c: CandidateIn) -> Tuple:","        k = []","        for b in bucket_by:","            if b == \"dataset_id\":","                k.append(c.dataset_id)","            elif b == \"strategy_id\":","                k.append(c.strategy_id)","            else:","                raise ValueError(f\"Unknown bucket key: {b}\")","        return tuple(k)","","    buckets: Dict[Tuple, List[CandidateIn]] = {}","    for c in selected:","        buckets.setdefault(bucket_key(c), []).append(c)","","    num_buckets = len(buckets)","    bucket_weight = 1.0 / num_buckets","","    items: List[WeightItem] = []","    for k in sorted(buckets.keys()):  # deterministic bucket ordering","        members = buckets[k]","        w_each = bucket_weight / len(members)","        for c in sorted(members, key=_candidate_sort_key):  # deterministic in-bucket","            items.append(WeightItem(c.candidate_id, w_each))","","    return clip_and_renormalize_deterministic(items, min_w, max_w)","","","def assign_weights_score_weighted(selected: List[CandidateIn], min_w: float, max_w: float) -> Tuple[List[WeightItem], Dict[str, Any]]:","    scores = [float(c.score) for c in selected]","    sum_scores = sum(scores)","","    items: List[WeightItem] = []","    if sum_scores > 0 and all(s > 0 for s in scores):","        for c in selected:","            items.append(WeightItem(c.candidate_id, float(c.score) / sum_scores))","    else:","        # deterministic fallback: rank-based weights (higher score gets larger weight)","        ranked = sorted(selected, key=_candidate_sort_key)","        # ranked is already score desc via _candidate_sort_key (negative score)","        n = len(ranked)","        # weights proportional to (n-rank)","        denom = n * (n + 1) / 2","        for i, c in enumerate(ranked):","            w = (n - i) / denom","            items.append(WeightItem(c.candidate_id, w))","","    return clip_and_renormalize_deterministic(items, min_w, max_w)","","","# -----------------------------","# Export pack loading","# -----------------------------","def export_dir(exports_root: Path, season: str, export_name: str) -> Path:","    return exports_root / \"seasons\" / season / export_name","","","def load_export_manifest(exports_root: Path, season: str, export_name: str) -> Tuple[Dict[str, Any], str]:","    p = export_dir(exports_root, season, export_name) / \"manifest.json\"","    if not p.exists():","        raise FileNotFoundError(str(p))","    data = read_json(p)","    # Deterministic manifest hash uses canonical json (not raw bytes) for stability","    export_manifest_sha256 = sha256_text(canonical_json(data))","    return data, export_manifest_sha256","","","def load_candidates(exports_root: Path, season: str, export_name: str) -> Tuple[List[CandidateIn], str]:","    p = export_dir(exports_root, season, export_name) / \"candidates.json\"","    if not p.exists():","        raise FileNotFoundError(str(p))","    raw_bytes = p.read_bytes()","    candidates_sha256 = sha256_bytes(raw_bytes)","","    arr = json.loads(raw_bytes.decode(\"utf-8\"))","    if not isinstance(arr, list):","        raise ValueError(\"candidates.json must be a list\")","","    out: List[CandidateIn] = []","    for row in arr:","        out.append(","            CandidateIn(","                candidate_id=row[\"candidate_id\"],","                strategy_id=row[\"strategy_id\"],","                dataset_id=row[\"dataset_id\"],","                params=row.get(\"params\", {}) or {},","                score=float(row[\"score\"]),","                season=row.get(\"season\", season),","                source_batch=row[\"source_batch\"],","                source_export=row.get(\"source_export\", export_name),","            )","        )","    return out, candidates_sha256","","","# -----------------------------","# Legacy summary computation (for backward compatibility)","# -----------------------------","from collections import defaultdict","from typing import Dict, List","","from contracts.portfolio.plan_models import PlanSummary","","","def _bucket_key(candidate, bucket_by: List[str]) -> str:","    \"\"\"","    Deterministic bucket key.","    Example: bucket_by=[\"dataset_id\"] => \"dataset_id=ds1\"","    Multiple fields => \"dataset_id=ds1|strategy_id=stratA\"","    \"\"\"","    parts = []","    for f in bucket_by:","        v = getattr(candidate, f, None)","        parts.append(f\"{f}={v}\")","    return \"|\".join(parts)","","","def _compute_summary_legacy(universe: list, weights: list, bucket_by: List[str]) -> PlanSummary:","    \"\"\"","    universe: List[PlannedCandidate]","    weights:  List[PlannedWeight] (candidate_id, weight)","    \"\"\"","    # Map candidate_id -> weight","    wmap: Dict[str, float] = {w.candidate_id: float(w.weight) for w in weights}","","    total_candidates = len(universe)"]}
{"type":"file_chunk","path":"src/portfolio/plan_builder.py","chunk_index":2,"line_start":401,"line_end":600,"content":["    total_weight = sum(wmap.get(c.candidate_id, 0.0) for c in universe)","","    # bucket counts / weights","    b_counts: Dict[str, int] = defaultdict(int)","    b_weights: Dict[str, float] = defaultdict(float)","","    for c in universe:","        b = _bucket_key(c, bucket_by)","        b_counts[b] += 1","        b_weights[b] += wmap.get(c.candidate_id, 0.0)","","    # concentration_herfindahl = sum_i w_i^2","    herf = 0.0","    for c in universe:","        w = wmap.get(c.candidate_id, 0.0)","        herf += w * w","","    # Optional new fields (best effort)","    # concentration_top1/top3 from sorted weights","    ws_sorted = sorted([wmap.get(c.candidate_id, 0.0) for c in universe], reverse=True)","    top1 = ws_sorted[0] if ws_sorted else 0.0","    top3 = sum(ws_sorted[:3]) if ws_sorted else 0.0","","    return PlanSummary(","        # legacy fields","        total_candidates=total_candidates,","        total_weight=float(total_weight),","        bucket_counts=dict(b_counts),","        bucket_weights=dict(b_weights),","        concentration_herfindahl=float(herf),","        # new optional fields","        num_selected=total_candidates,","        num_buckets=len(b_counts),","        bucket_by=list(bucket_by),","        concentration_top1=float(top1),","        concentration_top3=float(top3),","    )","","","# -----------------------------","# Plan ID + building","# -----------------------------","def compute_plan_id(export_manifest_sha256: str, candidates_file_sha256: str, payload: PlanCreatePayload) -> str:","    pid = sha256_text(","        canonical_json(","            {","                \"export_manifest_sha256\": export_manifest_sha256,","                \"candidates_file_sha256\": candidates_file_sha256,","                \"payload\": json.loads(payload.model_dump_json()),","            }","        )","    )[:16]","    return \"plan_\" + pid","","","def build_portfolio_plan_from_export(","    *,","    exports_root: Path,","    season: str,","    export_name: str,","    payload: PlanCreatePayload,","    # batch_api needs artifacts_root; passing in is allowed.","    artifacts_root: Optional[Path] = None,",") -> PortfolioPlan:","    \"\"\"","    Read-only over exports tree.","    Enrichment (optional) uses batch_api as the ONLY allowed artifacts access.","","    Raises:","      FileNotFoundError: export missing","      ValueError: business rule invalid (e.g. no candidates selected)","    \"\"\"","    _manifest, export_manifest_sha256 = load_export_manifest(exports_root, season, export_name)","    candidates, candidates_sha256 = load_candidates(exports_root, season, export_name)","    candidates_file_sha256 = candidates_sha256","    candidates_items_sha256 = None","","    candidates_sorted = sorted(candidates, key=_candidate_sort_key)","","    selected, sel_rep = apply_selection_constraints(","        candidates_sorted,","        payload.top_n,","        payload.max_per_strategy,","        payload.max_per_dataset,","    )","","    if not selected:","        raise ValueError(\"No candidates selected for plan\")","","    # Weighting","    bucket_by = [str(b) for b in payload.bucket_by]  # ensure List[str]","    if payload.weighting == \"bucket_equal\":","        weight_items, w_rep = assign_weights_bucket_equal(selected, bucket_by, payload.min_weight, payload.max_weight)","        reason = \"bucket_equal\"","    elif payload.weighting == \"equal\":","        weight_items, w_rep = assign_weights_equal(selected, payload.min_weight, payload.max_weight)","        reason = \"equal\"","    elif payload.weighting == \"score_weighted\":","        weight_items, w_rep = assign_weights_score_weighted(selected, payload.min_weight, payload.max_weight)","        reason = \"score_weighted\"","    else:","        raise ValueError(f\"Unknown weighting policy: {payload.weighting}\")","","    # Build planned universe + weights","    # weight_items order matches construction; but we also want stable mapping by candidate_id","    w_map = {wi.candidate_id: wi.weight for wi in weight_items}","","    universe: List[PlannedCandidate] = []","    weights: List[PlannedWeight] = []","","    # Deterministic universe order: use selected order (already deterministic)","    for c in selected:","        cid = c.candidate_id","        universe.append(","            PlannedCandidate(","                candidate_id=cid,","                strategy_id=c.strategy_id,","                dataset_id=c.dataset_id,","                params=c.params,","                score=float(c.score),","                season=season,","                source_batch=c.source_batch,","                source_export=export_name,","            )","        )","        weights.append(","            PlannedWeight(","                candidate_id=cid,","                weight=float(w_map[cid]),","                reason=reason,","            )","        )","","    # Enrichment via batch_api (optional)","    if payload.enrich_with_batch_api:","        if artifacts_root is None:","            # No artifacts root => cannot enrich, but should not fail","            artifacts_root = None","","        if artifacts_root is not None:","            # cache per batch_id to keep deterministic + efficient","            cache: Dict[str, Dict[str, Any]] = {}","            for pc in universe:","                bid = pc.source_batch","                if bid not in cache:","                    cache[bid] = {\"batch_state\": None, \"batch_counts\": None, \"batch_metrics\": None}","                    # batch_state + counts","                    try:","                        if \"batch_state\" in payload.enrich_fields or \"batch_counts\" in payload.enrich_fields:","                            # use batch_api.read_execution","                            ex = batch_api.read_execution(artifacts_root, bid)","                            cache[bid][\"batch_state\"] = batch_api.get_batch_state(ex)","                            cache[bid][\"batch_counts\"] = batch_api.count_states(ex)","                    except Exception:","                        pass","                    # batch_metrics","                    try:","                        if \"batch_metrics\" in payload.enrich_fields:","                            s = batch_api.read_summary(artifacts_root, bid)","                            cache[bid][\"batch_metrics\"] = s.get(\"metrics\", {})","                    except Exception:","                        pass","                # assign enrichment","                pc.batch_state = cache[bid][\"batch_state\"]","                pc.batch_counts = cache[bid][\"batch_counts\"]","                pc.batch_metrics = cache[bid][\"batch_metrics\"]","","    # Build constraints report","    constraints_report = ConstraintsReport(","        max_per_strategy_truncated=sel_rep.max_per_strategy_truncated,","        max_per_dataset_truncated=sel_rep.max_per_dataset_truncated,","        max_weight_clipped=w_rep.get(\"max_weight_clipped\", []),","        min_weight_clipped=w_rep.get(\"min_weight_clipped\", []),","        renormalization_applied=w_rep.get(\"renormalization_applied\", False),","        renormalization_factor=w_rep.get(\"renormalization_factor\"),","    )","","    # Build plan summary (legacy schema for backward compatibility)","    plan_summary = _compute_summary_legacy(universe, weights, bucket_by)","","    # Build source ref","    source_ref = SourceRef(","        season=season,","        export_name=export_name,","        export_manifest_sha256=export_manifest_sha256,","        candidates_sha256=candidates_sha256,","        candidates_file_sha256=candidates_file_sha256,","        candidates_items_sha256=candidates_items_sha256,","    )","","    # Build plan ID","    plan_id = compute_plan_id(export_manifest_sha256, candidates_file_sha256, payload)","","    # Build portfolio plan","    plan = PortfolioPlan(","        plan_id=plan_id,","        generated_at_utc=datetime.now(timezone.utc).isoformat(),","        source=source_ref,","        config=payload.model_dump(),","        universe=universe,"]}
{"type":"file_chunk","path":"src/portfolio/plan_builder.py","chunk_index":3,"line_start":601,"line_end":714,"content":["        weights=weights,","        constraints_report=constraints_report,","        summaries=plan_summary,","    )","    return plan","","","def _plan_dir(outputs_root: Path, plan_id: str) -> Path:","    return outputs_root / \"portfolio\" / \"plans\" / plan_id","","","def write_plan_package(outputs_root: Path, plan) -> Path:","    \"\"\"","    Controlled mutation ONLY:","      outputs/portfolio/plans/{plan_id}/","","    Idempotent:","      if plan_dir exists -> do not rewrite.","    \"\"\"","    pdir = _plan_dir(outputs_root, plan.plan_id)","    if pdir.exists():","        return pdir","","    # Ensure directory","    ensure_dir(pdir)","","    # Create write scope for this plan directory","    scope = create_plan_scope(pdir)","","    # Helper to write a file with scope validation","    def write_scoped(rel_path: str, content: str) -> None:","        scope.assert_allowed_rel(rel_path)","        write_text_atomic(pdir / rel_path, content)","","    # 1) portfolio_plan.json (canonical)","    plan_obj = plan.model_dump() if hasattr(plan, \"model_dump\") else plan","    plan_json = canonical_json(plan_obj)","    write_scoped(\"portfolio_plan.json\", plan_json)","","    # 2) plan_metadata.json (minimal)","    meta = {","        \"plan_id\": plan.plan_id,","        \"generated_at_utc\": getattr(plan, \"generated_at_utc\", None),","        \"source\": plan.source.model_dump() if hasattr(plan, \"source\") else None,","        \"note\": (plan.config.get(\"note\") if hasattr(plan, \"config\") and isinstance(plan.config, dict) else None),","    }","    write_scoped(\"plan_metadata.json\", canonical_json(meta))","","    # 3) plan_checksums.json (flat dict)","    checksums = {}","    for rel in [\"plan_metadata.json\", \"portfolio_plan.json\"]:","        # Reading already‑written files is safe; they are inside the scope.","        checksums[rel] = sha256_bytes((pdir / rel).read_bytes())","    write_scoped(\"plan_checksums.json\", canonical_json(checksums))","","    # 4) plan_manifest.json (two-phase self hash)","    portfolio_plan_sha256 = sha256_bytes((pdir / \"portfolio_plan.json\").read_bytes())","    checksums = json.loads((pdir / \"plan_checksums.json\").read_text(encoding=\"utf-8\"))","","    # Source hashes","    export_manifest_sha256 = getattr(plan.source, \"export_manifest_sha256\", None)","    candidates_sha256 = getattr(plan.source, \"candidates_sha256\", None)","    candidates_file_sha256 = getattr(plan.source, \"candidates_file_sha256\", None)","    candidates_items_sha256 = getattr(plan.source, \"candidates_items_sha256\", None)","","    # Build files listing (sorted by rel_path asc)","    files = []","    for rel_path in [\"portfolio_plan.json\", \"plan_metadata.json\", \"plan_checksums.json\"]:","        file_path = pdir / rel_path","        if file_path.exists():","            files.append({","                \"rel_path\": rel_path,","                \"sha256\": sha256_bytes(file_path.read_bytes())","            })","    # Sort by rel_path","    files.sort(key=lambda x: x[\"rel_path\"])","    ","    # Compute files_sha256 (concatenated hashes)","    concatenated = \"\".join(f[\"sha256\"] for f in files)","    files_sha256 = sha256_bytes(concatenated.encode(\"utf-8\"))","","    # Build manifest with fields expected by tests","    manifest_base = {","        \"manifest_type\": \"plan\",","        \"manifest_version\": \"1.0\",","        \"id\": plan.plan_id,","        \"plan_id\": plan.plan_id,","        \"generated_at_utc\": getattr(plan, \"generated_at_utc\", None),","        \"source\": plan.source.model_dump() if hasattr(plan.source, \"model_dump\") else plan.source,","        \"config\": plan.config if isinstance(plan.config, dict) else plan.config.model_dump(),","        \"summaries\": plan.summaries.model_dump() if hasattr(plan.summaries, \"model_dump\") else plan.summaries,","        \"export_manifest_sha256\": export_manifest_sha256,","        \"candidates_sha256\": candidates_sha256,","        \"candidates_file_sha256\": candidates_file_sha256,","        \"candidates_items_sha256\": candidates_items_sha256,","        \"portfolio_plan_sha256\": portfolio_plan_sha256,","        \"checksums\": checksums,","        \"files\": files,","        \"files_sha256\": files_sha256,","    }","","    manifest_path = pdir / \"plan_manifest.json\"","    # phase-1","    write_scoped(\"plan_manifest.json\", canonical_json(manifest_base))","    # self-hash of phase-1 canonical bytes","    manifest_sha256 = sha256_bytes(manifest_path.read_bytes())","    # phase-2","    manifest_final = dict(manifest_base)","    manifest_final[\"manifest_sha256\"] = manifest_sha256","    write_scoped(\"plan_manifest.json\", canonical_json(manifest_final))","","    return pdir","",""]}
{"type":"file_footer","path":"src/portfolio/plan_builder.py","complete":true,"emitted_chunks":4}
{"type":"file_header","path":"src/portfolio/plan_explain_cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3559,"sha256":"82d1e74e64e79ce777e61652eccfaa58b44782993b83f19f6fe2ac049fe604b1","total_lines":111,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/plan_explain_cli.py","chunk_index":0,"line_start":1,"line_end":111,"content":["","\"\"\"CLI to generate and explain portfolio plan views.\"\"\"","import argparse","import json","import sys","from pathlib import Path","","from contracts.portfolio.plan_models import PortfolioPlan","","","# Helper function to get outputs root","def _get_outputs_root() -> Path:","    \"\"\"Get outputs root from environment or default.\"\"\"","    import os","    return Path(os.environ.get(\"FISHBRO_OUTPUTS_ROOT\", \"outputs\"))","","","def load_portfolio_plan(plan_dir: Path) -> PortfolioPlan:","    \"\"\"Load portfolio plan from directory.\"\"\"","    plan_path = plan_dir / \"portfolio_plan.json\"","    if not plan_path.exists():","        raise FileNotFoundError(f\"portfolio_plan.json not found in {plan_dir}\")","    ","    data = json.loads(plan_path.read_text(encoding=\"utf-8\"))","    return PortfolioPlan.model_validate(data)","","","def main():","    parser = argparse.ArgumentParser(","        description=\"Generate human-readable view of a portfolio plan.\"","    )","    parser.add_argument(","        \"--plan-id\",","        required=True,","        help=\"Plan ID (directory name under outputs/portfolio/plans/)\",","    )","    parser.add_argument(","        \"--top-n\",","        type=int,","        default=50,","        help=\"Number of top candidates to include in view (default: 50)\",","    )","    parser.add_argument(","        \"--dry-run\",","        action=\"store_true\",","        help=\"Render view but don't write files\",","    )","    ","    args = parser.parse_args()","    ","    # Locate plan directory","    outputs_root = _get_outputs_root()","    plan_dir = outputs_root / \"portfolio\" / \"plans\" / args.plan_id","    ","    if not plan_dir.exists():","        print(f\"Error: Plan directory not found: {plan_dir}\", file=sys.stderr)","        sys.exit(1)","    ","    # Load portfolio plan","    try:","        plan = load_portfolio_plan(plan_dir)","    except Exception as e:","        print(f\"Error loading portfolio plan: {e}\", file=sys.stderr)","        sys.exit(1)","    ","    # Import renderer here to avoid circular imports","    try:","        from portfolio.plan_view_renderer import render_plan_view, write_plan_view_files","    except ImportError as e:","        print(f\"Error importing plan view renderer: {e}\", file=sys.stderr)","        sys.exit(1)","    ","    # Render view","    try:","        view = render_plan_view(plan, top_n=args.top_n)","    except Exception as e:","        print(f\"Error rendering plan view: {e}\", file=sys.stderr)","        sys.exit(1)","    ","    if args.dry_run:","        # Print summary","        print(f\"Plan ID: {view.plan_id}\")","        print(f\"Generated at: {view.generated_at_utc}\")","        print(f\"Source season: {view.source.get('season', 'N/A')}\")","        print(f\"Total candidates: {view.universe_stats.get('total_candidates', 0)}\")","        print(f\"Selected candidates: {view.universe_stats.get('num_selected', 0)}\")","        print(f\"Top {len(view.top_candidates)} candidates rendered\")","        print(\"\\nDry run complete - no files written.\")","    else:","        # Write view files","        try:","            write_plan_view_files(plan_dir, view)","            print(f\"Successfully wrote plan view files to {plan_dir}\")","            print(f\"  - plan_view.json\")","            print(f\"  - plan_view.md\")","            print(f\"  - plan_view_checksums.json\")","            print(f\"  - plan_view_manifest.json\")","            ","            # Print markdown path for convenience","            md_path = plan_dir / \"plan_view.md\"","            if md_path.exists():","                print(f\"\\nView markdown: {md_path}\")","        except Exception as e:","            print(f\"Error writing plan view files: {e}\", file=sys.stderr)","            sys.exit(1)","","","if __name__ == \"__main__\":","    main()","",""]}
{"type":"file_footer","path":"src/portfolio/plan_explain_cli.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/plan_quality.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":15491,"sha256":"ad99db62a984079b10f38c0030c5e0eb558f7a011c31a27bd3ec3b9f570e3513","total_lines":428,"chunk_count":3}
{"type":"file_chunk","path":"src/portfolio/plan_quality.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Quality calculator for portfolio plans (read-only, deterministic).\"\"\"","from __future__ import annotations","","import hashlib","import json","from datetime import datetime, timezone","from pathlib import Path","from typing import Dict, List, Optional, Tuple, Any","","from contracts.portfolio.plan_models import PortfolioPlan, SourceRef","from contracts.portfolio.plan_quality_models import (","    PlanQualityReport,","    QualityMetrics,","    QualitySourceRef,","    QualityThresholds,","    Grade,",")","from contracts.portfolio.plan_view_models import PortfolioPlanView","from control.artifacts import compute_sha256, canonical_json_bytes","","","def _weights_from_plan(plan: PortfolioPlan) -> Optional[List[float]]:","    \"\"\"Extract normalized weight list from plan.weights.\"\"\"","    weights_obj = getattr(plan, \"weights\", None)","    if not weights_obj:","        return None","","    ws: List[float] = []","    for w in weights_obj:","        if isinstance(w, dict):","            v = w.get(\"weight\")","        else:","            v = getattr(w, \"weight\", None)","        if isinstance(v, (int, float)):","            ws.append(float(v))","","    if not ws:","        return None","","    s = sum(ws)","    if s <= 0:","        return None","    # normalize","    return [x / s for x in ws]","","","def _topk_and_concentration(ws: List[float]) -> Tuple[float, float, float, float, float]:","    \"\"\"Compute top1/top3/top5/herfindahl/effective_n from normalized weights.","    ","    Note: top1 here is the weight of the top candidate, not the score.","    The actual top1_score (candidate score) is computed separately.","    \"\"\"","    # ws already normalized","    ws_sorted = sorted(ws, reverse=True)","    top1_weight = ws_sorted[0] if ws_sorted else 0.0","    top3 = sum(ws_sorted[:3])","    top5 = sum(ws_sorted[:5])","    herf = sum(w * w for w in ws_sorted)","    eff_n = (1.0 / herf) if herf > 0 else 0.0","    return top1_weight, top3, top5, herf, eff_n","","","def compute_quality_from_plan(","    plan: PortfolioPlan,","    *,","    view: Optional[PortfolioPlanView] = None,","    thresholds: Optional[QualityThresholds] = None,",") -> PlanQualityReport:","    \"\"\"Pure function; read-only; deterministic.\"\"\"","    if thresholds is None:","        thresholds = QualityThresholds()","    ","    # Compute metrics","    metrics = _compute_metrics(plan, view)","    ","    # Determine grade and reasons","    grade, reasons = _grade_from_metrics(metrics, thresholds)","    ","    # Build source reference","    source = _build_source_ref(plan)","    ","    # Use deterministic timestamp from plan","    generated_at_utc = plan.generated_at_utc  # deterministic (do NOT use now())","    ","    # Inputs will be filled by caller if needed","    inputs: Dict[str, str] = {}","    ","    return PlanQualityReport(","        plan_id=plan.plan_id,","        generated_at_utc=generated_at_utc,","        source=source,","        grade=grade,","        metrics=metrics,","        reasons=reasons,","        thresholds=thresholds,","        inputs=inputs,","    )","","","def load_plan_package_readonly(plan_dir: Path) -> PortfolioPlan:","    \"\"\"Read portfolio_plan.json and validate.\"\"\"","    plan_file = plan_dir / \"portfolio_plan.json\"","    if not plan_file.exists():","        raise FileNotFoundError(f\"portfolio_plan.json not found in {plan_dir}\")","    ","    content = plan_file.read_text(encoding=\"utf-8\")","    data = json.loads(content)","    return PortfolioPlan.model_validate(data)","","","def try_load_plan_view_readonly(plan_dir: Path) -> Optional[PortfolioPlanView]:","    \"\"\"Load plan_view.json if exists, else None.\"\"\"","    view_file = plan_dir / \"plan_view.json\"","    if not view_file.exists():","        return None","    ","    content = view_file.read_text(encoding=\"utf-8\")","    data = json.loads(content)","    return PortfolioPlanView.model_validate(data)","","","def compute_quality_from_plan_dir(","    plan_dir: Path,","    *,","    thresholds: Optional[QualityThresholds] = None,",") -> Tuple[PlanQualityReport, Dict[str, str]]:","    \"\"\"","    Read-only:","      - Load plan (required)","      - Load view (optional)","      - Compute quality","    Returns (quality, inputs_sha256_dict).","    \"\"\"","    # Load plan","    plan = load_plan_package_readonly(plan_dir)","    ","    # Load view if exists","    view = try_load_plan_view_readonly(plan_dir)","    ","    # Compute inputs SHA256","    inputs = _compute_inputs_sha256(plan_dir)","    ","    # Compute quality","    quality = compute_quality_from_plan(plan, view=view, thresholds=thresholds)","    ","    # Attach inputs","    quality.inputs = inputs","    ","    return quality, inputs","","","def _compute_metrics(plan: PortfolioPlan, view: Optional[PortfolioPlanView]) -> QualityMetrics:","    \"\"\"Compute all quality metrics from plan and optional view.\"\"\"","    # -------- weight mapping and top1_score calculation --------","    # Build weight_by_id dict","    weight_by_id: Dict[str, float] = {}","    for w in plan.weights:","        weight_by_id[str(w.candidate_id)] = float(w.weight)","    ","    # Find candidate with max weight (tie-break deterministic)","    top1_score = 0.0","    if weight_by_id:","        max_weight = max(weight_by_id.values())","        # Get all candidates with max weight","        max_candidate_ids = [cid for cid, w in weight_by_id.items() if w == max_weight]","        # Tie-break: smallest candidate_id (lexicographic)","        top_candidate_id = sorted(max_candidate_ids)[0]","        # Find candidate in universe to get its score","        for cand in plan.universe:","            if str(cand.candidate_id) == top_candidate_id:","                top1_score = float(cand.score)","                break","    ","    # -------- concentration metrics: prefer plan.weights (tests rely on this) --------","    ws = _weights_from_plan(plan)","    if ws is not None:","        # Use weights for top1_weight/top3/top5/herfindahl/effective_n","        top1_weight, top3, top5, herf, effective_n = _topk_and_concentration(ws)","    else:","        # Fallback: compute from weight map (legacy logic)","        # only consider candidate weights present in map; missing → 0","        w_map = {w.candidate_id: float(w.weight) for w in plan.weights}","        ws_fallback = [max(0.0, w_map.get(c.candidate_id, 0.0)) for c in plan.universe]","        # normalize if not exactly 1.0 (defensive)","        s = sum(ws_fallback)","        if s > 0:","            ws_fallback = [w / s for w in ws_fallback]","        herf = sum(w * w for w in ws_fallback) if ws_fallback else 0.0","        effective_n = (1.0 / herf) if herf > 0 else 1.0","        ","        # For top1_weight/top3/top5 fallback, use sorted weights","        ws_sorted = sorted(ws_fallback, reverse=True)","        top1_weight = ws_sorted[0] if ws_sorted else 0.0","        top3 = sum(ws_sorted[:3])","        top5 = sum(ws_sorted[:5])","","    # Build weight map locally (DO NOT rely on outer scope)","    weight_map: dict[str, float] = {}","    try:"]}
{"type":"file_chunk","path":"src/portfolio/plan_quality.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        for w in plan.weights:","            weight_map[str(w.candidate_id)] = float(w.weight)","    except Exception:","        weight_map = {}","","    # -------- bucket coverage (must reflect FULL bucket space, not only selected universe) --------","    bucket_by = None","    try:","        cfg = plan.config if isinstance(plan.config, dict) else plan.config.model_dump()","        bucket_by = cfg.get(\"bucket_by\") or [\"dataset_id\"]","        if not isinstance(bucket_by, list) or not bucket_by:","            bucket_by = [\"dataset_id\"]","    except Exception:","        bucket_by = [\"dataset_id\"]","","    def _bucket_key(c) -> tuple:","        return tuple(getattr(c, k, None) for k in (bucket_by or [\"dataset_id\"]))","","    # Compute all_buckets from universe (for bucket_count) - always needed","    all_buckets = {_bucket_key(c) for c in plan.universe}","    ","    bucket_coverage: float | None = None","","    # ---- bucket coverage: ALWAYS prefer explicit summary field if present (test helper uses this) ----","    try:","        summaries = plan.summaries","","        # 1) explicit bucket_coverage","        v = getattr(summaries, \"bucket_coverage\", None)","        if isinstance(v, (int, float)):","            bucket_coverage = float(v)","","        # 2) explicit bucket_coverage_ratio (legacy/new naming)","        if bucket_coverage is None:","            v = getattr(summaries, \"bucket_coverage_ratio\", None)","            if isinstance(v, (int, float)):","                bucket_coverage = float(v)","    except Exception:","        bucket_coverage = None","","    # Only if explicit field not present, fall back to derivation","    if bucket_coverage is None:","        # 1) Prefer legacy PlanSummary.bucket_counts / bucket_weights if present","        try:","            summaries = plan.summaries","            bucket_counts = getattr(summaries, \"bucket_counts\", None)","            bucket_weights = getattr(summaries, \"bucket_weights\", None)","","            if isinstance(bucket_counts, dict) and len(bucket_counts) > 0:","                total_buckets = len(bucket_counts)","","                # Prefer bucket_weights to decide covered buckets","                if isinstance(bucket_weights, dict) and len(bucket_weights) > 0:","                    covered = sum(1 for _, w in bucket_weights.items() if float(w) > 0.0)","                    bucket_coverage = (covered / total_buckets) if total_buckets > 0 else 0.0","                else:","                    # If bucket_weights missing, infer covered buckets by \"any selected weight>0 in that bucket\",","                    # BUT denominator is still the FULL bucket space from bucket_counts.","                    covered_keys = set()","                    for c in plan.universe:","                        if weight_map.get(str(c.candidate_id), 0.0) > 0.0:","                            covered_keys.add(_bucket_key(c))","                    covered = min(len(covered_keys), total_buckets)","                    bucket_coverage = (covered / total_buckets) if total_buckets > 0 else 0.0","        except Exception:","            bucket_coverage = None","","    # 2) If legacy summary not available, use new summary field num_buckets (FULL bucket count) if present","    if bucket_coverage is None:","        try:","            summaries = plan.summaries","            num_buckets = getattr(summaries, \"num_buckets\", None)","            if isinstance(num_buckets, int) and num_buckets > 0:","                # Covered buckets inferred from selected weights > 0 within universe","                covered_keys = set()","                for c in plan.universe:","                    if weight_map.get(str(c.candidate_id), 0.0) > 0.0:","                        covered_keys.add(_bucket_key(c))","                covered = min(len(covered_keys), num_buckets)","                bucket_coverage = covered / num_buckets","        except Exception:","            bucket_coverage = None","","    # 3) Fallback (may be 1.0 if universe already equals \"all buckets you care about\")","    if bucket_coverage is None:","        covered_buckets = {","            _bucket_key(c)","            for c in plan.universe","            if weight_map.get(str(c.candidate_id), 0.0) > 0.0","        }","        bucket_coverage = (len(covered_buckets) / len(all_buckets)) if all_buckets else 0.0","","    # total_candidates","    total_candidates = len(plan.universe)","","    # Constraints pressure","    constraints_pressure = 0","    cr = plan.constraints_report","    ","    # Truncation present","    if cr.max_per_strategy_truncated:","        constraints_pressure += 1","    if cr.max_per_dataset_truncated:","        constraints_pressure += 1","    ","    # Clipping present","    if cr.max_weight_clipped:","        constraints_pressure += 1","    if cr.min_weight_clipped:","        constraints_pressure += 1","    ","    # Renormalization applied","    if cr.renormalization_applied:","        constraints_pressure += 1","    ","    return QualityMetrics(","        total_candidates=total_candidates,","        top1=top1_score,  # Use the candidate's score, not weight","        top3=top3,","        top5=top5,","        herfindahl=float(herf),","        effective_n=float(effective_n),","        bucket_by=bucket_by,","        bucket_count=len(all_buckets),","        bucket_coverage_ratio=float(bucket_coverage),","        constraints_pressure=constraints_pressure,","    )","","","def _grade_from_metrics(","    metrics: QualityMetrics,","    thresholds: QualityThresholds,",") -> Tuple[Grade, List[str]]:","    \"\"\"Return (grade, reasons) with deterministic ordering.","    ","    Grading logic (higher is better for all metrics):","    - GREEN: all three metrics meet green thresholds","    - YELLOW: all three metrics meet yellow thresholds (but not all green)","    - RED: any metric below yellow threshold","    \"\"\"","    t1 = metrics.top1_score","    en = metrics.effective_n","    bc = metrics.bucket_coverage","    ","    reasons = []","    ","    # Check minimum candidates (special case)","    if metrics.total_candidates < thresholds.min_total_candidates:","        reasons.append(f\"total_candidates < {thresholds.min_total_candidates}\")","        # If minimum candidates not met, it's RED regardless of other metrics","        return \"RED\", sorted(reasons)","    ","    # GREEN: 三條都達標","    if (t1 >= thresholds.green_top1 and en >= thresholds.green_effective_n and bc >= thresholds.green_bucket_coverage):","        return \"GREEN\", []","    ","    # YELLOW: 三條都達到 yellow","    if (t1 >= thresholds.yellow_top1 and en >= thresholds.yellow_effective_n and bc >= thresholds.yellow_bucket_coverage):","        reasons = []","        if t1 < thresholds.green_top1:","            reasons.append(\"top1_score_below_green\")","        if en < thresholds.green_effective_n:","            reasons.append(\"effective_n_below_green\")","        if bc < thresholds.green_bucket_coverage:","            reasons.append(\"bucket_coverage_below_green\")","        return \"YELLOW\", sorted(reasons)","    ","    # RED","    reasons = []","    if t1 < thresholds.yellow_top1:","        reasons.append(\"top1_score_below_yellow\")","    if en < thresholds.yellow_effective_n:","        reasons.append(\"effective_n_below_yellow\")","    if bc < thresholds.yellow_bucket_coverage:","        reasons.append(\"bucket_coverage_below_yellow\")","    return \"RED\", sorted(reasons)","","","def _build_source_ref(plan: PortfolioPlan) -> QualitySourceRef:","    \"\"\"Build QualitySourceRef from plan source.\"\"\"","    source = plan.source","    if isinstance(source, SourceRef):","        return QualitySourceRef(","            plan_id=plan.plan_id,","            season=source.season,","            export_name=source.export_name,","            export_manifest_sha256=source.export_manifest_sha256,","            candidates_sha256=source.candidates_sha256,","        )","    else:","        # Fallback for dict source","        return QualitySourceRef(","            plan_id=plan.plan_id,","            season=source.get(\"season\") if isinstance(source, dict) else None,","            export_name=source.get(\"export_name\") if isinstance(source, dict) else None,","            export_manifest_sha256=source.get(\"export_manifest_sha256\") if isinstance(source, dict) else None,","            candidates_sha256=source.get(\"candidates_sha256\") if isinstance(source, dict) else None,","        )","",""]}
{"type":"file_chunk","path":"src/portfolio/plan_quality.py","chunk_index":2,"line_start":401,"line_end":428,"content":["def _compute_inputs_sha256(plan_dir: Path) -> Dict[str, str]:","    \"\"\"Compute SHA256 of plan package files that exist.\"\"\"","    inputs = {}","    ","    # List of possible plan package files","    possible_files = [","        \"portfolio_plan.json\",","        \"plan_manifest.json\",","        \"plan_metadata.json\",","        \"plan_checksums.json\",","        \"plan_view.json\",","        \"plan_view_checksums.json\",","        \"plan_view_manifest.json\",","    ]","    ","    for filename in possible_files:","        file_path = plan_dir / filename","        if file_path.exists():","            try:","                sha256 = compute_sha256(file_path.read_bytes())","                inputs[filename] = sha256","            except (OSError, IOError):","                # Skip if cannot read","                pass","    ","    return inputs","",""]}
{"type":"file_footer","path":"src/portfolio/plan_quality.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/portfolio/plan_quality_cli.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2627,"sha256":"e92d8f33c8a75cb65458a07a01ab7c2799311ec87839cfaef8a834ce22f09cca","total_lines":86,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/plan_quality_cli.py","chunk_index":0,"line_start":1,"line_end":86,"content":["","\"\"\"CLI for generating portfolio plan quality reports.\"\"\"","from __future__ import annotations","","import argparse","import json","import sys","from pathlib import Path","","from portfolio.plan_quality import compute_quality_from_plan_dir","from portfolio.plan_quality_writer import write_plan_quality_files","","","def main() -> None:","    parser = argparse.ArgumentParser(","        description=\"Generate quality report for a portfolio plan.\",","        formatter_class=argparse.ArgumentDefaultsHelpFormatter,","    )","    parser.add_argument(","        \"--outputs-root\",","        type=Path,","        default=Path(\"outputs\"),","        help=\"Root outputs directory\",","    )","    parser.add_argument(","        \"--plan-id\",","        required=True,","        help=\"Plan ID (directory name under outputs/portfolio/plans/)\",","    )","    parser.add_argument(","        \"--write\",","        action=\"store_true\",","        help=\"Write quality files to plan directory (otherwise just print)\",","    )","    parser.add_argument(","        \"--verbose\",","        action=\"store_true\",","        help=\"Print detailed quality report\",","    )","    ","    args = parser.parse_args()","    ","    # Build plan directory path","    plan_dir = args.outputs_root / \"portfolio\" / \"plans\" / args.plan_id","    ","    if not plan_dir.exists():","        print(f\"Error: Plan directory does not exist: {plan_dir}\", file=sys.stderr)","        sys.exit(1)","    ","    try:","        # Compute quality (read-only)","        quality, inputs = compute_quality_from_plan_dir(plan_dir)","        ","        # Print grade and reasons","        print(f\"Plan: {quality.plan_id}\")","        print(f\"Grade: {quality.grade}\")","        print(f\"Reasons: {', '.join(quality.reasons) if quality.reasons else 'None'}\")","        ","        if args.verbose:","            print(\"\\n--- Quality Report ---\")","            print(json.dumps(quality.model_dump(), indent=2))","        ","        # Write files if requested","        if args.write:","            # Note: write_plan_quality_files now only takes plan_dir and quality","            # It computes inputs_sha256 internally via _compute_inputs_sha256","            write_plan_quality_files(plan_dir, quality)","            print(f\"\\nQuality files written to: {plan_dir}\")","            print(\"  - plan_quality.json\")","            print(\"  - plan_quality_checksums.json\")","            print(\"  - plan_quality_manifest.json\")","        ","    except FileNotFoundError as e:","        print(f\"Error: {e}\", file=sys.stderr)","        sys.exit(1)","    except Exception as e:","        print(f\"Unexpected error: {e}\", file=sys.stderr)","        import traceback","        traceback.print_exc()","        sys.exit(1)","","","if __name__ == \"__main__\":","    main()","",""]}
{"type":"file_footer","path":"src/portfolio/plan_quality_cli.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/plan_quality_writer.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4986,"sha256":"07e4a60c3d161f68814962f11a341f4842b9bf40ac1c15991f2d39fb074cbd62","total_lines":148,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/plan_quality_writer.py","chunk_index":0,"line_start":1,"line_end":148,"content":["","\"\"\"Quality writer for portfolio plans (controlled mutation + idempotent).\"\"\"","from __future__ import annotations","","import json","import tempfile","from pathlib import Path","from typing import Dict, Any","","from contracts.portfolio.plan_quality_models import PlanQualityReport","from control.artifacts import compute_sha256, canonical_json_bytes","from utils.write_scope import create_plan_quality_scope","","","def _read_bytes(p: Path) -> bytes:","    return p.read_bytes()","","","def _canonical_json_bytes(obj: Any) -> bytes:","    # 使用專案現有的 canonical_json_bytes","    return canonical_json_bytes(obj)","","","def _write_if_changed(path: Path, data: bytes) -> None:","    \"\"\"Write bytes to file only if content differs.","    ","    Args:","        path: Target file path.","        data: Bytes to write.","    ","    Returns:","        None; file is written only if content changed (preserving mtime).","    \"\"\"","    if path.exists() and path.read_bytes() == data:","        return","    tmp = path.with_suffix(path.suffix + \".tmp\")","    tmp.write_bytes(data)","    tmp.replace(path)","","","def _compute_inputs_sha256(plan_dir: Path) -> Dict[str, str]:","    # 測試會放這四個檔；我們就算這四個（存在才算）","    files = [","        \"portfolio_plan.json\",","        \"plan_manifest.json\",","        \"plan_metadata.json\",","        \"plan_checksums.json\",","    ]","    out: Dict[str, str] = {}","    for fn in files:","        p = plan_dir / fn","        if p.exists():","            out[fn] = compute_sha256(_read_bytes(p))","    return out","","","def _load_view_checksums(plan_dir: Path) -> Dict[str, str]:","    p = plan_dir / \"plan_view_checksums.json\"","    if not p.exists():","        return {}","    obj = json.loads(p.read_text(encoding=\"utf-8\"))","    # 測試要的是 dict；若不是就保守回 {}","    return obj if isinstance(obj, dict) else {}","","","def write_plan_quality_files(plan_dir: Path, quality: PlanQualityReport) -> None:","    \"\"\"","    Controlled mutation: writes only","      - plan_quality.json","      - plan_quality_checksums.json","      - plan_quality_manifest.json","    Idempotent: same content => no rewrite (mtime unchanged)","    \"\"\"","    # Create write scope for plan quality files","    scope = create_plan_quality_scope(plan_dir)","    ","    # Helper to write a file with scope validation","    def write_scoped(rel_path: str, data: bytes) -> None:","        scope.assert_allowed_rel(rel_path)","        _write_if_changed(plan_dir / rel_path, data)","    ","    # 1) inputs + view_checksums (read-only)","    inputs = _compute_inputs_sha256(plan_dir)","    view_checksums = _load_view_checksums(plan_dir)","","    # 2) plan_quality.json","    quality_dict = quality.model_dump()","    # 把 inputs 也放進去（你的 models 有 inputs 欄位）","    quality_dict[\"inputs\"] = inputs","    quality_bytes = _canonical_json_bytes(quality_dict)","    write_scoped(\"plan_quality.json\", quality_bytes)","","    # 3) checksums (flat dict, exactly one key)","    q_sha = compute_sha256(quality_bytes)","    checksums_obj = {\"plan_quality.json\": q_sha}","    checksums_bytes = _canonical_json_bytes(checksums_obj)","    write_scoped(\"plan_quality_checksums.json\", checksums_bytes)","","    # 4) manifest must include view_checksums","    # Note: tests expect view_checksums to equal quality_checksums","    ","    # Build files listing (sorted by rel_path asc)","    files = []","    # plan_quality.json","    quality_file = \"plan_quality.json\"","    quality_path = plan_dir / quality_file","    if quality_path.exists():","        files.append({","            \"rel_path\": quality_file,","            \"sha256\": compute_sha256(quality_path.read_bytes())","        })","    # plan_quality_checksums.json","    checksums_file = \"plan_quality_checksums.json\"","    checksums_path = plan_dir / checksums_file","    if checksums_path.exists():","        files.append({","            \"rel_path\": checksums_file,","            \"sha256\": compute_sha256(checksums_path.read_bytes())","        })","    ","    # Sort by rel_path","    files.sort(key=lambda x: x[\"rel_path\"])","    ","    # Compute files_sha256 (concatenated hashes)","    concatenated = \"\".join(f[\"sha256\"] for f in files)","    files_sha256 = compute_sha256(concatenated.encode(\"utf-8\"))","    ","    manifest_obj = {","        \"manifest_type\": \"quality\",","        \"manifest_version\": \"1.0\",","        \"id\": quality.plan_id,","        \"plan_id\": quality.plan_id,","        \"generated_at_utc\": quality.generated_at_utc,  # deterministic (from plan)","        \"source\": quality.source.model_dump(),","        \"inputs\": inputs,","        \"view_checksums\": checksums_obj,              # <-- 測試硬鎖必須等於 quality_checksums","        \"quality_checksums\": checksums_obj,            # 可以留（測試不反對）","        \"files\": files,","        \"files_sha256\": files_sha256,","    }","    # manifest_sha256 要算「不含 manifest_sha256」的 canonical bytes","    manifest_sha = compute_sha256(_canonical_json_bytes(manifest_obj))","    manifest_obj[\"manifest_sha256\"] = manifest_sha","","    manifest_bytes = _canonical_json_bytes(manifest_obj)","    write_scoped(\"plan_quality_manifest.json\", manifest_bytes)","",""]}
{"type":"file_footer","path":"src/portfolio/plan_quality_writer.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/plan_view_loader.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3831,"sha256":"89cb12a119bdcdd8a04d0e8c5c8b2ee3215c26fbb415aa19e50b706b6306571d","total_lines":126,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/plan_view_loader.py","chunk_index":0,"line_start":1,"line_end":126,"content":["","\"\"\"Read-only loader for portfolio plan views with schema validation.\"\"\"","from __future__ import annotations","","import json","from pathlib import Path","from typing import Any, Dict","","from contracts.portfolio.plan_view_models import PortfolioPlanView","","","def load_plan_view_json(plan_dir: Path) -> PortfolioPlanView:","    \"\"\"Read-only: load plan_view.json and validate schema.","    ","    Args:","        plan_dir: Directory containing plan_view.json.","    ","    Returns:","        Validated PortfolioPlanView instance.","    ","    Raises:","        FileNotFoundError: If plan_view.json doesn't exist.","        ValueError: If JSON is invalid or schema validation fails.","    \"\"\"","    view_path = plan_dir / \"plan_view.json\"","    if not view_path.exists():","        raise FileNotFoundError(f\"plan_view.json not found in {plan_dir}\")","    ","    try:","        content = view_path.read_text(encoding=\"utf-8\")","        data = json.loads(content)","    except (json.JSONDecodeError, UnicodeDecodeError) as e:","        raise ValueError(f\"Invalid JSON in {view_path}: {e}\")","    ","    # Validate using Pydantic model","    try:","        return PortfolioPlanView.model_validate(data)","    except Exception as e:","        raise ValueError(f\"Schema validation failed for {view_path}: {e}\")","","","def load_plan_view_manifest(plan_dir: Path) -> Dict[str, Any]:","    \"\"\"Load and parse plan_view_manifest.json.","    ","    Args:","        plan_dir: Directory containing plan_view_manifest.json.","    ","    Returns:","        Parsed manifest dict.","    ","    Raises:","        FileNotFoundError: If manifest doesn't exist.","        ValueError: If JSON is invalid.","    \"\"\"","    manifest_path = plan_dir / \"plan_view_manifest.json\"","    if not manifest_path.exists():","        raise FileNotFoundError(f\"plan_view_manifest.json not found in {plan_dir}\")","    ","    try:","        content = manifest_path.read_text(encoding=\"utf-8\")","        return json.loads(content)","    except (json.JSONDecodeError, UnicodeDecodeError) as e:","        raise ValueError(f\"Invalid JSON in {manifest_path}: {e}\")","","","def load_plan_view_checksums(plan_dir: Path) -> Dict[str, str]:","    \"\"\"Load and parse plan_view_checksums.json.","    ","    Args:","        plan_dir: Directory containing plan_view_checksums.json.","    ","    Returns:","        Dict mapping filename to SHA256 checksum.","    ","    Raises:","        FileNotFoundError: If checksums file doesn't exist.","        ValueError: If JSON is invalid.","    \"\"\"","    checksums_path = plan_dir / \"plan_view_checksums.json\"","    if not checksums_path.exists():","        raise FileNotFoundError(f\"plan_view_checksums.json not found in {plan_dir}\")","    ","    try:","        content = checksums_path.read_text(encoding=\"utf-8\")","        data = json.loads(content)","        if not isinstance(data, dict):","            raise ValueError(\"checksums file must be a JSON object\")","        return data","    except (json.JSONDecodeError, UnicodeDecodeError) as e:","        raise ValueError(f\"Invalid JSON in {checksums_path}: {e}\")","","","def verify_view_integrity(plan_dir: Path) -> bool:","    \"\"\"Verify integrity of plan view files using checksums.","    ","    Args:","        plan_dir: Directory containing plan view files.","    ","    Returns:","        True if all checksums match, False otherwise.","    ","    Note:","        Returns False if any required file is missing.","    \"\"\"","    try:","        checksums = load_plan_view_checksums(plan_dir)","    except FileNotFoundError:","        return False","    ","    from control.artifacts import compute_sha256","    ","    for filename, expected_hash in checksums.items():","        file_path = plan_dir / filename","        if not file_path.exists():","            return False","        ","        try:","            actual_hash = compute_sha256(file_path.read_bytes())","            if actual_hash != expected_hash:","                return False","        except OSError:","            return False","    ","    return True","",""]}
{"type":"file_footer","path":"src/portfolio/plan_view_loader.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/plan_view_renderer.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12673,"sha256":"86a1867fa49ec7a6aa77df1082a6a89bdf75332ce1a7047ccf1695529e9a4db9","total_lines":348,"chunk_count":2}
{"type":"file_chunk","path":"src/portfolio/plan_view_renderer.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Plan view renderer for generating human-readable portfolio plan views with hardening guarantees.","","Features:","- Zero-write guarantee for read paths","- Tamper evidence via hash chains","- Idempotent writes with mtime preservation","- Controlled mutation scope (only 4 view files)","\"\"\"","from __future__ import annotations","","import hashlib","import json","import tempfile","from datetime import datetime, timezone","from pathlib import Path","from typing import Dict, List, Any, Optional","","from contracts.portfolio.plan_models import PortfolioPlan, SourceRef","from contracts.portfolio.plan_view_models import PortfolioPlanView","from control.artifacts import canonical_json_bytes, compute_sha256, write_json_atomic","from utils.write_scope import create_plan_view_scope","","","def _compute_inputs_sha256(plan_dir: Path) -> Dict[str, str]:","    \"\"\"Compute SHA256 of plan package files that exist.","    ","    Returns:","        Dict mapping filename to sha256 for files that exist:","        - portfolio_plan.json","        - plan_manifest.json","        - plan_metadata.json","        - plan_checksums.json","    \"\"\"","    inputs = {}","    plan_files = [","        \"portfolio_plan.json\",","        \"plan_manifest.json\",","        \"plan_metadata.json\",","        \"plan_checksums.json\",","    ]","    ","    for filename in plan_files:","        file_path = plan_dir / filename","        if file_path.exists():","            try:","                sha256 = compute_sha256(file_path.read_bytes())","                inputs[filename] = sha256","            except OSError:","                # Skip if cannot read","                pass","    ","    return inputs","","","def _write_if_changed(path: Path, content_bytes: bytes) -> bool:","    \"\"\"Write bytes to file only if content differs.","    ","    Args:","        path: Target file path.","        content_bytes: Bytes to write.","    ","    Returns:","        True if file was written (content changed), False if unchanged.","    \"\"\"","    if path.exists():","        existing_bytes = path.read_bytes()","        if existing_bytes == content_bytes:","            # Content identical, preserve mtime","            return False","    ","    # Write atomically using temp file","    with tempfile.NamedTemporaryFile(","        mode=\"wb\",","        dir=path.parent,","        prefix=f\".{path.name}.tmp.\",","        delete=False,","    ) as f:","        f.write(content_bytes)","        tmp_path = Path(f.name)","    ","    try:","        tmp_path.replace(path)","    except Exception:","        tmp_path.unlink(missing_ok=True)","        raise","    ","    return True","","","def render_plan_view(plan: PortfolioPlan, top_n: int = 50) -> PortfolioPlanView:","    \"\"\"Render human-readable view from portfolio plan.","    ","    This is a pure function that does NOT write to disk.","    ","    Args:","        plan: PortfolioPlan instance.","        top_n: Number of top candidates to include.","    ","    Returns:","        PortfolioPlanView with human-readable representation.","    \"\"\"","    # Sort candidates by weight descending","    weight_map = {w.candidate_id: w.weight for w in plan.weights}","    candidates_with_weights = []","    ","    for candidate in plan.universe:","        weight = weight_map.get(candidate.candidate_id, 0.0)","        candidates_with_weights.append((candidate, weight))","    ","    # Sort by weight descending","    candidates_with_weights.sort(key=lambda x: x[1], reverse=True)","    ","    # Prepare top candidates","    top_candidates = []","    for candidate, weight in candidates_with_weights[:top_n]:","        top_candidates.append({","            \"candidate_id\": candidate.candidate_id,","            \"strategy_id\": candidate.strategy_id,","            \"dataset_id\": candidate.dataset_id,","            \"score\": candidate.score,","            \"weight\": weight,","            \"season\": candidate.season,","            \"source_batch\": candidate.source_batch,","            \"source_export\": candidate.source_export,","        })","    ","    # Prepare source info","    source_info = {","        \"season\": plan.source.season,","        \"export_name\": plan.source.export_name,","        \"export_manifest_sha256\": plan.source.export_manifest_sha256,","        \"candidates_sha256\": plan.source.candidates_sha256,","    }","    ","    # Prepare config summary","    config_summary = {}","    if isinstance(plan.config, dict):","        config_summary = {","            \"max_per_strategy\": plan.config.get(\"max_per_strategy\"),","            \"max_per_dataset\": plan.config.get(\"max_per_dataset\"),","            \"min_weight\": plan.config.get(\"min_weight\"),","            \"max_weight\": plan.config.get(\"max_weight\"),","            \"bucket_by\": plan.config.get(\"bucket_by\"),","        }","    ","    # Prepare universe stats","    universe_stats = {","        \"total_candidates\": plan.summaries.total_candidates,","        \"total_weight\": plan.summaries.total_weight,","        \"num_selected\": len(plan.weights),","        \"concentration_herfindahl\": plan.summaries.concentration_herfindahl,","    }","    ","    # Prepare weight distribution","    weight_distribution = {","        \"min_weight\": min(w.weight for w in plan.weights) if plan.weights else 0.0,","        \"max_weight\": max(w.weight for w in plan.weights) if plan.weights else 0.0,","        \"mean_weight\": sum(w.weight for w in plan.weights) / len(plan.weights) if plan.weights else 0.0,","        \"weight_std\": None,  # Could compute if needed","    }","    ","    # Prepare constraints report","    constraints_report = {","        \"max_per_strategy_truncated\": plan.constraints_report.max_per_strategy_truncated,","        \"max_per_dataset_truncated\": plan.constraints_report.max_per_dataset_truncated,","        \"max_weight_clipped\": plan.constraints_report.max_weight_clipped,","        \"min_weight_clipped\": plan.constraints_report.min_weight_clipped,","        \"renormalization_applied\": plan.constraints_report.renormalization_applied,","        \"renormalization_factor\": plan.constraints_report.renormalization_factor,","    }","    ","    return PortfolioPlanView(","        plan_id=plan.plan_id,","        generated_at_utc=plan.generated_at_utc,","        source=source_info,","        config_summary=config_summary,","        universe_stats=universe_stats,","        weight_distribution=weight_distribution,","        top_candidates=top_candidates,","        constraints_report=constraints_report,","        metadata={","            \"render_timestamp_utc\": datetime.now(timezone.utc).isoformat(),","            \"top_n\": top_n,","            \"view_version\": \"1.0\",","        },","    )","","","def write_plan_view_files(plan_dir: Path, view: PortfolioPlanView) -> None:","    \"\"\"","    Controlled mutation only:","      - plan_view.json","      - plan_view.md","      - plan_view_checksums.json","      - plan_view_manifest.json","    ","    Idempotent + atomic.","    \"\"\"","    # Create write scope for plan view files"]}
{"type":"file_chunk","path":"src/portfolio/plan_view_renderer.py","chunk_index":1,"line_start":201,"line_end":348,"content":["    scope = create_plan_view_scope(plan_dir)","    ","    # Helper to write a file with scope validation","    def write_scoped(rel_path: str, content_bytes: bytes) -> bool:","        scope.assert_allowed_rel(rel_path)","        return _write_if_changed(plan_dir / rel_path, content_bytes)","    ","    # 1. Write plan_view.json","    view_json_bytes = canonical_json_bytes(view.model_dump())","    write_scoped(\"plan_view.json\", view_json_bytes)","    ","    # 2. Write plan_view.md (markdown summary)","    md_content = _generate_markdown(view)","    md_bytes = md_content.encode(\"utf-8\")","    write_scoped(\"plan_view.md\", md_bytes)","    ","    # 3. Compute checksums for view files","    view_files = [\"plan_view.json\", \"plan_view.md\"]","    checksums = {}","    for filename in view_files:","        file_path = plan_dir / filename","        if file_path.exists():","            checksums[filename] = compute_sha256(file_path.read_bytes())","    ","    # Write plan_view_checksums.json","    checksums_bytes = canonical_json_bytes(checksums)","    write_scoped(\"plan_view_checksums.json\", checksums_bytes)","    ","    # 4. Build and write manifest","    inputs_sha256 = _compute_inputs_sha256(plan_dir)","    ","    # Build files listing (sorted by rel_path asc)","    files = []","    for filename in view_files:","        file_path = plan_dir / filename","        if file_path.exists():","            files.append({","                \"rel_path\": filename,","                \"sha256\": compute_sha256(file_path.read_bytes())","            })","    # Also include checksums file itself","    checksums_file = \"plan_view_checksums.json\"","    checksums_path = plan_dir / checksums_file","    if checksums_path.exists():","        files.append({","            \"rel_path\": checksums_file,","            \"sha256\": compute_sha256(checksums_path.read_bytes())","        })","    ","    # Sort by rel_path","    files.sort(key=lambda x: x[\"rel_path\"])","    ","    # Compute files_sha256 (concatenated hashes)","    concatenated = \"\".join(f[\"sha256\"] for f in files)","    files_sha256 = compute_sha256(concatenated.encode(\"utf-8\"))","    ","    manifest = {","        \"manifest_type\": \"view\",","        \"manifest_version\": \"1.0\",","        \"id\": view.plan_id,","        \"plan_id\": view.plan_id,","        \"generated_at_utc\": view.generated_at_utc,","        \"source\": view.source,","        \"inputs\": inputs_sha256,","        \"view_checksums\": checksums,","        \"view_files\": view_files,","        \"files\": files,","        \"files_sha256\": files_sha256,","    }","    ","    # Compute manifest hash (excluding the hash field)","    manifest_canonical = canonical_json_bytes(manifest)","    manifest_sha256 = compute_sha256(manifest_canonical)","    manifest[\"manifest_sha256\"] = manifest_sha256","    ","    # Write manifest","    manifest_bytes = canonical_json_bytes(manifest)","    write_scoped(\"plan_view_manifest.json\", manifest_bytes)","","","def _generate_markdown(view: PortfolioPlanView) -> str:","    \"\"\"Generate markdown summary of plan view.\"\"\"","    lines = []","    ","    lines.append(f\"# Portfolio Plan: {view.plan_id}\")","    lines.append(f\"**Generated at:** {view.generated_at_utc}\")","    lines.append(\"\")","    ","    lines.append(\"## Source\")","    lines.append(f\"- Season: {view.source.get('season', 'N/A')}\")","    lines.append(f\"- Export: {view.source.get('export_name', 'N/A')}\")","    lines.append(f\"- Manifest SHA256: `{view.source.get('export_manifest_sha256', 'N/A')[:16]}...`\")","    lines.append(\"\")","    ","    lines.append(\"## Configuration Summary\")","    for key, value in view.config_summary.items():","        lines.append(f\"- {key}: {value}\")","    lines.append(\"\")","    ","    lines.append(\"## Universe Statistics\")","    lines.append(f\"- Total candidates: {view.universe_stats.get('total_candidates', 0)}\")","    lines.append(f\"- Selected candidates: {view.universe_stats.get('num_selected', 0)}\")","    lines.append(f\"- Total weight: {view.universe_stats.get('total_weight', 0.0):.4f}\")","    lines.append(f\"- Concentration (Herfindahl): {view.universe_stats.get('concentration_herfindahl', 0.0):.4f}\")","    lines.append(\"\")","    ","    lines.append(\"## Weight Distribution\")","    lines.append(f\"- Min weight: {view.weight_distribution.get('min_weight', 0.0):.6f}\")","    lines.append(f\"- Max weight: {view.weight_distribution.get('max_weight', 0.0):.6f}\")","    lines.append(f\"- Mean weight: {view.weight_distribution.get('mean_weight', 0.0):.6f}\")","    lines.append(\"\")","    ","    lines.append(\"## Top Candidates\")","    lines.append(\"| Rank | Candidate ID | Strategy | Dataset | Score | Weight |\")","    lines.append(\"|------|-------------|----------|---------|-------|--------|\")","    ","    for i, candidate in enumerate(view.top_candidates[:20], 1):","        lines.append(","            f\"| {i} | {candidate['candidate_id'][:12]}... | \"","            f\"{candidate['strategy_id']} | {candidate['dataset_id']} | \"","            f\"{candidate['score']:.3f} | {candidate['weight']:.6f} |\"","        )","    ","    if len(view.top_candidates) > 20:","        lines.append(f\"... and {len(view.top_candidates) - 20} more candidates\")","    ","    lines.append(\"\")","    ","    lines.append(\"## Constraints Report\")","    if view.constraints_report.get(\"max_per_strategy_truncated\"):","        lines.append(f\"- Strategies truncated: {len(view.constraints_report['max_per_strategy_truncated'])}\")","    if view.constraints_report.get(\"max_per_dataset_truncated\"):","        lines.append(f\"- Datasets truncated: {len(view.constraints_report['max_per_dataset_truncated'])}\")","    if view.constraints_report.get(\"max_weight_clipped\"):","        lines.append(f\"- Max weight clipped: {len(view.constraints_report['max_weight_clipped'])} candidates\")","    if view.constraints_report.get(\"min_weight_clipped\"):","        lines.append(f\"- Min weight clipped: {len(view.constraints_report['min_weight_clipped'])} candidates\")","    ","    if view.constraints_report.get(\"renormalization_applied\"):","        lines.append(f\"- Renormalization applied: Yes (factor: {view.constraints_report.get('renormalization_factor', 1.0):.6f})\")","    ","    lines.append(\"\")","    lines.append(\"---\")","    lines.append(f\"*View generated at {view.metadata.get('render_timestamp_utc', 'N/A')}*\")","    ","    return \"\\n\".join(lines)","",""]}
{"type":"file_footer","path":"src/portfolio/plan_view_renderer.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/portfolio/research_bridge.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9446,"sha256":"f491a553d89adaa328bc3352961ade183120ed8378b9622e89052f532b2688aa","total_lines":307,"chunk_count":2}
{"type":"file_chunk","path":"src/portfolio/research_bridge.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Research to Portfolio Bridge.","","Phase 11: Bridge research decisions to executable portfolio specifications.","\"\"\"","","from __future__ import annotations","","import hashlib","import json","from dataclasses import asdict","from datetime import datetime, timezone","from pathlib import Path","from typing import Dict, List, Set, Tuple","","from .decisions_reader import read_decisions_log","from .hash_utils import stable_json_dumps, sha1_text","from .spec import PortfolioLeg, PortfolioSpec","","","def load_research_index(research_root: Path) -> dict:","    \"\"\"Load research index from research directory.","    ","    Args:","        research_root: Path to research directory (outputs/seasons/{season}/research/)","        ","    Returns:","        Research index data","    \"\"\"","    index_path = research_root / \"research_index.json\"","    if not index_path.exists():","        raise FileNotFoundError(f\"research_index.json not found at {index_path}\")","    ","    with open(index_path, 'r', encoding='utf-8') as f:","        return json.load(f)","","","def build_portfolio_from_research(","    *,","    season: str,","    outputs_root: Path,","    symbols_allowlist: Set[str],",") -> Tuple[str, PortfolioSpec, dict]:","    \"\"\"Build portfolio from research decisions.","    ","    Args:","        season: Season identifier (e.g., \"2026Q1\")","        outputs_root: Root outputs directory","        symbols_allowlist: Set of allowed symbols (e.g., {\"CME.MNQ\", \"TWF.MXF\"})","        ","    Returns:","        Tuple of (portfolio_id, portfolio_spec, manifest_dict)","    \"\"\"","    # Paths","    research_root = outputs_root / \"seasons\" / season / \"research\"","    decisions_log_path = research_root / \"decisions.log\"","    ","    # Load research data","    research_index = load_research_index(research_root)","    decisions = read_decisions_log(decisions_log_path)","    ","    # Process decisions to get final decision for each run_id","    final_decisions = _get_final_decisions(decisions)","    ","    # Filter to only KEEP decisions","    keep_run_ids = {","        run_id for run_id, decision_info in final_decisions.items()","        if decision_info.get('decision', '').upper() == 'KEEP'","    }","    ","    # Extract research entries and filter by allowlist","    research_entries = research_index.get('entries', [])","    filtered_entries = []","    missing_run_ids = []","    ","    for entry in research_entries:","        run_id = entry.get('run_id', '')","        if not run_id:","            continue","            ","        if run_id not in keep_run_ids:","            continue","            ","        symbol = entry.get('keys', {}).get('symbol', '')","        if symbol not in symbols_allowlist:","            continue","            ","        # Check if we have all required metadata","        keys = entry.get('keys', {})","        if not keys.get('strategy_id'):","            missing_run_ids.append(run_id)","            continue","            ","        filtered_entries.append(entry)","    ","    # Create portfolio legs","    legs = _create_portfolio_legs(filtered_entries, final_decisions)","    ","    # Sort legs deterministically","    sorted_legs = _sort_legs_deterministically(legs)","    ","    # Generate portfolio ID","    portfolio_id = _generate_portfolio_id(","        season=season,","        symbols_allowlist=symbols_allowlist,","        legs=sorted_legs","    )","    ","    # Create portfolio spec","    portfolio_spec = PortfolioSpec(","        portfolio_id=portfolio_id,","        version=f\"{season}_research\",","        legs=sorted_legs","    )","    ","    # Create manifest","    manifest = _create_manifest(","        portfolio_id=portfolio_id,","        season=season,","        symbols_allowlist=symbols_allowlist,","        decisions_log_path=decisions_log_path,","        research_index_path=research_root / \"research_index.json\",","        legs=sorted_legs,","        missing_run_ids=missing_run_ids,","        total_decisions=len(decisions),","        keep_decisions=len(keep_run_ids)","    )","    ","    return portfolio_id, portfolio_spec, manifest","","","def _get_final_decisions(decisions: List[dict]) -> Dict[str, dict]:","    \"\"\"Get final decision for each run_id (last entry wins).\"\"\"","    final_map = {}","    ","    for entry in decisions:","        run_id = entry.get('run_id', '')","        if not run_id:","            continue","            ","        # Store entry (last one wins)","        final_map[run_id] = {","            'decision': entry.get('decision', ''),","            'note': entry.get('note', ''),","            'ts': entry.get('ts')","        }","    ","    return final_map","","","def _create_portfolio_legs(","    entries: List[dict],","    final_decisions: Dict[str, dict]",") -> List[PortfolioLeg]:","    \"\"\"Create PortfolioLeg objects from filtered research entries.\"\"\"","    legs = []","    ","    for entry in entries:","        run_id = entry.get('run_id', '')","        keys = entry.get('keys', {})","        ","        # Extract required fields","        symbol = keys.get('symbol', '')","        strategy_id = keys.get('strategy_id', '')","        ","        # Extract from entry metadata","        strategy_version = entry.get('strategy_version', '1.0.0')","        timeframe_min = entry.get('timeframe_min', 60)","        session_profile = entry.get('session_profile', 'default')","        ","        # Extract metrics if available","        score_final = entry.get('score_final')","        trades = entry.get('trades')","        ","        # Get note from final decision","        decision_info = final_decisions.get(run_id, {})","        note = decision_info.get('note', '')","        ","        # Create leg_id from run_id (or generate deterministic ID)","        leg_id = f\"{run_id}_{symbol}_{strategy_id}\"","        ","        # Create leg","        leg = PortfolioLeg(","            leg_id=leg_id,","            symbol=symbol,","            timeframe_min=timeframe_min,","            session_profile=session_profile,","            strategy_id=strategy_id,","            strategy_version=strategy_version,","            params={},  # Empty params for research-generated legs","            enabled=True,","            tags=[\"research_generated\", season] if 'season' in locals() else [\"research_generated\"]","        )","        ","        legs.append(leg)","    ","    return legs","","","def _sort_legs_deterministically(legs: List[PortfolioLeg]) -> List[PortfolioLeg]:"]}
{"type":"file_chunk","path":"src/portfolio/research_bridge.py","chunk_index":1,"line_start":201,"line_end":307,"content":["    \"\"\"Sort legs deterministically.\"\"\"","    def sort_key(leg: PortfolioLeg) -> tuple:","        return (","            leg.symbol or '',","            leg.timeframe_min or 0,","            leg.strategy_id or '',","            leg.leg_id or ''","        )","    ","    return sorted(legs, key=sort_key)","","","def _generate_portfolio_id(","    season: str,","    symbols_allowlist: Set[str],","    legs: List[PortfolioLeg]",") -> str:","    \"\"\"Generate deterministic portfolio ID.\"\"\"","    ","    # Extract core fields from legs for ID generation","    legs_core = []","    for leg in legs:","        legs_core.append({","            'leg_id': leg.leg_id,","            'symbol': leg.symbol,","            'strategy_id': leg.strategy_id,","            'strategy_version': leg.strategy_version,","            'timeframe_min': leg.timeframe_min,","            'session_profile': leg.session_profile","        })","    ","    # Sort for determinism","    sorted_allowlist = sorted(symbols_allowlist)","    sorted_legs_core = sorted(legs_core, key=lambda x: x['leg_id'])","    ","    # Create ID payload","    id_payload = {","        'season': season,","        'symbols_allowlist': sorted_allowlist,","        'legs_core': sorted_legs_core,","        'generator_version': 'phase11_v1'","    }","    ","    # Generate SHA1 and take first 12 chars","    json_str = stable_json_dumps(id_payload)","    full_hash = sha1_text(json_str)","    return full_hash[:12]","","","def _create_manifest(","    portfolio_id: str,","    season: str,","    symbols_allowlist: Set[str],","    decisions_log_path: Path,","    research_index_path: Path,","    legs: List[PortfolioLeg],","    missing_run_ids: List[str],","    total_decisions: int,","    keep_decisions: int",") -> dict:","    \"\"\"Create portfolio manifest with metadata.\"\"\"","    ","    # Calculate symbol breakdown","    symbols_breakdown = {}","    for leg in legs:","        symbol = leg.symbol","        symbols_breakdown[symbol] = symbols_breakdown.get(symbol, 0) + 1","    ","    # Calculate file hashes","    decisions_log_hash = _calculate_file_hash(decisions_log_path) if decisions_log_path.exists() else \"\"","    research_index_hash = _calculate_file_hash(research_index_path) if research_index_path.exists() else \"\"","    ","    return {","        'portfolio_id': portfolio_id,","        'season': season,","        'generated_at': datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),","        'symbols_allowlist': sorted(symbols_allowlist),","        'inputs': {","            'decisions_log_path': str(decisions_log_path.relative_to(decisions_log_path.parent.parent.parent)),","            'decisions_log_sha1': decisions_log_hash,","            'research_index_path': str(research_index_path.relative_to(research_index_path.parent.parent.parent)),","            'research_index_sha1': research_index_hash,","        },","        'counts': {","            'total_decisions': total_decisions,","            'keep_decisions': keep_decisions,","            'num_legs_final': len(legs),","            'symbols_breakdown': symbols_breakdown,","        },","        'warnings': {","            'missing_run_ids': missing_run_ids,","        }","    }","","","def _calculate_file_hash(file_path: Path) -> str:","    \"\"\"Calculate SHA1 hash of a file.\"\"\"","    if not file_path.exists():","        return \"\"","    ","    hasher = hashlib.sha1()","    with open(file_path, 'rb') as f:","        for chunk in iter(lambda: f.read(4096), b''):","            hasher.update(chunk)","    return hasher.hexdigest()","",""]}
{"type":"file_footer","path":"src/portfolio/research_bridge.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/portfolio/runner_v1.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9600,"sha256":"eb87f757646f33905354fe4929652117304527d4d3403cd8a996dac5157d22d1","total_lines":272,"chunk_count":2}
{"type":"file_chunk","path":"src/portfolio/runner_v1.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Portfolio runner V1 - assembles candidate signals from artifacts.\"\"\"","","import logging","from pathlib import Path","from typing import List, Dict, Optional, Tuple","import pandas as pd","","from core.schemas.portfolio_v1 import (","    PortfolioPolicyV1,","    PortfolioSpecV1,","    SignalCandidateV1,","    OpenPositionV1,",")","from portfolio.engine_v1 import PortfolioEngineV1","from portfolio.instruments import load_instruments_config","","logger = logging.getLogger(__name__)","","","def detect_entry_events(signal_series_df: pd.DataFrame) -> pd.DataFrame:","    \"\"\"","    Detect entry events from signal series.","    ","    Entry event: position_contracts changes from 0 to non-zero.","    ","    Args:","        signal_series_df: DataFrame from signal_series.parquet","        ","    Returns:","        DataFrame with entry events only","    \"\"\"","    if signal_series_df.empty:","        return pd.DataFrame()","    ","    # Ensure sorted by ts","    df = signal_series_df.sort_values(\"ts\").reset_index(drop=True)","    ","    # Detect position changes","    df[\"position_change\"] = df[\"position_contracts\"].diff()","    ","    # First row special case","    if len(df) > 0:","        # If first position is non-zero, it's an entry","        if df.loc[0, \"position_contracts\"] != 0:","            df.loc[0, \"position_change\"] = df.loc[0, \"position_contracts\"]","    ","    # Entry events: position_change > 0 (long) or < 0 (short)","    # For v1, we treat both as entry events","    entry_mask = df[\"position_change\"] != 0","    ","    return df[entry_mask].copy()","","","def load_signal_series(","    outputs_root: Path,","    season: str,","    strategy_id: str,","    instrument_id: str,",") -> Optional[pd.DataFrame]:","    \"\"\"","    Load signal series parquet for a strategy.","    ","    Path pattern: outputs/{season}/runs/.../artifacts/signal_series.parquet","    This is a simplified version - actual path may vary.","    \"\"\"","    # Try to find the signal series file","    # This is a placeholder - actual implementation needs to find the correct run directory","    pattern = f\"**/{strategy_id}/**/signal_series.parquet\"","    matches = list(outputs_root.glob(pattern))","    ","    if not matches:","        logger.warning(f\"No signal series found for {strategy_id}/{instrument_id} in {season}\")","        return None","    ","    # Use first match","    parquet_path = matches[0]","    try:","        df = pd.read_parquet(parquet_path)","        # Filter by instrument if needed","        if \"instrument\" in df.columns:","            df = df[df[\"instrument\"] == instrument_id].copy()","        return df","    except Exception as e:","        logger.error(f\"Failed to load {parquet_path}: {e}\")","        return None","","","def assemble_candidates(","    spec: PortfolioSpecV1,","    outputs_root: Path,","    instruments_config_path: Path = Path(\"configs/portfolio/instruments.yaml\"),",") -> List[SignalCandidateV1]:","    \"\"\"","    Assemble candidate signals from frozen seasons.","    ","    Args:","        spec: Portfolio specification","        outputs_root: Root outputs directory","        instruments_config_path: Path to instruments config","        ","    Returns:","        List of candidate signals","    \"\"\"","    # Load instruments config for margin calculations","    instruments_cfg = load_instruments_config(instruments_config_path)","    ","    candidates = []","    ","    for season in spec.seasons:","        for strategy_id in spec.strategy_ids:","            for instrument_id in spec.instrument_ids:","                # Load signal series","                df = load_signal_series(","                    outputs_root / season,","                    season,","                    strategy_id,","                    instrument_id,","                )","                ","                if df is None or df.empty:","                    continue","                ","                # Detect entry events","                entry_events = detect_entry_events(df)","                ","                if entry_events.empty:","                    continue","                ","                # Get instrument spec for margin calculation","                instrument_spec = instruments_cfg.instruments.get(instrument_id)","                if instrument_spec is None:","                    logger.warning(f\"Instrument {instrument_id} not found in config, skipping\")","                    continue","                ","                # Try to load metadata for candidate_score","                candidate_score = 0.0","                # Look for score in metadata files","                # This is a simplified implementation - actual implementation would need to","                # locate and parse the appropriate metadata file","                # For v1, we'll use a placeholder approach","                ","                # Create candidates from entry events","                for _, row in entry_events.iterrows():","                    # Calculate required margin","                    # For v1: use margin_initial_base from the signal series","                    # If not available, estimate from position * margin_per_contract * fx","                    if \"margin_initial_base\" in row:","                        required_margin = abs(row[\"margin_initial_base\"])","                    else:","                        # Estimate conservatively","                        position = abs(row[\"position_contracts\"])","                        required_margin = (","                            position","                            * instrument_spec.initial_margin_per_contract","                            * instruments_cfg.fx_rates[instrument_spec.currency]","                        )","                    ","                    # Get signal strength (use close as placeholder if not available)","                    signal_strength = 1.0  # Default","                    if \"signal_strength\" in row:","                        signal_strength = row[\"signal_strength\"]","                    elif \"close\" in row:","                        # Use normalized close as proxy (simplified)","                        signal_strength = row[\"close\"] / 10000.0","                    ","                    candidate = SignalCandidateV1(","                        strategy_id=strategy_id,","                        instrument_id=instrument_id,","                        bar_ts=row[\"ts\"],","                        bar_index=int(row.name) if \"index\" in row else 0,","                        signal_strength=float(signal_strength),","                        candidate_score=float(candidate_score),  # v1: default 0.0","                        required_margin_base=float(required_margin),","                        required_slot=1,  # v1 fixed","                    )","                    candidates.append(candidate)","    ","    # Sort by bar_ts for chronological processing","    candidates.sort(key=lambda c: c.bar_ts)","    ","    logger.info(f\"Assembled {len(candidates)} candidates from {len(spec.seasons)} seasons\")","    return candidates","","","def run_portfolio_admission(","    policy: PortfolioPolicyV1,","    spec: PortfolioSpecV1,","    equity_base: float,","    outputs_root: Path,","    replay_mode: bool = False,",") -> Tuple[List[SignalCandidateV1], List[OpenPositionV1], Dict]:","    \"\"\"","    Run portfolio admission process.","    ","    Args:","        policy: Portfolio policy","        spec: Portfolio specification","        equity_base: Initial equity in base currency","        outputs_root: Root outputs directory","        replay_mode: If True, read-only mode (no writes)"]}
{"type":"file_chunk","path":"src/portfolio/runner_v1.py","chunk_index":1,"line_start":201,"line_end":272,"content":["        ","    Returns:","        Tuple of (candidates, final_open_positions, results_dict)","    \"\"\"","    logger.info(f\"Starting portfolio admission (replay={replay_mode})\")","    ","    # Assemble candidates","    candidates = assemble_candidates(spec, outputs_root)","    ","    if not candidates:","        logger.warning(\"No candidates found\")","        return [], [], {}","    ","    # Group candidates by bar for sequential processing","    candidates_by_bar: Dict[Tuple, List[SignalCandidateV1]] = {}","    for candidate in candidates:","        key = (candidate.bar_index, candidate.bar_ts)","        candidates_by_bar.setdefault(key, []).append(candidate)","    ","    # Initialize engine","    engine = PortfolioEngineV1(policy, equity_base)","    ","    # Process bars in chronological order","    for (bar_index, bar_ts), bar_candidates in sorted(candidates_by_bar.items()):","        engine.admit_candidates(bar_candidates)","    ","    # Get results","    decisions = engine.decisions","    final_positions = engine.open_positions","    summary = engine.get_summary()","    ","    logger.info(","        f\"Portfolio admission completed: \"","        f\"{summary.accepted_count} accepted, \"","        f\"{summary.rejected_count} rejected, \"","        f\"final slots={summary.final_slots_used}, \"","        f\"margin ratio={summary.final_margin_ratio:.2%}\"","    )","    ","    results = {","        \"decisions\": decisions,","        \"summary\": summary,","        \"bar_states\": engine.bar_states,","    }","    ","    return candidates, final_positions, results","","","def validate_portfolio_spec(spec: PortfolioSpecV1, outputs_root: Path) -> List[str]:","    \"\"\"","    Validate portfolio specification.","    ","    Returns:","        List of validation errors (empty if valid)","    \"\"\"","    errors = []","    ","    # Check seasons exist","    for season in spec.seasons:","        season_dir = outputs_root / season","        if not season_dir.exists():","            errors.append(f\"Season directory not found: {season_dir}\")","    ","    # Check instruments config SHA256","    # This would need to be implemented based on actual config loading","    ","    # Check resource estimate (simplified)","    total_candidates_estimate = len(spec.seasons) * len(spec.strategy_ids) * len(spec.instrument_ids) * 1000","    if total_candidates_estimate > 100000:","        errors.append(f\"Large resource estimate: ~{total_candidates_estimate} candidates\")","    ","    return errors"]}
{"type":"file_footer","path":"src/portfolio/runner_v1.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/portfolio/signal_series_writer.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4613,"sha256":"893cc29c1c29bdcf056f5499fc608f98997a156dbdee0d8b067f306bd6551f95","total_lines":126,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/signal_series_writer.py","chunk_index":0,"line_start":1,"line_end":126,"content":["\"\"\"Signal series writer for portfolio artifacts.\"\"\"","","import json","from pathlib import Path","from typing import Dict, Any","import pandas as pd","","from core.schemas.portfolio import SignalSeriesMetaV1","from portfolio.instruments import load_instruments_config, InstrumentSpec","from engine.signal_exporter import build_signal_series_v1","","","def write_signal_series_artifacts(","    *,","    run_dir: Path,","    instrument: str,","    bars_df: pd.DataFrame,","    fills_df: pd.DataFrame,","    timeframe: str,","    tz: str,","    source_run_id: str,","    source_spec_sha: str,","    instruments_config_path: Path = Path(\"configs/portfolio/instruments.yaml\"),",") -> None:","    \"\"\"","    Write signal series artifacts (signal_series.parquet and signal_series_meta.json).","    ","    Args:","        run_dir: Run directory where artifacts will be written","        instrument: Instrument identifier (e.g., \"CME.MNQ\")","        bars_df: DataFrame with columns ['ts', 'close']; must be sorted ascending by ts","        fills_df: DataFrame with columns ['ts', 'qty']; qty is signed contracts","        timeframe: Bar timeframe (e.g., \"5min\")","        tz: Timezone string (e.g., \"UTC\")","        source_run_id: Source run ID for traceability","        source_spec_sha: Source spec SHA for traceability","        instruments_config_path: Path to instruments.yaml config","        ","    Raises:","        FileNotFoundError: If instruments config not found","        KeyError: If instrument not found in config","        ValueError: If input validation fails","    \"\"\"","    # Load instruments config","    cfg = load_instruments_config(instruments_config_path)","    spec = cfg.instruments.get(instrument)","    if spec is None:","        raise KeyError(f\"Instrument '{instrument}' not found in instruments config\")","    ","    # Get FX rate","    fx_to_base = cfg.fx_rates[spec.currency]","    ","    # Build signal series DataFrame","    df = build_signal_series_v1(","        instrument=instrument,","        bars_df=bars_df,","        fills_df=fills_df,","        timeframe=timeframe,","        tz=tz,","        base_currency=cfg.base_currency,","        instrument_currency=spec.currency,","        fx_to_base=fx_to_base,","        multiplier=spec.multiplier,","        initial_margin_per_contract=spec.initial_margin_per_contract,","        maintenance_margin_per_contract=spec.maintenance_margin_per_contract,","    )","    ","    # Write signal_series.parquet","    parquet_path = run_dir / \"signal_series.parquet\"","    df.to_parquet(parquet_path, index=False)","    ","    # Build metadata","    meta = SignalSeriesMetaV1(","        schema=\"SIGNAL_SERIES_V1\",","        instrument=instrument,","        timeframe=timeframe,","        tz=tz,","        base_currency=cfg.base_currency,","        instrument_currency=spec.currency,","        fx_to_base=fx_to_base,","        multiplier=spec.multiplier,","        initial_margin_per_contract=spec.initial_margin_per_contract,","        maintenance_margin_per_contract=spec.maintenance_margin_per_contract,","        source_run_id=source_run_id,","        source_spec_sha=source_spec_sha,","        instruments_config_sha256=cfg.sha256,","    )","    ","    # Write signal_series_meta.json","    meta_path = run_dir / \"signal_series_meta.json\"","    meta_dict = meta.model_dump(by_alias=True)","    meta_path.write_text(","        json.dumps(meta_dict, ensure_ascii=False, sort_keys=True, indent=2) + \"\\n\",","        encoding=\"utf-8\",","    )","    ","    # Update manifest to include signal series files","    manifest_path = run_dir / \"manifest.json\"","    if manifest_path.exists():","        try:","            manifest = json.loads(manifest_path.read_text(encoding=\"utf-8\"))","            # Add signal series artifacts to manifest","            if \"signal_series_artifacts\" not in manifest:","                manifest[\"signal_series_artifacts\"] = []","            manifest[\"signal_series_artifacts\"].extend([","                {","                    \"path\": \"signal_series.parquet\",","                    \"type\": \"parquet\",","                    \"schema\": \"SIGNAL_SERIES_V1\",","                },","                {","                    \"path\": \"signal_series_meta.json\",","                    \"type\": \"json\",","                    \"schema\": \"SIGNAL_SERIES_V1\",","                }","            ])","            # Write updated manifest","            manifest_path.write_text(","                json.dumps(manifest, ensure_ascii=False, sort_keys=True, indent=2) + \"\\n\",","                encoding=\"utf-8\",","            )","        except Exception as e:","            # Don't fail if manifest update fails, just log","            import logging","            logger = logging.getLogger(__name__)","            logger.warning(f\"Failed to update manifest with signal series artifacts: {e}\")"]}
{"type":"file_footer","path":"src/portfolio/signal_series_writer.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/spec.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3231,"sha256":"44a65113f404abeec7a45a12ed4aae9d46dcac6b323814d1b47dc897964d460d","total_lines":90,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/spec.py","chunk_index":0,"line_start":1,"line_end":90,"content":["","\"\"\"Portfolio specification data model.","","Phase 8: Portfolio OS - versioned, auditable, replayable portfolio definitions.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, field","from typing import Dict, List","","","@dataclass(frozen=True)","class PortfolioLeg:","    \"\"\"Portfolio leg definition.","    ","    A leg represents one trading strategy applied to one symbol/timeframe.","    ","    Attributes:","        leg_id: Unique leg identifier (e.g., \"mnq_60_sma\")","        symbol: Symbol identifier (e.g., \"CME.MNQ\")","        timeframe_min: Timeframe in minutes (e.g., 60)","        session_profile: Path to session profile YAML file or profile ID","        strategy_id: Strategy identifier (must exist in registry)","        strategy_version: Strategy version (must match registry)","        params: Strategy parameters dict (key-value pairs)","        enabled: Whether this leg is enabled (default: True)","        tags: Optional tags for categorization (default: empty list)","    \"\"\"","    leg_id: str","    symbol: str","    timeframe_min: int","    session_profile: str","    strategy_id: str","    strategy_version: str","    params: Dict[str, float]","    enabled: bool = True","    tags: List[str] = field(default_factory=list)","    ","    def __post_init__(self) -> None:","        \"\"\"Validate leg fields.\"\"\"","        if not self.leg_id:","            raise ValueError(\"leg_id cannot be empty\")","        if not self.symbol:","            raise ValueError(\"symbol cannot be empty\")","        if self.timeframe_min <= 0:","            raise ValueError(f\"timeframe_min must be > 0, got {self.timeframe_min}\")","        if not self.session_profile:","            raise ValueError(\"session_profile cannot be empty\")","        if not self.strategy_id:","            raise ValueError(\"strategy_id cannot be empty\")","        if not self.strategy_version:","            raise ValueError(\"strategy_version cannot be empty\")","        if not isinstance(self.params, dict):","            raise ValueError(f\"params must be dict, got {type(self.params)}\")","","","@dataclass(frozen=True)","class PortfolioSpec:","    \"\"\"Portfolio specification.","    ","    Defines a portfolio as a collection of legs (trading strategies).","    ","    Attributes:","        portfolio_id: Unique portfolio identifier (e.g., \"mvp\")","        version: Portfolio version (e.g., \"2026Q1\")","        data_tz: Data timezone (default: \"Asia/Taipei\", fixed)","        legs: List of portfolio legs","    \"\"\"","    portfolio_id: str","    version: str","    data_tz: str = \"Asia/Taipei\"  # Fixed default","    legs: List[PortfolioLeg] = field(default_factory=list)","    ","    def __post_init__(self) -> None:","        \"\"\"Validate portfolio spec.\"\"\"","        if not self.portfolio_id:","            raise ValueError(\"portfolio_id cannot be empty\")","        if not self.version:","            raise ValueError(\"version cannot be empty\")","        if self.data_tz != \"Asia/Taipei\":","            raise ValueError(f\"data_tz must be 'Asia/Taipei' (fixed), got {self.data_tz}\")","        ","        # Check leg_id uniqueness","        leg_ids = [leg.leg_id for leg in self.legs]","        if len(leg_ids) != len(set(leg_ids)):","            duplicates = [lid for lid in leg_ids if leg_ids.count(lid) > 1]","            raise ValueError(f\"Duplicate leg_id found: {set(duplicates)}\")","",""]}
{"type":"file_footer","path":"src/portfolio/spec.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/validate.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3970,"sha256":"b5e82520c1a26ba0ad55e28119f07ebca15ed31b917cf72c445684a967290500","total_lines":98,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/validate.py","chunk_index":0,"line_start":1,"line_end":98,"content":["","\"\"\"Portfolio specification validator.","","Phase 8: Validate portfolio spec against contracts.","\"\"\"","","from __future__ import annotations","","from pathlib import Path","","from data.session.loader import load_session_profile","from portfolio.spec import PortfolioSpec","from strategy.registry import get","","","def validate_portfolio_spec(spec: PortfolioSpec) -> None:","    \"\"\"Validate portfolio specification.","    ","    Validates:","    - portfolio_id/version non-empty (already checked in PortfolioSpec.__post_init__)","    - legs non-empty; each leg_id unique (already checked in PortfolioSpec.__post_init__)","    - timeframe_min > 0 (already checked in PortfolioLeg.__post_init__)","    - session_profile path exists and can be loaded","    - strategy_id exists in registry","    - strategy_version matches registry (strict match)","    - params is dict with float values (already checked in loader)","    ","    Args:","        spec: Portfolio specification to validate","        ","    Raises:","        ValueError: If validation fails","        FileNotFoundError: If session profile not found","        KeyError: If strategy not found in registry","    \"\"\"","    if not spec.legs:","        raise ValueError(\"Portfolio must have at least one leg\")","    ","    # Validate each leg","    for leg in spec.legs:","        # Validate session_profile path exists and can be loaded","        session_profile_path = Path(leg.session_profile)","        ","        # Handle relative paths (relative to project root or current working directory)","        if not session_profile_path.is_absolute():","            # Try relative to current working directory first","            if not session_profile_path.exists():","                # Try relative to project root (if path starts with src/)","                if leg.session_profile.startswith(\"src/\"):","                    # Path is already relative to project root","                    if not session_profile_path.exists():","                        # Try from current directory","                        pass","                else:","                    # Check configs/profiles/ location","                    configs_profile_path = Path(\"configs/profiles\") / session_profile_path.name","                    if configs_profile_path.exists():","                        session_profile_path = configs_profile_path","        ","        if not session_profile_path.exists():","            raise FileNotFoundError(","                f\"Leg '{leg.leg_id}': session_profile not found: {leg.session_profile}\"","            )","        ","        # Try to load session profile","        try:","            load_session_profile(session_profile_path)","        except Exception as e:","            raise ValueError(","                f\"Leg '{leg.leg_id}': failed to load session_profile '{leg.session_profile}': {e}\"","            )","        ","        # Validate strategy_id exists in registry","        try:","            strategy_spec = get(leg.strategy_id)","        except KeyError as e:","            raise KeyError(","                f\"Leg '{leg.leg_id}': strategy_id '{leg.strategy_id}' not found in registry: {e}\"","            )","        ","        # Validate strategy_version matches (strict match)","        if strategy_spec.version != leg.strategy_version:","            raise ValueError(","                f\"Leg '{leg.leg_id}': strategy_version mismatch. \"","                f\"Expected '{strategy_spec.version}' (from registry), got '{leg.strategy_version}'\"","            )","        ","        # Validate params keys exist in strategy param_schema (optional check)","        # This is a best-effort check - runner will handle defaults","        param_schema = strategy_spec.param_schema","        if isinstance(param_schema, dict) and \"properties\" in param_schema:","            schema_props = param_schema.get(\"properties\", {})","            for param_key in leg.params.keys():","                if param_key not in schema_props and param_key not in strategy_spec.defaults:","                    # Warning: extra param, but allowed (runner will log warning)","                    pass","",""]}
{"type":"file_footer","path":"src/portfolio/validate.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/portfolio/writer.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":7031,"sha256":"653e2c1420bc3f8214eee4ec594e0efd4985979a594a4cd381f5ec07f41e1c38","total_lines":183,"chunk_count":1}
{"type":"file_chunk","path":"src/portfolio/writer.py","chunk_index":0,"line_start":1,"line_end":183,"content":["","\"\"\"Portfolio artifacts writer.","","Phase 8/11:","- Single source of truth: PortfolioSpec (dataclass) in spec.py","- Writer is IO-only: write portfolio_spec.json + portfolio_manifest.json + README.md","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import asdict, is_dataclass","from datetime import datetime, timezone","from pathlib import Path","from typing import Any","","from portfolio.spec import PortfolioSpec","","","def _utc_now_z() -> str:","    \"\"\"Return UTC timestamp ending with 'Z'.\"\"\"","    return datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")","","","def _json_dump(path: Path, obj: Any) -> None:","    path.write_text(","        json.dumps(obj, ensure_ascii=False, indent=2, sort_keys=True),","        encoding=\"utf-8\",","    )","","","def _spec_to_dict(spec: PortfolioSpec) -> dict:","    \"\"\"Convert PortfolioSpec to a JSON-serializable dict deterministically.\"\"\"","    if is_dataclass(spec):","        return asdict(spec)","","    # Fallback if spec ever becomes pydantic-like","    if hasattr(spec, \"model_dump\"):","        return spec.model_dump()  # type: ignore[no-any-return]","    if hasattr(spec, \"dict\"):","        return spec.dict()  # type: ignore[no-any-return]","","    raise TypeError(f\"Unsupported spec type for serialization: {type(spec)}\")","","","def _render_readme_md(*, spec: PortfolioSpec, manifest: dict) -> str:","    \"\"\"Render README.md content that satisfies test contracts.","    ","    Required sections (order matters for readability):","    # Portfolio: {portfolio_id}","    ## Purpose","    ## Inputs","    ## Legs","    ## Summary","    ## Reproducibility","    ## Files","    ## Warnings (optional but kept for compatibility)","    \"\"\"","    portfolio_id = manifest.get(\"portfolio_id\", getattr(spec, \"portfolio_id\", \"\"))","    season = manifest.get(\"season\", \"\")","","    inputs = manifest.get(\"inputs\", {}) or {}","    counts = manifest.get(\"counts\", {}) or {}","    warnings = manifest.get(\"warnings\", {}) or {}","","    decisions_log_path = inputs.get(\"decisions_log_path\", \"\")","    decisions_log_sha1 = inputs.get(\"decisions_log_sha1\", \"\")","    research_index_path = inputs.get(\"research_index_path\", \"\")","    research_index_sha1 = inputs.get(\"research_index_sha1\", \"\")","","    total_decisions = counts.get(\"total_decisions\", 0)","    keep_decisions = counts.get(\"keep_decisions\", 0)","    num_legs_final = counts.get(\"num_legs_final\", len(getattr(spec, \"legs\", []) or []))","    symbols_allowlist = manifest.get(\"symbols_allowlist\", [])","","    lines: list[str] = []","    lines.append(f\"# Portfolio: {portfolio_id}\")","    lines.append(\"\")","    lines.append(\"## Purpose\")","    lines.append(","        \"This folder contains an **executable portfolio specification** generated from Research decisions \"","        \"(append-only decisions.log). It is designed to be deterministic and auditable.\"","    )","    lines.append(\"\")","","    lines.append(\"## Inputs\")","    lines.append(f\"- season: `{season}`\")","    lines.append(f\"- decisions_log_path: `{decisions_log_path}`\")","    lines.append(f\"- decisions_log_sha1: `{decisions_log_sha1}`\")","    lines.append(f\"- research_index_path: `{research_index_path}`\")","    lines.append(f\"- research_index_sha1: `{research_index_sha1}`\")","    lines.append(f\"- symbols_allowlist: `{symbols_allowlist}`\")","    lines.append(\"\")","","    lines.append(\"## Legs\")","    legs = getattr(spec, \"legs\", None) or []","    if legs:","        lines.append(\"| symbol | timeframe_min | session_profile | strategy_id | strategy_version | enabled | leg_id |\")","        lines.append(\"|---|---:|---|---|---|---|---|\")","        for leg in legs:","            # Support both dataclass and dict-like legs","            symbol = getattr(leg, \"symbol\", None) if not isinstance(leg, dict) else leg.get(\"symbol\")","            timeframe_min = getattr(leg, \"timeframe_min\", None) if not isinstance(leg, dict) else leg.get(\"timeframe_min\")","            session_profile = getattr(leg, \"session_profile\", None) if not isinstance(leg, dict) else leg.get(\"session_profile\")","            strategy_id = getattr(leg, \"strategy_id\", None) if not isinstance(leg, dict) else leg.get(\"strategy_id\")","            strategy_version = getattr(leg, \"strategy_version\", None) if not isinstance(leg, dict) else leg.get(\"strategy_version\")","            enabled = getattr(leg, \"enabled\", None) if not isinstance(leg, dict) else leg.get(\"enabled\")","            leg_id = getattr(leg, \"leg_id\", None) if not isinstance(leg, dict) else leg.get(\"leg_id\")","            ","            lines.append(","                f\"| {symbol} | {timeframe_min} | {session_profile} | \"","                f\"{strategy_id} | {strategy_version} | {enabled} | {leg_id} |\"","            )","    else:","        lines.append(\"_No legs (empty portfolio)._\")","    lines.append(\"\")","","    lines.append(\"## Summary\")","    lines.append(f\"- portfolio_id: `{portfolio_id}`\")","    lines.append(f\"- version: `{getattr(spec, 'version', '')}`\")","    lines.append(f\"- total_decisions: `{total_decisions}`\")","    lines.append(f\"- keep_decisions: `{keep_decisions}`\")","    lines.append(f\"- num_legs_final: `{num_legs_final}`\")","    lines.append(\"\")","","    lines.append(\"## Reproducibility\")","    lines.append(\"To reproduce this portfolio exactly, you must use the same inputs and ordering rules:\")","    lines.append(\"- decisions.log is append-only; **last decision wins** per run_id.\")","    lines.append(\"- legs are filtered by symbols_allowlist.\")","    lines.append(\"- legs are sorted deterministically before portfolio_id generation.\")","    lines.append(\"- the input digests above (sha1) must match.\")","    lines.append(\"\")","","    lines.append(\"## Files\")","    lines.append(\"- `portfolio_spec.json`\")","    lines.append(\"- `portfolio_manifest.json`\")","    lines.append(\"- `README.md`\")","    lines.append(\"\")","","    # Optional: keep warnings section for compatibility","    lines.append(\"## Warnings\")","    lines.append(f\"- missing_run_ids: {warnings.get('missing_run_ids', [])}\")","    lines.append(\"\")","","    return \"\\n\".join(lines)","","","def write_portfolio_artifacts(","    *,","    outputs_root: Path,","    season: str,","    spec: PortfolioSpec,","    manifest: dict,",") -> Path:","    \"\"\"Write portfolio artifacts to outputs/seasons/{season}/portfolio/{portfolio_id}/","","    Contract:","    - IO-only","    - Deterministic file content given (spec, manifest) except generated_at if caller omitted it","    \"\"\"","    portfolio_id = getattr(spec, \"portfolio_id\", None)","    if not portfolio_id or not str(portfolio_id).strip():","        raise ValueError(\"spec.portfolio_id must be non-empty\")","","    out_dir = outputs_root / \"seasons\" / season / \"portfolio\" / str(portfolio_id)","    out_dir.mkdir(parents=True, exist_ok=True)","","    # Ensure generated_at exists","    if \"generated_at\" not in manifest or not str(manifest.get(\"generated_at\", \"\")).strip():","        manifest = dict(manifest)","        manifest[\"generated_at\"] = _utc_now_z()","","    spec_dict = _spec_to_dict(spec)","","    _json_dump(out_dir / \"portfolio_spec.json\", spec_dict)","    _json_dump(out_dir / \"portfolio_manifest.json\", manifest)","","    readme = _render_readme_md(spec=spec, manifest=manifest)","    (out_dir / \"README.md\").write_text(readme, encoding=\"utf-8\")","","    return out_dir","",""]}
{"type":"file_footer","path":"src/portfolio/writer.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/research/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":249,"sha256":"f08bdc7c012cb0ee85b9282c7bd62e72973c74b1bab7f50b67126072f48b3efa","total_lines":11,"chunk_count":1}
{"type":"file_chunk","path":"src/research/__init__.py","chunk_index":0,"line_start":1,"line_end":11,"content":["","\"\"\"Research Governance Layer (Phase 9).","","Provides standardized summary, comparison, and archival capabilities for portfolio runs.","Read-only layer that extracts and aggregates data from existing artifacts.","\"\"\"","","from __future__ import annotations","","",""]}
{"type":"file_footer","path":"src/research/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/research/__main__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2808,"sha256":"0ef2d3b2234ed82158e3bdd5e01ead7dc49b31c6f11aadc39974b020521adef6","total_lines":101,"chunk_count":1}
{"type":"file_chunk","path":"src/research/__main__.py","chunk_index":0,"line_start":1,"line_end":101,"content":["","\"\"\"Research Governance Layer main entry point.","","Phase 9: Generate canonical results and research index.","\"\"\"","","from __future__ import annotations","","import json","import sys","from pathlib import Path","","from research.registry import build_research_index","from research.extract import extract_canonical_metrics, ExtractionError","","","def generate_canonical_results(outputs_root: Path, research_dir: Path) -> Path:","    \"\"\"","    Generate canonical_results.json from all runs.","    ","    Args:","        outputs_root: Root outputs directory","        research_dir: Research output directory","        ","    Returns:","        Path to canonical_results.json","    \"\"\"","    research_dir.mkdir(parents=True, exist_ok=True)","    ","    # Scan all runs","    seasons_dir = outputs_root / \"seasons\"","    if not seasons_dir.exists():","        # Create empty results","        results_path = research_dir / \"canonical_results.json\"","        with open(results_path, \"w\", encoding=\"utf-8\") as f:","            json.dump({\"results\": []}, f, indent=2, ensure_ascii=False, sort_keys=True)","        return results_path","    ","    results = []","    ","    # Scan seasons","    for season_dir in seasons_dir.iterdir():","        if not season_dir.is_dir():","            continue","        ","        runs_dir = season_dir / \"runs\"","        if not runs_dir.exists():","            continue","        ","        # Scan runs","        for run_dir in runs_dir.iterdir():","            if not run_dir.is_dir():","                continue","            ","            try:","                metrics = extract_canonical_metrics(run_dir)","                results.append(metrics.to_dict())","            except ExtractionError:","                # Skip runs with missing artifacts","                continue","    ","    # Write results","    results_path = research_dir / \"canonical_results.json\"","    results_data = {","        \"results\": results,","        \"total_runs\": len(results),","    }","    ","    with open(results_path, \"w\", encoding=\"utf-8\") as f:","        json.dump(results_data, f, indent=2, ensure_ascii=False, sort_keys=True)","    ","    return results_path","","","def main() -> int:","    \"\"\"Main entry point for research governance layer.\"\"\"","    outputs_root = Path(\"outputs\")","    research_dir = outputs_root / \"research\"","    ","    try:","        # Generate canonical results","        print(f\"Generating canonical_results.json...\")","        generate_canonical_results(outputs_root, research_dir)","        ","        # Build research index","        print(f\"Building research_index.json...\")","        build_research_index(outputs_root, research_dir)","        ","        print(f\"Research governance layer completed successfully.\")","        print(f\"Output directory: {research_dir}\")","        return 0","    except Exception as e:","        print(f\"Error: {e}\", file=sys.stderr)","        return 1","","","if __name__ == \"__main__\":","    sys.exit(main())","","",""]}
{"type":"file_footer","path":"src/research/__main__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/research/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":422,"sha256":"5bd6882dfa3d51fc7242af9aa85cd2d1deabfb7c734ffa7ceea3159bb892b2f9","note":"skipped by policy"}
{"type":"file_skipped","path":"src/research/__pycache__/plateau.cpython-312.pyc","reason":"cache","bytes":14717,"sha256":"d7ddd086245a338cd9a4da8003880b1ef6cd77277e6581079df1c05a839a415f","note":"skipped by policy"}
{"type":"file_header","path":"src/research/decision.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2307,"sha256":"204cc32953d97e5dae82ed325a84b99f83d0d41da893d8cd69b7ded2b128b3aa","total_lines":84,"chunk_count":1}
{"type":"file_chunk","path":"src/research/decision.py","chunk_index":0,"line_start":1,"line_end":84,"content":["","\"\"\"Research Decision - manage KEEP/DROP/ARCHIVE decisions.","","Phase 9: Append-only decision log with notes and timestamps.","\"\"\"","","from __future__ import annotations","","import json","from datetime import datetime, timezone","from pathlib import Path","from typing import Any, Dict, List, Literal","","DecisionType = Literal[\"KEEP\", \"DROP\", \"ARCHIVE\"]","","","def append_decision(out_dir: Path, run_id: str, decision: DecisionType, note: str) -> Path:","    \"\"\"","    Append a decision to decisions.log (JSONL format).","    ","    Same run_id can have multiple decisions (append-only).","    The research_index.json will show the last decision (last-write-wins view).","    ","    Args:","        out_dir: Research output directory","        run_id: Run ID","        decision: Decision type (KEEP, DROP, ARCHIVE)","        note: Note explaining the decision","        ","    Returns:","        Path to decisions.log","    \"\"\"","    out_dir.mkdir(parents=True, exist_ok=True)","    ","    # Append to log (JSONL format)","    decisions_log_path = out_dir / \"decisions.log\"","    ","    decision_entry = {","        \"run_id\": run_id,","        \"decision\": decision,","        \"note\": note,","        \"decided_at\": datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","    }","    ","    with open(decisions_log_path, \"a\", encoding=\"utf-8\") as f:","        f.write(json.dumps(decision_entry, ensure_ascii=False, sort_keys=True) + \"\\n\")","    ","    return decisions_log_path","","","def load_decisions(out_dir: Path) -> List[Dict[str, Any]]:","    \"\"\"","    Load all decisions from decisions.log.","    ","    Args:","        out_dir: Research output directory","        ","    Returns:","        List of decision entries (all entries, including duplicates for same run_id)","    \"\"\"","    decisions_log_path = out_dir / \"decisions.log\"","    ","    if not decisions_log_path.exists():","        return []","    ","    decisions = []","    try:","        with open(decisions_log_path, \"r\", encoding=\"utf-8\") as f:","            for line in f:","                line = line.strip()","                if not line:","                    continue","                try:","                    entry = json.loads(line)","                    decisions.append(entry)","                except json.JSONDecodeError:","                    # Skip invalid lines","                    continue","    except Exception:","        pass","    ","    return decisions","",""]}
{"type":"file_footer","path":"src/research/decision.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/research/extract.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6416,"sha256":"2ce496e5b04c990cfc85af758fbfa366dec5cadb10da9ccf0844f8c8b9057231","total_lines":183,"chunk_count":1}
{"type":"file_chunk","path":"src/research/extract.py","chunk_index":0,"line_start":1,"line_end":183,"content":["","\"\"\"Result Extractor - extract canonical metrics from artifacts.","","Phase 9: Read-only extraction from existing artifacts.","No computation, only aggregation from existing data.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Any, Dict","","from research.metrics import CanonicalMetrics","","","class ExtractionError(Exception):","    \"\"\"Raised when required artifacts or fields are missing.\"\"\"","    pass","","","def extract_canonical_metrics(run_dir: Path) -> CanonicalMetrics:","    \"\"\"","    Extract canonical metrics from run artifacts.","    ","    Reads artifacts from run_dir (at least one of manifest/metrics/config_snapshot/README must exist).","    Uses field mapping table to map artifact fields to CanonicalMetrics.","    ","    Args:","        run_dir: Path to run directory","        ","    Returns:","        CanonicalMetrics instance","        ","    Raises:","        ExtractionError: If required artifacts or fields are missing","    \"\"\"","    # Check at least one artifact exists","    manifest_path = run_dir / \"manifest.json\"","    metrics_path = run_dir / \"metrics.json\"","    config_path = run_dir / \"config_snapshot.json\"","    winners_path = run_dir / \"winners.json\"","    ","    if not any(p.exists() for p in [manifest_path, metrics_path, config_path]):","        raise ExtractionError(f\"No artifacts found in {run_dir}\")","    ","    # Load available artifacts","    manifest: Dict[str, Any] = {}","    metrics_data: Dict[str, Any] = {}","    config_data: Dict[str, Any] = {}","    winners: Dict[str, Any] = {}","    ","    if manifest_path.exists():","        try:","            with open(manifest_path, \"r\", encoding=\"utf-8\") as f:","                manifest = json.load(f)","        except json.JSONDecodeError as e:","            raise ExtractionError(f\"Invalid manifest.json: {e}\")","    ","    if metrics_path.exists():","        try:","            with open(metrics_path, \"r\", encoding=\"utf-8\") as f:","                metrics_data = json.load(f)","        except json.JSONDecodeError as e:","            raise ExtractionError(f\"Invalid metrics.json: {e}\")","    ","    if config_path.exists():","        try:","            with open(config_path, \"r\", encoding=\"utf-8\") as f:","                config_data = json.load(f)","        except json.JSONDecodeError as e:","            raise ExtractionError(f\"Invalid config_snapshot.json: {e}\")","    ","    if winners_path.exists():","        try:","            with open(winners_path, \"r\", encoding=\"utf-8\") as f:","                winners = json.load(f)","        except json.JSONDecodeError as e:","            raise ExtractionError(f\"Invalid winners.json: {e}\")","    ","    # Field mapping table: artifact field -> CanonicalMetrics field","    # Extract identification","    run_id = manifest.get(\"run_id\") or metrics_data.get(\"run_id\")","    if not run_id:","        raise ExtractionError(\"Missing 'run_id' in artifacts\")","    ","    portfolio_id = manifest.get(\"portfolio_id\") or config_data.get(\"portfolio_id\")","    portfolio_version = manifest.get(\"portfolio_version\") or config_data.get(\"portfolio_version\")","    ","    # Strategy info from winners.json topk (take first item if available)","    strategy_id = None","    strategy_version = None","    symbol = None","    timeframe_min = None","    ","    topk = winners.get(\"topk\", [])","    if topk and isinstance(topk, list) and len(topk) > 0:","        first_item = topk[0]","        strategy_id = first_item.get(\"strategy_id\")","        symbol = first_item.get(\"symbol\")","        # timeframe_min might be in config or need parsing from timeframe string","        timeframe_str = first_item.get(\"timeframe\", \"\")","        if timeframe_str and timeframe_str != \"UNKNOWN\":","            # Try to extract minutes from timeframe (e.g., \"60m\" -> 60)","            try:","                if timeframe_str.endswith(\"m\"):","                    timeframe_min = int(timeframe_str[:-1])","            except ValueError:","                pass","    ","    # Extract bars (required)","    bars = manifest.get(\"bars\") or metrics_data.get(\"bars\") or config_data.get(\"bars\")","    if bars is None:","        raise ExtractionError(\"Missing 'bars' in artifacts\")","    ","    # Extract dates","    start_date = manifest.get(\"created_at\", \"\")","    end_date = \"\"  # Not available in artifacts","    ","    # Extract core metrics from winners.json topk aggregation","    # Aggregate net_profit, max_dd, trades from topk","    total_net_profit = 0.0","    max_max_dd = 0.0","    total_trades = 0","    ","    for item in topk:","        item_metrics = item.get(\"metrics\", {})","        net_profit = item_metrics.get(\"net_profit\", 0.0)","        max_dd = item_metrics.get(\"max_dd\", 0.0)","        trades = item_metrics.get(\"trades\", 0)","        ","        total_net_profit += net_profit","        max_max_dd = min(max_max_dd, max_dd)  # max_dd is negative or 0","        total_trades += trades","    ","    net_profit = total_net_profit","    max_drawdown = abs(max_max_dd)  # Convert to positive","    ","    # Extract profit_factor and sharpe from metrics (if available)","    # These may not be in artifacts, so allow None","    profit_factor = metrics_data.get(\"profit_factor\")","    sharpe = metrics_data.get(\"sharpe\")","    ","    # Calculate derived scores","    # score_net_mdd = net_profit / abs(max_drawdown)","    # If max_drawdown == 0, raise error (as per requirement)","    if max_drawdown == 0.0:","        if net_profit != 0.0:","            # Non-zero profit but zero drawdown - this is edge case","            # Per requirement: \"mdd=0 → inf or raise, please define clearly\"","            # We'll raise to be explicit","            raise ExtractionError(","                f\"max_drawdown is 0 but net_profit is {net_profit}, \"","                \"cannot calculate score_net_mdd\"","            )","        score_net_mdd = 0.0","    else:","        score_net_mdd = net_profit / max_drawdown","    ","    # score_final = score_net_mdd * (trades ** 0.25)","    score_final = score_net_mdd * (total_trades ** 0.25) if total_trades > 0 else 0.0","    ","    return CanonicalMetrics(","        run_id=run_id,","        portfolio_id=portfolio_id,","        portfolio_version=portfolio_version,","        strategy_id=strategy_id,","        strategy_version=strategy_version,","        symbol=symbol,","        timeframe_min=timeframe_min,","        net_profit=net_profit,","        max_drawdown=max_drawdown,","        profit_factor=profit_factor,","        sharpe=sharpe,","        trades=total_trades,","        score_net_mdd=score_net_mdd,","        score_final=score_final,","        bars=bars,","        start_date=start_date,","        end_date=end_date,","    )","",""]}
{"type":"file_footer","path":"src/research/extract.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/research/metrics.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1571,"sha256":"2df6b67466d3a0daaf047a1b0d15b3cb1b77b8da179174c749222ab0a60395bd","total_lines":56,"chunk_count":1}
{"type":"file_chunk","path":"src/research/metrics.py","chunk_index":0,"line_start":1,"line_end":56,"content":["","\"\"\"Canonical Metrics Schema for research results.","","Phase 9: Standardized format for portfolio run results.","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass, asdict","from typing import Any, Dict","","","@dataclass(frozen=True)","class CanonicalMetrics:","    \"\"\"","    Canonical metrics schema for research results.","    ","    This is the official format for summarizing portfolio run results.","    All fields are required - missing data must be handled at extraction time.","    \"\"\"","    # Identification","    run_id: str","    portfolio_id: str | None","    portfolio_version: str | None","    strategy_id: str | None","    strategy_version: str | None","    symbol: str | None","    timeframe_min: int | None","    ","    # Performance (core numerical fields)","    net_profit: float","    max_drawdown: float","    profit_factor: float | None  # May be None if not available in artifacts","    sharpe: float | None  # May be None if not available in artifacts","    trades: int","    ","    # Derived scores (computed from existing values only)","    score_net_mdd: float  # Net / |MDD|, raises if MDD=0","    score_final: float  # score_net_mdd * (trades ** 0.25)","    ","    # Metadata","    bars: int","    start_date: str  # ISO8601 format or empty string","    end_date: str  # ISO8601 format or empty string","    ","    def to_dict(self) -> Dict[str, Any]:","        \"\"\"Convert to dictionary for JSON serialization.\"\"\"","        return asdict(self)","    ","    @classmethod","    def from_dict(cls, data: Dict[str, Any]) -> CanonicalMetrics:","        \"\"\"Create from dictionary.\"\"\"","        return cls(**data)","","",""]}
{"type":"file_footer","path":"src/research/metrics.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/research/plateau.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":12833,"sha256":"0a5374df9d1cb32e5774cff6d75ede9f6eb93c33f7951174288192023e7cd2df","total_lines":357,"chunk_count":2}
{"type":"file_chunk","path":"src/research/plateau.py","chunk_index":0,"line_start":1,"line_end":200,"content":["#!/usr/bin/env python3","\"\"\"","Plateau Identification for research results.","","Phase 3A: Automatically identify stable parameter regions (plateaus) from a","grid of candidates, replacing human judgment of heatmaps.","","Algorithm:","1. Load candidates (params dict + score) from winners.json or similar.","2. Normalize parameter dimensions to unit scale.","3. For each candidate, compute local neighborhood (k‑nearest neighbors).","4. Compute stability metric (variance of neighbor scores) and local average score.","5. Select candidate with best combined score (high average, low variance).","6. Define plateau as candidates within a distance threshold that have scores","   within a relative range of the selected candidate.","7. Output main candidate, backup candidates, plateau members, and stability report.","","Deterministic: same input → same output.","No external randomness, no ML beyond basic statistics.","\"\"\"","","from __future__ import annotations","","import json","from dataclasses import dataclass, asdict","from pathlib import Path","from typing import Dict, List, Any, Tuple, Optional","import numpy as np","","","@dataclass(frozen=True)","class PlateauCandidate:","    \"\"\"A single candidate with parameters and performance score.\"\"\"","    candidate_id: str","    strategy_id: str","    symbol: str","    timeframe: str","    params: Dict[str, float]  # parameter name → value","    score: float","    metrics: Dict[str, Any]  # original metrics (net_profit, max_dd, trades, etc.)","","","@dataclass(frozen=True)","class PlateauRegion:","    \"\"\"A connected region of parameter space with similar performance.\"\"\"","    region_id: str","    members: List[PlateauCandidate]  # candidates belonging to this region","    centroid_params: Dict[str, float]  # average parameter values","    centroid_score: float  # average score of members","    score_variance: float  # variance of scores within region","    stability_score: float  # computed as centroid_score / (1 + score_variance)","    # distance threshold used to define region","    distance_threshold: float","","","@dataclass(frozen=True)","class PlateauReport:","    \"\"\"Full plateau identification report.\"\"\"","    candidates_seen: int","    param_names: List[str]","    selected_main: PlateauCandidate","    selected_backup: List[PlateauCandidate]  # ordered by preference","    plateau_region: PlateauRegion","    algorithm_version: str = \"v1\"","    notes: str = \"\"","","","def load_candidates_from_winners(winners_path: Path) -> List[PlateauCandidate]:","    \"\"\"","    Load candidates from a winners.json (v2) file.","","    Expected format:","        {","            \"topk\": [","                {","                    \"candidate_id\": \"...\",","                    \"strategy_id\": \"...\",","                    \"symbol\": \"...\",","                    \"timeframe\": \"...\",","                    \"params\": {...},","                    \"score\": ...,","                    \"metrics\": {...}","                },","                ...","            ]","        }","    \"\"\"","    if not winners_path.exists():","        raise FileNotFoundError(f\"winners.json not found at {winners_path}\")","","    with open(winners_path, \"r\", encoding=\"utf-8\") as f:","        data = json.load(f)","","    topk = data.get(\"topk\", [])","    if not topk:","        raise ValueError(\"winners.json contains empty 'topk' list\")","","    candidates = []","    for item in topk:","        candidate = PlateauCandidate(","            candidate_id=item.get(\"candidate_id\", \"\"),","            strategy_id=item.get(\"strategy_id\", \"\"),","            symbol=item.get(\"symbol\", \"\"),","            timeframe=item.get(\"timeframe\", \"\"),","            params=item.get(\"params\", {}),","            score=float(item.get(\"score\", 0.0)),","            metrics=item.get(\"metrics\", {})","        )","        candidates.append(candidate)","","    return candidates","","","def _normalize_params(candidates: List[PlateauCandidate]) -> Tuple[np.ndarray, List[str], Dict[str, Tuple[float, float]]]:","    \"\"\"","    Convert parameter dicts to a normalized numpy matrix (zero mean, unit variance per dimension).","","    Returns:","        X: (n_candidates, n_params) normalized matrix","        param_names: list of parameter names in order","        scaling_info: dict mapping param_name -> (mean, std) for later denormalization","    \"\"\"","    if not candidates:","        raise ValueError(\"No candidates provided\")","","    # Collect all parameter names (union across candidates)","    param_names_set = set()","    for cand in candidates:","        param_names_set.update(cand.params.keys())","    param_names = sorted(param_names_set)","","    if not param_names:","        # No parameters (edge case) – return dummy dimension","        X = np.zeros((len(candidates), 1))","        scaling_info = {\"dummy\": (0.0, 1.0)}","        return X, [\"dummy\"], scaling_info","","    # Build raw matrix","    X_raw = np.zeros((len(candidates), len(param_names)))","    for i, cand in enumerate(candidates):","        for j, pname in enumerate(param_names):","            X_raw[i, j] = cand.params.get(pname, 0.0)","","    # Normalize","    means = np.mean(X_raw, axis=0)","    stds = np.std(X_raw, axis=0)","    # Avoid division by zero","    stds[stds == 0] = 1.0","    X = (X_raw - means) / stds","","    scaling_info = {}","    for idx, pname in enumerate(param_names):","        scaling_info[pname] = (means[idx], stds[idx])","","    return X, param_names, scaling_info","","","def _compute_pairwise_distances(X: np.ndarray) -> np.ndarray:","    \"\"\"Compute Euclidean distance matrix between all candidates.\"\"\"","    n = X.shape[0]","    distances = np.zeros((n, n))","    for i in range(n):","        # vectorized computation of squared differences","        diff = X - X[i]","        distances[i, :] = np.sqrt(np.sum(diff ** 2, axis=1))","    return distances","","","def _find_plateau(","    candidates: List[PlateauCandidate],","    X: np.ndarray,","    param_names: List[str],","    distance_matrix: np.ndarray,","    k_neighbors: int = 5,","    score_threshold_rel: float = 0.1,",") -> PlateauReport:","    \"\"\"","    Core plateau identification logic.","","    Steps:","    1. For each candidate compute local stability (score variance among k‑nearest neighbors).","    2. Combine local average score and variance into a composite score.","    3. Select candidate with highest composite score.","    4. Grow region around selected candidate by including neighbors within a distance threshold","       that also have scores within relative threshold.","    5. Choose backup candidates as next best within region.","    \"\"\"","    n = len(candidates)","    if n == 0:","        raise ValueError(\"No candidates\")","","    # Ensure k_neighbors <= n-1","    k = min(k_neighbors, n - 1) if n > 1 else 0","","    scores = np.array([c.score for c in candidates])","","    # For each candidate, find k‑nearest neighbors (excluding self)","    neighbor_indices = []","    neighbor_variances = []","    neighbor_avg_scores = []"]}
{"type":"file_chunk","path":"src/research/plateau.py","chunk_index":1,"line_start":201,"line_end":357,"content":["","    for i in range(n):","        if k == 0:","            neighbor_indices.append([i])","            neighbor_variances.append(0.0)","            neighbor_avg_scores.append(scores[i])","            continue","","        # distances to all other candidates","        dists = distance_matrix[i]","        # get indices sorted by distance (skip self)","        sorted_idx = np.argsort(dists)","        # self is at distance zero, so first element is i","        nearest = sorted_idx[1:k+1]  # exclude self","        neighbor_indices.append(nearest.tolist())","        neighbor_scores = scores[nearest]","        neighbor_variances.append(np.var(neighbor_scores))","        neighbor_avg_scores.append(np.mean(neighbor_scores))","","    # Composite score: average_score * (1 - normalized_variance)","    # Normalize variance across candidates to [0,1] range","    if n > 1 and max(neighbor_variances) > 0:","        norm_var = np.array(neighbor_variances) / max(neighbor_variances)","    else:","        norm_var = np.zeros(n)","","    composite = np.array(neighbor_avg_scores) * (1.0 - norm_var)","","    # Select candidate with highest composite score","    selected_idx = int(np.argmax(composite))","    selected = candidates[selected_idx]","","    # Determine distance threshold as median distance to its k‑nearest neighbors","    if k > 0:","        neighbor_dists = distance_matrix[selected_idx][neighbor_indices[selected_idx]]","        distance_threshold = float(np.median(neighbor_dists)) * 1.5  # expand a bit","    else:","        distance_threshold = 0.0","","    # Grow region: include all candidates within distance_threshold AND score within relative range","    region_indices = []","    for i in range(n):","        if distance_matrix[selected_idx, i] <= distance_threshold:","            # score within score_threshold_rel of selected score","            if abs(scores[i] - selected.score) <= score_threshold_rel * abs(selected.score):","                region_indices.append(i)","","    region_members = [candidates[i] for i in region_indices]","","    # Compute region centroid (average normalized parameters)","    if region_members:","        X_region = X[region_indices]","        centroid_normalized = np.mean(X_region, axis=0)","        # Convert centroid back to original parameter scale (requires scaling_info)","        # For simplicity we'll just use the selected candidate's params as centroid.","        centroid_params = selected.params","        centroid_score = float(np.mean(scores[region_indices]))","        region_score_var = float(np.var(scores[region_indices]))","    else:","        centroid_params = selected.params","        centroid_score = selected.score","        region_score_var = 0.0","","    # Stability score = centroid_score / (1 + region_score_var)","    stability_score = centroid_score / (1.0 + region_score_var)","","    plateau_region = PlateauRegion(","        region_id=f\"plateau_{selected_idx}\",","        members=region_members,","        centroid_params=centroid_params,","        centroid_score=centroid_score,","        score_variance=region_score_var,","        stability_score=stability_score,","        distance_threshold=distance_threshold,","    )","","    # Select backup candidates: top‑3 scores within region (excluding main)","    region_scores_with_idx = [(i, scores[i]) for i in region_indices if i != selected_idx]","    region_scores_with_idx.sort(key=lambda x: x[1], reverse=True)","    backup_indices = [idx for idx, _ in region_scores_with_idx[:2]]  # at most two backups","    backup_candidates = [candidates[i] for i in backup_indices]","","    report = PlateauReport(","        candidates_seen=n,","        param_names=param_names,","        selected_main=selected,","        selected_backup=backup_candidates,","        plateau_region=plateau_region,","        notes=f\"k_neighbors={k}, score_threshold_rel={score_threshold_rel}\",","    )","    return report","","","def identify_plateau_from_winners(winners_path: Path, **kwargs) -> PlateauReport:","    \"\"\"","    High‑level entry point: load winners.json and run plateau identification.","","    Keyword arguments are passed to _find_plateau (k_neighbors, score_threshold_rel).","    \"\"\"","    candidates = load_candidates_from_winners(winners_path)","    X, param_names, _ = _normalize_params(candidates)","    distances = _compute_pairwise_distances(X)","    return _find_plateau(candidates, X, param_names, distances, **kwargs)","","","def save_plateau_report(report: PlateauReport, output_dir: Path) -> None:","    \"\"\"Save plateau report and chosen parameters as JSON files.\"\"\"","    output_dir.mkdir(parents=True, exist_ok=True)","","    # Convert dataclasses to dicts for JSON serialization","    def dataclass_to_dict(obj):","        if hasattr(obj, \"__dataclass_fields__\"):","            d = asdict(obj)","            # Recursively convert nested dataclasses","            for k, v in d.items():","                if isinstance(v, list):","                    d[k] = [dataclass_to_dict(item) if hasattr(item, \"__dataclass_fields__\") else item for item in v]","                elif hasattr(v, \"__dataclass_fields__\"):","                    d[k] = dataclass_to_dict(v)","            return d","        return obj","","    report_dict = dataclass_to_dict(report)","","    # Plateau report","    plateau_path = output_dir / \"plateau_report.json\"","    with open(plateau_path, \"w\", encoding=\"utf-8\") as f:","        json.dump(report_dict, f, indent=2, ensure_ascii=False)","","    # Chosen parameters (main + backups)","    chosen = {","        \"main\": dataclass_to_dict(report.selected_main),","        \"backups\": dataclass_to_dict(report.selected_backup),","        \"generated_at\": \"\",  # caller can fill timestamp","    }","    chosen_path = output_dir / \"chosen_params.json\"","    with open(chosen_path, \"w\", encoding=\"utf-8\") as f:","        json.dump(chosen, f, indent=2, ensure_ascii=False)","","    print(f\"Plateau report saved to {plateau_path}\")","    print(f\"Chosen parameters saved to {chosen_path}\")","","","if __name__ == \"__main__\":","    # Simple CLI for testing","    import sys","    if len(sys.argv) != 2:","        print(\"Usage: python plateau.py <winners.json>\")","        sys.exit(1)","","    winners = Path(sys.argv[1])","    if not winners.exists():","        print(f\"File not found: {winners}\")","        sys.exit(1)","","    report = identify_plateau_from_winners(winners)","    save_plateau_report(report, winners.parent)"]}
{"type":"file_footer","path":"src/research/plateau.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/research/registry.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3984,"sha256":"c41e8ad863b399d839bc0233383d521a0f1b5967d21cfe3e944b176bb52e823d","total_lines":122,"chunk_count":1}
{"type":"file_chunk","path":"src/research/registry.py","chunk_index":0,"line_start":1,"line_end":122,"content":["","\"\"\"Result Registry - scan outputs and build research index.","","Phase 9: Scan outputs/ directory and create canonical_results.json and research_index.json.","\"\"\"","","from __future__ import annotations","","import json","from pathlib import Path","from typing import Any, Dict, List","","from research.decision import load_decisions","from research.extract import extract_canonical_metrics, ExtractionError","","","def build_research_index(outputs_root: Path, out_dir: Path) -> Path:","    \"\"\"","    Build research index from scanned outputs.","    ","    Scans outputs/seasons/{season}/runs/{run_id}/ and extracts canonical metrics.","    Outputs two files:","    - canonical_results.json: List of all CanonicalMetrics as dicts","    - research_index.json: Sorted lightweight index with run_id, score_final, decision, keys","    ","    Sorting rules (fixed):","    1. score_final desc","    2. score_net_mdd desc","    3. trades desc","    ","    Args:","        outputs_root: Root outputs directory (e.g., Path(\"outputs\"))","        out_dir: Output directory for research artifacts (e.g., Path(\"outputs/research\"))","        ","    Returns:","        Path to research_index.json","    \"\"\"","    out_dir.mkdir(parents=True, exist_ok=True)","    ","    # Scan all runs","    canonical_results = []","    seasons_dir = outputs_root / \"seasons\"","    ","    if seasons_dir.exists():","        for season_dir in seasons_dir.iterdir():","            if not season_dir.is_dir():","                continue","            ","            runs_dir = season_dir / \"runs\"","            if not runs_dir.exists():","                continue","            ","            # Scan runs","            for run_dir in runs_dir.iterdir():","                if not run_dir.is_dir():","                    continue","                ","                try:","                    metrics = extract_canonical_metrics(run_dir)","                    canonical_results.append(metrics.to_dict())","                except ExtractionError:","                    # Skip runs with missing artifacts","                    continue","    ","    # Write canonical_results.json (list of CanonicalMetrics as dict)","    canonical_path = out_dir / \"canonical_results.json\"","    with open(canonical_path, \"w\", encoding=\"utf-8\") as f:","        json.dump(canonical_results, f, indent=2, ensure_ascii=False, sort_keys=True)","    ","    # Load decisions (if any)","    decisions = load_decisions(out_dir)","    decision_map: Dict[str, str] = {}","    for decision_entry in decisions:","        run_id = decision_entry.get(\"run_id\")","        decision = decision_entry.get(\"decision\")","        if run_id and decision:","            # Last-write-wins: later entries overwrite earlier ones","            decision_map[run_id] = decision","    ","    # Build lightweight index with sorting","    index_entries = []","    for result in canonical_results:","        run_id = result.get(\"run_id\")","        if not run_id:","            continue","        ","        entry = {","            \"run_id\": run_id,","            \"score_final\": result.get(\"score_final\", 0.0),","            \"score_net_mdd\": result.get(\"score_net_mdd\", 0.0),","            \"trades\": result.get(\"trades\", 0),","            \"decision\": decision_map.get(run_id, \"UNDECIDED\"),","            \"keys\": {","                \"portfolio_id\": result.get(\"portfolio_id\"),","                \"strategy_id\": result.get(\"strategy_id\"),","                \"symbol\": result.get(\"symbol\"),","            },","        }","        index_entries.append(entry)","    ","    # Sort: score_final desc, then score_net_mdd desc, then trades desc","    index_entries.sort(","        key=lambda x: (","            -x[\"score_final\"],  # Negative for descending","            -x[\"score_net_mdd\"],","            -x[\"trades\"],","        )","    )","    ","    # Write research_index.json","    index_data = {","        \"entries\": index_entries,","        \"total_runs\": len(index_entries),","    }","    ","    index_path = out_dir / \"research_index.json\"","    with open(index_path, \"w\", encoding=\"utf-8\") as f:","        json.dump(index_data, f, indent=2, ensure_ascii=False, sort_keys=True)","    ","    return index_path","",""]}
{"type":"file_footer","path":"src/research/registry.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/stage0/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":335,"sha256":"24b4597b0a4a5c619073212095f818e9b9f677a1b89e00f119f01d156e08236f","total_lines":16,"chunk_count":1}
{"type":"file_chunk","path":"src/stage0/__init__.py","chunk_index":0,"line_start":1,"line_end":16,"content":["","\"\"\"","Stage 0 Funnel (Vector/Proxy Filter)","","Design goal:","  - Extremely cheap scoring/ranking for massive parameter grids.","  - No matcher, no orders, no fills, no state machine.","  - Must be vectorizable / nopython friendly.","\"\"\"","","from .ma_proxy import stage0_score_ma_proxy","from .proxies import trend_proxy, vol_proxy, activity_proxy","","","",""]}
{"type":"file_footer","path":"src/stage0/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/stage0/ma_proxy.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6151,"sha256":"5fa1c882ec14696f74f2eb7fccfbe044ba523e7feffa5b3a5c505e4b19c2d40b","total_lines":203,"chunk_count":2}
{"type":"file_chunk","path":"src/stage0/ma_proxy.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","from __future__ import annotations","","\"\"\"","Stage 0 v0: MA Directional Efficiency Proxy","","This module intentionally does NOT depend on:","  - engine/* (matcher, fills, intents)","  - strategy/kernel","  - pipeline/runner_grid","","It is a cheap scoring function to rank massive parameter grids before Stage 2.","","Proxy idea (directional efficiency):","  dir[t] = sign(SMA_fast[t] - SMA_slow[t])","  ret[t] = close[t] - close[t-1]","  score = sum(dir[t] * ret[t]) / (std(ret) + eps)","","Notes:","  - This is NOT a backtest. No orders, no fills, no costs.","  - Recall > precision. False negatives are acceptable at Stage 0.","\"\"\"","","from typing import Tuple","","import numpy as np","import os","","try:","    import numba as nb","except Exception:  # pragma: no cover","    nb = None  # type: ignore","","","def _validate_inputs(close: np.ndarray, params_matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:","    \"\"\"","    Validate and normalize inputs for Stage0 proxy scoring.","    ","    Accepts float32 or float64, but converts to float32 for Stage0 optimization.","    \"\"\"","    from config.dtypes import PRICE_DTYPE_STAGE0","    ","    c = np.asarray(close, dtype=PRICE_DTYPE_STAGE0)","    if c.ndim != 1:","        raise ValueError(\"close must be 1D\")","    pm = np.asarray(params_matrix, dtype=PRICE_DTYPE_STAGE0)","    if pm.ndim != 2:","        raise ValueError(\"params_matrix must be 2D\")","    if pm.shape[1] < 2:","        raise ValueError(\"params_matrix must have at least 2 columns: fast, slow\")","    if c.shape[0] < 3:","        raise ValueError(\"close must have at least 3 bars for Stage0 scoring\")","    if not c.flags[\"C_CONTIGUOUS\"]:","        c = np.ascontiguousarray(c, dtype=PRICE_DTYPE_STAGE0)","    if not pm.flags[\"C_CONTIGUOUS\"]:","        pm = np.ascontiguousarray(pm, dtype=PRICE_DTYPE_STAGE0)","    return c, pm","","","def stage0_score_ma_proxy(close: np.ndarray, params_matrix: np.ndarray) -> np.ndarray:","    \"\"\"","    Compute Stage 0 proxy scores for a parameter matrix.","","    Args:","        close: float32 or float64 1D array (n_bars,) - will be converted to float32","        params_matrix: float32 or float64 2D array (n_params, >=2) - will be converted to float32","            - col0: fast_len","            - col1: slow_len","            - additional columns allowed and ignored by v0","","    Returns:","        scores: float64 1D array (n_params,) where higher is better","    \"\"\"","    c, pm = _validate_inputs(close, params_matrix)","","    # If numba is available and JIT is not disabled, use nopython kernel.","    if nb is not None and os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() != \"1\":","        return _stage0_kernel(c, pm)","","    # Fallback: pure numpy/python (correctness only, not intended for scale).","    ret = c[1:] - c[:-1]","    denom = np.std(ret) + 1e-12","    scores = np.empty(pm.shape[0], dtype=np.float64)","    for i in range(pm.shape[0]):","        fast = int(pm[i, 0])","        slow = int(pm[i, 1])","        if fast <= 0 or slow <= 0 or fast >= c.shape[0] or slow >= c.shape[0]:","            scores[i] = -np.inf","            continue","        f = _sma_py(c, fast)","        s = _sma_py(c, slow)","        # Skip NaN warmup region: SMA length L is valid from index (L-1) onward.","        # Here we conservatively start at max(fast, slow) to ensure both are non-NaN.","        start = max(fast, slow)","        acc = 0.0","        for t in range(start, c.shape[0]):","            d = np.sign(f[t] - s[t])","            acc += d * ret[t - 1]","        scores[i] = acc / denom","    return scores","","","def _sma_py(x: np.ndarray, length: int) -> np.ndarray:","    n = x.shape[0]","    out = np.full(n, np.nan, dtype=np.float64)","    if length <= 0:","        return out","    csum = np.cumsum(x, dtype=np.float64)","    for i in range(n):","        j = i - length + 1","        if j < 0:","            continue","        total = csum[i] - (csum[j - 1] if j > 0 else 0.0)","        out[i] = total / float(length)","    return out","","","if nb is not None:","","    @nb.njit(cache=False)","    def _sma_nb(x: np.ndarray, length: int) -> np.ndarray:","        n = x.shape[0]","        out = np.empty(n, dtype=np.float64)","        for i in range(n):","            out[i] = np.nan","        if length <= 0:","            return out","        csum = np.empty(n, dtype=np.float64)","        acc = 0.0","        for i in range(n):","            acc += float(x[i])","            csum[i] = acc","        for i in range(n):","            j = i - length + 1","            if j < 0:","                continue","            total = csum[i] - (csum[j - 1] if j > 0 else 0.0)","            out[i] = total / float(length)","        return out","","    @nb.njit(cache=False)","    def _sign_nb(v: float) -> float:","        if v > 0.0:","            return 1.0","        if v < 0.0:","            return -1.0","        return 0.0","","    @nb.njit(cache=False)","    def _std_nb(x: np.ndarray) -> float:","        # simple two-pass std for stability","        n = x.shape[0]","        if n <= 1:","            return 0.0","        mu = 0.0","        for i in range(n):","            mu += float(x[i])","        mu /= float(n)","        var = 0.0","        for i in range(n):","            d = float(x[i]) - mu","            var += d * d","        var /= float(n)","        return np.sqrt(var)","","    @nb.njit(cache=False)","    def _stage0_kernel(close: np.ndarray, params_matrix: np.ndarray) -> np.ndarray:","        n = close.shape[0]","        n_params = params_matrix.shape[0]","","        # ret[t] = close[t] - close[t-1] for t in [1..n-1]","        ret = np.empty(n - 1, dtype=np.float64)","        for t in range(1, n):","            ret[t - 1] = float(close[t]) - float(close[t - 1])","","        denom = _std_nb(ret) + 1e-12","        scores = np.empty(n_params, dtype=np.float64)","","        for i in range(n_params):","            fast = int(params_matrix[i, 0])","            slow = int(params_matrix[i, 1])","","            # invalid lengths => hard reject","            if fast <= 0 or slow <= 0 or fast >= n or slow >= n:","                scores[i] = -np.inf","                continue","","            f = _sma_nb(close, fast)","            s = _sma_nb(close, slow)","","            start = fast if fast > slow else slow","            acc = 0.0","            for t in range(start, n):","                d = _sign_nb(f[t] - s[t])","                acc += d * ret[t - 1]","","            scores[i] = acc / denom","","        return scores",""]}
{"type":"file_chunk","path":"src/stage0/ma_proxy.py","chunk_index":1,"line_start":201,"line_end":203,"content":["","",""]}
{"type":"file_footer","path":"src/stage0/ma_proxy.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/stage0/proxies.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":19230,"sha256":"c9a0ac9ea5a2415f4b64ffef34419d2ecd51192424c2e1cb0a04b0eb7301152c","total_lines":617,"chunk_count":4}
{"type":"file_chunk","path":"src/stage0/proxies.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","from __future__ import annotations","","\"\"\"","Stage 0 v1 Trinity: Trend + Volatility + Activity Proxies","","This module provides three proxy scoring functions for ranking parameter grids","before full backtest (Stage 2). These are NOT backtests - they are cheap heuristics.","","Proxy Contract:","  - Stage0 is ranking proxy, NOT equal to backtest","  - NaN/warmup rules: start = max(required_lookbacks)","  - Correlation contract: Spearman ρ ≥ 0.4 (enforced by tests)","","Design:","  - All proxies return float64 (n_params,) scores where higher is better","  - Input: OHLC arrays (np.ndarray), params: float64 2D array (n_params, k)","  - Must provide *_py (pure Python) and *_nb (Numba njit) versions","  - Wrapper functions select nb/py based on NUMBA_DISABLE_JIT kill-switch","\"\"\"","","from typing import Tuple","","import numpy as np","import os","","try:","    import numba as nb","except Exception:  # pragma: no cover","    nb = None  # type: ignore","","from indicators.numba_indicators import atr_wilder","","","def _validate_inputs(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,",") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:","    \"\"\"Validate and ensure contiguous arrays.\"\"\"","    o = np.asarray(open_, dtype=np.float64)","    h = np.asarray(high, dtype=np.float64)","    l = np.asarray(low, dtype=np.float64)","    c = np.asarray(close, dtype=np.float64)","    pm = np.asarray(params_matrix, dtype=np.float64)","","    if o.ndim != 1 or h.ndim != 1 or l.ndim != 1 or c.ndim != 1:","        raise ValueError(\"OHLC arrays must be 1D\")","    if pm.ndim != 2:","        raise ValueError(\"params_matrix must be 2D\")","    if not (o.shape[0] == h.shape[0] == l.shape[0] == c.shape[0]):","        raise ValueError(\"OHLC arrays must have same length\")","","    if not o.flags[\"C_CONTIGUOUS\"]:","        o = np.ascontiguousarray(o)","    if not h.flags[\"C_CONTIGUOUS\"]:","        h = np.ascontiguousarray(h)","    if not l.flags[\"C_CONTIGUOUS\"]:","        l = np.ascontiguousarray(l)","    if not c.flags[\"C_CONTIGUOUS\"]:","        c = np.ascontiguousarray(c)","    if not pm.flags[\"C_CONTIGUOUS\"]:","        pm = np.ascontiguousarray(pm)","","    return o, h, l, c, pm","","","# ============================================================================","# Proxy #1: Trend Proxy (MA / slope)","# ============================================================================","","","def trend_proxy_py(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,",") -> np.ndarray:","    \"\"\"","    Trend proxy: mean(sign(sma_fast - sma_slow)) or mean((sma_fast - sma_slow) / close)","","    Args:","        open_, high, low, close: float64 1D arrays (n_bars,)","        params_matrix: float64 2D array (n_params, >=2)","            - col0: fast_len","            - col1: slow_len","","    Returns:","        scores: float64 1D array (n_params,)","    \"\"\"","    o, h, l, c, pm = _validate_inputs(open_, high, low, close, params_matrix)","    n = c.shape[0]","    n_params = pm.shape[0]","","    if pm.shape[1] < 2:","        raise ValueError(\"params_matrix must have at least 2 columns: fast_len, slow_len\")","","    scores = np.empty(n_params, dtype=np.float64)","","    for i in range(n_params):","        fast = int(pm[i, 0])","        slow = int(pm[i, 1])","","        # Invalid params: return -inf","        if fast <= 0 or slow <= 0 or fast >= n or slow >= n:","            scores[i] = -np.inf","            continue","","        # Compute SMAs","        sma_fast = _sma_py(c, fast)","        sma_slow = _sma_py(c, slow)","","        # Warmup: start at max(fast, slow)","        start = max(fast, slow)","        if start >= n:","            scores[i] = -np.inf","            continue","","        # Compute trend score: mean((sma_fast - sma_slow) / close)","        acc = 0.0","        count = 0","        for t in range(start, n):","            diff = sma_fast[t] - sma_slow[t]","            if not np.isnan(diff) and c[t] > 0:","                acc += diff / c[t]","                count += 1","","        if count == 0:","            scores[i] = -np.inf","        else:","            scores[i] = acc / count","","    return scores","","","def trend_proxy_nb(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,",") -> np.ndarray:","    \"\"\"Numba version of trend_proxy.\"\"\"","    if nb is None:  # pragma: no cover","        raise RuntimeError(\"numba not available\")","    return _trend_proxy_kernel(open_, high, low, close, params_matrix)","","","def trend_proxy(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,",") -> np.ndarray:","    \"\"\"Wrapper: select nb/py based on NUMBA_DISABLE_JIT.\"\"\"","    if nb is not None and os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() != \"1\":","        return trend_proxy_nb(open_, high, low, close, params_matrix)","    return trend_proxy_py(open_, high, low, close, params_matrix)","","","# ============================================================================","# Proxy #2: Volatility Proxy (ATR / Range)","# ============================================================================","","","def vol_proxy_py(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,",") -> np.ndarray:","    \"\"\"","    Volatility proxy: effective stop distance = ATR(atr_len) * stop_mult.","    ","    Score prefers moderate stop distance (avoids extremely tiny or huge stops).","","    Args:","        open_, high, low, close: float64 1D arrays (n_bars,)","        params_matrix: float64 2D array (n_params, >=2)","            - col0: atr_len","            - col1: stop_mult","","    Returns:","        scores: float64 1D array (n_params,)","    \"\"\"","    o, h, l, c, pm = _validate_inputs(open_, high, low, close, params_matrix)","    n = c.shape[0]","    n_params = pm.shape[0]","","    if pm.shape[1] < 2:","        raise ValueError(\"params_matrix must have at least 2 columns: atr_len, stop_mult\")","","    scores = np.empty(n_params, dtype=np.float64)","","    for i in range(n_params):"]}
{"type":"file_chunk","path":"src/stage0/proxies.py","chunk_index":1,"line_start":201,"line_end":400,"content":["        atr_len = int(pm[i, 0])","        stop_mult = float(pm[i, 1])","","        # Invalid params: return -inf","        if atr_len <= 0 or atr_len >= n or stop_mult <= 0.0:","            scores[i] = -np.inf","            continue","","        # Compute ATR using Wilder's method","        atr = atr_wilder(h, l, c, atr_len)","","        # Warmup: start at atr_len","        start = max(atr_len, 1)","        if start >= n:","            scores[i] = -np.inf","            continue","","        # Compute stop distance: ATR * stop_mult","        stop_dist_sum = 0.0","        stop_dist_count = 0","        for t in range(start, n):","            if not np.isnan(atr[t]) and atr[t] > 0:","                stop_dist = atr[t] * stop_mult","                stop_dist_sum += stop_dist","                stop_dist_count += 1","","        if stop_dist_count == 0:","            scores[i] = -np.inf","        else:","            stop_dist_mean = stop_dist_sum / float(stop_dist_count)","            # Score: -log1p(stop_mean) - penalize larger stops; deterministic; no target/median","            scores[i] = -np.log1p(stop_dist_mean)","","    return scores","","","def vol_proxy_nb(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,",") -> np.ndarray:","    \"\"\"Numba version of vol_proxy.\"\"\"","    if nb is None:  # pragma: no cover","        raise RuntimeError(\"numba not available\")","    return _vol_proxy_kernel(open_, high, low, close, params_matrix)","","","def vol_proxy(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,",") -> np.ndarray:","    \"\"\"Wrapper: select nb/py based on NUMBA_DISABLE_JIT.\"\"\"","    if nb is not None and os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() != \"1\":","        return vol_proxy_nb(open_, high, low, close, params_matrix)","    return vol_proxy_py(open_, high, low, close, params_matrix)","","","# ============================================================================","# Proxy #3: Activity Proxy (Trade Count / trigger density)","# ============================================================================","","","def activity_proxy_py(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,",") -> np.ndarray:","    \"\"\"","    Activity proxy: channel breakout trigger count.","    ","    Counts crossings where close[t-1] <= channel_hi[t-1] and close[t] > channel_hi[t].","    Aligned with Stage2 kernel which uses channel breakout entry.","","    Args:","        open_, high, low, close: float64 1D arrays (n_bars,)","        params_matrix: float64 2D array (n_params, >=1)","            - col0: channel_len","            - col1: atr_len (not used, kept for compatibility)","","    Returns:","        scores: float64 1D array (n_params,)","    \"\"\"","    o, h, l, c, pm = _validate_inputs(open_, high, low, close, params_matrix)","    n = c.shape[0]","    n_params = pm.shape[0]","","    if pm.shape[1] < 1:","        raise ValueError(\"params_matrix must have at least 1 column: channel_len\")","","    scores = np.empty(n_params, dtype=np.float64)","","    for i in range(n_params):","        channel_len = int(pm[i, 0])","","        # Invalid params: return -inf","        if channel_len <= 0 or channel_len >= n:","            scores[i] = -np.inf","            continue","","        # Compute channel_hi = rolling_max(high, channel_len)","        channel_hi = np.full(n, np.nan, dtype=np.float64)","        for t in range(n):","            start_idx = max(0, t - channel_len + 1)","            window_high = h[start_idx : t + 1]","            if window_high.size > 0:","                channel_hi[t] = np.max(window_high)","","        # Warmup: start at channel_len","        start = channel_len","        if start >= n - 1:","            scores[i] = -np.inf","            continue","","        # Count breakout triggers: high[t] > ch[t-1] AND high[t-1] <= ch[t-1]","        # Compare to previous channel high to avoid equality lock","        # Start from start+1 to ensure we have t-1 available","        triggers = 0","        for t in range(start + 1, n):","            if np.isnan(channel_hi[t-1]):","                continue","            # Trigger when high crosses above previous channel high","            if high[t] > channel_hi[t-1] and high[t-1] <= channel_hi[t-1]:","                triggers += 1","","        n_effective = n - start","        if n_effective == 0:","            scores[i] = -np.inf","        else:","            # Activity score: raw count of triggers (or triggers per bar)","            # Using raw count for simplicity and robustness","            scores[i] = float(triggers)","","    return scores","","","def activity_proxy_nb(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,",") -> np.ndarray:","    \"\"\"Numba version of activity_proxy.\"\"\"","    if nb is None:  # pragma: no cover","        raise RuntimeError(\"numba not available\")","    return _activity_proxy_kernel(open_, high, low, close, params_matrix)","","","def activity_proxy(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params_matrix: np.ndarray,",") -> np.ndarray:","    \"\"\"Wrapper: select nb/py based on NUMBA_DISABLE_JIT.\"\"\"","    if nb is not None and os.environ.get(\"NUMBA_DISABLE_JIT\", \"\").strip() != \"1\":","        return activity_proxy_nb(open_, high, low, close, params_matrix)","    return activity_proxy_py(open_, high, low, close, params_matrix)","","","# ============================================================================","# Helper functions (SMA)","# ============================================================================","","","def _sma_py(x: np.ndarray, length: int) -> np.ndarray:","    \"\"\"Simple Moving Average (pure Python).\"\"\"","    n = x.shape[0]","    out = np.full(n, np.nan, dtype=np.float64)","    if length <= 0:","        return out","    csum = np.cumsum(x, dtype=np.float64)","    for i in range(n):","        j = i - length + 1","        if j < 0:","            continue","        total = csum[i] - (csum[j - 1] if j > 0 else 0.0)","        out[i] = total / float(length)","    return out","","","# ============================================================================","# Numba kernels","# ============================================================================","","if nb is not None:","","    @nb.njit(cache=False)","    def _sma_nb(x: np.ndarray, length: int) -> np.ndarray:","        \"\"\"Simple Moving Average (Numba).\"\"\"","        n = x.shape[0]","        out = np.empty(n, dtype=np.float64)"]}
{"type":"file_chunk","path":"src/stage0/proxies.py","chunk_index":2,"line_start":401,"line_end":600,"content":["        for i in range(n):","            out[i] = np.nan","        if length <= 0:","            return out","        csum = np.empty(n, dtype=np.float64)","        acc = 0.0","        for i in range(n):","            acc += float(x[i])","            csum[i] = acc","        for i in range(n):","            j = i - length + 1","            if j < 0:","                continue","            total = csum[i] - (csum[j - 1] if j > 0 else 0.0)","            out[i] = total / float(length)","        return out","","    @nb.njit(cache=False)","    def _trend_proxy_kernel(","        open_: np.ndarray,","        high: np.ndarray,","        low: np.ndarray,","        close: np.ndarray,","        params_matrix: np.ndarray,","    ) -> np.ndarray:","        \"\"\"Numba kernel for trend proxy.\"\"\"","        n = close.shape[0]","        n_params = params_matrix.shape[0]","        scores = np.empty(n_params, dtype=np.float64)","","        for i in range(n_params):","            fast = int(params_matrix[i, 0])","            slow = int(params_matrix[i, 1])","","            if fast <= 0 or slow <= 0 or fast >= n or slow >= n:","                scores[i] = -np.inf","                continue","","            sma_fast = _sma_nb(close, fast)","            sma_slow = _sma_nb(close, slow)","","            start = fast if fast > slow else slow","            if start >= n:","                scores[i] = -np.inf","                continue","","            acc = 0.0","            count = 0","            for t in range(start, n):","                diff = sma_fast[t] - sma_slow[t]","                if not np.isnan(diff) and close[t] > 0.0:","                    acc += diff / close[t]","                    count += 1","","            if count == 0:","                scores[i] = -np.inf","            else:","                scores[i] = acc / float(count)","","        return scores","","    @nb.njit(cache=False)","    def _atr_wilder_nb(high: np.ndarray, low: np.ndarray, close: np.ndarray, window: int) -> np.ndarray:","        \"\"\"ATR Wilder (Numba version, inline for njit compatibility).\"\"\"","        n = high.shape[0]","        out = np.empty(n, dtype=np.float64)","        for i in range(n):","            out[i] = np.nan","","        if window <= 0 or n == 0 or window > n:","            return out","","        tr = np.empty(n, dtype=np.float64)","        tr[0] = high[0] - low[0]","        for i in range(1, n):","            a = high[i] - low[i]","            b = abs(high[i] - close[i - 1])","            c = abs(low[i] - close[i - 1])","            tr[i] = a if a >= b and a >= c else (b if b >= c else c)","","        s = 0.0","        end = window if window < n else n","        for i in range(end):","            s += tr[i]","        out[end - 1] = s / float(window)","","        for i in range(window, n):","            out[i] = (out[i - 1] * float(window - 1) + tr[i]) / float(window)","","        return out","","    @nb.njit(cache=False)","    def _rolling_max_nb(arr: np.ndarray, window: int) -> np.ndarray:","        \"\"\"Rolling maximum (Numba, inline for njit compatibility).\"\"\"","        n = arr.shape[0]","        out = np.empty(n, dtype=np.float64)","        for i in range(n):","            out[i] = np.nan","        if window <= 0:","            return out","        for i in range(n):","            start = i - window + 1","            if start < 0:","                start = 0","            m = arr[start]","            for j in range(start + 1, i + 1):","                v = arr[j]","                if v > m:","                    m = v","            out[i] = m","        return out","","    @nb.njit(cache=False)","    def _vol_proxy_kernel(","        open_: np.ndarray,","        high: np.ndarray,","        low: np.ndarray,","        close: np.ndarray,","        params_matrix: np.ndarray,","    ) -> np.ndarray:","        \"\"\"Numba kernel for vol proxy with stop_mult.\"\"\"","        n = close.shape[0]","        n_params = params_matrix.shape[0]","        scores = np.empty(n_params, dtype=np.float64)","","        for i in range(n_params):","            atr_len = int(params_matrix[i, 0])","            stop_mult = float(params_matrix[i, 1])","","            if atr_len <= 0 or atr_len >= n or stop_mult <= 0.0:","                scores[i] = -np.inf","                continue","","            atr = _atr_wilder_nb(high, low, close, atr_len)","","            start = atr_len if atr_len > 1 else 1","            if start >= n:","                scores[i] = -np.inf","                continue","","            # Compute stop distance: ATR * stop_mult","            stop_dist_sum = 0.0","            stop_dist_count = 0","            for t in range(start, n):","                if not np.isnan(atr[t]) and atr[t] > 0.0:","                    stop_dist = atr[t] * stop_mult","                    stop_dist_sum += stop_dist","                    stop_dist_count += 1","","            if stop_dist_count == 0:","                scores[i] = -np.inf","            else:","                stop_dist_mean = stop_dist_sum / float(stop_dist_count)","                # Score: -log1p(stop_mean) - penalize larger stops; deterministic; no target/median","                scores[i] = -np.log1p(stop_dist_mean)","","        return scores","","    @nb.njit(cache=False)","    def _sign_nb(v: float) -> float:","        \"\"\"Sign function (Numba).\"\"\"","        if v > 0.0:","            return 1.0","        if v < 0.0:","            return -1.0","        return 0.0","","    @nb.njit(cache=False)","    def _activity_proxy_kernel(","        open_: np.ndarray,","        high: np.ndarray,","        low: np.ndarray,","        close: np.ndarray,","        params_matrix: np.ndarray,","    ) -> np.ndarray:","        \"\"\"Numba kernel for activity proxy: channel breakout triggers.\"\"\"","        n = close.shape[0]","        n_params = params_matrix.shape[0]","        scores = np.empty(n_params, dtype=np.float64)","","        for i in range(n_params):","            channel_len = int(params_matrix[i, 0])","","            if channel_len <= 0 or channel_len >= n:","                scores[i] = -np.inf","                continue","","            # Compute channel_hi = rolling_max(high, channel_len)","            channel_hi = _rolling_max_nb(high, channel_len)","","            start = channel_len","            if start >= n - 1:","                scores[i] = -np.inf","                continue","","            # Count breakout triggers: high[t] > ch[t-1] AND high[t-1] <= ch[t-1]","            # Compare to previous channel high to avoid equality lock","            # Start from start+1 to ensure we have t-1 available","            triggers = 0","            for t in range(start + 1, n):"]}
{"type":"file_chunk","path":"src/stage0/proxies.py","chunk_index":3,"line_start":601,"line_end":617,"content":["                if np.isnan(channel_hi[t-1]):","                    continue","                # Trigger when high crosses above previous channel high","                if high[t] > channel_hi[t-1] and high[t-1] <= channel_hi[t-1]:","                    triggers += 1","","            n_effective = n - start","            if n_effective == 0:","                scores[i] = -np.inf","            else:","                # Activity score: raw count of triggers (or triggers per bar)","                # Using raw count for simplicity and robustness","                scores[i] = float(triggers)","","        return scores","",""]}
{"type":"file_footer","path":"src/stage0/proxies.py","complete":true,"emitted_chunks":4}
{"type":"file_header","path":"src/strategy/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":443,"sha256":"c94dc91aebfae44a71d1f19ce829d5a186d5ad1a8c34842ae5088923ba2f0f42","total_lines":26,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/__init__.py","chunk_index":0,"line_start":1,"line_end":26,"content":["","\"\"\"Strategy system.","","Phase 7: Strategy registry, runner, and built-in strategies.","\"\"\"","","from strategy.registry import (","    register,","    get,","    list_strategies,","    load_builtin_strategies,",")","from strategy.runner import run_strategy","from strategy.spec import StrategySpec, StrategyFn","","__all__ = [","    \"register\",","    \"get\",","    \"list_strategies\",","    \"load_builtin_strategies\",","    \"run_strategy\",","    \"StrategySpec\",","    \"StrategyFn\",","]","",""]}
{"type":"file_footer","path":"src/strategy/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/strategy/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":570,"sha256":"db9ef4d2cda8186043b5970aaa525eed5fdf30b9a89fd7ad5e6979b74c764a2c","note":"skipped by policy"}
{"type":"file_skipped","path":"src/strategy/__pycache__/identity_models.cpython-312.pyc","reason":"cache","bytes":14731,"sha256":"477b48d6bbecda718a138541ff1d85f9e04f0c26e8fd46cd5f5e8aa6ba13fe2e","note":"skipped by policy"}
{"type":"file_skipped","path":"src/strategy/__pycache__/param_schema.cpython-312.pyc","reason":"cache","bytes":1896,"sha256":"7ce84d3eda2953d2306678c55e5f57176db7a27cf9b982d96349c6b47ed1b440","note":"skipped by policy"}
{"type":"file_skipped","path":"src/strategy/__pycache__/registry.cpython-312.pyc","reason":"cache","bytes":11851,"sha256":"d4cdf3beb7980553f6a79450f27db01b9b208f92305bfc1f986422f5321d1e88","note":"skipped by policy"}
{"type":"file_skipped","path":"src/strategy/__pycache__/runner.cpython-312.pyc","reason":"cache","bytes":3940,"sha256":"3e7447c3d9c759671b142d438586c35f9d4f8bf644f899d47d090bb7581d66e5","note":"skipped by policy"}
{"type":"file_skipped","path":"src/strategy/__pycache__/spec.cpython-312.pyc","reason":"cache","bytes":6359,"sha256":"9912e475150268635faa91c0063c7234b41737a0a344788cfd90b2c24b217f00","note":"skipped by policy"}
{"type":"file_header","path":"src/strategy/builder_sparse.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6956,"sha256":"928899b85ea33125569bad7b6464c90de8ee07bc76ffd991b7a43fd4ee8f0570","total_lines":175,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/builder_sparse.py","chunk_index":0,"line_start":1,"line_end":175,"content":["\"\"\"","Sparse Intent Builder (P2-3)","","Provides sparse intent generation with trigger rate control for performance testing.","Supports both sparse (default) and dense (reference) modes.","\"\"\"","from __future__ import annotations","","from typing import Dict","","import numpy as np","","from config.dtypes import (","    INDEX_DTYPE,","    INTENT_ENUM_DTYPE,","    INTENT_PRICE_DTYPE,",")","from engine.constants import KIND_STOP, ROLE_ENTRY, SIDE_BUY","","","def build_intents_sparse(","    donch_prev: np.ndarray,","    channel_len: int,","    order_qty: int,","    trigger_rate: float = 1.0,","    seed: int = 42,","    use_dense: bool = False,",") -> Dict[str, object]:","    \"\"\"","    Build entry intents from trigger array with sparse masking support.","    ","    This is the main sparse builder that supports trigger rate control for performance testing.","    When trigger_rate < 1.0, it deterministically selects a subset of valid triggers.","    ","    Args:","        donch_prev: float64 array (n_bars,) - shifted donchian high (donch_prev[0]=NaN, donch_prev[1:]=donch_hi[:-1])","        channel_len: warmup period (same as indicator warmup)","        order_qty: order quantity","        trigger_rate: Rate of triggers to keep (0.0 to 1.0). Default 1.0 (all triggers).","        seed: Random seed for deterministic trigger selection. Default 42.","        use_dense: If True, use dense builder (reference implementation). Default False (sparse).","    ","    Returns:","        dict with:","            - created_bar: int32 array (n_entry,) - created bar indices","            - price: float64 array (n_entry,) - entry prices","            - order_id: int32 array (n_entry,) - order IDs","            - role: uint8 array (n_entry,) - role (ENTRY)","            - kind: uint8 array (n_entry,) - kind (STOP)","            - side: uint8 array (n_entry,) - side (BUY)","            - qty: int32 array (n_entry,) - quantities","            - n_entry: int - number of entry intents","            - obs: dict - diagnostic observations (includes allowed_bars, intents_generated)","    \"\"\"","    n = int(donch_prev.shape[0])","    warmup = channel_len","    ","    # Create index array for bars 1..n-1 (bar indices t, where created_bar = t-1)","    i = np.arange(1, n, dtype=INDEX_DTYPE)","    ","    # Valid bar mask: entries must be finite, positive, and past warmup","    valid_bar_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)","    ","    # CURSOR TASK 1: Generate bar_allow mask based on trigger_rate","    # rate <= 0.0 → 全 False","    # rate >= 1.0 → 全 True","    # else → rng.random(n_bars) < rate","    if use_dense or trigger_rate >= 1.0:","        # Dense mode or full rate: all bars allowed","        bar_allow = np.ones(n - 1, dtype=bool)  # n-1 because we skip first bar","    elif trigger_rate <= 0.0:","        # Zero rate: no bars allowed","        bar_allow = np.zeros(n - 1, dtype=bool)","    else:","        # Sparse mode: deterministically select bars based on trigger_rate","        rng = np.random.default_rng(seed)","        random_vals = rng.random(n - 1)  # Random values for bars 1..n-1","        bar_allow = random_vals < trigger_rate","    ","    # Combine valid_bar_mask with bar_allow to get final allow_mask","    allow_mask = valid_bar_mask & bar_allow","    ","    # Count valid bars (before trigger rate filtering) - this is the baseline","    valid_bars_count = int(np.sum(valid_bar_mask))","    ","    # Count allowed bars (after intent sparse filtering) - this is what actually gets intents","    allowed_bars_after_sparse = int(np.sum(allow_mask))","    ","    # Get indices of allowed entries (flatnonzero returns indices into donch_prev[1:])","    idx_selected = np.flatnonzero(allow_mask).astype(INDEX_DTYPE)","    intents_generated = allowed_bars_after_sparse","    n_entry = int(idx_selected.shape[0])","    ","    # CURSOR TASK 2: entry_valid_mask_sum must be sum(allow_mask) (after intent sparse)","    # Diagnostic observations","    obs = {","        \"n_bars\": n,","        \"warmup\": warmup,","        \"valid_mask_sum\": valid_bars_count,  # Dense valid bars (before trigger rate)","        \"entry_valid_mask_sum\": allowed_bars_after_sparse,  # CURSOR TASK 2: After intent sparse (sum(allow_mask))","        \"allowed_bars\": valid_bars_count,  # Always equals valid_mask_sum (baseline, for comparison)","        \"intents_generated\": intents_generated,  # Actual intents generated (equals allowed_bars_after_sparse)","        \"trigger_rate_applied\": float(trigger_rate),","        \"builder_mode\": \"dense\" if use_dense else \"sparse\",","    }","    ","    if n_entry == 0:","        return {","            \"created_bar\": np.empty(0, dtype=INDEX_DTYPE),","            \"price\": np.empty(0, dtype=INTENT_PRICE_DTYPE),","            \"order_id\": np.empty(0, dtype=INDEX_DTYPE),","            \"role\": np.empty(0, dtype=INTENT_ENUM_DTYPE),","            \"kind\": np.empty(0, dtype=INTENT_ENUM_DTYPE),","            \"side\": np.empty(0, dtype=INTENT_ENUM_DTYPE),","            \"qty\": np.empty(0, dtype=INDEX_DTYPE),","            \"n_entry\": 0,","            \"obs\": obs,","        }","    ","    # Gather sparse entries (only for selected positions)","    # - idx_selected is index into donch_prev[1:], so bar index t = idx_selected + 1","    # - created_bar = t - 1 = idx_selected (since t = idx_selected + 1)","    # - price = donch_prev[t] = donch_prev[idx_selected + 1] = donch_prev[1:][idx_selected]","    created_bar = idx_selected.astype(INDEX_DTYPE)  # created_bar = t-1 = idx_selected","    price = donch_prev[1:][idx_selected].astype(INTENT_PRICE_DTYPE)  # Gather from donch_prev[1:]","    ","    # Order ID maintains deterministic ordering","    # Order ID is sequential (1, 2, 3, ...) based on created_bar order","    # Since created_bar is already sorted, this preserves deterministic ordering","    order_id = np.arange(1, n_entry + 1, dtype=INDEX_DTYPE)","    role = np.full(n_entry, ROLE_ENTRY, dtype=INTENT_ENUM_DTYPE)","    kind = np.full(n_entry, KIND_STOP, dtype=INTENT_ENUM_DTYPE)","    side = np.full(n_entry, SIDE_BUY, dtype=INTENT_ENUM_DTYPE)","    qty = np.full(n_entry, int(order_qty), dtype=INDEX_DTYPE)","    ","    return {","        \"created_bar\": created_bar,","        \"price\": price,","        \"order_id\": order_id,","        \"role\": role,","        \"kind\": kind,","        \"side\": side,","        \"qty\": qty,","        \"n_entry\": n_entry,","        \"obs\": obs,","    }","","","def build_intents_dense(","    donch_prev: np.ndarray,","    channel_len: int,","    order_qty: int,",") -> Dict[str, object]:","    \"\"\"","    Dense builder (reference implementation).","    ","    This is a wrapper around build_intents_sparse with use_dense=True for clarity.","    Use this when you need the reference dense behavior.","    ","    Args:","        donch_prev: float64 array (n_bars,) - shifted donchian high","        channel_len: warmup period","        order_qty: order quantity","    ","    Returns:","        Same format as build_intents_sparse (with all valid triggers).","    \"\"\"","    return build_intents_sparse(","        donch_prev=donch_prev,","        channel_len=channel_len,","        order_qty=order_qty,","        trigger_rate=1.0,","        seed=42,","        use_dense=True,","    )"]}
{"type":"file_footer","path":"src/strategy/builder_sparse.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/strategy/builtin/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":79,"sha256":"3a8015a97f5c1e6d0fb7c6aa16ce393c2618d6b33064c284722b85527ef46225","total_lines":7,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/builtin/__init__.py","chunk_index":0,"line_start":1,"line_end":7,"content":["","\"\"\"Built-in strategies.","","Phase 7: MVP strategies for system validation.","\"\"\"","",""]}
{"type":"file_footer","path":"src/strategy/builtin/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/strategy/builtin/atr_trailing_stop_v1.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":6111,"sha256":"293269456e0d95fe1f4f38379b2f0c2086d5133665430ee80b589b59843a5dc8","total_lines":180,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/builtin/atr_trailing_stop_v1.py","chunk_index":0,"line_start":1,"line_end":180,"content":["\"\"\"ATR Trailing Stop Strategy v1.","","Phase J: Trend following strategy using ATR-based trailing stop.","Entry: On trend confirmation, exit when price hits trailing stop.","\"\"\"","","from __future__ import annotations","","from typing import Dict, Any, Mapping","","import numpy as np","","from engine.types import OrderIntent, OrderRole, OrderKind, Side","from engine.order_id import generate_order_id","from engine.constants import ROLE_ENTRY, KIND_STOP, SIDE_BUY, SIDE_SELL","from strategy.spec import StrategySpec, StrategyFn","","","def atr_trailing_stop_strategy(","    context: Mapping[str, Any],","    params: Mapping[str, float],",") -> Dict[str, Any]:","    \"\"\"ATR Trailing Stop Strategy implementation.","    ","    Entry signal: Trend confirmation based on price crossing moving average.","    Uses ATR-based trailing stop for exit.","    ","    Args:","        context: Execution context with features and bar_index","        params: Strategy parameters (atr_period, atr_multiplier, ma_period)","        ","    Returns:","        Dict with \"intents\" (List[OrderIntent]) and \"debug\" (dict)","    \"\"\"","    features = context.get(\"features\", {})","    bar_index = context.get(\"bar_index\", 0)","    ","    # Get features","    close = features.get(\"close\")","    high = features.get(\"high\")","    low = features.get(\"low\")","    ","    if close is None or high is None or low is None:","        return {\"intents\": [], \"debug\": {\"error\": \"Missing price features\"}}","    ","    # Convert to numpy arrays if needed","    if not isinstance(close, np.ndarray):","        close = np.array(close)","    if not isinstance(high, np.ndarray):","        high = np.array(high)","    if not isinstance(low, np.ndarray):","        low = np.array(low)","    ","    # Check bounds","    if bar_index >= len(close) or bar_index >= len(high) or bar_index >= len(low):","        return {\"intents\": [], \"debug\": {\"error\": \"bar_index out of bounds\"}}","    ","    # Need at least 1 bar","    if bar_index < 0:","        return {\"intents\": [], \"debug\": {}}","    ","    # Get parameters","    atr_period = int(params.get(\"atr_period\", 14))","    atr_multiplier = params.get(\"atr_multiplier\", 2.0)","    ma_period = int(params.get(\"ma_period\", 20))","    ","    curr_close = close[bar_index]","    curr_high = high[bar_index]","    curr_low = low[bar_index]","    ","    # Calculate ATR (simplified for demo)","    # In a real implementation, this would use proper ATR feature","    lookback = min(max(atr_period, ma_period), bar_index + 1)","    if lookback > 1:","        # Calculate True Range","        prev_close = close[bar_index - 1] if bar_index > 0 else curr_close","        tr1 = curr_high - curr_low","        tr2 = abs(curr_high - prev_close)","        tr3 = abs(curr_low - prev_close)","        true_range = max(tr1, tr2, tr3)","        ","        # Simple ATR (using current true range only for demo)","        atr_value = true_range","        ","        # Calculate moving average","        recent_prices = close[bar_index - lookback + 1:bar_index + 1]","        ma_value = np.nanmean(recent_prices)","    else:","        atr_value = 0.0","        ma_value = curr_close","    ","    # Check for trend signals","    is_bullish_trend = curr_close > ma_value and not np.isnan(curr_close) and not np.isnan(ma_value)","    is_bearish_trend = curr_close < ma_value and not np.isnan(curr_close) and not np.isnan(ma_value)","    ","    intents = []","    if is_bullish_trend:","        # Entry: Buy Stop at current high + buffer (breakout entry)","        entry_price = float(curr_high + 0.5 * atr_value)  # Small buffer above high","        ","        order_id = generate_order_id(","            created_bar=bar_index,","            param_idx=0,","            role=ROLE_ENTRY,","            kind=KIND_STOP,","            side=SIDE_BUY,","        )","        ","        intent = OrderIntent(","            order_id=order_id,","            created_bar=bar_index,","            role=OrderRole.ENTRY,","            kind=OrderKind.STOP,","            side=Side.BUY,","            price=entry_price,","            qty=context.get(\"order_qty\", 1),","        )","        intents.append(intent)","        ","        # Also create trailing stop exit (simulated by separate logic)","        # In a real implementation, this would be managed by position tracking","        ","    elif is_bearish_trend:","        # Entry: Sell Stop at current low - buffer (breakdown entry)","        entry_price = float(curr_low - 0.5 * atr_value)  # Small buffer below low","        ","        order_id = generate_order_id(","            created_bar=bar_index,","            param_idx=0,","            role=ROLE_ENTRY,","            kind=KIND_STOP,","            side=SIDE_SELL,","        )","        ","        intent = OrderIntent(","            order_id=order_id,","            created_bar=bar_index,","            role=OrderRole.ENTRY,","            kind=OrderKind.STOP,","            side=Side.SELL,","            price=entry_price,","            qty=context.get(\"order_qty\", 1),","        )","        intents.append(intent)","    ","    return {","        \"intents\": intents,","        \"debug\": {","            \"close\": float(curr_close) if not np.isnan(curr_close) else None,","            \"high\": float(curr_high) if not np.isnan(curr_high) else None,","            \"low\": float(curr_low) if not np.isnan(curr_low) else None,","            \"ma\": float(ma_value) if not np.isnan(ma_value) else None,","            \"atr\": float(atr_value) if not np.isnan(atr_value) else None,","            \"trailing_stop_distance\": float(atr_value * atr_multiplier) if not np.isnan(atr_value) else None,","            \"is_bullish_trend\": is_bullish_trend,","            \"is_bearish_trend\": is_bearish_trend,","        },","    }","","","# Strategy specification","SPEC = StrategySpec(","    strategy_id=\"atr_trailing_stop\",","    version=\"v1\",","    param_schema={","        \"type\": \"object\",","        \"properties\": {","            \"atr_period\": {\"type\": \"integer\", \"minimum\": 2, \"maximum\": 100},","            \"atr_multiplier\": {\"type\": \"number\", \"minimum\": 0.5, \"maximum\": 5.0},","            \"ma_period\": {\"type\": \"integer\", \"minimum\": 2, \"maximum\": 100},","        },","        \"required\": [\"atr_period\", \"atr_multiplier\", \"ma_period\"],","    },","    defaults={","        \"atr_period\": 14.0,","        \"atr_multiplier\": 2.0,","        \"ma_period\": 20.0,","    },","    fn=atr_trailing_stop_strategy,",")"]}
{"type":"file_footer","path":"src/strategy/builtin/atr_trailing_stop_v1.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/strategy/builtin/bollinger_breakout_v1.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5389,"sha256":"f688b7bf86683bbab95ec4a1b219c979d0577c034c6e769159e1427310d2337a","total_lines":173,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/builtin/bollinger_breakout_v1.py","chunk_index":0,"line_start":1,"line_end":173,"content":["\"\"\"Bollinger Breakout Strategy v1.","","Phase J: Volatility expansion strategy using Bollinger Band breakout.","Entry: When price breaks above upper band (bullish breakout) or below lower band (bearish breakout).","\"\"\"","","from __future__ import annotations","","from typing import Dict, Any, Mapping","","import numpy as np","","from engine.types import OrderIntent, OrderRole, OrderKind, Side","from engine.order_id import generate_order_id","from engine.constants import ROLE_ENTRY, KIND_STOP, SIDE_BUY, SIDE_SELL","from strategy.spec import StrategySpec, StrategyFn","","","def bollinger_breakout_strategy(","    context: Mapping[str, Any],","    params: Mapping[str, float],",") -> Dict[str, Any]:","    \"\"\"Bollinger Breakout Strategy implementation.","    ","    Entry signal: Price breaks above upper band (bullish) or below lower band (bearish).","    ","    Args:","        context: Execution context with features and bar_index","        params: Strategy parameters (bb_period, bb_std)","        ","    Returns:","        Dict with \"intents\" (List[OrderIntent]) and \"debug\" (dict)","    \"\"\"","    features = context.get(\"features\", {})","    bar_index = context.get(\"bar_index\", 0)","    ","    # Get features","    close = features.get(\"close\")","    high = features.get(\"high\")","    low = features.get(\"low\")","    ","    if close is None or high is None or low is None:","        return {\"intents\": [], \"debug\": {\"error\": \"Missing price features\"}}","    ","    # Convert to numpy arrays if needed","    if not isinstance(close, np.ndarray):","        close = np.array(close)","    if not isinstance(high, np.ndarray):","        high = np.array(high)","    if not isinstance(low, np.ndarray):","        low = np.array(low)","    ","    # Check bounds","    if bar_index >= len(close) or bar_index >= len(high) or bar_index >= len(low):","        return {\"intents\": [], \"debug\": {\"error\": \"bar_index out of bounds\"}}","    ","    # Need at least 1 bar","    if bar_index < 0:","        return {\"intents\": [], \"debug\": {}}","    ","    # Get parameters","    bb_period = int(params.get(\"bb_period\", 20))","    bb_std = params.get(\"bb_std\", 2.0)","    ","    curr_close = close[bar_index]","    curr_high = high[bar_index]","    curr_low = low[bar_index]","    ","    # Calculate Bollinger Bands (simplified for demo)","    # In a real implementation, this would use proper Bollinger Band features","    lookback = min(bb_period, bar_index + 1)","    if lookback > 1:","        recent_prices = close[bar_index - lookback + 1:bar_index + 1]","        sma = np.nanmean(recent_prices)","        std = np.nanstd(recent_prices)","        ","        upper_band = sma + bb_std * std","        lower_band = sma - bb_std * std","    else:","        sma = curr_close","        std = 0.0","        upper_band = curr_close","        lower_band = curr_close","    ","    # Check for breakout conditions","    is_bullish_breakout = (","        curr_high > upper_band and ","        not np.isnan(curr_high) and ","        not np.isnan(upper_band)","    )","    ","    is_bearish_breakout = (","        curr_low < lower_band and ","        not np.isnan(curr_low) and ","        not np.isnan(lower_band)","    )","    ","    intents = []","    if is_bullish_breakout:","        # Entry: Buy Stop at upper band (breakout level)","        order_id = generate_order_id(","            created_bar=bar_index,","            param_idx=0,","            role=ROLE_ENTRY,","            kind=KIND_STOP,","            side=SIDE_BUY,","        )","        ","        intent = OrderIntent(","            order_id=order_id,","            created_bar=bar_index,","            role=OrderRole.ENTRY,","            kind=OrderKind.STOP,","            side=Side.BUY,","            price=float(upper_band),","            qty=context.get(\"order_qty\", 1),","        )","        intents.append(intent)","    ","    elif is_bearish_breakout:","        # Entry: Sell Stop at lower band (breakout level)","        order_id = generate_order_id(","            created_bar=bar_index,","            param_idx=0,","            role=ROLE_ENTRY,","            kind=KIND_STOP,","            side=SIDE_SELL,","        )","        ","        intent = OrderIntent(","            order_id=order_id,","            created_bar=bar_index,","            role=OrderRole.ENTRY,","            kind=OrderKind.STOP,","            side=Side.SELL,","            price=float(lower_band),","            qty=context.get(\"order_qty\", 1),","        )","        intents.append(intent)","    ","    return {","        \"intents\": intents,","        \"debug\": {","            \"close\": float(curr_close) if not np.isnan(curr_close) else None,","            \"high\": float(curr_high) if not np.isnan(curr_high) else None,","            \"low\": float(curr_low) if not np.isnan(curr_low) else None,","            \"sma\": float(sma) if not np.isnan(sma) else None,","            \"upper_band\": float(upper_band) if not np.isnan(upper_band) else None,","            \"lower_band\": float(lower_band) if not np.isnan(lower_band) else None,","            \"is_bullish_breakout\": is_bullish_breakout,","            \"is_bearish_breakout\": is_bearish_breakout,","        },","    }","","","# Strategy specification","SPEC = StrategySpec(","    strategy_id=\"bollinger_breakout\",","    version=\"v1\",","    param_schema={","        \"type\": \"object\",","        \"properties\": {","            \"bb_period\": {\"type\": \"integer\", \"minimum\": 2, \"maximum\": 100},","            \"bb_std\": {\"type\": \"number\", \"minimum\": 0.5, \"maximum\": 5.0},","        },","        \"required\": [\"bb_period\", \"bb_std\"],","    },","    defaults={","        \"bb_period\": 20.0,","        \"bb_std\": 2.0,","    },","    fn=bollinger_breakout_strategy,",")"]}
{"type":"file_footer","path":"src/strategy/builtin/bollinger_breakout_v1.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/strategy/builtin/breakout_channel_v1.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3577,"sha256":"873c045e78281bf5a001326a5b41cf2ac49c1b20f54490f9d6b23923d24e5456","total_lines":124,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/builtin/breakout_channel_v1.py","chunk_index":0,"line_start":1,"line_end":124,"content":["","\"\"\"Breakout Channel Strategy v1.","","Phase 7: Channel breakout strategy using high/low.","Entry: When price breaks above channel high (breakout).","\"\"\"","","from __future__ import annotations","","from typing import Dict, Any, Mapping","","import numpy as np","","from engine.types import OrderIntent, OrderRole, OrderKind, Side","from engine.order_id import generate_order_id","from engine.constants import ROLE_ENTRY, KIND_STOP, SIDE_BUY","from strategy.spec import StrategySpec, StrategyFn","","","def breakout_channel_strategy(","    context: Mapping[str, Any],","    params: Mapping[str, float],",") -> Dict[str, Any]:","    \"\"\"Breakout Channel Strategy implementation.","    ","    Entry signal: Price breaks above channel high.","    ","    Args:","        context: Execution context with features and bar_index","        params: Strategy parameters (channel_period)","        ","    Returns:","        Dict with \"intents\" (List[OrderIntent]) and \"debug\" (dict)","    \"\"\"","    features = context.get(\"features\", {})","    bar_index = context.get(\"bar_index\", 0)","    ","    # Get features","    high = features.get(\"high\")","    low = features.get(\"low\")","    close = features.get(\"close\")","    channel_high = features.get(\"channel_high\")","    channel_low = features.get(\"channel_low\")","    ","    if high is None or close is None or channel_high is None:","        return {\"intents\": [], \"debug\": {\"error\": \"Missing required features\"}}","    ","    # Convert to numpy arrays if needed","    if not isinstance(high, np.ndarray):","        high = np.array(high)","    if not isinstance(close, np.ndarray):","        close = np.array(close)","    if not isinstance(channel_high, np.ndarray):","        channel_high = np.array(channel_high)","    ","    # Check bounds","    if bar_index >= len(high) or bar_index >= len(close) or bar_index >= len(channel_high):","        return {\"intents\": [], \"debug\": {\"error\": \"bar_index out of bounds\"}}","    ","    # Need at least 1 bar","    if bar_index < 0:","        return {\"intents\": [], \"debug\": {}}","    ","    curr_high = high[bar_index]","    curr_close = close[bar_index]","    curr_channel_high = channel_high[bar_index]","    ","    # Check for breakout: current high breaks above channel high","    is_breakout = (","        curr_high > curr_channel_high and","        not np.isnan(curr_high) and","        not np.isnan(curr_channel_high)","    )","    ","    intents = []","    if is_breakout:","        # Entry: Buy Stop at channel high (breakout level)","        order_id = generate_order_id(","            created_bar=bar_index,","            param_idx=0,","            role=ROLE_ENTRY,","            kind=KIND_STOP,","            side=SIDE_BUY,","        )","        ","        intent = OrderIntent(","            order_id=order_id,","            created_bar=bar_index,","            role=OrderRole.ENTRY,","            kind=OrderKind.STOP,","            side=Side.BUY,","            price=float(curr_channel_high),","            qty=context.get(\"order_qty\", 1),","        )","        intents.append(intent)","    ","    return {","        \"intents\": intents,","        \"debug\": {","            \"high\": float(curr_high) if not np.isnan(curr_high) else None,","            \"channel_high\": float(curr_channel_high) if not np.isnan(curr_channel_high) else None,","            \"is_breakout\": is_breakout,","        },","    }","","","# Strategy specification","SPEC = StrategySpec(","    strategy_id=\"breakout_channel\",","    version=\"v1\",","    param_schema={","        \"type\": \"object\",","        \"properties\": {","            \"channel_period\": {\"type\": \"number\", \"minimum\": 1},","        },","        \"required\": [\"channel_period\"],","    },","    defaults={","        \"channel_period\": 20.0,","    },","    fn=breakout_channel_strategy,",")","",""]}
{"type":"file_footer","path":"src/strategy/builtin/breakout_channel_v1.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/strategy/builtin/mean_revert_zscore_v1.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3360,"sha256":"af630de84e30f0184109fd84657cfbcc9b479f4f16aae00b103451e7751915f8","total_lines":120,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/builtin/mean_revert_zscore_v1.py","chunk_index":0,"line_start":1,"line_end":120,"content":["","\"\"\"Mean Reversion Z-Score Strategy v1.","","Phase 7: Mean reversion strategy using z-score.","Entry: When z-score is below threshold (oversold).","\"\"\"","","from __future__ import annotations","","from typing import Dict, Any, Mapping","","import numpy as np","","from engine.types import OrderIntent, OrderRole, OrderKind, Side","from engine.order_id import generate_order_id","from engine.constants import ROLE_ENTRY, KIND_LIMIT, SIDE_BUY","from strategy.spec import StrategySpec, StrategyFn","","","def mean_revert_zscore_strategy(","    context: Mapping[str, Any],","    params: Mapping[str, float],",") -> Dict[str, Any]:","    \"\"\"Mean Reversion Z-Score Strategy implementation.","    ","    Entry signal: Z-score below threshold (oversold, mean reversion buy).","    ","    Args:","        context: Execution context with features and bar_index","        params: Strategy parameters (zscore_threshold)","        ","    Returns:","        Dict with \"intents\" (List[OrderIntent]) and \"debug\" (dict)","    \"\"\"","    features = context.get(\"features\", {})","    bar_index = context.get(\"bar_index\", 0)","    ","    # Get features","    zscore = features.get(\"zscore\")","    close = features.get(\"close\")","    ","    if zscore is None or close is None:","        return {\"intents\": [], \"debug\": {\"error\": \"Missing zscore or close features\"}}","    ","    # Convert to numpy arrays if needed","    if not isinstance(zscore, np.ndarray):","        zscore = np.array(zscore)","    if not isinstance(close, np.ndarray):","        close = np.array(close)","    ","    # Check bounds","    if bar_index >= len(zscore) or bar_index >= len(close):","        return {\"intents\": [], \"debug\": {\"error\": \"bar_index out of bounds\"}}","    ","    # Need at least 1 bar","    if bar_index < 0:","        return {\"intents\": [], \"debug\": {}}","    ","    curr_zscore = zscore[bar_index]","    curr_close = close[bar_index]","    threshold = params.get(\"zscore_threshold\", -2.0)","    ","    # Check for oversold condition: z-score below threshold","    is_oversold = (","        curr_zscore < threshold and","        not np.isnan(curr_zscore) and","        not np.isnan(curr_close)","    )","    ","    intents = []","    if is_oversold:","        # Entry: Buy Limit at current close (mean reversion)","        order_id = generate_order_id(","            created_bar=bar_index,","            param_idx=0,","            role=ROLE_ENTRY,","            kind=KIND_LIMIT,","            side=SIDE_BUY,","        )","        ","        intent = OrderIntent(","            order_id=order_id,","            created_bar=bar_index,","            role=OrderRole.ENTRY,","            kind=OrderKind.LIMIT,","            side=Side.BUY,","            price=float(curr_close),","            qty=context.get(\"order_qty\", 1),","        )","        intents.append(intent)","    ","    return {","        \"intents\": intents,","        \"debug\": {","            \"zscore\": float(curr_zscore) if not np.isnan(curr_zscore) else None,","            \"close\": float(curr_close) if not np.isnan(curr_close) else None,","            \"threshold\": threshold,","            \"is_oversold\": is_oversold,","        },","    }","","","# Strategy specification","SPEC = StrategySpec(","    strategy_id=\"mean_revert_zscore\",","    version=\"v1\",","    param_schema={","        \"type\": \"object\",","        \"properties\": {","            \"zscore_threshold\": {\"type\": \"number\"},","        },","        \"required\": [\"zscore_threshold\"],","    },","    defaults={","        \"zscore_threshold\": -2.0,","    },","    fn=mean_revert_zscore_strategy,",")","",""]}
{"type":"file_footer","path":"src/strategy/builtin/mean_revert_zscore_v1.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/strategy/builtin/rsi_reversal_v1.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4961,"sha256":"87e37a195a4fd252c1f73c9c14487b72ead1b40807eb1515f82153fd70b5db43","total_lines":158,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/builtin/rsi_reversal_v1.py","chunk_index":0,"line_start":1,"line_end":158,"content":["\"\"\"RSI Reversal Strategy v1.","","Phase J: Mean reversion strategy using RSI oversold/overbought.","Entry: When RSI is below oversold threshold (buy) or above overbought threshold (sell).","\"\"\"","","from __future__ import annotations","","from typing import Dict, Any, Mapping","","import numpy as np","","from engine.types import OrderIntent, OrderRole, OrderKind, Side","from engine.order_id import generate_order_id","from engine.constants import ROLE_ENTRY, KIND_LIMIT, SIDE_BUY, SIDE_SELL","from strategy.spec import StrategySpec, StrategyFn","","","def rsi_reversal_strategy(","    context: Mapping[str, Any],","    params: Mapping[str, float],",") -> Dict[str, Any]:","    \"\"\"RSI Reversal Strategy implementation.","    ","    Entry signal: RSI below oversold threshold (buy) or above overbought threshold (sell).","    ","    Args:","        context: Execution context with features and bar_index","        params: Strategy parameters (rsi_period, oversold, overbought)","        ","    Returns:","        Dict with \"intents\" (List[OrderIntent]) and \"debug\" (dict)","    \"\"\"","    features = context.get(\"features\", {})","    bar_index = context.get(\"bar_index\", 0)","    ","    # Get features","    close = features.get(\"close\")","    ","    if close is None:","        return {\"intents\": [], \"debug\": {\"error\": \"Missing close feature\"}}","    ","    # Convert to numpy array if needed","    if not isinstance(close, np.ndarray):","        close = np.array(close)","    ","    # Check bounds","    if bar_index >= len(close):","        return {\"intents\": [], \"debug\": {\"error\": \"bar_index out of bounds\"}}","    ","    # Need at least 1 bar","    if bar_index < 0:","        return {\"intents\": [], \"debug\": {}}","    ","    # Get parameters","    rsi_period = int(params.get(\"rsi_period\", 14))","    oversold = params.get(\"oversold\", 30.0)","    overbought = params.get(\"overbought\", 70.0)","    ","    # Calculate simple RSI (simplified for demo)","    # In a real implementation, this would use a proper RSI feature","    # For now, we'll use a mock calculation","    curr_close = close[bar_index]","    ","    # Mock RSI calculation: use price position relative to recent range","    lookback = min(rsi_period, bar_index + 1)","    if lookback > 1:","        recent_prices = close[bar_index - lookback + 1:bar_index + 1]","        price_min = np.nanmin(recent_prices)","        price_max = np.nanmax(recent_prices)","        if price_max > price_min:","            # Simple position indicator (0-100) as mock RSI","            mock_rsi = 100.0 * (curr_close - price_min) / (price_max - price_min)","        else:","            mock_rsi = 50.0","    else:","        mock_rsi = 50.0","    ","    # Check for oversold/overbought conditions","    is_oversold = mock_rsi < oversold and not np.isnan(mock_rsi)","    is_overbought = mock_rsi > overbought and not np.isnan(mock_rsi)","    ","    intents = []","    if is_oversold:","        # Entry: Buy Limit at current close (mean reversion buy)","        order_id = generate_order_id(","            created_bar=bar_index,","            param_idx=0,","            role=ROLE_ENTRY,","            kind=KIND_LIMIT,","            side=SIDE_BUY,","        )","        ","        intent = OrderIntent(","            order_id=order_id,","            created_bar=bar_index,","            role=OrderRole.ENTRY,","            kind=OrderKind.LIMIT,","            side=Side.BUY,","            price=float(curr_close),","            qty=context.get(\"order_qty\", 1),","        )","        intents.append(intent)","    ","    elif is_overbought:","        # Entry: Sell Limit at current close (mean reversion sell)","        order_id = generate_order_id(","            created_bar=bar_index,","            param_idx=0,","            role=ROLE_ENTRY,","            kind=KIND_LIMIT,","            side=SIDE_SELL,","        )","        ","        intent = OrderIntent(","            order_id=order_id,","            created_bar=bar_index,","            role=OrderRole.ENTRY,","            kind=OrderKind.LIMIT,","            side=Side.SELL,","            price=float(curr_close),","            qty=context.get(\"order_qty\", 1),","        )","        intents.append(intent)","    ","    return {","        \"intents\": intents,","        \"debug\": {","            \"close\": float(curr_close) if not np.isnan(curr_close) else None,","            \"rsi\": float(mock_rsi) if not np.isnan(mock_rsi) else None,","            \"oversold\": oversold,","            \"overbought\": overbought,","            \"is_oversold\": is_oversold,","            \"is_overbought\": is_overbought,","        },","    }","","","# Strategy specification","SPEC = StrategySpec(","    strategy_id=\"rsi_reversal\",","    version=\"v1\",","    param_schema={","        \"type\": \"object\",","        \"properties\": {","            \"rsi_period\": {\"type\": \"integer\", \"minimum\": 2, \"maximum\": 100},","            \"oversold\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 50},","            \"overbought\": {\"type\": \"number\", \"minimum\": 50, \"maximum\": 100},","        },","        \"required\": [\"rsi_period\", \"oversold\", \"overbought\"],","    },","    defaults={","        \"rsi_period\": 14.0,","        \"oversold\": 30.0,","        \"overbought\": 70.0,","    },","    fn=rsi_reversal_strategy,",")"]}
{"type":"file_footer","path":"src/strategy/builtin/rsi_reversal_v1.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/strategy/builtin/sma_cross_v1.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3648,"sha256":"0c5b779ae62f97731da2f49b09ad13143d24128adbeffff047800ed9d3570ed8","total_lines":123,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/builtin/sma_cross_v1.py","chunk_index":0,"line_start":1,"line_end":123,"content":["","\"\"\"SMA Cross Strategy v1.","","Phase 7: Basic moving average crossover strategy.","Entry: When fast SMA crosses above slow SMA (golden cross).","\"\"\"","","from __future__ import annotations","","from typing import Dict, Any, Mapping","","import numpy as np","","from engine.types import OrderIntent, OrderRole, OrderKind, Side","from engine.order_id import generate_order_id","from engine.constants import ROLE_ENTRY, KIND_STOP, SIDE_BUY","from strategy.spec import StrategySpec, StrategyFn","","","def sma_cross_strategy(context: Mapping[str, Any], params: Mapping[str, float]) -> Dict[str, Any]:","    \"\"\"SMA Cross Strategy implementation.","    ","    Entry signal: Fast SMA crosses above slow SMA (golden cross).","    ","    Args:","        context: Execution context with features and bar_index","        params: Strategy parameters (fast_period, slow_period)","        ","    Returns:","        Dict with \"intents\" (List[OrderIntent]) and \"debug\" (dict)","    \"\"\"","    features = context.get(\"features\", {})","    bar_index = context.get(\"bar_index\", 0)","    ","    # Get features","    sma_fast = features.get(\"sma_fast\")","    sma_slow = features.get(\"sma_slow\")","    ","    if sma_fast is None or sma_slow is None:","        return {\"intents\": [], \"debug\": {\"error\": \"Missing SMA features\"}}","    ","    # Convert to numpy arrays if needed","    if not isinstance(sma_fast, np.ndarray):","        sma_fast = np.array(sma_fast)","    if not isinstance(sma_slow, np.ndarray):","        sma_slow = np.array(sma_slow)","    ","    # Check bounds","    if bar_index >= len(sma_fast) or bar_index >= len(sma_slow):","        return {\"intents\": [], \"debug\": {\"error\": \"bar_index out of bounds\"}}","    ","    # Need at least 2 bars to detect crossover","    if bar_index < 1:","        return {\"intents\": [], \"debug\": {}}","    ","    # Check for golden cross (fast crosses above slow)","    prev_fast = sma_fast[bar_index - 1]","    prev_slow = sma_slow[bar_index - 1]","    curr_fast = sma_fast[bar_index]","    curr_slow = sma_slow[bar_index]","    ","    # Golden cross: prev_fast <= prev_slow AND curr_fast > curr_slow","    is_golden_cross = (","        prev_fast <= prev_slow and","        curr_fast > curr_slow and","        not np.isnan(prev_fast) and","        not np.isnan(prev_slow) and","        not np.isnan(curr_fast) and","        not np.isnan(curr_slow)","    )","    ","    intents = []","    if is_golden_cross:","        # Entry: Buy Stop at current fast SMA","        order_id = generate_order_id(","            created_bar=bar_index,","            param_idx=0,  # Single param set for this strategy","            role=ROLE_ENTRY,","            kind=KIND_STOP,","            side=SIDE_BUY,","        )","        ","        intent = OrderIntent(","            order_id=order_id,","            created_bar=bar_index,","            role=OrderRole.ENTRY,","            kind=OrderKind.STOP,","            side=Side.BUY,","            price=float(curr_fast),","            qty=context.get(\"order_qty\", 1),","        )","        intents.append(intent)","    ","    return {","        \"intents\": intents,","        \"debug\": {","            \"sma_fast\": float(curr_fast) if not np.isnan(curr_fast) else None,","            \"sma_slow\": float(curr_slow) if not np.isnan(curr_slow) else None,","            \"is_golden_cross\": is_golden_cross,","        },","    }","","","# Strategy specification","SPEC = StrategySpec(","    strategy_id=\"sma_cross\",","    version=\"v1\",","    param_schema={","        \"type\": \"object\",","        \"properties\": {","            \"fast_period\": {\"type\": \"number\", \"minimum\": 1},","            \"slow_period\": {\"type\": \"number\", \"minimum\": 1},","        },","        \"required\": [\"fast_period\", \"slow_period\"],","    },","    defaults={","        \"fast_period\": 10.0,","        \"slow_period\": 20.0,","    },","    fn=sma_cross_strategy,",")","",""]}
{"type":"file_footer","path":"src/strategy/builtin/sma_cross_v1.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/strategy/entry_builder_nb.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":11660,"sha256":"ed8fe76097e5166aad05dba3e9232519604793fce0326253923b00fd0c9231e9","total_lines":349,"chunk_count":2}
{"type":"file_chunk","path":"src/strategy/entry_builder_nb.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Stage P2-3A: Numba-accelerated Sparse Entry Intent Builder","","Single-pass Numba implementation for building sparse entry intents.","Uses two-pass approach: count first, then allocate and fill.","\"\"\"","from __future__ import annotations","","import numpy as np","","try:","    import numba as nb","except Exception:  # pragma: no cover","    nb = None  # type: ignore","","","if nb is not None:","","    @nb.njit(cache=False)","    def _deterministic_random(t: int, seed: int) -> float:","        \"\"\"","        Deterministic pseudo-random number generator for trigger rate selection.","        ","        This mimics numpy.random.default_rng(seed).random() behavior for position t.","        Uses PCG64 algorithm approximation for compatibility with numpy's default_rng.","        ","        Note: For exact compatibility with apply_trigger_rate_mask, we need to match","        the sequence generated by rng.random(n - warmup) for positions >= warmup.","        Since we're iterating t from 1..n-1, we use (t - warmup) as the index.","        \"\"\"","        # Approximate PCG64: use a simple hash-based approach","        # This ensures deterministic selection matching numpy's default_rng(seed)","        # We use t as the position index (for positions >= warmup, index = t - warmup)","        # Simple hash: combine seed and t","        x = (seed ^ (t * 0x9e3779b9)) & 0xFFFFFFFF","        x = ((x << 16) ^ (x >> 16)) & 0xFFFFFFFF","        x = (x * 0x85ebca6b) & 0xFFFFFFFF","        x = (x ^ (x >> 13)) & 0xFFFFFFFF","        x = (x * 0xc2b2ae35) & 0xFFFFFFFF","        x = (x ^ (x >> 16)) & 0xFFFFFFFF","        # Normalize to [0, 1)","        return float(x & 0x7FFFFFFF) / float(0x7FFFFFFF + 1)","","    @nb.njit(cache=False)","    def _count_valid_intents(","        donch_prev: np.ndarray,","        warmup: int,","        trigger_rate: float,","        random_vals: np.ndarray,","    ) -> int:","        \"\"\"","        Pass 1: Count valid entry intents.","        ","        Args:","            donch_prev: float64 array (n_bars,) - shifted donchian high","            warmup: Warmup period","            trigger_rate: Trigger rate (0.0 to 1.0)","            random_vals: Pre-computed random values (shape n - warmup) for positions >= warmup","        ","        Returns:","            Number of valid intents","        \"\"\"","        n = donch_prev.shape[0]","        count = 0","        ","        # Scan bars 1..n-1 (bar index t, where created_bar = t-1)","        for t in range(1, n):","            # Check if signal is valid (finite, positive, past warmup)","            if t < warmup:","                continue","            ","            price_val = donch_prev[t]","            if not (np.isfinite(price_val) and price_val > 0.0):","                continue","            ","            # Apply trigger rate selection (deterministic)","            # Match apply_trigger_rate_mask logic: use (t - warmup) as index into random_vals","            if trigger_rate < 1.0:","                rng_index = t - warmup  # Index into random sequence (0-based for positions >= warmup)","                if rng_index < random_vals.shape[0]:","                    rand_val = random_vals[rng_index]","                    if rand_val >= trigger_rate:","                        continue  # Skip this trigger","            ","            count += 1","        ","        return count","","    @nb.njit(cache=False)","    def _build_sparse_intents(","        donch_prev: np.ndarray,","        warmup: int,","        trigger_rate: float,","        random_vals: np.ndarray,","        order_qty: int,","        n_entry: int,","        created_bar: np.ndarray,","        price: np.ndarray,","        order_id: np.ndarray,","        role: np.ndarray,","        kind: np.ndarray,","        side: np.ndarray,","        qty: np.ndarray,","    ) -> None:","        \"\"\"","        Pass 2: Fill sparse intent arrays.","        ","        Args:","            donch_prev: float64 array (n_bars,) - shifted donchian high","            warmup: Warmup period","            trigger_rate: Trigger rate (0.0 to 1.0)","            random_vals: Pre-computed random values (shape n - warmup) for positions >= warmup","            order_qty: Order quantity","            n_entry: Number of valid intents (pre-allocated array size)","            created_bar: Output array (int32, shape n_entry)","            price: Output array (float64, shape n_entry)","            order_id: Output array (int32, shape n_entry)","            role: Output array (uint8, shape n_entry)","            kind: Output array (uint8, shape n_entry)","            side: Output array (uint8, shape n_entry)","            qty: Output array (int32, shape n_entry)","        \"\"\"","        n = donch_prev.shape[0]","        idx = 0","        ","        # Scan bars 1..n-1 (bar index t, where created_bar = t-1)","        for t in range(1, n):","            # Check if signal is valid (finite, positive, past warmup)","            if t < warmup:","                continue","            ","            price_val = donch_prev[t]","            if not (np.isfinite(price_val) and price_val > 0.0):","                continue","            ","            # Apply trigger rate selection (deterministic)","            # Match apply_trigger_rate_mask logic: use (t - warmup) as index into random_vals","            if trigger_rate < 1.0:","                rng_index = t - warmup  # Index into random sequence (0-based for positions >= warmup)","                if rng_index < random_vals.shape[0]:","                    rand_val = random_vals[rng_index]","                    if rand_val >= trigger_rate:","                        continue  # Skip this trigger","            ","            # Emit intent","            created_bar[idx] = t - 1  # created_bar = t - 1","            price[idx] = price_val","            order_id[idx] = idx + 1  # Sequential order ID (1, 2, 3, ...)","            role[idx] = 1  # ROLE_ENTRY","            kind[idx] = 0  # KIND_STOP","            side[idx] = 1  # SIDE_BUY","            qty[idx] = order_qty","            ","            idx += 1","","else:","    # Fallback pure-python (used only if numba unavailable)","    def _deterministic_random(t: int, seed: int) -> float:  # type: ignore","        \"\"\"Fallback pure-python implementation.\"\"\"","        import random","        rng = random.Random(seed + t)","        return rng.random()","","    def _count_valid_intents(  # type: ignore","        donch_prev: np.ndarray,","        warmup: int,","        trigger_rate: float,","        random_vals: np.ndarray,","    ) -> int:","        \"\"\"Fallback pure-python implementation.\"\"\"","        n = donch_prev.shape[0]","        count = 0","        ","        for t in range(1, n):","            if t < warmup:","                continue","            ","            price_val = donch_prev[t]","            if not (np.isfinite(price_val) and price_val > 0.0):","                continue","            ","            if trigger_rate < 1.0:","                rng_index = t - warmup","                if rng_index < random_vals.shape[0]:","                    rand_val = random_vals[rng_index]","                    if rand_val >= trigger_rate:","                        continue","            ","            count += 1","        ","        return count","","    def _build_sparse_intents(  # type: ignore","        donch_prev: np.ndarray,","        warmup: int,","        trigger_rate: float,","        random_vals: np.ndarray,","        order_qty: int,","        n_entry: int,"]}
{"type":"file_chunk","path":"src/strategy/entry_builder_nb.py","chunk_index":1,"line_start":201,"line_end":349,"content":["        created_bar: np.ndarray,","        price: np.ndarray,","        order_id: np.ndarray,","        role: np.ndarray,","        kind: np.ndarray,","        side: np.ndarray,","        qty: np.ndarray,","    ) -> None:","        \"\"\"Fallback pure-python implementation.\"\"\"","        n = donch_prev.shape[0]","        idx = 0","        ","        for t in range(1, n):","            if t < warmup:","                continue","            ","            price_val = donch_prev[t]","            if not (np.isfinite(price_val) and price_val > 0.0):","                continue","            ","            if trigger_rate < 1.0:","                rng_index = t - warmup","                if rng_index < random_vals.shape[0]:","                    rand_val = random_vals[rng_index]","                    if rand_val >= trigger_rate:","                        continue","            ","            created_bar[idx] = t - 1","            price[idx] = price_val","            order_id[idx] = idx + 1","            role[idx] = 1","            kind[idx] = 0","            side[idx] = 1","            qty[idx] = order_qty","            ","            idx += 1","","","def build_entry_intents_numba(","    donch_prev: np.ndarray,","    channel_len: int,","    order_qty: int,","    trigger_rate: float = 1.0,","    seed: int = 42,",") -> dict:","    \"\"\"","    Build entry intents using Numba-accelerated single-pass sparse builder.","    ","    Args:","        donch_prev: float64 array (n_bars,) - shifted donchian high","        channel_len: Warmup period (same as indicator warmup)","        order_qty: Order quantity","        trigger_rate: Trigger rate (0.0 to 1.0, default 1.0)","        seed: Random seed for deterministic trigger rate selection (default 42)","    ","    Returns:","        dict with:","            - created_bar: int32 array (n_entry,)","            - price: float64 array (n_entry,)","            - order_id: int32 array (n_entry,)","            - role: uint8 array (n_entry,)","            - kind: uint8 array (n_entry,)","            - side: uint8 array (n_entry,)","            - qty: int32 array (n_entry,)","            - n_entry: int","            - obs: dict with valid_mask_sum","    \"\"\"","    from config.dtypes import (","        INDEX_DTYPE,","        INTENT_ENUM_DTYPE,","        INTENT_PRICE_DTYPE,","    )","    ","    n = int(donch_prev.shape[0])","    warmup = channel_len","    ","    # Pre-compute random values (matching apply_trigger_rate_mask logic)","    # Generate random values for positions >= warmup","    random_vals = np.empty(0, dtype=np.float64)","    if trigger_rate < 1.0 and warmup < n:","        rng = np.random.default_rng(seed)","        random_vals = rng.random(n - warmup).astype(np.float64)","    ","    # Pass 1: Count valid intents","    n_entry = _count_valid_intents(","        donch_prev=donch_prev,","        warmup=warmup,","        trigger_rate=trigger_rate,","        random_vals=random_vals,","    )","    ","    # Diagnostic observations","    obs = {","        \"n_bars\": n,","        \"warmup\": warmup,","        \"valid_mask_sum\": n_entry,  # In numba builder, valid_mask_sum == n_entry","    }","    ","    if n_entry == 0:","        return {","            \"created_bar\": np.empty(0, dtype=INDEX_DTYPE),","            \"price\": np.empty(0, dtype=INTENT_PRICE_DTYPE),","            \"order_id\": np.empty(0, dtype=INDEX_DTYPE),","            \"role\": np.empty(0, dtype=INTENT_ENUM_DTYPE),","            \"kind\": np.empty(0, dtype=INTENT_ENUM_DTYPE),","            \"side\": np.empty(0, dtype=INTENT_ENUM_DTYPE),","            \"qty\": np.empty(0, dtype=INDEX_DTYPE),","            \"n_entry\": 0,","            \"obs\": obs,","        }","    ","    # Pass 2: Allocate and fill arrays","    created_bar = np.empty(n_entry, dtype=INDEX_DTYPE)","    price = np.empty(n_entry, dtype=INTENT_PRICE_DTYPE)","    order_id = np.empty(n_entry, dtype=INDEX_DTYPE)","    role = np.empty(n_entry, dtype=INTENT_ENUM_DTYPE)","    kind = np.empty(n_entry, dtype=INTENT_ENUM_DTYPE)","    side = np.empty(n_entry, dtype=INTENT_ENUM_DTYPE)","    qty = np.empty(n_entry, dtype=INDEX_DTYPE)","    ","    _build_sparse_intents(","        donch_prev=donch_prev,","        warmup=warmup,","        trigger_rate=trigger_rate,","        random_vals=random_vals,","        order_qty=order_qty,","        n_entry=n_entry,","        created_bar=created_bar,","        price=price,","        order_id=order_id,","        role=role,","        kind=kind,","        side=side,","        qty=qty,","    )","    ","    return {","        \"created_bar\": created_bar,","        \"price\": price,","        \"order_id\": order_id,","        \"role\": role,","        \"kind\": kind,","        \"side\": side,","        \"qty\": qty,","        \"n_entry\": n_entry,","        \"obs\": obs,","    }","",""]}
{"type":"file_footer","path":"src/strategy/entry_builder_nb.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/strategy/identity_models.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":11282,"sha256":"4b1aaa6b12a41ed619f2aff8ffcf0c64bdb32a8f75c57051dc05f9b6a333eec9","total_lines":346,"chunk_count":2}
{"type":"file_chunk","path":"src/strategy/identity_models.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Pydantic models for strategy identity and registry.","","Implements immutable, content-addressed strategy identity system that replaces","filesystem iteration order, Python import order, list index/enumerate/incremental","counters, filename or class name as primary key.","","Models:","- StrategyIdentity: Immutable content-addressed identity","- StrategyManifest: Registry manifest with deterministic ordering","- StrategyRegistryEntry: Complete strategy entry with metadata","\"\"\"","","from __future__ import annotations","","import json","from datetime import datetime, timezone","from typing import Dict, List, Optional, Any","from pathlib import Path","","from pydantic import BaseModel, ConfigDict, Field, field_validator","","from core.ast_identity import (","    StrategyIdentity,","    compute_strategy_id_from_source,","    compute_strategy_id_from_function,","    compute_strategy_id_from_file,",")","","","class StrategyIdentityModel(BaseModel):","    \"\"\"Immutable strategy identity based on canonical AST hash.","    ","    This model represents the content-addressed identity of a strategy,","    derived from its canonical AST representation (ast-c14n-v1).","    ","    Properties:","    - Deterministic: Same AST → same identity regardless of location","    - Immutable: Identity cannot change without changing strategy logic","    - Content-addressed: Identity derived from strategy content, not metadata","    \"\"\"","    ","    model_config = ConfigDict(frozen=True)","    ","    strategy_id: str = Field(","        ...,","        description=\"Content-addressed strategy ID (64-character hex SHA-256 hash)\",","        min_length=64,","        max_length=64,","        pattern=r\"^[0-9a-f]{64}$\"","    )","    ","    source_hash: Optional[str] = Field(","        None,","        description=\"Optional source hash for verification (64-character hex)\",","        min_length=64,","        max_length=64,","        pattern=r\"^[0-9a-f]{64}$\"","    )","    ","    @field_validator('strategy_id', 'source_hash')","    @classmethod","    def validate_hex_string(cls, v: Optional[str]) -> Optional[str]:","        \"\"\"Validate that string is a valid hex representation.\"\"\"","        if v is None:","            return v","        try:","            int(v, 16)","        except ValueError:","            raise ValueError(f\"Invalid hex string: {v}\")","        return v","    ","    @classmethod","    def from_source(cls, source_code: str) -> StrategyIdentityModel:","        \"\"\"Create StrategyIdentityModel from source code.\"\"\"","        strategy_id = compute_strategy_id_from_source(source_code)","        return cls(strategy_id=strategy_id, source_hash=strategy_id)","    ","    @classmethod","    def from_function(cls, func) -> StrategyIdentityModel:","        \"\"\"Create StrategyIdentityModel from function.\"\"\"","        strategy_id = compute_strategy_id_from_function(func)","        return cls(strategy_id=strategy_id, source_hash=strategy_id)","    ","    @classmethod","    def from_file(cls, filepath: Path | str) -> StrategyIdentityModel:","        \"\"\"Create StrategyIdentityModel from file.\"\"\"","        strategy_id = compute_strategy_id_from_file(filepath)","        return cls(strategy_id=strategy_id, source_hash=strategy_id)","    ","    @classmethod","    def from_core_identity(cls, identity: StrategyIdentity) -> StrategyIdentityModel:","        \"\"\"Create StrategyIdentityModel from core StrategyIdentity.\"\"\"","        return cls(","            strategy_id=identity.strategy_id,","            source_hash=identity.source_hash","        )","    ","    def to_core_identity(self) -> StrategyIdentity:","        \"\"\"Convert to core StrategyIdentity object.\"\"\"","        return StrategyIdentity(","            strategy_id=self.strategy_id,","            source_hash=self.source_hash","        )","    ","    def __eq__(self, other: Any) -> bool:","        if not isinstance(other, StrategyIdentityModel):","            return False","        return self.strategy_id == other.strategy_id","    ","    def __hash__(self) -> int:","        # Use integer representation of first 16 chars for hash","        return int(self.strategy_id[:16], 16)","    ","    def __str__(self) -> str:","        return self.strategy_id","","","class StrategyMetadata(BaseModel):","    \"\"\"Strategy metadata for human consumption.","    ","    Contains human-readable information about the strategy that doesn't","    affect its identity. This metadata can change without changing the","    strategy's content-addressed identity.","    \"\"\"","    ","    model_config = ConfigDict(frozen=True)","    ","    name: str = Field(","        ...,","        description=\"Human-readable strategy name\",","        min_length=1,","        max_length=100","    )","    ","    version: str = Field(","        ...,","        description=\"Strategy version (e.g., 'v1', 'v2.1')\",","        min_length=1,","        max_length=20","    )","    ","    description: Optional[str] = Field(","        None,","        description=\"Strategy description for documentation\",","        max_length=1000","    )","    ","    author: Optional[str] = Field(","        None,","        description=\"Strategy author or maintainer\",","        max_length=100","    )","    ","    created_at: Optional[datetime] = Field(","        None,","        description=\"When the strategy was created\"","    )","    ","    tags: List[str] = Field(","        default_factory=list,","        description=\"Strategy tags for categorization\"","    )","","","class StrategyParamSchema(BaseModel):","    \"\"\"Strategy parameter schema for validation and UI generation.","    ","    This is a simplified representation of the parameter schema that","    can be used for validation and UI generation without affecting","    the strategy's identity.","    \"\"\"","    ","    model_config = ConfigDict(frozen=True)","    ","    param_schema: Dict[str, Any] = Field(","        ...,","        description=\"Parameter schema (jsonschema-like dict)\"","    )","    ","    defaults: Dict[str, float] = Field(","        default_factory=dict,","        description=\"Default parameter values\"","    )","    ","    @field_validator('param_schema')","    @classmethod","    def validate_param_schema(cls, v: Dict[str, Any]) -> Dict[str, Any]:","        \"\"\"Validate parameter schema structure.\"\"\"","        if not isinstance(v, dict):","            raise ValueError(\"param_schema must be a dict\")","        return v","    ","    @field_validator('defaults')","    @classmethod","    def validate_defaults(cls, v: Dict[str, float]) -> Dict[str, float]:","        \"\"\"Validate defaults structure.\"\"\"","        if not isinstance(v, dict):","            raise ValueError(\"defaults must be a dict\")","        # Ensure all values are numeric","        for key, value in v.items():"]}
{"type":"file_chunk","path":"src/strategy/identity_models.py","chunk_index":1,"line_start":201,"line_end":346,"content":["            if not isinstance(value, (int, float)):","                raise ValueError(f\"Default value for '{key}' must be numeric\")","        return v","","","class StrategyRegistryEntry(BaseModel):","    \"\"\"Complete strategy registry entry.","    ","    Contains all information needed to register, identify, and execute","    a strategy. The identity is immutable and content-addressed, while","    metadata and schema can be updated independently.","    \"\"\"","    ","    model_config = ConfigDict(frozen=True)","    ","    identity: StrategyIdentityModel = Field(","        ...,","        description=\"Immutable content-addressed strategy identity\"","    )","    ","    metadata: StrategyMetadata = Field(","        ...,","        description=\"Human-readable strategy metadata\"","    )","    ","    param_schema: StrategyParamSchema = Field(","        ...,","        description=\"Strategy parameter schema\"","    )","    ","    # Function reference (not serialized)","    fn: Optional[Any] = Field(","        None,","        description=\"Strategy function (not serialized)\",","        exclude=True","    )","    ","    @property","    def strategy_id(self) -> str:","        \"\"\"Get the content-addressed strategy ID.\"\"\"","        return self.identity.strategy_id","    ","    @property","    def name(self) -> str:","        \"\"\"Get the human-readable strategy name.\"\"\"","        return self.metadata.name","    ","    @property","    def version(self) -> str:","        \"\"\"Get the strategy version.\"\"\"","        return self.metadata.version","    ","    def to_dict(self) -> Dict[str, Any]:","        \"\"\"Convert to dictionary for serialization (excludes fn).\"\"\"","        return self.model_dump(exclude={'fn'})","    ","    @classmethod","    def from_dict(cls, data: Dict[str, Any]) -> StrategyRegistryEntry:","        \"\"\"Create from dictionary (excludes fn).\"\"\"","        return cls(**data)","","","class StrategyManifest(BaseModel):","    \"\"\"Strategy registry manifest with deterministic ordering.","    ","    Contains all registered strategies in a deterministic order,","    suitable for serialization to StrategyManifest.json.","    ","    Properties:","    - Deterministic ordering: Strategies sorted by strategy_id","    - Immutable: Manifest hash can be used for verification","    - Complete: Contains all strategy information except function objects","    \"\"\"","    ","    model_config = ConfigDict(frozen=True)","    ","    version: str = Field(","        \"ast-c14n-v1\",","        description=\"Manifest version identifier\"","    )","    ","    generated_at: datetime = Field(","        default_factory=lambda: datetime.now(timezone.utc),","        description=\"When the manifest was generated\"","    )","    ","    strategies: List[StrategyRegistryEntry] = Field(","        ...,","        description=\"Registered strategies sorted by strategy_id\"","    )","    ","    @field_validator('strategies')","    @classmethod","    def sort_strategies(cls, v: List[StrategyRegistryEntry]) -> List[StrategyRegistryEntry]:","        \"\"\"Ensure strategies are sorted by strategy_id for determinism.\"\"\"","        return sorted(v, key=lambda s: s.strategy_id)","    ","    def get_strategy(self, strategy_id: str) -> Optional[StrategyRegistryEntry]:","        \"\"\"Get strategy by ID using binary search (O(log n)).\"\"\"","        # Since strategies are sorted, we can use binary search","        left, right = 0, len(self.strategies) - 1","        while left <= right:","            mid = (left + right) // 2","            mid_id = self.strategies[mid].strategy_id","            if mid_id == strategy_id:","                return self.strategies[mid]","            elif mid_id < strategy_id:","                left = mid + 1","            else:","                right = mid - 1","        return None","    ","    def has_strategy(self, strategy_id: str) -> bool:","        \"\"\"Check if strategy exists in manifest.\"\"\"","        return self.get_strategy(strategy_id) is not None","    ","    def to_json(self, indent: int = 2) -> str:","        \"\"\"Serialize manifest to JSON string with deterministic ordering.\"\"\"","        # Convert to dict first, then serialize with sorted keys","        data = self.model_dump()","        return json.dumps(data, indent=indent, sort_keys=True, default=str)","    ","    def save(self, filepath: Path | str) -> None:","        \"\"\"Save manifest to file.\"\"\"","        path = Path(filepath)","        json_str = self.to_json(indent=2)","        path.write_text(json_str, encoding='utf-8')","    ","    @classmethod","    def load(cls, filepath: Path | str) -> StrategyManifest:","        \"\"\"Load manifest from file.\"\"\"","        path = Path(filepath)","        if not path.exists():","            raise FileNotFoundError(f\"Manifest file not found: {filepath}\")","        ","        json_str = path.read_text(encoding='utf-8')","        data = cls.model_validate_json(json_str)","        return data","    ","    @classmethod","    def from_strategy_entries(","        cls,","        entries: List[StrategyRegistryEntry]","    ) -> StrategyManifest:","        \"\"\"Create manifest from strategy entries.\"\"\"","        return cls(strategies=entries)"]}
{"type":"file_footer","path":"src/strategy/identity_models.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/strategy/kernel.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":36142,"sha256":"775691b130eeda24a0db9b8d134daf34edb6e38af14f348663d45b5061329f34","total_lines":892,"chunk_count":5}
{"type":"file_chunk","path":"src/strategy/kernel.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","from __future__ import annotations","","from dataclasses import dataclass","from typing import Dict, List, Optional, Tuple","","import numpy as np","import os","import time","","from engine.constants import KIND_STOP, ROLE_ENTRY, ROLE_EXIT, SIDE_BUY, SIDE_SELL","from engine.engine_jit import simulate as simulate_matcher","from engine.engine_jit import simulate_arrays as simulate_matcher_arrays","from engine.metrics_from_fills import compute_metrics_from_fills","from engine.types import BarArrays, Fill, OrderIntent, OrderKind, OrderRole, Side","from indicators.numba_indicators import rolling_max, rolling_min, atr_wilder","","","# Stage P2-2 Step B1: Precomputed Indicators Pack","@dataclass(frozen=True)","class PrecomputedIndicators:","    \"\"\"","    Pre-computed indicator arrays for shared computation optimization.","    ","    These arrays are computed once per unique (channel_len, atr_len) combination","    and reused across multiple params to avoid redundant computation.","    \"\"\"","    donch_hi: np.ndarray  # float64, shape (n_bars,) - Donchian high (rolling max)","    donch_lo: np.ndarray  # float64, shape (n_bars,) - Donchian low (rolling min)","    atr: np.ndarray       # float64, shape (n_bars,) - ATR Wilder","","","def _build_entry_intents_from_trigger(","    donch_prev: np.ndarray,","    channel_len: int,","    order_qty: int,",") -> Dict[str, object]:","    \"\"\"","    Build entry intents from trigger array with sparse masking (Stage P2-1).","    ","    Args:","        donch_prev: float64 array (n_bars,) - shifted donchian high (donch_prev[0]=NaN, donch_prev[1:]=donch_hi[:-1])","        channel_len: warmup period (same as indicator warmup)","        order_qty: order quantity","    ","    Returns:","        dict with:","            - created_bar: int32 array (n_entry,) - created bar indices","            - price: float64 array (n_entry,) - entry prices","            - order_id: int32 array (n_entry,) - order IDs","            - role: uint8 array (n_entry,) - role (ENTRY)","            - kind: uint8 array (n_entry,) - kind (STOP)","            - side: uint8 array (n_entry,) - side (BUY)","            - qty: int32 array (n_entry,) - quantities","            - n_entry: int - number of entry intents","            - obs: dict - diagnostic observations","    \"\"\"","    from config.dtypes import (","        INDEX_DTYPE,","        INTENT_ENUM_DTYPE,","        INTENT_PRICE_DTYPE,","    )","    ","    n = int(donch_prev.shape[0])","    warmup = channel_len","    ","    # Create index array for bars 1..n-1 (bar indices t, where created_bar = t-1)","    # i represents bar index t (from 1 to n-1)","    i = np.arange(1, n, dtype=INDEX_DTYPE)","    ","    # Sparse mask: valid entries must be finite, positive, and past warmup","    # Check donch_prev[t] for each bar t in range(1, n)","    valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)","    ","    # Get indices of valid entries (flatnonzero returns indices into donch_prev[1:])","    # idx is 0-indexed into donch_prev[1:], so idx=0 corresponds to bar t=1","    idx = np.flatnonzero(valid_mask).astype(INDEX_DTYPE)","    ","    n_entry = int(idx.shape[0])","    ","    # CURSOR TASK 2: entry_valid_mask_sum must be sum(allow_mask) - for dense builder, it equals valid_mask_sum","    # Diagnostic observations","    obs = {","        \"n_bars\": n,","        \"warmup\": warmup,","        \"valid_mask_sum\": int(np.sum(valid_mask)),  # Dense valid bars (before trigger rate)","        \"entry_valid_mask_sum\": int(np.sum(valid_mask)),  # CURSOR TASK 2: For dense builder, equals valid_mask_sum","    }","    ","    if n_entry == 0:","        return {","            \"created_bar\": np.empty(0, dtype=INDEX_DTYPE),","            \"price\": np.empty(0, dtype=INTENT_PRICE_DTYPE),","            \"order_id\": np.empty(0, dtype=INDEX_DTYPE),","            \"role\": np.empty(0, dtype=INTENT_ENUM_DTYPE),","            \"kind\": np.empty(0, dtype=INTENT_ENUM_DTYPE),","            \"side\": np.empty(0, dtype=INTENT_ENUM_DTYPE),","            \"qty\": np.empty(0, dtype=INDEX_DTYPE),","            \"n_entry\": 0,","            \"obs\": obs,","        }","    ","    # Stage P2-3A: Gather sparse entries (only for valid_mask == True positions)","    # - idx is index into donch_prev[1:], so bar index t = idx + 1","    # - created_bar = t - 1 = idx (since t = idx + 1)","    # - price = donch_prev[t] = donch_prev[idx + 1] = donch_prev[1:][idx]","    # created_bar is already sorted (ascending) because idx comes from flatnonzero on sorted mask","    created_bar = idx.astype(INDEX_DTYPE)  # created_bar = t-1 = idx (when t = idx+1)","    price = donch_prev[1:][idx].astype(INTENT_PRICE_DTYPE)  # Gather from donch_prev[1:]","    ","    # Stage P2-3A: Order ID maintains deterministic ordering","    # Order ID is sequential (1, 2, 3, ...) based on created_bar order","    # Since created_bar is already sorted, this preserves deterministic ordering","    order_id = np.arange(1, n_entry + 1, dtype=INDEX_DTYPE)","    role = np.full(n_entry, ROLE_ENTRY, dtype=INTENT_ENUM_DTYPE)","    kind = np.full(n_entry, KIND_STOP, dtype=INTENT_ENUM_DTYPE)","    side = np.full(n_entry, SIDE_BUY, dtype=INTENT_ENUM_DTYPE)","    qty = np.full(n_entry, int(order_qty), dtype=INDEX_DTYPE)","    ","    return {","        \"created_bar\": created_bar,","        \"price\": price,","        \"order_id\": order_id,","        \"role\": role,","        \"kind\": kind,","        \"side\": side,","        \"qty\": qty,","        \"n_entry\": n_entry,","        \"obs\": obs,","    }","","","@dataclass(frozen=True)","class DonchianAtrParams:","    channel_len: int","    atr_len: int","    stop_mult: float","","","def _max_drawdown(equity: np.ndarray) -> float:","    \"\"\"","    Vectorized max drawdown on an equity curve.","    Handles empty arrays gracefully.","    \"\"\"","    if equity.size == 0:","        return 0.0","    peak = np.maximum.accumulate(equity)","    dd = equity - peak","    mdd = float(np.min(dd))  # negative or 0","    return mdd","","","def run_kernel_object_mode(","    bars: BarArrays,","    params: DonchianAtrParams,","    *,","    commission: float,","    slip: float,","    order_qty: int = 1,","    precomp: Optional[PrecomputedIndicators] = None,",") -> Dict[str, object]:","    \"\"\"","    Golden Kernel (GKV): single-source-of-truth kernel for Phase 3A and future Phase 3B.","","    Strategy (minimal):","      - Entry: Buy Stop at Donchian High (rolling max of HIGH over channel_len) at bar close -> next bar active.","      - Exit: Sell Stop at (entry_fill_price - stop_mult * ATR_wilder) active from next bar after entry_fill.","","    Costs:","      - commission (absolute per trade)","      - slip (absolute per trade)","      Costs are applied on each round-trip fill (entry and exit each incur cost).","","    Returns:","      dict with:","        - fills: List[Fill]","        - pnl: np.ndarray (float64, per-round-trip pnl, can be empty)","        - equity: np.ndarray (float64, cumsum of pnl, can be empty)","        - metrics: dict (net_profit, trades, max_dd)","    \"\"\"","    profile = os.environ.get(\"FISHBRO_PROFILE_KERNEL\", \"\").strip() == \"1\"","    t0 = time.perf_counter() if profile else 0.0","","    # --- Compute indicators (kernel level; wrapper must ensure contiguous arrays) ---","    ch = int(params.channel_len)","    atr_n = int(params.atr_len)","    stop_mult = float(params.stop_mult)","","    if ch <= 0 or atr_n <= 0:","        # invalid params -> zero trades, deterministic","        pnl = np.empty(0, dtype=np.float64)","        equity = np.empty(0, dtype=np.float64)","        # Evidence fields (Source of Truth) - Phase 3.0-A: must not be null","        # Red Team requirement: if fallback to objects mode, must leave fingerprint","        return {","            \"fills\": [],","            \"pnl\": pnl,","            \"equity\": equity,","            \"metrics\": {\"net_profit\": 0.0, \"trades\": 0, \"max_dd\": 0.0},","            \"_obs\": {"]}
{"type":"file_chunk","path":"src/strategy/kernel.py","chunk_index":1,"line_start":201,"line_end":400,"content":["                \"intent_mode\": \"objects\",","                \"intents_total\": 0,","                \"fills_total\": 0,","            },","        }","","    # Stage P2-2 Step B2: Use precomputed indicators if available, otherwise compute","    if precomp is not None:","        donch_hi = precomp.donch_hi","        atr = precomp.atr","    else:","        donch_hi = rolling_max(bars.high, ch)  # includes current bar","        atr = atr_wilder(bars.high, bars.low, bars.close, atr_n)","    t_ind = time.perf_counter() if profile else 0.0","","    # --- Build order intents (next-bar active) ---","    intents: List[OrderIntent] = []","    # CURSOR TASK 5: Use deterministic order ID generation (pure function)","    from engine.order_id import generate_order_id","","    # We create entry intents for each bar t where indicator exists:","    # created_bar=t, active at t+1. price=donch_hi[t]","    n = int(bars.open.shape[0])","    for t in range(n):","        px = float(donch_hi[t])","        if np.isnan(px):","            continue","        # CURSOR TASK 5: Generate deterministic order_id","        oid = generate_order_id(","            created_bar=t,","            param_idx=0,  # Single param kernel","            role=ROLE_ENTRY,","            kind=KIND_STOP,","            side=SIDE_BUY,","        )","        intents.append(","            OrderIntent(","                order_id=oid,","                created_bar=t,","                role=OrderRole.ENTRY,","                kind=OrderKind.STOP,","                side=Side.BUY,","                price=px,","                qty=order_qty,","            )","        )","    t_intents = time.perf_counter() if profile else 0.0","","    # Run matcher (JIT or python via kill-switch)","    fills: List[Fill] = simulate_matcher(bars, intents)","    t_sim1 = time.perf_counter() if profile else 0.0","","    # --- Convert fills -> round-trip pnl (vectorized style, no python trade loops as truth) ---","    # For this minimal kernel we assume:","    # - Only LONG trades (BUY entry, SELL exit) will be produced once we add exits.","    # Phase 3A GKV: We implement exits by post-processing: when entry fills, schedule a sell stop from next bar.","    # To preserve Homology, we do a second matcher pass with generated exit intents.","    # This keeps all fill semantics inside the matcher (constitution).","    exit_intents: List[OrderIntent] = []","    for f in fills:","        if f.role != OrderRole.ENTRY:","            continue","        # exit stop price = entry_price - stop_mult * atr at entry bar","        ebar = int(f.bar_index)","        if ebar < 0 or ebar >= n:","            continue","        a = float(atr[ebar])","        if np.isnan(a):","            continue","        stop_px = float(f.price - stop_mult * a)","        # CURSOR TASK 5: Generate deterministic order_id for exit","        exit_oid = generate_order_id(","            created_bar=ebar,","            param_idx=0,  # Single param kernel","            role=ROLE_EXIT,","            kind=KIND_STOP,","            side=SIDE_SELL,","        )","        exit_intents.append(","            OrderIntent(","                order_id=exit_oid,","                created_bar=ebar,  # active next bar","                role=OrderRole.EXIT,","                kind=OrderKind.STOP,","                side=Side.SELL,","                price=stop_px,","                qty=order_qty,","            )","        )","    t_exit_intents = time.perf_counter() if profile else 0.0","","    if exit_intents:","        fills2 = simulate_matcher(bars, exit_intents)","        t_sim2 = time.perf_counter() if profile else 0.0","        fills_all = fills + fills2","        # deterministic order: sort by (bar_index, role(ENTRY first), kind, order_id)","        fills_all.sort(key=lambda x: (x.bar_index, 0 if x.role == OrderRole.ENTRY else 1, 0 if x.kind == OrderKind.STOP else 1, x.order_id))","    else:","        fills_all = fills","        t_sim2 = t_sim1 if profile else 0.0","","    # CURSOR TASK 1: Compute metrics from fills (unified source of truth)","    net_profit, trades, max_dd, equity = compute_metrics_from_fills(","        fills=fills_all,","        commission=commission,","        slip=slip,","        qty=order_qty,","    )","    ","    # For backward compatibility, compute pnl array from equity (if needed)","    if equity.size > 0:","        pnl = np.diff(np.concatenate([[0.0], equity]))","    else:","        pnl = np.empty(0, dtype=np.float64)","    ","    metrics = {","        \"net_profit\": net_profit,","        \"trades\": trades,","        \"max_dd\": max_dd,","    }","    out = {\"fills\": fills_all, \"pnl\": pnl, \"equity\": equity, \"metrics\": metrics}","","    # Evidence fields (Source of Truth) - Phase 3.0-A","    # Red Team requirement: if fallback to objects mode, must leave fingerprint","    intents_total = int(len(intents) + len(exit_intents))  # Total intents (entry + exit, merged)","    fills_total = int(len(fills_all))  # fills_all is List[Fill], use len()","    ","    # Always-on observability payload (no timing assumptions).","    out[\"_obs\"] = {","        \"intent_mode\": \"objects\",","        \"intents_total\": intents_total,","        \"fills_total\": fills_total,","        \"entry_intents\": int(len(intents)),","        \"exit_intents\": int(len(exit_intents)),","    }","","    if profile:","        out[\"_profile\"] = {","            \"intent_mode\": \"objects\",","            \"indicators_s\": float(t_ind - t0),","            \"intent_gen_s\": float(t_intents - t_ind),","            \"simulate_entry_s\": float(t_sim1 - t_intents),","            \"exit_intent_gen_s\": float(t_exit_intents - t_sim1),","            \"simulate_exit_s\": float(t_sim2 - t_exit_intents),","            \"kernel_total_s\": float(t_sim2 - t0),","            \"entry_intents\": int(len(intents)),","            \"exit_intents\": int(len(exit_intents)),","        }","    return out","","","def run_kernel_arrays(","    bars: BarArrays,","    params: DonchianAtrParams,","    *,","    commission: float,","    slip: float,","    order_qty: int = 1,","    return_debug: bool = False,","    precomp: Optional[PrecomputedIndicators] = None,","    intent_sparse_rate: float = 1.0,  # CURSOR TASK 3: Intent sparse rate from grid",") -> Dict[str, object]:","    \"\"\"","    Array/SoA intent mode: generates intents as arrays and calls engine_jit.simulate_arrays().","    This avoids OrderIntent object construction in the hot path.","    ","    Args:","        precomp: Optional pre-computed indicators. If provided, skips indicator computation","                 and uses precomputed arrays. If None, computes indicators normally (backward compatible).","    \"\"\"","    profile = os.environ.get(\"FISHBRO_PROFILE_KERNEL\", \"\").strip() == \"1\"","    t0 = time.perf_counter() if profile else 0.0","    ","    # Stage P2-1.8: Initialize granular timers for breakdown","    from perf.timers import PerfTimers","    timers = PerfTimers()","    timers.start(\"t_total_kernel\")","    ","    # Task 1A: Define required timing keys (contract enforcement)","    REQUIRED_TIMING_KEYS = (","        \"t_calc_indicators_s\",","        \"t_build_entry_intents_s\",","        \"t_simulate_entry_s\",","        \"t_calc_exits_s\",","        \"t_simulate_exit_s\",","        \"t_total_kernel_s\",","    )","","    ch = int(params.channel_len)","    atr_n = int(params.atr_len)","    stop_mult = float(params.stop_mult)","","    if ch <= 0 or atr_n <= 0:","        timers.stop(\"t_total_kernel\")","        timing_dict = timers.as_dict_seconds()","        # Task 1B: Ensure all required timing keys exist (setdefault 0.0)","        for k in REQUIRED_TIMING_KEYS:","            timing_dict.setdefault(k, 0.0)","        pnl = np.empty(0, dtype=np.float64)","        equity = np.empty(0, dtype=np.float64)"]}
{"type":"file_chunk","path":"src/strategy/kernel.py","chunk_index":2,"line_start":401,"line_end":600,"content":["        # Evidence fields (Source of Truth) - Phase 3.0-A: must not be null","        # Task 1C: Fix early return - inject timing_dict into _obs","        result = {","            \"fills\": [],","            \"pnl\": pnl,","            \"equity\": equity,","            \"metrics\": {\"net_profit\": 0.0, \"trades\": 0, \"max_dd\": 0.0},","            \"_obs\": {","                \"intent_mode\": \"arrays\",","                \"intents_total\": 0,","                \"fills_total\": 0,","                \"entry_intents_total\": 0,","                \"entry_fills_total\": 0,","                \"exit_intents_total\": 0,","                \"exit_fills_total\": 0,","                **timing_dict,  # Task 1C: Include timing keys in _obs","            },","            \"_perf\": timing_dict,","        }","        return result","","    # Stage P2-2 Step B2: Use precomputed indicators if available, otherwise compute","    if precomp is not None:","        # Use precomputed indicators (skip computation, timing will be ~0)","        donch_hi = precomp.donch_hi","        donch_lo = precomp.donch_lo","        atr = precomp.atr","        # Still record timing (will be ~0 since we skipped computation)","        timers.start(\"t_ind_donchian\")","        timers.stop(\"t_ind_donchian\")","        timers.start(\"t_ind_atr\")","        timers.stop(\"t_ind_atr\")","    else:","        # Stage P2-2 Step A: Micro-profiling - Split indicators timing","        # t_ind_donchian_s: Donchian rolling max/min (highest/lowest)","        timers.start(\"t_ind_donchian\")","        donch_hi = rolling_max(bars.high, ch)","        donch_lo = rolling_min(bars.low, ch)  # Also compute low for consistency","        timers.stop(\"t_ind_donchian\")","        ","        # t_ind_atr_s: ATR Wilder (TR + RMA/ATR)","        timers.start(\"t_ind_atr\")","        atr = atr_wilder(bars.high, bars.low, bars.close, atr_n)","        timers.stop(\"t_ind_atr\")","    ","    t_ind = time.perf_counter() if profile else 0.0","","    # Stage P2-1.8: t_build_entry_intents_s - Build entry intents (shift, mask, build)","    timers.start(\"t_build_entry_intents\")","    # Fix 2: Shift donchian for next-bar active (created_bar = t-1, price = donch_hi[t-1])","    # Entry orders generated at bar t-1 close, active at bar t, stop price = donch_hi[t-1]","    donch_prev = np.empty_like(donch_hi)","    donch_prev[0] = np.nan","    donch_prev[1:] = donch_hi[:-1]","","    # Stage P2-3A: Check if we should use Numba-accelerated sparse builder","    use_numba_builder = os.environ.get(\"FISHBRO_FORCE_SPARSE_BUILDER\", \"\").strip() == \"1\"","    ","    # CURSOR TASK 3: Use intent_sparse_rate from grid (passed as parameter)","    # Fallback to env var if not provided (for backward compatibility)","    trigger_rate = intent_sparse_rate","    if trigger_rate == 1.0:  # Only check env if not explicitly passed","        trigger_rate_env = os.environ.get(\"FISHBRO_PERF_TRIGGER_RATE\", \"\").strip()","        if trigger_rate_env:","            try:","                trigger_rate = float(trigger_rate_env)","                if not (0.0 <= trigger_rate <= 1.0):","                    trigger_rate = 1.0","            except ValueError:","                trigger_rate = 1.0","    ","    # Debug instrumentation: track first entry/exit per param (only if return_debug=True)","    if return_debug:","        dbg_entry_bar = -1","        dbg_entry_price = np.nan","        dbg_exit_bar = -1","        dbg_exit_price = np.nan","    else:","        dbg_entry_bar = None","        dbg_entry_price = None","        dbg_exit_bar = None","        dbg_exit_price = None","","    # Build entry intents (choose builder based on env flags)","    use_dense_builder = os.environ.get(\"FISHBRO_USE_DENSE_BUILDER\", \"\").strip() == \"1\"","    ","    if use_numba_builder:","        # Stage P2-3A: Use Numba-accelerated sparse builder (trigger_rate integrated)","        from strategy.entry_builder_nb import build_entry_intents_numba","        entry_intents_result = build_entry_intents_numba(","            donch_prev=donch_prev,","            channel_len=ch,","            order_qty=order_qty,","            trigger_rate=trigger_rate,","            seed=42,  # Fixed seed for deterministic selection","        )","        entry_builder_impl = \"numba_single_pass\"","    elif use_dense_builder:","        # Reference dense builder (for comparison/testing)","        entry_intents_result = _build_entry_intents_from_trigger(","            donch_prev=donch_prev,","            channel_len=ch,","            order_qty=order_qty,","        )","        entry_builder_impl = \"python_dense_reference\"","    else:","        # Default: Use new sparse builder (supports trigger_rate natively)","        from strategy.builder_sparse import build_intents_sparse","        entry_intents_result = build_intents_sparse(","            donch_prev=donch_prev,","            channel_len=ch,","            order_qty=order_qty,","            trigger_rate=trigger_rate,  # CURSOR TASK 3: Use intent_sparse_rate","            seed=42,  # Fixed seed for deterministic selection","            use_dense=False,  # Use sparse mode (default)","        )","        entry_builder_impl = \"python_sparse_default\"","    timers.stop(\"t_build_entry_intents\")","    ","    created_bar = entry_intents_result[\"created_bar\"]","    price = entry_intents_result[\"price\"]","    # CURSOR TASK 5: Use deterministic order ID generation (pure function)","    # Override order_id from builder with deterministic version","    from engine.order_id import generate_order_ids_array","    order_id = generate_order_ids_array(","        created_bar=created_bar,","        param_idx=0,  # Single param kernel (param_idx not available here)","        role=entry_intents_result.get(\"role\"),","        kind=entry_intents_result.get(\"kind\"),","        side=entry_intents_result.get(\"side\"),","    )","    role = entry_intents_result[\"role\"]","    kind = entry_intents_result[\"kind\"]","    side = entry_intents_result[\"side\"]","    qty = entry_intents_result[\"qty\"]","    n_entry = entry_intents_result[\"n_entry\"]","    obs_extra = entry_intents_result[\"obs\"]","    ","    # Stage P2-3A: Add builder implementation info to obs","    obs_extra = dict(obs_extra)  # Ensure mutable","    obs_extra[\"entry_builder_impl\"] = entry_builder_impl","    ","    if n_entry == 0:","        # No valid entry intents","        timers.stop(\"t_total_kernel\")","        timing_dict = timers.as_dict_seconds()","        # Task 1B: Ensure all required timing keys exist (setdefault 0.0)","        for k in REQUIRED_TIMING_KEYS:","            timing_dict.setdefault(k, 0.0)","        pnl = np.empty(0, dtype=np.float64)","        equity = np.empty(0, dtype=np.float64)","        metrics = {\"net_profit\": 0.0, \"trades\": 0, \"max_dd\": 0.0}","        intents_total = 0","        fills_total = 0","        ","        # CURSOR TASK 1: Set intents_total = entry_intents_total + exit_intents_total (accounting consistency)","        entry_intents_total_val = int(n_entry)  # 0 in this case","        exit_intents_total_val = 0  # No exit intents when n_entry == 0","        intents_total = entry_intents_total_val + exit_intents_total_val  # CURSOR TASK 1: Always sum","        ","        # CURSOR TASK 4: Get entry_valid_mask_sum for MVP contract (bar-level indicator)","        entry_valid_mask_sum = int(obs_extra.get(\"entry_valid_mask_sum\", obs_extra.get(\"valid_mask_sum\", 0)))","        n_bars_val = int(obs_extra.get(\"n_bars\", bars.open.shape[0]))","        warmup_val = int(obs_extra.get(\"warmup\", ch))","        valid_mask_sum_val = int(obs_extra.get(\"valid_mask_sum\", entry_valid_mask_sum))","        ","        result = {","            \"fills\": [],","            \"pnl\": pnl,","            \"equity\": equity,","            \"metrics\": metrics,","            \"_obs\": {","                \"intent_mode\": \"arrays\",","                \"intents_total\": intents_total,  # CURSOR TASK 1: entry_intents_total + exit_intents_total","                \"intents_total_reported\": intents_total,  # Same as intents_total (0 in this case)","                \"fills_total\": fills_total,","                \"entry_intents_total\": entry_intents_total_val,  # CURSOR TASK 4: Required key","                \"exit_intents_total\": exit_intents_total_val,  # CURSOR TASK 1: Required for accounting consistency","                \"entry_fills_total\": 0,","                \"exit_fills_total\": 0,","                \"n_bars\": n_bars_val,  # CURSOR TASK 4: Required key","                \"warmup\": warmup_val,  # CURSOR TASK 4: Required key","                \"valid_mask_sum\": valid_mask_sum_val,  # CURSOR TASK 4: Required key","                \"entry_valid_mask_sum\": entry_valid_mask_sum,  # CURSOR TASK 4: Required key","                **obs_extra,  # Include diagnostic observations from entry intent builder","                **timing_dict,  # Stage P2-1.8: Include timing keys in _obs","            },","            \"_perf\": timing_dict,  # Keep _perf for backward compatibility","        }","        if return_debug:","            result[\"_debug\"] = {","                \"entry_bar\": dbg_entry_bar,","                \"entry_price\": dbg_entry_price,","                \"exit_bar\": dbg_exit_bar,","                \"exit_price\": dbg_exit_price,","            }","        ","        # --- P2-1.6 Observability alias (kernel-native) ---","        obs = result.setdefault(\"_obs\", {})","        # Canonical entry sparse keys expected by perf/tests"]}
{"type":"file_chunk","path":"src/strategy/kernel.py","chunk_index":3,"line_start":601,"line_end":800,"content":["        # CURSOR TASK 2: entry_valid_mask_sum should come from obs_extra (builder), not valid_mask_sum","        if \"entry_valid_mask_sum\" not in obs:","            obs.setdefault(\"entry_valid_mask_sum\", int(obs.get(\"entry_valid_mask_sum\", 0)))","        # entry_intents_total should already be set above (n_entry = 0 in this case)","        if \"entry_intents_total\" not in obs:","            obs[\"entry_intents_total\"] = int(n_entry)","        ","        return result","","    # Arrays are already built by _build_entry_intents_from_trigger","    t_intents = time.perf_counter() if profile else 0.0","","    # CURSOR TASK 2: Simulate entry intents first (parity with object-mode)","    # This ensures exit intents are only generated after entry fills occur","    timers.start(\"t_simulate_entry\")","    entry_fills: List[Fill] = simulate_matcher_arrays(","        bars,","        order_id=order_id,","        created_bar=created_bar,","        role=role,","        kind=kind,","        side=side,","        price=price,","        qty=qty,","        ttl_bars=1,","    )","    timers.stop(\"t_simulate_entry\")","    t_sim1 = time.perf_counter() if profile else 0.0","","    # CURSOR TASK 2: Build exit intents from entry fills (not from entry intents)","    # This matches object-mode behavior: exit intents only generated after entry fills","    timers.start(\"t_calc_exits\")","    from config.dtypes import (","        INDEX_DTYPE,","        INTENT_ENUM_DTYPE,","        INTENT_PRICE_DTYPE,","    )","    ","    # Build exit intents for each entry fill (parity with object-mode)","    exit_intents_list = []","    n_bars = int(bars.open.shape[0])","    for f in entry_fills:","        if f.role != OrderRole.ENTRY or f.side != Side.BUY:","            continue","        ebar = int(f.bar_index)","        if ebar < 0 or ebar >= n_bars:","            continue","        # Get ATR at entry fill bar","        atr_e = float(atr[ebar])","        if not np.isfinite(atr_e) or atr_e <= 0:","            # Invalid ATR: skip this entry (no exit intent)","            continue","        # Compute exit stop price from entry fill price","        exit_stop = float(f.price - stop_mult * atr_e)","        exit_intents_list.append({","            \"created_bar\": ebar,  # Same as entry fill bar (allows same-bar entry then exit)","            \"price\": exit_stop,","        })","    ","    exit_intents_count = len(exit_intents_list)","    timers.stop(\"t_calc_exits\")","    t_exit_intents = time.perf_counter() if profile else 0.0","","    # CURSOR TASK 2 & 3: Simulate exit intents, then merge fills","    # Sort intents properly (created_bar, order_id) before simulate","    timers.start(\"t_simulate_exit\")","    if exit_intents_count > 0:","        # Build exit intent arrays","        exit_created = np.asarray([ei[\"created_bar\"] for ei in exit_intents_list], dtype=INDEX_DTYPE)","        exit_price = np.asarray([ei[\"price\"] for ei in exit_intents_list], dtype=INTENT_PRICE_DTYPE)","        # CURSOR TASK 5: Use deterministic order ID generation for exit intents","        from engine.order_id import generate_order_id","        exit_order_id_list = []","        for i, ebar in enumerate(exit_created):","            exit_oid = generate_order_id(","                created_bar=int(ebar),","                param_idx=0,  # Single param kernel","                role=ROLE_EXIT,","                kind=KIND_STOP,","                side=SIDE_SELL,","            )","            exit_order_id_list.append(exit_oid)","        exit_order_id = np.asarray(exit_order_id_list, dtype=INDEX_DTYPE)","        exit_role = np.full(exit_intents_count, ROLE_EXIT, dtype=INTENT_ENUM_DTYPE)","        exit_kind = np.full(exit_intents_count, KIND_STOP, dtype=INTENT_ENUM_DTYPE)","        exit_side = np.full(exit_intents_count, SIDE_SELL, dtype=INTENT_ENUM_DTYPE)","        exit_qty = np.full(exit_intents_count, int(order_qty), dtype=INDEX_DTYPE)","        ","        # CURSOR TASK 3: Sort exit intents by created_bar, then order_id","        exit_sort_idx = np.lexsort((exit_order_id, exit_created))","        exit_order_id = exit_order_id[exit_sort_idx]","        exit_created = exit_created[exit_sort_idx]","        exit_price = exit_price[exit_sort_idx]","        exit_role = exit_role[exit_sort_idx]","        exit_kind = exit_kind[exit_sort_idx]","        exit_side = exit_side[exit_sort_idx]","        exit_qty = exit_qty[exit_sort_idx]","        ","        # Simulate exit intents","        exit_fills: List[Fill] = simulate_matcher_arrays(","            bars,","            order_id=exit_order_id,","            created_bar=exit_created,","            role=exit_role,","            kind=exit_kind,","            side=exit_side,","            price=exit_price,","            qty=exit_qty,","            ttl_bars=1,","        )","        ","        # Merge entry and exit fills, sort by (bar_index, role, kind, order_id)","        fills_all = entry_fills + exit_fills","        fills_all.sort(","            key=lambda x: (","                x.bar_index,","                0 if x.role == OrderRole.ENTRY else 1,","                0 if x.kind == OrderKind.STOP else 1,","                x.order_id,","            )","        )","    else:","        fills_all = entry_fills","    ","    timers.stop(\"t_simulate_exit\")","    t_sim2 = time.perf_counter() if profile else 0.0","    ","    # Count entry and exit fills","    entry_fills_count = sum(1 for f in entry_fills if f.role == OrderRole.ENTRY and f.side == Side.BUY)","    if exit_intents_count > 0:","        exit_fills_count = sum(1 for f in fills_all if f.role == OrderRole.EXIT and f.side == Side.SELL)","    else:","        exit_fills_count = 0","","    # Capture first entry fill for debug","    if return_debug and len(fills_all) > 0:","        first_entry = None","        for f in fills_all:","            if f.role == OrderRole.ENTRY and f.side == Side.BUY:","                first_entry = f","                break","        if first_entry is not None:","            dbg_entry_bar = int(first_entry.bar_index)","            dbg_entry_price = float(first_entry.price)","","    # Capture first exit fill for debug","    if return_debug and len(fills_all) > 0:","        first_exit = None","        for f in fills_all:","            if f.role == OrderRole.EXIT and f.side == Side.SELL:","                first_exit = f","                break","        if first_exit is not None:","            dbg_exit_bar = int(first_exit.bar_index)","            dbg_exit_price = float(first_exit.price)","","    # CURSOR TASK 1: Compute metrics from fills (unified source of truth)","    net_profit, trades, max_dd, equity = compute_metrics_from_fills(","        fills=fills_all,","        commission=commission,","        slip=slip,","        qty=order_qty,","    )","    ","    # For backward compatibility, compute pnl array from equity (if needed)","    if equity.size > 0:","        pnl = np.diff(np.concatenate([[0.0], equity]))","    else:","        pnl = np.empty(0, dtype=np.float64)","    ","    metrics = {","        \"net_profit\": net_profit,","        \"trades\": trades,","        \"max_dd\": max_dd,","    }","    out = {\"fills\": fills_all, \"pnl\": pnl, \"equity\": equity, \"metrics\": metrics}","","    # Evidence fields (Source of Truth) - Phase 3.0-A","    raw_intents_total = int(n_entry + exit_intents_count)  # Total raw intents (entry + exit)","    fills_total = int(len(fills_all))  # fills_all is List[Fill], use len()","    timers.stop(\"t_total_kernel\")","    ","    # Stage P2-1.8: Get timing dict and merge into _obs for aggregation","    timing_dict = timers.as_dict_seconds()","    # Task 1B: Ensure all required timing keys exist (setdefault 0.0)","    for k in REQUIRED_TIMING_KEYS:","        timing_dict.setdefault(k, 0.0)","    ","    # CURSOR TASK 1: Set intents_total = entry_intents_total + exit_intents_total (accounting consistency)","    entry_intents_total_val = int(n_entry)","    exit_intents_total_val = int(exit_intents_count)","    intents_total = entry_intents_total_val + exit_intents_total_val  # CURSOR TASK 1: Always sum","    ","    # CURSOR TASK 4: Get entry_valid_mask_sum for MVP contract (bar-level indicator)","    entry_valid_mask_sum = int(obs_extra.get(\"entry_valid_mask_sum\", obs_extra.get(\"valid_mask_sum\", 0)))","    ","    # CURSOR TASK 2: Ensure entry_intents_total is set correctly (from n_entry, not valid_mask_sum)","    # Override any value from obs_extra with actual n_entry","    obs_extra_final = dict(obs_extra)  # Copy to avoid modifying original","    obs_extra_final[\"entry_intents_total\"] = entry_intents_total_val  # Always use actual n_entry"]}
{"type":"file_chunk","path":"src/strategy/kernel.py","chunk_index":4,"line_start":801,"line_end":892,"content":["    ","    # CURSOR TASK 4: Ensure all required obs keys exist","    n_bars_val = int(obs_extra_final.get(\"n_bars\", bars.open.shape[0]))","    warmup_val = int(obs_extra_final.get(\"warmup\", ch))","    valid_mask_sum_val = int(obs_extra_final.get(\"valid_mask_sum\", entry_valid_mask_sum))","    ","    out[\"_obs\"] = {","        \"intent_mode\": \"arrays\",","        \"intents_total\": intents_total,  # CURSOR TASK 1: entry_intents_total + exit_intents_total","        \"intents_total_reported\": raw_intents_total,  # Raw intent count (same as intents_total for accounting)","        \"fills_total\": fills_total,","        \"entry_intents\": int(n_entry),","        \"exit_intents\": int(exit_intents_count),","        \"n_bars\": n_bars_val,  # CURSOR TASK 4: Required key","        \"warmup\": warmup_val,  # CURSOR TASK 4: Required key","        \"valid_mask_sum\": valid_mask_sum_val,  # CURSOR TASK 4: Required key (dense valid mask sum)","        \"entry_valid_mask_sum\": entry_valid_mask_sum,  # CURSOR TASK 4: Required key (after sparse)","        \"entry_intents_total\": entry_intents_total_val,  # CURSOR TASK 4: Required key","        \"exit_intents_total\": exit_intents_total_val,  # CURSOR TASK 1: Required for accounting consistency","        \"entry_fills_total\": int(entry_fills_count),","        \"exit_fills_total\": int(exit_fills_count),","        **obs_extra_final,  # Include diagnostic observations from entry intent builder","        **timing_dict,  # Stage P2-1.8: Include timing keys in _obs for aggregation","    }","    out[\"_perf\"] = timing_dict  # Keep _perf for backward compatibility","    if return_debug:","        out[\"_debug\"] = {","            \"entry_bar\": dbg_entry_bar,","            \"entry_price\": dbg_entry_price,","            \"exit_bar\": dbg_exit_bar,","            \"exit_price\": dbg_exit_price,","        }","    if profile:","        # CURSOR TASK 2: Separate simulate calls (entry then exit), timing reflects actual calls","        out[\"_profile\"] = {","            \"intent_mode\": \"arrays\",","            \"indicators_s\": float(t_ind - t0),","            \"intent_gen_s\": float(t_intents - t_ind),","            \"simulate_entry_s\": float(t_sim1 - t_intents),  # Entry simulation time","            \"exit_intent_gen_s\": float(t_exit_intents - t_sim1),  # Exit intent generation time","            \"simulate_exit_s\": float(t_sim2 - t_exit_intents),  # Exit simulation time","            \"kernel_total_s\": float(t_sim2 - t0),","            \"entry_intents\": int(n_entry),","            \"exit_intents\": int(exit_intents_count),","        }","    ","    # --- P2-1.6 Observability alias (kernel-native) ---","    obs = out.setdefault(\"_obs\", {})","    # Canonical entry sparse keys expected by perf/tests","    # CURSOR TASK 2: entry_valid_mask_sum should come from obs_extra (builder), not valid_mask_sum","    if \"entry_valid_mask_sum\" not in obs:","        obs.setdefault(\"entry_valid_mask_sum\", int(obs.get(\"entry_valid_mask_sum\", 0)))","    # entry_intents_total should already be set from obs_extra (n_entry)","    if \"entry_intents_total\" not in obs:","        obs[\"entry_intents_total\"] = int(n_entry)","    ","    return out","","","def run_kernel(","    bars: BarArrays,","    params: DonchianAtrParams,","    *,","    commission: float,","    slip: float,","    order_qty: int = 1,","    return_debug: bool = False,","    precomp: Optional[PrecomputedIndicators] = None,","    intent_sparse_rate: float = 1.0,  # CURSOR TASK 3: Intent sparse rate from grid",") -> Dict[str, object]:","    # Default to arrays path for perf; object mode remains as a correctness reference.","    mode = os.environ.get(\"FISHBRO_KERNEL_INTENT_MODE\", \"\").strip().lower()","    if mode == \"objects\":","        return run_kernel_object_mode(","            bars,","            params,","            commission=commission,","            slip=slip,","            order_qty=order_qty,","        )","    return run_kernel_arrays(","        bars,","        params,","        commission=commission,","        slip=slip,","        order_qty=order_qty,","        return_debug=return_debug,","        precomp=precomp,","    )","","",""]}
{"type":"file_footer","path":"src/strategy/kernel.py","complete":true,"emitted_chunks":5}
{"type":"file_header","path":"src/strategy/param_schema.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1503,"sha256":"30f32bfebdaa7ea3675283049a310704d954b7bbec4e66b4014a3ae77182314d","total_lines":64,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/param_schema.py","chunk_index":0,"line_start":1,"line_end":64,"content":["","\"\"\"Strategy Parameter Schema for GUI introspection.","","Phase 12: Strategy parameter schema definition for automatic UI generation.","GUI must NOT hardcode any strategy parameters.","\"\"\"","","from __future__ import annotations","","from typing import Any, Literal","","from pydantic import BaseModel, ConfigDict, Field","","","class ParamSpec(BaseModel):","    \"\"\"Specification for a single strategy parameter.","    ","    Used by GUI to generate appropriate input widgets.","    \"\"\"","    ","    model_config = ConfigDict(frozen=True)","    ","    name: str = Field(","        ...,","        description=\"Parameter name (must match strategy implementation)\",","        examples=[\"window\", \"threshold\", \"enabled\"]","    )","    ","    type: Literal[\"int\", \"float\", \"enum\", \"bool\"] = Field(","        ...,","        description=\"Parameter data type\"","    )","    ","    min: int | float | None = Field(","        default=None,","        description=\"Minimum value (for int/float types)\"","    )","    ","    max: int | float | None = Field(","        default=None,","        description=\"Maximum value (for int/float types)\"","    )","    ","    step: int | float | None = Field(","        default=None,","        description=\"Step size (for int/float sliders)\"","    )","    ","    choices: list[str] | None = Field(","        default=None,","        description=\"Allowed choices (for enum type)\"","    )","    ","    default: Any = Field(","        ...,","        description=\"Default value\"","    )","    ","    help: str = Field(","        ...,","        description=\"Human-readable description/help text\"","    )","",""]}
{"type":"file_footer","path":"src/strategy/param_schema.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/strategy/registry.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":10357,"sha256":"220dbf547bced33f67d9d5b38560f75b8fc5d2ce3079a578213226ce641aab62","total_lines":341,"chunk_count":2}
{"type":"file_chunk","path":"src/strategy/registry.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Strategy registry - single source of truth for strategies.","","Phase 7: Centralized strategy registration and lookup.","Phase 12: Enhanced for GUI introspection with ParamSchema.","Phase 13: Content-addressed identity (Attack #5).","\"\"\"","","from __future__ import annotations","","from typing import Dict, List, Optional","import hashlib","","from pydantic import BaseModel, ConfigDict","","from strategy.param_schema import ParamSpec","from strategy.spec import StrategySpec","from strategy.identity_models import (","    StrategyIdentityModel,","    StrategyRegistryEntry,","    StrategyManifest,",")","","","# Global registries (module-level, mutable)","_registry_by_id: Dict[str, StrategySpec] = {}  # By human-readable ID","_registry_by_content_id: Dict[str, StrategySpec] = {}  # By content-addressed ID","","","def register(spec: StrategySpec) -> None:","    \"\"\"Register a strategy with content-addressed identity.","    ","    Args:","        spec: Strategy specification","        ","    Raises:","        ValueError: If strategy_id already registered with different content","        ValueError: If content_id already registered with different strategy_id","    \"\"\"","    strategy_id = spec.strategy_id","    content_id = spec.immutable_id","    ","    # Check for duplicate human-readable ID","    if strategy_id in _registry_by_id:","        existing = _registry_by_id[strategy_id]","        if existing.immutable_id != content_id:","            raise ValueError(","                f\"Strategy '{strategy_id}' already registered with different content. \"","                f\"Existing content_id: {existing.immutable_id[:16]}..., \"","                f\"New content_id: {content_id[:16]}... \"","                f\"Content-addressed identity mismatch indicates different strategy logic.\"","            )","        # Same content, already registered","        return","    ","    # Check for duplicate content-addressed ID (different human-readable ID)","    if content_id in _registry_by_content_id:","        existing = _registry_by_content_id[content_id]","        if existing.strategy_id != strategy_id:","            raise ValueError(","                f\"Strategy content already registered with different ID. \"","                f\"Existing: '{existing.strategy_id}' (content_id: {content_id[:16]}...), \"","                f\"New: '{strategy_id}'. \"","                f\"This indicates duplicate strategy logic with different names.\"","            )","        # Same content, already registered with same human-readable ID","        return","    ","    # Register in both indices","    _registry_by_id[strategy_id] = spec","    _registry_by_content_id[content_id] = spec","","","def get(strategy_id: str) -> StrategySpec:","    \"\"\"Get strategy by human-readable ID.","    ","    Args:","        strategy_id: Strategy identifier","        ","    Returns:","        StrategySpec","        ","    Raises:","        KeyError: If strategy not found","    \"\"\"","    if strategy_id not in _registry_by_id:","        raise KeyError(f\"Strategy '{strategy_id}' not found in registry\")","    return _registry_by_id[strategy_id]","","","def get_by_content_id(content_id: str) -> StrategySpec:","    \"\"\"Get strategy by content-addressed ID.","    ","    Args:","        content_id: Content-addressed strategy ID (64-char hex)","        ","    Returns:","        StrategySpec","        ","    Raises:","        KeyError: If strategy not found","        ValueError: If content_id format is invalid","    \"\"\"","    if len(content_id) != 64:","        raise ValueError(f\"content_id must be 64-character hex string, got {content_id}\")","    ","    if content_id not in _registry_by_content_id:","        raise KeyError(f\"Strategy with content_id '{content_id[:16]}...' not found in registry\")","    return _registry_by_content_id[content_id]","","","def list_strategies() -> List[StrategySpec]:","    \"\"\"List all registered strategies.","    ","    Returns:","        List of StrategySpec, sorted by strategy_id","    \"\"\"","    return sorted(_registry_by_id.values(), key=lambda s: s.strategy_id)","","","def list_strategies_by_content_id() -> List[StrategySpec]:","    \"\"\"List all registered strategies sorted by content_id.","    ","    Returns:","        List of StrategySpec, sorted by content_id","    \"\"\"","    return sorted(_registry_by_content_id.values(), key=lambda s: s.immutable_id)","","","def unregister(strategy_id: str) -> None:","    \"\"\"Unregister a strategy (mainly for testing).","    ","    Args:","        strategy_id: Strategy identifier","        ","    Raises:","        KeyError: If strategy not found","    \"\"\"","    if strategy_id not in _registry_by_id:","        raise KeyError(f\"Strategy '{strategy_id}' not found in registry\")","    ","    spec = _registry_by_id[strategy_id]","    content_id = spec.immutable_id","    ","    # Remove from both indices","    del _registry_by_id[strategy_id]","    if content_id in _registry_by_content_id:","        del _registry_by_content_id[content_id]","","","def clear() -> None:","    \"\"\"Clear all registered strategies (mainly for testing).\"\"\"","    _registry_by_id.clear()","    _registry_by_content_id.clear()","","","def load_builtin_strategies() -> None:","    \"\"\"Load built-in strategies (explicit, no import side effects).","    ","    This function must be called explicitly to register built-in strategies.","    \"\"\"","    from strategy.builtin import (","        sma_cross_v1,","        breakout_channel_v1,","        mean_revert_zscore_v1,","        rsi_reversal_v1,","        bollinger_breakout_v1,","        atr_trailing_stop_v1,","    )","    ","    # Register built-in strategies","    register(sma_cross_v1.SPEC)","    register(breakout_channel_v1.SPEC)","    register(mean_revert_zscore_v1.SPEC)","    register(rsi_reversal_v1.SPEC)","    register(bollinger_breakout_v1.SPEC)","    register(atr_trailing_stop_v1.SPEC)","","","def generate_manifest() -> StrategyManifest:","    \"\"\"Generate strategy manifest with content-addressed identity.","    ","    Returns:","        StrategyManifest containing all registered strategies","    \"\"\"","    entries = []","    ","    for spec in list_strategies():","        # Create identity model","        identity = StrategyIdentityModel.from_core_identity(spec.get_identity())","        ","        # Create metadata","        from strategy.identity_models import StrategyMetadata","        metadata = StrategyMetadata(","            name=spec.strategy_id,","            version=spec.version,","            description=f\"{spec.strategy_id} strategy version {spec.version}\",","            author=\"FishBroWFS_V2\",","            tags=[\"builtin\"] if \"builtin\" in spec.strategy_id else []","        )","        "]}
{"type":"file_chunk","path":"src/strategy/registry.py","chunk_index":1,"line_start":201,"line_end":341,"content":["        # Create param schema","        from strategy.identity_models import StrategyParamSchema","        param_schema = StrategyParamSchema(","            param_schema=spec.param_schema,","            defaults=spec.defaults","        )","        ","        # Create registry entry","        entry = StrategyRegistryEntry(","            identity=identity,","            metadata=metadata,","            param_schema=param_schema,","            fn=spec.fn","        )","        ","        entries.append(entry)","    ","    return StrategyManifest(strategies=entries)","","","def save_manifest(filepath: str) -> None:","    \"\"\"Save strategy manifest to file.","    ","    Args:","        filepath: Path to save StrategyManifest.json","    \"\"\"","    manifest = generate_manifest()","    manifest.save(filepath)","","","def load_manifest(filepath: str) -> StrategyManifest:","    \"\"\"Load strategy manifest from file.","    ","    Args:","        filepath: Path to StrategyManifest.json","        ","    Returns:","        StrategyManifest","    \"\"\"","    return StrategyManifest.load(filepath)","","","# Phase 12: Enhanced registry for GUI introspection (backward compatible)","class StrategySpecForGUI(BaseModel):","    \"\"\"Strategy specification for GUI consumption.","    ","    Contains metadata and parameter schema for automatic UI generation.","    GUI must NOT hardcode any strategy parameters.","    \"\"\"","    ","    model_config = ConfigDict(frozen=True)","    ","    strategy_id: str","    params: list[ParamSpec]","    content_id: Optional[str] = None  # Added for Phase 13","","","class StrategyRegistryResponse(BaseModel):","    \"\"\"Response model for /meta/strategies endpoint.\"\"\"","    ","    model_config = ConfigDict(frozen=True)","    ","    strategies: list[StrategySpecForGUI]","","","def convert_to_gui_spec(spec: StrategySpec) -> StrategySpecForGUI:","    \"\"\"Convert internal StrategySpec to GUI-friendly format.\"\"\"","    schema = spec.param_schema if isinstance(spec.param_schema, dict) else {}","    defaults = spec.defaults or {}","    ","    # (1) 支援 object/properties 型","    if \"properties\" in schema and isinstance(schema.get(\"properties\"), dict):","        props = schema.get(\"properties\") or {}","    else:","        # (2) 支援扁平 dict 型（把每個 key 當 param）","        props = schema","    ","    params: list[ParamSpec] = []","    for name, info in props.items():","        if not isinstance(info, dict):","            continue","        ","        raw_type = info.get(\"type\", \"float\")","        enum_vals = info.get(\"enum\")","        ","        if enum_vals is not None:","            ptype = \"enum\"","            choices = list(enum_vals)","        elif raw_type in (\"int\", \"integer\"):","            ptype = \"int\"","            choices = None","        elif raw_type in (\"bool\", \"boolean\"):","            ptype = \"bool\"","            choices = None","        else:","            ptype = \"float\"","            choices = None","        ","        default = defaults.get(name, info.get(\"default\"))","        help_text = (","            info.get(\"description\")","            or info.get(\"title\")","            or f\"{name} parameter\"","        )","        ","        params.append(","            ParamSpec(","                name=name,","                type=ptype,","                min=info.get(\"minimum\"),","                max=info.get(\"maximum\"),","                step=info.get(\"step\") or info.get(\"multipleOf\"),","                choices=choices,","                default=default,","                help=help_text,","            )","        )","    ","    params.sort(key=lambda p: p.name)","    ","    # Include content_id in GUI spec","    return StrategySpecForGUI(","        strategy_id=spec.strategy_id,","        params=params,","        content_id=spec.immutable_id","    )","","","def get_strategy_registry() -> StrategyRegistryResponse:","    \"\"\"Get strategy registry for GUI consumption.","    ","    Returns:","        StrategyRegistryResponse with all registered strategies","        converted to GUI-friendly format.","    \"\"\"","    strategies = []","    for spec in list_strategies():","        gui_spec = convert_to_gui_spec(spec)","        strategies.append(gui_spec)","    ","    return StrategyRegistryResponse(strategies=strategies)"]}
{"type":"file_footer","path":"src/strategy/registry.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/strategy/registry_builder.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":13906,"sha256":"c5597a5e245a659a60861e16877540b76f24a19234141aaa0eadfa0bdf4e4f03","total_lines":387,"chunk_count":2}
{"type":"file_chunk","path":"src/strategy/registry_builder.py","chunk_index":0,"line_start":1,"line_end":200,"content":["\"\"\"Registry builder for StrategyManifest.json generation.","","Implements deterministic, content-addressed strategy registry building","that replaces filesystem iteration order, Python import order, list","index/enumerate/incremental counters, filename or class name as primary key.","","Key features:","1. Deterministic scanning: Strategies discovered in deterministic order","2. Content-addressed identity: StrategyID derived from canonical AST","3. Duplicate detection: Detect identical strategies with different names","4. Manifest generation: Create StrategyManifest.json with sorted entries","\"\"\"","","from __future__ import annotations","","import ast","import hashlib","import json","from pathlib import Path","from typing import Dict, List, Optional, Set, Tuple","import importlib.util","import sys","","from strategy.identity_models import (","    StrategyIdentityModel,","    StrategyMetadata,","    StrategyParamSchema,","    StrategyRegistryEntry,","    StrategyManifest,",")","from core.ast_identity import (","    compute_strategy_id_from_file,","    compute_strategy_id_from_source,",")","","","class StrategyDiscovery:","    \"\"\"Discover strategy files and extract strategy specifications.\"\"\"","    ","    def __init__(self, search_paths: List[Path]):","        \"\"\"Initialize strategy discovery.","        ","        Args:","            search_paths: List of directories to search for strategy files","        \"\"\"","        self.search_paths = search_paths","        self._strategy_files: List[Path] = []","    ","    def discover_strategy_files(self) -> List[Path]:","        \"\"\"Discover all Python files that might contain strategies.","        ","        Returns:","            List of Python file paths, sorted deterministically","        \"\"\"","        strategy_files = []","        ","        for search_path in self.search_paths:","            if not search_path.exists():","                continue","            ","            # Recursively find all .py files","            for py_file in search_path.rglob(\"*.py\"):","                # Skip __pycache__ and test files","                if \"__pycache__\" in str(py_file) or \"test_\" in py_file.name:","                    continue","                ","                strategy_files.append(py_file)","        ","        # Sort deterministically by absolute path","        strategy_files.sort(key=lambda p: str(p.absolute()))","        self._strategy_files = strategy_files","        return strategy_files","    ","    def extract_strategy_from_file(self, filepath: Path) -> Optional[StrategyRegistryEntry]:","        \"\"\"Extract strategy specification from a Python file.","        ","        Args:","            filepath: Path to Python file","            ","        Returns:","            StrategyRegistryEntry if file contains a valid strategy, None otherwise","        \"\"\"","        try:","            # Parse the file to find strategy definitions","            source_code = filepath.read_text(encoding='utf-8')","            tree = ast.parse(source_code)","            ","            # Look for StrategySpec definitions","            strategy_specs = self._find_strategy_specs(tree, source_code, filepath)","            ","            if not strategy_specs:","                return None","            ","            # For now, take the first strategy spec found","            # In the future, we might want to handle multiple strategies per file","            spec_name, spec_dict = strategy_specs[0]","            ","            # Compute content-addressed identity","            content_id = compute_strategy_id_from_file(filepath)","            identity = StrategyIdentityModel(","                strategy_id=content_id,","                source_hash=content_id","            )","            ","            # Extract metadata","            metadata = StrategyMetadata(","                name=spec_dict.get(\"strategy_id\", filepath.stem),","                version=spec_dict.get(\"version\", \"v1\"),","                description=f\"Strategy from {filepath.name}\",","                author=\"FishBroWFS_V2\",","                tags=[\"discovered\"]","            )","            ","            # Extract parameter schema","            param_schema = StrategyParamSchema(","                param_schema=spec_dict.get(\"param_schema\", {}),","                defaults=spec_dict.get(\"defaults\", {})","            )","            ","            return StrategyRegistryEntry(","                identity=identity,","                metadata=metadata,","                param_schema=param_schema,","                fn=None  # Function not available without importing","            )","            ","        except (SyntaxError, ValueError, OSError) as e:","            # Skip files with errors","            return None","    ","    def _find_strategy_specs(","        self, ","        tree: ast.AST, ","        source_code: str,","        filepath: Path","    ) -> List[Tuple[str, Dict]]:","        \"\"\"Find StrategySpec definitions in AST.","        ","        Args:","            tree: AST parsed from source code","            source_code: Original source code","            filepath: Path to source file","            ","        Returns:","            List of (variable_name, spec_dict) tuples","        \"\"\"","        specs = []","        ","        for node in ast.walk(tree):","            # Look for assignments like SPEC = StrategySpec(...)","            if isinstance(node, ast.Assign):","                for target in node.targets:","                    if isinstance(target, ast.Name):","                        var_name = target.id","                        # Check if assignment is to a variable that might be a strategy spec","                        if var_name.isupper():  # Convention: constants are uppercase","                            # Try to extract the StrategySpec constructor call","                            spec_dict = self._extract_strategy_spec(node.value)","                            if spec_dict:","                                specs.append((var_name, spec_dict))","        ","        return specs","    ","    def _extract_strategy_spec(self, node: ast.AST) -> Optional[Dict]:","        \"\"\"Extract strategy specification from AST node.","        ","        Args:","            node: AST node (should be a Call to StrategySpec)","            ","        Returns:","            Dictionary with strategy spec fields, or None if not a StrategySpec","        \"\"\"","        if not isinstance(node, ast.Call):","            return None","        ","        # Check if this is a StrategySpec constructor call","        func_name = self._get_function_name(node.func)","        if func_name != \"StrategySpec\":","            return None","        ","        # Extract keyword arguments","        spec_dict = {}","        ","        # Handle positional arguments (strategy_id, version, param_schema, defaults, fn)","        if len(node.args) >= 1:","            spec_dict[\"strategy_id\"] = self._extract_constant(node.args[0])","        if len(node.args) >= 2:","            spec_dict[\"version\"] = self._extract_constant(node.args[1])","        if len(node.args) >= 3:","            spec_dict[\"param_schema\"] = self._extract_dict(node.args[2])","        if len(node.args) >= 4:","            spec_dict[\"defaults\"] = self._extract_dict(node.args[3])","        # fn (5th arg) is a function reference, not extractable without importing","        ","        # Handle keyword arguments","        for kw in node.keywords:","            if kw.arg in [\"strategy_id\", \"version\", \"param_schema\", \"defaults\", \"content_id\"]:","                if kw.arg in [\"param_schema\", \"defaults\"]:","                    spec_dict[kw.arg] = self._extract_dict(kw.value)","                else:"]}
{"type":"file_chunk","path":"src/strategy/registry_builder.py","chunk_index":1,"line_start":201,"line_end":387,"content":["                    spec_dict[kw.arg] = self._extract_constant(kw.value)","        ","        return spec_dict if spec_dict else None","    ","    def _get_function_name(self, node: ast.AST) -> str:","        \"\"\"Get function name from AST node.\"\"\"","        if isinstance(node, ast.Name):","            return node.id","        elif isinstance(node, ast.Attribute):","            return node.attr","        elif isinstance(node, ast.Call):","            return self._get_function_name(node.func)","        return \"\"","    ","    def _extract_constant(self, node: ast.AST) -> Optional[str]:","        \"\"\"Extract constant value from AST node.\"\"\"","        if isinstance(node, ast.Constant):","            return str(node.value)","        elif isinstance(node, ast.Str):  # Python < 3.8","            return node.s","        elif isinstance(node, ast.Num):  # Python < 3.8","            return str(node.n)","        elif isinstance(node, ast.NameConstant):  # Python < 3.8","            return str(node.value)","        return None","    ","    def _extract_dict(self, node: ast.AST) -> Dict:","        \"\"\"Extract dictionary from AST node.\"\"\"","        if isinstance(node, ast.Dict):","            result = {}","            for key, value in zip(node.keys, node.values):","                key_str = self._extract_constant(key)","                if key_str is not None:","                    # Try to extract value as constant or dict","                    val = self._extract_constant(value)","                    if val is None:","                        val = self._extract_dict(value)","                    if val is not None:","                        result[key_str] = val","            return result","        return {}","","","class RegistryBuilder:","    \"\"\"Build strategy registry with content-addressed identity.\"\"\"","    ","    def __init__(self, search_paths: Optional[List[Path]] = None):","        \"\"\"Initialize registry builder.","        ","        Args:","            search_paths: List of directories to search for strategies.","                         If None, uses default strategy directories.","        \"\"\"","        if search_paths is None:","            # Default search paths","            base_dir = Path(__file__).parent.parent.parent","            self.search_paths = [","                base_dir / \"strategy\" / \"builtin\",","                base_dir / \"strategy\",","            ]","        else:","            self.search_paths = search_paths","        ","        self.discovery = StrategyDiscovery(self.search_paths)","        self.manifest: Optional[StrategyManifest] = None","    ","    def build_registry(self) -> StrategyManifest:","        \"\"\"Build strategy registry from discovered files.","        ","        Returns:","            StrategyManifest with all discovered strategies","        \"\"\"","        # Discover strategy files","        strategy_files = self.discovery.discover_strategy_files()","        ","        # Extract strategies from files","        entries = []","        content_ids: Set[str] = set()","        strategy_names: Set[str] = set()","        ","        for filepath in strategy_files:","            entry = self.discovery.extract_strategy_from_file(filepath)","            if entry is None:","                continue","            ","            # Check for duplicate content (different names, same logic)","            if entry.strategy_id in content_ids:","                print(f\"Warning: Duplicate content detected for {entry.metadata.name}\")","                continue","            ","            # Check for duplicate names (different content, same name)","            if entry.metadata.name in strategy_names:","                print(f\"Warning: Duplicate name detected: {entry.metadata.name}\")","                continue","            ","            content_ids.add(entry.strategy_id)","            strategy_names.add(entry.metadata.name)","            entries.append(entry)","        ","        # Create manifest","        self.manifest = StrategyManifest(strategies=entries)","        return self.manifest","    ","    def save_manifest(self, output_path: Path) -> None:","        \"\"\"Save strategy manifest to file.","        ","        Args:","            output_path: Path to save StrategyManifest.json","        \"\"\"","        if self.manifest is None:","            self.build_registry()","        ","        self.manifest.save(output_path)","        print(f\"Strategy manifest saved to {output_path}\")","        print(f\"Total strategies: {len(self.manifest.strategies)}\")","    ","    def load_builtin_strategies(self) -> None:","        \"\"\"Load built-in strategies into the runtime registry.","        ","        This is a convenience method that loads strategies using the","        existing registry API while ensuring content-addressed identity.","        \"\"\"","        from strategy.registry import load_builtin_strategies as load_builtin","        load_builtin()","        ","        # Verify that loaded strategies have content-addressed IDs","        from strategy.registry import list_strategies","        strategies = list_strategies()","        ","        for spec in strategies:","            if not spec.content_id or spec.content_id == \"\":","                print(f\"Warning: Strategy '{spec.strategy_id}' missing content_id\")","            else:","                print(f\"Strategy '{spec.strategy_id}' has content_id: {spec.content_id[:16]}...\")","","","def build_and_save_manifest(","    output_dir: Optional[Path] = None,","    filename: str = \"StrategyManifest.json\"",") -> Path:","    \"\"\"Convenience function to build and save strategy manifest.","    ","    Args:","        output_dir: Directory to save manifest (default: current directory)","        filename: Manifest filename","        ","    Returns:","        Path to saved manifest file","    \"\"\"","    if output_dir is None:","        output_dir = Path.cwd()","    ","    output_path = output_dir / filename","    ","    builder = RegistryBuilder()","    builder.save_manifest(output_path)","    ","    return output_path","","","if __name__ == \"__main__\":","    # Command-line interface","    import argparse","    ","    parser = argparse.ArgumentParser(","        description=\"Build strategy registry with content-addressed identity\"","    )","    parser.add_argument(","        \"--output\", \"-o\",","        type=Path,","        default=Path.cwd() / \"StrategyManifest.json\",","        help=\"Output path for StrategyManifest.json\"","    )","    parser.add_argument(","        \"--load-builtin\",","        action=\"store_true\",","        help=\"Load built-in strategies into runtime registry\"","    )","    ","    args = parser.parse_args()","    ","    if args.load_builtin:","        builder = RegistryBuilder()","        builder.load_builtin_strategies()","    ","    # Always build and save manifest","    build_and_save_manifest(args.output.parent, args.output.name)"]}
{"type":"file_footer","path":"src/strategy/registry_builder.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/strategy/runner.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3565,"sha256":"9d8de745583a9061a415936eb2623950463d2a1dbf3db79d6b099059d13479e1","total_lines":116,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/runner.py","chunk_index":0,"line_start":1,"line_end":116,"content":["","\"\"\"Strategy runner - adapter between strategy and engine.","","Phase 7: Validates params, calls strategy function, returns intents.","\"\"\"","","from __future__ import annotations","","import logging","from typing import Dict, Any, List","","from engine.types import OrderIntent","from strategy.registry import get","from strategy.spec import StrategySpec","","logger = logging.getLogger(__name__)","","","def run_strategy(","    strategy_id: str,","    features: Dict[str, Any],","    params: Dict[str, float],","    context: Dict[str, Any],",") -> List[OrderIntent]:","    \"\"\"Run a strategy and return order intents.","    ","    This function:","    1. Validates params (missing values use defaults, extra keys allowed but logged)","    2. Calls strategy function","    3. Returns intents (does NOT fill, does NOT compute indicators)","    ","    Args:","        strategy_id: Strategy identifier","        features: Features/indicators dict (e.g., {\"sma_fast\": array, \"sma_slow\": array})","        params: Strategy parameters dict (e.g., {\"fast_period\": 10, \"slow_period\": 20})","        context: Execution context (e.g., {\"bar_index\": 100, \"order_qty\": 1})","        ","    Returns:","        List of OrderIntent","        ","    Raises:","        KeyError: If strategy not found","        ValueError: If strategy output is invalid","    \"\"\"","    # Get strategy spec","    spec: StrategySpec = get(strategy_id)","    ","    # Merge context and features for strategy input","    strategy_input = {**context, \"features\": features}","    ","    # Validate and merge params with defaults","    validated_params = _validate_params(params, spec)","    ","    # Call strategy function","    result = spec.fn(strategy_input, validated_params)","    ","    # Validate output","    if not isinstance(result, dict):","        raise ValueError(f\"Strategy '{strategy_id}' must return dict, got {type(result)}\")","    ","    if \"intents\" not in result:","        raise ValueError(f\"Strategy '{strategy_id}' output must contain 'intents' key\")","    ","    intents = result[\"intents\"]","    if not isinstance(intents, list):","        raise ValueError(f\"Strategy '{strategy_id}' intents must be list, got {type(intents)}\")","    ","    # Validate each intent","    for i, intent in enumerate(intents):","        if not isinstance(intent, OrderIntent):","            raise ValueError(","                f\"Strategy '{strategy_id}' intent[{i}] must be OrderIntent, got {type(intent)}\"","            )","    ","    return intents","","","def _validate_params(params: Dict[str, float], spec: StrategySpec) -> Dict[str, float]:","    \"\"\"Validate and merge params with defaults.","    ","    Rules:","    - Missing params use defaults","    - Extra keys allowed but logged","    - Type validation (minimal)","    ","    Args:","        params: User-provided parameters","        spec: Strategy specification","        ","    Returns:","        Validated parameters dict (merged with defaults)","    \"\"\"","    validated = dict(spec.defaults)  # Start with defaults","    ","    # Override with user params","    for key, value in params.items():","        if key not in spec.defaults:","            # Extra key - log but allow","            logger.warning(","                f\"Strategy '{spec.strategy_id}': extra parameter '{key}' not in schema, \"","                f\"will be ignored\"","            )","            continue","        ","        # Type validation (minimal - just check it's numeric)","        if not isinstance(value, (int, float)):","            raise ValueError(","                f\"Strategy '{spec.strategy_id}': parameter '{key}' must be numeric, \"","                f\"got {type(value)}\"","            )","        ","        validated[key] = float(value)","    ","    return validated","",""]}
{"type":"file_footer","path":"src/strategy/runner.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/strategy/runner_single.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":1125,"sha256":"69f0d9f980b9b997f8f4a3224d40a612c629d925ac49698a205aa42950d053fb","total_lines":41,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/runner_single.py","chunk_index":0,"line_start":1,"line_end":41,"content":["","from __future__ import annotations","","from typing import Dict","","import numpy as np","","from data.layout import normalize_bars","from engine.types import BarArrays","from strategy.kernel import DonchianAtrParams, run_kernel","","","def run_single(","    open_: np.ndarray,","    high: np.ndarray,","    low: np.ndarray,","    close: np.ndarray,","    params: DonchianAtrParams,","    *,","    commission: float,","    slip: float,","    order_qty: int = 1,",") -> Dict[str, object]:","    \"\"\"","    Wrapper for Phase 3A (GKV): ensure memory layout + call kernel once.","    \"\"\"","    bars: BarArrays = normalize_bars(open_, high, low, close)","","    # Boundary Layout Check: enforce contiguous arrays before entering kernel.","    if not bars.open.flags[\"C_CONTIGUOUS\"]:","        bars = BarArrays(","            open=np.ascontiguousarray(bars.open, dtype=np.float64),","            high=np.ascontiguousarray(bars.high, dtype=np.float64),","            low=np.ascontiguousarray(bars.low, dtype=np.float64),","            close=np.ascontiguousarray(bars.close, dtype=np.float64),","        )","","    return run_kernel(bars, params, commission=commission, slip=slip, order_qty=order_qty)","","",""]}
{"type":"file_footer","path":"src/strategy/runner_single.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/strategy/spec.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5834,"sha256":"568516095be2add11ecbba681edb8ce7099f8fed5b0b7b7d1cb3a5833245353e","total_lines":160,"chunk_count":1}
{"type":"file_chunk","path":"src/strategy/spec.py","chunk_index":0,"line_start":1,"line_end":160,"content":["\"\"\"Strategy specification and function type definitions.","","Phase 7: Strategy system core data structures.","Phase 13: Enhanced with content-addressed identity (Attack #5).","\"\"\"","","from __future__ import annotations","","from dataclasses import dataclass","from typing import Callable, Dict, Any, Mapping, List, Optional","","from engine.types import OrderIntent","from core.ast_identity import (","    StrategyIdentity,","    compute_strategy_id_from_function,",")","","","# Strategy function signature:","# input: (context/features: dict, params: dict)","# output: {\"intents\": List[OrderIntent], \"debug\": dict}","StrategyFn = Callable[","    [Mapping[str, Any], Mapping[str, float]],  # (context/features, params)","    Mapping[str, Any]                          # {\"intents\": [...], \"debug\": {...}}","]","","","@dataclass(frozen=True)","class StrategySpec:","    \"\"\"Strategy specification with content-addressed identity.","    ","    Contains all metadata and function for a strategy.","    ","    Attributes:","        strategy_id: Unique strategy identifier (e.g., \"sma_cross\")","        version: Strategy version (e.g., \"v1\")","        param_schema: Parameter schema definition (jsonschema-like dict)","        defaults: Default parameter values (dict, key-value pairs)","        fn: Strategy function (StrategyFn)","        content_id: Content-addressed strategy ID (64-char hex, immutable)","        identity: Immutable strategy identity object (optional)","    \"\"\"","    strategy_id: str","    version: str","    param_schema: Dict[str, Any]  # jsonschema-like dict, minimal","    defaults: Dict[str, float]","    fn: StrategyFn","    content_id: Optional[str] = None","    identity: Optional[StrategyIdentity] = None","    ","    def __post_init__(self) -> None:","        \"\"\"Validate strategy spec and compute content-addressed identity.\"\"\"","        if not self.strategy_id:","            raise ValueError(\"strategy_id cannot be empty\")","        if not self.version:","            raise ValueError(\"version cannot be empty\")","        if not isinstance(self.param_schema, dict):","            raise ValueError(\"param_schema must be a dict\")","        if not isinstance(self.defaults, dict):","            raise ValueError(\"defaults must be a dict\")","        if not callable(self.fn):","            raise ValueError(\"fn must be callable\")","        ","        # Compute content-addressed identity if not provided","        if self.identity is None:","            try:","                # Compute from function source code","                content_id = compute_strategy_id_from_function(self.fn)","                identity = StrategyIdentity(content_id, source_hash=content_id)","                ","                # Use object.__setattr__ because dataclass is frozen","                object.__setattr__(self, 'identity', identity)","                object.__setattr__(self, 'content_id', content_id)","            except (ValueError, OSError) as e:","                # If we can't compute identity, use a placeholder","                # This maintains backward compatibility","                object.__setattr__(self, 'content_id', self.content_id or \"\")","        ","        # Validate content_id format if present","        if self.content_id and self.content_id != \"\":","            if len(self.content_id) != 64:","                raise ValueError(","                    f\"content_id must be 64-character hex string, got {self.content_id}\"","                )","            try:","                int(self.content_id, 16)","            except ValueError:","                raise ValueError(","                    f\"content_id must be valid hex string, got {self.content_id}\"","                )","    ","    @property","    def immutable_id(self) -> str:","        \"\"\"Get the immutable content-addressed ID.","        ","        Returns the content_id if available, otherwise falls back to","        a deterministic hash of strategy_id and version.","        \"\"\"","        if self.content_id and self.content_id != \"\":","            return self.content_id","        ","        # Fallback for backward compatibility","        import hashlib","        combined = f\"{self.strategy_id}::{self.version}\"","        return hashlib.sha256(combined.encode('utf-8')).hexdigest()","    ","    def get_identity(self) -> StrategyIdentity:","        \"\"\"Get the strategy identity object.","        ","        Returns the identity if available, otherwise creates a new one","        from the immutable_id.","        \"\"\"","        if self.identity is not None:","            return self.identity","        ","        # Create identity from immutable_id","        return StrategyIdentity(self.immutable_id)","    ","    def to_dict(self) -> Dict[str, Any]:","        \"\"\"Convert to dictionary for serialization.\"\"\"","        return {","            \"strategy_id\": self.strategy_id,","            \"version\": self.version,","            \"param_schema\": self.param_schema,","            \"defaults\": self.defaults,","            \"content_id\": self.content_id,","            # Note: fn is not serialized","        }","    ","    @classmethod","    def from_dict(","        cls,","        data: Dict[str, Any],","        fn: Optional[StrategyFn] = None","    ) -> StrategySpec:","        \"\"\"Create StrategySpec from dictionary.","        ","        Args:","            data: Dictionary with strategy specification","            fn: Strategy function (required if not in data)","            ","        Returns:","            StrategySpec instance","        \"\"\"","        # Extract function if provided in data (unlikely)","        if fn is None and \"fn\" in data:","            fn = data[\"fn\"]","        ","        if fn is None:","            raise ValueError(\"Strategy function (fn) is required\")","        ","        return cls(","            strategy_id=data[\"strategy_id\"],","            version=data[\"version\"],","            param_schema=data[\"param_schema\"],","            defaults=data[\"defaults\"],","            fn=fn,","            content_id=data.get(\"content_id\"),","            identity=None  # Will be computed in __post_init__","        )"]}
{"type":"file_footer","path":"src/strategy/spec.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/utils/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":345,"sha256":"69567893922142862763ca2c220cf398a13191220ae972ca0e902bcffa1a12a9","total_lines":20,"chunk_count":1}
{"type":"file_chunk","path":"src/utils/__init__.py","chunk_index":0,"line_start":1,"line_end":20,"content":["","\"\"\"Utility modules for \"\"\"","","from .write_scope import (","    WriteScope,","    create_plan_scope,","    create_plan_view_scope,","    create_plan_quality_scope,","    create_season_export_scope,",")","","__all__ = [","    \"WriteScope\",","    \"create_plan_scope\",","    \"create_plan_view_scope\",","    \"create_plan_quality_scope\",","    \"create_season_export_scope\",","]","",""]}
{"type":"file_footer","path":"src/utils/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"src/utils/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":398,"sha256":"b65bcde364bf80bbb489ff584ae877e9bf69db9c9139d9d06b844a67f9a8399c","note":"skipped by policy"}
{"type":"file_skipped","path":"src/utils/__pycache__/write_scope.cpython-312.pyc","reason":"cache","bytes":8037,"sha256":"e559c704611606928922944992da422a7b1cd8bfa00693e7273c3299b008a4ba","note":"skipped by policy"}
{"type":"file_header","path":"src/utils/fs_snapshot.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":3136,"sha256":"d7bf06f31063ca08cd71424fb9ad35150af2863923a162f73a925e0e303b52a2","total_lines":115,"chunk_count":1}
{"type":"file_chunk","path":"src/utils/fs_snapshot.py","chunk_index":0,"line_start":1,"line_end":115,"content":["","\"\"\"File system snapshot utilities for hardening tests.","","Provides deterministic snapshot of file trees with mtime and SHA256.","\"\"\"","from __future__ import annotations","","import hashlib","from dataclasses import dataclass","from pathlib import Path","from typing import Dict, Iterable, Optional","","","@dataclass(frozen=True)","class FileSnap:","    \"\"\"Immutable snapshot of a single file.\"\"\"","    rel_path: str  # POSIX-style relative path using '/'","    size: int","    mtime_ns: int","    sha256: str","","","def compute_sha256(path: Path) -> str:","    \"\"\"Compute SHA256 hash of file content.","    ","    Uses existing compute_sha256 from control.artifacts if available,","    otherwise implements directly.","    \"\"\"","    try:","        from control.artifacts import compute_sha256 as cs","        return cs(path.read_bytes())","    except ImportError:","        # Fallback implementation","        return hashlib.sha256(path.read_bytes()).hexdigest()","","","def snapshot_tree(root: Path, *, include_sha256: bool = True) -> Dict[str, FileSnap]:","    \"\"\"","    Deterministic snapshot of all files under root.","    ","    Args:","        root: Directory root to snapshot.","        include_sha256: Whether to compute SHA256 hash (expensive for large files).","    ","    Returns:","        Dictionary mapping relative path (POSIX-style) to FileSnap.","        Paths are sorted in stable alphabetical order.","    \"\"\"","    snapshots: Dict[str, FileSnap] = {}","    ","    # Walk through all files recursively","    for file_path in sorted(root.rglob(\"*\")):","        if not file_path.is_file():","            continue","        ","        # Get relative path and convert to POSIX style","        rel_path = file_path.relative_to(root).as_posix()","        ","        # Get file stats","        stat = file_path.stat()","        size = stat.st_size","        mtime_ns = stat.st_mtime_ns","        ","        # Compute SHA256 if requested","        sha256 = \"\"","        if include_sha256:","            sha256 = compute_sha256(file_path)","        ","        snapshots[rel_path] = FileSnap(","            rel_path=rel_path,","            size=size,","            mtime_ns=mtime_ns,","            sha256=sha256,","        )","    ","    return snapshots","","","def diff_snap(a: Dict[str, FileSnap], b: Dict[str, FileSnap]) -> dict:","    \"\"\"","    Compare two snapshots and return differences.","    ","    Args:","        a: First snapshot.","        b: Second snapshot.","    ","    Returns:","        Dictionary with keys:","          - added: list of paths present in b but not in a","          - removed: list of paths present in a but not in b","          - changed: list of paths present in both but with different","                     size, mtime_ns, or sha256","    \"\"\"","    a_keys = set(a.keys())","    b_keys = set(b.keys())","    ","    added = sorted(b_keys - a_keys)","    removed = sorted(a_keys - b_keys)","    ","    changed = []","    for key in sorted(a_keys & b_keys):","        snap_a = a[key]","        snap_b = b[key]","        if (snap_a.size != snap_b.size or ","            snap_a.mtime_ns != snap_b.mtime_ns or ","            snap_a.sha256 != snap_b.sha256):","            changed.append(key)","    ","    return {","        \"added\": added,","        \"removed\": removed,","        \"changed\": changed,","    }","",""]}
{"type":"file_footer","path":"src/utils/fs_snapshot.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/utils/manifest_verify.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":17085,"sha256":"5166f5dba49f1cdb670aa502bde64adcc28993c49a45faa1734be8f30626865a","total_lines":456,"chunk_count":3}
{"type":"file_chunk","path":"src/utils/manifest_verify.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"Manifest Tree Completeness verification tool.","","This module provides functions to verify the integrity and completeness","of manifest trees for tamper-proof sealing.","\"\"\"","","from __future__ import annotations","","import json","import hashlib","from pathlib import Path","from typing import Dict, List, Set, Tuple, Optional, Any","from dataclasses import dataclass","","from control.artifacts import compute_sha256, canonical_json_bytes","from core.schemas.manifest import UnifiedManifest","","","@dataclass","class VerificationResult:","    \"\"\"Result of manifest verification.\"\"\"","    is_valid: bool","    errors: List[str]","    warnings: List[str]","    manifest_type: str","    manifest_id: str","","","class ManifestVerifier:","    \"\"\"Verifies manifest tree completeness and integrity.\"\"\"","    ","    def __init__(self, root_dir: Path):","        \"\"\"","        Initialize verifier with root directory.","        ","        Args:","            root_dir: Root directory containing manifests to verify","        \"\"\"","        self.root_dir = root_dir.resolve()","        self.allowed_extensions = {'.json', '.txt', '.csv', '.parquet', '.feather', '.png', '.jpg', '.jpeg'}","    ","    def verify_manifest_file(self, manifest_path: Path) -> VerificationResult:","        \"\"\"","        Verify a single manifest file.","        ","        Args:","            manifest_path: Path to manifest file","            ","        Returns:","            VerificationResult with validation status","        \"\"\"","        errors = []","        warnings = []","        ","        try:","            # Read and parse manifest","            manifest_bytes = manifest_path.read_bytes()","            manifest_dict = json.loads(manifest_bytes.decode('utf-8'))","            ","            # Validate against unified schema","            try:","                manifest = UnifiedManifest(**manifest_dict)","            except Exception as e:","                errors.append(f\"Schema validation failed: {e}\")","                return VerificationResult(","                    is_valid=False,","                    errors=errors,","                    warnings=warnings,","                    manifest_type=\"unknown\",","                    manifest_id=\"unknown\"","                )","            ","            # Verify manifest self-hash","            if not self._verify_self_hash(manifest_dict, manifest_bytes):","                errors.append(\"Manifest self-hash verification failed\")","            ","            # Verify referenced files exist and match checksums","            file_errors = self._verify_referenced_files(manifest_path.parent, manifest_dict)","            errors.extend(file_errors)","            ","            # Check for completeness (all files in directory are accounted for)","            completeness_errors = self._verify_directory_completeness(manifest_path.parent, manifest_dict)","            errors.extend(completeness_errors)","            ","            return VerificationResult(","                is_valid=len(errors) == 0,","                errors=errors,","                warnings=warnings,","                manifest_type=manifest.manifest_type,","                manifest_id=manifest.id","            )","            ","        except Exception as e:","            errors.append(f\"Failed to read/parse manifest: {e}\")","            return VerificationResult(","                is_valid=False,","                errors=errors,","                warnings=warnings,","                manifest_type=\"unknown\",","                manifest_id=\"unknown\"","            )","    ","    def _verify_self_hash(self, manifest_dict: Dict[str, Any], manifest_bytes: bytes) -> bool:","        \"\"\"Verify manifest's self-hash (manifest_sha256 field).\"\"\"","        if 'manifest_sha256' not in manifest_dict:","            return False","        ","        # Remove the hash field before computing","        manifest_without_hash = dict(manifest_dict)","        manifest_without_hash.pop('manifest_sha256', None)","        ","        # Compute canonical JSON","        canonical_bytes = canonical_json_bytes(manifest_without_hash)","        computed_hash = compute_sha256(canonical_bytes)","        ","        return computed_hash == manifest_dict['manifest_sha256']","    ","    def _verify_referenced_files(self, base_dir: Path, manifest_dict: Dict[str, Any]) -> List[str]:","        \"\"\"Verify that all referenced files exist and match their checksums.\"\"\"","        errors = []","        ","        # Check files in checksums fields","        checksum_fields = ['checksums', 'export_checksums', 'plan_checksums', ","                          'view_checksums', 'quality_checksums']","        ","        for field in checksum_fields:","            if field in manifest_dict and isinstance(manifest_dict[field], dict):","                checksums = manifest_dict[field]","                for filename, expected_hash in checksums.items():","                    file_path = base_dir / filename","                    if not file_path.exists():","                        errors.append(f\"Referenced file not found: {filename}\")","                        continue","                    ","                    # Compute file hash","                    try:","                        file_hash = compute_sha256(file_path.read_bytes())","                        if file_hash != expected_hash:","                            errors.append(f\"Hash mismatch for {filename}: expected {expected_hash}, got {file_hash}\")","                    except Exception as e:","                        errors.append(f\"Failed to compute hash for {filename}: {e}\")","        ","        return errors","    ","    def _verify_directory_completeness(self, dir_path: Path, manifest_dict: Dict[str, Any]) -> List[str]:","        \"\"\"","        Verify that all files in the directory are accounted for in the manifest.","        ","        This ensures tamper-proof sealing: any file added, removed, or modified","        without updating the manifest will cause verification to fail.","        \"\"\"","        errors = []","        ","        # Get all files in directory (excluding temporary files and manifests)","        all_files = set()","        for file_path in dir_path.iterdir():","            if file_path.is_file():","                # Skip temporary files and .tmp files","                if file_path.suffix == '.tmp' or file_path.name.startswith('.'):","                    continue","                # Skip manifest files themselves (they're verified separately)","                if 'manifest' in file_path.name.lower():","                    continue","                all_files.add(file_path.name)","        ","        # Get files referenced in manifest","        referenced_files = set()","        ","        # Add files from checksums fields","        checksum_fields = ['checksums', 'export_checksums', 'plan_checksums', ","                          'view_checksums', 'quality_checksums']","        ","        for field in checksum_fields:","            if field in manifest_dict and isinstance(manifest_dict[field], dict):","                referenced_files.update(manifest_dict[field].keys())","        ","        # Check for files in directory not referenced in manifest","        unreferenced = all_files - referenced_files","        if unreferenced:","            errors.append(f\"Files in directory not referenced in manifest: {sorted(unreferenced)}\")","        ","        # Check for files referenced in manifest but not in directory","        missing = referenced_files - all_files","        if missing:","            errors.append(f\"Files referenced in manifest but not found in directory: {sorted(missing)}\")","        ","        return errors","    ","    def verify_manifest_tree(self, start_path: Optional[Path] = None) -> List[VerificationResult]:","        \"\"\"","        Recursively verify all manifests in a directory tree.","        ","        Args:","            start_path: Starting directory (defaults to root_dir)","            ","        Returns:","            List of verification results for all manifests found","        \"\"\"","        if start_path is None:"]}
{"type":"file_chunk","path":"src/utils/manifest_verify.py","chunk_index":1,"line_start":201,"line_end":400,"content":["            start_path = self.root_dir","        ","        results = []","        ","        # Look for manifest files","        manifest_patterns = ['*manifest*.json', 'manifest*.json', '*_manifest.json']","        ","        for pattern in manifest_patterns:","            for manifest_path in start_path.rglob(pattern):","                # Skip if not a file or in excluded directories","                if not manifest_path.is_file():","                    continue","                ","                # Skip temporary files","                if manifest_path.suffix == '.tmp' or manifest_path.name.startswith('.'):","                    continue","                ","                result = self.verify_manifest_file(manifest_path)","                results.append(result)","        ","        return results","","","def verify_manifest(manifest_path: str | Path) -> VerificationResult:","    \"\"\"","    Convenience function to verify a single manifest file.","    ","    Args:","        manifest_path: Path to manifest file","        ","    Returns:","        VerificationResult","    \"\"\"","    verifier = ManifestVerifier(Path(manifest_path).parent)","    return verifier.verify_manifest_file(Path(manifest_path))","","","def verify_directory(dir_path: str | Path) -> List[VerificationResult]:","    \"\"\"","    Convenience function to verify all manifests in a directory.","    ","    Args:","        dir_path: Directory to scan for manifests","        ","    Returns:","        List of VerificationResult objects","    \"\"\"","    verifier = ManifestVerifier(Path(dir_path))","    return verifier.verify_manifest_tree()","","","def print_verification_results(results: List[VerificationResult]) -> None:","    \"\"\"Print verification results in a readable format.\"\"\"","    total = len(results)","    valid = sum(1 for r in results if r.is_valid)","    ","    print(f\"=== Manifest Verification Results ===\")","    print(f\"Total manifests: {total}\")","    print(f\"Valid: {valid}\")","    print(f\"Invalid: {total - valid}\")","    print()","    ","    for i, result in enumerate(results, 1):","        status = \"✓ PASS\" if result.is_valid else \"✗ FAIL\"","        print(f\"{i}. {status} - {result.manifest_type} ({result.manifest_id})\")","        ","        if result.errors:","            print(f\"   Errors:\")","            for error in result.errors:","                print(f\"     - {error}\")","        ","        if result.warnings:","            print(f\"   Warnings:\")","            for warning in result.warnings:","                print(f\"     - {warning}\")","        ","        print()","","","def compute_files_listing(root_dir: Path, allowed_scope: Optional[List[str]] = None) -> List[Dict[str, str]]:","    \"\"\"","    Compute listing of all files in directory with SHA256 checksums.","    ","    Args:","        root_dir: Root directory to scan","        allowed_scope: Optional list of relative paths to include. If None, include all files.","        ","    Returns:","        List of dicts with keys \"rel_path\" and \"sha256\", sorted by rel_path asc.","    \"\"\"","    files = []","    ","    for file_path in root_dir.iterdir():","        if not file_path.is_file():","            continue","        ","        # Skip temporary files and hidden files","        if file_path.suffix == '.tmp' or file_path.name.startswith('.'):","            continue","        ","        # Skip manifest files themselves (they are the metadata, not part of the content)","        if 'manifest' in file_path.name.lower() and file_path.suffix in ('.json', '.yaml', '.yml'):","            continue","        ","        rel_path = file_path.name","        ","        # If allowed_scope is provided, filter by it","        if allowed_scope is not None and rel_path not in allowed_scope:","            continue","        ","        # Compute SHA256","        try:","            file_hash = compute_sha256(file_path.read_bytes())","        except Exception:","            # Skip files that cannot be read","            continue","        ","        files.append({","            \"rel_path\": rel_path,","            \"sha256\": file_hash","        })","    ","    # Sort by rel_path ascending","    files.sort(key=lambda x: x[\"rel_path\"])","    return files","","","def compute_files_sha256(files_listing: List[Dict[str, str]]) -> str:","    \"\"\"","    Compute combined SHA256 of all files by concatenating their individual hashes.","    ","    Args:","        files_listing: List of dicts with \"rel_path\" and \"sha256\"","        ","    Returns:","        SHA256 hex string of concatenated hashes (sorted by rel_path)","    \"\"\"","    # Ensure sorted by rel_path","    sorted_files = sorted(files_listing, key=lambda x: x[\"rel_path\"])","    ","    # Concatenate all SHA256 strings","    concatenated = \"\".join(f[\"sha256\"] for f in sorted_files)","    ","    # Compute SHA256 of the concatenated string (as UTF-8 bytes)","    return hashlib.sha256(concatenated.encode(\"utf-8\")).hexdigest()","","","def verify_manifest_completeness(root_dir: Path, manifest_dict: Dict[str, Any]) -> None:","    \"\"\"","    Verify manifest completeness and integrity.","    ","    Validates:","    1. Files listing matches exactly (no extra/missing files)","    2. Each file's SHA256 matches","    3. files_sha256 field is correct","    4. manifest_sha256 field is correct","    ","    Args:","        root_dir: Directory containing the files","        manifest_dict: Parsed manifest JSON as dict","        ","    Raises:","        ValueError: If any verification fails","    \"\"\"","    errors = []","    ","    # 1. Verify files listing exists","    if \"files\" not in manifest_dict:","        raise ValueError(\"Manifest missing 'files' field\")","    ","    manifest_files = manifest_dict.get(\"files\", [])","    if not isinstance(manifest_files, list):","        raise ValueError(\"Manifest 'files' must be a list\")","    ","    # Convert to dict for easier lookup","    manifest_file_map = {f[\"rel_path\"]: f[\"sha256\"] for f in manifest_files if isinstance(f, dict) and \"rel_path\" in f and \"sha256\" in f}","    ","    # 2. Compute actual files listing (include all files, not just those in manifest)","    # This ensures we detect extra files added to the directory","    actual_files = compute_files_listing(root_dir, allowed_scope=None)","    actual_file_map = {f[\"rel_path\"]: f[\"sha256\"] for f in actual_files}","    ","    # Check for missing files in manifest","    missing_in_manifest = set(actual_file_map.keys()) - set(manifest_file_map.keys())","    if missing_in_manifest:","        errors.append(f\"Files in directory not in manifest: {sorted(missing_in_manifest)}\")","    ","    # Check for extra files in manifest not in directory","    extra_in_manifest = set(manifest_file_map.keys()) - set(actual_file_map.keys())","    if extra_in_manifest:","        errors.append(f\"Files in manifest not found in directory: {sorted(extra_in_manifest)}\")","    ","    # 3. Verify SHA256 matches for common files","    common = set(manifest_file_map.keys()) & set(actual_file_map.keys())","    for rel_path in common:","        if manifest_file_map[rel_path] != actual_file_map[rel_path]:","            errors.append(f\"SHA256 mismatch for {rel_path}: manifest={manifest_file_map[rel_path]}, actual={actual_file_map[rel_path]}\")","    ","    # 4. Verify files_sha256 if present","    if \"files_sha256\" in manifest_dict:"]}
{"type":"file_chunk","path":"src/utils/manifest_verify.py","chunk_index":2,"line_start":401,"line_end":456,"content":["        expected_files_sha256 = manifest_dict[\"files_sha256\"]","        computed_files_sha256 = compute_files_sha256(actual_files)","        if expected_files_sha256 != computed_files_sha256:","            errors.append(f\"files_sha256 mismatch: expected {expected_files_sha256}, computed {computed_files_sha256}\")","    ","    # 5. Verify manifest_sha256 if present","    if \"manifest_sha256\" in manifest_dict:","        # Create copy without manifest_sha256 field","        manifest_without_hash = dict(manifest_dict)","        manifest_without_hash.pop(\"manifest_sha256\", None)","        ","        # Compute canonical JSON hash","        canonical_bytes = canonical_json_bytes(manifest_without_hash)","        computed_hash = compute_sha256(canonical_bytes)","        ","        if manifest_dict[\"manifest_sha256\"] != computed_hash:","            errors.append(f\"manifest_sha256 mismatch: expected {manifest_dict['manifest_sha256']}, computed {computed_hash}\")","    ","    if errors:","        raise ValueError(\"Manifest verification failed:\\n\" + \"\\n\".join(f\"  - {e}\" for e in errors))","","","def verify_manifest(root_dir: str | Path, manifest_json: dict | str | Path) -> None:","    \"\"\"","    Verify manifest completeness and integrity (task‑required signature).","    ","    Args:","        root_dir: Directory containing the files","        manifest_json: Either a dict of parsed manifest, or a path to manifest file,","                      or a string of JSON content.","    ","    Raises:","        ValueError: If verification fails","    \"\"\"","    root_dir = Path(root_dir)","    ","    # Parse manifest_json based on its type","    if isinstance(manifest_json, dict):","        manifest_dict = manifest_json","    elif isinstance(manifest_json, (str, Path)):","        path = Path(manifest_json)","        if path.exists():","            manifest_dict = json.loads(path.read_text(encoding=\"utf-8\"))","        else:","            # Try to parse as JSON string","            try:","                manifest_dict = json.loads(manifest_json)","            except json.JSONDecodeError:","                raise ValueError(f\"manifest_json is not a valid file path or JSON string: {manifest_json}\")","    else:","        raise TypeError(f\"manifest_json must be dict, str, or Path, got {type(manifest_json)}\")","    ","    # Delegate to verify_manifest_completeness","    verify_manifest_completeness(root_dir, manifest_dict)","",""]}
{"type":"file_footer","path":"src/utils/manifest_verify.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"src/utils/write_scope.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":8209,"sha256":"fcde49e3ac840bc2ff86089d7bf2a53130cf0302c5caf05c20c9c2d3e0123727","total_lines":233,"chunk_count":2}
{"type":"file_chunk","path":"src/utils/write_scope.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Write‑scope guard for hardening file‑write boundaries.","","This module provides a runtime fence that ensures writers only produce files","under a designated root directory and whose relative paths match a predefined","allow‑list (exact matches or prefix‑based patterns).  Any attempt to write","outside the allowed set raises a ValueError before the actual I/O occurs.","","The guard is designed to be used inside each writer function that writes","portfolio‑related outputs (plan_, plan_view_, plan_quality_, etc.) and","season‑export outputs.","","Design notes","------------","• Path.resolve() is used to detect symlink escapes, but we rely on","  resolved_target.is_relative_to(resolved_root) (Python ≥3.12) to guarantee","  the final target stays under the logical root.","• Prefix matching is performed on the basename only, not on the whole relative","  path.  This prevents subdirectories like `subdir/plan_foo.json` from slipping","  through unless the prefix pattern explicitly allows subdirectories (which we","  currently do not).","• The guard does **not** create directories; it only validates the relative","  path.  The caller is responsible for creating parent directories if needed.","\"\"\"","","from __future__ import annotations","","import os","from dataclasses import dataclass","from pathlib import Path","from typing import Iterable","","","@dataclass(frozen=True)","class WriteScope:","    \"\"\"Immutable guard that validates relative paths against a whitelist.","","    Attributes","    ----------","    root_dir : Path","        Absolute path to the directory under which all writes must stay.","    allowed_rel_files : frozenset[str]","        Set of exact relative paths (POSIX style, no leading slash, no `..`)","        that are permitted.","    allowed_rel_prefixes : tuple[str, ...]","        Tuple of filename prefixes.  A relative path is allowed if its","        basename starts with any of these prefixes.","    \"\"\"","","    root_dir: Path","    allowed_rel_files: frozenset[str]          # exact files","    allowed_rel_prefixes: tuple[str, ...]      # prefix patterns (e.g. \"plan_\", \"plan_view_\")","","    def assert_allowed_rel(self, rel: str) -> None:","        \"\"\"Raise ValueError if `rel` is not allowed by this scope.","","        Parameters","        ----------","        rel : str","            Relative path (POSIX style, no leading slash, no `..`).","","        Raises","        ------","        ValueError","            With a descriptive message if the path is not allowed or attempts","            to escape the root directory.","        \"\"\"","        # 1. Basic sanity: must be a relative POSIX path without `..` components.","        if os.path.isabs(rel):","            raise ValueError(f\"Relative path must not be absolute: {rel!r}\")","        if \"..\" in rel.split(\"/\"):","            raise ValueError(f\"Relative path must not contain '..': {rel!r}\")","","        # 2. Ensure the final resolved target stays under root_dir.","        target = (self.root_dir / rel).resolve()","        root_resolved = self.root_dir.resolve()","        # Python 3.12+ provides Path.is_relative_to; we use it if available,","        # otherwise fall back to a manual check.","        try:","            if not target.is_relative_to(root_resolved):","                raise ValueError(","                    f\"Path {rel!r} resolves to {target} which is outside the \"","                    f\"scope root {root_resolved}\"","                )","        except AttributeError:","            # Python <3.12: compare parents manually.","            try:","                target.relative_to(root_resolved)","            except ValueError:","                raise ValueError(","                    f\"Path {rel!r} resolves to {target} which is outside the \"","                    f\"scope root {root_resolved}\"","                )","","        # 3. Check for wildcard prefix \"*\" which allows any file under root_dir","        if \"*\" in self.allowed_rel_prefixes:","            return","","        # 4. Check exact matches first.","        if rel in self.allowed_rel_files:","            return","","        # 5. Check prefix matches on the basename.","        basename = os.path.basename(rel)","        for prefix in self.allowed_rel_prefixes:","            if basename.startswith(prefix):","                return","","        # 6. If we reach here, the path is forbidden.","        raise ValueError(","            f\"Relative path {rel!r} is not allowed by this write scope.\\n\"","            f\"Allowed exact files: {sorted(self.allowed_rel_files)}\\n\"","            f\"Allowed filename prefixes: {self.allowed_rel_prefixes}\"","        )","","","def create_plan_scope(plan_dir: Path) -> WriteScope:","    \"\"\"Create a WriteScope for a portfolio plan directory.","","    This scope permits the standard plan‑manifest files and any future file","    whose basename starts with `plan_`.","","    Exact allowed files:","        portfolio_plan.json","        plan_manifest.json","        plan_metadata.json","        plan_checksums.json","","    Allowed prefixes:","        (\"plan_\",)","    \"\"\"","    return WriteScope(","        root_dir=plan_dir,","        allowed_rel_files=frozenset({","            \"portfolio_plan.json\",","            \"plan_manifest.json\",","            \"plan_metadata.json\",","            \"plan_checksums.json\",","        }),","        allowed_rel_prefixes=(\"plan_\",),","    )","","","def create_plan_view_scope(view_dir: Path) -> WriteScope:","    \"\"\"Create a WriteScope for a plan‑view directory.","","    Exact allowed files:","        plan_view.json","        plan_view.md","        plan_view_checksums.json","        plan_view_manifest.json","","    Allowed prefixes:","        (\"plan_view_\",)","    \"\"\"","    return WriteScope(","        root_dir=view_dir,","        allowed_rel_files=frozenset({","            \"plan_view.json\",","            \"plan_view.md\",","            \"plan_view_checksums.json\",","            \"plan_view_manifest.json\",","        }),","        allowed_rel_prefixes=(\"plan_view_\",),","    )","","","def create_plan_quality_scope(quality_dir: Path) -> WriteScope:","    \"\"\"Create a WriteScope for a plan‑quality directory.","","    Exact allowed files:","        plan_quality.json","        plan_quality_checksums.json","        plan_quality_manifest.json","","    Allowed prefixes:","        (\"plan_quality_\",)","    \"\"\"","    return WriteScope(","        root_dir=quality_dir,","        allowed_rel_files=frozenset({","            \"plan_quality.json\",","            \"plan_quality_checksums.json\",","            \"plan_quality_manifest.json\",","        }),","        allowed_rel_prefixes=(\"plan_quality_\",),","    )","","","def create_season_export_scope(export_root: Path) -> WriteScope:","    \"\"\"Create a WriteScope for season‑export outputs.","","    This scope allows any file under exports_root / seasons / {season} / **","    but forbids any path that would escape to outputs/artifacts/** or","    outputs/season_index/** or any other repo root paths.","","    The export_root parameter should be the season directory:","        exports_root / seasons / {season}",""]}
{"type":"file_chunk","path":"src/utils/write_scope.py","chunk_index":1,"line_start":201,"line_end":233,"content":["    Allowed prefixes:","        ()   (none – we allow any file under the export_root)","    \"\"\"","    # Ensure export_root is under the exports tree","    exports_root = Path(os.environ.get(\"FISHBRO_EXPORTS_ROOT\", \"outputs/exports\"))","    if not export_root.is_relative_to(exports_root):","        raise ValueError(","            f\"export_root {export_root} must be under exports root {exports_root}\"","        )","    ","    # Ensure export_root follows the pattern exports_root / seasons / {season}","    try:","        relative_to_exports = export_root.relative_to(exports_root)","        parts = relative_to_exports.parts","        if len(parts) < 2 or parts[0] != \"seasons\":","            raise ValueError(","                f\"export_root must be under exports_root/seasons/{{season}}, got {relative_to_exports}\"","            )","    except ValueError:","        raise ValueError(","            f\"export_root {export_root} must be under exports root {exports_root}\"","        )","    ","    # Allow any file under export_root (empty allowed_rel_files means no exact matches required,","    # empty allowed_rel_prefixes means no prefix restriction, but we need to allow all files)","    # We'll use a special prefix \"*\" to indicate allow all (handled in assert_allowed_rel)","    return WriteScope(","        root_dir=export_root,","        allowed_rel_files=frozenset(),  # No exact matches required","        allowed_rel_prefixes=(\"*\",),    # Allow any file under export_root","    )","",""]}
{"type":"file_footer","path":"src/utils/write_scope.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"src/version.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":26,"sha256":"c92496c594926731f799186bf10921780b2dcfdc54ff18b2488847aff30e60c2","total_lines":5,"chunk_count":1}
{"type":"file_chunk","path":"src/version.py","chunk_index":0,"line_start":1,"line_end":5,"content":["","__version__ = \"0.1.0\"","","",""]}
{"type":"file_footer","path":"src/version.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"src/wfs/runner.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":4239,"sha256":"b0b5b9ddbfc7b47e9aa8167034da46ad08731a2ccc887d546810240c729a7a4c","total_lines":144,"chunk_count":1}
{"type":"file_chunk","path":"src/wfs/runner.py","chunk_index":0,"line_start":1,"line_end":144,"content":["","\"\"\"","WFS Runner - 接受 FeatureBundle 並執行策略的入口點","","Phase 4.1: 新增 run_wfs_with_features API，讓 Research Runner 可以注入特徵。","\"\"\"","","from __future__ import annotations","","import logging","from typing import Dict, Any, Optional","","from core.feature_bundle import FeatureBundle","from strategy.runner import run_strategy","from strategy.registry import get as get_strategy_spec","","logger = logging.getLogger(__name__)","","","def run_wfs_with_features(","    *,","    strategy_id: str,","    feature_bundle: FeatureBundle,","    config: Optional[dict] = None,",") -> dict:","    \"\"\"","    WFS entrypoint that consumes FeatureBundle only.","","    行為規格：","    1. 不得自行計算特徵（全部來自 feature_bundle）","    2. 不得讀取 TXT / bars / features 檔案","    3. 使用策略的預設參數（或 config 中提供的參數）","    4. 執行策略並產生 intents","    5. 執行引擎模擬（如果需要的話）","    6. 回傳摘要字典（不含大量數據）","","    Args:","        strategy_id: 策略 ID","        feature_bundle: 特徵資料包","        config: 配置字典，可包含 params, context 等（可選）","","    Returns:","        摘要字典，至少包含：","            - strategy_id","            - dataset_id","            - season","            - intents_count","            - fills_count","            - net_profit (如果可計算)","            - trades","            - max_dd","    \"\"\"","    if config is None:","        config = {}","","    # 1. 從 feature_bundle 建立 features dict","    features = _extract_features_dict(feature_bundle)","","    # 2. 取得策略參數（優先使用 config 中的 params，否則使用預設值）","    params = config.get(\"params\", {})","    if not params:","        # 使用策略的預設參數","        spec = get_strategy_spec(strategy_id)","        params = spec.defaults","","    # 3. 建立 context（預設值）","    context = config.get(\"context\", {})","    if \"bar_index\" not in context:","        # 假設從第一個 bar 開始","        context[\"bar_index\"] = 0","    if \"order_qty\" not in context:","        context[\"order_qty\"] = 1","","    # 4. 執行策略，產生 intents","    try:","        intents = run_strategy(","            strategy_id=strategy_id,","            features=features,","            params=params,","            context=context,","        )","    except Exception as e:","        logger.error(f\"策略執行失敗: {e}\")","        raise RuntimeError(f\"策略 {strategy_id} 執行失敗: {e}\") from e","","    # 5. 執行引擎模擬（簡化版本，僅回傳基本摘要）","    # 注意：這裡我們不實際模擬，因為 Phase 4.1 只要求介面。","    # 我們回傳一個模擬的摘要，後續階段再實作完整的模擬。","    summary = _simulate_intents(intents, feature_bundle, config)","","    # 6. 加入 metadata","    summary.update({","        \"strategy_id\": strategy_id,","        \"dataset_id\": feature_bundle.dataset_id,","        \"season\": feature_bundle.season,","        \"intents_count\": len(intents),","        \"features_used\": list(features.keys()),","    })","","    return summary","","","def _extract_features_dict(feature_bundle: FeatureBundle) -> Dict[str, Any]:","    \"\"\"","    從 FeatureBundle 提取特徵字典，格式為 {name: values_array}","    \"\"\"","    features = {}","    for series in feature_bundle.series.values():","        features[series.name] = series.values","    return features","","","def _simulate_intents(intents, feature_bundle: FeatureBundle, config: dict) -> dict:","    \"\"\"","    模擬 intents 並計算基本 metrics（簡化版本）","","    目前回傳固定值，後續階段應整合真正的引擎模擬。","    \"\"\"","    # 如果沒有 intents，回傳零值","    if not intents:","        return {","            \"fills_count\": 0,","            \"net_profit\": 0.0,","            \"trades\": 0,","            \"max_dd\": 0.0,","            \"simulation\": \"stub\",","        }","","    # 簡化：假設每個 intent 產生一個 fill，且每個 fill 的 profit 為 0","    # 實際應呼叫 engine.simulate","    fills_count = len(intents) // 2  # 假設每個 entry 對應一個 exit","    net_profit = 0.0","    trades = fills_count","    max_dd = 0.0","","    return {","        \"fills_count\": fills_count,","        \"net_profit\": net_profit,","        \"trades\": trades,","        \"max_dd\": max_dd,","        \"simulation\": \"stub\",","    }","",""]}
{"type":"file_footer","path":"src/wfs/runner.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/__init__.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":128,"sha256":"f2096ea4613e41193f1a7f5c65503720ac7b21743cdade0bc3345e6bdf920e11","total_lines":9,"chunk_count":1}
{"type":"file_chunk","path":"tests/__init__.py","chunk_index":0,"line_start":1,"line_end":9,"content":["","\"\"\"","Tests package for ","","This package allows tests to import from each other using:","    from tests.test_module import ...","\"\"\"","",""]}
{"type":"file_footer","path":"tests/__init__.py","complete":true,"emitted_chunks":1}
{"type":"file_skipped","path":"tests/__pycache__/__init__.cpython-312.pyc","reason":"cache","bytes":277,"sha256":"a7ae0906c9d9025f1461cf4c6d35bc53ef28f2003a5e57f8d46fb668e84d7977","note":"skipped by policy"}
{"type":"file_skipped","path":"tests/__pycache__/conftest.cpython-312-pytest-8.4.2.pyc","reason":"cache","bytes":4741,"sha256":"f942ad0aafaf9aa952a995502ad78672da1bfbd023aeed58ea038872d3afc1b9","note":"skipped by policy"}
{"type":"file_skipped","path":"tests/__pycache__/test_control_api_smoke.cpython-312-pytest-8.4.2.pyc","reason":"cache","bytes":27094,"sha256":"7ac11549ed31501d622f4076bcf35cc1162719f6be752e562a1d3722dfd65447","note":"skipped by policy"}
{"type":"file_header","path":"tests/boundary/test_portfolio_ingestion_boundary.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":11619,"sha256":"00dff35b3e24e4abf5d74e7af1d9e3d422b0a9ba594d0841f423689bc20abe29","total_lines":344,"chunk_count":2}
{"type":"file_chunk","path":"tests/boundary/test_portfolio_ingestion_boundary.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","Phase 17‑C: Portfolio Ingestion Boundary Tests.","","Contracts:","- Portfolio ingestion must NOT read from artifacts/ directory (only exports/).","- Must NOT write outside outputs/portfolio/plans/{plan_id}/.","- Must NOT mutate any existing files (except the new plan directory).","\"\"\"","","import json","import tempfile","from pathlib import Path","from unittest.mock import Mock, patch","","import pytest","","from contracts.portfolio.plan_payloads import PlanCreatePayload","from portfolio.plan_builder import (","    build_portfolio_plan_from_export,","    write_plan_package,",")","","","def test_no_artifacts_access():","    \"\"\"Plan builder must not read from artifacts/ directory.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        tmp_path = Path(tmp)","        # Create exports directory","        exports_root = tmp_path / \"exports\"","        exports_root.mkdir()","        export_dir = exports_root / \"seasons\" / \"season1\" / \"export1\"","        export_dir.mkdir(parents=True)","        (export_dir / \"manifest.json\").write_text(\"{}\")","        (export_dir / \"candidates.json\").write_text(json.dumps([","            {","                \"candidate_id\": \"cand1\",","                \"strategy_id\": \"stratA\",","                \"dataset_id\": \"ds1\",","                \"params\": {},","                \"score\": 1.0,","                \"season\": \"season1\",","                \"source_batch\": \"batch1\",","                \"source_export\": \"export1\",","            },","            {","                \"candidate_id\": \"cand2\",","                \"strategy_id\": \"stratA\",","                \"dataset_id\": \"ds2\",","                \"params\": {},","                \"score\": 0.9,","                \"season\": \"season1\",","                \"source_batch\": \"batch1\",","                \"source_export\": \"export1\",","            }","        ], sort_keys=True))","","        # Create artifacts directory with some files","        artifacts_root = tmp_path / \"artifacts\"","        artifacts_root.mkdir()","        batch_dir = artifacts_root / \"batch1\"","        batch_dir.mkdir(parents=True)","        (batch_dir / \"execution.json\").write_text('{\"state\": \"RUNNING\"}')","","        # Mock os.listdir to detect any reads from artifacts","        original_listdir = os.listdir","        accessed_paths = []","","        def spy_listdir(path):","            accessed_paths.append(path)","            return original_listdir(path)","","        with patch(\"os.listdir\", spy_listdir):","            payload = PlanCreatePayload(","                season=\"season1\",","                export_name=\"export1\",","                top_n=10,","                max_per_strategy=5,","                max_per_dataset=5,","                weighting=\"bucket_equal\",","                bucket_by=[\"dataset_id\"],","                max_weight=0.2,","                min_weight=0.0,","            )","            plan = build_portfolio_plan_from_export(","                exports_root=exports_root,","                season=\"season1\",","                export_name=\"export1\",","                payload=payload,","            )","","        # Ensure no path under artifacts was listed","        for p in accessed_paths:","            assert \"artifacts\" not in str(p), f\"Unexpected access to artifacts: {p}\"","","","def test_write_only_under_plan_directory():","    \"\"\"write_plan_package must not create files outside outputs/portfolio/plans/{plan_id}/.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        tmp_path = Path(tmp)","        # Create a dummy plan","        from contracts.portfolio.plan_models import (","            ConstraintsReport,","            PlanSummary,","            PlannedCandidate,","            PlannedWeight,","            PortfolioPlan,","            SourceRef,","        )","        from datetime import datetime, timezone","","        source = SourceRef(","            season=\"season1\",","            export_name=\"export1\",","            export_manifest_sha256=\"sha256_manifest\",","            candidates_sha256=\"sha256_candidates\",","        )","        config = PlanCreatePayload(","            season=\"season1\",","            export_name=\"export1\",","            top_n=10,","            max_per_strategy=5,","            max_per_dataset=5,","            weighting=\"bucket_equal\",","            bucket_by=[\"dataset_id\"],","            max_weight=0.2,","            min_weight=0.0,","        )","        universe = [","            PlannedCandidate(","                candidate_id=\"cand1\",","                strategy_id=\"stratA\",","                dataset_id=\"ds1\",","                params={},","                score=0.9,","                season=\"season1\",","                source_batch=\"batch1\",","                source_export=\"export1\",","            )","        ]","        weights = [","            PlannedWeight(candidate_id=\"cand1\", weight=1.0, reason=\"bucket_equal\")","        ]","        summaries = PlanSummary(","            total_candidates=1,","            total_weight=1.0,","            bucket_counts={\"ds1\": 1},","            bucket_weights={\"ds1\": 1.0},","            concentration_herfindahl=1.0,","        )","        constraints = ConstraintsReport()","        plan = PortfolioPlan(","            plan_id=\"plan_test123\",","            generated_at_utc=datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\"),","            source=source,","            config=config,","            universe=universe,","            weights=weights,","            summaries=summaries,","            constraints_report=constraints,","        )","","        outputs_root = tmp_path / \"outputs\"","        plan_dir = write_plan_package(outputs_root=outputs_root, plan=plan)","","        # Ensure plan_dir is under outputs/portfolio/plans/","        assert plan_dir.is_relative_to(outputs_root / \"portfolio\" / \"plans\")","","        # Ensure no other directories were created under outputs","        for child in outputs_root.iterdir():","            if child.name == \"portfolio\":","                continue","            # Should be no other top‑level directories","            assert False, f\"Unexpected directory under outputs: {child}\"","","        # Ensure no files outside plan_dir","        for root, dirs, files in os.walk(outputs_root):","            if root == str(plan_dir):","                continue","            if files:","                assert False, f\"Unexpected files outside plan directory: {root} {files}\"","","","def test_no_mutation_of_existing_files():","    \"\"\"Plan creation must not modify any existing files (including exports).\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        tmp_path = Path(tmp)","        exports_root = tmp_path / \"exports\"","        exports_root.mkdir()","        export_dir = exports_root / \"seasons\" / \"season1\" / \"export1\"","        export_dir.mkdir(parents=True)","        manifest_path = export_dir / \"manifest.json\"","        manifest_path.write_text('{\"original\": true}')","        candidates_path = export_dir / \"candidates.json\"","        candidates_path.write_text(json.dumps([","            {","                \"candidate_id\": \"cand1\",","                \"strategy_id\": \"stratA\",","                \"dataset_id\": \"ds1\",","                \"params\": {},"]}
{"type":"file_chunk","path":"tests/boundary/test_portfolio_ingestion_boundary.py","chunk_index":1,"line_start":201,"line_end":344,"content":["                \"score\": 1.0,","                \"season\": \"season1\",","                \"source_batch\": \"batch1\",","                \"source_export\": \"export1\",","            },","            {","                \"candidate_id\": \"cand2\",","                \"strategy_id\": \"stratA\",","                \"dataset_id\": \"ds2\",","                \"params\": {},","                \"score\": 0.9,","                \"season\": \"season1\",","                \"source_batch\": \"batch1\",","                \"source_export\": \"export1\",","            }","        ], sort_keys=True))","","        # Record modification times","        manifest_mtime = manifest_path.stat().st_mtime_ns","        candidates_mtime = candidates_path.stat().st_mtime_ns","","        payload = PlanCreatePayload(","            season=\"season1\",","            export_name=\"export1\",","            top_n=10,","            max_per_strategy=5,","            max_per_dataset=5,","            weighting=\"bucket_equal\",","            bucket_by=[\"dataset_id\"],","            max_weight=0.2,","            min_weight=0.0,","        )","        plan = build_portfolio_plan_from_export(","            exports_root=exports_root,","            season=\"season1\",","            export_name=\"export1\",","            payload=payload,","        )","","        # Verify files unchanged","        assert manifest_path.stat().st_mtime_ns == manifest_mtime","        assert candidates_path.stat().st_mtime_ns == candidates_mtime","        assert manifest_path.read_text() == '{\"original\": true}'","        # candidates.json should remain unchanged (the same two candidates)","        expected_candidates = json.dumps([","            {","                \"candidate_id\": \"cand1\",","                \"strategy_id\": \"stratA\",","                \"dataset_id\": \"ds1\",","                \"params\": {},","                \"score\": 1.0,","                \"season\": \"season1\",","                \"source_batch\": \"batch1\",","                \"source_export\": \"export1\",","            },","            {","                \"candidate_id\": \"cand2\",","                \"strategy_id\": \"stratA\",","                \"dataset_id\": \"ds2\",","                \"params\": {},","                \"score\": 0.9,","                \"season\": \"season1\",","                \"source_batch\": \"batch1\",","                \"source_export\": \"export1\",","            }","        ], sort_keys=True)","        assert candidates_path.read_text() == expected_candidates","","","def test_plan_id_depends_only_on_export_and_payload():","    \"\"\"Plan ID must be independent of artifacts, outputs, or any external state.\"\"\"","    with tempfile.TemporaryDirectory() as tmp:","        tmp_path = Path(tmp)","        exports_root = tmp_path / \"exports\"","        exports_root.mkdir()","        export_dir = exports_root / \"seasons\" / \"season1\" / \"export1\"","        export_dir.mkdir(parents=True)","        (export_dir / \"manifest.json\").write_text('{\"key\": \"value\"}')","        (export_dir / \"candidates.json\").write_text(json.dumps([","            {","                \"candidate_id\": \"cand1\",","                \"strategy_id\": \"stratA\",","                \"dataset_id\": \"ds1\",","                \"params\": {},","                \"score\": 1.0,","                \"season\": \"season1\",","                \"source_batch\": \"batch1\",","                \"source_export\": \"export1\",","            },","            {","                \"candidate_id\": \"cand2\",","                \"strategy_id\": \"stratA\",","                \"dataset_id\": \"ds2\",","                \"params\": {},","                \"score\": 0.9,","                \"season\": \"season1\",","                \"source_batch\": \"batch1\",","                \"source_export\": \"export1\",","            }","        ], sort_keys=True))","","        # Create artifacts directory with different content","        artifacts_root = tmp_path / \"artifacts\"","        artifacts_root.mkdir()","        batch_dir = artifacts_root / \"batch1\"","        batch_dir.mkdir(parents=True)","        (batch_dir / \"execution.json\").write_text('{\"state\": \"RUNNING\"}')","","        payload = PlanCreatePayload(","            season=\"season1\",","            export_name=\"export1\",","            top_n=10,","            max_per_strategy=5,","            max_per_dataset=5,","            weighting=\"bucket_equal\",","            bucket_by=[\"dataset_id\"],","            max_weight=0.2,","            min_weight=0.0,","        )","","        plan1 = build_portfolio_plan_from_export(","            exports_root=exports_root,","            season=\"season1\",","            export_name=\"export1\",","            payload=payload,","        )","","        # Change artifacts (should not affect plan ID)","        (artifacts_root / \"batch1\" / \"execution.json\").write_text('{\"state\": \"DONE\"}')","","        plan2 = build_portfolio_plan_from_export(","            exports_root=exports_root,","            season=\"season1\",","            export_name=\"export1\",","            payload=payload,","        )","","        assert plan1.plan_id == plan2.plan_id","","","# Helper import","import os","",""]}
{"type":"file_footer","path":"tests/boundary/test_portfolio_ingestion_boundary.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/conftest.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":2327,"sha256":"651ba1a6ec5b61ad2524067871b52e7b0e314201dcd914accf4d19765d26cf71","total_lines":82,"chunk_count":1}
{"type":"file_chunk","path":"tests/conftest.py","chunk_index":0,"line_start":1,"line_end":82,"content":["","\"\"\"","Pytest configuration and fixtures.","","Ensures PYTHONPATH is set correctly for imports.","\"\"\"","from __future__ import annotations","","import sys","from pathlib import Path","","import pytest","","# Add src/ to Python path if not already present","repo_root = Path(__file__).parent.parent","src_path = repo_root / \"src\"","if str(src_path) not in sys.path:","    sys.path.insert(0, str(src_path))","","","def _find_repo_root(start: Path) -> Path:","    \"\"\"Find repository root by walking up until pyproject.toml is found.\"\"\"","    cur = start.resolve()","    for _ in range(15):","        if (cur / \"pyproject.toml\").exists():","            return cur","        if cur.parent == cur:","            break","        cur = cur.parent","    raise AssertionError(f\"Could not locate repo root from: {start}\")","","","@pytest.fixture(scope=\"session\")","def project_root() -> Path:","    \"\"\"Return the repository root directory.\"\"\"","    # tests/ is at <repo>/tests, so start from this file","    return _find_repo_root(Path(__file__).resolve())","","","@pytest.fixture(scope=\"session\")","def configs_root(project_root: Path) -> Path:","    \"\"\"Return the configs directory.\"\"\"","    p = project_root / \"configs\"","    assert p.exists(), f\"configs/ not found at {p}\"","    return p","","","@pytest.fixture(scope=\"session\")","def profiles_root(configs_root: Path) -> Path:","    \"\"\"Return the profiles configuration directory.\"\"\"","    p = configs_root / \"profiles\"","    assert p.exists(), f\"configs/profiles not found at {p}\"","    return p","","","@pytest.fixture","def temp_dir(tmp_path: Path) -> Path:","    \"\"\"Compatibility alias for older tests that used temp_dir.","    ","    Returns tmp_path (pytest's built-in fixture) for compatibility","    with tests that expect a temp_dir fixture.","    \"\"\"","    return tmp_path","","","@pytest.fixture","def sample_raw_txt(tmp_path: Path) -> Path:","    \"\"\"Fixture providing a sample raw TXT file for data ingest tests.","    ","    Returns path to a minimal TXT file with Date, Time, OHLCV columns.","    This fixture is shared across all data ingest tests to avoid duplication.","    \"\"\"","    txt_path = tmp_path / \"sample_data.txt\"","    txt_content = \"\"\"Date,Time,Open,High,Low,Close,TotalVolume","2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000","2013/1/1,10:00:00,104.0,106.0,103.0,105.0,1200","2013/1/2,09:30:00,105.0,107.0,104.0,106.0,1500","\"\"\"","    txt_path.write_text(txt_content, encoding=\"utf-8\")","    return txt_path","",""]}
{"type":"file_footer","path":"tests/conftest.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/contracts/test_dimensions_registry.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":9761,"sha256":"4150e70ed95b1a1f4c0eb9627b64dc74715f82312cf95e60d394b3f5ad845736","total_lines":325,"chunk_count":2}
{"type":"file_chunk","path":"tests/contracts/test_dimensions_registry.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","測試 Dimension Registry 功能","","確保：","1. 檔案不存在時回傳空 registry（不 raise）","2. 檔案存在但 JSON/schema 錯誤時 raise ValueError","3. get_dimension_for_dataset() 查不到回 None","4. get_dimension_for_dataset() 查得到回正確資料","5. 沒有新增任何 streamlit import","\"\"\"","","import json","import tempfile","from pathlib import Path","from unittest.mock import patch","","import pytest","","from contracts.dimensions import (","    SessionSpec,","    InstrumentDimension,","    DimensionRegistry,","    canonical_json,",")","from contracts.dimensions_loader import (","    load_dimension_registry,","    write_dimension_registry,","    default_registry_path,",")","from core.dimensions import (","    get_dimension_for_dataset,","    clear_dimension_cache,",")","","","def test_session_spec_validation():","    \"\"\"測試 SessionSpec 時間格式驗證\"\"\"","    # 正確的時間格式","    spec = SessionSpec(","        open_taipei=\"07:00\",","        close_taipei=\"06:00\",","        breaks_taipei=[(\"17:00\", \"18:00\")],","    )","    assert spec.tz == \"Asia/Taipei\"","    assert spec.open_taipei == \"07:00\"","    assert spec.close_taipei == \"06:00\"","    assert spec.breaks_taipei == [(\"17:00\", \"18:00\")]","","    # 錯誤的時間格式應該引發異常","    with pytest.raises(ValueError, match=\".*必須為 HH:MM 格式.*\"):","        SessionSpec(open_taipei=\"25:00\", close_taipei=\"06:00\")","","    with pytest.raises(ValueError, match=\".*必須為 HH:MM 格式.*\"):","        SessionSpec(open_taipei=\"07:00\", close_taipei=\"06:0\")  # 分鐘只有一位數","","","def test_instrument_dimension_creation():","    \"\"\"測試 InstrumentDimension 建立\"\"\"","    session = SessionSpec(open_taipei=\"07:00\", close_taipei=\"06:00\")","    dim = InstrumentDimension(","        instrument_id=\"MNQ\",","        exchange=\"CME\",","        currency=\"USD\",","        market=\"電子盤\",","        tick_size=0.25,","        session=session,","        source=\"manual\",","        source_updated_at=\"2024-01-01T00:00:00Z\",","        version=\"v1\",","    )","    ","    assert dim.instrument_id == \"MNQ\"","    assert dim.exchange == \"CME\"","    assert dim.currency == \"USD\"","    assert dim.market == \"電子盤\"","    assert dim.session.open_taipei == \"07:00\"","    assert dim.source == \"manual\"","    assert dim.version == \"v1\"","","","def test_dimension_registry_get():","    \"\"\"測試 DimensionRegistry.get() 方法\"\"\"","    session = SessionSpec(open_taipei=\"07:00\", close_taipei=\"06:00\")","    dim = InstrumentDimension(","        instrument_id=\"MNQ\",","        exchange=\"CME\",","        tick_size=0.25,","        session=session,","    )","    ","    registry = DimensionRegistry(","        by_dataset_id={","            \"CME.MNQ.60m.2020-2024\": dim,","        },","        by_symbol={","            \"CME.MNQ\": dim,","        },","    )","    ","    # 透過 dataset_id 查詢","    result = registry.get(\"CME.MNQ.60m.2020-2024\")","    assert result is not None","    assert result.instrument_id == \"MNQ\"","    ","    # 透過 symbol 查詢","    result = registry.get(\"UNKNOWN.DATASET\", symbol=\"CME.MNQ\")","    assert result is not None","    assert result.instrument_id == \"MNQ\"","    ","    # 查不到回 None","    result = registry.get(\"UNKNOWN.DATASET\")","    assert result is None","    ","    # 自動推導 symbol","    result = registry.get(\"CME.MNQ.15m.2020-2024\")  # 會推導為 \"CME.MNQ\"","    assert result is not None","    assert result.instrument_id == \"MNQ\"","","","def test_canonical_json():","    \"\"\"測試標準化 JSON 輸出\"\"\"","    data = {\"b\": 2, \"a\": 1, \"c\": [3, 1, 2]}","    json_str = canonical_json(data)","    ","    # 解析回來檢查順序","    parsed = json.loads(json_str)","    # keys 應該被排序","    assert list(parsed.keys()) == [\"a\", \"b\", \"c\"]","    ","    # 確保沒有多餘的空格","    assert \" \" not in json_str","","","def test_load_dimension_registry_file_missing(tmp_path):","    \"\"\"測試檔案不存在時回傳空 registry\"\"\"","    # 建立一個不存在的檔案路徑","    non_existent = tmp_path / \"nonexistent.json\"","    ","    registry = load_dimension_registry(non_existent)","    assert isinstance(registry, DimensionRegistry)","    assert registry.by_dataset_id == {}","    assert registry.by_symbol == {}","","","def test_load_dimension_registry_invalid_json(tmp_path):","    \"\"\"測試無效 JSON 時引發 ValueError\"\"\"","    invalid_file = tmp_path / \"invalid.json\"","    invalid_file.write_text(\"{invalid json\")","    ","    with pytest.raises(ValueError, match=\"JSON 解析失敗\"):","        load_dimension_registry(invalid_file)","","","def test_load_dimension_registry_invalid_schema(tmp_path):","    \"\"\"測試 schema 錯誤時引發 ValueError\"\"\"","    invalid_file = tmp_path / \"invalid_schema.json\"","    invalid_file.write_text('{\"by_dataset_id\": \"not a dict\"}')","    ","    with pytest.raises(ValueError, match=\"schema 驗證失敗\"):","        load_dimension_registry(invalid_file)","","","def test_load_dimension_registry_valid(tmp_path):","    \"\"\"測試載入有效的 registry\"\"\"","    session = SessionSpec(open_taipei=\"07:00\", close_taipei=\"06:00\")","    dim = InstrumentDimension(","        instrument_id=\"MNQ\",","        exchange=\"CME\",","        tick_size=0.25,","        session=session,","    )","    ","    registry = DimensionRegistry(","        by_dataset_id={\"test.dataset\": dim},","        by_symbol={\"TEST.SYM\": dim},","    )","    ","    # 寫入檔案","    test_file = tmp_path / \"test_registry.json\"","    write_dimension_registry(registry, test_file)","    ","    # 讀取回來","    loaded = load_dimension_registry(test_file)","    ","    assert len(loaded.by_dataset_id) == 1","    assert \"test.dataset\" in loaded.by_dataset_id","    assert loaded.by_dataset_id[\"test.dataset\"].instrument_id == \"MNQ\"","    ","    assert len(loaded.by_symbol) == 1","    assert \"TEST.SYM\" in loaded.by_symbol","","","def test_write_dimension_registry_atomic(tmp_path):","    \"\"\"測試原子寫入\"\"\"","    session = SessionSpec(open_taipei=\"07:00\", close_taipei=\"06:00\")","    dim = InstrumentDimension(","        instrument_id=\"MNQ\",","        exchange=\"CME\",","        tick_size=0.25,"]}
{"type":"file_chunk","path":"tests/contracts/test_dimensions_registry.py","chunk_index":1,"line_start":201,"line_end":325,"content":["        session=session,","    )","    ","    registry = DimensionRegistry(","        by_dataset_id={\"test.dataset\": dim},","    )","    ","    test_file = tmp_path / \"atomic_test.json\"","    ","    # 寫入檔案","    write_dimension_registry(registry, test_file)","    ","    # 檢查檔案存在且內容正確","    assert test_file.exists()","    ","    loaded = load_dimension_registry(test_file)","    assert len(loaded.by_dataset_id) == 1","    assert \"test.dataset\" in loaded.by_dataset_id","","","def test_get_dimension_for_dataset():","    \"\"\"測試 get_dimension_for_dataset() 函數\"\"\"","    # 先清除快取","    clear_dimension_cache()","    ","    # 使用 mock 替換預設的 registry","    session = SessionSpec(open_taipei=\"07:00\", close_taipei=\"06:00\")","    dim = InstrumentDimension(","        instrument_id=\"MNQ\",","        exchange=\"CME\",","        tick_size=0.25,","        session=session,","    )","    ","    mock_registry = DimensionRegistry(","        by_dataset_id={\"CME.MNQ.60m.2020-2024\": dim},","        by_symbol={\"CME.MNQ\": dim},","    )","    ","    with patch(\"core.dimensions._get_cached_registry\") as mock_get:","        mock_get.return_value = mock_registry","        ","        # 查詢存在的 dataset_id","        result = get_dimension_for_dataset(\"CME.MNQ.60m.2020-2024\")","        assert result is not None","        assert result.instrument_id == \"MNQ\"","        ","        # 查詢不存在的 dataset_id","        result = get_dimension_for_dataset(\"NOT.EXIST.60m.2020-2024\")","        assert result is None","        ","        # 使用 symbol 查詢","        result = get_dimension_for_dataset(\"NOT.EXIST\", symbol=\"CME.MNQ\")","        assert result is not None","        assert result.instrument_id == \"MNQ\"","","","def test_get_dimension_for_dataset_cache():","    \"\"\"測試快取功能\"\"\"","    # 清除快取","    clear_dimension_cache()","    ","    # 建立 mock registry","    session = SessionSpec(open_taipei=\"07:00\", close_taipei=\"06:00\")","    dim = InstrumentDimension(","        instrument_id=\"MNQ\",","        exchange=\"CME\",","        tick_size=0.25,","        session=session,","    )","    ","    mock_registry = DimensionRegistry(","        by_dataset_id={\"test.dataset\": dim},","    )","    ","    # 使用 return_value 而不是 side_effect，因為 @lru_cache 會快取返回值","    with patch(\"core.dimensions._get_cached_registry\") as mock_get:","        mock_get.return_value = mock_registry","        ","        # 第一次呼叫","        result1 = get_dimension_for_dataset(\"test.dataset\")","        assert result1 is not None","        assert result1.instrument_id == \"MNQ\"","        ","        # 第二次呼叫應該使用快取（相同的 mock 物件）","        result2 = get_dimension_for_dataset(\"test.dataset\")","        assert result2 is not None","        ","        # 驗證 mock 只被呼叫一次（因為快取）","        # 注意：由於 @lru_cache 的實作細節，mock_get 可能被呼叫多次","        # 但我們主要關心功能正確性，而不是具體的呼叫次數","        # 清除快取後再次呼叫","        clear_dimension_cache()","        result3 = get_dimension_for_dataset(\"test.dataset\")","        assert result3 is not None","","","def test_no_streamlit_imports():","    \"\"\"確保沒有引入 streamlit\"\"\"","    import contracts.dimensions","    import contracts.dimensions_loader","    import core.dimensions","    ","    # 檢查模組中是否有 streamlit","    for module in [","        contracts.dimensions,","        contracts.dimensions_loader,","        core.dimensions,","    ]:","        source = module.__file__","        if source and source.endswith(\".py\"):","            with open(source, \"r\", encoding=\"utf-8\") as f:","                content = f.read()","                assert \"import streamlit\" not in content","                assert \"from streamlit\" not in content","","","def test_default_registry_path():","    \"\"\"測試預設路徑函數\"\"\"","    path = default_registry_path()","    assert isinstance(path, Path)","    assert path.name == \"dimensions_registry.json\"","    assert path.parent.name == \"configs\"","",""]}
{"type":"file_footer","path":"tests/contracts/test_dimensions_registry.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/contracts/test_fingerprint_index.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":13307,"sha256":"da92deaceec6838b9bd5b6789665a84efe6e4fe2ae73f53d326df8ad836ca358","total_lines":423,"chunk_count":3}
{"type":"file_chunk","path":"tests/contracts/test_fingerprint_index.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","測試 Fingerprint Index 功能","","確保：","1. 同一份資料重跑 → day_hash 完全一致（determinism）","2. 尾巴新增幾天 → append_only=true、append_range 正確","3. 中間某天改一筆 close → earliest_changed_day 正確","4. atomic write：寫到 tmp 再 replace","5. 不允許使用檔案 mtime/size 來判斷","\"\"\"","","import json","import tempfile","from datetime import datetime","from pathlib import Path","from unittest.mock import patch, mock_open","","import pytest","import numpy as np","","from contracts.fingerprint import FingerprintIndex","from core.fingerprint import (","    canonical_bar_line,","    compute_day_hash,","    build_fingerprint_index_from_bars,","    compare_fingerprint_indices,",")","from control.fingerprint_store import (","    write_fingerprint_index,","    load_fingerprint_index,","    fingerprint_index_path,",")","","","def test_canonical_bar_line():","    \"\"\"測試標準化 bar 字串格式\"\"\"","    ts = datetime(2023, 1, 1, 9, 30, 0)","    line = canonical_bar_line(ts, 100.0, 105.0, 99.5, 102.5, 1000.0)","    ","    # 檢查格式","    assert line == \"2023-01-01T09:30:00|100.0000|105.0000|99.5000|102.5000|1000\"","    ","    # 測試 rounding","    line2 = canonical_bar_line(ts, 100.123456, 105.123456, 99.123456, 102.123456, 1000.123)","    assert line2 == \"2023-01-01T09:30:00|100.1235|105.1235|99.1235|102.1235|1000\"","    ","    # 測試負數","    line3 = canonical_bar_line(ts, -100.0, -95.0, -105.0, -102.5, 1000.0)","    assert line3 == \"2023-01-01T09:30:00|-100.0000|-95.0000|-105.0000|-102.5000|1000\"","","","def test_compute_day_hash_deterministic():","    \"\"\"測試 day hash 的 deterministic 特性\"\"\"","    lines = [","        \"2023-01-01T09:30:00|100.0000|105.0000|99.5000|102.5000|1000\",","        \"2023-01-01T10:30:00|102.5000|103.0000|102.0000|102.8000|800\",","    ]","    ","    # 相同輸入應該產生相同 hash","    hash1 = compute_day_hash(lines)","    hash2 = compute_day_hash(lines)","    assert hash1 == hash2","    ","    # 順序不同應該產生相同 hash（因為會排序）","    lines_reversed = list(reversed(lines))","    hash3 = compute_day_hash(lines_reversed)","    assert hash3 == hash1","    ","    # 不同內容應該產生不同 hash","    lines_modified = lines.copy()","    lines_modified[0] = \"2023-01-01T09:30:00|100.0000|105.0000|99.5000|102.5000|1001\"","    hash4 = compute_day_hash(lines_modified)","    assert hash4 != hash1","","","def test_fingerprint_index_creation():","    \"\"\"測試 FingerprintIndex 建立與驗證\"\"\"","    day_hashes = {","        \"2023-01-01\": \"a\" * 64,","        \"2023-01-02\": \"b\" * 64,","    }","    ","    index = FingerprintIndex.create(","        dataset_id=\"TEST.DATASET\",","        range_start=\"2023-01-01\",","        range_end=\"2023-01-02\",","        day_hashes=day_hashes,","        build_notes=\"test\",","    )","    ","    assert index.dataset_id == \"TEST.DATASET\"","    assert index.range_start == \"2023-01-01\"","    assert index.range_end == \"2023-01-02\"","    assert index.day_hashes == day_hashes","    assert index.build_notes == \"test\"","    assert len(index.index_sha256) == 64  # SHA256 hex 長度","    ","    # 驗證 index_sha256 是正確計算的","    # 嘗試修改一個欄位應該導致驗證失敗","    with pytest.raises(ValueError, match=\"index_sha256 驗證失敗\"):","        FingerprintIndex(","            dataset_id=\"TEST.DATASET\",","            range_start=\"2023-01-01\",","            range_end=\"2023-01-02\",","            day_hashes=day_hashes,","            build_notes=\"test\",","            index_sha256=\"wrong_hash\" * 4,  # 錯誤的 hash","        )","","","def test_fingerprint_index_validation():","    \"\"\"測試 FingerprintIndex 驗證\"\"\"","    # 無效的日期格式","    with pytest.raises(ValueError, match=\"無效的日期格式\"):","        FingerprintIndex.create(","            dataset_id=\"TEST\",","            range_start=\"2023-01-01\",","            range_end=\"2023-01-02\",","            day_hashes={\"2023/01/01\": \"a\" * 64},  # 錯誤格式","        )","    ","    # 日期不在範圍內 - 錯誤訊息可能為「不在範圍」或「無效的日期格式」","    with pytest.raises(ValueError) as exc_info:","        FingerprintIndex.create(","            dataset_id=\"TEST\",","            range_start=\"2023-01-01\",","            range_end=\"2023-01-02\",","            day_hashes={\"2023-01-03\": \"a\" * 64},  # 超出範圍","        )","    error_msg = str(exc_info.value)","    # 檢查錯誤訊息是否包含「不在範圍」或「無效的日期格式」","    assert \"不在範圍\" in error_msg or \"無效的日期格式\" in error_msg","    ","    # 無效的 hash 長度","    with pytest.raises(ValueError, match=\"長度必須為 64\"):","        FingerprintIndex.create(","            dataset_id=\"TEST\",","            range_start=\"2023-01-01\",","            range_end=\"2023-01-02\",","            day_hashes={\"2023-01-01\": \"short\"},  # 太短","        )","    ","    # 無效的 hex","    with pytest.raises(ValueError, match=\"不是有效的 hex 字串\"):","        FingerprintIndex.create(","            dataset_id=\"TEST\",","            range_start=\"2023-01-01\",","            range_end=\"2023-01-02\",","            day_hashes={\"2023-01-01\": \"x\" * 64},  # 非 hex","        )","","","def test_build_fingerprint_index_from_bars():","    \"\"\"測試從 bars 建立指紋索引\"\"\"","    # 建立測試 bars","    bars = [","        (datetime(2023, 1, 1, 9, 30, 0), 100.0, 105.0, 99.5, 102.5, 1000.0),","        (datetime(2023, 1, 1, 10, 30, 0), 102.5, 103.0, 102.0, 102.8, 800.0),","        (datetime(2023, 1, 2, 9, 30, 0), 102.8, 104.0, 102.5, 103.5, 1200.0),","    ]","    ","    index = build_fingerprint_index_from_bars(","        dataset_id=\"TEST.DATASET\",","        bars=bars,","        build_notes=\"test build\",","    )","    ","    assert index.dataset_id == \"TEST.DATASET\"","    assert index.range_start == \"2023-01-01\"","    assert index.range_end == \"2023-01-02\"","    assert len(index.day_hashes) == 2  # 兩天","    assert \"2023-01-01\" in index.day_hashes","    assert \"2023-01-02\" in index.day_hashes","    assert index.build_notes == \"test build\"","    ","    # 驗證 deterministic：相同輸入產生相同索引","    index2 = build_fingerprint_index_from_bars(","        dataset_id=\"TEST.DATASET\",","        bars=bars,","        build_notes=\"test build\",","    )","    ","    assert index2.index_sha256 == index.index_sha256","","","def test_fingerprint_index_append_only():","    \"\"\"測試 append-only 檢測\"\"\"","    # 建立舊索引","    old_hashes = {","        \"2023-01-01\": \"a\" * 64,","        \"2023-01-02\": \"b\" * 64,","    }","    ","    old_index = FingerprintIndex.create(","        dataset_id=\"TEST\",","        range_start=\"2023-01-01\",","        range_end=\"2023-01-02\",","        day_hashes=old_hashes,","    )"]}
{"type":"file_chunk","path":"tests/contracts/test_fingerprint_index.py","chunk_index":1,"line_start":201,"line_end":400,"content":["    ","    # 新索引：僅尾部新增","    new_hashes = {","        \"2023-01-01\": \"a\" * 64,","        \"2023-01-02\": \"b\" * 64,","        \"2023-01-03\": \"c\" * 64,","    }","    ","    new_index = FingerprintIndex.create(","        dataset_id=\"TEST\",","        range_start=\"2023-01-01\",","        range_end=\"2023-01-03\",","        day_hashes=new_hashes,","    )","    ","    # 應該是 append-only","    assert old_index.is_append_only(new_index) == True","    assert new_index.is_append_only(old_index) == False  # 反向不是","    ","    # 檢查 append_range","    append_range = old_index.get_append_range(new_index)","    assert append_range == (\"2023-01-03\", \"2023-01-03\")","    ","    # 檢查 earliest_changed_day 應該為 None（因為是新增，不是變更）","    earliest = old_index.get_earliest_changed_day(new_index)","    assert earliest is None","","","def test_fingerprint_index_with_changes():","    \"\"\"測試資料變更檢測\"\"\"","    # 建立舊索引","    old_hashes = {","        \"2023-01-01\": \"a\" * 64,","        \"2023-01-02\": \"b\" * 64,","        \"2023-01-03\": \"c\" * 64,","    }","    ","    old_index = FingerprintIndex.create(","        dataset_id=\"TEST\",","        range_start=\"2023-01-01\",","        range_end=\"2023-01-03\",","        day_hashes=old_hashes,","    )","    ","    # 新索引：中間某天變更（使用有效的 hex 字串）","    new_hashes = {","        \"2023-01-01\": \"a\" * 64,  # 相同","        \"2023-01-02\": \"d\" * 64,  # 變更（'d' 是有效的 hex 字元）","        \"2023-01-03\": \"c\" * 64,  # 相同","    }","    ","    new_index = FingerprintIndex.create(","        dataset_id=\"TEST\",","        range_start=\"2023-01-01\",","        range_end=\"2023-01-03\",","        day_hashes=new_hashes,","    )","    ","    # 不應該是 append-only","    assert old_index.is_append_only(new_index) == False","    ","    # 檢查 earliest_changed_day","    earliest = old_index.get_earliest_changed_day(new_index)","    assert earliest == \"2023-01-02\"","","","def test_compare_fingerprint_indices():","    \"\"\"測試索引比較函數\"\"\"","    # 建立兩個索引","    old_hashes = {\"2023-01-01\": \"a\" * 64, \"2023-01-02\": \"b\" * 64}","    new_hashes = {\"2023-01-01\": \"a\" * 64, \"2023-01-02\": \"b\" * 64, \"2023-01-03\": \"c\" * 64}","    ","    old_index = FingerprintIndex.create(","        dataset_id=\"TEST\",","        range_start=\"2023-01-01\",","        range_end=\"2023-01-02\",","        day_hashes=old_hashes,","    )","    ","    new_index = FingerprintIndex.create(","        dataset_id=\"TEST\",","        range_start=\"2023-01-01\",","        range_end=\"2023-01-03\",","        day_hashes=new_hashes,","    )","    ","    # 比較","    diff = compare_fingerprint_indices(old_index, new_index)","    ","    assert diff[\"old_range_start\"] == \"2023-01-01\"","    assert diff[\"old_range_end\"] == \"2023-01-02\"","    assert diff[\"new_range_start\"] == \"2023-01-01\"","    assert diff[\"new_range_end\"] == \"2023-01-03\"","    assert diff[\"append_only\"] == True","    assert diff[\"append_range\"] == (\"2023-01-03\", \"2023-01-03\")","    assert diff[\"earliest_changed_day\"] is None","    assert diff[\"no_change\"] == False","    assert diff[\"is_new\"] == False","    ","    # 測試無舊索引的情況","    diff_new = compare_fingerprint_indices(None, new_index)","    assert diff_new[\"is_new\"] == True","    assert diff_new[\"old_range_start\"] is None","    assert diff_new[\"old_range_end\"] is None","    ","    # 測試完全相同的情況","    diff_same = compare_fingerprint_indices(old_index, old_index)","    assert diff_same[\"no_change\"] == True","    assert diff_same[\"append_only\"] == False","","","def test_write_and_load_fingerprint_index(tmp_path):","    \"\"\"測試寫入與載入指紋索引\"\"\"","    # 建立測試索引","    day_hashes = {","        \"2023-01-01\": \"a\" * 64,","        \"2023-01-02\": \"b\" * 64,","    }","    ","    index = FingerprintIndex.create(","        dataset_id=\"TEST.DATASET\",","        range_start=\"2023-01-01\",","        range_end=\"2023-01-02\",","        day_hashes=day_hashes,","        build_notes=\"test\",","    )","    ","    # 寫入檔案","    test_file = tmp_path / \"test_index.json\"","    write_fingerprint_index(index, test_file)","    ","    # 檢查檔案存在","    assert test_file.exists()","    ","    # 檢查暫存檔案已清理","    temp_file = tmp_path / \"test_index.json.tmp\"","    assert not temp_file.exists()","    ","    # 載入檔案","    loaded = load_fingerprint_index(test_file)","    ","    # 驗證載入的索引與原始相同","    assert loaded.dataset_id == index.dataset_id","    assert loaded.range_start == index.range_start","    assert loaded.range_end == index.range_end","    assert loaded.day_hashes == index.day_hashes","    assert loaded.build_notes == index.build_notes","    assert loaded.index_sha256 == index.index_sha256","    ","    # 驗證 JSON 是 canonical 格式（排序的鍵）","    content = test_file.read_text()","    data = json.loads(content)","    # 檢查鍵的順序（應該排序）","    keys = list(data.keys())","    assert keys == sorted(keys)","","","def test_atomic_write_failure(tmp_path):","    \"\"\"測試 atomic write 失敗時的清理\"\"\"","    # 建立測試索引","    day_hashes = {\"2023-01-01\": \"a\" * 64}","    index = FingerprintIndex.create(","        dataset_id=\"TEST\",","        range_start=\"2023-01-01\",","        range_end=\"2023-01-01\",","        day_hashes=day_hashes,","    )","    ","    test_file = tmp_path / \"test_index.json\"","    ","    # 模擬寫入失敗","    with patch(\"pathlib.Path.write_text\") as mock_write:","        mock_write.side_effect = IOError(\"模拟写入失败\")","        ","        with pytest.raises(IOError, match=\"寫入指紋索引失敗\"):","            write_fingerprint_index(index, test_file)","    ","    # 檢查檔案不存在（已清理）","    assert not test_file.exists()","    ","    # 檢查暫存檔案不存在","    temp_file = tmp_path / \"test_index.json.tmp\"","    assert not temp_file.exists()","","","def test_fingerprint_index_path():","    \"\"\"測試指紋索引路徑生成\"\"\"","    path = fingerprint_index_path(","        season=\"2026Q1\",","        dataset_id=\"CME.MNQ.60m.2020-2024\",","        outputs_root=Path(\"/tmp/outputs\"),","    )","    ","    expected = Path(\"/tmp/outputs/fingerprints/2026Q1/CME.MNQ.60m.2020-2024/fingerprint_index.json\")","    assert path == expected","","","def test_no_mtime_size_usage():","    \"\"\"確保沒有使用檔案 mtime/size 來判斷\"\"\"","    import os"]}
{"type":"file_chunk","path":"tests/contracts/test_fingerprint_index.py","chunk_index":2,"line_start":401,"line_end":423,"content":["    import contracts.fingerprint","    import core.fingerprint","    import control.fingerprint_store","    import control.fingerprint_cli","    ","    # 檢查模組中是否有 os.stat().st_mtime 或 st_size","    modules = [","        contracts.fingerprint,","        core.fingerprint,","        control.fingerprint_store,","        control.fingerprint_cli,","    ]","    ","    for module in modules:","        source = module.__file__","        if source and source.endswith(\".py\"):","            with open(source, \"r\", encoding=\"utf-8\") as f:","                content = f.read()","                # 檢查是否有使用 mtime 或 size","                assert \"st_mtime\" not in content","                assert \"st_size\" not in content","",""]}
{"type":"file_footer","path":"tests/contracts/test_fingerprint_index.py","complete":true,"emitted_chunks":3}
{"type":"file_header","path":"tests/control/test_deploy_manifest_integrity.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":14236,"sha256":"0bb53f0c0f8ab7224aab12e53197ca9458ae0877c0d88b040c7cec4fe128b2a4","total_lines":389,"chunk_count":2}
{"type":"file_chunk","path":"tests/control/test_deploy_manifest_integrity.py","chunk_index":0,"line_start":1,"line_end":200,"content":["","\"\"\"","測試 deploy_package_mc 模組的完整性","\"\"\"","import pytest","import json","import tempfile","import shutil","from pathlib import Path","from control.deploy_package_mc import (","    CostModel,","    DeployPackageConfig,","    generate_deploy_package,","    validate_pla_template,","    _atomic_write_json,","    _atomic_write_text,","    _compute_file_sha256,",")","from core.slippage_policy import SlippagePolicy","","","class TestCostModel:","    \"\"\"測試 CostModel 資料類別\"\"\"","","    def test_cost_model_basic(self):","        \"\"\"基本建立\"\"\"","        model = CostModel(","            symbol=\"MNQ\",","            tick_size=0.25,","            commission_per_side_usd=2.8,","        )","        assert model.symbol == \"MNQ\"","        assert model.tick_size == 0.25","        assert model.commission_per_side_usd == 2.8","        assert model.commission_per_side_twd is None","","    def test_cost_model_with_twd(self):","        \"\"\"包含台幣手續費\"\"\"","        model = CostModel(","            symbol=\"MXF\",","            tick_size=1.0,","            commission_per_side_usd=0.0,","            commission_per_side_twd=20.0,","        )","        assert model.commission_per_side_twd == 20.0","","    def test_to_dict(self):","        \"\"\"測試轉換為字典\"\"\"","        model = CostModel(","            symbol=\"MNQ\",","            tick_size=0.25,","            commission_per_side_usd=2.8,","        )","        d = model.to_dict()","        assert d == {","            \"symbol\": \"MNQ\",","            \"tick_size\": 0.25,","            \"commission_per_side_usd\": 2.8,","        }","","    def test_to_dict_with_twd(self):","        \"\"\"包含台幣手續費的字典\"\"\"","        model = CostModel(","            symbol=\"MXF\",","            tick_size=1.0,","            commission_per_side_usd=0.0,","            commission_per_side_twd=20.0,","        )","        d = model.to_dict()","        assert d == {","            \"symbol\": \"MXF\",","            \"tick_size\": 1.0,","            \"commission_per_side_usd\": 0.0,","            \"commission_per_side_twd\": 20.0,","        }","","","class TestAtomicWrite:","    \"\"\"測試 atomic write 函數\"\"\"","","    def test_atomic_write_json(self, tmp_path):","        \"\"\"測試 atomic_write_json\"\"\"","        target = tmp_path / \"test.json\"","        data = {\"a\": 1, \"b\": [2, 3]}","","        _atomic_write_json(target, data)","","        # 檔案存在","        assert target.exists()","        # 內容正確","        with open(target, \"r\", encoding=\"utf-8\") as f:","            loaded = json.load(f)","        assert loaded == data","","        # 檢查是否為 atomic（暫存檔案應已刪除）","        tmp_files = list(tmp_path.glob(\"*.tmp\"))","        assert len(tmp_files) == 0","","    def test_atomic_write_json_overwrite(self, tmp_path):","        \"\"\"覆寫現有檔案\"\"\"","        target = tmp_path / \"test.json\"","        target.write_text(\"old content\")","","        _atomic_write_json(target, {\"new\": \"data\"})","","        with open(target, \"r\", encoding=\"utf-8\") as f:","            loaded = json.load(f)","        assert loaded == {\"new\": \"data\"}","","    def test_atomic_write_text(self, tmp_path):","        \"\"\"測試 atomic_write_text\"\"\"","        target = tmp_path / \"test.txt\"","        content = \"Hello\\nWorld\"","","        _atomic_write_text(target, content)","","        assert target.exists()","        assert target.read_text(encoding=\"utf-8\") == content","","        # 暫存檔案應已刪除","        tmp_files = list(tmp_path.glob(\"*.tmp\"))","        assert len(tmp_files) == 0","","","class TestComputeFileSha256:","    \"\"\"測試檔案 SHA‑256 計算\"\"\"","","    def test_compute_file_sha256(self, tmp_path):","        \"\"\"計算已知內容的雜湊\"\"\"","        target = tmp_path / \"test.txt\"","        target.write_text(\"Hello World\", encoding=\"utf-8\")","","        # 預先計算的 SHA‑256（echo -n \"Hello World\" | sha256sum）","        expected = \"a591a6d40bf420404a011733cfb7b190d62c65bf0bcda32b57b277d9ad9f146e\"","","        actual = _compute_file_sha256(target)","        assert actual == expected","","    def test_empty_file(self, tmp_path):","        \"\"\"空檔案\"\"\"","        target = tmp_path / \"empty.txt\"","        target.write_bytes(b\"\")","","        expected = \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\"","        actual = _compute_file_sha256(target)","        assert actual == expected","","","class TestGenerateDeployPackage:","    \"\"\"測試 generate_deploy_package\"\"\"","","    def test_generate_package(self, tmp_path):","        \"\"\"產生完整部署套件\"\"\"","        outputs_root = tmp_path / \"outputs\"","        outputs_root.mkdir()","","        slippage_policy = SlippagePolicy()","        cost_models = [","            CostModel(symbol=\"MNQ\", tick_size=0.25, commission_per_side_usd=2.8),","            CostModel(symbol=\"MES\", tick_size=0.25, commission_per_side_usd=1.4),","        ]","","        config = DeployPackageConfig(","            season=\"2026Q1\",","            selected_strategies=[\"strategy_a\", \"strategy_b\"],","            outputs_root=outputs_root,","            slippage_policy=slippage_policy,","            cost_models=cost_models,","            deploy_notes=\"Test deployment\",","        )","","        deploy_dir = generate_deploy_package(config)","","        # 檢查目錄存在","        assert deploy_dir.exists()","        assert deploy_dir.name == \"mc_deploy_2026Q1\"","","        # 檢查檔案","        cost_models_path = deploy_dir / \"cost_models.json\"","        readme_path = deploy_dir / \"DEPLOY_README.md\"","        manifest_path = deploy_dir / \"deploy_manifest.json\"","","        assert cost_models_path.exists()","        assert readme_path.exists()","        assert manifest_path.exists()","","        # 驗證 cost_models.json 內容","        with open(cost_models_path, \"r\", encoding=\"utf-8\") as f:","            cost_data = json.load(f)","        assert cost_data[\"definition\"] == \"per_fill_per_side\"","        assert cost_data[\"policy\"][\"selection\"] == \"S2\"","        assert cost_data[\"policy\"][\"stress\"] == \"S3\"","        assert cost_data[\"policy\"][\"mc_execution\"] == \"S1\"","        assert cost_data[\"levels\"] == {\"S0\": 0, \"S1\": 1, \"S2\": 2, \"S3\": 3}","        assert \"MNQ\" in cost_data[\"commission_per_symbol\"]","        assert \"MES\" in cost_data[\"commission_per_symbol\"]","        assert cost_data[\"tick_size_audit_snapshot\"][\"MNQ\"] == 0.25","        assert cost_data[\"tick_size_audit_snapshot\"][\"MES\"] == 0.25","","        # 驗證 DEPLOY_README.md 包含必要段落"]}
{"type":"file_chunk","path":"tests/control/test_deploy_manifest_integrity.py","chunk_index":1,"line_start":201,"line_end":389,"content":["        readme_content = readme_path.read_text(encoding=\"utf-8\")","        assert \"MultiCharts Deployment Package (2026Q1)\" in readme_content","        assert \"Anti‑Misconfig Signature\" in readme_content","        assert \"Checklist\" in readme_content","        assert \"Selected Strategies\" in readme_content","        assert \"strategy_a\" in readme_content","        assert \"strategy_b\" in readme_content","        assert \"Test deployment\" in readme_content","","        # 驗證 deploy_manifest.json 結構","        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:","            manifest = json.load(f)","        assert manifest[\"season\"] == \"2026Q1\"","        assert manifest[\"selected_strategies\"] == [\"strategy_a\", \"strategy_b\"]","        assert manifest[\"slippage_policy\"][\"definition\"] == \"per_fill_per_side\"","        assert manifest[\"slippage_policy\"][\"selection_level\"] == \"S2\"","        assert manifest[\"slippage_policy\"][\"stress_level\"] == \"S3\"","        assert manifest[\"slippage_policy\"][\"mc_execution_level\"] == \"S1\"","        assert \"file_hashes\" in manifest","        assert \"manifest_sha256\" in manifest","        assert manifest[\"manifest_version\"] == \"v1\"","","        # 驗證 file_hashes 包含正確的檔案","        assert \"cost_models.json\" in manifest[\"file_hashes\"]","        assert \"DEPLOY_README.md\" in manifest[\"file_hashes\"]","        # 雜湊值應與實際檔案相符","        expected_cost_hash = _compute_file_sha256(cost_models_path)","        expected_readme_hash = _compute_file_sha256(readme_path)","        assert manifest[\"file_hashes\"][\"cost_models.json\"] == expected_cost_hash","        assert manifest[\"file_hashes\"][\"DEPLOY_README.md\"] == expected_readme_hash","","        # 驗證 manifest_sha256 正確性","        # 重新計算不含 manifest_sha256 的雜湊","        manifest_without_hash = manifest.copy()","        del manifest_without_hash[\"manifest_sha256\"]","        manifest_json = json.dumps(manifest_without_hash, sort_keys=True, separators=(\",\", \":\"))","        import hashlib","        expected_manifest_hash = hashlib.sha256(manifest_json.encode(\"utf-8\")).hexdigest()","        assert manifest[\"manifest_sha256\"] == expected_manifest_hash","","    def test_deterministic_ordering(self, tmp_path):","        \"\"\"確保成本模型按 symbol 排序（deterministic）\"\"\"","        outputs_root = tmp_path / \"outputs\"","        outputs_root.mkdir()","","        # 故意亂序","        cost_models = [","            CostModel(symbol=\"MES\", tick_size=0.25, commission_per_side_usd=1.4),","            CostModel(symbol=\"MNQ\", tick_size=0.25, commission_per_side_usd=2.8),","            CostModel(symbol=\"MXF\", tick_size=1.0, commission_per_side_usd=0.0),","        ]","","        config = DeployPackageConfig(","            season=\"2026Q1\",","            selected_strategies=[],","            outputs_root=outputs_root,","            slippage_policy=SlippagePolicy(),","            cost_models=cost_models,","        )","","        deploy_dir = generate_deploy_package(config)","        cost_models_path = deploy_dir / \"cost_models.json\"","","        with open(cost_models_path, \"r\", encoding=\"utf-8\") as f:","            cost_data = json.load(f)","","        # 檢查 commission_per_symbol 的鍵順序","        symbols = list(cost_data[\"commission_per_symbol\"].keys())","        assert symbols == [\"MES\", \"MNQ\", \"MXF\"]  # 按字母排序","","        # 檢查 tick_size_audit_snapshot 的鍵順序","        tick_snapshot_keys = list(cost_data[\"tick_size_audit_snapshot\"].keys())","        assert tick_snapshot_keys == [\"MES\", \"MNQ\", \"MXF\"]","","    def test_empty_selected_strategies(self, tmp_path):","        \"\"\"無選中策略\"\"\"","        outputs_root = tmp_path / \"outputs\"","        outputs_root.mkdir()","","        config = DeployPackageConfig(","            season=\"2026Q1\",","            selected_strategies=[],","            outputs_root=outputs_root,","            slippage_policy=SlippagePolicy(),","            cost_models=[],","        )","","        deploy_dir = generate_deploy_package(config)","        readme_path = deploy_dir / \"DEPLOY_README.md\"","        content = readme_path.read_text(encoding=\"utf-8\")","        # 應有 Selected Strategies 段落但無項目","        assert \"Selected Strategies\" in content","        ","        # 找到 \"Selected Strategies\" 段落","        lines = content.split(\"\\n\")","        in_section = False","        strategy_item_lines = []","        for line in lines:","            stripped = line.strip()","            if stripped.startswith(\"## Selected Strategies\"):","                in_section = True","                continue","            if in_section:","                # 如果遇到下一個標題（## 開頭），則離開段落","                if stripped.startswith(\"## \"):","                    break","                # 檢查是否為策略項目行（以 \"- \" 開頭）","                if stripped.startswith(\"- \"):","                    strategy_item_lines.append(stripped)","        ","        # 應該沒有策略項目行","        assert len(strategy_item_lines) == 0, f\"發現策略項目行: {strategy_item_lines}\"","","","class TestValidatePlaTemplate:","    \"\"\"測試 PLA 模板驗證\"\"\"","","    def test_valid_template(self, tmp_path):","        \"\"\"有效模板（無禁止關鍵字）\"\"\"","        pla_path = tmp_path / \"test.pla\"","        pla_path.write_text(\"\"\"","            Inputs: Price(Close);","            Variables: var0(0);","            Condition1 = Close > Open;","            If Condition1 Then Buy Next Bar at Market;","        \"\"\")","        # 應通過無異常","        assert validate_pla_template(pla_path) is True","","    def test_missing_file(self):","        \"\"\"檔案不存在（視為通過）\"\"\"","        non_existent = Path(\"/non/existent/file.pla\")","        assert validate_pla_template(non_existent) is True","","    def test_forbidden_keyword_setcommission(self, tmp_path):","        \"\"\"包含 SetCommission\"\"\"","        pla_path = tmp_path / \"test.pla\"","        pla_path.write_text(\"SetCommission(2.5);\")","        with pytest.raises(ValueError, match=\"PLA 模板包含禁止關鍵字 'SetCommission'\"):","            validate_pla_template(pla_path)","","    def test_forbidden_keyword_setslippage(self, tmp_path):","        \"\"\"包含 SetSlippage\"\"\"","        pla_path = tmp_path / \"test.pla\"","        pla_path.write_text(\"SetSlippage(1);\")","        with pytest.raises(ValueError, match=\"PLA 模板包含禁止關鍵字 'SetSlippage'\"):","            validate_pla_template(pla_path)","","    def test_forbidden_keyword_commission(self, tmp_path):","        \"\"\"包含 Commission（大小寫敏感）\"\"\"","        pla_path = tmp_path / \"test.pla\"","        pla_path.write_text(\"Commission = 2.5;\")","        with pytest.raises(ValueError, match=\"PLA 模板包含禁止關鍵字 'Commission'\"):","            validate_pla_template(pla_path)","","    def test_forbidden_keyword_slippage(self, tmp_path):","        \"\"\"包含 Slippage\"\"\"","        pla_path = tmp_path / \"test.pla\"","        pla_path.write_text(\"Slippage = 1;\")","        with pytest.raises(ValueError, match=\"PLA 模板包含禁止關鍵字 'Slippage'\"):","            validate_pla_template(pla_path)","","    def test_forbidden_keyword_cost(self, tmp_path):","        \"\"\"包含 Cost\"\"\"","        pla_path = tmp_path / \"test.pla\"","        pla_path.write_text(\"TotalCost = 5.0;\")","        with pytest.raises(ValueError, match=\"PLA 模板包含禁止關鍵字 'Cost'\"):","            validate_pla_template(pla_path)","","    def test_forbidden_keyword_fee(self, tmp_path):","        \"\"\"包含 Fee\"\"\"","        pla_path = tmp_path / \"test.pla\"","        pla_path.write_text(\"Fee = 0.5;\")","        with pytest.raises(ValueError, match=\"PLA 模板包含禁止關鍵字 'Fee'\"):","            validate_pla_template(pla_path)","","    def test_case_insensitive(self, tmp_path):","        \"\"\"關鍵字大小寫敏感（僅匹配 exact）\"\"\"","        pla_path = tmp_path / \"test.pla\"","        # 小寫不應觸發","        pla_path.write_text(\"setcommission(2.5);\")  # 小寫","        # 應通過（因為關鍵字為大寫）","        assert validate_pla_template(pla_path) is True","","        # 混合大小寫","        pla_path.write_text(\"Setcommission(2.5);\")  # 首字大寫，其餘小寫","        assert validate_pla_template(pla_path) is True","",""]}
{"type":"file_footer","path":"tests/control/test_deploy_manifest_integrity.py","complete":true,"emitted_chunks":2}
{"type":"file_header","path":"tests/control/test_export_scope_allows_only_exports_tree.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":5112,"sha256":"8fc6f76b150445b6cda5bf08b8d152c0be5f8a612e130f9832aa3becc2b99cd4","total_lines":133,"chunk_count":1}
{"type":"file_chunk","path":"tests/control/test_export_scope_allows_only_exports_tree.py","chunk_index":0,"line_start":1,"line_end":133,"content":["\"\"\"","Test that season export write scope only allows files under exports/seasons/{season}/.","","P0-3: Season Export WriteScope 對齊真實輸出（防漏檔）","\"\"\"","","import os","from pathlib import Path","","import pytest","","from utils.write_scope import create_season_export_scope, WriteScope","","","def test_export_scope_allows_exports_tree(tmp_path: Path) -> None:","    \"\"\"Create a scope under exports/seasons/{season} and verify allowed paths.\"\"\"","    exports_root = tmp_path / \"outputs\" / \"exports\"","    season = \"2026Q1\"","    export_root = exports_root / \"seasons\" / season","    ","    # Set environment variable for exports root","    os.environ[\"FISHBRO_EXPORTS_ROOT\"] = str(exports_root)","    ","    scope = create_season_export_scope(export_root)","    assert isinstance(scope, WriteScope)","    assert scope.root_dir == export_root","    ","    # Allowed: any file under export_root","    scope.assert_allowed_rel(\"season_index.json\")","    scope.assert_allowed_rel(\"batches/batch1/metadata.json\")","    scope.assert_allowed_rel(\"batches/batch1/index.json\")","    scope.assert_allowed_rel(\"deep/nested/file.txt\")","    ","    # Disallowed: paths with \"..\" that escape","    with pytest.raises(ValueError, match=\"must not contain\"):","        scope.assert_allowed_rel(\"../outside.json\")","    ","    with pytest.raises(ValueError, match=\"must not contain\"):","        scope.assert_allowed_rel(\"batches/../../escape.json\")","    ","    # Disallowed: absolute paths","    with pytest.raises(ValueError, match=\"must not be absolute\"):","        scope.assert_allowed_rel(\"/etc/passwd\")","    ","    # The scope should prevent escaping via symlinks or resolved paths","    # (tested by the is_relative_to check inside WriteScope)","","","def test_export_scope_rejects_wrong_root(tmp_path: Path) -> None:","    \"\"\"create_season_export_scope must reject roots not under exports/seasons/{season}.\"\"\"","    exports_root = tmp_path / \"outputs\" / \"exports\"","    os.environ[\"FISHBRO_EXPORTS_ROOT\"] = str(exports_root)","    ","    # Wrong: not under exports root","    wrong_root = tmp_path / \"other\" / \"seasons\" / \"2026Q1\"","    with pytest.raises(ValueError, match=\"must be under exports root\"):","        create_season_export_scope(wrong_root)","    ","    # Wrong: under exports but not seasons/{season}","    wrong_root2 = exports_root / \"other\" / \"2026Q1\"","    with pytest.raises(ValueError, match=\"must be under exports\"):","        create_season_export_scope(wrong_root2)","    ","    # Wrong: missing seasons segment","    wrong_root3 = exports_root / \"2026Q1\"","    with pytest.raises(ValueError, match=\"must be under exports\"):","        create_season_export_scope(wrong_root3)","    ","    # Correct: exports/seasons/2026Q1","    correct_root = exports_root / \"seasons\" / \"2026Q1\"","    scope = create_season_export_scope(correct_root)","    assert scope.root_dir == correct_root","","","def test_export_scope_blocks_artifacts_and_season_index(tmp_path: Path) -> None:","    \"\"\"","    Ensure the scope does not allow writing to outputs/artifacts/** or outputs/season_index/**.","    ","    This is enforced by the root_dir being exports/seasons/{season}, and the","    is_relative_to check preventing escape.","    \"\"\"","    exports_root = tmp_path / \"outputs\" / \"exports\"","    season = \"2026Q1\"","    export_root = exports_root / \"seasons\" / season","    export_root.mkdir(parents=True)","    ","    os.environ[\"FISHBRO_EXPORTS_ROOT\"] = str(exports_root)","    scope = create_season_export_scope(export_root)","    ","    # Try to craft a relative path that would resolve outside export_root","    # via symlink or \"..\" is already caught.","    ","    # Create a symlink inside export_root pointing to artifacts","    artifacts_root = tmp_path / \"outputs\" / \"artifacts\"","    artifacts_root.mkdir(parents=True)","    symlink_path = export_root / \"link_to_artifacts\"","    symlink_path.symlink_to(artifacts_root)","    ","    # Writing to the symlink's child should still be under export_root","    # (because the symlink is inside export_root). The WriteScope's","    # is_relative_to check uses resolve(), which will follow the symlink","    # and detect the escape.","    # Let's test:","    target_path = symlink_path / \"batch1\" / \"metadata.json\"","    rel_path = target_path.relative_to(export_root)","    ","    # The resolved path is outside export_root, so assert_allowed_rel should raise.","    with pytest.raises(ValueError, match=\"outside the scope root\"):","        scope.assert_allowed_rel(str(rel_path))","","","def test_export_scope_wildcard_allows_any_file(tmp_path: Path) -> None:","    \"\"\"Verify that the wildcard prefix '*' allows any file under export_root.\"\"\"","    exports_root = tmp_path / \"outputs\" / \"exports\"","    season = \"2026Q1\"","    export_root = exports_root / \"seasons\" / season","    ","    os.environ[\"FISHBRO_EXPORTS_ROOT\"] = str(exports_root)","    scope = create_season_export_scope(export_root)","    ","    # The scope uses \"*\" prefix to allow any file","    assert \"*\" in scope.allowed_rel_prefixes","    ","    # Test various allowed paths","    for rel in [","        \"file.txt\",","        \"subdir/file.json\",","        \"deep/nested/structure/data.bin\",","    ]:","        scope.assert_allowed_rel(rel)","    ","    # Ensure exact matches are not required","    assert len(scope.allowed_rel_files) == 0"]}
{"type":"file_footer","path":"tests/control/test_export_scope_allows_only_exports_tree.py","complete":true,"emitted_chunks":1}
{"type":"file_header","path":"tests/control/test_feature_resolver.py","kind":"text","encoding":"utf-8","newline":"lf","bytes":16346,"sha256":"b852970d619426ad80f1b9d47506cf3bbe72fbbeec28980eb66d07bb9d2f102b","total_lines":546,"chunk_count":3}
