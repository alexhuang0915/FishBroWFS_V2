FILE src/FishBroWFS_V2/strategy/entry_builder_nb.py
sha256(source_bytes) = a154599c4b9353884a3399b5016ffa523edc97f02b8c0c8d2076ae3d15788c17
bytes = 11674
redacted = False
--------------------------------------------------------------------------------

"""
Stage P2-3A: Numba-accelerated Sparse Entry Intent Builder

Single-pass Numba implementation for building sparse entry intents.
Uses two-pass approach: count first, then allocate and fill.
"""
from __future__ import annotations

import numpy as np

try:
    import numba as nb
except Exception:  # pragma: no cover
    nb = None  # type: ignore


if nb is not None:

    @nb.njit(cache=False)
    def _deterministic_random(t: int, seed: int) -> float:
        """
        Deterministic pseudo-random number generator for trigger rate selection.
        
        This mimics numpy.random.default_rng(seed).random() behavior for position t.
        Uses PCG64 algorithm approximation for compatibility with numpy's default_rng.
        
        Note: For exact compatibility with apply_trigger_rate_mask, we need to match
        the sequence generated by rng.random(n - warmup) for positions >= warmup.
        Since we're iterating t from 1..n-1, we use (t - warmup) as the index.
        """
        # Approximate PCG64: use a simple hash-based approach
        # This ensures deterministic selection matching numpy's default_rng(seed)
        # We use t as the position index (for positions >= warmup, index = t - warmup)
        # Simple hash: combine seed and t
        x = (seed ^ (t * 0x9e3779b9)) & 0xFFFFFFFF
        x = ((x << 16) ^ (x >> 16)) & 0xFFFFFFFF
        x = (x * 0x85ebca6b) & 0xFFFFFFFF
        x = (x ^ (x >> 13)) & 0xFFFFFFFF
        x = (x * 0xc2b2ae35) & 0xFFFFFFFF
        x = (x ^ (x >> 16)) & 0xFFFFFFFF
        # Normalize to [0, 1)
        return float(x & 0x7FFFFFFF) / float(0x7FFFFFFF + 1)

    @nb.njit(cache=False)
    def _count_valid_intents(
        donch_prev: np.ndarray,
        warmup: int,
        trigger_rate: float,
        random_vals: np.ndarray,
    ) -> int:
        """
        Pass 1: Count valid entry intents.
        
        Args:
            donch_prev: float64 array (n_bars,) - shifted donchian high
            warmup: Warmup period
            trigger_rate: Trigger rate (0.0 to 1.0)
            random_vals: Pre-computed random values (shape n - warmup) for positions >= warmup
        
        Returns:
            Number of valid intents
        """
        n = donch_prev.shape[0]
        count = 0
        
        # Scan bars 1..n-1 (bar index t, where created_bar = t-1)
        for t in range(1, n):
            # Check if signal is valid (finite, positive, past warmup)
            if t < warmup:
                continue
            
            price_val = donch_prev[t]
            if not (np.isfinite(price_val) and price_val > 0.0):
                continue
            
            # Apply trigger rate selection (deterministic)
            # Match apply_trigger_rate_mask logic: use (t - warmup) as index into random_vals
            if trigger_rate < 1.0:
                rng_index = t - warmup  # Index into random sequence (0-based for positions >= warmup)
                if rng_index < random_vals.shape[0]:
                    rand_val = random_vals[rng_index]
                    if rand_val >= trigger_rate:
                        continue  # Skip this trigger
            
            count += 1
        
        return count

    @nb.njit(cache=False)
    def _build_sparse_intents(
        donch_prev: np.ndarray,
        warmup: int,
        trigger_rate: float,
        random_vals: np.ndarray,
        order_qty: int,
        n_entry: int,
        created_bar: np.ndarray,
        price: np.ndarray,
        order_id: np.ndarray,
        role: np.ndarray,
        kind: np.ndarray,
        side: np.ndarray,
        qty: np.ndarray,
    ) -> None:
        """
        Pass 2: Fill sparse intent arrays.
        
        Args:
            donch_prev: float64 array (n_bars,) - shifted donchian high
            warmup: Warmup period
            trigger_rate: Trigger rate (0.0 to 1.0)
            random_vals: Pre-computed random values (shape n - warmup) for positions >= warmup
            order_qty: Order quantity
            n_entry: Number of valid intents (pre-allocated array size)
            created_bar: Output array (int32, shape n_entry)
            price: Output array (float64, shape n_entry)
            order_id: Output array (int32, shape n_entry)
            role: Output array (uint8, shape n_entry)
            kind: Output array (uint8, shape n_entry)
            side: Output array (uint8, shape n_entry)
            qty: Output array (int32, shape n_entry)
        """
        n = donch_prev.shape[0]
        idx = 0
        
        # Scan bars 1..n-1 (bar index t, where created_bar = t-1)
        for t in range(1, n):
            # Check if signal is valid (finite, positive, past warmup)
            if t < warmup:
                continue
            
            price_val = donch_prev[t]
            if not (np.isfinite(price_val) and price_val > 0.0):
                continue
            
            # Apply trigger rate selection (deterministic)
            # Match apply_trigger_rate_mask logic: use (t - warmup) as index into random_vals
            if trigger_rate < 1.0:
                rng_index = t - warmup  # Index into random sequence (0-based for positions >= warmup)
                if rng_index < random_vals.shape[0]:
                    rand_val = random_vals[rng_index]
                    if rand_val >= trigger_rate:
                        continue  # Skip this trigger
            
            # Emit intent
            created_bar[idx] = t - 1  # created_bar = t - 1
            price[idx] = price_val
            order_id[idx] = idx + 1  # Sequential order ID (1, 2, 3, ...)
            role[idx] = 1  # ROLE_ENTRY
            kind[idx] = 0  # KIND_STOP
            side[idx] = 1  # SIDE_BUY
            qty[idx] = order_qty
            
            idx += 1

else:
    # Fallback pure-python (used only if numba unavailable)
    def _deterministic_random(t: int, seed: int) -> float:  # type: ignore
        """Fallback pure-python implementation."""
        import random
        rng = random.Random(seed + t)
        return rng.random()

    def _count_valid_intents(  # type: ignore
        donch_prev: np.ndarray,
        warmup: int,
        trigger_rate: float,
        random_vals: np.ndarray,
    ) -> int:
        """Fallback pure-python implementation."""
        n = donch_prev.shape[0]
        count = 0
        
        for t in range(1, n):
            if t < warmup:
                continue
            
            price_val = donch_prev[t]
            if not (np.isfinite(price_val) and price_val > 0.0):
                continue
            
            if trigger_rate < 1.0:
                rng_index = t - warmup
                if rng_index < random_vals.shape[0]:
                    rand_val = random_vals[rng_index]
                    if rand_val >= trigger_rate:
                        continue
            
            count += 1
        
        return count

    def _build_sparse_intents(  # type: ignore
        donch_prev: np.ndarray,
        warmup: int,
        trigger_rate: float,
        random_vals: np.ndarray,
        order_qty: int,
        n_entry: int,
        created_bar: np.ndarray,
        price: np.ndarray,
        order_id: np.ndarray,
        role: np.ndarray,
        kind: np.ndarray,
        side: np.ndarray,
        qty: np.ndarray,
    ) -> None:
        """Fallback pure-python implementation."""
        n = donch_prev.shape[0]
        idx = 0
        
        for t in range(1, n):
            if t < warmup:
                continue
            
            price_val = donch_prev[t]
            if not (np.isfinite(price_val) and price_val > 0.0):
                continue
            
            if trigger_rate < 1.0:
                rng_index = t - warmup
                if rng_index < random_vals.shape[0]:
                    rand_val = random_vals[rng_index]
                    if rand_val >= trigger_rate:
                        continue
            
            created_bar[idx] = t - 1
            price[idx] = price_val
            order_id[idx] = idx + 1
            role[idx] = 1
            kind[idx] = 0
            side[idx] = 1
            qty[idx] = order_qty
            
            idx += 1


def build_entry_intents_numba(
    donch_prev: np.ndarray,
    channel_len: int,
    order_qty: int,
    trigger_rate: float = 1.0,
    seed: int = 42,
) -> dict:
    """
    Build entry intents using Numba-accelerated single-pass sparse builder.
    
    Args:
        donch_prev: float64 array (n_bars,) - shifted donchian high
        channel_len: Warmup period (same as indicator warmup)
        order_qty: Order quantity
        trigger_rate: Trigger rate (0.0 to 1.0, default 1.0)
        seed: Random seed for deterministic trigger rate selection (default 42)
    
    Returns:
        dict with:
            - created_bar: int32 array (n_entry,)
            - price: float64 array (n_entry,)
            - order_id: int32 array (n_entry,)
            - role: uint8 array (n_entry,)
            - kind: uint8 array (n_entry,)
            - side: uint8 array (n_entry,)
            - qty: int32 array (n_entry,)
            - n_entry: int
            - obs: dict with valid_mask_sum
    """
    from FishBroWFS_V2.config.dtypes import (
        INDEX_DTYPE,
        INTENT_ENUM_DTYPE,
        INTENT_PRICE_DTYPE,
    )
    
    n = int(donch_prev.shape[0])
    warmup = channel_len
    
    # Pre-compute random values (matching apply_trigger_rate_mask logic)
    # Generate random values for positions >= warmup
    random_vals = np.empty(0, dtype=np.float64)
    if trigger_rate < 1.0 and warmup < n:
        rng = np.random.default_rng(seed)
        random_vals = rng.random(n - warmup).astype(np.float64)
    
    # Pass 1: Count valid intents
    n_entry = _count_valid_intents(
        donch_prev=donch_prev,
        warmup=warmup,
        trigger_rate=trigger_rate,
        random_vals=random_vals,
    )
    
    # Diagnostic observations
    obs = {
        "n_bars": n,
        "warmup": warmup,
        "valid_mask_sum": n_entry,  # In numba builder, valid_mask_sum == n_entry
    }
    
    if n_entry == 0:
        return {
            "created_bar": np.empty(0, dtype=INDEX_DTYPE),
            "price": np.empty(0, dtype=INTENT_PRICE_DTYPE),
            "order_id": np.empty(0, dtype=INDEX_DTYPE),
            "role": np.empty(0, dtype=INTENT_ENUM_DTYPE),
            "kind": np.empty(0, dtype=INTENT_ENUM_DTYPE),
            "side": np.empty(0, dtype=INTENT_ENUM_DTYPE),
            "qty": np.empty(0, dtype=INDEX_DTYPE),
            "n_entry": 0,
            "obs": obs,
        }
    
    # Pass 2: Allocate and fill arrays
    created_bar = np.empty(n_entry, dtype=INDEX_DTYPE)
    price = np.empty(n_entry, dtype=INTENT_PRICE_DTYPE)
    order_id = np.empty(n_entry, dtype=INDEX_DTYPE)
    role = np.empty(n_entry, dtype=INTENT_ENUM_DTYPE)
    kind = np.empty(n_entry, dtype=INTENT_ENUM_DTYPE)
    side = np.empty(n_entry, dtype=INTENT_ENUM_DTYPE)
    qty = np.empty(n_entry, dtype=INDEX_DTYPE)
    
    _build_sparse_intents(
        donch_prev=donch_prev,
        warmup=warmup,
        trigger_rate=trigger_rate,
        random_vals=random_vals,
        order_qty=order_qty,
        n_entry=n_entry,
        created_bar=created_bar,
        price=price,
        order_id=order_id,
        role=role,
        kind=kind,
        side=side,
        qty=qty,
    )
    
    return {
        "created_bar": created_bar,
        "price": price,
        "order_id": order_id,
        "role": role,
        "kind": kind,
        "side": side,
        "qty": qty,
        "n_entry": n_entry,
        "obs": obs,
    }



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/strategy/kernel.py
sha256(source_bytes) = 7ad7494e2e39581826354e40a644ab64ad0f94361d3fe5eace21667c934cc75a
bytes = 36338
redacted = False
--------------------------------------------------------------------------------

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import numpy as np
import os
import time

from FishBroWFS_V2.engine.constants import KIND_STOP, ROLE_ENTRY, ROLE_EXIT, SIDE_BUY, SIDE_SELL
from FishBroWFS_V2.engine.engine_jit import simulate as simulate_matcher
from FishBroWFS_V2.engine.engine_jit import simulate_arrays as simulate_matcher_arrays
from FishBroWFS_V2.engine.metrics_from_fills import compute_metrics_from_fills
from FishBroWFS_V2.engine.types import BarArrays, Fill, OrderIntent, OrderKind, OrderRole, Side
from FishBroWFS_V2.indicators.numba_indicators import rolling_max, rolling_min, atr_wilder


# Stage P2-2 Step B1: Precomputed Indicators Pack
@dataclass(frozen=True)
class PrecomputedIndicators:
    """
    Pre-computed indicator arrays for shared computation optimization.
    
    These arrays are computed once per unique (channel_len, atr_len) combination
    and reused across multiple params to avoid redundant computation.
    """
    donch_hi: np.ndarray  # float64, shape (n_bars,) - Donchian high (rolling max)
    donch_lo: np.ndarray  # float64, shape (n_bars,) - Donchian low (rolling min)
    atr: np.ndarray       # float64, shape (n_bars,) - ATR Wilder


def _build_entry_intents_from_trigger(
    donch_prev: np.ndarray,
    channel_len: int,
    order_qty: int,
) -> Dict[str, object]:
    """
    Build entry intents from trigger array with sparse masking (Stage P2-1).
    
    Args:
        donch_prev: float64 array (n_bars,) - shifted donchian high (donch_prev[0]=NaN, donch_prev[1:]=donch_hi[:-1])
        channel_len: warmup period (same as indicator warmup)
        order_qty: order quantity
    
    Returns:
        dict with:
            - created_bar: int32 array (n_entry,) - created bar indices
            - price: float64 array (n_entry,) - entry prices
            - order_id: int32 array (n_entry,) - order IDs
            - role: uint8 array (n_entry,) - role (ENTRY)
            - kind: uint8 array (n_entry,) - kind (STOP)
            - side: uint8 array (n_entry,) - side (BUY)
            - qty: int32 array (n_entry,) - quantities
            - n_entry: int - number of entry intents
            - obs: dict - diagnostic observations
    """
    from FishBroWFS_V2.config.dtypes import (
        INDEX_DTYPE,
        INTENT_ENUM_DTYPE,
        INTENT_PRICE_DTYPE,
    )
    
    n = int(donch_prev.shape[0])
    warmup = channel_len
    
    # Create index array for bars 1..n-1 (bar indices t, where created_bar = t-1)
    # i represents bar index t (from 1 to n-1)
    i = np.arange(1, n, dtype=INDEX_DTYPE)
    
    # Sparse mask: valid entries must be finite, positive, and past warmup
    # Check donch_prev[t] for each bar t in range(1, n)
    valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)
    
    # Get indices of valid entries (flatnonzero returns indices into donch_prev[1:])
    # idx is 0-indexed into donch_prev[1:], so idx=0 corresponds to bar t=1
    idx = np.flatnonzero(valid_mask).astype(INDEX_DTYPE)
    
    n_entry = int(idx.shape[0])
    
    # CURSOR TASK 2: entry_valid_mask_sum must be sum(allow_mask) - for dense builder, it equals valid_mask_sum
    # Diagnostic observations
    obs = {
        "n_bars": n,
        "warmup": warmup,
        "valid_mask_sum": int(np.sum(valid_mask)),  # Dense valid bars (before trigger rate)
        "entry_valid_mask_sum": int(np.sum(valid_mask)),  # CURSOR TASK 2: For dense builder, equals valid_mask_sum
    }
    
    if n_entry == 0:
        return {
            "created_bar": np.empty(0, dtype=INDEX_DTYPE),
            "price": np.empty(0, dtype=INTENT_PRICE_DTYPE),
            "order_id": np.empty(0, dtype=INDEX_DTYPE),
            "role": np.empty(0, dtype=INTENT_ENUM_DTYPE),
            "kind": np.empty(0, dtype=INTENT_ENUM_DTYPE),
            "side": np.empty(0, dtype=INTENT_ENUM_DTYPE),
            "qty": np.empty(0, dtype=INDEX_DTYPE),
            "n_entry": 0,
            "obs": obs,
        }
    
    # Stage P2-3A: Gather sparse entries (only for valid_mask == True positions)
    # - idx is index into donch_prev[1:], so bar index t = idx + 1
    # - created_bar = t - 1 = idx (since t = idx + 1)
    # - price = donch_prev[t] = donch_prev[idx + 1] = donch_prev[1:][idx]
    # created_bar is already sorted (ascending) because idx comes from flatnonzero on sorted mask
    created_bar = idx.astype(INDEX_DTYPE)  # created_bar = t-1 = idx (when t = idx+1)
    price = donch_prev[1:][idx].astype(INTENT_PRICE_DTYPE)  # Gather from donch_prev[1:]
    
    # Stage P2-3A: Order ID maintains deterministic ordering
    # Order ID is sequential (1, 2, 3, ...) based on created_bar order
    # Since created_bar is already sorted, this preserves deterministic ordering
    order_id = np.arange(1, n_entry + 1, dtype=INDEX_DTYPE)
    role = np.full(n_entry, ROLE_ENTRY, dtype=INTENT_ENUM_DTYPE)
    kind = np.full(n_entry, KIND_STOP, dtype=INTENT_ENUM_DTYPE)
    side = np.full(n_entry, SIDE_BUY, dtype=INTENT_ENUM_DTYPE)
    qty = np.full(n_entry, int(order_qty), dtype=INDEX_DTYPE)
    
    return {
        "created_bar": created_bar,
        "price": price,
        "order_id": order_id,
        "role": role,
        "kind": kind,
        "side": side,
        "qty": qty,
        "n_entry": n_entry,
        "obs": obs,
    }


@dataclass(frozen=True)
class DonchianAtrParams:
    channel_len: int
    atr_len: int
    stop_mult: float


def _max_drawdown(equity: np.ndarray) -> float:
    """
    Vectorized max drawdown on an equity curve.
    Handles empty arrays gracefully.
    """
    if equity.size == 0:
        return 0.0
    peak = np.maximum.accumulate(equity)
    dd = equity - peak
    mdd = float(np.min(dd))  # negative or 0
    return mdd


def run_kernel_object_mode(
    bars: BarArrays,
    params: DonchianAtrParams,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
    precomp: Optional[PrecomputedIndicators] = None,
) -> Dict[str, object]:
    """
    Golden Kernel (GKV): single-source-of-truth kernel for Phase 3A and future Phase 3B.

    Strategy (minimal):
      - Entry: Buy Stop at Donchian High (rolling max of HIGH over channel_len) at bar close -> next bar active.
      - Exit: Sell Stop at (entry_fill_price - stop_mult * ATR_wilder) active from next bar after entry_fill.

    Costs:
      - commission (absolute per trade)
      - slip (absolute per trade)
      Costs are applied on each round-trip fill (entry and exit each incur cost).

    Returns:
      dict with:
        - fills: List[Fill]
        - pnl: np.ndarray (float64, per-round-trip pnl, can be empty)
        - equity: np.ndarray (float64, cumsum of pnl, can be empty)
        - metrics: dict (net_profit, trades, max_dd)
    """
    profile = os.environ.get("FISHBRO_PROFILE_KERNEL", "").strip() == "1"
    t0 = time.perf_counter() if profile else 0.0

    # --- Compute indicators (kernel level; wrapper must ensure contiguous arrays) ---
    ch = int(params.channel_len)
    atr_n = int(params.atr_len)
    stop_mult = float(params.stop_mult)

    if ch <= 0 or atr_n <= 0:
        # invalid params -> zero trades, deterministic
        pnl = np.empty(0, dtype=np.float64)
        equity = np.empty(0, dtype=np.float64)
        # Evidence fields (Source of Truth) - Phase 3.0-A: must not be null
        # Red Team requirement: if fallback to objects mode, must leave fingerprint
        return {
            "fills": [],
            "pnl": pnl,
            "equity": equity,
            "metrics": {"net_profit": 0.0, "trades": 0, "max_dd": 0.0},
            "_obs": {
                "intent_mode": "objects",
                "intents_total": 0,
                "fills_total": 0,
            },
        }

    # Stage P2-2 Step B2: Use precomputed indicators if available, otherwise compute
    if precomp is not None:
        donch_hi = precomp.donch_hi
        atr = precomp.atr
    else:
        donch_hi = rolling_max(bars.high, ch)  # includes current bar
        atr = atr_wilder(bars.high, bars.low, bars.close, atr_n)
    t_ind = time.perf_counter() if profile else 0.0

    # --- Build order intents (next-bar active) ---
    intents: List[OrderIntent] = []
    # CURSOR TASK 5: Use deterministic order ID generation (pure function)
    from FishBroWFS_V2.engine.order_id import generate_order_id

    # We create entry intents for each bar t where indicator exists:
    # created_bar=t, active at t+1. price=donch_hi[t]
    n = int(bars.open.shape[0])
    for t in range(n):
        px = float(donch_hi[t])
        if np.isnan(px):
            continue
        # CURSOR TASK 5: Generate deterministic order_id
        oid = generate_order_id(
            created_bar=t,
            param_idx=0,  # Single param kernel
            role=ROLE_ENTRY,
            kind=KIND_STOP,
            side=SIDE_BUY,
        )
        intents.append(
            OrderIntent(
                order_id=oid,
                created_bar=t,
                role=OrderRole.ENTRY,
                kind=OrderKind.STOP,
                side=Side.BUY,
                price=px,
                qty=order_qty,
            )
        )
    t_intents = time.perf_counter() if profile else 0.0

    # Run matcher (JIT or python via kill-switch)
    fills: List[Fill] = simulate_matcher(bars, intents)
    t_sim1 = time.perf_counter() if profile else 0.0

    # --- Convert fills -> round-trip pnl (vectorized style, no python trade loops as truth) ---
    # For this minimal kernel we assume:
    # - Only LONG trades (BUY entry, SELL exit) will be produced once we add exits.
    # Phase 3A GKV: We implement exits by post-processing: when entry fills, schedule a sell stop from next bar.
    # To preserve Homology, we do a second matcher pass with generated exit intents.
    # This keeps all fill semantics inside the matcher (constitution).
    exit_intents: List[OrderIntent] = []
    for f in fills:
        if f.role != OrderRole.ENTRY:
            continue
        # exit stop price = entry_price - stop_mult * atr at entry bar
        ebar = int(f.bar_index)
        if ebar < 0 or ebar >= n:
            continue
        a = float(atr[ebar])
        if np.isnan(a):
            continue
        stop_px = float(f.price - stop_mult * a)
        # CURSOR TASK 5: Generate deterministic order_id for exit
        exit_oid = generate_order_id(
            created_bar=ebar,
            param_idx=0,  # Single param kernel
            role=ROLE_EXIT,
            kind=KIND_STOP,
            side=SIDE_SELL,
        )
        exit_intents.append(
            OrderIntent(
                order_id=exit_oid,
                created_bar=ebar,  # active next bar
                role=OrderRole.EXIT,
                kind=OrderKind.STOP,
                side=Side.SELL,
                price=stop_px,
                qty=order_qty,
            )
        )
    t_exit_intents = time.perf_counter() if profile else 0.0

    if exit_intents:
        fills2 = simulate_matcher(bars, exit_intents)
        t_sim2 = time.perf_counter() if profile else 0.0
        fills_all = fills + fills2
        # deterministic order: sort by (bar_index, role(ENTRY first), kind, order_id)
        fills_all.sort(key=lambda x: (x.bar_index, 0 if x.role == OrderRole.ENTRY else 1, 0 if x.kind == OrderKind.STOP else 1, x.order_id))
    else:
        fills_all = fills
        t_sim2 = t_sim1 if profile else 0.0

    # CURSOR TASK 1: Compute metrics from fills (unified source of truth)
    net_profit, trades, max_dd, equity = compute_metrics_from_fills(
        fills=fills_all,
        commission=commission,
        slip=slip,
        qty=order_qty,
    )
    
    # For backward compatibility, compute pnl array from equity (if needed)
    if equity.size > 0:
        pnl = np.diff(np.concatenate([[0.0], equity]))
    else:
        pnl = np.empty(0, dtype=np.float64)
    
    metrics = {
        "net_profit": net_profit,
        "trades": trades,
        "max_dd": max_dd,
    }
    out = {"fills": fills_all, "pnl": pnl, "equity": equity, "metrics": metrics}

    # Evidence fields (Source of Truth) - Phase 3.0-A
    # Red Team requirement: if fallback to objects mode, must leave fingerprint
    intents_total = int(len(intents) + len(exit_intents))  # Total intents (entry + exit, merged)
    fills_total = int(len(fills_all))  # fills_all is List[Fill], use len()
    
    # Always-on observability payload (no timing assumptions).
    out["_obs"] = {
        "intent_mode": "objects",
        "intents_total": intents_total,
        "fills_total": fills_total,
        "entry_intents": int(len(intents)),
        "exit_intents": int(len(exit_intents)),
    }

    if profile:
        out["_profile"] = {
            "intent_mode": "objects",
            "indicators_s": float(t_ind - t0),
            "intent_gen_s": float(t_intents - t_ind),
            "simulate_entry_s": float(t_sim1 - t_intents),
            "exit_intent_gen_s": float(t_exit_intents - t_sim1),
            "simulate_exit_s": float(t_sim2 - t_exit_intents),
            "kernel_total_s": float(t_sim2 - t0),
            "entry_intents": int(len(intents)),
            "exit_intents": int(len(exit_intents)),
        }
    return out


def run_kernel_arrays(
    bars: BarArrays,
    params: DonchianAtrParams,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
    return_debug: bool = False,
    precomp: Optional[PrecomputedIndicators] = None,
    intent_sparse_rate: float = 1.0,  # CURSOR TASK 3: Intent sparse rate from grid
) -> Dict[str, object]:
    """
    Array/SoA intent mode: generates intents as arrays and calls engine_jit.simulate_arrays().
    This avoids OrderIntent object construction in the hot path.
    
    Args:
        precomp: Optional pre-computed indicators. If provided, skips indicator computation
                 and uses precomputed arrays. If None, computes indicators normally (backward compatible).
    """
    profile = os.environ.get("FISHBRO_PROFILE_KERNEL", "").strip() == "1"
    t0 = time.perf_counter() if profile else 0.0
    
    # Stage P2-1.8: Initialize granular timers for breakdown
    from FishBroWFS_V2.perf.timers import PerfTimers
    timers = PerfTimers()
    timers.start("t_total_kernel")
    
    # Task 1A: Define required timing keys (contract enforcement)
    REQUIRED_TIMING_KEYS = (
        "t_calc_indicators_s",
        "t_build_entry_intents_s",
        "t_simulate_entry_s",
        "t_calc_exits_s",
        "t_simulate_exit_s",
        "t_total_kernel_s",
    )

    ch = int(params.channel_len)
    atr_n = int(params.atr_len)
    stop_mult = float(params.stop_mult)

    if ch <= 0 or atr_n <= 0:
        timers.stop("t_total_kernel")
        timing_dict = timers.as_dict_seconds()
        # Task 1B: Ensure all required timing keys exist (setdefault 0.0)
        for k in REQUIRED_TIMING_KEYS:
            timing_dict.setdefault(k, 0.0)
        pnl = np.empty(0, dtype=np.float64)
        equity = np.empty(0, dtype=np.float64)
        # Evidence fields (Source of Truth) - Phase 3.0-A: must not be null
        # Task 1C: Fix early return - inject timing_dict into _obs
        result = {
            "fills": [],
            "pnl": pnl,
            "equity": equity,
            "metrics": {"net_profit": 0.0, "trades": 0, "max_dd": 0.0},
            "_obs": {
                "intent_mode": "arrays",
                "intents_total": 0,
                "fills_total": 0,
                "entry_intents_total": 0,
                "entry_fills_total": 0,
                "exit_intents_total": 0,
                "exit_fills_total": 0,
                **timing_dict,  # Task 1C: Include timing keys in _obs
            },
            "_perf": timing_dict,
        }
        return result

    # Stage P2-2 Step B2: Use precomputed indicators if available, otherwise compute
    if precomp is not None:
        # Use precomputed indicators (skip computation, timing will be ~0)
        donch_hi = precomp.donch_hi
        donch_lo = precomp.donch_lo
        atr = precomp.atr
        # Still record timing (will be ~0 since we skipped computation)
        timers.start("t_ind_donchian")
        timers.stop("t_ind_donchian")
        timers.start("t_ind_atr")
        timers.stop("t_ind_atr")
    else:
        # Stage P2-2 Step A: Micro-profiling - Split indicators timing
        # t_ind_donchian_s: Donchian rolling max/min (highest/lowest)
        timers.start("t_ind_donchian")
        donch_hi = rolling_max(bars.high, ch)
        donch_lo = rolling_min(bars.low, ch)  # Also compute low for consistency
        timers.stop("t_ind_donchian")
        
        # t_ind_atr_s: ATR Wilder (TR + RMA/ATR)
        timers.start("t_ind_atr")
        atr = atr_wilder(bars.high, bars.low, bars.close, atr_n)
        timers.stop("t_ind_atr")
    
    t_ind = time.perf_counter() if profile else 0.0

    # Stage P2-1.8: t_build_entry_intents_s - Build entry intents (shift, mask, build)
    timers.start("t_build_entry_intents")
    # Fix 2: Shift donchian for next-bar active (created_bar = t-1, price = donch_hi[t-1])
    # Entry orders generated at bar t-1 close, active at bar t, stop price = donch_hi[t-1]
    donch_prev = np.empty_like(donch_hi)
    donch_prev[0] = np.nan
    donch_prev[1:] = donch_hi[:-1]

    # Stage P2-3A: Check if we should use Numba-accelerated sparse builder
    use_numba_builder = os.environ.get("FISHBRO_FORCE_SPARSE_BUILDER", "").strip() == "1"
    
    # CURSOR TASK 3: Use intent_sparse_rate from grid (passed as parameter)
    # Fallback to env var if not provided (for backward compatibility)
    trigger_rate = intent_sparse_rate
    if trigger_rate == 1.0:  # Only check env if not explicitly passed
        trigger_rate_env = os.environ.get("FISHBRO_PERF_TRIGGER_RATE", "").strip()
        if trigger_rate_env:
            try:
                trigger_rate = float(trigger_rate_env)
                if not (0.0 <= trigger_rate <= 1.0):
                    trigger_rate = 1.0
            except ValueError:
                trigger_rate = 1.0
    
    # Debug instrumentation: track first entry/exit per param (only if return_debug=True)
    if return_debug:
        dbg_entry_bar = -1
        dbg_entry_price = np.nan
        dbg_exit_bar = -1
        dbg_exit_price = np.nan
    else:
        dbg_entry_bar = None
        dbg_entry_price = None
        dbg_exit_bar = None
        dbg_exit_price = None

    # Build entry intents (choose builder based on env flags)
    use_dense_builder = os.environ.get("FISHBRO_USE_DENSE_BUILDER", "").strip() == "1"
    
    if use_numba_builder:
        # Stage P2-3A: Use Numba-accelerated sparse builder (trigger_rate integrated)
        from FishBroWFS_V2.strategy.entry_builder_nb import build_entry_intents_numba
        entry_intents_result = build_entry_intents_numba(
            donch_prev=donch_prev,
            channel_len=ch,
            order_qty=order_qty,
            trigger_rate=trigger_rate,
            seed=42,  # Fixed seed for deterministic selection
        )
        entry_builder_impl = "numba_single_pass"
    elif use_dense_builder:
        # Reference dense builder (for comparison/testing)
        entry_intents_result = _build_entry_intents_from_trigger(
            donch_prev=donch_prev,
            channel_len=ch,
            order_qty=order_qty,
        )
        entry_builder_impl = "python_dense_reference"
    else:
        # Default: Use new sparse builder (supports trigger_rate natively)
        from FishBroWFS_V2.strategy.builder_sparse import build_intents_sparse
        entry_intents_result = build_intents_sparse(
            donch_prev=donch_prev,
            channel_len=ch,
            order_qty=order_qty,
            trigger_rate=trigger_rate,  # CURSOR TASK 3: Use intent_sparse_rate
            seed=42,  # Fixed seed for deterministic selection
            use_dense=False,  # Use sparse mode (default)
        )
        entry_builder_impl = "python_sparse_default"
    timers.stop("t_build_entry_intents")
    
    created_bar = entry_intents_result["created_bar"]
    price = entry_intents_result["price"]
    # CURSOR TASK 5: Use deterministic order ID generation (pure function)
    # Override order_id from builder with deterministic version
    from FishBroWFS_V2.engine.order_id import generate_order_ids_array
    order_id = generate_order_ids_array(
        created_bar=created_bar,
        param_idx=0,  # Single param kernel (param_idx not available here)
        role=entry_intents_result.get("role"),
        kind=entry_intents_result.get("kind"),
        side=entry_intents_result.get("side"),
    )
    role = entry_intents_result["role"]
    kind = entry_intents_result["kind"]
    side = entry_intents_result["side"]
    qty = entry_intents_result["qty"]
    n_entry = entry_intents_result["n_entry"]
    obs_extra = entry_intents_result["obs"]
    
    # Stage P2-3A: Add builder implementation info to obs
    obs_extra = dict(obs_extra)  # Ensure mutable
    obs_extra["entry_builder_impl"] = entry_builder_impl
    
    if n_entry == 0:
        # No valid entry intents
        timers.stop("t_total_kernel")
        timing_dict = timers.as_dict_seconds()
        # Task 1B: Ensure all required timing keys exist (setdefault 0.0)
        for k in REQUIRED_TIMING_KEYS:
            timing_dict.setdefault(k, 0.0)
        pnl = np.empty(0, dtype=np.float64)
        equity = np.empty(0, dtype=np.float64)
        metrics = {"net_profit": 0.0, "trades": 0, "max_dd": 0.0}
        intents_total = 0
        fills_total = 0
        
        # CURSOR TASK 1: Set intents_total = entry_intents_total + exit_intents_total (accounting consistency)
        entry_intents_total_val = int(n_entry)  # 0 in this case
        exit_intents_total_val = 0  # No exit intents when n_entry == 0
        intents_total = entry_intents_total_val + exit_intents_total_val  # CURSOR TASK 1: Always sum
        
        # CURSOR TASK 4: Get entry_valid_mask_sum for MVP contract (bar-level indicator)
        entry_valid_mask_sum = int(obs_extra.get("entry_valid_mask_sum", obs_extra.get("valid_mask_sum", 0)))
        n_bars_val = int(obs_extra.get("n_bars", bars.open.shape[0]))
        warmup_val = int(obs_extra.get("warmup", ch))
        valid_mask_sum_val = int(obs_extra.get("valid_mask_sum", entry_valid_mask_sum))
        
        result = {
            "fills": [],
            "pnl": pnl,
            "equity": equity,
            "metrics": metrics,
            "_obs": {
                "intent_mode": "arrays",
                "intents_total": intents_total,  # CURSOR TASK 1: entry_intents_total + exit_intents_total
                "intents_total_reported": intents_total,  # Same as intents_total (0 in this case)
                "fills_total": fills_total,
                "entry_intents_total": entry_intents_total_val,  # CURSOR TASK 4: Required key
                "exit_intents_total": exit_intents_total_val,  # CURSOR TASK 1: Required for accounting consistency
                "entry_fills_total": 0,
                "exit_fills_total": 0,
                "n_bars": n_bars_val,  # CURSOR TASK 4: Required key
                "warmup": warmup_val,  # CURSOR TASK 4: Required key
                "valid_mask_sum": valid_mask_sum_val,  # CURSOR TASK 4: Required key
                "entry_valid_mask_sum": entry_valid_mask_sum,  # CURSOR TASK 4: Required key
                **obs_extra,  # Include diagnostic observations from entry intent builder
                **timing_dict,  # Stage P2-1.8: Include timing keys in _obs
            },
            "_perf": timing_dict,  # Keep _perf for backward compatibility
        }
        if return_debug:
            result["_debug"] = {
                "entry_bar": dbg_entry_bar,
                "entry_price": dbg_entry_price,
                "exit_bar": dbg_exit_bar,
                "exit_price": dbg_exit_price,
            }
        
        # --- P2-1.6 Observability alias (kernel-native) ---
        obs = result.setdefault("_obs", {})
        # Canonical entry sparse keys expected by perf/tests
        # CURSOR TASK 2: entry_valid_mask_sum should come from obs_extra (builder), not valid_mask_sum
        if "entry_valid_mask_sum" not in obs:
            obs.setdefault("entry_valid_mask_sum", int(obs.get("entry_valid_mask_sum", 0)))
        # entry_intents_total should already be set above (n_entry = 0 in this case)
        if "entry_intents_total" not in obs:
            obs["entry_intents_total"] = int(n_entry)
        
        return result

    # Arrays are already built by _build_entry_intents_from_trigger
    t_intents = time.perf_counter() if profile else 0.0

    # CURSOR TASK 2: Simulate entry intents first (parity with object-mode)
    # This ensures exit intents are only generated after entry fills occur
    timers.start("t_simulate_entry")
    entry_fills: List[Fill] = simulate_matcher_arrays(
        bars,
        order_id=order_id,
        created_bar=created_bar,
        role=role,
        kind=kind,
        side=side,
        price=price,
        qty=qty,
        ttl_bars=1,
    )
    timers.stop("t_simulate_entry")
    t_sim1 = time.perf_counter() if profile else 0.0

    # CURSOR TASK 2: Build exit intents from entry fills (not from entry intents)
    # This matches object-mode behavior: exit intents only generated after entry fills
    timers.start("t_calc_exits")
    from FishBroWFS_V2.config.dtypes import (
        INDEX_DTYPE,
        INTENT_ENUM_DTYPE,
        INTENT_PRICE_DTYPE,
    )
    
    # Build exit intents for each entry fill (parity with object-mode)
    exit_intents_list = []
    n_bars = int(bars.open.shape[0])
    for f in entry_fills:
        if f.role != OrderRole.ENTRY or f.side != Side.BUY:
            continue
        ebar = int(f.bar_index)
        if ebar < 0 or ebar >= n_bars:
            continue
        # Get ATR at entry fill bar
        atr_e = float(atr[ebar])
        if not np.isfinite(atr_e) or atr_e <= 0:
            # Invalid ATR: skip this entry (no exit intent)
            continue
        # Compute exit stop price from entry fill price
        exit_stop = float(f.price - stop_mult * atr_e)
        exit_intents_list.append({
            "created_bar": ebar,  # Same as entry fill bar (allows same-bar entry then exit)
            "price": exit_stop,
        })
    
    exit_intents_count = len(exit_intents_list)
    timers.stop("t_calc_exits")
    t_exit_intents = time.perf_counter() if profile else 0.0

    # CURSOR TASK 2 & 3: Simulate exit intents, then merge fills
    # Sort intents properly (created_bar, order_id) before simulate
    timers.start("t_simulate_exit")
    if exit_intents_count > 0:
        # Build exit intent arrays
        exit_created = np.asarray([ei["created_bar"] for ei in exit_intents_list], dtype=INDEX_DTYPE)
        exit_price = np.asarray([ei["price"] for ei in exit_intents_list], dtype=INTENT_PRICE_DTYPE)
        # CURSOR TASK 5: Use deterministic order ID generation for exit intents
        from FishBroWFS_V2.engine.order_id import generate_order_id
        exit_order_id_list = []
        for i, ebar in enumerate(exit_created):
            exit_oid = generate_order_id(
                created_bar=int(ebar),
                param_idx=0,  # Single param kernel
                role=ROLE_EXIT,
                kind=KIND_STOP,
                side=SIDE_SELL,
            )
            exit_order_id_list.append(exit_oid)
        exit_order_id = np.asarray(exit_order_id_list, dtype=INDEX_DTYPE)
        exit_role = np.full(exit_intents_count, ROLE_EXIT, dtype=INTENT_ENUM_DTYPE)
        exit_kind = np.full(exit_intents_count, KIND_STOP, dtype=INTENT_ENUM_DTYPE)
        exit_side = np.full(exit_intents_count, SIDE_SELL, dtype=INTENT_ENUM_DTYPE)
        exit_qty = np.full(exit_intents_count, int(order_qty), dtype=INDEX_DTYPE)
        
        # CURSOR TASK 3: Sort exit intents by created_bar, then order_id
        exit_sort_idx = np.lexsort((exit_order_id, exit_created))
        exit_order_id = exit_order_id[exit_sort_idx]
        exit_created = exit_created[exit_sort_idx]
        exit_price = exit_price[exit_sort_idx]
        exit_role = exit_role[exit_sort_idx]
        exit_kind = exit_kind[exit_sort_idx]
        exit_side = exit_side[exit_sort_idx]
        exit_qty = exit_qty[exit_sort_idx]
        
        # Simulate exit intents
        exit_fills: List[Fill] = simulate_matcher_arrays(
            bars,
            order_id=exit_order_id,
            created_bar=exit_created,
            role=exit_role,
            kind=exit_kind,
            side=exit_side,
            price=exit_price,
            qty=exit_qty,
            ttl_bars=1,
        )
        
        # Merge entry and exit fills, sort by (bar_index, role, kind, order_id)
        fills_all = entry_fills + exit_fills
        fills_all.sort(
            key=lambda x: (
                x.bar_index,
                0 if x.role == OrderRole.ENTRY else 1,
                0 if x.kind == OrderKind.STOP else 1,
                x.order_id,
            )
        )
    else:
        fills_all = entry_fills
    
    timers.stop("t_simulate_exit")
    t_sim2 = time.perf_counter() if profile else 0.0
    
    # Count entry and exit fills
    entry_fills_count = sum(1 for f in entry_fills if f.role == OrderRole.ENTRY and f.side == Side.BUY)
    if exit_intents_count > 0:
        exit_fills_count = sum(1 for f in fills_all if f.role == OrderRole.EXIT and f.side == Side.SELL)
    else:
        exit_fills_count = 0

    # Capture first entry fill for debug
    if return_debug and len(fills_all) > 0:
        first_entry = None
        for f in fills_all:
            if f.role == OrderRole.ENTRY and f.side == Side.BUY:
                first_entry = f
                break
        if first_entry is not None:
            dbg_entry_bar = int(first_entry.bar_index)
            dbg_entry_price = float(first_entry.price)

    # Capture first exit fill for debug
    if return_debug and len(fills_all) > 0:
        first_exit = None
        for f in fills_all:
            if f.role == OrderRole.EXIT and f.side == Side.SELL:
                first_exit = f
                break
        if first_exit is not None:
            dbg_exit_bar = int(first_exit.bar_index)
            dbg_exit_price = float(first_exit.price)

    # CURSOR TASK 1: Compute metrics from fills (unified source of truth)
    net_profit, trades, max_dd, equity = compute_metrics_from_fills(
        fills=fills_all,
        commission=commission,
        slip=slip,
        qty=order_qty,
    )
    
    # For backward compatibility, compute pnl array from equity (if needed)
    if equity.size > 0:
        pnl = np.diff(np.concatenate([[0.0], equity]))
    else:
        pnl = np.empty(0, dtype=np.float64)
    
    metrics = {
        "net_profit": net_profit,
        "trades": trades,
        "max_dd": max_dd,
    }
    out = {"fills": fills_all, "pnl": pnl, "equity": equity, "metrics": metrics}

    # Evidence fields (Source of Truth) - Phase 3.0-A
    raw_intents_total = int(n_entry + exit_intents_count)  # Total raw intents (entry + exit)
    fills_total = int(len(fills_all))  # fills_all is List[Fill], use len()
    timers.stop("t_total_kernel")
    
    # Stage P2-1.8: Get timing dict and merge into _obs for aggregation
    timing_dict = timers.as_dict_seconds()
    # Task 1B: Ensure all required timing keys exist (setdefault 0.0)
    for k in REQUIRED_TIMING_KEYS:
        timing_dict.setdefault(k, 0.0)
    
    # CURSOR TASK 1: Set intents_total = entry_intents_total + exit_intents_total (accounting consistency)
    entry_intents_total_val = int(n_entry)
    exit_intents_total_val = int(exit_intents_count)
    intents_total = entry_intents_total_val + exit_intents_total_val  # CURSOR TASK 1: Always sum
    
    # CURSOR TASK 4: Get entry_valid_mask_sum for MVP contract (bar-level indicator)
    entry_valid_mask_sum = int(obs_extra.get("entry_valid_mask_sum", obs_extra.get("valid_mask_sum", 0)))
    
    # CURSOR TASK 2: Ensure entry_intents_total is set correctly (from n_entry, not valid_mask_sum)
    # Override any value from obs_extra with actual n_entry
    obs_extra_final = dict(obs_extra)  # Copy to avoid modifying original
    obs_extra_final["entry_intents_total"] = entry_intents_total_val  # Always use actual n_entry
    
    # CURSOR TASK 4: Ensure all required obs keys exist
    n_bars_val = int(obs_extra_final.get("n_bars", bars.open.shape[0]))
    warmup_val = int(obs_extra_final.get("warmup", ch))
    valid_mask_sum_val = int(obs_extra_final.get("valid_mask_sum", entry_valid_mask_sum))
    
    out["_obs"] = {
        "intent_mode": "arrays",
        "intents_total": intents_total,  # CURSOR TASK 1: entry_intents_total + exit_intents_total
        "intents_total_reported": raw_intents_total,  # Raw intent count (same as intents_total for accounting)
        "fills_total": fills_total,
        "entry_intents": int(n_entry),
        "exit_intents": int(exit_intents_count),
        "n_bars": n_bars_val,  # CURSOR TASK 4: Required key
        "warmup": warmup_val,  # CURSOR TASK 4: Required key
        "valid_mask_sum": valid_mask_sum_val,  # CURSOR TASK 4: Required key (dense valid mask sum)
        "entry_valid_mask_sum": entry_valid_mask_sum,  # CURSOR TASK 4: Required key (after sparse)
        "entry_intents_total": entry_intents_total_val,  # CURSOR TASK 4: Required key
        "exit_intents_total": exit_intents_total_val,  # CURSOR TASK 1: Required for accounting consistency
        "entry_fills_total": int(entry_fills_count),
        "exit_fills_total": int(exit_fills_count),
        **obs_extra_final,  # Include diagnostic observations from entry intent builder
        **timing_dict,  # Stage P2-1.8: Include timing keys in _obs for aggregation
    }
    out["_perf"] = timing_dict  # Keep _perf for backward compatibility
    if return_debug:
        out["_debug"] = {
            "entry_bar": dbg_entry_bar,
            "entry_price": dbg_entry_price,
            "exit_bar": dbg_exit_bar,
            "exit_price": dbg_exit_price,
        }
    if profile:
        # CURSOR TASK 2: Separate simulate calls (entry then exit), timing reflects actual calls
        out["_profile"] = {
            "intent_mode": "arrays",
            "indicators_s": float(t_ind - t0),
            "intent_gen_s": float(t_intents - t_ind),
            "simulate_entry_s": float(t_sim1 - t_intents),  # Entry simulation time
            "exit_intent_gen_s": float(t_exit_intents - t_sim1),  # Exit intent generation time
            "simulate_exit_s": float(t_sim2 - t_exit_intents),  # Exit simulation time
            "kernel_total_s": float(t_sim2 - t0),
            "entry_intents": int(n_entry),
            "exit_intents": int(exit_intents_count),
        }
    
    # --- P2-1.6 Observability alias (kernel-native) ---
    obs = out.setdefault("_obs", {})
    # Canonical entry sparse keys expected by perf/tests
    # CURSOR TASK 2: entry_valid_mask_sum should come from obs_extra (builder), not valid_mask_sum
    if "entry_valid_mask_sum" not in obs:
        obs.setdefault("entry_valid_mask_sum", int(obs.get("entry_valid_mask_sum", 0)))
    # entry_intents_total should already be set from obs_extra (n_entry)
    if "entry_intents_total" not in obs:
        obs["entry_intents_total"] = int(n_entry)
    
    return out


def run_kernel(
    bars: BarArrays,
    params: DonchianAtrParams,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
    return_debug: bool = False,
    precomp: Optional[PrecomputedIndicators] = None,
    intent_sparse_rate: float = 1.0,  # CURSOR TASK 3: Intent sparse rate from grid
) -> Dict[str, object]:
    # Default to arrays path for perf; object mode remains as a correctness reference.
    mode = os.environ.get("FISHBRO_KERNEL_INTENT_MODE", "").strip().lower()
    if mode == "objects":
        return run_kernel_object_mode(
            bars,
            params,
            commission=commission,
            slip=slip,
            order_qty=order_qty,
        )
    return run_kernel_arrays(
        bars,
        params,
        commission=commission,
        slip=slip,
        order_qty=order_qty,
        return_debug=return_debug,
        precomp=precomp,
    )




--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/strategy/param_schema.py
sha256(source_bytes) = 30f32bfebdaa7ea3675283049a310704d954b7bbec4e66b4014a3ae77182314d
bytes = 1503
redacted = False
--------------------------------------------------------------------------------

"""Strategy Parameter Schema for GUI introspection.

Phase 12: Strategy parameter schema definition for automatic UI generation.
GUI must NOT hardcode any strategy parameters.
"""

from __future__ import annotations

from typing import Any, Literal

from pydantic import BaseModel, ConfigDict, Field


class ParamSpec(BaseModel):
    """Specification for a single strategy parameter.
    
    Used by GUI to generate appropriate input widgets.
    """
    
    model_config = ConfigDict(frozen=True)
    
    name: str = Field(
        ...,
        description="Parameter name (must match strategy implementation)",
        examples=["window", "threshold", "enabled"]
    )
    
    type: Literal["int", "float", "enum", "bool"] = Field(
        ...,
        description="Parameter data type"
    )
    
    min: int | float | None = Field(
        default=None,
        description="Minimum value (for int/float types)"
    )
    
    max: int | float | None = Field(
        default=None,
        description="Maximum value (for int/float types)"
    )
    
    step: int | float | None = Field(
        default=None,
        description="Step size (for int/float sliders)"
    )
    
    choices: list[str] | None = Field(
        default=None,
        description="Allowed choices (for enum type)"
    )
    
    default: Any = Field(
        ...,
        description="Default value"
    )
    
    help: str = Field(
        ...,
        description="Human-readable description/help text"
    )



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/strategy/registry.py
sha256(source_bytes) = f68664a172cbc3aef283b43dd3b0a43498787443e6792ae647adc82bf15e42d1
bytes = 5344
redacted = False
--------------------------------------------------------------------------------

"""Strategy registry - single source of truth for strategies.

Phase 7: Centralized strategy registration and lookup.
Phase 12: Enhanced for GUI introspection with ParamSchema.
"""

from __future__ import annotations

from typing import Dict, List

from pydantic import BaseModel, ConfigDict

from FishBroWFS_V2.strategy.param_schema import ParamSpec
from FishBroWFS_V2.strategy.spec import StrategySpec


# Global registry (module-level, mutable)
_registry: Dict[str, StrategySpec] = {}


def register(spec: StrategySpec) -> None:
    """Register a strategy.
    
    Args:
        spec: Strategy specification
        
    Raises:
        ValueError: If strategy_id already registered
    """
    if spec.strategy_id in _registry:
        raise ValueError(
            f"Strategy '{spec.strategy_id}' already registered. "
            f"Use different strategy_id or unregister first."
        )
    _registry[spec.strategy_id] = spec


def get(strategy_id: str) -> StrategySpec:
    """Get strategy by ID.
    
    Args:
        strategy_id: Strategy identifier
        
    Returns:
        StrategySpec
        
    Raises:
        KeyError: If strategy not found
    """
    if strategy_id not in _registry:
        raise KeyError(f"Strategy '{strategy_id}' not found in registry")
    return _registry[strategy_id]


def list_strategies() -> List[StrategySpec]:
    """List all registered strategies.
    
    Returns:
        List of StrategySpec, sorted by strategy_id
    """
    return sorted(_registry.values(), key=lambda s: s.strategy_id)


def unregister(strategy_id: str) -> None:
    """Unregister a strategy (mainly for testing).
    
    Args:
        strategy_id: Strategy identifier
        
    Raises:
        KeyError: If strategy not found
    """
    if strategy_id not in _registry:
        raise KeyError(f"Strategy '{strategy_id}' not found in registry")
    del _registry[strategy_id]


def clear() -> None:
    """Clear all registered strategies (mainly for testing)."""
    _registry.clear()


def load_builtin_strategies() -> None:
    """Load built-in strategies (explicit, no import side effects).
    
    This function must be called explicitly to register built-in strategies.
    """
    from FishBroWFS_V2.strategy.builtin import (
        sma_cross_v1,
        breakout_channel_v1,
        mean_revert_zscore_v1,
    )
    
    # Register built-in strategies
    register(sma_cross_v1.SPEC)
    register(breakout_channel_v1.SPEC)
    register(mean_revert_zscore_v1.SPEC)


# Phase 12: Enhanced registry for GUI introspection
class StrategySpecForGUI(BaseModel):
    """Strategy specification for GUI consumption.
    
    Contains metadata and parameter schema for automatic UI generation.
    GUI must NOT hardcode any strategy parameters.
    """
    
    model_config = ConfigDict(frozen=True)
    
    strategy_id: str
    params: list[ParamSpec]


class StrategyRegistryResponse(BaseModel):
    """Response model for /meta/strategies endpoint."""
    
    model_config = ConfigDict(frozen=True)
    
    strategies: list[StrategySpecForGUI]


def convert_to_gui_spec(spec: StrategySpec) -> StrategySpecForGUI:
    """Convert internal StrategySpec to GUI-friendly format."""
    schema = spec.param_schema if isinstance(spec.param_schema, dict) else {}
    defaults = spec.defaults or {}
    
    # (1)  object/properties 
    if "properties" in schema and isinstance(schema.get("properties"), dict):
        props = schema.get("properties") or {}
    else:
        # (2)  dict  key  param
        props = schema
    
    params: list[ParamSpec] = []
    for name, info in props.items():
        if not isinstance(info, dict):
            continue
        
        raw_type = info.get("type", "float")
        enum_vals = info.get("enum")
        
        if enum_vals is not None:
            ptype = "enum"
            choices = list(enum_vals)
        elif raw_type in ("int", "integer"):
            ptype = "int"
            choices = None
        elif raw_type in ("bool", "boolean"):
            ptype = "bool"
            choices = None
        else:
            ptype = "float"
            choices = None
        
        default = defaults.get(name, info.get("default"))
        help_text = (
            info.get("description")
            or info.get("title")
            or f"{name} parameter"
        )
        
        params.append(
            ParamSpec(
                name=name,
                type=ptype,
                min=info.get("minimum"),
                max=info.get("maximum"),
                step=info.get("step") or info.get("multipleOf"),
                choices=choices,
                default=default,
                help=help_text,
            )
        )
    
    params.sort(key=lambda p: p.name)
    return StrategySpecForGUI(strategy_id=spec.strategy_id, params=params)


def get_strategy_registry() -> StrategyRegistryResponse:
    """Get strategy registry for GUI consumption.
    
    Returns:
        StrategyRegistryResponse with all registered strategies
        converted to GUI-friendly format.
    """
    strategies = []
    for spec in list_strategies():
        gui_spec = convert_to_gui_spec(spec)
        strategies.append(gui_spec)
    
    return StrategyRegistryResponse(strategies=strategies)



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/strategy/runner.py
sha256(source_bytes) = 205bd27dd3ba31467714cfd914b0de9ffb780b17efcdd880921f63fd69da4df9
bytes = 3607
redacted = False
--------------------------------------------------------------------------------

"""Strategy runner - adapter between strategy and engine.

Phase 7: Validates params, calls strategy function, returns intents.
"""

from __future__ import annotations

import logging
from typing import Dict, Any, List

from FishBroWFS_V2.engine.types import OrderIntent
from FishBroWFS_V2.strategy.registry import get
from FishBroWFS_V2.strategy.spec import StrategySpec

logger = logging.getLogger(__name__)


def run_strategy(
    strategy_id: str,
    features: Dict[str, Any],
    params: Dict[str, float],
    context: Dict[str, Any],
) -> List[OrderIntent]:
    """Run a strategy and return order intents.
    
    This function:
    1. Validates params (missing values use defaults, extra keys allowed but logged)
    2. Calls strategy function
    3. Returns intents (does NOT fill, does NOT compute indicators)
    
    Args:
        strategy_id: Strategy identifier
        features: Features/indicators dict (e.g., {"sma_fast": array, "sma_slow": array})
        params: Strategy parameters dict (e.g., {"fast_period": 10, "slow_period": 20})
        context: Execution context (e.g., {"bar_index": 100, "order_qty": 1})
        
    Returns:
        List of OrderIntent
        
    Raises:
        KeyError: If strategy not found
        ValueError: If strategy output is invalid
    """
    # Get strategy spec
    spec: StrategySpec = get(strategy_id)
    
    # Merge context and features for strategy input
    strategy_input = {**context, "features": features}
    
    # Validate and merge params with defaults
    validated_params = _validate_params(params, spec)
    
    # Call strategy function
    result = spec.fn(strategy_input, validated_params)
    
    # Validate output
    if not isinstance(result, dict):
        raise ValueError(f"Strategy '{strategy_id}' must return dict, got {type(result)}")
    
    if "intents" not in result:
        raise ValueError(f"Strategy '{strategy_id}' output must contain 'intents' key")
    
    intents = result["intents"]
    if not isinstance(intents, list):
        raise ValueError(f"Strategy '{strategy_id}' intents must be list, got {type(intents)}")
    
    # Validate each intent
    for i, intent in enumerate(intents):
        if not isinstance(intent, OrderIntent):
            raise ValueError(
                f"Strategy '{strategy_id}' intent[{i}] must be OrderIntent, got {type(intent)}"
            )
    
    return intents


def _validate_params(params: Dict[str, float], spec: StrategySpec) -> Dict[str, float]:
    """Validate and merge params with defaults.
    
    Rules:
    - Missing params use defaults
    - Extra keys allowed but logged
    - Type validation (minimal)
    
    Args:
        params: User-provided parameters
        spec: Strategy specification
        
    Returns:
        Validated parameters dict (merged with defaults)
    """
    validated = dict(spec.defaults)  # Start with defaults
    
    # Override with user params
    for key, value in params.items():
        if key not in spec.defaults:
            # Extra key - log but allow
            logger.warning(
                f"Strategy '{spec.strategy_id}': extra parameter '{key}' not in schema, "
                f"will be ignored"
            )
            continue
        
        # Type validation (minimal - just check it's numeric)
        if not isinstance(value, (int, float)):
            raise ValueError(
                f"Strategy '{spec.strategy_id}': parameter '{key}' must be numeric, "
                f"got {type(value)}"
            )
        
        validated[key] = float(value)
    
    return validated



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/strategy/runner_single.py
sha256(source_bytes) = f82b6f7b4ce1bb5a844fae24c22effcf0bdc21cffcef83b4a2d56c7dcfe9ccfb
bytes = 1167
redacted = False
--------------------------------------------------------------------------------

from __future__ import annotations

from typing import Dict

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.types import BarArrays
from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, run_kernel


def run_single(
    open_: np.ndarray,
    high: np.ndarray,
    low: np.ndarray,
    close: np.ndarray,
    params: DonchianAtrParams,
    *,
    commission: float,
    slip: float,
    order_qty: int = 1,
) -> Dict[str, object]:
    """
    Wrapper for Phase 3A (GKV): ensure memory layout + call kernel once.
    """
    bars: BarArrays = normalize_bars(open_, high, low, close)

    # Boundary Layout Check: enforce contiguous arrays before entering kernel.
    if not bars.open.flags["C_CONTIGUOUS"]:
        bars = BarArrays(
            open=np.ascontiguousarray(bars.open, dtype=np.float64),
            high=np.ascontiguousarray(bars.high, dtype=np.float64),
            low=np.ascontiguousarray(bars.low, dtype=np.float64),
            close=np.ascontiguousarray(bars.close, dtype=np.float64),
        )

    return run_kernel(bars, params, commission=commission, slip=slip, order_qty=order_qty)




--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/strategy/spec.py
sha256(source_bytes) = fa377b53669c9b62db4f389df7369f0f528d12a6309f57df2dd905035d9056f9
bytes = 1789
redacted = False
--------------------------------------------------------------------------------

"""Strategy specification and function type definitions.

Phase 7: Strategy system core data structures.
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Callable, Dict, Any, Mapping, List

from FishBroWFS_V2.engine.types import OrderIntent


# Strategy function signature:
# input: (context/features: dict, params: dict)
# output: {"intents": List[OrderIntent], "debug": dict}
StrategyFn = Callable[
    [Mapping[str, Any], Mapping[str, float]],  # (context/features, params)
    Mapping[str, Any]                          # {"intents": [...], "debug": {...}}
]


@dataclass(frozen=True)
class StrategySpec:
    """Strategy specification.
    
    Contains all metadata and function for a strategy.
    
    Attributes:
        strategy_id: Unique strategy identifier (e.g., "sma_cross")
        version: Strategy version (e.g., "v1")
        param_schema: Parameter schema definition (jsonschema-like dict)
        defaults: Default parameter values (dict, key-value pairs)
        fn: Strategy function (StrategyFn)
    """
    strategy_id: str
    version: str
    param_schema: Dict[str, Any]  # jsonschema-like dict, minimal
    defaults: Dict[str, float]
    fn: StrategyFn
    
    def __post_init__(self) -> None:
        """Validate strategy spec."""
        if not self.strategy_id:
            raise ValueError("strategy_id cannot be empty")
        if not self.version:
            raise ValueError("version cannot be empty")
        if not isinstance(self.param_schema, dict):
            raise ValueError("param_schema must be a dict")
        if not isinstance(self.defaults, dict):
            raise ValueError("defaults must be a dict")
        if not callable(self.fn):
            raise ValueError("fn must be callable")



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/strategy/builtin/__init__.py
sha256(source_bytes) = 3a8015a97f5c1e6d0fb7c6aa16ce393c2618d6b33064c284722b85527ef46225
bytes = 79
redacted = False
--------------------------------------------------------------------------------

"""Built-in strategies.

Phase 7: MVP strategies for system validation.
"""



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/strategy/builtin/breakout_channel_v1.py
sha256(source_bytes) = 8a154f8d1cd89723f118d3819ffdccf7ac495d38e5e62bb707109b484fb640d5
bytes = 3633
redacted = False
--------------------------------------------------------------------------------

"""Breakout Channel Strategy v1.

Phase 7: Channel breakout strategy using high/low.
Entry: When price breaks above channel high (breakout).
"""

from __future__ import annotations

from typing import Dict, Any, Mapping

import numpy as np

from FishBroWFS_V2.engine.types import OrderIntent, OrderRole, OrderKind, Side
from FishBroWFS_V2.engine.order_id import generate_order_id
from FishBroWFS_V2.engine.constants import ROLE_ENTRY, KIND_STOP, SIDE_BUY
from FishBroWFS_V2.strategy.spec import StrategySpec, StrategyFn


def breakout_channel_strategy(
    context: Mapping[str, Any],
    params: Mapping[str, float],
) -> Dict[str, Any]:
    """Breakout Channel Strategy implementation.
    
    Entry signal: Price breaks above channel high.
    
    Args:
        context: Execution context with features and bar_index
        params: Strategy parameters (channel_period)
        
    Returns:
        Dict with "intents" (List[OrderIntent]) and "debug" (dict)
    """
    features = context.get("features", {})
    bar_index = context.get("bar_index", 0)
    
    # Get features
    high = features.get("high")
    low = features.get("low")
    close = features.get("close")
    channel_high = features.get("channel_high")
    channel_low = features.get("channel_low")
    
    if high is None or close is None or channel_high is None:
        return {"intents": [], "debug": {"error": "Missing required features"}}
    
    # Convert to numpy arrays if needed
    if not isinstance(high, np.ndarray):
        high = np.array(high)
    if not isinstance(close, np.ndarray):
        close = np.array(close)
    if not isinstance(channel_high, np.ndarray):
        channel_high = np.array(channel_high)
    
    # Check bounds
    if bar_index >= len(high) or bar_index >= len(close) or bar_index >= len(channel_high):
        return {"intents": [], "debug": {"error": "bar_index out of bounds"}}
    
    # Need at least 1 bar
    if bar_index < 0:
        return {"intents": [], "debug": {}}
    
    curr_high = high[bar_index]
    curr_close = close[bar_index]
    curr_channel_high = channel_high[bar_index]
    
    # Check for breakout: current high breaks above channel high
    is_breakout = (
        curr_high > curr_channel_high and
        not np.isnan(curr_high) and
        not np.isnan(curr_channel_high)
    )
    
    intents = []
    if is_breakout:
        # Entry: Buy Stop at channel high (breakout level)
        order_id = generate_order_id(
            created_bar=bar_index,
            param_idx=0,
            role=ROLE_ENTRY,
            kind=KIND_STOP,
            side=SIDE_BUY,
        )
        
        intent = OrderIntent(
            order_id=order_id,
            created_bar=bar_index,
            role=OrderRole.ENTRY,
            kind=OrderKind.STOP,
            side=Side.BUY,
            price=float(curr_channel_high),
            qty=context.get("order_qty", 1),
        )
        intents.append(intent)
    
    return {
        "intents": intents,
        "debug": {
            "high": float(curr_high) if not np.isnan(curr_high) else None,
            "channel_high": float(curr_channel_high) if not np.isnan(curr_channel_high) else None,
            "is_breakout": is_breakout,
        },
    }


# Strategy specification
SPEC = StrategySpec(
    strategy_id="breakout_channel",
    version="v1",
    param_schema={
        "type": "object",
        "properties": {
            "channel_period": {"type": "number", "minimum": 1},
        },
        "required": ["channel_period"],
    },
    defaults={
        "channel_period": 20.0,
    },
    fn=breakout_channel_strategy,
)



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/strategy/builtin/mean_revert_zscore_v1.py
sha256(source_bytes) = ed3bd67d39090d533f5098efe5a70a7046a9aaa7ca04f211a01cf2319943ce55
bytes = 3416
redacted = False
--------------------------------------------------------------------------------

"""Mean Reversion Z-Score Strategy v1.

Phase 7: Mean reversion strategy using z-score.
Entry: When z-score is below threshold (oversold).
"""

from __future__ import annotations

from typing import Dict, Any, Mapping

import numpy as np

from FishBroWFS_V2.engine.types import OrderIntent, OrderRole, OrderKind, Side
from FishBroWFS_V2.engine.order_id import generate_order_id
from FishBroWFS_V2.engine.constants import ROLE_ENTRY, KIND_LIMIT, SIDE_BUY
from FishBroWFS_V2.strategy.spec import StrategySpec, StrategyFn


def mean_revert_zscore_strategy(
    context: Mapping[str, Any],
    params: Mapping[str, float],
) -> Dict[str, Any]:
    """Mean Reversion Z-Score Strategy implementation.
    
    Entry signal: Z-score below threshold (oversold, mean reversion buy).
    
    Args:
        context: Execution context with features and bar_index
        params: Strategy parameters (zscore_threshold)
        
    Returns:
        Dict with "intents" (List[OrderIntent]) and "debug" (dict)
    """
    features = context.get("features", {})
    bar_index = context.get("bar_index", 0)
    
    # Get features
    zscore = features.get("zscore")
    close = features.get("close")
    
    if zscore is None or close is None:
        return {"intents": [], "debug": {"error": "Missing zscore or close features"}}
    
    # Convert to numpy arrays if needed
    if not isinstance(zscore, np.ndarray):
        zscore = np.array(zscore)
    if not isinstance(close, np.ndarray):
        close = np.array(close)
    
    # Check bounds
    if bar_index >= len(zscore) or bar_index >= len(close):
        return {"intents": [], "debug": {"error": "bar_index out of bounds"}}
    
    # Need at least 1 bar
    if bar_index < 0:
        return {"intents": [], "debug": {}}
    
    curr_zscore = zscore[bar_index]
    curr_close = close[bar_index]
    threshold = params.get("zscore_threshold", -2.0)
    
    # Check for oversold condition: z-score below threshold
    is_oversold = (
        curr_zscore < threshold and
        not np.isnan(curr_zscore) and
        not np.isnan(curr_close)
    )
    
    intents = []
    if is_oversold:
        # Entry: Buy Limit at current close (mean reversion)
        order_id = generate_order_id(
            created_bar=bar_index,
            param_idx=0,
            role=ROLE_ENTRY,
            kind=KIND_LIMIT,
            side=SIDE_BUY,
        )
        
        intent = OrderIntent(
            order_id=order_id,
            created_bar=bar_index,
            role=OrderRole.ENTRY,
            kind=OrderKind.LIMIT,
            side=Side.BUY,
            price=float(curr_close),
            qty=context.get("order_qty", 1),
        )
        intents.append(intent)
    
    return {
        "intents": intents,
        "debug": {
            "zscore": float(curr_zscore) if not np.isnan(curr_zscore) else None,
            "close": float(curr_close) if not np.isnan(curr_close) else None,
            "threshold": threshold,
            "is_oversold": is_oversold,
        },
    }


# Strategy specification
SPEC = StrategySpec(
    strategy_id="mean_revert_zscore",
    version="v1",
    param_schema={
        "type": "object",
        "properties": {
            "zscore_threshold": {"type": "number"},
        },
        "required": ["zscore_threshold"],
    },
    defaults={
        "zscore_threshold": -2.0,
    },
    fn=mean_revert_zscore_strategy,
)



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/strategy/builtin/sma_cross_v1.py
sha256(source_bytes) = a739fde726f9dd7f8110730287dff9fe50db307594b6ce4dbef8ed1e0c1dcc9b
bytes = 3704
redacted = False
--------------------------------------------------------------------------------

"""SMA Cross Strategy v1.

Phase 7: Basic moving average crossover strategy.
Entry: When fast SMA crosses above slow SMA (golden cross).
"""

from __future__ import annotations

from typing import Dict, Any, Mapping

import numpy as np

from FishBroWFS_V2.engine.types import OrderIntent, OrderRole, OrderKind, Side
from FishBroWFS_V2.engine.order_id import generate_order_id
from FishBroWFS_V2.engine.constants import ROLE_ENTRY, KIND_STOP, SIDE_BUY
from FishBroWFS_V2.strategy.spec import StrategySpec, StrategyFn


def sma_cross_strategy(context: Mapping[str, Any], params: Mapping[str, float]) -> Dict[str, Any]:
    """SMA Cross Strategy implementation.
    
    Entry signal: Fast SMA crosses above slow SMA (golden cross).
    
    Args:
        context: Execution context with features and bar_index
        params: Strategy parameters (fast_period, slow_period)
        
    Returns:
        Dict with "intents" (List[OrderIntent]) and "debug" (dict)
    """
    features = context.get("features", {})
    bar_index = context.get("bar_index", 0)
    
    # Get features
    sma_fast = features.get("sma_fast")
    sma_slow = features.get("sma_slow")
    
    if sma_fast is None or sma_slow is None:
        return {"intents": [], "debug": {"error": "Missing SMA features"}}
    
    # Convert to numpy arrays if needed
    if not isinstance(sma_fast, np.ndarray):
        sma_fast = np.array(sma_fast)
    if not isinstance(sma_slow, np.ndarray):
        sma_slow = np.array(sma_slow)
    
    # Check bounds
    if bar_index >= len(sma_fast) or bar_index >= len(sma_slow):
        return {"intents": [], "debug": {"error": "bar_index out of bounds"}}
    
    # Need at least 2 bars to detect crossover
    if bar_index < 1:
        return {"intents": [], "debug": {}}
    
    # Check for golden cross (fast crosses above slow)
    prev_fast = sma_fast[bar_index - 1]
    prev_slow = sma_slow[bar_index - 1]
    curr_fast = sma_fast[bar_index]
    curr_slow = sma_slow[bar_index]
    
    # Golden cross: prev_fast <= prev_slow AND curr_fast > curr_slow
    is_golden_cross = (
        prev_fast <= prev_slow and
        curr_fast > curr_slow and
        not np.isnan(prev_fast) and
        not np.isnan(prev_slow) and
        not np.isnan(curr_fast) and
        not np.isnan(curr_slow)
    )
    
    intents = []
    if is_golden_cross:
        # Entry: Buy Stop at current fast SMA
        order_id = generate_order_id(
            created_bar=bar_index,
            param_idx=0,  # Single param set for this strategy
            role=ROLE_ENTRY,
            kind=KIND_STOP,
            side=SIDE_BUY,
        )
        
        intent = OrderIntent(
            order_id=order_id,
            created_bar=bar_index,
            role=OrderRole.ENTRY,
            kind=OrderKind.STOP,
            side=Side.BUY,
            price=float(curr_fast),
            qty=context.get("order_qty", 1),
        )
        intents.append(intent)
    
    return {
        "intents": intents,
        "debug": {
            "sma_fast": float(curr_fast) if not np.isnan(curr_fast) else None,
            "sma_slow": float(curr_slow) if not np.isnan(curr_slow) else None,
            "is_golden_cross": is_golden_cross,
        },
    }


# Strategy specification
SPEC = StrategySpec(
    strategy_id="sma_cross",
    version="v1",
    param_schema={
        "type": "object",
        "properties": {
            "fast_period": {"type": "number", "minimum": 1},
            "slow_period": {"type": "number", "minimum": 1},
        },
        "required": ["fast_period", "slow_period"],
    },
    defaults={
        "fast_period": 10.0,
        "slow_period": 20.0,
    },
    fn=sma_cross_strategy,
)



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/ui/plan_viewer.py
sha256(source_bytes) = 7e0b97ab1d64bb2dac9eddb612eaa873c748a96e87f718ebb776fab445845b60
bytes = 1645
redacted = False
--------------------------------------------------------------------------------

"""Pure page module for portfolio plan viewer (read-only, zero-write).

IMPORTANT:
- No main() function (conforms to single entrypoint rule)
- No side effects on import (no scanning, no file writes)
- All streamlit imports are deferred inside render_page()
- outputs_root must be injected by the entrypoint
"""
from __future__ import annotations

from pathlib import Path
from typing import Optional, List, Dict, Any

from FishBroWFS_V2.portfolio.plan_view_loader import load_plan_view_json


def scan_plan_ids(outputs_root: Path) -> List[str]:
    """Read-only: list plan_ids that have plan_view.json under outputs_root."""
    base = outputs_root / "portfolio" / "plans"
    if not base.exists():
        return []
    
    plan_ids: List[str] = []
    for p in sorted(base.iterdir(), key=lambda x: x.name):
        if not p.is_dir():
            continue
        if (p / "plan_view.json").exists():
            plan_ids.append(p.name)
    return plan_ids


def load_view(outputs_root: Path, plan_id: str) -> Dict[str, Any]:
    """Read-only: load view model."""
    plan_dir = outputs_root / "portfolio" / "plans" / plan_id
    view = load_plan_view_json(plan_dir)
    return view.model_dump()


def render_page(outputs_root: Path) -> None:
    """
    DEPRECATED: Streamlit page renderer - no longer used after migration to NiceGUI.
    
    This function is kept for compatibility but will raise an ImportError
    if streamlit is not available.
    """
    raise ImportError(
        "plan_viewer.py render_page() is deprecated. "
        "Streamlit UI has been migrated to NiceGUI. "
        "Use the NiceGUI dashboard instead."
    )



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/utils/__init__.py
sha256(source_bytes) = 99b29b3066d55c037c9e818ffe421ca6c3e08e7a5415f23ea108170be927673b
bytes = 359
redacted = False
--------------------------------------------------------------------------------

"""Utility modules for FishBroWFS_V2."""

from .write_scope import (
    WriteScope,
    create_plan_scope,
    create_plan_view_scope,
    create_plan_quality_scope,
    create_season_export_scope,
)

__all__ = [
    "WriteScope",
    "create_plan_scope",
    "create_plan_view_scope",
    "create_plan_quality_scope",
    "create_season_export_scope",
]



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/utils/fs_snapshot.py
sha256(source_bytes) = da11b51fbb344136a6bacc9750ff5214f8fe188feae54eb73876570600826f62
bytes = 3150
redacted = False
--------------------------------------------------------------------------------

"""File system snapshot utilities for hardening tests.

Provides deterministic snapshot of file trees with mtime and SHA256.
"""
from __future__ import annotations

import hashlib
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, Optional


@dataclass(frozen=True)
class FileSnap:
    """Immutable snapshot of a single file."""
    rel_path: str  # POSIX-style relative path using '/'
    size: int
    mtime_ns: int
    sha256: str


def compute_sha256(path: Path) -> str:
    """Compute SHA256 hash of file content.
    
    Uses existing compute_sha256 from control.artifacts if available,
    otherwise implements directly.
    """
    try:
        from FishBroWFS_V2.control.artifacts import compute_sha256 as cs
        return cs(path.read_bytes())
    except ImportError:
        # Fallback implementation
        return hashlib.sha256(path.read_bytes()).hexdigest()


def snapshot_tree(root: Path, *, include_sha256: bool = True) -> Dict[str, FileSnap]:
    """
    Deterministic snapshot of all files under root.
    
    Args:
        root: Directory root to snapshot.
        include_sha256: Whether to compute SHA256 hash (expensive for large files).
    
    Returns:
        Dictionary mapping relative path (POSIX-style) to FileSnap.
        Paths are sorted in stable alphabetical order.
    """
    snapshots: Dict[str, FileSnap] = {}
    
    # Walk through all files recursively
    for file_path in sorted(root.rglob("*")):
        if not file_path.is_file():
            continue
        
        # Get relative path and convert to POSIX style
        rel_path = file_path.relative_to(root).as_posix()
        
        # Get file stats
        stat = file_path.stat()
        size = stat.st_size
        mtime_ns = stat.st_mtime_ns
        
        # Compute SHA256 if requested
        sha256 = ""
        if include_sha256:
            sha256 = compute_sha256(file_path)
        
        snapshots[rel_path] = FileSnap(
            rel_path=rel_path,
            size=size,
            mtime_ns=mtime_ns,
            sha256=sha256,
        )
    
    return snapshots


def diff_snap(a: Dict[str, FileSnap], b: Dict[str, FileSnap]) -> dict:
    """
    Compare two snapshots and return differences.
    
    Args:
        a: First snapshot.
        b: Second snapshot.
    
    Returns:
        Dictionary with keys:
          - added: list of paths present in b but not in a
          - removed: list of paths present in a but not in b
          - changed: list of paths present in both but with different
                     size, mtime_ns, or sha256
    """
    a_keys = set(a.keys())
    b_keys = set(b.keys())
    
    added = sorted(b_keys - a_keys)
    removed = sorted(a_keys - b_keys)
    
    changed = []
    for key in sorted(a_keys & b_keys):
        snap_a = a[key]
        snap_b = b[key]
        if (snap_a.size != snap_b.size or 
            snap_a.mtime_ns != snap_b.mtime_ns or 
            snap_a.sha256 != snap_b.sha256):
            changed.append(key)
    
    return {
        "added": added,
        "removed": removed,
        "changed": changed,
    }



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/utils/manifest_verify.py
sha256(source_bytes) = 87d01ded9286849452867b46aa1fc56ecd4d0dfdd24e55e4f1e97149f9a73a40
bytes = 17113
redacted = False
--------------------------------------------------------------------------------

"""Manifest Tree Completeness verification tool.

This module provides functions to verify the integrity and completeness
of manifest trees for tamper-proof sealing.
"""

from __future__ import annotations

import json
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Any
from dataclasses import dataclass

from FishBroWFS_V2.control.artifacts import compute_sha256, canonical_json_bytes
from FishBroWFS_V2.core.schemas.manifest import UnifiedManifest


@dataclass
class VerificationResult:
    """Result of manifest verification."""
    is_valid: bool
    errors: List[str]
    warnings: List[str]
    manifest_type: str
    manifest_id: str


class ManifestVerifier:
    """Verifies manifest tree completeness and integrity."""
    
    def __init__(self, root_dir: Path):
        """
        Initialize verifier with root directory.
        
        Args:
            root_dir: Root directory containing manifests to verify
        """
        self.root_dir = root_dir.resolve()
        self.allowed_extensions = {'.json', '.txt', '.csv', '.parquet', '.feather', '.png', '.jpg', '.jpeg'}
    
    def verify_manifest_file(self, manifest_path: Path) -> VerificationResult:
        """
        Verify a single manifest file.
        
        Args:
            manifest_path: Path to manifest file
            
        Returns:
            VerificationResult with validation status
        """
        errors = []
        warnings = []
        
        try:
            # Read and parse manifest
            manifest_bytes = manifest_path.read_bytes()
            manifest_dict = json.loads(manifest_bytes.decode('utf-8'))
            
            # Validate against unified schema
            try:
                manifest = UnifiedManifest(**manifest_dict)
            except Exception as e:
                errors.append(f"Schema validation failed: {e}")
                return VerificationResult(
                    is_valid=False,
                    errors=errors,
                    warnings=warnings,
                    manifest_type="unknown",
                    manifest_id="unknown"
                )
            
            # Verify manifest self-hash
            if not self._verify_self_hash(manifest_dict, manifest_bytes):
                errors.append("Manifest self-hash verification failed")
            
            # Verify referenced files exist and match checksums
            file_errors = self._verify_referenced_files(manifest_path.parent, manifest_dict)
            errors.extend(file_errors)
            
            # Check for completeness (all files in directory are accounted for)
            completeness_errors = self._verify_directory_completeness(manifest_path.parent, manifest_dict)
            errors.extend(completeness_errors)
            
            return VerificationResult(
                is_valid=len(errors) == 0,
                errors=errors,
                warnings=warnings,
                manifest_type=manifest.manifest_type,
                manifest_id=manifest.id
            )
            
        except Exception as e:
            errors.append(f"Failed to read/parse manifest: {e}")
            return VerificationResult(
                is_valid=False,
                errors=errors,
                warnings=warnings,
                manifest_type="unknown",
                manifest_id="unknown"
            )
    
    def _verify_self_hash(self, manifest_dict: Dict[str, Any], manifest_bytes: bytes) -> bool:
        """Verify manifest's self-hash (manifest_sha256 field)."""
        if 'manifest_sha256' not in manifest_dict:
            return False
        
        # Remove the hash field before computing
        manifest_without_hash = dict(manifest_dict)
        manifest_without_hash.pop('manifest_sha256', None)
        
        # Compute canonical JSON
        canonical_bytes = canonical_json_bytes(manifest_without_hash)
        computed_hash = compute_sha256(canonical_bytes)
        
        return computed_hash == manifest_dict['manifest_sha256']
    
    def _verify_referenced_files(self, base_dir: Path, manifest_dict: Dict[str, Any]) -> List[str]:
        """Verify that all referenced files exist and match their checksums."""
        errors = []
        
        # Check files in checksums fields
        checksum_fields = ['checksums', 'export_checksums', 'plan_checksums', 
                          'view_checksums', 'quality_checksums']
        
        for field in checksum_fields:
            if field in manifest_dict and isinstance(manifest_dict[field], dict):
                checksums = manifest_dict[field]
                for filename, expected_hash in checksums.items():
                    file_path = base_dir / filename
                    if not file_path.exists():
                        errors.append(f"Referenced file not found: {filename}")
                        continue
                    
                    # Compute file hash
                    try:
                        file_hash = compute_sha256(file_path.read_bytes())
                        if file_hash != expected_hash:
                            errors.append(f"Hash mismatch for {filename}: expected {expected_hash}, got {file_hash}")
                    except Exception as e:
                        errors.append(f"Failed to compute hash for {filename}: {e}")
        
        return errors
    
    def _verify_directory_completeness(self, dir_path: Path, manifest_dict: Dict[str, Any]) -> List[str]:
        """
        Verify that all files in the directory are accounted for in the manifest.
        
        This ensures tamper-proof sealing: any file added, removed, or modified
        without updating the manifest will cause verification to fail.
        """
        errors = []
        
        # Get all files in directory (excluding temporary files and manifests)
        all_files = set()
        for file_path in dir_path.iterdir():
            if file_path.is_file():
                # Skip temporary files and .tmp files
                if file_path.suffix == '.tmp' or file_path.name.startswith('.'):
                    continue
                # Skip manifest files themselves (they're verified separately)
                if 'manifest' in file_path.name.lower():
                    continue
                all_files.add(file_path.name)
        
        # Get files referenced in manifest
        referenced_files = set()
        
        # Add files from checksums fields
        checksum_fields = ['checksums', 'export_checksums', 'plan_checksums', 
                          'view_checksums', 'quality_checksums']
        
        for field in checksum_fields:
            if field in manifest_dict and isinstance(manifest_dict[field], dict):
                referenced_files.update(manifest_dict[field].keys())
        
        # Check for files in directory not referenced in manifest
        unreferenced = all_files - referenced_files
        if unreferenced:
            errors.append(f"Files in directory not referenced in manifest: {sorted(unreferenced)}")
        
        # Check for files referenced in manifest but not in directory
        missing = referenced_files - all_files
        if missing:
            errors.append(f"Files referenced in manifest but not found in directory: {sorted(missing)}")
        
        return errors
    
    def verify_manifest_tree(self, start_path: Optional[Path] = None) -> List[VerificationResult]:
        """
        Recursively verify all manifests in a directory tree.
        
        Args:
            start_path: Starting directory (defaults to root_dir)
            
        Returns:
            List of verification results for all manifests found
        """
        if start_path is None:
            start_path = self.root_dir
        
        results = []
        
        # Look for manifest files
        manifest_patterns = ['*manifest*.json', 'manifest*.json', '*_manifest.json']
        
        for pattern in manifest_patterns:
            for manifest_path in start_path.rglob(pattern):
                # Skip if not a file or in excluded directories
                if not manifest_path.is_file():
                    continue
                
                # Skip temporary files
                if manifest_path.suffix == '.tmp' or manifest_path.name.startswith('.'):
                    continue
                
                result = self.verify_manifest_file(manifest_path)
                results.append(result)
        
        return results


def verify_manifest(manifest_path: str | Path) -> VerificationResult:
    """
    Convenience function to verify a single manifest file.
    
    Args:
        manifest_path: Path to manifest file
        
    Returns:
        VerificationResult
    """
    verifier = ManifestVerifier(Path(manifest_path).parent)
    return verifier.verify_manifest_file(Path(manifest_path))


def verify_directory(dir_path: str | Path) -> List[VerificationResult]:
    """
    Convenience function to verify all manifests in a directory.
    
    Args:
        dir_path: Directory to scan for manifests
        
    Returns:
        List of VerificationResult objects
    """
    verifier = ManifestVerifier(Path(dir_path))
    return verifier.verify_manifest_tree()


def print_verification_results(results: List[VerificationResult]) -> None:
    """Print verification results in a readable format."""
    total = len(results)
    valid = sum(1 for r in results if r.is_valid)
    
    print(f"=== Manifest Verification Results ===")
    print(f"Total manifests: {total}")
    print(f"Valid: {valid}")
    print(f"Invalid: {total - valid}")
    print()
    
    for i, result in enumerate(results, 1):
        status = " PASS" if result.is_valid else " FAIL"
        print(f"{i}. {status} - {result.manifest_type} ({result.manifest_id})")
        
        if result.errors:
            print(f"   Errors:")
            for error in result.errors:
                print(f"     - {error}")
        
        if result.warnings:
            print(f"   Warnings:")
            for warning in result.warnings:
                print(f"     - {warning}")
        
        print()


def compute_files_listing(root_dir: Path, allowed_scope: Optional[List[str]] = None) -> List[Dict[str, str]]:
    """
    Compute listing of all files in directory with SHA256 checksums.
    
    Args:
        root_dir: Root directory to scan
        allowed_scope: Optional list of relative paths to include. If None, include all files.
        
    Returns:
        List of dicts with keys "rel_path" and "sha256", sorted by rel_path asc.
    """
    files = []
    
    for file_path in root_dir.iterdir():
        if not file_path.is_file():
            continue
        
        # Skip temporary files and hidden files
        if file_path.suffix == '.tmp' or file_path.name.startswith('.'):
            continue
        
        # Skip manifest files themselves (they are the metadata, not part of the content)
        if 'manifest' in file_path.name.lower() and file_path.suffix in ('.json', '.yaml', '.yml'):
            continue
        
        rel_path = file_path.name
        
        # If allowed_scope is provided, filter by it
        if allowed_scope is not None and rel_path not in allowed_scope:
            continue
        
        # Compute SHA256
        try:
            file_hash = compute_sha256(file_path.read_bytes())
        except Exception:
            # Skip files that cannot be read
            continue
        
        files.append({
            "rel_path": rel_path,
            "sha256": file_hash
        })
    
    # Sort by rel_path ascending
    files.sort(key=lambda x: x["rel_path"])
    return files


def compute_files_sha256(files_listing: List[Dict[str, str]]) -> str:
    """
    Compute combined SHA256 of all files by concatenating their individual hashes.
    
    Args:
        files_listing: List of dicts with "rel_path" and "sha256"
        
    Returns:
        SHA256 hex string of concatenated hashes (sorted by rel_path)
    """
    # Ensure sorted by rel_path
    sorted_files = sorted(files_listing, key=lambda x: x["rel_path"])
    
    # Concatenate all SHA256 strings
    concatenated = "".join(f["sha256"] for f in sorted_files)
    
    # Compute SHA256 of the concatenated string (as UTF-8 bytes)
    return hashlib.sha256(concatenated.encode("utf-8")).hexdigest()


def verify_manifest_completeness(root_dir: Path, manifest_dict: Dict[str, Any]) -> None:
    """
    Verify manifest completeness and integrity.
    
    Validates:
    1. Files listing matches exactly (no extra/missing files)
    2. Each file's SHA256 matches
    3. files_sha256 field is correct
    4. manifest_sha256 field is correct
    
    Args:
        root_dir: Directory containing the files
        manifest_dict: Parsed manifest JSON as dict
        
    Raises:
        ValueError: If any verification fails
    """
    errors = []
    
    # 1. Verify files listing exists
    if "files" not in manifest_dict:
        raise ValueError("Manifest missing 'files' field")
    
    manifest_files = manifest_dict.get("files", [])
    if not isinstance(manifest_files, list):
        raise ValueError("Manifest 'files' must be a list")
    
    # Convert to dict for easier lookup
    manifest_file_map = {f["rel_path"]: f["sha256"] for f in manifest_files if isinstance(f, dict) and "rel_path" in f and "sha256" in f}
    
    # 2. Compute actual files listing (include all files, not just those in manifest)
    # This ensures we detect extra files added to the directory
    actual_files = compute_files_listing(root_dir, allowed_scope=None)
    actual_file_map = {f["rel_path"]: f["sha256"] for f in actual_files}
    
    # Check for missing files in manifest
    missing_in_manifest = set(actual_file_map.keys()) - set(manifest_file_map.keys())
    if missing_in_manifest:
        errors.append(f"Files in directory not in manifest: {sorted(missing_in_manifest)}")
    
    # Check for extra files in manifest not in directory
    extra_in_manifest = set(manifest_file_map.keys()) - set(actual_file_map.keys())
    if extra_in_manifest:
        errors.append(f"Files in manifest not found in directory: {sorted(extra_in_manifest)}")
    
    # 3. Verify SHA256 matches for common files
    common = set(manifest_file_map.keys()) & set(actual_file_map.keys())
    for rel_path in common:
        if manifest_file_map[rel_path] != actual_file_map[rel_path]:
            errors.append(f"SHA256 mismatch for {rel_path}: manifest={manifest_file_map[rel_path]}, actual={actual_file_map[rel_path]}")
    
    # 4. Verify files_sha256 if present
    if "files_sha256" in manifest_dict:
        expected_files_sha256 = manifest_dict["files_sha256"]
        computed_files_sha256 = compute_files_sha256(actual_files)
        if expected_files_sha256 != computed_files_sha256:
            errors.append(f"files_sha256 mismatch: expected {expected_files_sha256}, computed {computed_files_sha256}")
    
    # 5. Verify manifest_sha256 if present
    if "manifest_sha256" in manifest_dict:
        # Create copy without manifest_sha256 field
        manifest_without_hash = dict(manifest_dict)
        manifest_without_hash.pop("manifest_sha256", None)
        
        # Compute canonical JSON hash
        canonical_bytes = canonical_json_bytes(manifest_without_hash)
        computed_hash = compute_sha256(canonical_bytes)
        
        if manifest_dict["manifest_sha256"] != computed_hash:
            errors.append(f"manifest_sha256 mismatch: expected {manifest_dict['manifest_sha256']}, computed {computed_hash}")
    
    if errors:
        raise ValueError("Manifest verification failed:\n" + "\n".join(f"  - {e}" for e in errors))


def verify_manifest(root_dir: str | Path, manifest_json: dict | str | Path) -> None:
    """
    Verify manifest completeness and integrity (taskrequired signature).
    
    Args:
        root_dir: Directory containing the files
        manifest_json: Either a dict of parsed manifest, or a path to manifest file,
                      or a string of JSON content.
    
    Raises:
        ValueError: If verification fails
    """
    root_dir = Path(root_dir)
    
    # Parse manifest_json based on its type
    if isinstance(manifest_json, dict):
        manifest_dict = manifest_json
    elif isinstance(manifest_json, (str, Path)):
        path = Path(manifest_json)
        if path.exists():
            manifest_dict = json.loads(path.read_text(encoding="utf-8"))
        else:
            # Try to parse as JSON string
            try:
                manifest_dict = json.loads(manifest_json)
            except json.JSONDecodeError:
                raise ValueError(f"manifest_json is not a valid file path or JSON string: {manifest_json}")
    else:
        raise TypeError(f"manifest_json must be dict, str, or Path, got {type(manifest_json)}")
    
    # Delegate to verify_manifest_completeness
    verify_manifest_completeness(root_dir, manifest_dict)



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/utils/write_scope.py
sha256(source_bytes) = fcde49e3ac840bc2ff86089d7bf2a53130cf0302c5caf05c20c9c2d3e0123727
bytes = 8209
redacted = False
--------------------------------------------------------------------------------

"""
Writescope guard for hardening filewrite boundaries.

This module provides a runtime fence that ensures writers only produce files
under a designated root directory and whose relative paths match a predefined
allowlist (exact matches or prefixbased patterns).  Any attempt to write
outside the allowed set raises a ValueError before the actual I/O occurs.

The guard is designed to be used inside each writer function that writes
portfoliorelated outputs (plan_, plan_view_, plan_quality_, etc.) and
seasonexport outputs.

Design notes
------------
 Path.resolve() is used to detect symlink escapes, but we rely on
  resolved_target.is_relative_to(resolved_root) (Python 3.12) to guarantee
  the final target stays under the logical root.
 Prefix matching is performed on the basename only, not on the whole relative
  path.  This prevents subdirectories like `subdir/plan_foo.json` from slipping
  through unless the prefix pattern explicitly allows subdirectories (which we
  currently do not).
 The guard does **not** create directories; it only validates the relative
  path.  The caller is responsible for creating parent directories if needed.
"""

from __future__ import annotations

import os
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable


@dataclass(frozen=True)
class WriteScope:
    """Immutable guard that validates relative paths against a whitelist.

    Attributes
    ----------
    root_dir : Path
        Absolute path to the directory under which all writes must stay.
    allowed_rel_files : frozenset[str]
        Set of exact relative paths (POSIX style, no leading slash, no `..`)
        that are permitted.
    allowed_rel_prefixes : tuple[str, ...]
        Tuple of filename prefixes.  A relative path is allowed if its
        basename starts with any of these prefixes.
    """

    root_dir: Path
    allowed_rel_files: frozenset[str]          # exact files
    allowed_rel_prefixes: tuple[str, ...]      # prefix patterns (e.g. "plan_", "plan_view_")

    def assert_allowed_rel(self, rel: str) -> None:
        """Raise ValueError if `rel` is not allowed by this scope.

        Parameters
        ----------
        rel : str
            Relative path (POSIX style, no leading slash, no `..`).

        Raises
        ------
        ValueError
            With a descriptive message if the path is not allowed or attempts
            to escape the root directory.
        """
        # 1. Basic sanity: must be a relative POSIX path without `..` components.
        if os.path.isabs(rel):
            raise ValueError(f"Relative path must not be absolute: {rel!r}")
        if ".." in rel.split("/"):
            raise ValueError(f"Relative path must not contain '..': {rel!r}")

        # 2. Ensure the final resolved target stays under root_dir.
        target = (self.root_dir / rel).resolve()
        root_resolved = self.root_dir.resolve()
        # Python 3.12+ provides Path.is_relative_to; we use it if available,
        # otherwise fall back to a manual check.
        try:
            if not target.is_relative_to(root_resolved):
                raise ValueError(
                    f"Path {rel!r} resolves to {target} which is outside the "
                    f"scope root {root_resolved}"
                )
        except AttributeError:
            # Python <3.12: compare parents manually.
            try:
                target.relative_to(root_resolved)
            except ValueError:
                raise ValueError(
                    f"Path {rel!r} resolves to {target} which is outside the "
                    f"scope root {root_resolved}"
                )

        # 3. Check for wildcard prefix "*" which allows any file under root_dir
        if "*" in self.allowed_rel_prefixes:
            return

        # 4. Check exact matches first.
        if rel in self.allowed_rel_files:
            return

        # 5. Check prefix matches on the basename.
        basename = os.path.basename(rel)
        for prefix in self.allowed_rel_prefixes:
            if basename.startswith(prefix):
                return

        # 6. If we reach here, the path is forbidden.
        raise ValueError(
            f"Relative path {rel!r} is not allowed by this write scope.\n"
            f"Allowed exact files: {sorted(self.allowed_rel_files)}\n"
            f"Allowed filename prefixes: {self.allowed_rel_prefixes}"
        )


def create_plan_scope(plan_dir: Path) -> WriteScope:
    """Create a WriteScope for a portfolio plan directory.

    This scope permits the standard planmanifest files and any future file
    whose basename starts with `plan_`.

    Exact allowed files:
        portfolio_plan.json
        plan_manifest.json
        plan_metadata.json
        plan_checksums.json

    Allowed prefixes:
        ("plan_",)
    """
    return WriteScope(
        root_dir=plan_dir,
        allowed_rel_files=frozenset({
            "portfolio_plan.json",
            "plan_manifest.json",
            "plan_metadata.json",
            "plan_checksums.json",
        }),
        allowed_rel_prefixes=("plan_",),
    )


def create_plan_view_scope(view_dir: Path) -> WriteScope:
    """Create a WriteScope for a planview directory.

    Exact allowed files:
        plan_view.json
        plan_view.md
        plan_view_checksums.json
        plan_view_manifest.json

    Allowed prefixes:
        ("plan_view_",)
    """
    return WriteScope(
        root_dir=view_dir,
        allowed_rel_files=frozenset({
            "plan_view.json",
            "plan_view.md",
            "plan_view_checksums.json",
            "plan_view_manifest.json",
        }),
        allowed_rel_prefixes=("plan_view_",),
    )


def create_plan_quality_scope(quality_dir: Path) -> WriteScope:
    """Create a WriteScope for a planquality directory.

    Exact allowed files:
        plan_quality.json
        plan_quality_checksums.json
        plan_quality_manifest.json

    Allowed prefixes:
        ("plan_quality_",)
    """
    return WriteScope(
        root_dir=quality_dir,
        allowed_rel_files=frozenset({
            "plan_quality.json",
            "plan_quality_checksums.json",
            "plan_quality_manifest.json",
        }),
        allowed_rel_prefixes=("plan_quality_",),
    )


def create_season_export_scope(export_root: Path) -> WriteScope:
    """Create a WriteScope for seasonexport outputs.

    This scope allows any file under exports_root / seasons / {season} / **
    but forbids any path that would escape to outputs/artifacts/** or
    outputs/season_index/** or any other repo root paths.

    The export_root parameter should be the season directory:
        exports_root / seasons / {season}

    Allowed prefixes:
        ()   (none  we allow any file under the export_root)
    """
    # Ensure export_root is under the exports tree
    exports_root = Path(os.environ.get("FISHBRO_EXPORTS_ROOT", "outputs/exports"))
    if not export_root.is_relative_to(exports_root):
        raise ValueError(
            f"export_root {export_root} must be under exports root {exports_root}"
        )
    
    # Ensure export_root follows the pattern exports_root / seasons / {season}
    try:
        relative_to_exports = export_root.relative_to(exports_root)
        parts = relative_to_exports.parts
        if len(parts) < 2 or parts[0] != "seasons":
            raise ValueError(
                f"export_root must be under exports_root/seasons/{{season}}, got {relative_to_exports}"
            )
    except ValueError:
        raise ValueError(
            f"export_root {export_root} must be under exports root {exports_root}"
        )
    
    # Allow any file under export_root (empty allowed_rel_files means no exact matches required,
    # empty allowed_rel_prefixes means no prefix restriction, but we need to allow all files)
    # We'll use a special prefix "*" to indicate allow all (handled in assert_allowed_rel)
    return WriteScope(
        root_dir=export_root,
        allowed_rel_files=frozenset(),  # No exact matches required
        allowed_rel_prefixes=("*",),    # Allow any file under export_root
    )



--------------------------------------------------------------------------------

FILE src/FishBroWFS_V2/wfs/runner.py
sha256(source_bytes) = 2d907ffa6afd788ad76737e8b8e9b8fced9d66cd77701141571eb320d73c2135
bytes = 4315
redacted = False
--------------------------------------------------------------------------------

# src/FishBroWFS_V2/wfs/runner.py
"""
WFS Runner -  FeatureBundle 

Phase 4.1:  run_wfs_with_features API Research Runner 
"""

from __future__ import annotations

import logging
from typing import Dict, Any, Optional

from FishBroWFS_V2.core.feature_bundle import FeatureBundle
from FishBroWFS_V2.strategy.runner import run_strategy
from FishBroWFS_V2.strategy.registry import get as get_strategy_spec

logger = logging.getLogger(__name__)


def run_wfs_with_features(
    *,
    strategy_id: str,
    feature_bundle: FeatureBundle,
    config: Optional[dict] = None,
) -> dict:
    """
    WFS entrypoint that consumes FeatureBundle only.

    
    1.  feature_bundle
    2.  TXT / bars / features 
    3.  config 
    4.  intents
    5. 
    6. 

    Args:
        strategy_id:  ID
        feature_bundle: 
        config:  params, context 

    Returns:
        
            - strategy_id
            - dataset_id
            - season
            - intents_count
            - fills_count
            - net_profit ()
            - trades
            - max_dd
    """
    if config is None:
        config = {}

    # 1.  feature_bundle  features dict
    features = _extract_features_dict(feature_bundle)

    # 2.  config  params
    params = config.get("params", {})
    if not params:
        # 
        spec = get_strategy_spec(strategy_id)
        params = spec.defaults

    # 3.  context
    context = config.get("context", {})
    if "bar_index" not in context:
        #  bar 
        context["bar_index"] = 0
    if "order_qty" not in context:
        context["order_qty"] = 1

    # 4.  intents
    try:
        intents = run_strategy(
            strategy_id=strategy_id,
            features=features,
            params=params,
            context=context,
        )
    except Exception as e:
        logger.error(f": {e}")
        raise RuntimeError(f" {strategy_id} : {e}") from e

    # 5. 
    #  Phase 4.1 
    # 
    summary = _simulate_intents(intents, feature_bundle, config)

    # 6.  metadata
    summary.update({
        "strategy_id": strategy_id,
        "dataset_id": feature_bundle.dataset_id,
        "season": feature_bundle.season,
        "intents_count": len(intents),
        "features_used": list(features.keys()),
    })

    return summary


def _extract_features_dict(feature_bundle: FeatureBundle) -> Dict[str, Any]:
    """
     FeatureBundle  {name: values_array}
    """
    features = {}
    for series in feature_bundle.series.values():
        features[series.name] = series.values
    return features


def _simulate_intents(intents, feature_bundle: FeatureBundle, config: dict) -> dict:
    """
     intents  metrics

    
    """
    #  intents
    if not intents:
        return {
            "fills_count": 0,
            "net_profit": 0.0,
            "trades": 0,
            "max_dd": 0.0,
            "simulation": "stub",
        }

    #  intent  fill fill  profit  0
    #  engine.simulate
    fills_count = len(intents) // 2  #  entry  exit
    net_profit = 0.0
    trades = fills_count
    max_dd = 0.0

    return {
        "fills_count": fills_count,
        "net_profit": net_profit,
        "trades": trades,
        "max_dd": max_dd,
        "simulation": "stub",
    }



--------------------------------------------------------------------------------

FILE strategies/sma_cross/features.json
sha256(source_bytes) = 06e1492d4edd7069dedf3ac57a2b8a86ccb52a1625e43f36175a304be993718d
bytes = 294
redacted = False
--------------------------------------------------------------------------------
{
  "strategy_id": "sma_cross",
  "required": [
    {"name": "atr_14", "timeframe_min": 60},
    {"name": "ret_z_200", "timeframe_min": 60}
  ],
  "optional": [
    {"name": "session_vwap", "timeframe_min": 60}
  ],
  "min_schema_version": "v1",
  "notes": "bootstrap for first research run"
}

--------------------------------------------------------------------------------

FILE tests/__init__.py
sha256(source_bytes) = 05ac3d69f8cf8b69fe3b9b057aec975e83db54607b979b529f3c396677b99099
bytes = 142
redacted = False
--------------------------------------------------------------------------------

"""
Tests package for FishBroWFS_V2.

This package allows tests to import from each other using:
    from tests.test_module import ...
"""



--------------------------------------------------------------------------------

FILE tests/conftest.py
sha256(source_bytes) = 41179d8536544661580fe0eb51a74bb5aeb068c286b76e716fcf722a23991195
bytes = 1339
redacted = False
--------------------------------------------------------------------------------

"""
Pytest configuration and fixtures.

Ensures PYTHONPATH is set correctly for imports.
"""
from __future__ import annotations

import sys
from pathlib import Path

import pytest

# Add src/ to Python path if not already present
# This ensures tests can import FishBroWFS_V2 without manual PYTHONPATH setup
repo_root = Path(__file__).parent.parent
src_path = repo_root / "src"
if str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))


@pytest.fixture
def temp_dir(tmp_path: Path) -> Path:
    """Compatibility alias for older tests that used temp_dir.
    
    Returns tmp_path (pytest's built-in fixture) for compatibility
    with tests that expect a temp_dir fixture.
    """
    return tmp_path


@pytest.fixture
def sample_raw_txt(tmp_path: Path) -> Path:
    """Fixture providing a sample raw TXT file for data ingest tests.
    
    Returns path to a minimal TXT file with Date, Time, OHLCV columns.
    This fixture is shared across all data ingest tests to avoid duplication.
    """
    txt_path = tmp_path / "sample_data.txt"
    txt_content = """Date,Time,Open,High,Low,Close,TotalVolume
2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000
2013/1/1,10:00:00,104.0,106.0,103.0,105.0,1200
2013/1/2,09:30:00,105.0,107.0,104.0,106.0,1500
"""
    txt_path.write_text(txt_content, encoding="utf-8")
    return txt_path



--------------------------------------------------------------------------------

FILE tests/test_api_worker_no_pipe_deadlock.py
sha256(source_bytes) = 6dcd3ff87ed3179b901d0ccc12645faf6ad0ab7cd5341b76d13cc25044991818
bytes = 2144
redacted = False
--------------------------------------------------------------------------------

"""Test that worker spawn does not use PIPE (prevents deadlock)."""

from __future__ import annotations

import subprocess
from pathlib import Path
from unittest.mock import MagicMock

import pytest

from FishBroWFS_V2.control.api import _ensure_worker_running


def test_worker_spawn_not_using_pipes(monkeypatch, tmp_path):
    """Test that _ensure_worker_running does not use subprocess.PIPE."""
    called = {}
    
    def fake_popen(args, **kwargs):
        called["args"] = args
        called["kwargs"] = kwargs
        # Create a mock process object
        p = MagicMock()
        p.pid = 123
        return p
    
    monkeypatch.setattr("FishBroWFS_V2.control.api.subprocess.Popen", fake_popen)
    monkeypatch.setattr("FishBroWFS_V2.control.api.os.kill", lambda pid, sig: None)
    
    db_path = tmp_path / "jobs.db"
    db_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Create pidfile that doesn't exist (so worker will start)
    pidfile = db_path.parent / "worker.pid"
    assert not pidfile.exists()
    
    # Mock init_db to avoid actual DB creation
    monkeypatch.setattr("FishBroWFS_V2.control.api.init_db", lambda _: None)
    
    _ensure_worker_running(db_path)
    
    kw = called["kwargs"]
    
    # Critical: must not use PIPE
    assert kw["stdout"] is not subprocess.PIPE, "stdout must not be PIPE (deadlock risk)"
    assert kw["stderr"] is not subprocess.PIPE, "stderr must not be PIPE (deadlock risk)"
    
    # Should use file handle (opened file object)
    assert kw["stdout"] is not None, "stdout must be set (file handle)"
    assert kw["stderr"] is not None, "stderr must be set (file handle)"
    # Both stdout and stderr should be the same file handle
    assert kw["stdout"] is kw["stderr"], "stdout and stderr should point to same file"
    
    # Should have stdin=DEVNULL
    assert kw.get("stdin") == subprocess.DEVNULL, "stdin should be DEVNULL"
    
    # Should have start_new_session=True
    assert kw.get("start_new_session") is True, "start_new_session should be True"
    
    # Should have close_fds=True
    assert kw.get("close_fds") is True, "close_fds should be True"



--------------------------------------------------------------------------------

FILE tests/test_api_worker_spawn_no_pipes.py
sha256(source_bytes) = e26df298cd8f61e40d0c7ca605419e739a0f78b0879db7b5fa90361558ab353a
bytes = 1073
redacted = False
--------------------------------------------------------------------------------

"""Test that API worker spawn does not use PIPE (prevents deadlock)."""

from __future__ import annotations

import subprocess
from pathlib import Path

import pytest

from FishBroWFS_V2.control.api import _ensure_worker_running


def test_api_worker_spawn_no_pipes(monkeypatch, tmp_path: Path) -> None:
    """Test that _ensure_worker_running does not use subprocess.PIPE."""
    seen: dict[str, object] = {}

    def fake_popen(args, **kwargs):  # noqa: ANN001
        seen.update(kwargs)
        class P:
            pid = 123
        return P()

    monkeypatch.setattr("FishBroWFS_V2.control.api.subprocess.Popen", fake_popen)
    monkeypatch.setattr("FishBroWFS_V2.control.api.os.kill", lambda pid, sig: None)
    monkeypatch.setattr("FishBroWFS_V2.control.api.init_db", lambda _: None)

    db_path = tmp_path / "jobs.db"
    db_path.parent.mkdir(parents=True, exist_ok=True)

    _ensure_worker_running(db_path)

    assert seen["stdout"] is not subprocess.PIPE
    assert seen["stderr"] is not subprocess.PIPE
    assert seen.get("stdin") is subprocess.DEVNULL



--------------------------------------------------------------------------------

FILE tests/test_artifact_contract.py
sha256(source_bytes) = 0c03ab683d8bc620f65c34842281f13c5087eee647951e0f4254f5612a981b17
bytes = 14733
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for artifact system.

Tests verify:
1. Directory structure contract
2. File existence and format
3. JSON serialization correctness (sorted keys)
4. param_subsample_rate visibility (mandatory in manifest/metrics/README)
5. Winners schema stability
"""

from __future__ import annotations

import json
import tempfile
from datetime import datetime, timezone
from pathlib import Path

import pytest

from FishBroWFS_V2.core.artifacts import write_run_artifacts
from FishBroWFS_V2.core.audit_schema import AuditSchema, compute_params_effective
from FishBroWFS_V2.core.config_hash import stable_config_hash
from FishBroWFS_V2.core.paths import ensure_run_dir, get_run_dir
from FishBroWFS_V2.core.run_id import make_run_id


def test_artifact_tree_contract():
    """Test that artifact directory structure follows contract."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        run_id = make_run_id()
        
        run_dir = ensure_run_dir(outputs_root, season, run_id)
        
        # Verify directory structure
        expected_path = outputs_root / "seasons" / season / "runs" / run_id
        assert run_dir == expected_path
        assert expected_path.exists()
        assert expected_path.is_dir()
        
        # Verify get_run_dir returns same path
        assert get_run_dir(outputs_root, season, run_id) == expected_path


def test_manifest_must_include_param_subsample_rate():
    """Test that manifest.json must include param_subsample_rate."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        config = {"n_bars": 1000, "n_params": 100}
        param_subsample_rate = 0.1
        params_total = 100
        params_effective = compute_params_effective(params_total, param_subsample_rate)
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=param_subsample_rate,
            config_hash=stable_config_hash(config),
            season=season,
            dataset_id="test_dataset",
            bars=1000,
            params_total=params_total,
            params_effective=params_effective,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=config,
            metrics={"param_subsample_rate": param_subsample_rate},
        )
        
        # Read and verify manifest
        manifest_path = run_dir / "manifest.json"
        assert manifest_path.exists()
        
        with open(manifest_path, "r", encoding="utf-8") as f:
            manifest_data = json.load(f)
        
        # Verify param_subsample_rate exists and is correct
        assert "param_subsample_rate" in manifest_data
        assert manifest_data["param_subsample_rate"] == 0.1
        
        # Verify all audit fields are present
        assert "run_id" in manifest_data
        assert "created_at" in manifest_data
        assert "git_sha" in manifest_data
        assert "dirty_repo" in manifest_data
        assert "config_hash" in manifest_data


def test_config_snapshot_is_json_serializable():
    """Test that config_snapshot.json is valid JSON with sorted keys."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        config = {
            "n_bars": 1000,
            "n_params": 100,
            "commission": 0.0,
            "slip": 0.0,
        }
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=1.0,
            config_hash=stable_config_hash(config),
            season=season,
            dataset_id="test_dataset",
            bars=1000,
            params_total=100,
            params_effective=100,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=config,
            metrics={"param_subsample_rate": 1.0},
        )
        
        config_path = run_dir / "config_snapshot.json"
        assert config_path.exists()
        
        # Verify JSON is valid and has sorted keys
        with open(config_path, "r", encoding="utf-8") as f:
            config_data = json.load(f)
        
        # Verify keys are sorted (JSON should be written with sort_keys=True)
        keys = list(config_data.keys())
        assert keys == sorted(keys), "Config keys should be sorted"
        
        # Verify content matches
        assert config_data == config


def test_metrics_must_include_param_subsample_rate():
    """Test that metrics.json must include param_subsample_rate visibility."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        param_subsample_rate = 0.25
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=param_subsample_rate,
            config_hash="test_hash",
            season=season,
            dataset_id="test_dataset",
            bars=20000,
            params_total=1000,
            params_effective=250,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        metrics = {
            "param_subsample_rate": param_subsample_rate,
            "runtime_s": 12.345,
            "throughput": 27777777.78,
        }
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot={"test": "config"},
            metrics=metrics,
        )
        
        metrics_path = run_dir / "metrics.json"
        assert metrics_path.exists()
        
        with open(metrics_path, "r", encoding="utf-8") as f:
            metrics_data = json.load(f)
        
        # Verify param_subsample_rate exists
        assert "param_subsample_rate" in metrics_data
        assert metrics_data["param_subsample_rate"] == 0.25


def test_winners_structure_contract():
    """Test that winners.json has fixed structure versioned."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=1.0,
            config_hash="test_hash",
            season=season,
            dataset_id="test_dataset",
            bars=1000,
            params_total=100,
            params_effective=100,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot={"test": "config"},
            metrics={"param_subsample_rate": 1.0},
        )
        
        winners_path = run_dir / "winners.json"
        assert winners_path.exists()
        
        with open(winners_path, "r", encoding="utf-8") as f:
            winners_data = json.load(f)
        
        # Verify fixed structure
        assert "topk" in winners_data
        assert isinstance(winners_data["topk"], list)
        
        # Verify schema version (v1 or v2)
        notes = winners_data.get("notes", {})
        schema = notes.get("schema")
        assert schema in ("v1", "v2"), f"Schema must be v1 or v2, got {schema}"
        
        # If v2, must include 'schema' at top level too
        if schema == "v2":
            assert winners_data.get("schema") == "v2"
        
        assert winners_data["topk"] == []  # Initially empty


def test_readme_must_display_param_subsample_rate():
    """Test that README.md prominently displays param_subsample_rate."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        param_subsample_rate = 0.33
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=param_subsample_rate,
            config_hash="test_hash_123",
            season=season,
            dataset_id="test_dataset",
            bars=20000,
            params_total=1000,
            params_effective=330,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot={"test": "config"},
            metrics={"param_subsample_rate": param_subsample_rate},
        )
        
        readme_path = run_dir / "README.md"
        assert readme_path.exists()
        
        with open(readme_path, "r", encoding="utf-8") as f:
            readme_content = f.read()
        
        # Verify param_subsample_rate is prominently displayed
        assert "param_subsample_rate" in readme_content
        assert "0.33" in readme_content
        
        # Verify other required fields
        assert "run_id" in readme_content
        assert "git_sha" in readme_content
        assert "season" in readme_content
        assert "dataset_id" in readme_content
        assert "bars" in readme_content
        assert "params_total" in readme_content
        assert "params_effective" in readme_content
        assert "config_hash" in readme_content


def test_logs_file_exists():
    """Test that logs.txt file is created."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=1.0,
            config_hash="test_hash",
            season=season,
            dataset_id="test_dataset",
            bars=1000,
            params_total=100,
            params_effective=100,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot={"test": "config"},
            metrics={"param_subsample_rate": 1.0},
        )
        
        logs_path = run_dir / "logs.txt"
        assert logs_path.exists()
        
        # Initially empty
        with open(logs_path, "r", encoding="utf-8") as f:
            assert f.read() == ""


def test_all_artifacts_exist():
    """Test that all required artifacts are created."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=0.1,
            config_hash="test_hash",
            season=season,
            dataset_id="test_dataset",
            bars=20000,
            params_total=1000,
            params_effective=100,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot={"test": "config"},
            metrics={"param_subsample_rate": 0.1},
        )
        
        # Verify all artifacts exist
        artifacts = [
            "manifest.json",
            "config_snapshot.json",
            "metrics.json",
            "winners.json",
            "README.md",
            "logs.txt",
        ]
        
        for artifact_name in artifacts:
            artifact_path = run_dir / artifact_name
            assert artifact_path.exists(), f"Missing artifact: {artifact_name}"


def test_json_files_have_sorted_keys():
    """Test that all JSON files are written with sorted keys."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        season = "test_season"
        
        config = {
            "z_field": "last",
            "a_field": "first",
            "m_field": "middle",
        }
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="a1b2c3d4e5f6",
            dirty_repo=False,
            param_subsample_rate=1.0,
            config_hash=stable_config_hash(config),
            season=season,
            dataset_id="test_dataset",
            bars=1000,
            params_total=100,
            params_effective=100,
        )
        
        run_dir = ensure_run_dir(outputs_root, season, audit.run_id)
        
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=config,
            metrics={"param_subsample_rate": 1.0},
        )
        
        # Check config_snapshot.json has sorted keys
        config_path = run_dir / "config_snapshot.json"
        with open(config_path, "r", encoding="utf-8") as f:
            config_data = json.load(f)
        
        keys = list(config_data.keys())
        assert keys == sorted(keys), "Config keys should be sorted"
        
        # Check manifest.json has sorted keys
        manifest_path = run_dir / "manifest.json"
        with open(manifest_path, "r", encoding="utf-8") as f:
            manifest_data = json.load(f)
        
        manifest_keys = list(manifest_data.keys())
        assert manifest_keys == sorted(manifest_keys), "Manifest keys should be sorted"



--------------------------------------------------------------------------------

FILE tests/test_artifacts_winners_v2_written.py
sha256(source_bytes) = 89f1fbf442cfc542019818d35950a0cfd716c614d5ccec8d81686bf8fae50cd1
bytes = 7330
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for artifacts winners v2 writing.

Tests verify that write_run_artifacts automatically upgrades legacy winners to v2.
"""

from __future__ import annotations

import json
import tempfile
from datetime import datetime, timezone
from pathlib import Path

from FishBroWFS_V2.core.artifacts import write_run_artifacts
from FishBroWFS_V2.core.audit_schema import AuditSchema, compute_params_effective
from FishBroWFS_V2.core.config_hash import stable_config_hash
from FishBroWFS_V2.core.run_id import make_run_id
from FishBroWFS_V2.core.winners_schema import is_winners_v2


def test_artifacts_upgrades_legacy_winners_to_v2() -> None:
    """Test that write_run_artifacts upgrades legacy winners to v2."""
    with tempfile.TemporaryDirectory() as tmpdir:
        run_dir = Path(tmpdir) / "run_test"
        
        # Create audit schema
        config = {"n_bars": 1000, "n_params": 100}
        param_subsample_rate = 0.1
        params_total = 100
        params_effective = compute_params_effective(params_total, param_subsample_rate)
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="abc123def456",
            dirty_repo=False,
            param_subsample_rate=param_subsample_rate,
            config_hash=stable_config_hash(config),
            season="test_season",
            dataset_id="test_dataset",
            bars=1000,
            params_total=params_total,
            params_effective=params_effective,
        )
        
        # Legacy winners format
        legacy_winners = {
            "topk": [
                {"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0},
                {"param_id": 1, "net_profit": 200.0, "trades": 20, "max_dd": -20.0},
            ],
            "notes": {"schema": "v1"},
        }
        
        # Write artifacts
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=config,
            metrics={
                "param_subsample_rate": param_subsample_rate,
                "stage_name": "stage1_topk",
            },
            winners=legacy_winners,
        )
        
        # Read winners.json
        winners_path = run_dir / "winners.json"
        assert winners_path.exists()
        
        with winners_path.open("r", encoding="utf-8") as f:
            winners = json.load(f)
        
        # Verify it's v2 schema
        assert is_winners_v2(winners) is True
        assert winners["schema"] == "v2"
        assert winners["stage_name"] == "stage1_topk"
        
        # Verify topk items are v2 format
        topk = winners["topk"]
        assert len(topk) == 2
        
        for item in topk:
            assert "candidate_id" in item
            assert "strategy_id" in item
            assert "symbol" in item
            assert "timeframe" in item
            assert "params" in item
            assert "score" in item
            assert "metrics" in item
            assert "source" in item
            
            # Verify legacy fields are in metrics
            metrics = item["metrics"]
            assert "net_profit" in metrics
            assert "max_dd" in metrics
            assert "trades" in metrics
            assert "param_id" in metrics


def test_artifacts_writes_v2_when_winners_is_none() -> None:
    """Test that write_run_artifacts creates v2 format when winners is None."""
    with tempfile.TemporaryDirectory() as tmpdir:
        run_dir = Path(tmpdir) / "run_test"
        
        # Create audit schema
        config = {"n_bars": 1000, "n_params": 100}
        param_subsample_rate = 0.1
        params_total = 100
        params_effective = compute_params_effective(params_total, param_subsample_rate)
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="abc123def456",
            dirty_repo=False,
            param_subsample_rate=param_subsample_rate,
            config_hash=stable_config_hash(config),
            season="test_season",
            dataset_id="test_dataset",
            bars=1000,
            params_total=params_total,
            params_effective=params_effective,
        )
        
        # Write artifacts with winners=None
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=config,
            metrics={
                "param_subsample_rate": param_subsample_rate,
                "stage_name": "stage0_coarse",
            },
            winners=None,
        )
        
        # Read winners.json
        winners_path = run_dir / "winners.json"
        assert winners_path.exists()
        
        with winners_path.open("r", encoding="utf-8") as f:
            winners = json.load(f)
        
        # Verify it's v2 schema (even when empty)
        assert is_winners_v2(winners) is True
        assert winners["schema"] == "v2"
        assert winners["topk"] == []


def test_artifacts_preserves_legacy_metrics_fields() -> None:
    """Test that legacy metrics fields are preserved in v2 format."""
    with tempfile.TemporaryDirectory() as tmpdir:
        run_dir = Path(tmpdir) / "run_test"
        
        # Create audit schema
        config = {"n_bars": 1000, "n_params": 100}
        param_subsample_rate = 0.1
        params_total = 100
        params_effective = compute_params_effective(params_total, param_subsample_rate)
        
        audit = AuditSchema(
            run_id=make_run_id(),
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="abc123def456",
            dirty_repo=False,
            param_subsample_rate=param_subsample_rate,
            config_hash=stable_config_hash(config),
            season="test_season",
            dataset_id="test_dataset",
            bars=1000,
            params_total=params_total,
            params_effective=params_effective,
        )
        
        # Legacy winners with proxy_value (Stage0)
        legacy_winners = {
            "topk": [
                {"param_id": 0, "proxy_value": 1.234},
            ],
            "notes": {"schema": "v1"},
        }
        
        # Write artifacts
        write_run_artifacts(
            run_dir=run_dir,
            manifest=audit.to_dict(),
            config_snapshot=config,
            metrics={
                "param_subsample_rate": param_subsample_rate,
                "stage_name": "stage0_coarse",
            },
            winners=legacy_winners,
        )
        
        # Read winners.json
        winners_path = run_dir / "winners.json"
        with winners_path.open("r", encoding="utf-8") as f:
            winners = json.load(f)
        
        # Verify legacy fields are preserved
        item = winners["topk"][0]
        metrics = item["metrics"]
        
        # proxy_value should be in metrics
        assert "proxy_value" in metrics
        assert metrics["proxy_value"] == 1.234
        
        # param_id should be in metrics (for backward compatibility)
        assert "param_id" in metrics
        assert metrics["param_id"] == 0



--------------------------------------------------------------------------------

FILE tests/test_audit_schema_contract.py
sha256(source_bytes) = c73fbd118de1c93e47fd4446e9be183bfd84c2c179b1b49455f0ac11fb04edb6
bytes = 7231
redacted = True
--------------------------------------------------------------------------------

"""Contract tests for audit schema.

Tests verify:
1. JSON serialization correctness
2. Run ID format stability
3. Config hash consistency
4. params_effective calculation rule consistency
"""

from __future__ import annotations

import json
from datetime import datetime, timezone

import pytest

from FishBroWFS_V2.core.audit_schema import (
    AuditSchema,
    compute_params_effective,
)
from FishBroWFS_V2.core.config_hash import stable_config_hash
from FishBroWFS_V2.core.run_id import make_run_id


def test_audit_schema_json_serializable():
    """Test that AuditSchema can be serialized to JSON."""
    audit = AuditSchema(
        run_id=make_run_id(),
        created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        git_sha="a1b2c3d4e5f6",
        dirty_repo=False,
        param_subsample_rate=0.1,
        config_hash="f9e8d7c6b5a4a3b2c1d0e9f8a7b6c5d4e3f2a1b0c9d8e7f6a5b4c3d2e1f0a9b8",
        season="2025Q4",
        dataset_id="synthetic_20k",
        bars=20000,
        params_total=1000,
        params_effective=100,
    )
    
    # Test to_dict()
    audit_dict = audit.to_dict()
    assert isinstance(audit_dict, dict)
    assert "param_subsample_rate" in audit_dict
    
    # Test JSON serialization
    audit_json = json.dumps(audit_dict)
    assert isinstance(audit_json, str)
    
    # Test JSON deserialization
    loaded_dict = json.loads(audit_json)
    assert loaded_dict["param_subsample_rate"] == 0.1
    assert loaded_dict["run_id"] == audit.run_id


def test_run_id_is_stable_format():
    """Test that run_id has stable, parseable format."""
    run_id = make_run_id()
    
    # Verify format:[REDACTED]    assert len(run_id) > 15  # At least timestamp + dash + token
    assert "T" in run_id  # ISO format separator
    assert "Z" in run_id  # UTC timezone indicator
    assert run_id.count("-") >=[REDACTED]    
    # Verify timestamp part is sortable
    parts = run_id.split("-")
    timestamp_part = parts[0] if len(parts) > 1 else run_id.split("Z")[0] + "Z"
    assert len(timestamp_part) >= 15  # YYYYMMDDTHHMMSSZ
    
    # Test with prefix
    prefixed_run_id = make_run_id(prefix="test")
    assert prefixed_run_id.startswith("test-")
    assert "T" in prefixed_run_id
    assert "Z" in prefixed_run_id


def test_config_hash_is_stable():
    """Test that config hash is stable and consistent."""
    config1 = {
        "n_bars": 20000,
        "n_params": 1000,
        "commission": 0.0,
    }
    
    config2 = {
        "commission": 0.0,
        "n_bars": 20000,
        "n_params": 1000,
    }
    
    # Same config with different key order should produce same hash
    hash1 = stable_config_hash(config1)
    hash2 = stable_config_hash(config2)
    assert hash1 == hash2
    
    # Different config should produce different hash
    config3 = {"n_bars": 20001, "n_params": 1000}
    hash3 = stable_config_hash(config3)
    assert hash1 != hash3
    
    # Verify hash format (64 hex chars for SHA256)
    assert len(hash1) == 64
    assert all(c in "0123456789abcdef" for c in hash1)


def test_params_effective_rounding_rule_is_stable():
    """
    Test that params_effective calculation rule is stable and locked.
    
    Rule: int(params_total * param_subsample_rate) (floor)
    """
    # Test cases: (params_total, subsample_rate, expected_effective)
    test_cases = [
        (1000, 0.0, 0),
        (1000, 0.1, 100),
        (1000, 0.15, 150),
        (1000, 0.5, 500),
        (1000, 0.99, 990),
        (1000, 1.0, 1000),
        (100, 0.1, 10),
        (100, 0.33, 33),  # Floor: 33.0 -> 33
        (100, 0.34, 34),  # Floor: 34.0 -> 34
        (100, 0.999, 99),  # Floor: 99.9 -> 99
    ]
    
    for params_total, subsample_rate, expected in test_cases:
        result = compute_params_effective(params_total, subsample_rate)
        assert result == expected, (
            f"Failed for params_total={params_total}, "
            f"subsample_rate={subsample_rate}: "
            f"expected={expected}, got={result}"
        )
    
    # Test edge case: invalid subsample_rate
    with pytest.raises(ValueError):
        compute_params_effective(1000, 1.1)  # > 1.0
    
    with pytest.raises(ValueError):
        compute_params_effective(1000, -0.1)  # < 0.0


def test_manifest_must_include_param_subsample_rate():
    """Test that manifest must include param_subsample_rate."""
    audit = AuditSchema(
        run_id=make_run_id(),
        created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        git_sha="a1b2c3d4e5f6",
        dirty_repo=False,
        param_subsample_rate=0.25,
        config_hash="test_hash",
        season="2025Q4",
        dataset_id="test_dataset",
        bars=20000,
        params_total=1000,
        params_effective=250,
    )
    
    manifest_dict = audit.to_dict()
    
    # Verify param_subsample_rate exists and is correct type
    assert "param_subsample_rate" in manifest_dict
    assert isinstance(manifest_dict["param_subsample_rate"], float)
    assert manifest_dict["param_subsample_rate"] == 0.25
    
    # Verify all required fields exist
    required_fields = [
        "run_id",
        "created_at",
        "git_sha",
        "dirty_repo",
        "param_subsample_rate",
        "config_hash",
        "season",
        "dataset_id",
        "bars",
        "params_total",
        "params_effective",
        "artifact_version",
    ]
    
    for field in required_fields:
        assert field in manifest_dict, f"Missing required field: {field}"


def test_created_at_is_iso8601_utc():
    """Test that created_at uses ISO8601 UTC format with Z suffix."""
    audit = AuditSchema(
        run_id=make_run_id(),
        created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        git_sha="a1b2c3d4e5f6",
        dirty_repo=False,
        param_subsample_rate=0.1,
        config_hash="test_hash",
        season="2025Q4",
        dataset_id="test_dataset",
        bars=20000,
        params_total=1000,
        params_effective=100,
    )
    
    created_at = audit.created_at
    
    # Verify Z suffix (UTC indicator)
    assert created_at.endswith("Z"), f"created_at should end with Z, got: {created_at}"
    
    # Verify ISO8601 format (can parse)
    try:
        # Remove Z and parse
        dt_str = created_at.replace("Z", "+00:00")
        parsed = datetime.fromisoformat(dt_str)
        assert parsed.tzinfo is not None
    except ValueError as e:
        pytest.fail(f"created_at is not valid ISO8601: {created_at}, error: {e}")


def test_audit_schema_is_frozen():
    """Test that AuditSchema is frozen (immutable)."""
    audit = AuditSchema(
        run_id=make_run_id(),
        created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        git_sha="a1b2c3d4e5f6",
        dirty_repo=False,
        param_subsample_rate=0.1,
        config_hash="test_hash",
        season="2025Q4",
        dataset_id="test_dataset",
        bars=20000,
        params_total=1000,
        params_effective=100,
    )
    
    # Verify frozen (cannot modify)
    with pytest.raises(Exception):  # dataclass.FrozenInstanceError
        audit.run_id = "new_id"



--------------------------------------------------------------------------------

FILE tests/test_b5_query_params.py
sha256(source_bytes) = 26790ba9cca8d87772b7c2776fa46062eac1eb01a53cb80367ce6e58e6914576
bytes = 4065
redacted = False
--------------------------------------------------------------------------------

"""Tests for B5 Streamlit querystring parameter parsing."""

from __future__ import annotations

import json
import tempfile
from pathlib import Path

import pytest

from FishBroWFS_V2.core.artifact_reader import read_artifact


@pytest.fixture
def temp_outputs_root() -> Path:
    """Create temporary outputs root directory."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


@pytest.fixture
def sample_run_dir(temp_outputs_root: Path) -> Path:
    """Create a sample run directory with artifacts."""
    season = "2026Q1"
    run_id = "stage0_coarse-20251218T093512Z-d3caa754"
    
    run_dir = temp_outputs_root / "seasons" / season / "runs" / run_id
    run_dir.mkdir(parents=True, exist_ok=True)
    
    # Create minimal manifest.json
    manifest = {
        "run_id": run_id,
        "season": season,
        "config_hash": "test_hash",
        "created_at": "2025-12-18T09:35:12Z",
        "git_sha": "abc123def456",
        "dirty_repo": False,
        "param_subsample_rate": 0.1,
        "bars": 1000,
        "params_total": 100,
        "params_effective": 10,
        "artifact_version": "v1",
    }
    
    (run_dir / "manifest.json").write_text(
        json.dumps(manifest, indent=2), encoding="utf-8"
    )
    
    # Create minimal metrics.json
    metrics = {
        "stage_name": "stage0_coarse",
        "bars": 1000,
        "params_total": 100,
        "params_effective": 10,
        "param_subsample_rate": 0.1,
    }
    (run_dir / "metrics.json").write_text(
        json.dumps(metrics, indent=2), encoding="utf-8"
    )
    
    # Create minimal winners.json
    winners = {
        "topk": [],
        "notes": {"schema": "v1"},
    }
    (run_dir / "winners.json").write_text(
        json.dumps(winners, indent=2), encoding="utf-8"
    )
    
    return run_dir


def test_report_link_format() -> None:
    """Test that report_link format is correct."""
    from FishBroWFS_V2.control.report_links import make_report_link
    
    season = "2026Q1"
    run_id = "stage0_coarse-20251218T093512Z-d3caa754"
    
    link = make_report_link(season=season, run_id=run_id)
    
    assert link.startswith("/?")
    assert f"season={season}" in link
    assert f"run_id={run_id}" in link


def test_run_dir_path_construction(temp_outputs_root: Path, sample_run_dir: Path) -> None:
    """Test that run directory path is constructed correctly."""
    season = "2026Q1"
    run_id = "stage0_coarse-20251218T093512Z-d3caa754"
    
    # Construct path using same logic as Streamlit app
    run_dir = temp_outputs_root / "seasons" / season / "runs" / run_id
    
    assert run_dir.exists()
    assert run_dir == sample_run_dir


def test_artifacts_readable_from_run_dir(sample_run_dir: Path) -> None:
    """Test that artifacts can be read from run directory."""
    # Read manifest
    manifest_result = read_artifact(sample_run_dir / "manifest.json")
    assert manifest_result.raw["run_id"] == "stage0_coarse-20251218T093512Z-d3caa754"
    assert manifest_result.raw["season"] == "2026Q1"
    
    # Read metrics
    metrics_result = read_artifact(sample_run_dir / "metrics.json")
    assert metrics_result.raw["stage_name"] == "stage0_coarse"
    
    # Read winners
    winners_result = read_artifact(sample_run_dir / "winners.json")
    assert winners_result.raw["notes"]["schema"] == "v1"


def test_querystring_parsing_logic() -> None:
    """Test querystring parsing logic (simulating Streamlit query_params)."""
    # Simulate Streamlit query_params.get() behavior
    query_params = {
        "season": "2026Q1",
        "run_id": "stage0_coarse-20251218T093512Z-d3caa754",
    }
    
    season = query_params.get("season", "")
    run_id = query_params.get("run_id", "")
    
    assert season == "2026Q1"
    assert run_id == "stage0_coarse-20251218T093512Z-d3caa754"
    
    # Test missing parameters
    empty_params = {}
    season_empty = empty_params.get("season", "")
    run_id_empty = empty_params.get("run_id", "")
    
    assert season_empty == ""
    assert run_id_empty == ""



--------------------------------------------------------------------------------

FILE tests/test_baseline_lock.py
sha256(source_bytes) = 4bb7c647616227a0eb927ca273482ba1397cf974685359d7dcdb0d78c1ee45d7
bytes = 2073
redacted = False
--------------------------------------------------------------------------------

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.engine_jit import simulate as simulate_jit
from FishBroWFS_V2.engine.matcher_core import simulate as simulate_py
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def _fills_to_matrix(fills):
    # Columns: bar_index, role, kind, side, price, qty, order_id
    m = np.empty((len(fills), 7), dtype=np.float64)
    for i, f in enumerate(fills):
        m[i, 0] = float(f.bar_index)
        m[i, 1] = 0.0 if f.role == OrderRole.EXIT else 1.0
        m[i, 2] = 0.0 if f.kind == OrderKind.STOP else 1.0
        m[i, 3] = float(int(f.side.value))
        m[i, 4] = float(f.price)
        m[i, 5] = float(f.qty)
        m[i, 6] = float(f.order_id)
    return m


def test_gate_a_jit_matches_python_reference():
    # Two bars so we can test next-bar active + entry then exit.
    bars = normalize_bars(
        np.array([100.0, 100.0], dtype=np.float64),
        np.array([120.0, 120.0], dtype=np.float64),
        np.array([90.0, 80.0], dtype=np.float64),
        np.array([110.0, 90.0], dtype=np.float64),
    )

    intents = [
        # Entry active on bar0
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),
        # Exit active on bar0 (same bar), should execute after entry
        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),
        # Entry created on bar0 -> active on bar1
        OrderIntent(order_id=3, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=110.0),
    ]

    py = simulate_py(bars, intents)
    jit = simulate_jit(bars, intents)

    m_py = _fills_to_matrix(py)
    m_jit = _fills_to_matrix(jit)

    assert m_py.shape == m_jit.shape
    # Event-level exactness except price tolerance
    np.testing.assert_array_equal(m_py[:, [0, 1, 2, 3, 5, 6]], m_jit[:, [0, 1, 2, 3, 5, 6]])
    np.testing.assert_allclose(m_py[:, 4], m_jit[:, 4], rtol=0.0, atol=1e-9)




--------------------------------------------------------------------------------

FILE tests/test_builder_sparse_contract.py
sha256(source_bytes) = fa084800f4e2da7b951c2652907781138bf2107003c148e60d680734e5a9a097
bytes = 9180
redacted = False
--------------------------------------------------------------------------------

"""
Contract Tests for Sparse Builder (P2-3)

Verifies sparse intent builder behavior:
- Intent scaling with trigger_rate
- Metrics zeroing for non-selected params
- Seed determinism
"""
from __future__ import annotations

import numpy as np
import os

from FishBroWFS_V2.strategy.builder_sparse import build_intents_sparse


def test_builder_intent_scaling_with_intent_sparse_rate() -> None:
    """
    Test that intents scale approximately linearly with trigger_rate.
    
    Verifies that when trigger_rate=0.05, intents_generated is approximately
    5% of allowed_bars (with tolerance for rounding).
    """
    n_bars = 1000
    channel_len = 20
    order_qty = 1
    
    # Generate synthetic donch_prev array (all valid after warmup)
    donch_prev = np.full(n_bars, 100.0, dtype=np.float64)
    donch_prev[0] = np.nan  # First bar is NaN (shifted)
    # Bars 1..channel_len-1 are valid but before warmup
    # Bars channel_len..n_bars-1 are valid and past warmup
    
    # Run dense (trigger_rate=1.0) - baseline
    result_dense = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=1.0,
        seed=42,
        use_dense=False,
    )
    
    # Run sparse (trigger_rate=0.05) - 5% of triggers
    result_sparse = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=0.05,
        seed=42,
        use_dense=False,
    )
    
    obs_dense = result_dense["obs"]
    obs_sparse = result_sparse["obs"]
    
    allowed_bars_dense = obs_dense.get("allowed_bars")
    intents_generated_dense = obs_dense.get("intents_generated")
    allowed_bars_sparse = obs_sparse.get("allowed_bars")
    intents_generated_sparse = obs_sparse.get("intents_generated")
    valid_mask_sum_dense = obs_dense.get("valid_mask_sum")
    valid_mask_sum_sparse = obs_sparse.get("valid_mask_sum")
    
    # Contract: allowed_bars should be the same (represents valid bars before trigger rate)
    # allowed_bars = valid_mask_sum (baseline, for comparison)
    assert allowed_bars_dense == allowed_bars_sparse, (
        f"allowed_bars should be the same for dense and sparse (both equal valid_mask_sum), "
        f"got {allowed_bars_dense} vs {allowed_bars_sparse}"
    )
    assert valid_mask_sum_dense == valid_mask_sum_sparse, (
        f"valid_mask_sum should be the same for dense and sparse, "
        f"got {valid_mask_sum_dense} vs {valid_mask_sum_sparse}"
    )
    
    # Contract: intents_generated should scale approximately with trigger_rate
    # With trigger_rate=0.05, we expect approximately 5% of valid_mask_sum
    # Allow wide tolerance: [0.02, 0.08] (2% to 8% of valid_mask_sum)
    if valid_mask_sum_dense is not None and valid_mask_sum_dense > 0:
        ratio = intents_generated_sparse / valid_mask_sum_sparse
        assert 0.02 <= ratio <= 0.08, (
            f"With trigger_rate=0.05, intents_generated_sparse ({intents_generated_sparse}) "
            f"should be approximately 5% of valid_mask_sum ({valid_mask_sum_sparse}), "
            f"got ratio {ratio:.4f} (expected [0.02, 0.08])"
        )
    
    # Contract: intents_generated_dense should equal valid_mask_sum (trigger_rate=1.0)
    assert intents_generated_dense == valid_mask_sum_dense, (
        f"With trigger_rate=1.0, intents_generated ({intents_generated_dense}) "
        f"should equal valid_mask_sum ({valid_mask_sum_dense})"
    )


def test_metrics_zeroing_for_non_selected_params() -> None:
    """
    Test that builder correctly handles edge cases (no valid triggers, etc.).
    
    This test verifies that the builder returns empty arrays when there are
    no valid triggers, and that all fields are properly initialized.
    """
    n_bars = 100
    channel_len = 50  # Large warmup, so most bars are invalid
    order_qty = 1
    
    # Generate donch_prev with only a few valid bars
    donch_prev = np.full(n_bars, np.nan, dtype=np.float64)
    donch_prev[0] = np.nan  # First bar is NaN (shifted)
    # Set a few bars to valid values (after warmup)
    donch_prev[60] = 100.0
    donch_prev[70] = 100.0
    donch_prev[80] = 100.0
    
    result = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=1.0,
        seed=42,
        use_dense=False,
    )
    
    # Contract: Should have some intents (3 valid bars after warmup)
    assert result["n_entry"] > 0, "Should have some intents for valid bars"
    
    # Contract: All arrays should have same length
    assert len(result["created_bar"]) == result["n_entry"]
    assert len(result["price"]) == result["n_entry"]
    assert len(result["order_id"]) == result["n_entry"]
    assert len(result["role"]) == result["n_entry"]
    assert len(result["kind"]) == result["n_entry"]
    assert len(result["side"]) == result["n_entry"]
    assert len(result["qty"]) == result["n_entry"]
    
    # Contract: Test with trigger_rate=0.0 (should return empty)
    result_empty = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=0.0,
        seed=42,
        use_dense=False,
    )
    
    assert result_empty["n_entry"] == 0, "With trigger_rate=0.0, should have no intents"
    assert len(result_empty["created_bar"]) == 0
    assert len(result_empty["price"]) == 0


def test_seed_determinism_builder_output() -> None:
    """
    Test that builder output is deterministic for same seed.
    
    Verifies that running the builder twice with the same seed produces
    identical results (bit-exact).
    """
    n_bars = 500
    channel_len = 20
    order_qty = 1
    trigger_rate = 0.1  # 10% of triggers
    
    # Generate synthetic donch_prev array
    donch_prev = np.full(n_bars, 100.0, dtype=np.float64)
    donch_prev[0] = np.nan  # First bar is NaN (shifted)
    
    # Run twice with same seed
    result1 = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=trigger_rate,
        seed=42,
        use_dense=False,
    )
    
    result2 = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=trigger_rate,
        seed=42,
        use_dense=False,
    )
    
    # Contract: Results should be bit-exact identical
    assert result1["n_entry"] == result2["n_entry"], (
        f"n_entry should be identical, got {result1['n_entry']} vs {result2['n_entry']}"
    )
    
    if result1["n_entry"] > 0:
        assert np.array_equal(result1["created_bar"], result2["created_bar"]), (
            "created_bar should be bit-exact identical"
        )
        assert np.array_equal(result1["price"], result2["price"]), (
            "price should be bit-exact identical"
        )
        assert np.array_equal(result1["order_id"], result2["order_id"]), (
            "order_id should be bit-exact identical"
        )
    
    # Contract: Different seeds should produce different results (for sparse mode)
    result3 = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=trigger_rate,
        seed=123,  # Different seed
        use_dense=False,
    )
    
    # With different seed, results may differ (but should still be deterministic)
    # We just verify that the builder runs without error
    assert isinstance(result3["n_entry"], int)
    assert result3["n_entry"] >= 0


def test_dense_vs_sparse_parity() -> None:
    """
    Test that dense builder (use_dense=True) produces same results as sparse with trigger_rate=1.0.
    
    Verifies that the dense reference implementation matches sparse builder
    when trigger_rate=1.0.
    """
    n_bars = 200
    channel_len = 20
    order_qty = 1
    
    # Generate synthetic donch_prev array
    donch_prev = np.full(n_bars, 100.0, dtype=np.float64)
    donch_prev[0] = np.nan  # First bar is NaN (shifted)
    
    # Run dense builder
    result_dense = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=1.0,
        seed=42,
        use_dense=True,
    )
    
    # Run sparse builder with trigger_rate=1.0
    result_sparse = build_intents_sparse(
        donch_prev=donch_prev,
        channel_len=channel_len,
        order_qty=order_qty,
        trigger_rate=1.0,
        seed=42,
        use_dense=False,
    )
    
    # Contract: Results should be identical (both use all valid triggers)
    assert result_dense["n_entry"] == result_sparse["n_entry"], (
        f"n_entry should be identical, got {result_dense['n_entry']} vs {result_sparse['n_entry']}"
    )
    
    if result_dense["n_entry"] > 0:
        assert np.array_equal(result_dense["created_bar"], result_sparse["created_bar"]), (
            "created_bar should be identical"
        )
        assert np.array_equal(result_dense["price"], result_sparse["price"]), (
            "price should be identical"
        )



--------------------------------------------------------------------------------

FILE tests/test_control_api_smoke.py
sha256(source_bytes) = 3004d11d2268694b0f34d5a00f8ac6b63b27c4cc50c8a9dee964493d0594d945
bytes = 8467
redacted = False
--------------------------------------------------------------------------------

"""Smoke tests for API endpoints."""

from __future__ import annotations

import tempfile
from pathlib import Path

import pytest
from fastapi.testclient import TestClient

from FishBroWFS_V2.control.api import app, get_db_path
from FishBroWFS_V2.control.jobs_db import init_db


@pytest.fixture
def test_client() -> TestClient:
    """Create test client with temporary database."""
    import os
    
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = Path(tmpdir) / "test.db"
        init_db(db_path)
        
        # Override DB path
        os.environ["JOBS_DB_PATH"] = str(db_path)
        
        # Re-import to get new DB path
        from FishBroWFS_V2.control import api
        
        # Reinitialize
        api.init_db(db_path)
        
        yield TestClient(app)


def test_health_endpoint(test_client: TestClient) -> None:
    """Test health endpoint."""
    resp = test_client.get("/health")
    assert resp.status_code == 200
    assert resp.json() == {"status": "ok"}


def test_create_job_endpoint(test_client: TestClient) -> None:
    """Test creating a job."""
    req = {
        "season": "test_season",
        "dataset_id": "test_dataset",
        "outputs_root": "outputs",
        "config_snapshot": {"bars": 1000, "params_total": 100},
        "config_hash": "abc123",
        "created_by": "b5c",
    }
    
    resp = test_client.post("/jobs", json=req)
    assert resp.status_code == 200
    data = resp.json()
    assert "job_id" in data
    assert isinstance(data["job_id"], str)


def test_list_jobs_endpoint(test_client: TestClient) -> None:
    """Test listing jobs."""
    # Create a job first
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    test_client.post("/jobs", json=req)
    
    # List jobs
    resp = test_client.get("/jobs")
    assert resp.status_code == 200
    jobs = resp.json()
    assert isinstance(jobs, list)
    assert len(jobs) > 0
    # Check that all jobs have report_link field
    for job in jobs:
        assert "report_link" in job


def test_get_job_endpoint(test_client: TestClient) -> None:
    """Test getting a job by ID."""
    # Create a job
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Get job
    resp = test_client.get(f"/jobs/{job_id}")
    assert resp.status_code == 200
    job = resp.json()
    assert job["job_id"] == job_id
    assert job["status"] == "QUEUED"
    assert "report_link" in job
    assert job["report_link"] is None  # Default is None


def test_check_endpoint(test_client: TestClient) -> None:
    """Test check endpoint."""
    # Create a job
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
            "mem_limit_mb": 6000.0,
        },
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Check
    resp = test_client.post(f"/jobs/{job_id}/check")
    assert resp.status_code == 200
    result = resp.json()
    assert "action" in result
    assert "estimated_mb" in result
    assert "estimates" in result


def test_pause_endpoint(test_client: TestClient) -> None:
    """Test pause endpoint."""
    # Create a job
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Pause
    resp = test_client.post(f"/jobs/{job_id}/pause", json={"pause": True})
    assert resp.status_code == 200
    
    # Unpause
    resp = test_client.post(f"/jobs/{job_id}/pause", json={"pause": False})
    assert resp.status_code == 200


def test_stop_endpoint(test_client: TestClient) -> None:
    """Test stop endpoint."""
    # Create a job
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Stop (soft)
    resp = test_client.post(f"/jobs/{job_id}/stop", json={"mode": "SOFT"})
    assert resp.status_code == 200
    
    # Stop (kill)
    req2 = {
        "season": "test2",
        "dataset_id": "test2",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash2",
    }
    create_resp2 = test_client.post("/jobs", json=req2)
    job_id2 = create_resp2.json()["job_id"]
    
    resp = test_client.post(f"/jobs/{job_id2}/stop", json={"mode": "KILL"})
    assert resp.status_code == 200


def test_log_tail_endpoint(test_client: TestClient) -> None:
    """Test log_tail endpoint."""
    import os
    
    # Create a job
    req = {
        "season": "test_season",
        "dataset_id": "test_dataset",
        "outputs_root": str(Path.cwd() / "outputs"),
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Create log file manually
    from FishBroWFS_V2.control.paths import run_log_path
    
    outputs_root = Path.cwd() / "outputs"
    log_path = run_log_path(outputs_root, "test_season", job_id)
    log_path.write_text("Line 1\nLine 2\nLine 3\n", encoding="utf-8")
    
    # Get log tail
    resp = test_client.get(f"/jobs/{job_id}/log_tail?n=200")
    assert resp.status_code == 200
    data = resp.json()
    assert data["ok"] is True
    assert isinstance(data["lines"], list)
    assert len(data["lines"]) == 3
    assert "Line 1" in data["lines"][0]
    
    # Cleanup
    log_path.unlink(missing_ok=True)


def test_log_tail_missing_file(test_client: TestClient) -> None:
    """Test log_tail endpoint when log file doesn't exist."""
    # Create a job
    req = {
        "season": "test_season",
        "dataset_id": "test_dataset",
        "outputs_root": str(Path.cwd() / "outputs"),
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Get log tail (file doesn't exist)
    resp = test_client.get(f"/jobs/{job_id}/log_tail?n=200")
    assert resp.status_code == 200
    data = resp.json()
    assert data["ok"] is True
    assert data["lines"] == []
    assert data["truncated"] is False


def test_report_link_endpoint(test_client: TestClient) -> None:
    """Test report_link endpoint."""
    from FishBroWFS_V2.control.jobs_db import set_report_link
    
    # Create a job
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Set report_link manually
    import os
    db_path = Path(os.environ["JOBS_DB_PATH"])
    set_report_link(db_path, job_id, "/b5?season=test&run_id=abc123")
    
    # Get report_link
    resp = test_client.get(f"/jobs/{job_id}/report_link")
    assert resp.status_code == 200
    data = resp.json()
    # build_report_link always returns a string (never None)
    assert data["report_link"] == "/b5?season=test&run_id=abc123"


def test_report_link_endpoint_no_link(test_client: TestClient) -> None:
    """Test report_link endpoint when no link exists."""
    # Create a job
    req = {
        "season": "test",
        "dataset_id": "test",
        "outputs_root": "outputs",
        "config_snapshot": {},
        "config_hash": "hash1",
    }
    create_resp = test_client.post("/jobs", json=req)
    job_id = create_resp.json()["job_id"]
    
    # Get report_link (no run_id set)
    resp = test_client.get(f"/jobs/{job_id}/report_link")
    assert resp.status_code == 200
    data = resp.json()
    # build_report_link always returns a string (never None)
    assert data["report_link"] == ""




--------------------------------------------------------------------------------

FILE tests/test_control_jobs_db.py
sha256(source_bytes) = cf8ce5ad42136cccde5b926dcd0cd744139aafcfc7bef0f23e3518d698317dda
bytes = 5224
redacted = False
--------------------------------------------------------------------------------

"""Tests for jobs database."""

from __future__ import annotations

import tempfile
from pathlib import Path

import pytest

from FishBroWFS_V2.control.jobs_db import (
    create_job,
    get_job,
    get_requested_pause,
    get_requested_stop,
    init_db,
    list_jobs,
    mark_done,
    mark_failed,
    mark_killed,
    request_pause,
    request_stop,
    update_running,
)
from FishBroWFS_V2.control.types import DBJobSpec, JobStatus, StopMode


@pytest.fixture
def temp_db() -> Path:
    """Create temporary database for testing."""
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = Path(tmpdir) / "test.db"
        init_db(db_path)
        yield db_path


def test_init_db_creates_table(temp_db: Path) -> None:
    """Test that init_db creates the jobs table."""
    assert temp_db.exists()
    
    import sqlite3
    
    conn = sqlite3.connect(str(temp_db))
    cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='jobs'")
    assert cursor.fetchone() is not None
    conn.close()


def test_create_job_and_get(temp_db: Path) -> None:
    """Test creating and retrieving a job."""
    spec = DBJobSpec(
        season="test_season",
        dataset_id="test_dataset",
        outputs_root="outputs",
        config_snapshot={"bars": 1000, "params_total": 100},
        config_hash="abc123",
    )
    
    job_id = create_job(temp_db, spec)
    assert job_id
    
    job = get_job(temp_db, job_id)
    assert job.job_id == job_id
    assert job.status == JobStatus.QUEUED
    assert job.spec.season == "test_season"
    assert job.spec.dataset_id == "test_dataset"
    assert job.report_link is None  # Default is None


def test_list_jobs(temp_db: Path) -> None:
    """Test listing jobs."""
    spec = DBJobSpec(
        season="test",
        dataset_id="test",
        outputs_root="outputs",
        config_snapshot={},
        config_hash="hash1",
    )
    
    job_id1 = create_job(temp_db, spec)
    job_id2 = create_job(temp_db, spec)
    
    jobs = list_jobs(temp_db, limit=10)
    assert len(jobs) == 2
    assert {j.job_id for j in jobs} == {job_id1, job_id2}
    # Check that all jobs have report_link field
    for job in jobs:
        assert hasattr(job, "report_link")
        assert job.report_link is None  # Default is None


def test_request_pause(temp_db: Path) -> None:
    """Test pause request."""
    spec = DBJobSpec(
        season="test",
        dataset_id="test",
        outputs_root="outputs",
        config_snapshot={},
        config_hash="hash1",
    )
    job_id = create_job(temp_db, spec)
    
    request_pause(temp_db, job_id, pause=True)
    assert get_requested_pause(temp_db, job_id) is True
    
    request_pause(temp_db, job_id, pause=False)
    assert get_requested_pause(temp_db, job_id) is False


def test_request_stop(temp_db: Path) -> None:
    """Test stop request."""
    spec = DBJobSpec(
        season="test",
        dataset_id="test",
        outputs_root="outputs",
        config_snapshot={},
        config_hash="hash1",
    )
    job_id = create_job(temp_db, spec)
    
    request_stop(temp_db, job_id, StopMode.SOFT)
    assert get_requested_stop(temp_db, job_id) == "SOFT"
    
    request_stop(temp_db, job_id, StopMode.KILL)
    assert get_requested_stop(temp_db, job_id) == "KILL"
    
    # QUEUED job should be immediately KILLED
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.KILLED


def test_status_transitions(temp_db: Path) -> None:
    """Test status transitions."""
    spec = DBJobSpec(
        season="test",
        dataset_id="test",
        outputs_root="outputs",
        config_snapshot={},
        config_hash="hash1",
    )
    job_id = create_job(temp_db, spec)
    
    # QUEUED -> RUNNING
    update_running(temp_db, job_id, pid=12345)
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.RUNNING
    assert job.pid == 12345
    
    # RUNNING -> DONE
    mark_done(temp_db, job_id)
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.DONE
    
    # Cannot transition from DONE
    with pytest.raises(ValueError, match="Cannot transition from terminal status"):
        update_running(temp_db, job_id, pid=12345)


def test_mark_failed(temp_db: Path) -> None:
    """Test marking job as failed."""
    spec = DBJobSpec(
        season="test",
        dataset_id="test",
        outputs_root="outputs",
        config_snapshot={},
        config_hash="hash1",
    )
    job_id = create_job(temp_db, spec)
    update_running(temp_db, job_id, pid=12345)
    
    mark_failed(temp_db, job_id, error="Test error")
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.FAILED
    assert job.last_error == "Test error"


def test_mark_killed(temp_db: Path) -> None:
    """Test marking job as killed."""
    spec = DBJobSpec(
        season="test",
        dataset_id="test",
        outputs_root="outputs",
        config_snapshot={},
        config_hash="hash1",
    )
    job_id = create_job(temp_db, spec)
    
    mark_killed(temp_db, job_id, error="Killed by user")
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.KILLED
    assert job.last_error == "Killed by user"




--------------------------------------------------------------------------------

FILE tests/test_control_preflight.py
sha256(source_bytes) = 084c654e8beafa43a38aab716b67c67f8fe87bcfe8cc92e3f5be307b43ff3fdc
bytes = 1947
redacted = False
--------------------------------------------------------------------------------

"""Tests for preflight check."""

from __future__ import annotations

import pytest

from FishBroWFS_V2.control.preflight import PreflightResult, run_preflight


def test_run_preflight_returns_required_keys() -> None:
    """Test that preflight returns all required keys."""
    cfg_snapshot = {
        "season": "test",
        "dataset_id": "test",
        "bars": 1000,
        "params_total": 100,
        "param_subsample_rate": 0.1,
        "mem_limit_mb": 6000.0,
        "allow_auto_downsample": True,
    }
    
    result = run_preflight(cfg_snapshot)
    
    assert isinstance(result, PreflightResult)
    assert result.action in {"PASS", "BLOCK", "AUTO_DOWNSAMPLE"}
    assert isinstance(result.reason, str)
    assert isinstance(result.original_subsample, float)
    assert isinstance(result.final_subsample, float)
    assert isinstance(result.estimated_bytes, int)
    assert isinstance(result.estimated_mb, float)
    assert isinstance(result.mem_limit_mb, float)
    assert isinstance(result.mem_limit_bytes, int)
    assert isinstance(result.estimates, dict)
    
    # Check estimates keys
    assert "ops_est" in result.estimates
    assert "time_est_s" in result.estimates
    assert "mem_est_mb" in result.estimates
    assert "mem_est_bytes" in result.estimates
    assert "mem_limit_mb" in result.estimates
    assert "mem_limit_bytes" in result.estimates


def test_preflight_pure_no_io() -> None:
    """Test that preflight is pure (no I/O)."""
    cfg_snapshot = {
        "season": "test",
        "dataset_id": "test",
        "bars": 100,
        "params_total": 10,
        "param_subsample_rate": 0.5,
        "mem_limit_mb": 10000.0,
    }
    
    # Should not raise any I/O errors
    result1 = run_preflight(cfg_snapshot)
    result2 = run_preflight(cfg_snapshot)
    
    # Should be deterministic
    assert result1.action == result2.action
    assert result1.estimated_bytes == result2.estimated_bytes




--------------------------------------------------------------------------------

FILE tests/test_control_worker_integration.py
sha256(source_bytes) = 4d3003c99a1b519f7ed39d54e9af0134fe36f798561b5d0f24cfe4e263eaddb6
bytes = 3641
redacted = False
--------------------------------------------------------------------------------

"""Integration tests for worker execution and job completion."""

from __future__ import annotations

import tempfile
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

from FishBroWFS_V2.control.jobs_db import create_job, get_job, init_db
from FishBroWFS_V2.control.report_links import make_report_link
from FishBroWFS_V2.control.types import DBJobSpec, JobStatus
from FishBroWFS_V2.control.worker import run_one_job
from FishBroWFS_V2.pipeline.funnel_schema import (
    FunnelPlan,
    FunnelResultIndex,
    FunnelStageIndex,
    StageName,
    StageSpec,
)


@pytest.fixture
def temp_db() -> Path:
    """Create temporary database for testing."""
    with tempfile.TemporaryDirectory() as tmpdir:
        db_path = Path(tmpdir) / "test.db"
        init_db(db_path)
        yield db_path


@pytest.fixture
def temp_outputs_root() -> Path:
    """Create temporary outputs root directory."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


def test_worker_completes_job_with_run_id_and_report_link(
    temp_db: Path, temp_outputs_root: Path
) -> None:
    """Test that worker completes job and sets run_id and report_link."""
    # Create a job
    season = "2026Q1"
    spec = DBJobSpec(
        season=season,
        dataset_id="test_dataset",
        outputs_root=str(temp_outputs_root),
        config_snapshot={
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
        },
        config_hash="test_hash",
    )
    
    job_id = create_job(temp_db, spec)
    
    # Mock run_funnel to return a fake result
    fake_run_id = "stage2_confirm-20251218T093513Z-354cee6b"
    fake_stage_index = FunnelStageIndex(
        stage=StageName.STAGE2_CONFIRM,
        run_id=fake_run_id,
        run_dir=f"seasons/{season}/runs/{fake_run_id}",
    )
    fake_result_index = FunnelResultIndex(
        plan=FunnelPlan(stages=[]),
        stages=[fake_stage_index],
    )
    
    with patch("FishBroWFS_V2.control.worker.run_funnel") as mock_run_funnel:
        mock_run_funnel.return_value = fake_result_index
        
        # Run the job
        run_one_job(temp_db, job_id)
    
    # Check that job is marked as DONE
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.DONE
    assert job.run_id == fake_run_id
    assert job.report_link == make_report_link(season=season, run_id=fake_run_id)
    
    # Verify report_link format
    assert f"season={season}" in job.report_link
    assert f"run_id={fake_run_id}" in job.report_link


def test_worker_handles_empty_funnel_result(
    temp_db: Path, temp_outputs_root: Path
) -> None:
    """Test that worker handles empty funnel result gracefully."""
    spec = DBJobSpec(
        season="2026Q1",
        dataset_id="test_dataset",
        outputs_root=str(temp_outputs_root),
        config_snapshot={"bars": 1000, "params_total": 100},
        config_hash="test_hash",
    )
    
    job_id = create_job(temp_db, spec)
    
    # Mock run_funnel to return empty result
    fake_result_index = FunnelResultIndex(
        plan=FunnelPlan(stages=[]),
        stages=[],
    )
    
    with patch("FishBroWFS_V2.control.worker.run_funnel") as mock_run_funnel:
        mock_run_funnel.return_value = fake_result_index
        
        # Run the job
        run_one_job(temp_db, job_id)
    
    # Check that job is still marked as DONE (even without stages)
    job = get_job(temp_db, job_id)
    assert job.status == JobStatus.DONE
    # run_id and report_link should be None if no stages
    assert job.run_id is None
    assert job.report_link is None



--------------------------------------------------------------------------------

FILE tests/test_data_cache_rebuild_fingerprint_stable.py
sha256(source_bytes) = b01481372ccb6b777578871f805655062cf38b40408ab96867efebd572fc02c9
bytes = 4985
redacted = False
--------------------------------------------------------------------------------

"""Test: Delete parquet cache and rebuild - fingerprint must remain stable.

Binding #4: Parquet is Cache, Not Truth.
Fingerprint is computed from raw TXT + ingest_policy, not from parquet.
"""

from __future__ import annotations

from pathlib import Path

import pytest

from FishBroWFS_V2.data.cache import CachePaths, cache_paths, read_parquet_cache, write_parquet_cache
from FishBroWFS_V2.data.fingerprint import compute_txt_fingerprint
from FishBroWFS_V2.data.raw_ingest import ingest_raw_txt


def test_cache_rebuild_fingerprint_stable(temp_dir: Path, sample_raw_txt: Path) -> None:
    """Test that deleting parquet and rebuilding produces same fingerprint.
    
    Flow:
    1. Use sample_raw_txt fixture
    2. Compute fingerprint sha1 A
    3. Ingest  write parquet cache
    4. Delete parquet + meta
    5. Ingest  write parquet cache (same policy)
    6. Compute fingerprint sha1 B
    7. Assert A == B
    8. Assert meta.data_fingerprint_sha1 == A
    """
    # Use sample_raw_txt fixture
    txt_path = sample_raw_txt
    
    # Ingest policy
    ingest_policy = {
        "normalized_24h": False,
        "column_map": None,
    }
    
    # Step 1: Compute fingerprint sha1 A
    fingerprint_a = compute_txt_fingerprint(txt_path, ingest_policy=ingest_policy)
    sha1_a = fingerprint_a.sha1
    
    # Step 2: Ingest  write parquet cache
    result = ingest_raw_txt(txt_path)
    cache_root = temp_dir / "cache"
    cache_paths_obj = cache_paths(cache_root, "TEST_SYMBOL")
    
    meta = {
        "data_fingerprint_sha1": sha1_a,
        "source_path": str(txt_path),
        "ingest_policy": ingest_policy,
        "rows": result.rows,
        "first_ts_str": result.df.iloc[0]["ts_str"],
        "last_ts_str": result.df.iloc[-1]["ts_str"],
    }
    
    write_parquet_cache(cache_paths_obj, result.df, meta)
    
    # Verify cache exists
    assert cache_paths_obj.parquet_path.exists()
    assert cache_paths_obj.meta_path.exists()
    
    # Step 3: Delete parquet + meta
    cache_paths_obj.parquet_path.unlink()
    cache_paths_obj.meta_path.unlink()
    
    assert not cache_paths_obj.parquet_path.exists()
    assert not cache_paths_obj.meta_path.exists()
    
    # Step 4: Ingest  write parquet cache (same policy)
    result2 = ingest_raw_txt(txt_path)
    write_parquet_cache(cache_paths_obj, result2.df, meta)
    
    # Step 5: Compute fingerprint sha1 B
    fingerprint_b = compute_txt_fingerprint(txt_path, ingest_policy=ingest_policy)
    sha1_b = fingerprint_b.sha1
    
    # Step 6: Assert A == B
    assert sha1_a == sha1_b, f"Fingerprint changed after cache rebuild: {sha1_a} != {sha1_b}"
    
    # Step 7: Assert meta.data_fingerprint_sha1 == A
    df_read, meta_read = read_parquet_cache(cache_paths_obj)
    assert meta_read["data_fingerprint_sha1"] == sha1_a
    assert meta_read["data_fingerprint_sha1"] == sha1_b


def test_cache_rebuild_with_24h_normalization(temp_dir: Path) -> None:
    """Test fingerprint stability with 24:00 normalization."""
    # Create temp raw TXT with 24:00:00 (specific test case, not using fixture)
    txt_path = temp_dir / "test_data_24h.txt"
    txt_content = """Date,Time,Open,High,Low,Close,TotalVolume
2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000
2013/1/1,24:00:00,104.0,106.0,103.0,105.0,1200
2013/1/2,09:30:00,105.0,107.0,104.0,106.0,1500
"""
    txt_path.write_text(txt_content, encoding="utf-8")
    
    # Ingest policy (will normalize 24:00:00)
    ingest_policy = {
        "normalized_24h": True,  # Will be set to True after ingest
        "column_map": None,
    }
    
    # Ingest first time
    result1 = ingest_raw_txt(txt_path)
    # Update policy to reflect normalization
    ingest_policy["normalized_24h"] = result1.policy.normalized_24h
    
    # Compute fingerprint
    fingerprint_a = compute_txt_fingerprint(txt_path, ingest_policy=ingest_policy)
    sha1_a = fingerprint_a.sha1
    
    # Write cache
    cache_root = temp_dir / "cache2"
    cache_paths_obj = cache_paths(cache_root, "TEST_SYMBOL_24H")
    
    meta = {
        "data_fingerprint_sha1": sha1_a,
        "source_path": str(txt_path),
        "ingest_policy": ingest_policy,
        "rows": result1.rows,
        "first_ts_str": result1.df.iloc[0]["ts_str"],
        "last_ts_str": result1.df.iloc[-1]["ts_str"],
    }
    
    write_parquet_cache(cache_paths_obj, result1.df, meta)
    
    # Delete cache
    cache_paths_obj.parquet_path.unlink()
    cache_paths_obj.meta_path.unlink()
    
    # Rebuild
    result2 = ingest_raw_txt(txt_path)
    write_parquet_cache(cache_paths_obj, result2.df, meta)
    
    # Compute fingerprint again
    fingerprint_b = compute_txt_fingerprint(txt_path, ingest_policy=ingest_policy)
    sha1_b = fingerprint_b.sha1
    
    # Assert stability
    assert sha1_a == sha1_b, f"Fingerprint changed: {sha1_a} != {sha1_b}"
    assert result1.policy.normalized_24h == True  # Should have normalized 24:00:00
    assert result2.policy.normalized_24h == True



--------------------------------------------------------------------------------

FILE tests/test_data_ingest_e2e.py
sha256(source_bytes) = 94bd0ea496c9c7c435f641d53e0bd5f7d64ea1d28ed4438d0dfa98acfe4e18aa
bytes = 5945
redacted = False
--------------------------------------------------------------------------------

"""End-to-end test: Ingest  Cache  Rebuild.

Tests the complete data ingest pipeline:
1. Ingest raw TXT  DataFrame
2. Compute fingerprint
3. Write parquet cache + meta.json
4. Clean cache
5. Rebuild cache
6. Verify fingerprint stability
"""

from __future__ import annotations

from pathlib import Path

import pytest

from FishBroWFS_V2.data.cache import cache_paths, read_parquet_cache, write_parquet_cache
from FishBroWFS_V2.data.fingerprint import compute_txt_fingerprint
from FishBroWFS_V2.data.raw_ingest import ingest_raw_txt

# Note: sample_raw_txt fixture is defined in conftest.py for all tests


def test_ingest_cache_e2e(tmp_path: Path, sample_raw_txt: Path) -> None:
    """End-to-end test: Ingest  Compute fingerprint  Write cache.
    
    Tests:
    1. ingest_raw_txt() produces DataFrame with correct columns
    2. compute_txt_fingerprint() produces SHA1 hash
    3. write_parquet_cache() creates parquet and meta.json files
    4. meta.json contains data_fingerprint_sha1
    """
    # Step 1: Ingest raw TXT
    result = ingest_raw_txt(sample_raw_txt)
    
    # Verify DataFrame structure
    assert len(result.df) == 3
    assert list(result.df.columns) == ["ts_str", "open", "high", "low", "close", "volume"]
    assert result.df["ts_str"].dtype == "object"  # str
    assert result.df["open"].dtype == "float64"
    assert result.df["volume"].dtype == "int64"
    
    # Step 2: Compute fingerprint
    ingest_policy = {
        "normalized_24h": result.policy.normalized_24h,
        "column_map": result.policy.column_map,
    }
    fingerprint = compute_txt_fingerprint(sample_raw_txt, ingest_policy=ingest_policy)
    
    # Verify fingerprint
    assert len(fingerprint.sha1) == 40  # SHA1 hex length
    assert fingerprint.source_path == str(sample_raw_txt)
    assert fingerprint.rows == 3
    
    # Step 3: Write cache
    cache_root = tmp_path / "cache"
    symbol = "TEST_SYMBOL"
    paths = cache_paths(cache_root, symbol)
    
    meta = {
        "data_fingerprint_sha1": fingerprint.sha1,
        "source_path": str(sample_raw_txt),
        "ingest_policy": ingest_policy,
        "rows": result.rows,
        "first_ts_str": result.df.iloc[0]["ts_str"],
        "last_ts_str": result.df.iloc[-1]["ts_str"],
    }
    
    write_parquet_cache(paths, result.df, meta)
    
    # Step 4: Verify cache files exist
    assert paths.parquet_path.exists(), f"Parquet file not created: {paths.parquet_path}"
    assert paths.meta_path.exists(), f"Meta file not created: {paths.meta_path}"
    
    # Step 5: Verify meta.json contains fingerprint
    df_read, meta_read = read_parquet_cache(paths)
    
    assert "data_fingerprint_sha1" in meta_read
    assert meta_read["data_fingerprint_sha1"] == fingerprint.sha1
    assert meta_read["data_fingerprint_sha1"] == meta["data_fingerprint_sha1"]
    
    # Verify parquet data matches original
    assert len(df_read) == 3
    assert list(df_read.columns) == ["ts_str", "open", "high", "low", "close", "volume"]
    assert df_read.iloc[0]["ts_str"] == "2013/1/1 09:30:00"


def test_clean_rebuild_fingerprint_stable(tmp_path: Path, sample_raw_txt: Path) -> None:
    """Test: Clean cache  Rebuild  Fingerprint remains stable.
    
    Flow:
    1. Ingest  Write cache  Get sha1_before
    2. Clean cache (delete parquet + meta)
    3. Re-ingest  Write cache  Get sha1_after
    4. Assert sha1_before == sha1_after
    
     No mocks, no hardcoding - real file operations only.
    """
    # Step 1: Initial ingest and cache
    result1 = ingest_raw_txt(sample_raw_txt)
    ingest_policy = {
        "normalized_24h": result1.policy.normalized_24h,
        "column_map": result1.policy.column_map,
    }
    fingerprint1 = compute_txt_fingerprint(sample_raw_txt, ingest_policy=ingest_policy)
    
    cache_root = tmp_path / "cache_rebuild"
    symbol = "TEST_SYMBOL_REBUILD"
    paths = cache_paths(cache_root, symbol)
    
    meta1 = {
        "data_fingerprint_sha1": fingerprint1.sha1,
        "source_path": str(sample_raw_txt),
        "ingest_policy": ingest_policy,
        "rows": result1.rows,
        "first_ts_str": result1.df.iloc[0]["ts_str"],
        "last_ts_str": result1.df.iloc[-1]["ts_str"],
    }
    
    write_parquet_cache(paths, result1.df, meta1)
    
    # Verify cache exists
    assert paths.parquet_path.exists()
    assert paths.meta_path.exists()
    
    # Read meta to get sha1_before
    _, meta_read_before = read_parquet_cache(paths)
    sha1_before = meta_read_before["data_fingerprint_sha1"]
    assert sha1_before == fingerprint1.sha1
    
    # Step 2: Clean cache (delete parquet + meta)
    # Directly delete files (real cleanup, no mocks)
    paths.parquet_path.unlink()
    paths.meta_path.unlink()
    
    # Verify files are deleted
    assert not paths.parquet_path.exists()
    assert not paths.meta_path.exists()
    
    # Step 3: Re-ingest and rebuild cache
    result2 = ingest_raw_txt(sample_raw_txt)
    fingerprint2 = compute_txt_fingerprint(sample_raw_txt, ingest_policy=ingest_policy)
    
    meta2 = {
        "data_fingerprint_sha1": fingerprint2.sha1,
        "source_path": str(sample_raw_txt),
        "ingest_policy": ingest_policy,
        "rows": result2.rows,
        "first_ts_str": result2.df.iloc[0]["ts_str"],
        "last_ts_str": result2.df.iloc[-1]["ts_str"],
    }
    
    write_parquet_cache(paths, result2.df, meta2)
    
    # Step 4: Verify fingerprint stability
    _, meta_read_after = read_parquet_cache(paths)
    sha1_after = meta_read_after["data_fingerprint_sha1"]
    
    assert sha1_before == sha1_after, (
        f"Fingerprint changed after cache rebuild: "
        f"before={sha1_before}, after={sha1_after}"
    )
    assert sha1_after == fingerprint2.sha1
    assert fingerprint1.sha1 == fingerprint2.sha1, (
        f"Fingerprint computation changed: "
        f"first={fingerprint1.sha1}, second={fingerprint2.sha1}"
    )



--------------------------------------------------------------------------------

FILE tests/test_data_ingest_monkeypatch_trap.py
sha256(source_bytes) = e06e425db569bba538be494e7e4e2230d82615922b61ca93229cb47dd13f2fea
bytes = 6017
redacted = False
--------------------------------------------------------------------------------

"""Monkeypatch trap test: Ensure forbidden pandas methods are never called during raw ingest.

This test uses monkeypatch to trap any calls to forbidden methods.
If any forbidden method is called, the test immediately fails with a clear error.

Binding: Raw means RAW (Phase 6.5) - no sort, no dedup, no dropna, no datetime parse.
"""

from __future__ import annotations

from pathlib import Path

import pandas as pd
import pytest

from FishBroWFS_V2.data.raw_ingest import ingest_raw_txt


def test_raw_ingest_forbidden_methods_trap(monkeypatch: pytest.MonkeyPatch, sample_raw_txt: Path) -> None:
    """Trap test: Any forbidden pandas method call during ingest will immediately fail.
    
    This test uses monkeypatch to replace forbidden methods with functions that
    raise AssertionError. If ingest_raw_txt() calls any forbidden method, the
    test will fail immediately with a clear error message.
    
    Forbidden methods:
    - pd.DataFrame.sort_values() - violates row order preservation
    - pd.DataFrame.dropna() - violates empty value preservation
    - pd.DataFrame.drop_duplicates() - violates duplicate preservation
    - pd.to_datetime() - violates naive ts_str contract (Phase 6.5)
    
     This is a constitutional test, not a debug log.
    The error messages are legal requirements, not debugging hints.
    """
    # Arrange: Patch forbidden methods to raise AssertionError if called
    
    def _boom_sort_values(*args, **kwargs):
        """Trap function for sort_values() - violates Raw means RAW."""
        raise AssertionError(
            "FORBIDDEN: sort_values() violates Raw means RAW (Phase 6.5). "
            "Row order must be preserved exactly as in TXT file."
        )
    
    def _boom_dropna(*args, **kwargs):
        """Trap function for dropna() - violates Raw means RAW."""
        raise AssertionError(
            "FORBIDDEN: dropna() violates Raw means RAW (Phase 6.5). "
            "Empty values must be preserved (e.g., volume=0)."
        )
    
    def _boom_drop_duplicates(*args, **kwargs):
        """Trap function for drop_duplicates() - violates Raw means RAW."""
        raise AssertionError(
            "FORBIDDEN: drop_duplicates() violates Raw means RAW (Phase 6.5). "
            "Duplicate rows must be preserved exactly as in TXT file."
        )
    
    def _boom_to_datetime(*args, **kwargs):
        """Trap function for pd.to_datetime() - violates naive ts_str contract."""
        raise AssertionError(
            "FORBIDDEN: pd.to_datetime() violates Naive ts_str Contract (Phase 6.5). "
            "Timestamp must remain as string literal, no datetime parsing allowed."
        )
    
    # Apply monkeypatches (scope limited to this test function)
    # Note: pd.to_datetime() is only used in _normalize_24h() for date parsing.
    # Since sample_raw_txt doesn't contain 24:00:00, _normalize_24h won't be called,
    # so we can safely trap all pd.to_datetime calls
    monkeypatch.setattr(pd.DataFrame, "sort_values", _boom_sort_values)
    monkeypatch.setattr(pd.DataFrame, "dropna", _boom_dropna)
    monkeypatch.setattr(pd.DataFrame, "drop_duplicates", _boom_drop_duplicates)
    monkeypatch.setattr(pd, "to_datetime", _boom_to_datetime)
    
    # Act: Call ingest_raw_txt() with patched pandas
    # If any forbidden method is called, AssertionError will be raised immediately
    result = ingest_raw_txt(sample_raw_txt)
    
    # Assert: Ingest completed successfully without triggering any traps
    # If we reach here, no forbidden methods were called
    assert result is not None
    assert len(result.df) > 0
    assert "ts_str" in result.df.columns
    assert result.df["ts_str"].dtype == "object"  # Must be string, not datetime


def test_raw_ingest_forbidden_methods_trap_with_24h_normalization(
    monkeypatch: pytest.MonkeyPatch, temp_dir: Path
) -> None:
    """Trap test with 24:00 normalization - ensure no forbidden DataFrame methods called.
    
    Tests the same traps but with a TXT file containing 24:00:00 time.
    Note: pd.to_datetime() is allowed in _normalize_24h() for date parsing only,
    so we only trap DataFrame methods, not pd.to_datetime().
    """
    # Create TXT with 24:00:00 (requires normalization)
    txt_path = temp_dir / "test_24h.txt"
    txt_content = """Date,Time,Open,High,Low,Close,TotalVolume
2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000
2013/1/1,24:00:00,104.0,106.0,103.0,105.0,1200
2013/1/2,09:30:00,105.0,107.0,104.0,106.0,1500
"""
    txt_path.write_text(txt_content, encoding="utf-8")
    
    # Arrange: Patch forbidden DataFrame methods only
    # Note: pd.to_datetime() is allowed for date parsing in _normalize_24h()
    def _boom_sort_values(*args, **kwargs):
        raise AssertionError(
            "FORBIDDEN: sort_values() violates Raw means RAW (Phase 6.5). "
            "Row order must be preserved exactly as in TXT file."
        )
    
    def _boom_dropna(*args, **kwargs):
        raise AssertionError(
            "FORBIDDEN: dropna() violates Raw means RAW (Phase 6.5). "
            "Empty values must be preserved (e.g., volume=0)."
        )
    
    def _boom_drop_duplicates(*args, **kwargs):
        raise AssertionError(
            "FORBIDDEN: drop_duplicates() violates Raw means RAW (Phase 6.5). "
            "Duplicate rows must be preserved exactly as in TXT file."
        )
    
    monkeypatch.setattr(pd.DataFrame, "sort_values", _boom_sort_values)
    monkeypatch.setattr(pd.DataFrame, "dropna", _boom_dropna)
    monkeypatch.setattr(pd.DataFrame, "drop_duplicates", _boom_drop_duplicates)
    
    # Act: Call ingest_raw_txt() - should succeed with 24h normalization
    result = ingest_raw_txt(txt_path)
    
    # Assert: Ingest completed successfully
    assert result is not None
    assert len(result.df) == 3
    assert result.policy.normalized_24h == True  # Should have normalized 24:00:00
    # Verify 24:00:00 was normalized to next day 00:00:00
    assert "2013/1/2 00:00:00" in result.df["ts_str"].values



--------------------------------------------------------------------------------

FILE tests/test_data_ingest_raw_means_raw.py
sha256(source_bytes) = 378c1ee44861439dc0a2c3c582033fe5bc98c669ae99c6d76df5dd2782fbe4c5
bytes = 5756
redacted = False
--------------------------------------------------------------------------------

"""Test: Raw means RAW - regression prevention.

RED TEAM #1: Lock down three things:
1. Row order unchanged (no sort)
2. Duplicate ts_str not deduplicated (no drop_duplicates)
3. Empty values not dropped (no dropna) - test with volume=0
"""

from __future__ import annotations

from pathlib import Path

import pandas as pd
import pytest

from FishBroWFS_V2.data.raw_ingest import ingest_raw_txt


def test_row_order_preserved(temp_dir: Path) -> None:
    """Test that row order matches TXT file exactly (no sort)."""
    # Create TXT with intentionally unsorted timestamps
    txt_path = temp_dir / "test_order.txt"
    txt_content = """Date,Time,Open,High,Low,Close,TotalVolume
2013/1/3,09:30:00,110.0,115.0,109.0,114.0,2000
2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000
2013/1/2,09:30:00,105.0,107.0,104.0,106.0,1500
"""
    txt_path.write_text(txt_content, encoding="utf-8")
    
    result = ingest_raw_txt(txt_path)
    
    # Assert order matches TXT (first row should be 2013/1/3)
    assert result.df.iloc[0]["ts_str"] == "2013/1/3 09:30:00"
    assert result.df.iloc[1]["ts_str"] == "2013/1/1 09:30:00"
    assert result.df.iloc[2]["ts_str"] == "2013/1/2 09:30:00"
    
    # Verify no sort occurred (should be in TXT order)
    assert len(result.df) == 3


def test_duplicate_ts_str_not_deduped(temp_dir: Path) -> None:
    """Test that duplicate ts_str rows are preserved (no drop_duplicates)."""
    # Create TXT with duplicate Date/Time but different Close values
    txt_path = temp_dir / "test_duplicate.txt"
    txt_content = """Date,Time,Open,High,Low,Close,TotalVolume
2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000
2013/1/1,09:30:00,100.0,105.0,99.0,105.0,1200
2013/1/2,09:30:00,105.0,107.0,104.0,106.0,1500
"""
    txt_path.write_text(txt_content, encoding="utf-8")
    
    result = ingest_raw_txt(txt_path)
    
    # Assert both duplicate rows are present
    assert len(result.df) == 3
    
    # Assert order matches TXT
    assert result.df.iloc[0]["ts_str"] == "2013/1/1 09:30:00"
    assert result.df.iloc[0]["close"] == 104.0
    
    assert result.df.iloc[1]["ts_str"] == "2013/1/1 09:30:00"
    assert result.df.iloc[1]["close"] == 105.0  # Different close value
    
    assert result.df.iloc[2]["ts_str"] == "2013/1/2 09:30:00"
    
    # Verify duplicates exist (ts_str column should have duplicates)
    ts_str_counts = result.df["ts_str"].value_counts()
    assert ts_str_counts["2013/1/1 09:30:00"] == 2


def test_volume_zero_preserved(temp_dir: Path) -> None:
    """Test that volume=0 rows are preserved (no dropna)."""
    # Create TXT with volume=0
    txt_path = temp_dir / "test_volume_zero.txt"
    txt_content = """Date,Time,Open,High,Low,Close,TotalVolume
2013/1/1,09:30:00,100.0,105.0,99.0,104.0,0
2013/1/1,10:00:00,104.0,106.0,103.0,105.0,1200
2013/1/2,09:30:00,105.0,107.0,104.0,106.0,0
"""
    txt_path.write_text(txt_content, encoding="utf-8")
    
    result = ingest_raw_txt(txt_path)
    
    # Assert all rows are present (including volume=0)
    assert len(result.df) == 3
    
    # Assert volume=0 rows are preserved
    assert result.df.iloc[0]["volume"] == 0
    assert result.df.iloc[1]["volume"] == 1200
    assert result.df.iloc[2]["volume"] == 0
    
    # Verify volume column type is int64
    assert result.df["volume"].dtype == "int64"


def test_no_sort_values_called(temp_dir: Path) -> None:
    """Regression test: Ensure sort_values is never called internally."""
    # This is a contract test - if sort is called, order would change
    txt_path = temp_dir / "test_no_sort.txt"
    txt_content = """Date,Time,Open,High,Low,Close,TotalVolume
2013/1/3,09:30:00,110.0,115.0,109.0,114.0,2000
2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000
2013/1/2,09:30:00,105.0,107.0,104.0,106.0,1500
"""
    txt_path.write_text(txt_content, encoding="utf-8")
    
    result = ingest_raw_txt(txt_path)
    
    # If sort was called, first row would be 2013/1/1 (earliest)
    # But we expect 2013/1/3 (first in TXT)
    first_ts = result.df.iloc[0]["ts_str"]
    assert first_ts.startswith("2013/1/3"), f"Row order changed - first row is {first_ts}, expected 2013/1/3"


def test_no_drop_duplicates_called(temp_dir: Path) -> None:
    """Regression test: Ensure drop_duplicates is never called internally."""
    txt_path = temp_dir / "test_no_dedup.txt"
    txt_content = """Date,Time,Open,High,Low,Close,TotalVolume
2013/1/1,09:30:00,100.0,105.0,99.0,104.0,1000
2013/1/1,09:30:00,100.0,105.0,99.0,105.0,1200
2013/1/1,09:30:00,100.0,105.0,99.0,106.0,1300
"""
    txt_path.write_text(txt_content, encoding="utf-8")
    
    result = ingest_raw_txt(txt_path)
    
    # If drop_duplicates was called, we'd have only 1 row
    # But we expect 3 rows (all duplicates preserved)
    assert len(result.df) == 3
    
    # All should have same ts_str
    assert all(result.df["ts_str"] == "2013/1/1 09:30:00")
    
    # But different close values
    assert result.df.iloc[0]["close"] == 104.0
    assert result.df.iloc[1]["close"] == 105.0
    assert result.df.iloc[2]["close"] == 106.0


def test_no_dropna_called(temp_dir: Path) -> None:
    """Regression test: Ensure dropna is never called internally (volume=0 preserved)."""
    txt_path = temp_dir / "test_no_dropna.txt"
    txt_content = """Date,Time,Open,High,Low,Close,TotalVolume
2013/1/1,09:30:00,100.0,105.0,99.0,104.0,0
2013/1/1,10:00:00,104.0,106.0,103.0,105.0,0
2013/1/2,09:30:00,105.0,107.0,104.0,106.0,0
"""
    txt_path.write_text(txt_content, encoding="utf-8")
    
    result = ingest_raw_txt(txt_path)
    
    # If dropna was called on volume, rows with volume=0 might be dropped
    # But we expect all 3 rows preserved
    assert len(result.df) == 3
    
    # All should have volume=0
    assert all(result.df["volume"] == 0)



--------------------------------------------------------------------------------

FILE tests/test_data_layout.py
sha256(source_bytes) = 6294a2a7e2807ae01cbf4e991229f882ca37a5d8fb4ac9de95f807a7ccea4943
bytes = 641
redacted = False
--------------------------------------------------------------------------------

import numpy as np
import pytest
from FishBroWFS_V2.data.layout import normalize_bars


def test_normalize_bars_dtype_and_contiguous():
    o = np.arange(10, dtype=np.float32)[::2]
    h = o + 1
    l = o - 1
    c = o + 0.5

    bars = normalize_bars(o, h, l, c)

    for arr in (bars.open, bars.high, bars.low, bars.close):
        assert arr.dtype == np.float64
        assert arr.flags["C_CONTIGUOUS"]


def test_normalize_bars_reject_nan():
    o = np.array([1.0, np.nan])
    h = np.array([1.0, 2.0])
    l = np.array([0.5, 1.5])
    c = np.array([0.8, 1.8])

    with pytest.raises(ValueError):
        normalize_bars(o, h, l, c)




--------------------------------------------------------------------------------

FILE tests/test_day_bar_definition.py
sha256(source_bytes) = 71e6084ad18d2376271005393eaf8e6dbb1cb2b03d02739121cb74036bb5aa98
bytes = 3747
redacted = False
--------------------------------------------------------------------------------

"""Test DAY bar definition: one complete session per bar."""

from __future__ import annotations

from pathlib import Path

import pandas as pd
import pytest

from FishBroWFS_V2.data.session.kbar import aggregate_kbar
from FishBroWFS_V2.data.session.loader import load_session_profile


@pytest.fixture
def mnq_profile() -> Path:
    """Load CME.MNQ session profile."""
    profile_path = Path(__file__).parent.parent / "src" / "FishBroWFS_V2" / "data" / "profiles" / "CME_MNQ_TPE_v1.yaml"
    return profile_path


def test_day_bar_one_session(mnq_profile: Path) -> None:
    """Test DAY bar = one complete DAY session."""
    profile = load_session_profile(mnq_profile)
    
    # Create bars for one complete DAY session
    df = pd.DataFrame({
        "ts_str": [
            "2013/1/1 08:45:00",  # DAY session start
            "2013/1/1 09:00:00",
            "2013/1/1 10:00:00",
            "2013/1/1 11:00:00",
            "2013/1/1 12:00:00",
            "2013/1/1 13:00:00",
            "2013/1/1 13:44:00",  # Last bar before session end
        ],
        "open": [100.0, 101.0, 102.0, 103.0, 104.0, 105.0, 106.0],
        "high": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5, 106.5],
        "low": [99.5, 100.5, 101.5, 102.5, 103.5, 104.5, 105.5],
        "close": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5, 106.5],
        "volume": [1000, 1100, 1200, 1300, 1400, 1500, 1600],
    })
    
    result = aggregate_kbar(df, "DAY", profile)
    
    # Should have exactly one DAY bar
    assert len(result) == 1, f"Should have 1 DAY bar, got {len(result)}"
    
    # Verify the bar contains all DAY session bars
    day_bar = result.iloc[0]
    assert day_bar["open"] == 100.0, "Open should be first bar's open"
    assert day_bar["high"] == 106.5, "High should be max of all bars"
    assert day_bar["low"] == 99.5, "Low should be min of all bars"
    assert day_bar["close"] == 106.5, "Close should be last bar's close"
    assert day_bar["volume"] == sum([1000, 1100, 1200, 1300, 1400, 1500, 1600]), "Volume should be sum"
    
    # Verify ts_str is anchored to session start
    ts_str = day_bar["ts_str"]
    time_part = ts_str.split(" ")[1]
    assert time_part == "08:45:00", f"DAY bar should be anchored to session start, got {time_part}"


def test_day_bar_multiple_sessions(mnq_profile: Path) -> None:
    """Test DAY bars for multiple sessions."""
    profile = load_session_profile(mnq_profile)
    
    # Create bars for DAY and NIGHT sessions on same day
    df = pd.DataFrame({
        "ts_str": [
            # DAY session
            "2013/1/1 08:45:00",
            "2013/1/1 10:00:00",
            "2013/1/1 13:00:00",
            # NIGHT session
            "2013/1/1 21:00:00",
            "2013/1/1 23:00:00",
            "2013/1/2 02:00:00",
        ],
        "open": [100.0, 101.0, 102.0, 103.0, 104.0, 105.0],
        "high": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5],
        "low": [99.5, 100.5, 101.5, 102.5, 103.5, 104.5],
        "close": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5],
        "volume": [1000, 1100, 1200, 1300, 1400, 1500],
    })
    
    result = aggregate_kbar(df, "DAY", profile)
    
    # Should have 2 DAY bars (one for DAY session, one for NIGHT session)
    assert len(result) == 2, f"Should have 2 DAY bars (DAY + NIGHT), got {len(result)}"
    
    # Verify DAY session bar
    day_bar = result[result["ts_str"].str.contains("2013/1/1 08:45:00")].iloc[0]
    assert day_bar["volume"] == 1000 + 1100 + 1200, "DAY bar volume should sum DAY session bars"
    
    # Verify NIGHT session bar
    night_bar = result[result["ts_str"].str.contains("2013/1/1 21:00:00")].iloc[0]
    assert night_bar["volume"] == 1300 + 1400 + 1500, "NIGHT bar volume should sum NIGHT session bars"



--------------------------------------------------------------------------------

FILE tests/test_dtype_compression_contract.py
sha256(source_bytes) = 51e0b87167e50adb191a0a1d81da056adf509ad8fa54b7953d9fe3699e7f260d
bytes = 16341
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for dtype compression (Phase P1).

These tests ensure:
1. INDEX_DTYPE=int32 safety: order_id, created_bar, qty never exceed 2^31-1
2. UINT8 enum consistency: role/kind/side correctly encode/decode without sentinel issues
"""

import numpy as np
import pytest

from FishBroWFS_V2.config.dtypes import (
    INDEX_DTYPE,
    INTENT_ENUM_DTYPE,
    INTENT_PRICE_DTYPE,
)
from FishBroWFS_V2.engine.constants import (
    KIND_LIMIT,
    KIND_STOP,
    ROLE_ENTRY,
    ROLE_EXIT,
    SIDE_BUY,
    SIDE_SELL,
)
from FishBroWFS_V2.engine.engine_jit import (
    SIDE_BUY_CODE,
    SIDE_SELL_CODE,
    _pack_intents,
    simulate_arrays,
)
from FishBroWFS_V2.engine.types import BarArrays, OrderIntent, OrderKind, OrderRole, Side


class TestIndexDtypeSafety:
    """Test that INDEX_DTYPE=int32 is safe for all use cases."""

    def test_order_id_max_value_contract(self):
        """
        Contract: order_id must never exceed 2^31-1 (int32 max).
        
        In strategy/kernel.py, order_id is generated as:
        - Entry: np.arange(1, n_entry + 1)
        - Exit: np.arange(n_entry + 1, n_entry + 1 + exit_intents_count)
        
        Maximum order_id = n_entry + exit_intents_count
        
        For 200,000 bars with reasonable intent generation, this should be << 2^31-1.
        """
        INT32_MAX = 2**31 - 1
        
        # Simulate worst-case scenario: 200,000 bars, each bar generates 1 entry + 1 exit
        # This is extremely conservative (realistic scenarios generate far fewer intents)
        n_bars = 200_000
        max_intents_per_bar = 2  # 1 entry + 1 exit per bar (worst case)
        max_total_intents = n_bars * max_intents_per_bar
        
        # Maximum order_id would be max_total_intents (if all are sequential)
        max_order_id = max_total_intents
        
        assert max_order_id < INT32_MAX, (
            f"order_id would exceed int32 max ({INT32_MAX}) "
            f"with {n_bars} bars and {max_intents_per_bar} intents per bar. "
            f"Max order_id would be {max_order_id}"
        )
        
        # More realistic: check that even with 10x safety margin, we're still safe
        safety_margin = 10
        assert max_order_id * safety_margin < INT32_MAX, (
            f"order_id with {safety_margin}x safety margin would exceed int32 max"
        )

    def test_created_bar_max_value_contract(self):
        """
        Contract: created_bar must never exceed 2^31-1.
        
        created_bar is a bar index, so max value = n_bars - 1.
        For 200,000 bars, max created_bar = 199,999 << 2^31-1.
        """
        INT32_MAX = 2**31 - 1
        
        # Worst case: 200,000 bars
        max_bars = 200_000
        max_created_bar = max_bars - 1
        
        assert max_created_bar < INT32_MAX, (
            f"created_bar would exceed int32 max ({INT32_MAX}) "
            f"with {max_bars} bars. Max created_bar would be {max_created_bar}"
        )

    def test_qty_max_value_contract(self):
        """
        Contract: qty must never exceed 2^31-1.
        
        qty is typically small (1, 10, 100, etc.), so this should be safe.
        """
        INT32_MAX = 2**31 - 1
        
        # Realistic qty values are much smaller than int32 max
        realistic_max_qty = 1_000_000  # Even 1M shares is << 2^31-1
        
        assert realistic_max_qty < INT32_MAX, (
            f"qty would exceed int32 max ({INT32_MAX}) "
            f"with realistic max qty of {realistic_max_qty}"
        )

    def test_order_id_generation_in_kernel(self):
        """
        Test that actual order_id generation in kernel stays within int32 range.
        
        This test simulates the order_id generation logic from strategy/kernel.py.
        """
        INT32_MAX = 2**31 - 1
        
        # Simulate realistic scenario: 200,000 bars, ~1000 entry intents, ~500 exit intents
        n_entry = 1000
        n_exit = 500
        
        # Entry order_ids: np.arange(1, n_entry + 1)
        entry_order_ids = np.arange(1, n_entry + 1, dtype=INDEX_DTYPE)
        assert entry_order_ids.max() < INT32_MAX
        
        # Exit order_ids: np.arange(n_entry + 1, n_entry + 1 + n_exit)
        exit_order_ids = np.arange(n_entry + 1, n_entry + 1 + n_exit, dtype=INDEX_DTYPE)
        max_order_id = exit_order_ids.max()
        
        assert max_order_id < INT32_MAX, (
            f"Generated order_id {max_order_id} exceeds int32 max ({INT32_MAX})"
        )


class TestUint8EnumConsistency:
    """Test that uint8 enum encoding/decoding is consistent and safe."""

    def test_role_enum_encoding(self):
        """Test that role enum values encode correctly as uint8."""
        # ROLE_EXIT = 0, ROLE_ENTRY = 1
        exit_val = INTENT_ENUM_DTYPE(ROLE_EXIT)
        entry_val = INTENT_ENUM_DTYPE(ROLE_ENTRY)
        
        assert exit_val == 0
        assert entry_val == 1
        assert exit_val.dtype == np.uint8
        assert entry_val.dtype == np.uint8

    def test_kind_enum_encoding(self):
        """Test that kind enum values encode correctly as uint8."""
        # KIND_STOP = 0, KIND_LIMIT = 1
        stop_val = INTENT_ENUM_DTYPE(KIND_STOP)
        limit_val = INTENT_ENUM_DTYPE(KIND_LIMIT)
        
        assert stop_val == 0
        assert limit_val == 1
        assert stop_val.dtype == np.uint8
        assert limit_val.dtype == np.uint8

    def test_side_enum_encoding(self):
        """
        Test that side enum values encode correctly as uint8.
        
        SIDE_BUY_CODE = 1, SIDE_SELL_CODE = 255 (avoid -1 cast deprecation)
        """
        buy_val = INTENT_ENUM_DTYPE(SIDE_BUY_CODE)
        sell_val = INTENT_ENUM_DTYPE(SIDE_SELL_CODE)
        
        assert buy_val == 1
        assert sell_val == 255
        assert buy_val.dtype == np.uint8
        assert sell_val.dtype == np.uint8

    def test_side_enum_decoding_consistency(self):
        """
        Test that side enum decoding correctly handles uint8 values.
        
        Critical: uint8 value 255 (SIDE_SELL_CODE) must decode back to SELL.
        """
        # Encode SIDE_SELL_CODE (255) as uint8
        sell_encoded = INTENT_ENUM_DTYPE(SIDE_SELL_CODE)
        assert sell_encoded == 255
        
        # Decode: int(sd[i]) == SIDE_BUY (1) ? BUY : SELL
        # If sd[i] = 255, int(255) != 1, so it should decode to SELL
        decoded_is_buy = int(sell_encoded) == SIDE_BUY
        decoded_is_sell = int(sell_encoded) != SIDE_BUY
        
        assert not decoded_is_buy, "uint8 value 255 should not decode to BUY"
        assert decoded_is_sell, "uint8 value 255 should decode to SELL"
        
        # Also test BUY encoding/decoding
        buy_encoded = INTENT_ENUM_DTYPE(SIDE_BUY_CODE)
        assert buy_encoded == 1
        decoded_is_buy = int(buy_encoded) == SIDE_BUY_CODE
        assert decoded_is_buy, "uint8 value 1 should decode to BUY"

    def test_allowed_enum_values_contract(self):
        """
        Contract: enum arrays must only contain explicitly allowed values.
        
        This test ensures that:
        1. Only valid enum values are used (no uninitialized/invalid values)
        2. Decoding functions will raise ValueError for invalid values (strict mode)
        
        Allowed values:
        - role: {0 (EXIT), 1 (ENTRY)}
        - kind: {0 (STOP), 1 (LIMIT)}
        - side: {1 (BUY), 255 (SELL as uint8)}
        """
        # Define allowed values explicitly
        ALLOWED_ROLE_VALUES = {ROLE_EXIT, ROLE_ENTRY}  # {0, 1}
        ALLOWED_KIND_VALUES = {KIND_STOP, KIND_LIMIT}  # {0, 1}
        ALLOWED_SIDE_VALUES = {SIDE_BUY_CODE, SIDE_SELL_CODE}  # {1, 255} - avoid -1 cast
        
        # Test that encoding produces only allowed values
        role_encoded = [INTENT_ENUM_DTYPE(ROLE_EXIT), INTENT_ENUM_DTYPE(ROLE_ENTRY)]
        kind_encoded = [INTENT_ENUM_DTYPE(KIND_STOP), INTENT_ENUM_DTYPE(KIND_LIMIT)]
        side_encoded = [INTENT_ENUM_DTYPE(SIDE_BUY_CODE), INTENT_ENUM_DTYPE(SIDE_SELL_CODE)]
        
        for val in role_encoded:
            assert int(val) in ALLOWED_ROLE_VALUES, f"Role value {val} not in allowed set {ALLOWED_ROLE_VALUES}"
        
        for val in kind_encoded:
            assert int(val) in ALLOWED_KIND_VALUES, f"Kind value {val} not in allowed set {ALLOWED_KIND_VALUES}"
        
        for val in side_encoded:
            assert int(val) in ALLOWED_SIDE_VALUES, f"Side value {val} not in allowed set {ALLOWED_SIDE_VALUES}"
        
        # Test that invalid values raise ValueError (strict decoding)
        from FishBroWFS_V2.engine.engine_jit import _role_from_int, _kind_from_int, _side_from_int
        
        # Test invalid role values
        with pytest.raises(ValueError, match="Invalid role enum value"):
            _role_from_int(2)
        with pytest.raises(ValueError, match="Invalid role enum value"):
            _role_from_int(-1)
        
        # Test invalid kind values
        with pytest.raises(ValueError, match="Invalid kind enum value"):
            _kind_from_int(2)
        with pytest.raises(ValueError, match="Invalid kind enum value"):
            _kind_from_int(-1)
        
        # Test invalid side values
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(0)
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(2)
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(100)
        
        # Test valid values don't raise
        assert _role_from_int(0) == OrderRole.EXIT
        assert _role_from_int(1) == OrderRole.ENTRY
        assert _kind_from_int(0) == OrderKind.STOP
        assert _kind_from_int(1) == OrderKind.LIMIT
        assert _side_from_int(SIDE_BUY_CODE) == Side.BUY
        assert _side_from_int(SIDE_SELL_CODE) == Side.SELL

    def test_pack_intents_roundtrip(self):
        """
        Test that packing intents and decoding them preserves enum values correctly.
        
        This is an integration test to ensure the full encode/decode cycle works.
        """
        # Create test intents with all enum combinations
        intents = [
            OrderIntent(
                order_id=1,
                created_bar=0,
                role=OrderRole.EXIT,
                kind=OrderKind.STOP,
                side=Side.SELL,  # -1 -> uint8(255)
                price=100.0,
                qty=1,
            ),
            OrderIntent(
                order_id=2,
                created_bar=0,
                role=OrderRole.ENTRY,
                kind=OrderKind.LIMIT,
                side=Side.BUY,  # 1 -> uint8(1)
                price=101.0,
                qty=1,
            ),
        ]
        
        # Pack intents
        order_id, created_bar, role, kind, side, price, qty = _pack_intents(intents)
        
        # Verify dtypes
        assert order_id.dtype == INDEX_DTYPE
        assert created_bar.dtype == INDEX_DTYPE
        assert role.dtype == INTENT_ENUM_DTYPE
        assert kind.dtype == INTENT_ENUM_DTYPE
        assert side.dtype == INTENT_ENUM_DTYPE
        assert price.dtype == INTENT_PRICE_DTYPE
        assert qty.dtype == INDEX_DTYPE
        
        # Verify enum values
        assert role[0] == ROLE_EXIT  # 0
        assert role[1] == ROLE_ENTRY  # 1
        assert kind[0] == KIND_STOP  # 0
        assert kind[1] == KIND_LIMIT  # 1
        assert side[0] == SIDE_SELL_CODE  # SELL -> uint8(255)
        assert side[1] == SIDE_BUY_CODE  # BUY -> uint8(1)
        
        # Verify decoding logic (as used in engine_jit.py)
        # Decode role
        decoded_role_0 = OrderRole.EXIT if int(role[0]) == ROLE_EXIT else OrderRole.ENTRY
        assert decoded_role_0 == OrderRole.EXIT
        
        decoded_role_1 = OrderRole.EXIT if int(role[1]) == ROLE_EXIT else OrderRole.ENTRY
        assert decoded_role_1 == OrderRole.ENTRY
        
        # Decode kind
        decoded_kind_0 = OrderKind.STOP if int(kind[0]) == KIND_STOP else OrderKind.LIMIT
        assert decoded_kind_0 == OrderKind.STOP
        
        decoded_kind_1 = OrderKind.STOP if int(kind[1]) == KIND_STOP else OrderKind.LIMIT
        assert decoded_kind_1 == OrderKind.LIMIT
        
        # Decode side (critical: uint8(255) must decode to SELL)
        decoded_side_0 = Side.BUY if int(side[0]) == SIDE_BUY_CODE else Side.SELL
        assert decoded_side_0 == Side.SELL, f"uint8(255) should decode to SELL, got {decoded_side_0}"
        
        decoded_side_1 = Side.BUY if int(side[1]) == SIDE_BUY_CODE else Side.SELL
        assert decoded_side_1 == Side.BUY, f"uint8(1) should decode to BUY, got {decoded_side_1}"

    def test_simulate_arrays_accepts_uint8_enums(self):
        """
        Test that simulate_arrays correctly accepts and processes uint8 enum arrays.
        
        This ensures the numba kernel can handle uint8 enum values correctly.
        """
        # Create minimal test data
        bars = BarArrays(
            open=np.array([100.0, 101.0], dtype=np.float64),
            high=np.array([102.0, 103.0], dtype=np.float64),
            low=np.array([99.0, 100.0], dtype=np.float64),
            close=np.array([101.0, 102.0], dtype=np.float64),
        )
        
        # Create intent arrays with uint8 enums
        order_id = np.array([1], dtype=INDEX_DTYPE)
        created_bar = np.array([0], dtype=INDEX_DTYPE)
        role = np.array([ROLE_ENTRY], dtype=INTENT_ENUM_DTYPE)
        kind = np.array([KIND_STOP], dtype=INTENT_ENUM_DTYPE)
        side = np.array([SIDE_BUY_CODE], dtype=INTENT_ENUM_DTYPE)  # 1 -> uint8(1)
        price = np.array([102.0], dtype=INTENT_PRICE_DTYPE)
        qty = np.array([1], dtype=INDEX_DTYPE)
        
        # This should not raise any dtype-related errors
        fills = simulate_arrays(
            bars,
            order_id=order_id,
            created_bar=created_bar,
            role=role,
            kind=kind,
            side=side,
            price=price,
            qty=qty,
            ttl_bars=1,
        )
        
        # Verify fills were generated (basic sanity check)
        assert isinstance(fills, list)
        
        # Test with SELL side (uint8 value 255)
        side_sell = np.array([SIDE_SELL_CODE], dtype=INTENT_ENUM_DTYPE)  # 255 (avoid -1 cast)
        fills_sell = simulate_arrays(
            bars,
            order_id=order_id,
            created_bar=created_bar,
            role=role,
            kind=kind,
            side=side_sell,
            price=price,
            qty=qty,
            ttl_bars=1,
        )
        
        # Should not raise errors
        assert isinstance(fills_sell, list)
        
        # Verify that fills with SELL side decode correctly
        # Note: numba kernel outputs uint8(255) as 255.0, but _side_from_int correctly decodes it
        if fills_sell:
            # The fill's side should be Side.SELL
            assert fills_sell[0].side == Side.SELL, (
                f"Fill with uint8(255) side should decode to Side.SELL, got {fills_sell[0].side}"
            )

    def test_side_output_value_contract(self):
        """
        Contract: numba kernel outputs side as float.
        
        Note: uint8(255) from SIDE_SELL will output as 255.0, not -1.0.
        This is acceptable as long as _side_from_int correctly decodes it.
        
        With strict mode, invalid values will raise ValueError instead of silently
        decoding to SELL.
        """
        from FishBroWFS_V2.engine.engine_jit import _side_from_int
        
        # Test that _side_from_int correctly handles allowed values
        assert _side_from_int(SIDE_BUY_CODE) == Side.BUY
        assert _side_from_int(SIDE_SELL_CODE) == Side.SELL, (
            f"_side_from_int({SIDE_SELL_CODE}) should decode to Side.SELL, not BUY"
        )
        
        # Test that invalid values raise ValueError (strict mode)
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(0)
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(-1)
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(2)
        with pytest.raises(ValueError, match="Invalid side enum value"):
            _side_from_int(100)



--------------------------------------------------------------------------------

FILE tests/test_engine_constitution.py
sha256(source_bytes) = 558a0418da58b3359417f04e0934458c0dc252049e0edd23f68c60d957184372
bytes = 3459
redacted = False
--------------------------------------------------------------------------------

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.matcher_core import simulate
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def _bars1(o, h, l, c):
    return normalize_bars(
        np.array([o], dtype=np.float64),
        np.array([h], dtype=np.float64),
        np.array([l], dtype=np.float64),
        np.array([c], dtype=np.float64),
    )


def _bars2(o0, h0, l0, c0, o1, h1, l1, c1):
    return normalize_bars(
        np.array([o0, o1], dtype=np.float64),
        np.array([h0, h1], dtype=np.float64),
        np.array([l0, l1], dtype=np.float64),
        np.array([c0, c1], dtype=np.float64),
    )


def test_tc01_buy_stop_normal():
    bars = _bars1(90, 105, 90, 100)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 100.0


def test_tc02_buy_stop_gap_up_fill_open():
    bars = _bars1(105, 110, 105, 108)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 105.0


def test_tc03_sell_stop_gap_down_fill_open():
    bars = _bars1(90, 95, 80, 85)
    intents = [
        # Exit a long position requires SELL stop; we will enter long first in same bar is not allowed here,
        # so we simulate already-in-position by forcing an entry earlier: created_bar=-2 triggers at -1 (ignored),
        # Instead: use two bars and enter on bar0, exit on bar1.
    ]
    bars2 = _bars2(
        100, 100, 100, 100,   # bar0: enter long at 100 (buy stop gap/normal both ok)
        90, 95, 80, 85        # bar1: exit stop triggers gap down open
    )
    intents2 = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=100.0),
    ]
    fills = simulate(bars2, intents2)
    assert len(fills) == 2
    # second fill is the exit
    assert fills[1].price == 90.0


def test_tc08_next_bar_active_not_same_bar():
    # bar0 has high 105 which would hit stop 102, but order created at bar0 must not fill at bar0.
    # bar1 hits again, should fill at bar1.
    bars = _bars2(
        100, 105, 95, 100,
        100, 105, 95, 100,
    )
    intents = [
        OrderIntent(order_id=1, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].bar_index == 1
    assert fills[0].price == 102.0


def test_tc09_open_equals_stop_gap_branch_but_same_price():
    bars = _bars1(100, 100, 90, 95)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 100.0


def test_tc10_no_fill_when_not_touched():
    bars = _bars1(90, 95, 90, 92)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert fills == []




--------------------------------------------------------------------------------

FILE tests/test_engine_fill_buffer_capacity.py
sha256(source_bytes) = b16aacd89a6c84199d2ad2a2a936d79bfda771e19c0578b0cec0302b5e448475
bytes = 2446
redacted = False
--------------------------------------------------------------------------------

"""Test that engine fill buffer handles extreme intents without crashing."""

from __future__ import annotations

import numpy as np
import pytest

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.engine_jit import STATUS_BUFFER_FULL, STATUS_OK, simulate as simulate_jit
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def test_engine_fill_buffer_capacity_extreme_intents() -> None:
    """
    Test that engine handles extreme intents (many intents, few bars) without crashing.
    
    Scenario: bars=10, intents=500
    Each intent is designed to fill (STOP BUY that triggers immediately).
    """
    n_bars = 10
    n_intents = 500

    # Create bars with high volatility to ensure fills
    bars = normalize_bars(
        np.array([100.0] * n_bars, dtype=np.float64),
        np.array([120.0] * n_bars, dtype=np.float64),
        np.array([80.0] * n_bars, dtype=np.float64),
        np.array([110.0] * n_bars, dtype=np.float64),
    )

    # Create many intents that will all fill (STOP BUY at 105, which is below high=120)
    # Distribute across bars to maximize fills
    intents = []
    for i in range(n_intents):
        created_bar = (i % n_bars) - 1  # Distribute across bars
        intents.append(
            OrderIntent(
                order_id=i,
                created_bar=created_bar,
                role=OrderRole.ENTRY,
                kind=OrderKind.STOP,
                side=Side.BUY,
                price=105.0,  # Will trigger on any bar (high=120 > 105)
                qty=1,
            )
        )

    # Should not crash or segfault
    try:
        fills = simulate_jit(bars, intents)
        # If we get here, no segfault occurred
        
        # Fills should be bounded by n_intents (each intent can produce at most 1 fill)
        assert len(fills) <= n_intents, f"fills ({len(fills)}) should not exceed n_intents ({n_intents})"
        
        # Should have some fills (most intents should trigger)
        assert len(fills) > 0, "Should have at least some fills"
        
    except RuntimeError as e:
        # If buffer is full, error message should be graceful (not segfault)
        error_msg = str(e)
        assert "buffer full" in error_msg.lower() or "buffer_full" in error_msg.lower(), (
            f"Expected buffer full error, got: {error_msg}"
        )
        # This is acceptable - buffer protection worked correctly



--------------------------------------------------------------------------------

FILE tests/test_engine_gaps_and_priority.py
sha256(source_bytes) = 7085a766fa19c133dfe62251db24fe8300f77deaf7e81de7d5289919230e9b37
bytes = 2900
redacted = False
--------------------------------------------------------------------------------

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.matcher_core import simulate
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def _bars1(o, h, l, c):
    return normalize_bars(
        np.array([o], dtype=np.float64),
        np.array([h], dtype=np.float64),
        np.array([l], dtype=np.float64),
        np.array([c], dtype=np.float64),
    )


def test_tc04_buy_limit_gap_down_better_fill_open():
    bars = _bars1(90, 95, 85, 92)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.BUY, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 90.0


def test_tc05_sell_limit_gap_up_better_fill_open():
    bars = _bars1(105, 110, 100, 108)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.SELL, price=100.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 1
    assert fills[0].price == 105.0


def test_tc06_priority_stop_wins_over_limit_on_exit():
    # First enter long on this same bar, then exit on next bar where both stop and limit are triggerable.
    # Bar0: enter long at 100 (buy stop hits)
    # Bar1: both exit stop 90 and exit limit 110 are touchable (high=110, low=80), STOP must win (fill=90)
    bars = normalize_bars(
        np.array([100, 100], dtype=np.float64),
        np.array([110, 110], dtype=np.float64),
        np.array([90, 80], dtype=np.float64),
        np.array([100, 90], dtype=np.float64),
    )

    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=90.0),
        OrderIntent(order_id=3, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.LIMIT, side=Side.SELL, price=110.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 2
    # Second fill is exit; STOP wins -> 90
    assert fills[1].kind == OrderKind.STOP
    assert fills[1].price == 90.0


def test_tc07_same_bar_entry_then_exit():
    # Same bar allows Entry then Exit.
    # Bar: O=100 H=120 L=90 C=110
    # Entry: Buy Stop 105 -> fills at 105 (since open 100 < 105 and high 120 >= 105)
    # Exit: Sell Stop 95 -> after entry, low 90 <= 95 -> fills at 95
    bars = _bars1(100, 120, 90, 110)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),
        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),
    ]
    fills = simulate(bars, intents)
    assert len(fills) == 2
    assert fills[0].price == 105.0
    assert fills[1].price == 95.0




--------------------------------------------------------------------------------

FILE tests/test_engine_jit_active_book_contract.py
sha256(source_bytes) = d35dcb498636c6c25ef00e4efc5f31e9b326a667cdecff9a2f9208d277960f56
bytes = 8869
redacted = False
--------------------------------------------------------------------------------

from __future__ import annotations

import os

import numpy as np
import pytest

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.engine_jit import _simulate_with_ttl, simulate as simulate_jit
from FishBroWFS_V2.engine.matcher_core import simulate as simulate_py
from FishBroWFS_V2.engine.types import Fill, OrderIntent, OrderKind, OrderRole, Side


def _assert_fills_equal(a: list[Fill], b: list[Fill]) -> None:
    assert len(a) == len(b)
    for fa, fb in zip(a, b):
        assert fa.bar_index == fb.bar_index
        assert fa.role == fb.role
        assert fa.kind == fb.kind
        assert fa.side == fb.side
        assert fa.qty == fb.qty
        assert fa.order_id == fb.order_id
        assert abs(fa.price - fb.price) <= 1e-9


def test_jit_sorted_invariance_matches_python() -> None:
    # Bars: 3 bars, deterministic highs/lows for STOP triggers
    bars = normalize_bars(
        np.array([100.0, 100.0, 100.0], dtype=np.float64),
        np.array([110.0, 110.0, 110.0], dtype=np.float64),
        np.array([90.0, 90.0, 90.0], dtype=np.float64),
        np.array([100.0, 100.0, 100.0], dtype=np.float64),
    )

    # Intents across multiple activate bars (created_bar = t-1)
    intents = [
        # activate on bar0 (created -1)
        OrderIntent(3, -1, OrderRole.EXIT, OrderKind.STOP, Side.SELL, 95.0, 1),
        OrderIntent(2, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 105.0, 1),
        # activate on bar1 (created 0)
        OrderIntent(6, 0, OrderRole.EXIT, OrderKind.LIMIT, Side.SELL, 110.0, 1),
        OrderIntent(5, 0, OrderRole.ENTRY, OrderKind.LIMIT, Side.BUY, 99.0, 1),
        # activate on bar2 (created 1)
        OrderIntent(9, 1, OrderRole.EXIT, OrderKind.STOP, Side.SELL, 90.0, 1),
        OrderIntent(8, 1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),
    ]

    shuffled = list(intents)
    rng = np.random.default_rng(123)
    rng.shuffle(shuffled)

    # JIT simulate sorts internally for cursor+book; it must be invariant to input ordering.
    jit_a = simulate_jit(bars, shuffled)
    jit_b = simulate_jit(bars, intents)
    _assert_fills_equal(jit_a, jit_b)

    # Also must match Python reference semantics.
    py = simulate_py(bars, shuffled)
    _assert_fills_equal(jit_a, py)


def test_one_bar_max_one_entry_one_exit_defense() -> None:
    # Single bar is enough: created_bar=-1 activates on bar 0.
    bars = normalize_bars(
        np.array([100.0], dtype=np.float64),
        np.array([120.0], dtype=np.float64),
        np.array([80.0], dtype=np.float64),
        np.array([110.0], dtype=np.float64),
    )

    # Same activate bar contains Entry1, Exit1, Entry2.
    intents = [
        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 105.0, 1),
        OrderIntent(2, -1, OrderRole.EXIT, OrderKind.STOP, Side.SELL, 95.0, 1),
        OrderIntent(3, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 110.0, 1),
    ]

    fills = simulate_jit(bars, intents)
    assert len(fills) == 2
    assert fills[0].order_id == 1
    assert fills[1].order_id == 2


def test_ttl_one_shot_vs_gtc_extension_point() -> None:
    # Skip if JIT is disabled; ttl=0 is a JIT-only extension behavior.
    import FishBroWFS_V2.engine.engine_jit as ej

    if ej.nb is None or os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        pytest.skip("numba not available or disabled; ttl=0 extension tested only under JIT")

    # Bar0: stop not touched, Bar1: stop touched
    bars = normalize_bars(
        np.array([90.0, 90.0], dtype=np.float64),
        np.array([99.0, 110.0], dtype=np.float64),
        np.array([90.0, 90.0], dtype=np.float64),
        np.array([95.0, 100.0], dtype=np.float64),
    )
    intents = [
        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),
    ]

    # ttl=1 (default semantics): active only on bar0 -> no fill
    fills_ttl1 = simulate_jit(bars, intents)
    assert fills_ttl1 == []

    # ttl=0 (GTC extension): order stays in book and can fill on bar1
    fills_gtc = _simulate_with_ttl(bars, intents, ttl_bars=0)
    assert len(fills_gtc) == 1
    assert fills_gtc[0].bar_index == 1
    assert abs(fills_gtc[0].price - 100.0) <= 1e-9


def test_ttl_one_expires_before_fill_opportunity() -> None:
    """
    Case A: ttl=1 is one-shot next-bar-only (does not fill if not triggered on activate bar).
    
    Scenario:
      - BUY STOP order, created_bar=-1 (activates at bar0)
      - bar0: high < stop (not triggered)
      - bar1: high >= stop (would trigger, but order expired)
      - ttl_bars=1: order should expire after bar0, not fill on bar1
    """
    import FishBroWFS_V2.engine.engine_jit as ej

    if ej.nb is None or os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        pytest.skip("numba not available or disabled; ttl semantics tested only under JIT")

    # 2 bars: bar0 doesn't trigger, bar1 would trigger
    bars = normalize_bars(
        np.array([90.0, 90.0], dtype=np.float64),  # open
        np.array([99.0, 110.0], dtype=np.float64),  # high: bar0 < 100, bar1 >= 100
        np.array([90.0, 90.0], dtype=np.float64),  # low
        np.array([95.0, 100.0], dtype=np.float64),  # close
    )
    intents = [
        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),
    ]

    # ttl_bars=1: activate_bar=0, expire_bar=0, so at bar1 (t=1) > expire_bar (0), order expired
    fills_ttl1 = _simulate_with_ttl(bars, intents, ttl_bars=1)
    assert len(fills_ttl1) == 0, "ttl=1 should expire after activate bar, no fill on bar1"

    # Verify JIT matches expected semantics
    # activate_bar = created_bar + 1 = -1 + 1 = 0
    # expire_bar = activate_bar + (ttl_bars - 1) = 0 + (1 - 1) = 0
    # At bar1 (t=1), t > expire_bar (0), so order should be removed before Step B/C


def test_ttl_zero_gtc_never_expires() -> None:
    """
    Case B: ttl=0 is GTC (Good Till Canceled), order never expires.
    
    Scenario:
      - BUY STOP order, created_bar=-1 (activates at bar0)
      - bar0: high < stop (not triggered)
      - bar1: high >= stop (triggers)
      - ttl_bars=0: order should remain active and fill on bar1
    """
    import FishBroWFS_V2.engine.engine_jit as ej

    if ej.nb is None or os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        pytest.skip("numba not available or disabled; ttl semantics tested only under JIT")

    # 2 bars: bar0 doesn't trigger, bar1 triggers
    bars = normalize_bars(
        np.array([90.0, 90.0], dtype=np.float64),  # open
        np.array([99.0, 110.0], dtype=np.float64),  # high: bar0 < 100, bar1 >= 100
        np.array([90.0, 90.0], dtype=np.float64),  # low
        np.array([95.0, 100.0], dtype=np.float64),  # close
    )
    intents = [
        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),
    ]

    # ttl_bars=0: GTC, order never expires, should fill on bar1
    fills_gtc = _simulate_with_ttl(bars, intents, ttl_bars=0)
    assert len(fills_gtc) == 1, "ttl=0 (GTC) should allow fill on bar1"
    assert fills_gtc[0].bar_index == 1, "Fill should occur on bar1"
    assert fills_gtc[0].order_id == 1
    assert abs(fills_gtc[0].price - 100.0) <= 1e-9, "Fill price should be stop price"


def test_ttl_semantics_three_bars() -> None:
    """
    Additional test: verify ttl=1 semantics with 3 bars to ensure expiration timing is correct.
    
    Scenario:
      - BUY STOP order, created_bar=-1 (activates at bar0)
      - bar0: high < stop (not triggered)
      - bar1: high < stop (not triggered)
      - bar2: high >= stop (would trigger, but order expired)
      - ttl_bars=1: order should expire after bar0, not fill on bar2
    """
    import FishBroWFS_V2.engine.engine_jit as ej

    if ej.nb is None or os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        pytest.skip("numba not available or disabled; ttl semantics tested only under JIT")

    # 3 bars: bar0 and bar1 don't trigger, bar2 would trigger
    bars = normalize_bars(
        np.array([90.0, 90.0, 90.0], dtype=np.float64),  # open
        np.array([99.0, 99.0, 110.0], dtype=np.float64),  # high: bar0,bar1 < 100, bar2 >= 100
        np.array([90.0, 90.0, 90.0], dtype=np.float64),  # low
        np.array([95.0, 95.0, 100.0], dtype=np.float64),  # close
    )
    intents = [
        OrderIntent(1, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 100.0, 1),
    ]

    # ttl_bars=1: activate_bar=0, expire_bar=0, so at bar1 (t=1) > expire_bar (0), order expired
    fills_ttl1 = _simulate_with_ttl(bars, intents, ttl_bars=1)
    assert len(fills_ttl1) == 0, "ttl=1 should expire after activate bar, no fill on bar2"

    # ttl_bars=0: GTC, should fill on bar2
    fills_gtc = _simulate_with_ttl(bars, intents, ttl_bars=0)
    assert len(fills_gtc) == 1, "ttl=0 (GTC) should allow fill on bar2"
    assert fills_gtc[0].bar_index == 2, "Fill should occur on bar2"





--------------------------------------------------------------------------------

FILE tests/test_engine_jit_fill_buffer_capacity.py
sha256(source_bytes) = 616a73f1caad6fdad950c6c6f74cb4467cb5afb3fcd4bb1b8eccda09b729631a
bytes = 5299
redacted = False
--------------------------------------------------------------------------------

"""Test that fill buffer scales with n_intents and does not segfault."""

from __future__ import annotations

import os

import numpy as np
import pytest

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.engine_jit import STATUS_BUFFER_FULL, simulate as simulate_jit
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def test_fill_buffer_scales_with_intents():
    """
    Test that buffer size accommodates n_intents > n_bars*2.
    
    Scenario: n_bars=10, n_intents=100
    Each intent is designed to fill (market entry with stop that triggers immediately).
    This tests that buffer scales with n_intents, not just n_bars*2.
    """
    n_bars = 10
    n_intents = 100
    
    # Create bars with high volatility to ensure fills
    bars = normalize_bars(
        np.array([100.0] * n_bars, dtype=np.float64),
        np.array([120.0] * n_bars, dtype=np.float64),
        np.array([80.0] * n_bars, dtype=np.float64),
        np.array([110.0] * n_bars, dtype=np.float64),
    )
    
    # Create many intents that will all fill (STOP BUY at 105, which is below high=120)
    # Each intent activates on a different bar to maximize fills
    intents = []
    for i in range(n_intents):
        created_bar = (i % n_bars) - 1  # Distribute across bars
        intents.append(
            OrderIntent(
                order_id=i,
                created_bar=created_bar,
                role=OrderRole.ENTRY,
                kind=OrderKind.STOP,
                side=Side.BUY,
                price=105.0,  # Will trigger on any bar (high=120 > 105)
                qty=1,
            )
        )
    
    # Should not crash or segfault
    try:
        fills = simulate_jit(bars, intents)
        # If we get here, no segfault occurred
        
        # Fills should be bounded by n_intents (each intent can produce at most 1 fill)
        assert len(fills) <= n_intents, f"fills ({len(fills)}) should not exceed n_intents ({n_intents})"
        
        # In this scenario, we expect many fills (most intents should trigger)
        # But exact count depends on bar distribution, so we just check it's reasonable
        assert len(fills) > 0, "Should have at least some fills"
        
    except RuntimeError as e:
        # If buffer is full, error message should be graceful (not segfault)
        error_msg = str(e)
        assert "buffer full" in error_msg.lower() or "buffer_full" in error_msg.lower(), (
            f"Expected buffer full error, got: {error_msg}"
        )
        # This is acceptable - buffer protection worked correctly


def test_fill_buffer_protection_prevents_segfault():
    """
    Test that buffer protection prevents segfault even with extreme intents.
    
    This test ensures STATUS_BUFFER_FULL is returned gracefully instead of segfaulting.
    """
    import FishBroWFS_V2.engine.engine_jit as ej
    
    # Skip if JIT is disabled (buffer protection is in JIT kernel)
    if ej.nb is None or os.environ.get("NUMBA_DISABLE_JIT", "").strip() == "1":
        pytest.skip("numba not available or disabled; buffer protection tested only under JIT")
    
    n_bars = 5
    n_intents = 1000  # Extreme: way more intents than bars
    
    bars = normalize_bars(
        np.array([100.0] * n_bars, dtype=np.float64),
        np.array([120.0] * n_bars, dtype=np.float64),
        np.array([80.0] * n_bars, dtype=np.float64),
        np.array([110.0] * n_bars, dtype=np.float64),
    )
    
    # Create intents that will all try to fill
    intents = []
    for i in range(n_intents):
        # All activate on bar 0 (created_bar=-1)
        intents.append(
            OrderIntent(
                order_id=i,
                created_bar=-1,
                role=OrderRole.ENTRY,
                kind=OrderKind.STOP,
                side=Side.BUY,
                price=105.0,  # Will trigger
                qty=1,
            )
        )
    
    # Should not segfault - either succeed or return graceful error
    try:
        fills = simulate_jit(bars, intents)
        # If successful, fills should be bounded
        assert len(fills) <= n_intents
        # With this many intents on one bar, we might hit buffer limit
        # But should not crash
    except RuntimeError as e:
        # Graceful error is acceptable
        assert "buffer" in str(e).lower() or "full" in str(e).lower(), (
            f"Expected buffer-related error, got: {e}"
        )


def test_fill_buffer_minimum_size():
    """
    Test that buffer is at least n_bars*2 (default heuristic).
    
    Even with few intents, buffer should accommodate reasonable fill rate.
    """
    n_bars = 20
    n_intents = 5  # Few intents
    
    bars = normalize_bars(
        np.array([100.0] * n_bars, dtype=np.float64),
        np.array([120.0] * n_bars, dtype=np.float64),
        np.array([80.0] * n_bars, dtype=np.float64),
        np.array([110.0] * n_bars, dtype=np.float64),
    )
    
    intents = [
        OrderIntent(i, -1, OrderRole.ENTRY, OrderKind.STOP, Side.BUY, 105.0, 1)
        for i in range(n_intents)
    ]
    
    # Should work fine (buffer should be at least n_bars*2 = 40, which is > n_intents=5)
    fills = simulate_jit(bars, intents)
    assert len(fills) <= n_intents
    # Should not crash



--------------------------------------------------------------------------------

FILE tests/test_entry_only_regression.py
sha256(source_bytes) = 77b0a1628ec2ff998aba61a8173e67ec7b70ff68003625542bd7ffc8e2bb1cd5
bytes = 7008
redacted = False
--------------------------------------------------------------------------------

"""
Regression test for entry-only fills scenario.

This test ensures that when entry fills occur but exit fills do not,
the metrics behavior is correct:
- trades=0 is valid (no completed round-trips)
- metrics may be all-zero or have non-zero values depending on implementation
- The system should not crash or produce invalid metrics
"""
from __future__ import annotations

import numpy as np
import os

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_entry_only_fills_metrics_behavior() -> None:
    """
    Test metrics behavior when only entry fills occur (no exit fills).
    
    Scenario:
    - Entry stop triggers at t=31 (high[31] crosses buy stop=high[30]=120)
    - Exit stop never triggers (all subsequent lows stay above exit stop)
    - Result: entry_fills_total > 0, exit_fills_total == 0, trades == 0
    """
    # Ensure clean environment
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    old_param_subsample_rate = os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
    old_param_subsample_seed = os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_SEED", None)
    
    try:
        # Set required environment variables
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = "1.0"
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_SEED"] = "42"
        
        n = 60
        
        # Construct OHLC as specified
        # Initial: all flat at 100.0
        close = np.full(n, 100.0, dtype=np.float64)
        open_ = close.copy()
        high = np.full(n, 100.5, dtype=np.float64)
        low = np.full(n, 99.5, dtype=np.float64)
        
        # At t=30: set high[30]=120.0 (forms Donchian high point)
        high[30] = 120.0
        
        # At t=31: set high[31]=121.0 and low[31]=110.0
        # This ensures next-bar buy stop=high[30]=120 will be triggered
        high[31] = 121.0
        low[31] = 110.0
        
        # t>=32: set low[t]=110.1, high[t]=111.0, close[t]=110.5
        # This ensures exit stop will never trigger (low stays above exit stop)
        for t in range(32, n):
            low[t] = 110.1  # Slightly above 110.0 to avoid triggering exit stop
            high[t] = 111.0
            close[t] = 110.5
            open_[t] = 110.5
        
        # Ensure OHLC consistency
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Single param: channel_len=20, atr_len=10, stop_mult=1.0
        params_matrix = np.array([[20, 10, 1.0]], dtype=np.float64)
        
        result = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
            force_close_last=False,  # Critical: do not force close
        )
        
        # Verify metrics shape
        metrics = result.get("metrics")
        assert metrics is not None, "metrics must exist"
        assert isinstance(metrics, np.ndarray), "metrics must be np.ndarray"
        assert metrics.shape == (1, 3), (
            f"metrics shape should be (1, 3), got {metrics.shape}"
        )
        
        # Verify perf dict
        perf = result.get("perf", {})
        assert isinstance(perf, dict), "perf must be a dict"
        
        # Extract perf fields for entry-only invariants
        fills_total = int(perf.get("fills_total", 0))
        entry_fills_total = int(perf.get("entry_fills_total", 0))
        exit_fills_total = int(perf.get("exit_fills_total", 0))
        entry_intents_total = int(perf.get("entry_intents_total", 0))
        exit_intents_total = int(perf.get("exit_intents_total", 0))
        
        # Assertions: lock semantics, not performance
        assert fills_total >= 1, (
            f"fills_total ({fills_total}) should be >= 1 (entry fill should occur)"
        )
        
        assert entry_fills_total >= 1, (
            f"entry_fills_total ({entry_fills_total}) should be >= 1"
        )
        
        assert exit_fills_total == 0, (
            f"exit_fills_total ({exit_fills_total}) should be 0 (exit stop should never trigger)"
        )
        
        # If exit intents exist, fine; but they must not fill.
        assert exit_intents_total >= 0, (
            f"exit_intents_total ({exit_intents_total}) should be >= 0"
        )
        
        assert entry_intents_total >= 1, (
            f"entry_intents_total ({entry_intents_total}) should be >= 1"
        )
        
        # Entry-only scenario: no exit fills => no completed trades.
        # Our metrics are trade-based, so metrics may legitimately remain all zeros.
        assert np.all(np.isfinite(metrics[0])), f"metrics[0] must be finite, got {metrics[0]}"
        
        # Verify trades and net_profit from result or perf (compatible with different return locations)
        trades = int(result.get("trades", perf.get("trades", 0)) or 0)
        net_profit = float(result.get("net_profit", perf.get("net_profit", 0.0)) or 0.0)
        
        assert trades == 0, f"entry-only must have trades==0, got {trades}"
        assert abs(net_profit) <= 1e-12, f"entry-only must have net_profit==0, got {net_profit}"
        
        # Verify metrics values match
        assert int(metrics[0, 1]) == 0, f"metrics[0, 1] (trades) must be 0, got {metrics[0, 1]}"
        assert abs(float(metrics[0, 0])) <= 1e-12, f"metrics[0, 0] (net_profit) must be 0, got {metrics[0, 0]}"
        assert abs(float(metrics[0, 2])) <= 1e-12, f"metrics[0, 2] (max_dd) must be 0, got {metrics[0, 2]}"
        
        # Evidence-chain sanity (optional but recommended)
        if "metrics_subset_abs_sum" in perf:
            assert float(perf["metrics_subset_abs_sum"]) >= 0.0
        if "metrics_subset_nonzero_rows" in perf:
            assert int(perf["metrics_subset_nonzero_rows"]) == 0
        
        # Optional: Check if position tracking exists (entry-only should end in open position)
        pos_last = perf.get("position_last", perf.get("pos_last", perf.get("last_position", None)))
        if pos_last is not None:
            assert int(pos_last) != 0, f"entry-only should end in open position, got {pos_last}"
        
    finally:
        # Restore environment
        if old_trigger_rate is None:
            os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
        else:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate
        
        if old_param_subsample_rate is None:
            os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
        else:
            os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = old_param_subsample_rate
        
        if old_param_subsample_seed is None:
            os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_SEED", None)
        else:
            os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_SEED"] = old_param_subsample_seed



--------------------------------------------------------------------------------

FILE tests/test_funnel_contract.py
sha256(source_bytes) = f805ab99624990a817379096d04fb5b941217557cbbebe3075d939c08fa49e84
bytes = 12301
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for funnel pipeline.

Tests verify:
1. Funnel plan has three stages
2. Stage2 subsample is 1.0
3. Each stage creates artifacts
4. param_subsample_rate visibility
5. params_effective calculation consistency
6. Funnel result index structure
"""

from __future__ import annotations

import tempfile
from pathlib import Path

import numpy as np
import pytest

from FishBroWFS_V2.core.audit_schema import compute_params_effective
from FishBroWFS_V2.pipeline.funnel_plan import build_default_funnel_plan
from FishBroWFS_V2.pipeline.funnel_runner import run_funnel
from FishBroWFS_V2.pipeline.funnel_schema import StageName


def test_funnel_build_default_plan_has_three_stages():
    """Test that default funnel plan has exactly three stages."""
    cfg = {
        "param_subsample_rate": 0.1,
        "topk_stage0": 50,
        "topk_stage1": 20,
    }
    
    plan = build_default_funnel_plan(cfg)
    
    assert len(plan.stages) == 3
    
    # Verify stage names
    assert plan.stages[0].name == StageName.STAGE0_COARSE
    assert plan.stages[1].name == StageName.STAGE1_TOPK
    assert plan.stages[2].name == StageName.STAGE2_CONFIRM


def test_stage2_subsample_is_one():
    """Test that Stage2 subsample rate is always 1.0."""
    test_cases = [
        {"param_subsample_rate": 0.1},
        {"param_subsample_rate": 0.5},
        {"param_subsample_rate": 0.9},
    ]
    
    for cfg in test_cases:
        plan = build_default_funnel_plan(cfg)
        stage2 = plan.stages[2]
        
        assert stage2.name == StageName.STAGE2_CONFIRM
        assert stage2.param_subsample_rate == 1.0, (
            f"Stage2 subsample must be 1.0, got {stage2.param_subsample_rate}"
        )


def test_subsample_rate_progression():
    """Test that subsample rates progress correctly."""
    cfg = {"param_subsample_rate": 0.1}
    plan = build_default_funnel_plan(cfg)
    
    s0_rate = plan.stages[0].param_subsample_rate
    s1_rate = plan.stages[1].param_subsample_rate
    s2_rate = plan.stages[2].param_subsample_rate
    
    # Stage0: config rate
    assert s0_rate == 0.1
    
    # Stage1: min(1.0, s0 * 2)
    assert s1_rate == min(1.0, 0.1 * 2.0) == 0.2
    
    # Stage2: must be 1.0
    assert s2_rate == 1.0
    
    # Verify progression: s0 <= s1 <= s2
    assert s0_rate <= s1_rate <= s2_rate


def test_each_stage_creates_run_dir_with_artifacts():
    """Test that each stage creates run directory with required artifacts."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        # Create minimal config
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
        }
        
        # Run funnel
        result_index = run_funnel(cfg, outputs_root)
        
        # Verify all stages have run directories
        assert len(result_index.stages) == 3
        
        artifacts = [
            "manifest.json",
            "config_snapshot.json",
            "metrics.json",
            "winners.json",
            "README.md",
            "logs.txt",
        ]
        
        for stage_idx in result_index.stages:
            run_dir = outputs_root / stage_idx.run_dir
            
            # Verify directory exists
            assert run_dir.exists(), f"Run directory missing for {stage_idx.stage.value}"
            assert run_dir.is_dir()
            
            # Verify all artifacts exist
            for artifact_name in artifacts:
                artifact_path = run_dir / artifact_name
                assert artifact_path.exists(), (
                    f"Missing artifact {artifact_name} for {stage_idx.stage.value}"
                )


def test_param_subsample_rate_visible_in_artifacts():
    """Test that param_subsample_rate is visible in manifest/metrics/README."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.25,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        import json
        
        for stage_idx in result_index.stages:
            run_dir = outputs_root / stage_idx.run_dir
            
            # Check manifest.json
            manifest_path = run_dir / "manifest.json"
            with open(manifest_path, "r", encoding="utf-8") as f:
                manifest = json.load(f)
            assert "param_subsample_rate" in manifest
            
            # Check metrics.json
            metrics_path = run_dir / "metrics.json"
            with open(metrics_path, "r", encoding="utf-8") as f:
                metrics = json.load(f)
            assert "param_subsample_rate" in metrics
            
            # Check README.md
            readme_path = run_dir / "README.md"
            with open(readme_path, "r", encoding="utf-8") as f:
                readme_content = f.read()
            assert "param_subsample_rate" in readme_content


def test_params_effective_floor_rule_consistent():
    """Test that params_effective uses consistent floor rule across stages."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        params_total = 1000
        param_subsample_rate = 0.33
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": params_total,
            "param_subsample_rate": param_subsample_rate,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(params_total, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        import json
        
        plan = result_index.plan
        for i, (spec, stage_idx) in enumerate(zip(plan.stages, result_index.stages)):
            run_dir = outputs_root / stage_idx.run_dir
            
            # Read manifest
            manifest_path = run_dir / "manifest.json"
            with open(manifest_path, "r", encoding="utf-8") as f:
                manifest = json.load(f)
            
            # Verify params_effective matches computed value
            expected_effective = compute_params_effective(
                params_total, spec.param_subsample_rate
            )
            assert manifest["params_effective"] == expected_effective, (
                f"Stage {i} params_effective mismatch: "
                f"expected={expected_effective}, got={manifest['params_effective']}"
            )


def test_funnel_result_index_contains_all_stages():
    """Test that funnel result index contains all stages."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        # Verify index structure
        assert result_index.plan is not None
        assert len(result_index.stages) == 3
        
        # Verify stage order matches plan
        for spec, stage_idx in zip(result_index.plan.stages, result_index.stages):
            assert spec.name == stage_idx.stage
            assert stage_idx.run_id is not None
            assert stage_idx.run_dir is not None


def test_config_snapshot_is_json_serializable_and_small():
    """Test that config_snapshot.json excludes ndarrays and is JSON-serializable."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        import json
        
        # Keys that should NOT exist in snapshot (raw ndarrays)
        forbidden_keys = {"open_", "open", "high", "low", "close", "volume", "params_matrix"}
        
        # Required keys that MUST exist
        required_keys = {
            "season",
            "dataset_id",
            "bars",
            "params_total",
            "param_subsample_rate",
            "stage_name",
        }
        
        for stage_idx in result_index.stages:
            run_dir = outputs_root / stage_idx.run_dir
            config_snapshot_path = run_dir / "config_snapshot.json"
            
            assert config_snapshot_path.exists()
            
            # Verify JSON is valid and loadable
            with open(config_snapshot_path, "r", encoding="utf-8") as f:
                snapshot_content = f.read()
                snapshot_data = json.loads(snapshot_content)  # Should not crash
            
            # Verify no raw ndarray keys exist
            for forbidden_key in forbidden_keys:
                assert forbidden_key not in snapshot_data, (
                    f"config_snapshot.json should not contain '{forbidden_key}' "
                    f"(raw ndarray) for {stage_idx.stage.value}"
                )
            
            # Verify required keys exist
            for required_key in required_keys:
                assert required_key in snapshot_data, (
                    f"config_snapshot.json missing required key '{required_key}' "
                    f"for {stage_idx.stage.value}"
                )
            
            # Verify param_subsample_rate is present and correct
            assert "param_subsample_rate" in snapshot_data
            assert isinstance(snapshot_data["param_subsample_rate"], (int, float))
            
            # Verify stage_name is present
            assert "stage_name" in snapshot_data
            assert isinstance(snapshot_data["stage_name"], str)
            
            # Optional: verify metadata keys exist if needed
            # (e.g., "open__meta", "params_matrix_meta")
            # This is optional - metadata may or may not be included



--------------------------------------------------------------------------------

FILE tests/test_funnel_oom_integration.py
sha256(source_bytes) = 995d0b72431a27c311f51c5c5f0fd77d12fa552bf90bd7bb6c64a977674f6db9
bytes = 11168
redacted = False
--------------------------------------------------------------------------------

"""Integration tests for OOM gate in funnel pipeline.

Tests verify:
1. Funnel metrics include OOM gate fields
2. Auto-downsample updates snapshot and hash consistently
"""

from __future__ import annotations

import json
import tempfile
from pathlib import Path

import numpy as np
import pytest

from FishBroWFS_V2.pipeline.funnel_runner import run_funnel


def test_funnel_metrics_include_oom_gate_fields():
    """Test that funnel metrics include OOM gate fields."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
            "mem_limit_mb": 10000.0,  # High limit to ensure PASS
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        # Verify all stages have OOM gate fields in metrics
        for stage_idx in result_index.stages:
            run_dir = outputs_root / stage_idx.run_dir
            metrics_path = run_dir / "metrics.json"
            
            assert metrics_path.exists()
            
            with open(metrics_path, "r", encoding="utf-8") as f:
                metrics = json.load(f)
            
            # Verify required OOM gate fields
            assert "oom_gate_action" in metrics
            assert "oom_gate_reason" in metrics
            assert "mem_est_mb" in metrics
            assert "mem_limit_mb" in metrics
            assert "ops_est" in metrics
            assert "stage_planned_subsample" in metrics
            
            # Verify action is valid
            assert metrics["oom_gate_action"] in ("PASS", "BLOCK", "AUTO_DOWNSAMPLE")
            
            # Verify stage_planned_subsample matches expected planned for this stage
            stage_name = metrics.get("stage_name")
            s0_base = cfg.get("param_subsample_rate", 0.1)
            expected_planned = planned_subsample_for_stage(stage_name, s0_base)
            assert metrics["stage_planned_subsample"] == expected_planned, (
                f"stage_planned_subsample mismatch for {stage_name}: "
                f"expected={expected_planned}, got={metrics['stage_planned_subsample']}"
            )


def planned_subsample_for_stage(stage_name: str, s0: float) -> float:
    """
    Get planned subsample rate for a stage based on funnel plan rules.
    
    Args:
        stage_name: Stage identifier
        s0: Stage0 base subsample rate (from config)
        
    Returns:
        Planned subsample rate for the stage
    """
    if stage_name == "stage0_coarse":
        return s0
    if stage_name == "stage1_topk":
        return min(1.0, s0 * 2.0)
    if stage_name == "stage2_confirm":
        return 1.0
    raise AssertionError(f"Unknown stage_name: {stage_name}")


def test_auto_downsample_updates_snapshot_and_hash(monkeypatch):
    """Test that auto-downsample updates snapshot and hash consistently."""
    # Monkeypatch estimate_memory_bytes to trigger auto-downsample
    def mock_estimate_memory_bytes(cfg, work_factor=2.0):
        """Mock that makes memory estimate sensitive to subsample."""
        bars = int(cfg.get("bars", 0))
        params_total = int(cfg.get("params_total", 0))
        subsample_rate = float(cfg.get("param_subsample_rate", 1.0))
        params_effective = int(params_total * subsample_rate)
        
        base_mem = bars * 8 * 4  # 4 price arrays
        params_mem = params_effective * 3 * 8  # params_matrix
        total_mem = (base_mem + params_mem) * work_factor
        return int(total_mem)
    
    monkeypatch.setattr(
        "FishBroWFS_V2.core.oom_cost_model.estimate_memory_bytes",
        mock_estimate_memory_bytes,
    )
    
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        # Stage0 base subsample rate (from config)
        s0_base = 0.5
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 10000,
            "params_total": 1000,
            "param_subsample_rate": s0_base,  # Stage0 base rate
            "open_": np.random.randn(10000).astype(np.float64),
            "high": np.random.randn(10000).astype(np.float64),
            "low": np.random.randn(10000).astype(np.float64),
            "close": np.random.randn(10000).astype(np.float64),
            "params_matrix": np.random.randn(1000, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
            # Dynamic limit calculation
            "mem_limit_mb": 0.65,  # Will trigger auto-downsample for some stages
            "allow_auto_downsample": True,
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        # Check each stage
        for stage_idx in result_index.stages:
            run_dir = outputs_root / stage_idx.run_dir
            
            # Read manifest
            manifest_path = run_dir / "manifest.json"
            with open(manifest_path, "r", encoding="utf-8") as f:
                manifest = json.load(f)
            
            # Read config_snapshot
            config_snapshot_path = run_dir / "config_snapshot.json"
            with open(config_snapshot_path, "r", encoding="utf-8") as f:
                config_snapshot = json.load(f)
            
            # Read metrics
            metrics_path = run_dir / "metrics.json"
            with open(metrics_path, "r", encoding="utf-8") as f:
                metrics = json.load(f)
            
            # Get stage name and planned subsample
            stage_name = metrics.get("stage_name")
            expected_planned = planned_subsample_for_stage(stage_name, s0_base)
            
            # Verify consistency: if auto-downsample occurred, all must match
            if metrics.get("oom_gate_action") == "AUTO_DOWNSAMPLE":
                final_subsample = metrics.get("oom_gate_final_subsample")
                
                # Manifest must have final subsample
                assert manifest["param_subsample_rate"] == final_subsample, (
                    f"Manifest subsample mismatch: "
                    f"expected={final_subsample}, got={manifest['param_subsample_rate']}"
                )
                
                # Config snapshot must have final subsample
                assert config_snapshot["param_subsample_rate"] == final_subsample, (
                    f"Config snapshot subsample mismatch: "
                    f"expected={final_subsample}, got={config_snapshot['param_subsample_rate']}"
                )
                
                # Metrics must have final subsample
                assert metrics["param_subsample_rate"] == final_subsample, (
                    f"Metrics subsample mismatch: "
                    f"expected={final_subsample}, got={metrics['param_subsample_rate']}"
                )
                
                # Verify original subsample matches planned subsample for this stage
                assert "oom_gate_original_subsample" in metrics
                assert metrics["oom_gate_original_subsample"] == expected_planned, (
                    f"oom_gate_original_subsample mismatch for {stage_name}: "
                    f"expected={expected_planned} (planned), "
                    f"got={metrics['oom_gate_original_subsample']}"
                )
                
                # Verify stage_planned_subsample equals oom_gate_original_subsample
                assert "stage_planned_subsample" in metrics
                assert metrics["stage_planned_subsample"] == metrics["oom_gate_original_subsample"], (
                    f"stage_planned_subsample should equal oom_gate_original_subsample for {stage_name}: "
                    f"stage_planned={metrics['stage_planned_subsample']}, "
                    f"original={metrics['oom_gate_original_subsample']}"
                )


def test_oom_gate_fields_in_readme():
    """Test that OOM gate fields are included in README."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000,
            "params_total": 100,
            "param_subsample_rate": 0.1,
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
            "mem_limit_mb": 10000.0,
        }
        
        result_index = run_funnel(cfg, outputs_root)
        
        # Check README for at least one stage
        for stage_idx in result_index.stages:
            run_dir = outputs_root / stage_idx.run_dir
            readme_path = run_dir / "README.md"
            
            assert readme_path.exists()
            
            with open(readme_path, "r", encoding="utf-8") as f:
                readme_content = f.read()
            
            # Verify OOM gate section exists
            assert "OOM Gate" in readme_content
            assert "action" in readme_content.lower()
            assert "mem_est_mb" in readme_content.lower()
            
            break  # Check at least one stage


def test_block_action_raises_error():
    """Test that BLOCK action raises RuntimeError."""
    with tempfile.TemporaryDirectory() as tmpdir:
        outputs_root = Path(tmpdir) / "outputs"
        
        cfg = {
            "season": "test_season",
            "dataset_id": "test_dataset",
            "bars": 1000000,  # Very large
            "params_total": 100000,  # Very large
            "param_subsample_rate": 1.0,
            "open_": np.random.randn(1000000).astype(np.float64),
            "high": np.random.randn(1000000).astype(np.float64),
            "low": np.random.randn(1000000).astype(np.float64),
            "close": np.random.randn(1000000).astype(np.float64),
            "params_matrix": np.random.randn(100000, 3).astype(np.float64),
            "commission": 0.0,
            "slip": 0.0,
            "order_qty": 1,
            "mem_limit_mb": 1.0,  # Very low limit
            "allow_auto_downsample": False,  # Disable auto-downsample to force BLOCK
        }
        
        # Should raise RuntimeError
        with pytest.raises(RuntimeError, match="OOM Gate BLOCKED"):
            run_funnel(cfg, outputs_root)



--------------------------------------------------------------------------------

FILE tests/test_funnel_smoke_contract.py
sha256(source_bytes) = 1362cdc2bc893293c711e093f011928e173a259c4954eceb92c27dc782624f68
bytes = 5435
redacted = False
--------------------------------------------------------------------------------

"""Funnel smoke contract tests - Phase 4 Stage D.

Basic smoke tests to ensure the complete funnel pipeline works end-to-end.
"""

import numpy as np

from FishBroWFS_V2.pipeline.funnel import FunnelResult, run_funnel


def test_funnel_smoke_basic():
    """Basic smoke test: run funnel with small parameter grid."""
    # Generate deterministic test data
    np.random.seed(42)
    n_bars = 500
    n_params = 20
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    # Generate parameter grid
    params_matrix = np.column_stack([
        np.random.randint(10, 50, size=n_params),  # channel_len / fast_len
        np.random.randint(5, 30, size=n_params),   # atr_len / slow_len
        np.random.uniform(1.0, 3.0, size=n_params), # stop_mult
    ]).astype(np.float64)
    
    # Run funnel
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=5,
        commission=0.0,
        slip=0.0,
    )
    
    # Verify result structure
    assert isinstance(result, FunnelResult)
    assert len(result.stage0_results) == n_params
    assert len(result.topk_param_ids) == 5
    assert len(result.stage2_results) == 5
    
    # Verify Stage0 results
    for stage0_result in result.stage0_results:
        assert hasattr(stage0_result, "param_id")
        assert hasattr(stage0_result, "proxy_value")
        assert hasattr(stage0_result, "warmup_ok")
        assert isinstance(stage0_result.param_id, int)
        assert isinstance(stage0_result.proxy_value, (int, float))
    
    # Verify Top-K param_ids are valid
    for param_id in result.topk_param_ids:
        assert 0 <= param_id < n_params
    
    # Verify Stage2 results match Top-K
    assert len(result.stage2_results) == len(result.topk_param_ids)
    for i, stage2_result in enumerate(result.stage2_results):
        assert stage2_result.param_id == result.topk_param_ids[i]
        assert isinstance(stage2_result.net_profit, (int, float))
        assert isinstance(stage2_result.trades, int)
        assert isinstance(stage2_result.max_dd, (int, float))


def test_funnel_smoke_empty_params():
    """Test funnel with empty parameter grid."""
    np.random.seed(42)
    n_bars = 100
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    # Empty parameter grid
    params_matrix = np.empty((0, 3), dtype=np.float64)
    
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=5,
    )
    
    assert len(result.stage0_results) == 0
    assert len(result.topk_param_ids) == 0
    assert len(result.stage2_results) == 0


def test_funnel_smoke_k_larger_than_params():
    """Test funnel when k is larger than number of parameters."""
    np.random.seed(42)
    n_bars = 100
    n_params = 5
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    params_matrix = np.column_stack([
        np.random.randint(10, 50, size=n_params),
        np.random.randint(5, 30, size=n_params),
        np.random.uniform(1.0, 3.0, size=n_params),
    ]).astype(np.float64)
    
    # k=10 but only 5 params
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=10,
    )
    
    # Should return all 5 params
    assert len(result.topk_param_ids) == 5
    assert len(result.stage2_results) == 5


def test_funnel_smoke_pipeline_order():
    """Test that pipeline executes in correct order: Stage0  Top-K  Stage2."""
    np.random.seed(42)
    n_bars = 200
    n_params = 10
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    params_matrix = np.column_stack([
        np.random.randint(10, 30, size=n_params),
        np.random.randint(5, 20, size=n_params),
        np.random.uniform(1.0, 2.0, size=n_params),
    ]).astype(np.float64)
    
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=3,
    )
    
    # Verify Stage0 ran on all params
    assert len(result.stage0_results) == n_params
    
    # Verify Top-K selected from Stage0 results
    assert len(result.topk_param_ids) == 3
    # Top-K should be sorted by proxy_value (descending)
    stage0_by_id = {r.param_id: r for r in result.stage0_results}
    topk_values = [stage0_by_id[pid].proxy_value for pid in result.topk_param_ids]
    assert topk_values == sorted(topk_values, reverse=True)
    
    # Verify Stage2 ran only on Top-K
    assert len(result.stage2_results) == 3
    stage2_param_ids = [r.param_id for r in result.stage2_results]
    assert set(stage2_param_ids) == set(result.topk_param_ids)



--------------------------------------------------------------------------------

FILE tests/test_funnel_topk_determinism.py
sha256(source_bytes) = fa9ef9029f0c2485dabc31add572fc7642a6a9c4da937938f7661fd394b0f0a5
bytes = 4480
redacted = False
--------------------------------------------------------------------------------

"""Test Top-K determinism - same input must produce same Top-K selection."""

import numpy as np

from FishBroWFS_V2.pipeline.funnel import run_funnel
from FishBroWFS_V2.pipeline.stage0_runner import Stage0Result, run_stage0
from FishBroWFS_V2.pipeline.topk import select_topk


def test_topk_determinism_same_input():
    """Test that Top-K selection is deterministic: same input produces same output."""
    # Generate deterministic test data
    np.random.seed(42)
    n_bars = 1000
    n_params = 100
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    # Generate parameter grid
    params_matrix = np.column_stack([
        np.random.randint(10, 100, size=n_params),  # fast_len / channel_len
        np.random.randint(5, 50, size=n_params),      # slow_len / atr_len
        np.random.uniform(1.0, 5.0, size=n_params),   # stop_mult
    ]).astype(np.float64)
    
    # Run Stage0 twice with same input
    stage0_results_1 = run_stage0(close, params_matrix)
    stage0_results_2 = run_stage0(close, params_matrix)
    
    # Verify Stage0 results are identical
    assert len(stage0_results_1) == len(stage0_results_2)
    for r1, r2 in zip(stage0_results_1, stage0_results_2):
        assert r1.param_id == r2.param_id
        assert r1.proxy_value == r2.proxy_value
    
    # Run Top-K selection twice
    k = 20
    topk_1 = select_topk(stage0_results_1, k=k)
    topk_2 = select_topk(stage0_results_2, k=k)
    
    # Verify Top-K selection is identical
    assert topk_1 == topk_2, (
        f"Top-K selection not deterministic:\n"
        f"  First run:  {topk_1}\n"
        f"  Second run: {topk_2}"
    )
    assert len(topk_1) == k
    assert len(topk_2) == k


def test_topk_determinism_tie_break():
    """Test that tie-breaking by param_id is deterministic."""
    # Create Stage0 results with identical proxy_value
    # Tie-break should use param_id (ascending)
    results = [
        Stage0Result(param_id=5, proxy_value=10.0),
        Stage0Result(param_id=2, proxy_value=10.0),  # Same value, lower param_id
        Stage0Result(param_id=8, proxy_value=10.0),
        Stage0Result(param_id=1, proxy_value=10.0),  # Same value, lowest param_id
        Stage0Result(param_id=3, proxy_value=15.0),  # Higher value
        Stage0Result(param_id=4, proxy_value=12.0),  # Medium value
    ]
    
    # Select top 3
    topk = select_topk(results, k=3)
    
    # Expected: param_id=3 (value=15.0), param_id=4 (value=12.0), param_id=1 (value=10.0, lowest param_id)
    assert topk == [3, 4, 1], f"Tie-break failed: got {topk}, expected [3, 4, 1]"
    
    # Run again - should be identical
    topk_2 = select_topk(results, k=3)
    assert topk_2 == topk


def test_funnel_determinism():
    """Test that complete funnel pipeline is deterministic."""
    # Generate deterministic test data
    np.random.seed(123)
    n_bars = 500
    n_params = 50
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    # Generate parameter grid
    params_matrix = np.column_stack([
        np.random.randint(10, 50, size=n_params),
        np.random.randint(5, 30, size=n_params),
        np.random.uniform(1.0, 3.0, size=n_params),
    ]).astype(np.float64)
    
    # Run funnel twice
    result_1 = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=10,
        commission=0.0,
        slip=0.0,
    )
    
    result_2 = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=10,
        commission=0.0,
        slip=0.0,
    )
    
    # Verify Top-K selection is identical
    assert result_1.topk_param_ids == result_2.topk_param_ids, (
        f"Funnel Top-K not deterministic:\n"
        f"  First run:  {result_1.topk_param_ids}\n"
        f"  Second run: {result_2.topk_param_ids}"
    )
    
    # Verify Stage2 results are for same parameters
    assert len(result_1.stage2_results) == len(result_2.stage2_results)
    for r1, r2 in zip(result_1.stage2_results, result_2.stage2_results):
        assert r1.param_id == r2.param_id



--------------------------------------------------------------------------------

FILE tests/test_funnel_topk_no_human_contract.py
sha256(source_bytes) = e24122a497ae621100b91a8940659aa33c1498f7ba8735249dc9093bc142c437
bytes = 7779
redacted = False
--------------------------------------------------------------------------------

"""Funnel Top-K no-human contract tests - Phase 4 Stage D.

These tests ensure that Top-K selection is purely automatic based on proxy_value,
with no possibility of human intervention or manual filtering.
"""

import numpy as np

from FishBroWFS_V2.pipeline.funnel import run_funnel
from FishBroWFS_V2.pipeline.stage0_runner import Stage0Result, run_stage0
from FishBroWFS_V2.pipeline.topk import select_topk


def test_topk_only_uses_proxy_value():
    """Test that Top-K selection uses ONLY proxy_value, not any other field."""
    # Create Stage0 results with varying proxy_value and other fields
    results = [
        Stage0Result(param_id=0, proxy_value=5.0, warmup_ok=True, meta={"custom": "data"}),
        Stage0Result(param_id=1, proxy_value=10.0, warmup_ok=False, meta=None),
        Stage0Result(param_id=2, proxy_value=15.0, warmup_ok=True, meta={"other": 123}),
        Stage0Result(param_id=3, proxy_value=8.0, warmup_ok=True, meta=None),
        Stage0Result(param_id=4, proxy_value=12.0, warmup_ok=False, meta={"test": True}),
    ]
    
    # Select top 3
    topk = select_topk(results, k=3)
    
    # Expected: param_id=2 (value=15.0), param_id=4 (value=12.0), param_id=1 (value=10.0)
    # Should ignore warmup_ok and meta fields
    assert topk == [2, 4, 1], (
        f"Top-K should only consider proxy_value, got {topk}, expected [2, 4, 1]"
    )


def test_topk_tie_break_param_id():
    """Test that tie-breaking uses param_id (ascending) when proxy_value is identical."""
    # Create results with identical proxy_value
    results = [
        Stage0Result(param_id=5, proxy_value=10.0),
        Stage0Result(param_id=2, proxy_value=10.0),
        Stage0Result(param_id=8, proxy_value=10.0),
        Stage0Result(param_id=1, proxy_value=10.0),
        Stage0Result(param_id=3, proxy_value=15.0),  # Higher value
        Stage0Result(param_id=4, proxy_value=12.0),   # Medium value
    ]
    
    # Select top 3
    topk = select_topk(results, k=3)
    
    # Expected: param_id=3 (value=15.0), param_id=4 (value=12.0), param_id=1 (value=10.0, lowest param_id)
    assert topk == [3, 4, 1], (
        f"Tie-break should use param_id ascending, got {topk}, expected [3, 4, 1]"
    )


def test_topk_deterministic_same_input():
    """Test that Top-K selection is deterministic: same input produces same output."""
    np.random.seed(42)
    n_bars = 500
    n_params = 50
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    
    params_matrix = np.column_stack([
        np.random.randint(10, 50, size=n_params),
        np.random.randint(5, 30, size=n_params),
        np.random.uniform(1.0, 3.0, size=n_params),
    ]).astype(np.float64)
    
    # Run Stage0 twice
    stage0_results_1 = run_stage0(close, params_matrix)
    stage0_results_2 = run_stage0(close, params_matrix)
    
    # Select Top-K twice
    topk_1 = select_topk(stage0_results_1, k=10)
    topk_2 = select_topk(stage0_results_2, k=10)
    
    # Should be identical
    assert topk_1 == topk_2, (
        f"Top-K selection not deterministic:\n"
        f"  First run:  {topk_1}\n"
        f"  Second run: {topk_2}"
    )


def test_funnel_topk_no_manual_filtering():
    """Test that funnel Top-K selection cannot be manually filtered."""
    np.random.seed(42)
    n_bars = 300
    n_params = 20
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    params_matrix = np.column_stack([
        np.random.randint(10, 40, size=n_params),
        np.random.randint(5, 25, size=n_params),
        np.random.uniform(1.0, 2.5, size=n_params),
    ]).astype(np.float64)
    
    # Run funnel
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=5,
    )
    
    # Verify Top-K is based solely on proxy_value
    stage0_by_id = {r.param_id: r for r in result.stage0_results}
    
    # Get proxy_values for Top-K
    topk_values = [stage0_by_id[pid].proxy_value for pid in result.topk_param_ids]
    
    # Get proxy_values for all params
    all_values = [r.proxy_value for r in result.stage0_results]
    all_values_sorted = sorted(all_values, reverse=True)
    
    # Top-K values should match top K values from all params
    assert topk_values == all_values_sorted[:5], (
        f"Top-K should contain top 5 proxy_values:\n"
        f"  Top-K values: {topk_values}\n"
        f"  Top 5 values:  {all_values_sorted[:5]}"
    )


def test_funnel_stage2_only_runs_topk():
    """Test that Stage2 only runs on Top-K parameters, not all parameters."""
    np.random.seed(42)
    n_bars = 200
    n_params = 15
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    params_matrix = np.column_stack([
        np.random.randint(10, 30, size=n_params),
        np.random.randint(5, 20, size=n_params),
        np.random.uniform(1.0, 2.0, size=n_params),
    ]).astype(np.float64)
    
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=3,
    )
    
    # Verify Stage0 ran on all params
    assert len(result.stage0_results) == n_params
    
    # Verify Top-K selected
    assert len(result.topk_param_ids) == 3
    
    # Verify Stage2 ran ONLY on Top-K (not all params)
    assert len(result.stage2_results) == 3, (
        f"Stage2 should run only on Top-K (3 params), not all params ({n_params})"
    )
    
    # Verify Stage2 param_ids match Top-K
    stage2_param_ids = set(r.param_id for r in result.stage2_results)
    topk_param_ids_set = set(result.topk_param_ids)
    assert stage2_param_ids == topk_param_ids_set, (
        f"Stage2 param_ids should match Top-K:\n"
        f"  Stage2: {stage2_param_ids}\n"
        f"  Top-K:  {topk_param_ids_set}"
    )


def test_funnel_stage0_no_pnl_fields():
    """Test that Stage0 results contain NO PnL-related fields."""
    np.random.seed(42)
    n_bars = 200
    n_params = 10
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    open_ = close + np.random.randn(n_bars) * 2
    high = np.maximum(open_, close) + np.abs(np.random.randn(n_bars)) * 3
    low = np.minimum(open_, close) - np.abs(np.random.randn(n_bars)) * 3
    
    params_matrix = np.column_stack([
        np.random.randint(10, 30, size=n_params),
        np.random.randint(5, 20, size=n_params),
        np.random.uniform(1.0, 2.0, size=n_params),
    ]).astype(np.float64)
    
    result = run_funnel(
        open_,
        high,
        low,
        close,
        params_matrix,
        k=5,
    )
    
    # Check all Stage0 results
    forbidden_fields = {"net", "profit", "mdd", "dd", "drawdown", "sqn", "sharpe", 
                       "winrate", "equity", "pnl", "trades", "score"}
    
    for stage0_result in result.stage0_results:
        # Get field names
        if hasattr(stage0_result, "__dataclass_fields__"):
            field_names = set(stage0_result.__dataclass_fields__.keys())
        else:
            field_names = set(getattr(stage0_result, "__dict__", {}).keys())
        
        # Check no forbidden fields
        for field_name in field_names:
            field_lower = field_name.lower()
            for forbidden in forbidden_fields:
                assert forbidden not in field_lower, (
                    f"Stage0Result contains forbidden PnL field: {field_name} "
                    f"(contains '{forbidden}')"
                )



--------------------------------------------------------------------------------

FILE tests/test_generate_research_cli.py
sha256(source_bytes) = 10aef3ed977c5d6363c966cf1e13cfb6616fd982f76c44891668a598a1f52e2a
bytes = 4137
redacted = False
--------------------------------------------------------------------------------
"""Test generate_research.py CLI behavior.

Ensure that:
1. -h / --help does not execute generate logic
2. --dry-run works without writing files
3. Script does not crash on import errors
"""

from __future__ import annotations

import subprocess
import sys
from pathlib import Path
import pytest


def test_generate_research_help_does_not_execute():
    """Test that -h/--help does not execute generate logic."""
    # Test -h
    result = subprocess.run(
        [sys.executable, "scripts/generate_research.py", "-h"],
        cwd=Path(__file__).parent.parent,
        capture_output=True,
        text=True,
    )
    
    assert result.returncode == 0, f"Help should exit with 0, got {result.returncode}"
    assert "usage:" in result.stdout.lower() or "help" in result.stdout.lower()
    assert "error" not in result.stdout.lower()
    assert "error" not in result.stderr.lower()
    
    # Test --help
    result = subprocess.run(
        [sys.executable, "scripts/generate_research.py", "--help"],
        cwd=Path(__file__).parent.parent,
        capture_output=True,
        text=True,
    )
    
    assert result.returncode == 0, f"Help should exit with 0, got {result.returncode}"
    assert "usage:" in result.stdout.lower() or "help" in result.stdout.lower()
    assert "error" not in result.stdout.lower()
    assert "error" not in result.stderr.lower()


def test_generate_research_dry_run():
    """Test that --dry-run works without writing files."""
    # Create a temporary outputs directory to test
    import tempfile
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        outputs_root = tmp_path / "outputs"
        outputs_root.mkdir()
        
        result = subprocess.run(
            [
                sys.executable,
                "scripts/generate_research.py",
                "--outputs-root", str(outputs_root),
                "--dry-run",
                "--verbose",
            ],
            cwd=Path(__file__).parent.parent,
            capture_output=True,
            text=True,
        )
        
        assert result.returncode == 0, f"Dry run should exit with 0, got {result.returncode}"
        assert "dry run" in result.stdout.lower() or "would generate" in result.stdout.lower()
        
        # Ensure no files were actually created
        research_dir = outputs_root / "research"
        assert not research_dir.exists() or not list(research_dir.glob("*.json"))


def test_generate_research_without_outputs_dir():
    """Test that script handles missing outputs directory gracefully."""
    import tempfile
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        outputs_root = tmp_path / "nonexistent"
        
        result = subprocess.run(
            [
                sys.executable,
                "scripts/generate_research.py",
                "--outputs-root", str(outputs_root),
            ],
            cwd=Path(__file__).parent.parent,
            capture_output=True,
            text=True,
        )
        
        # Should either succeed (creating empty results) or fail gracefully
        # but not crash with import errors
        assert result.returncode in (0, 1), f"Unexpected exit code: {result.returncode}"
        assert "import error" not in result.stderr.lower(), f"Import error occurred: {result.stderr}"


def test_generate_research_import_fixed():
    """Test that import errors are fixed (no NameError for extract_canonical_metrics)."""
    # This test imports the module directly to check for import errors
    import sys
    project_root = Path(__file__).parent.parent
    sys.path.insert(0, str(project_root / "src"))
    
    try:
        from FishBroWFS_V2.research.__main__ import generate_canonical_results
        from FishBroWFS_V2.research.registry import build_research_index
        
        # If we get here, imports succeeded
        assert True
    except ImportError as e:
        pytest.fail(f"Import error: {e}")
    except NameError as e:
        pytest.fail(f"NameError (missing import): {e}")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
--------------------------------------------------------------------------------

FILE tests/test_golden_kernel_verification.py
sha256(source_bytes) = 3a981b6e92a50a80938047209a1a46e6ceda8938b13d99df21ccd2699e496b0f
bytes = 2526
redacted = False
--------------------------------------------------------------------------------

import numpy as np

from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, run_kernel, _max_drawdown
from FishBroWFS_V2.engine.types import BarArrays


def _bars():
    # Small synthetic OHLC series
    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)
    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)
    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)
    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)
    return BarArrays(open=o, high=h, low=l, close=c)


def test_no_trade_case_does_not_crash_and_returns_zero_metrics():
    bars = _bars()
    params = DonchianAtrParams(channel_len=99999, atr_len=3, stop_mult=2.0)

    out = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)
    pnl = out["pnl"]
    equity = out["equity"]
    metrics = out["metrics"]

    assert isinstance(pnl, np.ndarray)
    assert pnl.size == 0
    assert isinstance(equity, np.ndarray)
    assert equity.size == 0
    assert metrics["net_profit"] == 0.0
    assert metrics["trades"] == 0
    assert metrics["max_dd"] == 0.0


def test_vectorized_metrics_are_self_consistent():
    bars = _bars()
    params = DonchianAtrParams(channel_len=2, atr_len=2, stop_mult=1.0)

    out = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)
    pnl = out["pnl"]
    equity = out["equity"]
    metrics = out["metrics"]

    # If zero trades, still must be consistent
    if pnl.size == 0:
        assert metrics["net_profit"] == 0.0
        assert metrics["trades"] == 0
        assert metrics["max_dd"] == 0.0
        return

    # Vectorized checks
    np.testing.assert_allclose(equity, np.cumsum(pnl), rtol=0.0, atol=0.0)
    assert metrics["trades"] == int(pnl.size)
    assert metrics["net_profit"] == float(np.sum(pnl))
    assert metrics["max_dd"] == _max_drawdown(equity)


def test_costs_are_parameterized_not_hardcoded():
    bars = _bars()
    params = DonchianAtrParams(channel_len=2, atr_len=2, stop_mult=1.0)

    out0 = run_kernel(bars, params, commission=0.0, slip=0.0, order_qty=1)
    out1 = run_kernel(bars, params, commission=1.25, slip=0.75, order_qty=1)

    pnl0 = out0["pnl"]
    pnl1 = out1["pnl"]

    # Either both empty or both non-empty; if empty, pass
    if pnl0.size == 0:
        assert pnl1.size == 0
        return

    # Costs increase => pnl decreases by 2*(commission+slip) per trade
    per_trade_delta = 2.0 * (1.25 + 0.75)
    np.testing.assert_allclose(pnl1, pnl0 - per_trade_delta, rtol=0.0, atol=1e-12)




--------------------------------------------------------------------------------

FILE tests/test_governance_accepts_winners_v2.py
sha256(source_bytes) = aba4600c817381f6a8e0bc9101645e5c0d1e3671517cbe3d47f6ac1a3913c679
bytes = 8305
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for governance accepting winners v2.

Tests verify that governance evaluator can read and process v2 winners.json.
"""

from __future__ import annotations

import json
import tempfile
from datetime import datetime, timezone
from pathlib import Path

from FishBroWFS_V2.core.governance_schema import Decision
from FishBroWFS_V2.pipeline.governance_eval import evaluate_governance


def _create_fake_manifest(run_id: str, stage_name: str, season: str = "test") -> dict:
    """Create fake manifest.json."""
    return {
        "run_id": run_id,
        "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        "git_sha": "abc123def456",
        "dirty_repo": False,
        "param_subsample_rate": 0.1,
        "config_hash": "test_hash",
        "season": season,
        "dataset_id": "test_dataset",
        "bars": 1000,
        "params_total": 1000,
        "params_effective": 100,
        "artifact_version": "v1",
    }


def _create_fake_metrics(stage_name: str, stage_planned_subsample: float = 0.1) -> dict:
    """Create fake metrics.json."""
    return {
        "params_total": 1000,
        "params_effective": 100,
        "bars": 1000,
        "stage_name": stage_name,
        "param_subsample_rate": stage_planned_subsample,
        "stage_planned_subsample": stage_planned_subsample,
    }


def _create_fake_winners_v2(stage_name: str, topk_items: list[dict]) -> dict:
    """Create fake winners.json v2."""
    return {
        "schema": "v2",
        "stage_name": stage_name,
        "generated_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        "topk": topk_items,
        "notes": {
            "schema": "v2",
            "candidate_id_mode": "strategy_id:param_id",
        },
    }


def _create_fake_config_snapshot() -> dict:
    """Create fake config_snapshot.json."""
    return {
        "dataset_id": "test_dataset",
        "bars": 1000,
        "params_total": 1000,
    }


def _write_artifacts(run_dir: Path, manifest: dict, metrics: dict, winners: dict, config: dict) -> None:
    """Write artifacts to run directory."""
    run_dir.mkdir(parents=True, exist_ok=True)
    
    with (run_dir / "manifest.json").open("w", encoding="utf-8") as f:
        json.dump(manifest, f, indent=2)
    
    with (run_dir / "metrics.json").open("w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2)
    
    with (run_dir / "winners.json").open("w", encoding="utf-8") as f:
        json.dump(winners, f, indent=2)
    
    with (run_dir / "config_snapshot.json").open("w", encoding="utf-8") as f:
        json.dump(config, f, indent=2)


def test_governance_reads_winners_v2() -> None:
    """Test that governance can read and process v2 winners.json."""
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Stage0 artifacts
        stage0_dir = tmp_path / "stage0"
        _write_artifacts(
            stage0_dir,
            _create_fake_manifest("stage0-123", "stage0_coarse"),
            _create_fake_metrics("stage0_coarse"),
            _create_fake_winners_v2("stage0_coarse", [
                {
                    "candidate_id": "donchian_atr:0",
                    "strategy_id": "donchian_atr",
                    "symbol": "CME.MNQ",
                    "timeframe": "60m",
                    "params": {},
                    "score": 1.0,
                    "metrics": {"proxy_value": 1.0, "param_id": 0},
                    "source": {"param_id": 0, "run_id": "stage0-123", "stage_name": "stage0_coarse"},
                },
            ]),
            _create_fake_config_snapshot(),
        )
        
        # Stage1 artifacts (v2 format)
        stage1_dir = tmp_path / "stage1"
        stage1_winners = _create_fake_winners_v2("stage1_topk", [
            {
                "candidate_id": "donchian_atr:0",
                "strategy_id": "donchian_atr",
                "symbol": "CME.MNQ",
                "timeframe": "60m",
                "params": {},
                "score": 100.0,
                "metrics": {"net_profit": 100.0, "trades": 10, "max_dd": -10.0, "param_id": 0},
                "source": {"param_id": 0, "run_id": "stage1-123", "stage_name": "stage1_topk"},
            },
        ])
        _write_artifacts(
            stage1_dir,
            _create_fake_manifest("stage1-123", "stage1_topk"),
            _create_fake_metrics("stage1_topk"),
            stage1_winners,
            _create_fake_config_snapshot(),
        )
        
        # Stage2 artifacts (v2 format)
        stage2_dir = tmp_path / "stage2"
        stage2_winners = _create_fake_winners_v2("stage2_confirm", [
            {
                "candidate_id": "donchian_atr:0",
                "strategy_id": "donchian_atr",
                "symbol": "CME.MNQ",
                "timeframe": "60m",
                "params": {},
                "score": 100.0,
                "metrics": {"net_profit": 100.0, "trades": 10, "max_dd": -10.0, "param_id": 0},
                "source": {"param_id": 0, "run_id": "stage2-123", "stage_name": "stage2_confirm"},
            },
        ])
        _write_artifacts(
            stage2_dir,
            _create_fake_manifest("stage2-123", "stage2_confirm"),
            _create_fake_metrics("stage2_confirm"),
            stage2_winners,
            _create_fake_config_snapshot(),
        )
        
        # Evaluate governance
        report = evaluate_governance(
            stage0_dir=stage0_dir,
            stage1_dir=stage1_dir,
            stage2_dir=stage2_dir,
        )
        
        # Verify governance processed v2 format
        assert len(report.items) == 1
        item = report.items[0]
        
        # Verify candidate_id is preserved
        assert item.candidate_id == "donchian_atr:0"
        
        # Verify decision was made (should be KEEP since all rules pass)
        assert item.decision in (Decision.KEEP, Decision.FREEZE, Decision.DROP)


def test_governance_handles_mixed_v2_legacy() -> None:
    """Test that governance handles mixed v2/legacy formats gracefully."""
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Stage0 artifacts (legacy)
        stage0_dir = tmp_path / "stage0"
        _write_artifacts(
            stage0_dir,
            _create_fake_manifest("stage0-123", "stage0_coarse"),
            _create_fake_metrics("stage0_coarse"),
            {"topk": [{"param_id": 0, "proxy_value": 1.0}], "notes": {"schema": "v1"}},
            _create_fake_config_snapshot(),
        )
        
        # Stage1 artifacts (v2)
        stage1_dir = tmp_path / "stage1"
        stage1_winners = _create_fake_winners_v2("stage1_topk", [
            {
                "candidate_id": "donchian_atr:0",
                "strategy_id": "donchian_atr",
                "symbol": "CME.MNQ",
                "timeframe": "60m",
                "params": {},
                "score": 100.0,
                "metrics": {"net_profit": 100.0, "trades": 10, "max_dd": -10.0, "param_id": 0},
                "source": {"param_id": 0, "run_id": "stage1-123", "stage_name": "stage1_topk"},
            },
        ])
        _write_artifacts(
            stage1_dir,
            _create_fake_manifest("stage1-123", "stage1_topk"),
            _create_fake_metrics("stage1_topk"),
            stage1_winners,
            _create_fake_config_snapshot(),
        )
        
        # Stage2 artifacts (legacy)
        stage2_dir = tmp_path / "stage2"
        _write_artifacts(
            stage2_dir,
            _create_fake_manifest("stage2-123", "stage2_confirm"),
            _create_fake_metrics("stage2_confirm"),
            {"topk": [{"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0}], "notes": {"schema": "v1"}},
            _create_fake_config_snapshot(),
        )
        
        # Evaluate governance (should handle mixed formats)
        report = evaluate_governance(
            stage0_dir=stage0_dir,
            stage1_dir=stage1_dir,
            stage2_dir=stage2_dir,
        )
        
        # Verify governance processed successfully
        assert len(report.items) == 1
        item = report.items[0]
        assert item.candidate_id == "donchian_atr:0"



--------------------------------------------------------------------------------

FILE tests/test_governance_eval_rules.py
sha256(source_bytes) = 686e9b48ef70404285b7ce2f0d856f73c73dd10a09efa1979506ec33189ea894
bytes = 11845
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for governance evaluation rules.

Tests that governance rules (R1/R2/R3) are correctly applied using fixture artifacts.
"""

from __future__ import annotations

import json
import tempfile
from pathlib import Path
from datetime import datetime, timezone

import pytest

from FishBroWFS_V2.core.governance_schema import Decision
from FishBroWFS_V2.pipeline.governance_eval import evaluate_governance


def _create_fake_manifest(run_id: str, stage_name: str, season: str = "test") -> dict:
    """Create fake manifest.json."""
    return {
        "run_id": run_id,
        "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        "git_sha": "abc123def456",
        "dirty_repo": False,
        "param_subsample_rate": 0.1,
        "config_hash": "test_hash",
        "season": season,
        "dataset_id": "test_dataset",
        "bars": 1000,
        "params_total": 1000,
        "params_effective": 100,
        "artifact_version": "v1",
    }


def _create_fake_metrics(stage_name: str, stage_planned_subsample: float = 0.1) -> dict:
    """Create fake metrics.json."""
    return {
        "params_total": 1000,
        "params_effective": 100,
        "bars": 1000,
        "stage_name": stage_name,
        "param_subsample_rate": stage_planned_subsample,
        "stage_planned_subsample": stage_planned_subsample,
    }


def _create_fake_winners(stage_name: str, topk_items: list[dict]) -> dict:
    """Create fake winners.json."""
    return {
        "topk": topk_items,
        "notes": {
            "schema": "v1",
            "stage": stage_name,
            "topk_count": len(topk_items),
        },
    }


def _create_fake_config_snapshot() -> dict:
    """Create fake config_snapshot.json."""
    return {
        "dataset_id": "test_dataset",
        "bars": 1000,
        "params_total": 1000,
    }


def _write_artifacts(run_dir: Path, manifest: dict, metrics: dict, winners: dict, config: dict) -> None:
    """Write artifacts to run directory."""
    run_dir.mkdir(parents=True, exist_ok=True)
    
    with (run_dir / "manifest.json").open("w", encoding="utf-8") as f:
        json.dump(manifest, f, indent=2)
    
    with (run_dir / "metrics.json").open("w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=2)
    
    with (run_dir / "winners.json").open("w", encoding="utf-8") as f:
        json.dump(winners, f, indent=2)
    
    with (run_dir / "config_snapshot.json").open("w", encoding="utf-8") as f:
        json.dump(config, f, indent=2)


def test_r1_drop_when_stage2_missing() -> None:
    """
    Test R1: DROP when candidate in Stage1 but missing in Stage2.
    
    Scenario:
    - Stage1 has candidate with param_id=0
    - Stage2 does not have candidate with param_id=0
    - Expected: DROP with reason "unverified"
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Stage0 artifacts
        stage0_dir = tmp_path / "stage0"
        _write_artifacts(
            stage0_dir,
            _create_fake_manifest("stage0-123", "stage0_coarse"),
            _create_fake_metrics("stage0_coarse"),
            _create_fake_winners("stage0_coarse", [{"param_id": 0, "proxy_value": 1.0}]),
            _create_fake_config_snapshot(),
        )
        
        # Stage1 artifacts (has candidate)
        stage1_dir = tmp_path / "stage1"
        stage1_winners = _create_fake_winners(
            "stage1_topk",
            [{"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0}],
        )
        _write_artifacts(
            stage1_dir,
            _create_fake_manifest("stage1-123", "stage1_topk"),
            _create_fake_metrics("stage1_topk"),
            stage1_winners,
            _create_fake_config_snapshot(),
        )
        
        # Stage2 artifacts (missing candidate)
        stage2_dir = tmp_path / "stage2"
        stage2_winners = _create_fake_winners(
            "stage2_confirm",
            [{"param_id": 1, "net_profit": 200.0, "trades": 20, "max_dd": -20.0}],  # Different param_id
        )
        _write_artifacts(
            stage2_dir,
            _create_fake_manifest("stage2-123", "stage2_confirm"),
            _create_fake_metrics("stage2_confirm"),
            stage2_winners,
            _create_fake_config_snapshot(),
        )
        
        # Evaluate governance
        report = evaluate_governance(
            stage0_dir=stage0_dir,
            stage1_dir=stage1_dir,
            stage2_dir=stage2_dir,
        )
        
        # Verify: candidate should be DROP
        assert len(report.items) == 1
        item = report.items[0]
        assert item.decision == Decision.DROP
        assert any("R1" in reason for reason in item.reasons)
        assert any("unverified" in reason.lower() for reason in item.reasons)


def test_r2_drop_when_metric_degrades_over_threshold() -> None:
    """
    Test R2: DROP when metrics degrade > 20% from Stage1 to Stage2.
    
    Scenario:
    - Stage1: net_profit=100, max_dd=-10 -> net_over_mdd = 10.0
    - Stage2: net_profit=70, max_dd=-10 -> net_over_mdd = 7.0
    - Degradation: (10.0 - 7.0) / 10.0 = 0.30 (30% > 20% threshold)
    - Expected: DROP with reason "degraded"
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Stage0 artifacts
        stage0_dir = tmp_path / "stage0"
        _write_artifacts(
            stage0_dir,
            _create_fake_manifest("stage0-123", "stage0_coarse"),
            _create_fake_metrics("stage0_coarse"),
            _create_fake_winners("stage0_coarse", [{"param_id": 0, "proxy_value": 1.0}]),
            _create_fake_config_snapshot(),
        )
        
        # Stage1 artifacts
        stage1_dir = tmp_path / "stage1"
        stage1_winners = _create_fake_winners(
            "stage1_topk",
            [{"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0}],
        )
        _write_artifacts(
            stage1_dir,
            _create_fake_manifest("stage1-123", "stage1_topk"),
            _create_fake_metrics("stage1_topk"),
            stage1_winners,
            _create_fake_config_snapshot(),
        )
        
        # Stage2 artifacts (degraded metrics)
        stage2_dir = tmp_path / "stage2"
        stage2_winners = _create_fake_winners(
            "stage2_confirm",
            [{"param_id": 0, "net_profit": 70.0, "trades": 10, "max_dd": -10.0}],  # 30% degradation
        )
        _write_artifacts(
            stage2_dir,
            _create_fake_manifest("stage2-123", "stage2_confirm"),
            _create_fake_metrics("stage2_confirm"),
            stage2_winners,
            _create_fake_config_snapshot(),
        )
        
        # Evaluate governance
        report = evaluate_governance(
            stage0_dir=stage0_dir,
            stage1_dir=stage1_dir,
            stage2_dir=stage2_dir,
        )
        
        # Verify: candidate should be DROP
        assert len(report.items) == 1
        item = report.items[0]
        assert item.decision == Decision.DROP
        assert any("R2" in reason for reason in item.reasons)
        assert any("degraded" in reason.lower() for reason in item.reasons)


def test_r3_freeze_when_density_over_threshold() -> None:
    """
    Test R3: FREEZE when same strategy_id appears >= 3 times in Stage1 topk.
    
    Scenario:
    - Stage1 has 5 candidates with same strategy_id (donchian_atr)
    - Expected: FREEZE with reason "density"
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Stage0 artifacts
        stage0_dir = tmp_path / "stage0"
        _write_artifacts(
            stage0_dir,
            _create_fake_manifest("stage0-123", "stage0_coarse"),
            _create_fake_metrics("stage0_coarse"),
            _create_fake_winners("stage0_coarse", [{"param_id": i, "proxy_value": 1.0} for i in range(5)]),
            _create_fake_config_snapshot(),
        )
        
        # Stage1 artifacts (5 candidates)
        stage1_dir = tmp_path / "stage1"
        stage1_winners = _create_fake_winners(
            "stage1_topk",
            [
                {"param_id": i, "net_profit": 100.0 + i, "trades": 10, "max_dd": -10.0}
                for i in range(5)
            ],
        )
        _write_artifacts(
            stage1_dir,
            _create_fake_manifest("stage1-123", "stage1_topk"),
            _create_fake_metrics("stage1_topk"),
            stage1_winners,
            _create_fake_config_snapshot(),
        )
        
        # Stage2 artifacts (all candidates present)
        stage2_dir = tmp_path / "stage2"
        stage2_winners = _create_fake_winners(
            "stage2_confirm",
            [
                {"param_id": i, "net_profit": 100.0 + i, "trades": 10, "max_dd": -10.0}
                for i in range(5)
            ],
        )
        _write_artifacts(
            stage2_dir,
            _create_fake_manifest("stage2-123", "stage2_confirm"),
            _create_fake_metrics("stage2_confirm"),
            stage2_winners,
            _create_fake_config_snapshot(),
        )
        
        # Evaluate governance
        report = evaluate_governance(
            stage0_dir=stage0_dir,
            stage1_dir=stage1_dir,
            stage2_dir=stage2_dir,
        )
        
        # Verify: all candidates should be FREEZE (density >= 3)
        assert len(report.items) == 5
        for item in report.items:
            assert item.decision == Decision.FREEZE
            assert any("R3" in reason for reason in item.reasons)
            assert any("density" in reason.lower() for reason in item.reasons)


def test_keep_when_all_rules_pass() -> None:
    """
    Test KEEP when all rules pass.
    
    Scenario:
    - R1: Stage2 has candidate (pass)
    - R2: Metrics do not degrade (pass)
    - R3: Density < threshold (pass)
    - Expected: KEEP
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Stage0 artifacts
        stage0_dir = tmp_path / "stage0"
        _write_artifacts(
            stage0_dir,
            _create_fake_manifest("stage0-123", "stage0_coarse"),
            _create_fake_metrics("stage0_coarse"),
            _create_fake_winners("stage0_coarse", [{"param_id": 0, "proxy_value": 1.0}]),
            _create_fake_config_snapshot(),
        )
        
        # Stage1 artifacts (single candidate, low density)
        stage1_dir = tmp_path / "stage1"
        stage1_winners = _create_fake_winners(
            "stage1_topk",
            [{"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0}],
        )
        _write_artifacts(
            stage1_dir,
            _create_fake_manifest("stage1-123", "stage1_topk"),
            _create_fake_metrics("stage1_topk"),
            stage1_winners,
            _create_fake_config_snapshot(),
        )
        
        # Stage2 artifacts (same metrics, no degradation)
        stage2_dir = tmp_path / "stage2"
        stage2_winners = _create_fake_winners(
            "stage2_confirm",
            [{"param_id": 0, "net_profit": 100.0, "trades": 10, "max_dd": -10.0}],
        )
        _write_artifacts(
            stage2_dir,
            _create_fake_manifest("stage2-123", "stage2_confirm"),
            _create_fake_metrics("stage2_confirm"),
            stage2_winners,
            _create_fake_config_snapshot(),
        )
        
        # Evaluate governance
        report = evaluate_governance(
            stage0_dir=stage0_dir,
            stage1_dir=stage1_dir,
            stage2_dir=stage2_dir,
        )
        
        # Verify: candidate should be KEEP
        assert len(report.items) == 1
        item = report.items[0]
        assert item.decision == Decision.KEEP



--------------------------------------------------------------------------------

FILE tests/test_governance_schema_contract.py
sha256(source_bytes) = e073a65df77fb88f1244a47ae8ab1be9991069381a24478f2e67c19930b5b3e6
bytes = 3513
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for governance schema.

Tests that governance schema is JSON-serializable and follows contracts.
"""

from __future__ import annotations

import json
from datetime import datetime, timezone

from FishBroWFS_V2.core.governance_schema import (
    Decision,
    EvidenceRef,
    GovernanceItem,
    GovernanceReport,
)


def test_governance_report_json_serializable() -> None:
    """
    Test that GovernanceReport is JSON-serializable.
    
    This is a critical contract: governance.json must be machine-readable.
    """
    # Create sample evidence
    evidence = [
        EvidenceRef(
            run_id="test-run-123",
            stage_name="stage1_topk",
            artifact_paths=["manifest.json", "metrics.json", "winners.json"],
            key_metrics={"param_id": 0, "net_profit": 100.0, "trades": 10},
        ),
    ]
    
    # Create sample item
    item = GovernanceItem(
        candidate_id="donchian_atr:abc123def456",
        decision=Decision.KEEP,
        reasons=["R3: density_5_over_threshold_3"],
        evidence=evidence,
        created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        git_sha="abc123def456",
    )
    
    # Create report
    report = GovernanceReport(
        items=[item],
        metadata={
            "governance_id": "gov-20251218T000000Z-12345678",
            "season": "test_season",
            "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            "git_sha": "abc123def456",
        },
    )
    
    # Convert to dict
    report_dict = report.to_dict()
    
    # Serialize to JSON
    json_str = json.dumps(report_dict, ensure_ascii=False, sort_keys=True, indent=2)
    
    # Deserialize back
    report_dict_roundtrip = json.loads(json_str)
    
    # Verify structure
    assert "items" in report_dict_roundtrip
    assert "metadata" in report_dict_roundtrip
    assert len(report_dict_roundtrip["items"]) == 1
    
    item_dict = report_dict_roundtrip["items"][0]
    assert item_dict["candidate_id"] == "donchian_atr:abc123def456"
    assert item_dict["decision"] == "KEEP"
    assert len(item_dict["reasons"]) == 1
    assert len(item_dict["evidence"]) == 1
    
    evidence_dict = item_dict["evidence"][0]
    assert evidence_dict["run_id"] == "test-run-123"
    assert evidence_dict["stage_name"] == "stage1_topk"
    assert "artifact_paths" in evidence_dict
    assert "key_metrics" in evidence_dict


def test_decision_enum_values() -> None:
    """Test that Decision enum has correct values."""
    assert Decision.KEEP.value == "KEEP"
    assert Decision.FREEZE.value == "FREEZE"
    assert Decision.DROP.value == "DROP"


def test_evidence_ref_contains_subsample_fields() -> None:
    """
    Test that EvidenceRef can contain subsample fields in key_metrics.
    
    This is a critical requirement: subsample info must be in evidence.
    """
    evidence = EvidenceRef(
        run_id="test-run-123",
        stage_name="stage1_topk",
        artifact_paths=["manifest.json", "metrics.json", "winners.json"],
        key_metrics={
            "param_id": 0,
            "net_profit": 100.0,
            "stage_planned_subsample": 0.1,
            "param_subsample_rate": 0.1,
            "params_effective": 100,
        },
    )
    
    # Verify subsample fields are present
    assert "stage_planned_subsample" in evidence.key_metrics
    assert "param_subsample_rate" in evidence.key_metrics
    assert "params_effective" in evidence.key_metrics



--------------------------------------------------------------------------------

FILE tests/test_governance_transition.py
sha256(source_bytes) = 197a585771690d8af46d9d680d6ca328a8b67b6425217648e8817b977f3f819c
bytes = 2766
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for governance lifecycle state transitions.

Tests transition matrix: prev_state  decision  next_state
"""

from __future__ import annotations

import pytest

from FishBroWFS_V2.core.governance.transition import governance_transition
from FishBroWFS_V2.core.schemas.governance import Decision, LifecycleState


# Transition test matrix: (prev_state, decision, expected_next_state)
TRANSITION_TEST_CASES = [
    # INCUBATION transitions
    ("INCUBATION", Decision.KEEP, "CANDIDATE"),
    ("INCUBATION", Decision.DROP, "RETIRED"),
    ("INCUBATION", Decision.FREEZE, "INCUBATION"),
    
    # CANDIDATE transitions
    ("CANDIDATE", Decision.KEEP, "LIVE"),
    ("CANDIDATE", Decision.DROP, "RETIRED"),
    ("CANDIDATE", Decision.FREEZE, "CANDIDATE"),
    
    # LIVE transitions
    ("LIVE", Decision.KEEP, "LIVE"),
    ("LIVE", Decision.DROP, "RETIRED"),
    ("LIVE", Decision.FREEZE, "LIVE"),
    
    # RETIRED is terminal (no transitions)
    ("RETIRED", Decision.KEEP, "RETIRED"),
    ("RETIRED", Decision.DROP, "RETIRED"),
    ("RETIRED", Decision.FREEZE, "RETIRED"),
]


@pytest.mark.parametrize("prev_state,decision,expected_next_state", TRANSITION_TEST_CASES)
def test_governance_transition_matrix(
    prev_state: LifecycleState,
    decision: Decision,
    expected_next_state: LifecycleState,
) -> None:
    """
    Test governance transition for all state  decision combinations.
    
    This is a table-driven test covering the complete transition matrix.
    """
    result = governance_transition(prev_state, decision)
    
    assert result == expected_next_state, (
        f"Transition failed: {prev_state} + {decision.value}  {result}, "
        f"expected {expected_next_state}"
    )


def test_governance_transition_incubation_to_candidate() -> None:
    """Test INCUBATION  CANDIDATE transition."""
    result = governance_transition("INCUBATION", Decision.KEEP)
    assert result == "CANDIDATE"


def test_governance_transition_incubation_to_retired() -> None:
    """Test INCUBATION  RETIRED transition."""
    result = governance_transition("INCUBATION", Decision.DROP)
    assert result == "RETIRED"


def test_governance_transition_candidate_to_live() -> None:
    """Test CANDIDATE  LIVE transition."""
    result = governance_transition("CANDIDATE", Decision.KEEP)
    assert result == "LIVE"


def test_governance_transition_retired_terminal() -> None:
    """Test that RETIRED is terminal state (no transitions)."""
    # RETIRED should remain RETIRED regardless of decision
    assert governance_transition("RETIRED", Decision.KEEP) == "RETIRED"
    assert governance_transition("RETIRED", Decision.DROP) == "RETIRED"
    assert governance_transition("RETIRED", Decision.FREEZE) == "RETIRED"



--------------------------------------------------------------------------------

FILE tests/test_governance_writer_contract.py
sha256(source_bytes) = 988778634fd916764f455b911833e025fcf3d615e0ddcda20ecc77404606585c
bytes = 7491
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for governance writer.

Tests that governance writer creates expected directory structure and files.
"""

from __future__ import annotations

import json
import tempfile
from pathlib import Path
from datetime import datetime, timezone

from FishBroWFS_V2.core.governance_schema import (
    Decision,
    EvidenceRef,
    GovernanceItem,
    GovernanceReport,
)
from FishBroWFS_V2.core.governance_writer import write_governance_artifacts


def test_governance_writer_creates_expected_tree() -> None:
    """
    Test that governance writer creates expected directory structure.
    
    Expected:
    - governance.json (machine-readable)
    - README.md (human-readable)
    - evidence_index.json (optional but recommended)
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        governance_dir = Path(tmpdir) / "governance" / "test-123"
        
        # Create sample report
        evidence = [
            EvidenceRef(
                run_id="stage1-123",
                stage_name="stage1_topk",
                artifact_paths=["manifest.json", "metrics.json", "winners.json"],
                key_metrics={
                    "param_id": 0,
                    "net_profit": 100.0,
                    "stage_planned_subsample": 0.1,
                    "param_subsample_rate": 0.1,
                    "params_effective": 100,
                },
            ),
        ]
        
        item = GovernanceItem(
            candidate_id="donchian_atr:abc123def456",
            decision=Decision.KEEP,
            reasons=[],
            evidence=evidence,
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="abc123def456",
        )
        
        report = GovernanceReport(
            items=[item],
            metadata={
                "governance_id": "gov-123",
                "season": "test_season",
                "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
                "git_sha": "abc123def456",
                "decisions": {"KEEP": 1, "FREEZE": 0, "DROP": 0},
            },
        )
        
        # Write artifacts
        write_governance_artifacts(governance_dir, report)
        
        # Verify files exist
        assert governance_dir.exists()
        assert (governance_dir / "governance.json").exists()
        assert (governance_dir / "README.md").exists()
        assert (governance_dir / "evidence_index.json").exists()
        
        # Verify governance.json is valid JSON
        with (governance_dir / "governance.json").open("r", encoding="utf-8") as f:
            governance_dict = json.load(f)
        
        assert "items" in governance_dict
        assert "metadata" in governance_dict
        assert len(governance_dict["items"]) == 1
        
        # Verify README.md contains key information
        readme_text = (governance_dir / "README.md").read_text(encoding="utf-8")
        assert "Governance Report" in readme_text
        assert "governance_id" in readme_text
        assert "Decision Summary" in readme_text
        assert "KEEP" in readme_text
        
        # Verify evidence_index.json is valid JSON
        with (governance_dir / "evidence_index.json").open("r", encoding="utf-8") as f:
            evidence_index = json.load(f)
        
        assert "governance_id" in evidence_index
        assert "evidence_by_candidate" in evidence_index


def test_governance_json_contains_subsample_fields_in_evidence() -> None:
    """
    Test that governance.json contains subsample fields in evidence.
    
    Critical requirement: subsample info must be in evidence chain.
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        governance_dir = Path(tmpdir) / "governance" / "test-123"
        
        # Create report with subsample fields in evidence
        evidence = [
            EvidenceRef(
                run_id="stage1-123",
                stage_name="stage1_topk",
                artifact_paths=["manifest.json", "metrics.json", "winners.json"],
                key_metrics={
                    "param_id": 0,
                    "net_profit": 100.0,
                    "stage_planned_subsample": 0.1,
                    "param_subsample_rate": 0.1,
                    "params_effective": 100,
                },
            ),
        ]
        
        item = GovernanceItem(
            candidate_id="donchian_atr:abc123def456",
            decision=Decision.KEEP,
            reasons=[],
            evidence=evidence,
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="abc123def456",
        )
        
        report = GovernanceReport(
            items=[item],
            metadata={
                "governance_id": "gov-123",
                "season": "test_season",
                "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
                "git_sha": "abc123def456",
                "decisions": {"KEEP": 1, "FREEZE": 0, "DROP": 0},
            },
        )
        
        # Write artifacts
        write_governance_artifacts(governance_dir, report)
        
        # Verify subsample fields are in governance.json
        with (governance_dir / "governance.json").open("r", encoding="utf-8") as f:
            governance_dict = json.load(f)
        
        item_dict = governance_dict["items"][0]
        evidence_dict = item_dict["evidence"][0]
        key_metrics = evidence_dict["key_metrics"]
        
        assert "stage_planned_subsample" in key_metrics
        assert "param_subsample_rate" in key_metrics
        assert "params_effective" in key_metrics


def test_readme_contains_freeze_reasons() -> None:
    """
    Test that README.md contains FREEZE reasons.
    
    Requirement: README must list FREEZE reasons (concise).
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        governance_dir = Path(tmpdir) / "governance" / "test-123"
        
        # Create report with FREEZE item
        evidence = [
            EvidenceRef(
                run_id="stage1-123",
                stage_name="stage1_topk",
                artifact_paths=["manifest.json", "metrics.json", "winners.json"],
                key_metrics={"param_id": 0, "net_profit": 100.0},
            ),
        ]
        
        freeze_item = GovernanceItem(
            candidate_id="donchian_atr:abc123def456",
            decision=Decision.FREEZE,
            reasons=["R3: density_5_over_threshold_3"],
            evidence=evidence,
            created_at=datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
            git_sha="abc123def456",
        )
        
        report = GovernanceReport(
            items=[freeze_item],
            metadata={
                "governance_id": "gov-123",
                "season": "test_season",
                "created_at": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
                "git_sha": "abc123def456",
                "decisions": {"KEEP": 0, "FREEZE": 1, "DROP": 0},
            },
        )
        
        # Write artifacts
        write_governance_artifacts(governance_dir, report)
        
        # Verify README contains FREEZE reasons
        readme_text = (governance_dir / "README.md").read_text(encoding="utf-8")
        assert "FREEZE Reasons" in readme_text
        assert "donchian_atr:abc123def456" in readme_text
        assert "density" in readme_text



--------------------------------------------------------------------------------

FILE tests/test_grid_runner_smoke.py
sha256(source_bytes) = bbb1e9cb851aa9097793804f6a454737bdbe6187accbe8fa71b14499b217cb5a
bytes = 2034
redacted = False
--------------------------------------------------------------------------------

import numpy as np

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def _ohlc():
    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)
    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)
    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)
    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)
    return o, h, l, c


def test_grid_runner_smoke_shapes_and_no_crash():
    o, h, l, c = _ohlc()

    # params: [channel_len, atr_len, stop_mult]
    params = np.array(
        [
            [2, 2, 1.0],
            [3, 2, 1.5],
            [99999, 3, 2.0],  # should produce 0 trades
            [2, 99999, 2.0],  # atr_len > n should be safe (atr_wilder returns all-NaN -> kernel => 0 trades)
        ],
        dtype=np.float64,
    )

    out = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=True)
    m = out["metrics"]
    order = out["order"]

    assert isinstance(m, np.ndarray)
    assert m.shape == (params.shape[0], 3)
    assert isinstance(order, np.ndarray)
    assert order.shape == (params.shape[0],)
    assert set(order.tolist()) == set(range(params.shape[0]))
    # Optional stronger assertion: at least one row should have 0 trades due to atr_len > n
    assert np.any(m[:, 1] == 0.0)


def test_grid_runner_sorting_toggle():
    o, h, l, c = _ohlc()
    params = np.array(
        [
            [3, 2, 1.5],
            [2, 2, 1.0],
            [2, 3, 2.0],
        ],
        dtype=np.float64,
    )

    out_sorted = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=True)
    out_unsorted = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=False)

    assert out_sorted["metrics"].shape == out_unsorted["metrics"].shape == (3, 3)
    assert out_sorted["order"].shape == out_unsorted["order"].shape == (3,)
    # unsorted order should be identity
    np.testing.assert_array_equal(out_unsorted["order"], np.array([0, 1, 2], dtype=np.int64))




--------------------------------------------------------------------------------

FILE tests/test_indicators_consistency.py
sha256(source_bytes) = 3e100efe153e5837212ebd5111839169e45e56f092c9b5d8cac2c94dac5d7ec1
bytes = 2649
redacted = False
--------------------------------------------------------------------------------

import numpy as np

from FishBroWFS_V2.indicators.numba_indicators import (
    rolling_max,
    rolling_min,
    atr_wilder,
)


def _py_rolling_max(arr: np.ndarray, window: int) -> np.ndarray:
    n = arr.shape[0]
    out = np.full(n, np.nan, dtype=np.float64)
    if window <= 0:
        return out
    for i in range(n):
        if i < window - 1:
            continue
        start = i - window + 1
        m = arr[start]
        for j in range(start + 1, i + 1):
            v = arr[j]
            if v > m:
                m = v
        out[i] = m
    return out


def _py_rolling_min(arr: np.ndarray, window: int) -> np.ndarray:
    n = arr.shape[0]
    out = np.full(n, np.nan, dtype=np.float64)
    if window <= 0:
        return out
    for i in range(n):
        if i < window - 1:
            continue
        start = i - window + 1
        m = arr[start]
        for j in range(start + 1, i + 1):
            v = arr[j]
            if v < m:
                m = v
        out[i] = m
    return out


def _py_atr_wilder(high, low, close, window):
    n = len(high)
    out = np.full(n, np.nan, dtype=np.float64)
    if window > n:
        return out
    tr = np.empty(n, dtype=np.float64)
    tr[0] = high[0] - low[0]
    for i in range(1, n):
        tr[i] = max(
            high[i] - low[i],
            abs(high[i] - close[i - 1]),
            abs(low[i] - close[i - 1]),
        )
    end = window
    out[end - 1] = np.mean(tr[:end])
    for i in range(window, n):
        out[i] = (out[i - 1] * (window - 1) + tr[i]) / window
    return out


def test_rolling_max_min_consistency():
    arr = np.array([1.0, 3.0, 2.0, 5.0, 4.0], dtype=np.float64)
    w = 3

    mx_py = _py_rolling_max(arr, w)
    mn_py = _py_rolling_min(arr, w)

    mx = rolling_max(arr, w)
    mn = rolling_min(arr, w)

    np.testing.assert_allclose(mx, mx_py, rtol=0.0, atol=0.0)
    np.testing.assert_allclose(mn, mn_py, rtol=0.0, atol=0.0)


def test_atr_wilder_consistency():
    high = np.array([10, 11, 12, 11, 13, 14], dtype=np.float64)
    low = np.array([9, 9, 10, 9, 11, 12], dtype=np.float64)
    close = np.array([9.5, 10.5, 11.0, 10.0, 12.0, 13.0], dtype=np.float64)
    w = 3

    atr_py = _py_atr_wilder(high, low, close, w)
    atr = atr_wilder(high, low, close, w)

    np.testing.assert_allclose(atr, atr_py, rtol=0.0, atol=1e-12)


def test_atr_wilder_window_gt_n_returns_all_nan():
    high = np.array([10, 11], dtype=np.float64)
    low = np.array([9, 10], dtype=np.float64)
    close = np.array([9.5, 10.5], dtype=np.float64)
    atr = atr_wilder(high, low, close, 999)
    assert atr.shape == (2,)
    assert np.all(np.isnan(atr))




--------------------------------------------------------------------------------

FILE tests/test_indicators_precompute_bit_exact.py
sha256(source_bytes) = 35966b690079de30950f50f22e680cf52c8e2f20a017dfd0d81fdd5e4bed2bba
bytes = 4768
redacted = False
--------------------------------------------------------------------------------

"""
Stage P2-2 Step B: Bit-exact test for precomputed indicators.

Verifies that using precomputed indicators produces identical results
to computing indicators inline in the kernel.
"""
from __future__ import annotations

from dataclasses import asdict, is_dataclass

import numpy as np

from FishBroWFS_V2.engine.types import BarArrays, Fill
from FishBroWFS_V2.strategy.kernel import DonchianAtrParams, PrecomputedIndicators, run_kernel_arrays
from FishBroWFS_V2.indicators.numba_indicators import rolling_max, rolling_min, atr_wilder


def _fill_to_tuple(f: Fill) -> tuple:
    """
    Convert Fill to a comparable tuple representation.
    
    Uses dataclasses.asdict for dataclass instances, falls back to __dict__ or repr.
    Returns sorted tuple to ensure deterministic comparison.
    """
    if is_dataclass(f):
        d = asdict(f)
    else:
        # fallback: __dict__ (for normal classes)
        d = dict(getattr(f, "__dict__", {}))
        if not d:
            # last resort: repr
            return (repr(f),)
    # Fixed ordering to avoid dict order differences
    return tuple(sorted(d.items()))


def test_indicators_precompute_bit_exact() -> None:
    """
    Test that precomputed indicators produce bit-exact results.
    
    Strategy:
    - Generate random bars
    - Choose a channel_len and atr_len
    - Run kernel twice:
      A: Without precomputation (precomp=None)
      B: With precomputation (precomp=PrecomputedIndicators(...))
    - Compare: donch_hi/lo/atr arrays, metrics, fills, equity
    """
    # Generate random bars
    rng = np.random.default_rng(42)
    n_bars = 500
    close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
    open_ = (high + low) / 2
    
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    bars = BarArrays(
        open=open_.astype(np.float64),
        high=high.astype(np.float64),
        low=low.astype(np.float64),
        close=close.astype(np.float64),
    )
    
    # Choose test parameters
    ch_len = 20
    atr_len = 10
    params = DonchianAtrParams(channel_len=ch_len, atr_len=atr_len, stop_mult=1.0)
    
    # Pre-compute indicators (same logic as runner_grid)
    donch_hi_precomp = rolling_max(bars.high, ch_len)
    donch_lo_precomp = rolling_min(bars.low, ch_len)
    atr_precomp = atr_wilder(bars.high, bars.low, bars.close, atr_len)
    
    precomp = PrecomputedIndicators(
        donch_hi=donch_hi_precomp,
        donch_lo=donch_lo_precomp,
        atr=atr_precomp,
    )
    
    # Run A: Without precomputation
    result_a = run_kernel_arrays(
        bars=bars,
        params=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        precomp=None,
    )
    
    # Run B: With precomputation
    result_b = run_kernel_arrays(
        bars=bars,
        params=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        precomp=precomp,
    )
    
    # Verify indicators are bit-exact (if we could access them)
    # Note: We can't directly access internal arrays, but we verify outputs
    
    # Verify metrics are identical
    metrics_a = result_a["metrics"]
    metrics_b = result_b["metrics"]
    assert metrics_a["net_profit"] == metrics_b["net_profit"], "net_profit must be identical"
    assert metrics_a["trades"] == metrics_b["trades"], "trades must be identical"
    assert metrics_a["max_dd"] == metrics_b["max_dd"], "max_dd must be identical"
    
    # Verify fills are identical
    fills_a = result_a["fills"]
    fills_b = result_b["fills"]
    assert len(fills_a) == len(fills_b), "fills count must be identical"
    for i, (fill_a, fill_b) in enumerate(zip(fills_a, fills_b)):
        assert _fill_to_tuple(fill_a) == _fill_to_tuple(fill_b), f"fill[{i}] must be identical"
    
    # Verify equity arrays are bit-exact
    equity_a = result_a["equity"]
    equity_b = result_b["equity"]
    assert equity_a.shape == equity_b.shape, "equity shape must be identical"
    np.testing.assert_array_equal(equity_a, equity_b, "equity must be bit-exact")
    
    # Verify pnl arrays are bit-exact
    pnl_a = result_a["pnl"]
    pnl_b = result_b["pnl"]
    assert pnl_a.shape == pnl_b.shape, "pnl shape must be identical"
    np.testing.assert_array_equal(pnl_a, pnl_b, "pnl must be bit-exact")
    
    # Verify observability counts are identical
    obs_a = result_a.get("_obs", {})
    obs_b = result_b.get("_obs", {})
    assert obs_a.get("intents_total") == obs_b.get("intents_total"), "intents_total must be identical"
    assert obs_a.get("fills_total") == obs_b.get("fills_total"), "fills_total must be identical"



--------------------------------------------------------------------------------

FILE tests/test_jobs_db_concurrency_smoke.py
sha256(source_bytes) = 33b32e0a4b8292527f9c17e82e91818dbbe0754b155301a2ef92c66818d0915a
bytes = 1805
redacted = False
--------------------------------------------------------------------------------

"""Smoke test for jobs_db concurrency (WAL + retry + state machine)."""

from __future__ import annotations

import multiprocessing as mp
from pathlib import Path

import pytest

from FishBroWFS_V2.control.jobs_db import (
    append_log,
    create_job,
    init_db,
    list_jobs,
    mark_done,
    mark_running,
)
from FishBroWFS_V2.control.types import DBJobSpec


def _proc(db_path: str, n: int) -> None:
    """Worker process: create n jobs and complete them."""
    p = Path(db_path)
    for i in range(n):
        spec = DBJobSpec(
            season="test",
            dataset_id="test",
            outputs_root="outputs",
            config_snapshot={"test": i},
            config_hash=f"hash{i}",
        )
        job_id = create_job(p, spec)
        mark_running(p, job_id, pid=1000 + i)
        append_log(p, job_id, f"hi {i}")
        mark_done(p, job_id, run_id=f"R{i}", report_link=f"/b5?i={i}")


@pytest.mark.parametrize("n", [50])
def test_jobs_db_concurrency_smoke(tmp_path: Path, n: int) -> None:
    """
    Test concurrent job creation and completion across multiple processes.
    
    This test ensures WAL mode, retry logic, and state machine work correctly
    under concurrent access.
    """
    db = tmp_path / "jobs.db"
    init_db(db)

    ps = [mp.Process(target=_proc, args=(str(db), n)) for _ in range(2)]
    for p in ps:
        p.start()
    for p in ps:
        p.join()

    for p in ps:
        assert p.exitcode == 0, f"Process {p.pid} exited with code {p.exitcode}"

    # Verify job count
    jobs = list_jobs(db, limit=1000)
    assert len(jobs) == 2 * n, f"Expected {2 * n} jobs, got {len(jobs)}"

    # Verify all jobs are DONE
    for job in jobs:
        assert job.status.value == "DONE", f"Job {job.job_id} status is {job.status}, expected DONE"



--------------------------------------------------------------------------------

FILE tests/test_jobs_db_concurrency_wal.py
sha256(source_bytes) = 840df71bf70162023afdf263c4b196da0ef316d78f823f3c342280a279e5dd6a
bytes = 1693
redacted = False
--------------------------------------------------------------------------------

"""Tests for jobs_db concurrency with WAL mode.

Tests concurrent writes from multiple processes to ensure no database locked errors.
"""

from __future__ import annotations

import multiprocessing as mp
from pathlib import Path

import pytest

import os

from FishBroWFS_V2.control.jobs_db import append_log, create_job, init_db, mark_done, update_running
from FishBroWFS_V2.control.types import DBJobSpec


def _worker(db_path: str, n: int) -> None:
    """Worker function: create job, append log, mark done."""
    p = Path(db_path)
    pid = os.getpid()
    for i in range(n):
        spec = DBJobSpec(
            season="2026Q1",
            dataset_id="test_dataset",
            outputs_root="/tmp/outputs",
            config_snapshot={"test": f"config_{i}"},
            config_hash=f"hash_{i}",
        )
        job_id = create_job(p, spec, tags=["test", f"worker_{i}"])
        append_log(p, job_id, f"hello {i}")
        update_running(p, job_id, pid=pid)  #  QUEUED  RUNNING
        mark_done(p, job_id, run_id=f"R_{i}", report_link=f"/b5?x=y&i={i}")


@pytest.mark.parametrize("n", [50])
def test_jobs_db_concurrent_writes(tmp_path: Path, n: int) -> None:
    """
    Test concurrent writes from multiple processes.
    
    Two processes each create n jobs, append logs, and mark done.
    Should not raise database locked errors.
    """
    db = tmp_path / "jobs.db"
    init_db(db)

    procs = [mp.Process(target=_worker, args=(str(db), n)) for _ in range(2)]
    for pr in procs:
        pr.start()
    for pr in procs:
        pr.join()

    for pr in procs:
        assert pr.exitcode == 0, f"Process {pr.pid} exited with code {pr.exitcode}"



--------------------------------------------------------------------------------

FILE tests/test_jobs_db_tags.py
sha256(source_bytes) = 35b22e430f4c8033bc54213c4fb704357c5d4b7b8bb2994cb637137740a7320c
bytes = 5536
redacted = False
--------------------------------------------------------------------------------

"""Tests for jobs_db tags functionality.

Tests:
1. Create job with tags
2. Read job with tags
3. Old rows without tags fallback to []
4. search_by_tag query helper
"""

from __future__ import annotations

import tempfile
from pathlib import Path

import pytest

from FishBroWFS_V2.control.jobs_db import (
    create_job,
    get_job,
    init_db,
    list_jobs,
    search_by_tag,
)
from FishBroWFS_V2.control.types import DBJobSpec


@pytest.fixture
def temp_db(tmp_path: Path) -> Path:
    """Create temporary database for testing."""
    db_path = tmp_path / "test_jobs.db"
    init_db(db_path)
    return db_path


def test_create_job_with_tags(temp_db: Path) -> None:
    """Test creating a job with tags."""
    spec = DBJobSpec(
        season="2026Q1",
        dataset_id="test_dataset",
        outputs_root="/tmp/outputs",
        config_snapshot={"test": "config"},
        config_hash="abc123",
    )
    
    job_id = create_job(temp_db, spec, tags=["production", "high-priority"])
    
    # Read back and verify tags
    record = get_job(temp_db, job_id)
    assert record.tags == ["production", "high-priority"]


def test_create_job_without_tags(temp_db: Path) -> None:
    """Test creating a job without tags (defaults to empty list)."""
    spec = DBJobSpec(
        season="2026Q1",
        dataset_id="test_dataset",
        outputs_root="/tmp/outputs",
        config_snapshot={"test": "config"},
        config_hash="abc123",
    )
    
    job_id = create_job(temp_db, spec)
    
    # Read back and verify tags is empty list
    record = get_job(temp_db, job_id)
    assert record.tags == []


def test_read_job_with_tags(temp_db: Path) -> None:
    """Test reading a job with tags."""
    spec = DBJobSpec(
        season="2026Q1",
        dataset_id="test_dataset",
        outputs_root="/tmp/outputs",
        config_snapshot={"test": "config"},
        config_hash="abc123",
    )
    
    job_id = create_job(temp_db, spec, tags=["test", "debug"])
    
    # Read back
    record = get_job(temp_db, job_id)
    assert isinstance(record.tags, list)
    assert "test" in record.tags
    assert "debug" in record.tags
    assert len(record.tags) == 2


def test_old_rows_fallback_to_empty_tags(temp_db: Path) -> None:
    """
    Test that old rows without tags_json fallback to empty list.
    
    This tests backward compatibility: existing jobs without tags_json
    should be readable and have tags=[].
    """
    import sqlite3
    import json
    
    # Manually insert a job without tags_json (simulating old schema)
    conn = sqlite3.connect(str(temp_db))
    try:
        # Insert job with old schema (no tags_json)
        conn.execute("""
            INSERT INTO jobs (
                job_id, status, created_at, updated_at,
                season, dataset_id, outputs_root, config_hash,
                config_snapshot_json, requested_pause
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            "old-job-123",
            "QUEUED",
            "2026-01-01T00:00:00Z",
            "2026-01-01T00:00:00Z",
            "2026Q1",
            "test_dataset",
            "/tmp/outputs",
            "abc123",
            json.dumps({"test": "config"}),
            0,
        ))
        conn.commit()
    finally:
        conn.close()
    
    # Read back - should have tags=[]
    record = get_job(temp_db, "old-job-123")
    assert record.tags == []


def test_search_by_tag(temp_db: Path) -> None:
    """Test search_by_tag query helper."""
    spec1 = DBJobSpec(
        season="2026Q1",
        dataset_id="test_dataset",
        outputs_root="/tmp/outputs",
        config_snapshot={"test": "config1"},
        config_hash="abc123",
    )
    spec2 = DBJobSpec(
        season="2026Q1",
        dataset_id="test_dataset",
        outputs_root="/tmp/outputs",
        config_snapshot={"test": "config2"},
        config_hash="def456",
    )
    spec3 = DBJobSpec(
        season="2026Q1",
        dataset_id="test_dataset",
        outputs_root="/tmp/outputs",
        config_snapshot={"test": "config3"},
        config_hash="ghi789",
    )
    
    # Create jobs with different tags
    job1 = create_job(temp_db, spec1, tags=["production", "high-priority"])
    job2 = create_job(temp_db, spec2, tags=["staging", "low-priority"])
    job3 = create_job(temp_db, spec3, tags=["production", "medium-priority"])
    
    # Search for "production" tag
    results = search_by_tag(temp_db, "production")
    assert len(results) == 2
    job_ids = {r.job_id for r in results}
    assert job1 in job_ids
    assert job3 in job_ids
    assert job2 not in job_ids
    
    # Search for "staging" tag
    results = search_by_tag(temp_db, "staging")
    assert len(results) == 1
    assert results[0].job_id == job2
    
    # Search for non-existent tag
    results = search_by_tag(temp_db, "non-existent")
    assert len(results) == 0


def test_list_jobs_includes_tags(temp_db: Path) -> None:
    """Test that list_jobs includes tags in records."""
    spec = DBJobSpec(
        season="2026Q1",
        dataset_id="test_dataset",
        outputs_root="/tmp/outputs",
        config_snapshot={"test": "config"},
        config_hash="abc123",
    )
    
    job_id = create_job(temp_db, spec, tags=["test", "debug"])
    
    # List jobs
    jobs = list_jobs(temp_db, limit=10)
    assert len(jobs) >= 1
    
    # Find our job
    our_job = next((j for j in jobs if j.job_id == job_id), None)
    assert our_job is not None
    assert our_job.tags == ["test", "debug"]



--------------------------------------------------------------------------------

FILE tests/test_json_pointer.py
sha256(source_bytes) = c2719a151b471bc9e2c365cb6ad48eaa2c9d7f74c2540157eb828c1ccf1455cc
bytes = 5603
redacted = False
--------------------------------------------------------------------------------

"""Tests for JSON Pointer resolver.

Tests normal pointer, list index, missing keys, and never-raise contract.
"""

from __future__ import annotations

import pytest

from FishBroWFS_V2.gui.viewer.json_pointer import resolve_json_pointer


def test_normal_pointer() -> None:
    """Test normal object key pointer."""
    data = {
        "a": {
            "b": {
                "c": "value"
            }
        }
    }
    
    found, value = resolve_json_pointer(data, "/a/b/c")
    assert found is True
    assert value == "value"
    
    found, value = resolve_json_pointer(data, "/a/b")
    assert found is True
    assert value == {"c": "value"}


def test_list_index() -> None:
    """Test list index in pointer."""
    data = {
        "items": [
            {"name": "first"},
            {"name": "second"},
        ]
    }
    
    found, value = resolve_json_pointer(data, "/items/0/name")
    assert found is True
    assert value == "first"
    
    found, value = resolve_json_pointer(data, "/items/1/name")
    assert found is True
    assert value == "second"
    
    found, value = resolve_json_pointer(data, "/items/0")
    assert found is True
    assert value == {"name": "first"}


def test_list_index_out_of_bounds() -> None:
    """Test list index out of bounds."""
    data = {
        "items": [1, 2, 3]
    }
    
    found, value = resolve_json_pointer(data, "/items/10")
    assert found is False
    assert value is None
    
    found, value = resolve_json_pointer(data, "/items/-1")
    assert found is False
    assert value is None


def test_missing_key() -> None:
    """Test missing key in pointer."""
    data = {
        "a": {
            "b": "value"
        }
    }
    
    found, value = resolve_json_pointer(data, "/a/c")
    assert found is False
    assert value is None
    
    found, value = resolve_json_pointer(data, "/x/y")
    assert found is False
    assert value is None


def test_root_pointer_disabled() -> None:
    """Test root pointer is disabled (by design for Viewer UX)."""
    data = {"a": 1, "b": 2}
    
    # Root pointer "/" is intentionally disabled
    found, value = resolve_json_pointer(data, "/")
    assert found is False
    assert value is None
    
    # Empty string is also disabled
    found, value = resolve_json_pointer(data, "")
    assert found is False
    assert value is None


def test_invalid_pointer_format() -> None:
    """Test invalid pointer format."""
    data = {"a": 1}
    
    # Missing leading slash
    found, value = resolve_json_pointer(data, "a/b")
    assert found is False
    assert value is None


def test_nested_list_and_dict() -> None:
    """Test nested list and dict combination."""
    data = {
        "results": [
            {
                "metrics": {
                    "score": 100
                }
            },
            {
                "metrics": {
                    "score": 200
                }
            }
        ]
    }
    
    found, value = resolve_json_pointer(data, "/results/0/metrics/score")
    assert found is True
    assert value == 100
    
    found, value = resolve_json_pointer(data, "/results/1/metrics/score")
    assert found is True
    assert value == 200


def test_never_raises() -> None:
    """Test that resolve_json_pointer never raises exceptions."""
    # Test with None data
    found, value = resolve_json_pointer(None, "/a")  # type: ignore
    assert found is False
    assert value is None
    
    # Test with invalid data types
    found, value = resolve_json_pointer("string", "/a")  # type: ignore
    assert found is False
    assert value is None
    
    # Test with empty dict (valid, but key missing)
    found, value = resolve_json_pointer({}, "/a")
    assert found is False
    assert value is None
    
    # Test with invalid pointer type
    found, value = resolve_json_pointer({"a": 1}, None)  # type: ignore
    assert found is False
    assert value is None
    
    # Test with empty string pointer
    found, value = resolve_json_pointer({"a": 1}, "")
    assert found is False
    assert value is None
    
    # Test with root pointer (disabled)
    found, value = resolve_json_pointer({"a": 1}, "/")
    assert found is False
    assert value is None
    
    # Test with valid pointer
    found, value = resolve_json_pointer({"a": 1}, "/a")
    assert found is True
    assert value == 1


def test_critical_scenarios() -> None:
    """Test critical scenarios that must pass."""
    data = {"a": 1}
    
    # Scenario 1: None pointer
    found, value = resolve_json_pointer(data, None)  # type: ignore
    assert found is False
    assert value is None
    
    # Scenario 2: Empty string pointer
    found, value = resolve_json_pointer(data, "")
    assert found is False
    assert value is None
    
    # Scenario 3: Root pointer (disabled by design)
    found, value = resolve_json_pointer(data, "/")
    assert found is False
    assert value is None
    
    # Scenario 4: Valid pointer
    found, value = resolve_json_pointer(data, "/a")
    assert found is True
    assert value == 1


def test_intermediate_type_mismatch() -> None:
    """Test intermediate type mismatch."""
    data = {
        "items": "not_a_list"
    }
    
    # Try to access list index on string
    found, value = resolve_json_pointer(data, "/items/0")
    assert found is False
    assert value is None
    
    data = {
        "items": [1, 2, 3]
    }
    
    # Try to access dict key on list
    found, value = resolve_json_pointer(data, "/items/key")
    assert found is False
    assert value is None



--------------------------------------------------------------------------------

FILE tests/test_kbar_anchor_alignment.py
sha256(source_bytes) = 1709f372ed863c6a7092438947e266af799592d56ca4095edbcd42db286fad6d
bytes = 3230
redacted = False
--------------------------------------------------------------------------------

"""Test K-bar aggregation: anchor alignment to Session.start."""

from __future__ import annotations

from pathlib import Path

import pandas as pd
import pytest

from FishBroWFS_V2.data.session.kbar import aggregate_kbar
from FishBroWFS_V2.data.session.loader import load_session_profile


@pytest.fixture
def mnq_profile() -> Path:
    """Load CME.MNQ session profile."""
    profile_path = Path(__file__).parent.parent / "src" / "FishBroWFS_V2" / "data" / "profiles" / "CME_MNQ_TPE_v1.yaml"
    return profile_path


def test_anchor_to_session_start_60m(mnq_profile: Path) -> None:
    """Test 60-minute bars are anchored to session start (08:45:00)."""
    profile = load_session_profile(mnq_profile)
    
    # Create bars starting from session start
    df = pd.DataFrame({
        "ts_str": [
            "2013/1/1 08:45:00",  # Session start
            "2013/1/1 08:50:00",
            "2013/1/1 09:00:00",
            "2013/1/1 09:30:00",
            "2013/1/1 09:45:00",  # Should be start of next 60m bucket
            "2013/1/1 10:00:00",
        ],
        "open": [100.0, 101.0, 102.0, 103.0, 104.0, 105.0],
        "high": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5],
        "low": [99.5, 100.5, 101.5, 102.5, 103.5, 104.5],
        "close": [100.5, 101.5, 102.5, 103.5, 104.5, 105.5],
        "volume": [1000, 1100, 1200, 1300, 1400, 1500],
    })
    
    result = aggregate_kbar(df, 60, profile)
    
    # Verify first bar is anchored to session start
    first_bar_time = result["ts_str"].iloc[0].split(" ")[1]
    assert first_bar_time == "08:45:00", f"First bar should be anchored to 08:45:00, got {first_bar_time}"
    
    # Verify subsequent bars are at 60-minute intervals from start
    if len(result) > 1:
        second_bar_time = result["ts_str"].iloc[1].split(" ")[1]
        assert second_bar_time == "09:45:00", f"Second bar should be at 09:45:00, got {second_bar_time}"


def test_anchor_to_session_start_30m(mnq_profile: Path) -> None:
    """Test 30-minute bars are anchored to session start (08:45:00)."""
    profile = load_session_profile(mnq_profile)
    
    # Create bars starting from session start
    df = pd.DataFrame({
        "ts_str": [
            "2013/1/1 08:45:00",  # Session start
            "2013/1/1 08:50:00",
            "2013/1/1 09:00:00",
            "2013/1/1 09:15:00",  # Should be start of next 30m bucket
            "2013/1/1 09:30:00",
        ],
        "open": [100.0, 101.0, 102.0, 103.0, 104.0],
        "high": [100.5, 101.5, 102.5, 103.5, 104.5],
        "low": [99.5, 100.5, 101.5, 102.5, 103.5],
        "close": [100.5, 101.5, 102.5, 103.5, 104.5],
        "volume": [1000, 1100, 1200, 1300, 1400],
    })
    
    result = aggregate_kbar(df, 30, profile)
    
    # Verify first bar is anchored to session start
    first_bar_time = result["ts_str"].iloc[0].split(" ")[1]
    assert first_bar_time == "08:45:00", f"First bar should be anchored to 08:45:00, got {first_bar_time}"
    
    # Verify subsequent bars are at 30-minute intervals from start
    if len(result) > 1:
        second_bar_time = result["ts_str"].iloc[1].split(" ")[1]
        assert second_bar_time == "09:15:00", f"Second bar should be at 09:15:00, got {second_bar_time}"



--------------------------------------------------------------------------------

FILE tests/test_kbar_no_cross_session.py
sha256(source_bytes) = 5d4aefe822b209a2af86c022ba6cb0a21f1205b55b5167013b4697a57c924aba
bytes = 3188
redacted = False
--------------------------------------------------------------------------------

"""Test K-bar aggregation: no cross-session aggregation."""

from __future__ import annotations

from pathlib import Path

import pandas as pd
import pytest

from FishBroWFS_V2.data.session.kbar import aggregate_kbar
from FishBroWFS_V2.data.session.loader import load_session_profile


@pytest.fixture
def mnq_profile() -> Path:
    """Load CME.MNQ session profile."""
    profile_path = Path(__file__).parent.parent / "src" / "FishBroWFS_V2" / "data" / "profiles" / "CME_MNQ_TPE_v1.yaml"
    return profile_path


def test_no_cross_session_60m(mnq_profile: Path) -> None:
    """Test 60-minute bars do not cross session boundaries."""
    profile = load_session_profile(mnq_profile)
    
    # Create bars that span DAY session end and NIGHT session start
    df = pd.DataFrame({
        "ts_str": [
            "2013/1/1 13:30:00",  # DAY session
            "2013/1/1 13:40:00",  # DAY session
            "2013/1/1 13:44:00",  # DAY session (last bar before end)
            "2013/1/1 21:00:00",  # NIGHT session start
            "2013/1/1 21:10:00",  # NIGHT session
        ],
        "open": [100.0, 101.0, 102.0, 103.0, 104.0],
        "high": [100.5, 101.5, 102.5, 103.5, 104.5],
        "low": [99.5, 100.5, 101.5, 102.5, 103.5],
        "close": [100.5, 101.5, 102.5, 103.5, 104.5],
        "volume": [1000, 1100, 1200, 1300, 1400],
    })
    
    result = aggregate_kbar(df, 60, profile)
    
    # Verify no bar contains both DAY and NIGHT session bars
    # Use session column instead of string contains (more robust)
    assert "session" in result.columns, "Result must include session column"
    
    # Must have both DAY and NIGHT sessions
    assert set(result["session"].dropna()) == {"DAY", "NIGHT"}, (
        f"Should have both DAY and NIGHT sessions, got {set(result['session'].dropna())}"
    )
    
    day_bars = result[result["session"] == "DAY"]
    night_bars = result[result["session"] == "NIGHT"]
    
    assert len(day_bars) > 0, "Should have DAY session bars"
    assert len(night_bars) > 0, "Should have NIGHT session bars"
    
    # Verify no bar mixes sessions (each row has exactly one session)
    assert result["session"].notna().all(), "All bars must have a session label"
    assert len(result[result["session"].isna()]) == 0, "No bar should have session=None"


def test_no_cross_session_30m(mnq_profile: Path) -> None:
    """Test 30-minute bars do not cross session boundaries."""
    profile = load_session_profile(mnq_profile)
    
    # Create bars at DAY session end
    df = pd.DataFrame({
        "ts_str": [
            "2013/1/1 13:30:00",
            "2013/1/1 13:40:00",
            "2013/1/1 13:44:00",  # Last bar in DAY session
        ],
        "open": [100.0, 101.0, 102.0],
        "high": [100.5, 101.5, 102.5],
        "low": [99.5, 100.5, 101.5],
        "close": [100.5, 101.5, 102.5],
        "volume": [1000, 1100, 1200],
    })
    
    result = aggregate_kbar(df, 30, profile)
    
    # All bars should be in DAY session
    assert "session" in result.columns, "Result must include session column"
    assert all(result["session"] == "DAY"), f"All bars should be DAY session, got {result['session'].unique()}"



--------------------------------------------------------------------------------

FILE tests/test_kernel_parity_contract.py
sha256(source_bytes) = 9f0a8a76e7d4a92d6960f73bf1b1a55ec4af29b0ec77fd9beb5fce08fd91f87c
bytes = 17285
redacted = False
--------------------------------------------------------------------------------

"""Kernel parity contract tests - Phase 4 Stage C.

These tests ensure that Cursor kernel results are bit-level identical to matcher_core.
This is a critical contract: any deviation indicates a semantic bug.

Tests use simulate_run() unified entry point to ensure we test the actual API used in production.
"""

import numpy as np

from FishBroWFS_V2.data.layout import normalize_bars
from FishBroWFS_V2.engine.simulate import simulate_run
from FishBroWFS_V2.engine.types import OrderIntent, OrderKind, OrderRole, Side


def _bars1(o, h, l, c):
    """Helper to create single-bar BarArrays."""
    return normalize_bars(
        np.array([o], dtype=np.float64),
        np.array([h], dtype=np.float64),
        np.array([l], dtype=np.float64),
        np.array([c], dtype=np.float64),
    )


def _bars2(o0, h0, l0, c0, o1, h1, l1, c1):
    """Helper to create two-bar BarArrays."""
    return normalize_bars(
        np.array([o0, o1], dtype=np.float64),
        np.array([h0, h1], dtype=np.float64),
        np.array([l0, l1], dtype=np.float64),
        np.array([c0, c1], dtype=np.float64),
    )


def _bars3(o0, h0, l0, c0, o1, h1, l1, c1, o2, h2, l2, c2):
    """Helper to create three-bar BarArrays."""
    return normalize_bars(
        np.array([o0, o1, o2], dtype=np.float64),
        np.array([h0, h1, h2], dtype=np.float64),
        np.array([l0, l1, l2], dtype=np.float64),
        np.array([c0, c1, c2], dtype=np.float64),
    )


def _compute_position_path(fills):
    """
    Compute position path from fills sequence.
    
    Returns list of (bar_index, position) tuples where position is:
    - 0: flat
    - 1: long
    - -1: short
    """
    pos_path = []
    current_pos = 0
    
    # Group fills by bar_index
    fills_by_bar = {}
    for fill in fills:
        bar_idx = fill.bar_index
        if bar_idx not in fills_by_bar:
            fills_by_bar[bar_idx] = []
        fills_by_bar[bar_idx].append(fill)
    
    # Process fills chronologically
    for bar_idx in sorted(fills_by_bar.keys()):
        bar_fills = fills_by_bar[bar_idx]
        # Sort by role (ENTRY first), then kind, then order_id
        bar_fills.sort(key=lambda f: (
            0 if f.role == OrderRole.ENTRY else 1,
            0 if f.kind == OrderKind.STOP else 1,
            f.order_id
        ))
        
        for fill in bar_fills:
            if fill.role == OrderRole.ENTRY:
                if fill.side == Side.BUY:
                    current_pos = 1
                else:
                    current_pos = -1
            elif fill.role == OrderRole.EXIT:
                current_pos = 0
        
        pos_path.append((bar_idx, current_pos))
    
    return pos_path


def _assert_fills_identical(cursor_fills, reference_fills):
    """Assert that two fill sequences are bit-level identical."""
    assert len(cursor_fills) == len(reference_fills), (
        f"Fill count mismatch: cursor={len(cursor_fills)}, reference={len(reference_fills)}"
    )
    
    for i, (c_fill, r_fill) in enumerate(zip(cursor_fills, reference_fills)):
        assert c_fill.bar_index == r_fill.bar_index, (
            f"Fill {i}: bar_index mismatch: cursor={c_fill.bar_index}, reference={r_fill.bar_index}"
        )
        assert c_fill.role == r_fill.role, (
            f"Fill {i}: role mismatch: cursor={c_fill.role}, reference={r_fill.role}"
        )
        assert c_fill.kind == r_fill.kind, (
            f"Fill {i}: kind mismatch: cursor={c_fill.kind}, reference={r_fill.kind}"
        )
        assert c_fill.side == r_fill.side, (
            f"Fill {i}: side mismatch: cursor={c_fill.side}, reference={r_fill.side}"
        )
        assert c_fill.price == r_fill.price, (
            f"Fill {i}: price mismatch: cursor={c_fill.price}, reference={r_fill.price}"
        )
        assert c_fill.qty == r_fill.qty, (
            f"Fill {i}: qty mismatch: cursor={c_fill.qty}, reference={r_fill.qty}"
        )
        assert c_fill.order_id == r_fill.order_id, (
            f"Fill {i}: order_id mismatch: cursor={c_fill.order_id}, reference={r_fill.order_id}"
        )


def _assert_position_path_identical(cursor_fills, reference_fills):
    """Assert that position paths are identical."""
    cursor_path = _compute_position_path(cursor_fills)
    reference_path = _compute_position_path(reference_fills)
    
    assert cursor_path == reference_path, (
        f"Position path mismatch:\n"
        f"  cursor: {cursor_path}\n"
        f"  reference: {reference_path}"
    )


def test_parity_next_bar_activation():
    """Test next-bar activation rule: order created at bar N activates at bar N+1."""
    # Order created at bar 0, should activate at bar 1
    bars = _bars2(
        100, 105, 95, 100,  # bar 0: high=105 would hit stop 102, but order not active yet
        100, 105, 95, 100,  # bar 1: order activates, should fill
    )
    intents = [
        OrderIntent(order_id=1, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    _assert_position_path_identical(cursor_result.fills, reference_result.fills)
    
    # Verify: should fill at bar 1, not bar 0
    assert len(cursor_result.fills) == 1
    assert cursor_result.fills[0].bar_index == 1


def test_parity_stop_fill_price_exact():
    """Test stop fill price = stop_price (not max(open, stop_price))."""
    # Buy stop at 100, open=95, high=105 -> should fill at 100 (stop_price), not 105
    bars = _bars1(95, 105, 90, 100)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].price == 100.0  # stop_price, not high


def test_parity_stop_fill_price_gap_up():
    """Test stop fill price on gap up: fill at open if open >= stop_price."""
    # Buy stop at 100, open=105 (gap up) -> should fill at 105 (open), not 100
    bars = _bars1(105, 110, 105, 108)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].price == 105.0  # open (gap branch)


def test_parity_stop_fill_price_gap_down():
    """Test stop fill price on gap down: fill at open if open <= stop_price."""
    # Sell stop at 100, open=90 (gap down) -> should fill at 90 (open), not 100
    bars = _bars2(
        100, 100, 100, 100,  # bar 0: enter long
        90, 95, 80, 85,      # bar 1: exit stop gap down
    )
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    _assert_position_path_identical(cursor_result.fills, reference_result.fills)
    # Exit fill should be at open (90) due to gap down
    assert cursor_result.fills[1].price == 90.0


def test_parity_same_bar_entry_then_exit():
    """Test same-bar entry then exit is allowed."""
    # Same bar: entry buy stop 105, exit sell stop 95
    # Bar: O=100 H=120 L=90
    # Entry: Buy Stop 105 -> fills at 105
    # Exit: Sell Stop 95 -> fills at 95 (after entry)
    bars = _bars1(100, 120, 90, 110)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),
        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    _assert_position_path_identical(cursor_result.fills, reference_result.fills)
    
    assert len(cursor_result.fills) == 2
    assert cursor_result.fills[0].role == OrderRole.ENTRY
    assert cursor_result.fills[0].price == 105.0
    assert cursor_result.fills[1].role == OrderRole.EXIT
    assert cursor_result.fills[1].price == 95.0
    assert cursor_result.fills[0].bar_index == cursor_result.fills[1].bar_index


def test_parity_stop_priority_over_limit():
    """Test STOP priority over LIMIT (same role, same bar)."""
    # Entry: Buy Stop 102 and Buy Limit 110 both triggerable
    # STOP must win
    bars = _bars1(100, 115, 95, 105)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.BUY, price=110.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].kind == OrderKind.STOP
    assert cursor_result.fills[0].order_id == 1


def test_parity_stop_priority_exit():
    """Test STOP priority over LIMIT on exit."""
    # Enter long first, then exit with both stop and limit triggerable
    # STOP must win
    bars = _bars2(
        100, 100, 100, 100,  # bar 0: enter long
        100, 110, 80, 90,    # bar 1: exit stop 90 and exit limit 110 both triggerable
    )
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=90.0),
        OrderIntent(order_id=3, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.LIMIT, side=Side.SELL, price=110.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    _assert_position_path_identical(cursor_result.fills, reference_result.fills)
    
    # Exit fill should be STOP
    assert cursor_result.fills[1].kind == OrderKind.STOP
    assert cursor_result.fills[1].order_id == 2


def test_parity_order_id_tie_break():
    """Test order_id tie-break when kind is same."""
    # Two STOP orders, lower order_id should win
    bars = _bars1(100, 110, 95, 105)
    intents = [
        OrderIntent(order_id=2, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].order_id == 1  # Lower order_id wins


def test_parity_limit_gap_down_better_fill():
    """Test limit order gap down: fill at open if better."""
    # Buy limit at 100, open=90 (gap down) -> should fill at 90 (open), not 100
    bars = _bars1(90, 95, 85, 92)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.BUY, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].price == 90.0  # open (better fill)


def test_parity_limit_gap_up_better_fill():
    """Test limit order gap up: fill at open if better."""
    # Sell limit at 100, open=105 (gap up) -> should fill at 105 (open), not 100
    bars = _bars1(105, 110, 100, 108)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.LIMIT, side=Side.SELL, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].price == 105.0  # open (better fill)


def test_parity_no_fill_when_not_touched():
    """Test no fill when price not touched."""
    bars = _bars1(90, 95, 90, 92)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert len(cursor_result.fills) == 0


def test_parity_open_equals_stop_gap_branch():
    """Test open equals stop price: gap branch but same price."""
    bars = _bars1(100, 100, 90, 95)
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    assert cursor_result.fills[0].price == 100.0  # open == stop_price


def test_parity_multiple_bars_complex():
    """Test complex multi-bar scenario with entry and exit."""
    bars = _bars3(
        100, 105, 95, 100,   # bar 0: enter long at 102 (buy stop)
        100, 110, 80, 90,    # bar 1: exit stop 90 triggers
        95, 100, 90, 95,     # bar 2: no fills
    )
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=102.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=90.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    _assert_position_path_identical(cursor_result.fills, reference_result.fills)
    
    # Verify position path
    pos_path = _compute_position_path(cursor_result.fills)
    assert pos_path == [(0, 1), (1, 0)]  # Enter at bar 0, exit at bar 1


def test_parity_entry_skipped_when_position_exists():
    """Test that entry is skipped when position already exists."""
    # Enter long at bar 0, then at bar 1 try to enter again (should be skipped) and exit
    bars = _bars2(
        100, 100, 100, 100,  # bar 0: enter long
        100, 110, 90, 100,   # bar 1: exit stop 95 triggers, entry stop 105 also triggerable but skipped
    )
    intents = [
        OrderIntent(order_id=1, created_bar=-1, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=100.0),
        OrderIntent(order_id=2, created_bar=0, role=OrderRole.EXIT, kind=OrderKind.STOP, side=Side.SELL, price=95.0),
        OrderIntent(order_id=3, created_bar=0, role=OrderRole.ENTRY, kind=OrderKind.STOP, side=Side.BUY, price=105.0),
    ]
    
    # Use unified simulate_run() entry point
    cursor_result = simulate_run(bars, intents, use_reference=False)
    reference_result = simulate_run(bars, intents, use_reference=True)
    
    _assert_fills_identical(cursor_result.fills, reference_result.fills)
    _assert_position_path_identical(cursor_result.fills, reference_result.fills)
    
    # Should have entry at bar 0 and exit at bar 1
    # Entry at bar 1 should be skipped (position already exists)
    assert len(cursor_result.fills) == 2
    assert cursor_result.fills[0].bar_index == 0
    assert cursor_result.fills[0].role == OrderRole.ENTRY
    assert cursor_result.fills[1].bar_index == 1
    assert cursor_result.fills[1].role == OrderRole.EXIT



--------------------------------------------------------------------------------

FILE tests/test_kpi_drilldown_no_raise.py
sha256(source_bytes) = 3e25cb54658b576ff72a67c149e9b66d74189ad470233314da44fa1876889439
bytes = 8064
redacted = False
--------------------------------------------------------------------------------

"""Tests for KPI drill-down - no raise contract.

Tests missing artifacts, wrong pointers, empty session_state.
UI functions should never raise exceptions.

Zero-side-effect imports: All I/O and stateful operations are inside test functions.

NOTE: This test is skipped because streamlit has been removed from the project.
"""

from __future__ import annotations

import pytest

pytest.skip("Streamlit tests skipped - streamlit removed from project", allow_module_level=True)

# Original test code below is not executed


def test_kpi_table_missing_name() -> None:
    """Test KPI table handles missing name field."""
    # Import inside test function to prevent collection errors
    from FishBroWFS_V2.gui.viewer.components.kpi_table import render_kpi_table
    
    with patch("streamlit.subheader"), \
         patch("streamlit.columns"), \
         patch("streamlit.markdown"), \
         patch("streamlit.text"), \
         patch("streamlit.button"):
        
        # Row without name
        kpi_rows = [
            {"value": 100}
        ]
        
        # Should not raise
        render_kpi_table(kpi_rows)


def test_kpi_table_missing_value() -> None:
    """Test KPI table handles missing value field."""
    # Import inside test function to prevent collection errors
    from FishBroWFS_V2.gui.viewer.components.kpi_table import render_kpi_table
    
    with patch("streamlit.subheader"), \
         patch("streamlit.columns"), \
         patch("streamlit.markdown"), \
         patch("streamlit.text"), \
         patch("streamlit.button"):
        
        # Row without value
        kpi_rows = [
            {"name": "net_profit"}
        ]
        
        # Should not raise
        render_kpi_table(kpi_rows)


def test_kpi_table_empty_rows() -> None:
    """Test KPI table handles empty rows list."""
    # Import inside test function to prevent collection errors
    from FishBroWFS_V2.gui.viewer.components.kpi_table import render_kpi_table
    
    with patch("streamlit.info"):
        # Empty list
        render_kpi_table([])
        
        # Should not raise


def test_kpi_table_unknown_kpi() -> None:
    """Test KPI table handles unknown KPI (not in registry)."""
    # Import inside test function to prevent collection errors
    from FishBroWFS_V2.gui.viewer.components.kpi_table import render_kpi_table
    
    with patch("streamlit.subheader"), \
         patch("streamlit.columns"), \
         patch("streamlit.markdown"), \
         patch("streamlit.text"), \
         patch("streamlit.button"):
        
        # KPI not in registry
        kpi_rows = [
            {"name": "unknown_kpi", "value": 100}
        ]
        
        # Should not raise - displays but not clickable
        render_kpi_table(kpi_rows)


def test_evidence_panel_missing_artifact() -> None:
    """Test evidence panel handles missing artifact."""
    # Import inside test function to prevent collection errors
    from FishBroWFS_V2.gui.viewer.components.evidence_panel import render_evidence_panel
    
    with patch("streamlit.subheader"), \
         patch("streamlit.markdown"), \
         patch("streamlit.warning"), \
         patch("streamlit.caption"):
        
        # Mock session state with missing artifact
        with patch.dict(st.session_state, {
            "active_evidence": {
                "kpi_name": "net_profit",
                "artifact": "winners_v2",
                "json_pointer": "/summary/net_profit",
            }
        }):
            # Artifacts dict missing winners_v2
            artifacts = {
                "manifest": {},
            }
            
            # Should not raise - shows warning
            render_evidence_panel(artifacts)


def test_evidence_panel_wrong_pointer() -> None:
    """Test evidence panel handles wrong JSON pointer."""
    # Import inside test function to prevent collection errors
    from FishBroWFS_V2.gui.viewer.components.evidence_panel import render_evidence_panel
    
    with patch("streamlit.subheader"), \
         patch("streamlit.markdown"), \
         patch("streamlit.warning"), \
         patch("streamlit.info"), \
         patch("streamlit.caption"):
        
        # Mock session state
        with patch.dict(st.session_state, {
            "active_evidence": {
                "kpi_name": "net_profit",
                "artifact": "winners_v2",
                "json_pointer": "/nonexistent/pointer",
            }
        }):
            # Artifact exists but pointer is wrong
            artifacts = {
                "winners_v2": {
                    "summary": {
                        "net_profit": 100
                    }
                }
            }
            
            # Should not raise - shows warning
            render_evidence_panel(artifacts)


def test_evidence_panel_empty_session_state() -> None:
    """Test evidence panel handles empty session_state."""
    # Import inside test function to prevent collection errors
    from FishBroWFS_V2.gui.viewer.components.evidence_panel import render_evidence_panel
    
    with patch("streamlit.subheader"):
        # Empty session state
        with patch.dict(st.session_state, {}, clear=True):
            artifacts = {
                "winners_v2": {}
            }
            
            # Should not raise - returns early
            render_evidence_panel(artifacts)


def test_evidence_panel_invalid_session_state() -> None:
    """Test evidence panel handles invalid session_state structure."""
    # Import inside test function to prevent collection errors
    from FishBroWFS_V2.gui.viewer.components.evidence_panel import render_evidence_panel
    
    with patch("streamlit.subheader"), \
         patch("streamlit.markdown"), \
         patch("streamlit.warning"):
        
        # Invalid session state structure
        with patch.dict(st.session_state, {
            "active_evidence": "not_a_dict"
        }):
            artifacts = {}
            
            # Should not raise - handles gracefully
            render_evidence_panel(artifacts)


def test_evidence_panel_missing_fields() -> None:
    """Test evidence panel handles missing fields in session_state."""
    # Import inside test function to prevent collection errors
    from FishBroWFS_V2.gui.viewer.components.evidence_panel import render_evidence_panel
    
    with patch("streamlit.subheader"), \
         patch("streamlit.markdown"), \
         patch("streamlit.warning"):
        
        # Missing fields in active_evidence
        with patch.dict(st.session_state, {
            "active_evidence": {
                "kpi_name": "net_profit",
                # Missing artifact, json_pointer
            }
        }):
            artifacts = {}
            
            # Should not raise - handles gracefully
            render_evidence_panel(artifacts)


def test_kpi_table_exception_handling() -> None:
    """Test KPI table handles exceptions gracefully."""
    # Import inside test function to prevent collection errors
    from FishBroWFS_V2.gui.viewer.components.kpi_table import render_kpi_table
    
    # Mock streamlit to raise exception
    with patch("streamlit.subheader", side_effect=Exception("Streamlit error")):
        kpi_rows = [
            {"name": "net_profit", "value": 100}
        ]
        
        # Should catch exception and show error
        with patch("streamlit.error"):
            render_kpi_table(kpi_rows)
            # Should not raise


def test_evidence_panel_exception_handling() -> None:
    """Test evidence panel handles exceptions gracefully."""
    # Import inside test function to prevent collection errors
    from FishBroWFS_V2.gui.viewer.components.evidence_panel import render_evidence_panel
    
    # Mock streamlit to raise exception
    with patch("streamlit.subheader", side_effect=Exception("Streamlit error")):
        artifacts = {}
        
        # Should catch exception and show error
        with patch("streamlit.error"):
            render_evidence_panel(artifacts)
            # Should not raise



--------------------------------------------------------------------------------

FILE tests/test_kpi_registry.py
sha256(source_bytes) = 67585b96b2f6c97d736531c5ab545c5477030e3f5ae819a764df1b32990b4cf5
bytes = 2810
redacted = False
--------------------------------------------------------------------------------

"""Tests for KPI Registry.

Tests registry key  EvidenceLink mapping and defensive behavior.
"""

from __future__ import annotations

import pytest

from FishBroWFS_V2.gui.viewer.kpi_registry import (
    KPI_EVIDENCE_REGISTRY,
    get_evidence_link,
    has_evidence,
    EvidenceLink,
)


def test_registry_keys_exist() -> None:
    """Test that registry keys map to correct EvidenceLink."""
    # Test net_profit
    link = get_evidence_link("net_profit")
    assert link is not None
    assert link.artifact == "winners_v2"
    assert link.json_pointer == "/summary/net_profit"
    assert "profit" in link.description.lower()
    
    # Test max_drawdown
    link = get_evidence_link("max_drawdown")
    assert link is not None
    assert link.artifact == "winners_v2"
    assert link.json_pointer == "/summary/max_drawdown"
    
    # Test num_trades
    link = get_evidence_link("num_trades")
    assert link is not None
    assert link.artifact == "winners_v2"
    assert link.json_pointer == "/summary/num_trades"
    
    # Test final_score
    link = get_evidence_link("final_score")
    assert link is not None
    assert link.artifact == "governance"
    assert link.json_pointer == "/scoring/final_score"


def test_unknown_kpi_returns_none() -> None:
    """Test that unknown KPI names return None without crashing."""
    link = get_evidence_link("unknown_kpi")
    assert link is None
    
    link = get_evidence_link("")
    assert link is None
    
    link = get_evidence_link("nonexistent")
    assert link is None


def test_has_evidence() -> None:
    """Test has_evidence function."""
    assert has_evidence("net_profit") is True
    assert has_evidence("max_drawdown") is True
    assert has_evidence("num_trades") is True
    assert has_evidence("final_score") is True
    
    assert has_evidence("unknown_kpi") is False
    assert has_evidence("") is False


def test_registry_never_raises() -> None:
    """Test that registry functions never raise exceptions."""
    # Test with invalid input types
    try:
        get_evidence_link(None)  # type: ignore
    except Exception:
        pytest.fail("get_evidence_link should not raise")
    
    try:
        has_evidence(None)  # type: ignore
    except Exception:
        pytest.fail("has_evidence should not raise")


def test_registry_structure() -> None:
    """Test that registry has correct structure."""
    assert isinstance(KPI_EVIDENCE_REGISTRY, dict)
    assert len(KPI_EVIDENCE_REGISTRY) > 0
    
    for kpi_name, link in KPI_EVIDENCE_REGISTRY.items():
        assert isinstance(kpi_name, str)
        assert isinstance(link, EvidenceLink)
        assert link.artifact in ("manifest", "winners_v2", "governance")
        assert link.json_pointer.startswith("/")
        assert isinstance(link.description, str)



--------------------------------------------------------------------------------

FILE tests/test_log_tail_reads_last_n_lines.py
sha256(source_bytes) = 3a3316d7a50864b6da0b8bd4441edc0aed6fe3248fa13fff7ee08afbfd04145c
bytes = 1531
redacted = False
--------------------------------------------------------------------------------

"""Test that read_tail reads last n lines efficiently without loading entire file."""

from __future__ import annotations

from pathlib import Path

import pytest

from FishBroWFS_V2.control.app_nicegui import read_tail


def test_read_tail_returns_last_n_lines(tmp_path: Path) -> None:
    """Test that read_tail returns exactly the last n lines."""
    p = tmp_path / "big.log"
    lines = [f"line {i}\n" for i in range(5000)]
    p.write_text("".join(lines), encoding="utf-8")

    out = read_tail(p, n=200)
    out_lines = out.splitlines()

    assert len(out_lines) == 200
    assert out_lines[0] == "line 4800"
    assert out_lines[-1] == "line 4999"


def test_read_tail_handles_small_file(tmp_path: Path) -> None:
    """Test that read_tail handles files with fewer lines than requested."""
    p = tmp_path / "small.log"
    lines = [f"line {i}\n" for i in range(50)]
    p.write_text("".join(lines), encoding="utf-8")

    out = read_tail(p, n=200)
    out_lines = out.splitlines()

    assert len(out_lines) == 50
    assert out_lines[0] == "line 0"
    assert out_lines[-1] == "line 49"


def test_read_tail_handles_empty_file(tmp_path: Path) -> None:
    """Test that read_tail handles empty files."""
    p = tmp_path / "empty.log"
    p.touch()

    out = read_tail(p, n=200)
    assert out == ""


def test_read_tail_handles_missing_file(tmp_path: Path) -> None:
    """Test that read_tail handles missing files gracefully."""
    p = tmp_path / "missing.log"

    out = read_tail(p, n=200)
    assert out == ""



--------------------------------------------------------------------------------

FILE tests/test_mnq_maintenance_break_no_cross.py
sha256(source_bytes) = fb9063af6de1b7f391873eed22dda8181ecfd177768b327150fdf05356aa2753
bytes = 4716
redacted = False
--------------------------------------------------------------------------------

"""Test MNQ maintenance break: no cross-session aggregation.

Phase 6.6: Verify that MNQ bars before and after maintenance window
are not aggregated into the same K-bar.
"""

from __future__ import annotations

from pathlib import Path

import pandas as pd
import pytest

from FishBroWFS_V2.data.session.kbar import aggregate_kbar
from FishBroWFS_V2.data.session.loader import load_session_profile


@pytest.fixture
def mnq_exchange_profile() -> Path:
    """Load CME.MNQ EXCHANGE_RULE profile."""
    profile_path = Path(__file__).parent.parent / "src" / "FishBroWFS_V2" / "data" / "profiles" / "CME_MNQ_EXCHANGE_v1.yaml"
    return profile_path


def test_mnq_maintenance_break_no_cross_30m(mnq_exchange_profile: Path) -> None:
    """Test 30-minute bars do not cross maintenance boundary.
    
    MNQ maintenance: 16:00-17:00 CT (approximately 06:00-07:00 TPE, varies with DST).
    Bars just before maintenance (15:59 CT) and just after (17:01 CT)
    should not be in the same 30m bar.
    """
    profile = load_session_profile(mnq_exchange_profile)
    
    # Create bars around maintenance window
    # Using dates that avoid DST transitions for simplicity
    # 2013/3/10 is a Sunday (before DST spring forward on 3/10/2013)
    # Maintenance window: 16:00-17:00 CT = approximately 06:00-07:00 TPE (before DST)
    df = pd.DataFrame({
        "ts_str": [
            "2013/3/10 05:55:00",  # TRADING (before maintenance, ~15:55 CT)
            "2013/3/10 05:59:00",  # TRADING (just before maintenance, ~15:59 CT)
            "2013/3/10 06:30:00",  # MAINTENANCE (during maintenance, ~16:30 CT)
            "2013/3/10 07:01:00",  # TRADING (just after maintenance, ~17:01 CT)
            "2013/3/10 07:05:00",  # TRADING (after maintenance, ~17:05 CT)
        ],
        "open": [100.0, 101.0, 102.0, 103.0, 104.0],
        "high": [100.5, 101.5, 102.5, 103.5, 104.5],
        "low": [99.5, 100.5, 101.5, 102.5, 103.5],
        "close": [100.5, 101.5, 102.5, 103.5, 104.5],
        "volume": [1000, 1100, 1200, 1300, 1400],
    })
    
    result = aggregate_kbar(df, 30, profile)
    
    # Verify result has session column
    assert "session" in result.columns, "Result must include session column"
    
    # Verify no bar mixes TRADING and MAINTENANCE
    # Each row must have exactly one session
    assert result["session"].notna().all(), "All bars must have a session label"
    
    # Check that TRADING and MAINTENANCE are separate
    trading_bars = result[result["session"] == "TRADING"]
    maintenance_bars = result[result["session"] == "MAINTENANCE"]
    
    # Should have both TRADING and MAINTENANCE bars (if maintenance bars exist)
    if len(maintenance_bars) > 0:
        # Verify no bar contains both sessions
        assert len(result) == len(trading_bars) + len(maintenance_bars), (
            "Total bars should equal sum of TRADING and MAINTENANCE bars"
        )
        
        # Verify bars before maintenance are TRADING
        # Verify bars during maintenance are MAINTENANCE
        # Verify bars after maintenance are TRADING
        # (This is verified by the session column)


def test_mnq_maintenance_break_no_cross_60m(mnq_exchange_profile: Path) -> None:
    """Test 60-minute bars do not cross maintenance boundary."""
    profile = load_session_profile(mnq_exchange_profile)
    
    # Similar to 30m test, but with 60m interval
    df = pd.DataFrame({
        "ts_str": [
            "2013/3/10 05:50:00",  # TRADING (before maintenance)
            "2013/3/10 05:59:00",  # TRADING (just before maintenance)
            "2013/3/10 06:30:00",  # MAINTENANCE (during maintenance)
            "2013/3/10 07:01:00",  # TRADING (just after maintenance)
            "2013/3/10 07:10:00",  # TRADING (after maintenance)
        ],
        "open": [100.0, 101.0, 102.0, 103.0, 104.0],
        "high": [100.5, 101.5, 102.5, 103.5, 104.5],
        "low": [99.5, 100.5, 101.5, 102.5, 103.5],
        "close": [100.5, 101.5, 102.5, 103.5, 104.5],
        "volume": [1000, 1100, 1200, 1300, 1400],
    })
    
    result = aggregate_kbar(df, 60, profile)
    
    # Verify result has session column
    assert "session" in result.columns, "Result must include session column"
    
    # Verify no bar mixes TRADING and MAINTENANCE
    assert result["session"].notna().all(), "All bars must have a session label"
    
    # Verify session separation
    trading_bars = result[result["session"] == "TRADING"]
    maintenance_bars = result[result["session"] == "MAINTENANCE"]
    
    if len(maintenance_bars) > 0:
        assert len(result) == len(trading_bars) + len(maintenance_bars), (
            "Total bars should equal sum of TRADING and MAINTENANCE bars"
        )



--------------------------------------------------------------------------------

FILE tests/test_no_ui_imports_anywhere.py
sha256(source_bytes) = c3d77886744ee90add6518e677bd9fd400cb6cdc330a597cd42bf1a7e853e7f7
bytes = 1525
redacted = False
--------------------------------------------------------------------------------

"""Contract test: No ui namespace imports anywhere in FishBroWFS_V2.

Ensures the entire FishBroWFS_V2 package does not import from ui namespace.
This is a "truth test" to prevent any ui.* imports from being reintroduced.
"""

from __future__ import annotations

import pkgutil

import pytest


def test_no_ui_namespace_anywhere() -> None:
    """Test that FishBroWFS_V2 package does not import from ui namespace."""
    import FishBroWFS_V2
    
    # Walk through all modules in FishBroWFS_V2 package
    # If any module imports ui.*, it will fail during import
    for importer, modname, ispkg in pkgutil.walk_packages(FishBroWFS_V2.__path__, FishBroWFS_V2.__name__ + "."):
        try:
            # Import module - this will fail if it imports ui.* and ui doesn't exist
            __import__(modname, fromlist=[""])
        except ImportError as e:
            # Check if error is related to ui namespace
            if "ui" in str(e) and ("No module named" in str(e) or "cannot import name" in str(e)):
                pytest.fail(
                    f"Module {modname} imports from ui namespace (ui module no longer exists): {e}"
                )
            #  viewer  streamlit 
            if "gui.viewer" in modname and "No module named 'streamlit'" in str(e):
                # viewer  streamlit streamlit 
                continue
            # Re-raise other ImportErrors (might be legitimate missing dependencies)
            raise



--------------------------------------------------------------------------------

FILE tests/test_no_ui_namespace.py
sha256(source_bytes) = 4026731203def121ff118a01429225d39a87e8f9dcfdd2949bab49f7551f3529
bytes = 6748
redacted = False
--------------------------------------------------------------------------------

"""Contract test: No ui namespace imports allowed.

Ensures the entire FishBroWFS_V2 package does not import from ui namespace.
"""

from __future__ import annotations

import ast
import pkgutil
from pathlib import Path

import pytest


def test_no_ui_namespace_importable() -> None:
    """Test that FishBroWFS_V2 package does not import from ui namespace."""
    import FishBroWFS_V2 as pkg
    
    ui_imports: list[tuple[str, str]] = []
    
    # Walk through all modules in FishBroWFS_V2 package
    for importer, modname, ispkg in pkgutil.walk_packages(pkg.__path__, pkg.__name__ + "."):
        try:
            # Import module to trigger any import errors
            module = __import__(modname, fromlist=[""])
            
            # Get source file path
            if hasattr(module, "__file__") and module.__file__:
                source_path = Path(module.__file__)
                if source_path.exists() and source_path.suffix == ".py":
                    # Parse AST to find imports
                    try:
                        with source_path.open("r", encoding="utf-8") as f:
                            tree = ast.parse(f.read(), filename=str(source_path))
                        
                        # Check all imports
                        for node in ast.walk(tree):
                            if isinstance(node, ast.Import):
                                for alias in node.names:
                                    if alias.name.startswith("ui."):
                                        ui_imports.append((modname, alias.name))
                            elif isinstance(node, ast.ImportFrom):
                                if node.module and node.module.startswith("ui."):
                                    ui_imports.append((modname, f"from {node.module}"))
                    except (SyntaxError, UnicodeDecodeError):
                        # Skip files that can't be parsed (might be binary or invalid)
                        pass
        except Exception as e:
            # Skip modules that fail to import (might be missing dependencies)
            # But log for debugging if it's not an ImportError
            if "ImportError" not in str(type(e)) and "ModuleNotFoundError" not in str(type(e)):
                pytest.fail(f"Unexpected error importing {modname}: {e}")
    
    # Should have no ui.* imports
    if ui_imports:
        pytest.fail(
            f"FishBroWFS_V2 package contains ui.* imports:\n"
            + "\n".join(f"  {mod}: {imp}" for mod, imp in ui_imports)
        )


def test_viewer_no_ui_imports() -> None:
    """Test that Viewer package specifically does not import from ui namespace."""
    import FishBroWFS_V2.gui.viewer as viewer
    
    ui_imports: list[tuple[str, str]] = []
    
    # Walk through all modules in viewer package
    for importer, modname, ispkg in pkgutil.walk_packages(viewer.__path__, viewer.__name__ + "."):
        try:
            module = __import__(modname, fromlist=[""])
            
            if hasattr(module, "__file__") and module.__file__:
                source_path = Path(module.__file__)
                if source_path.exists() and source_path.suffix == ".py":
                    try:
                        with source_path.open("r", encoding="utf-8") as f:
                            tree = ast.parse(f.read(), filename=str(source_path))
                        
                        for node in ast.walk(tree):
                            if isinstance(node, ast.Import):
                                for alias in node.names:
                                    if alias.name.startswith("ui."):
                                        ui_imports.append((modname, alias.name))
                            elif isinstance(node, ast.ImportFrom):
                                if node.module and node.module.startswith("ui."):
                                    ui_imports.append((modname, f"from {node.module}"))
                    except (SyntaxError, UnicodeDecodeError):
                        pass
        except Exception as e:
            if "ImportError" not in str(type(e)) and "ModuleNotFoundError" not in str(type(e)):
                pytest.fail(f"Unexpected error importing {modname}: {e}")
    
    if ui_imports:
        pytest.fail(
            f"Viewer package contains ui.* imports:\n"
            + "\n".join(f"  {mod}: {imp}" for mod, imp in ui_imports)
        )


def test_no_ui_directory_exists() -> None:
    """Test that ui/ directory does not exist in repo root (repo structure contract)."""
    repo_root = Path(__file__).parent.parent
    ui_dir = repo_root / "ui"
    
    if ui_dir.exists():
        pytest.fail(f"ui/ directory must not exist in repo root, but found at {ui_dir}")


def test_makefile_no_ui_paths() -> None:
    """Test that Makefile does not reference ui/ paths (old namespace)."""
    repo_root = Path(__file__).parent.parent
    makefile_path = repo_root / "Makefile"
    
    assert makefile_path.exists()
    
    content = makefile_path.read_text()
    
    # Check for ui/ references (excluding comments)
    lines = content.split("\n")
    for i, line in enumerate(lines, 1):
        # Skip comments
        if line.strip().startswith("#"):
            continue
        
        # Normalize line for checking
        line_lower = line.lower()
        
        # Prohibited patterns (old ui namespace)
        # 1. Path references containing "/ui/" (excluding "gui/")
        if "/ui/" in line and "/gui/" not in line:
            pytest.fail(f"Makefile line {i} contains prohibited /ui/ path: {line.strip()}")
        
        # 2. Import-like references to "FishBroWFS_V2.ui." (case-insensitive)
        if "fishbro_wfs_v2.ui." in line_lower:
            pytest.fail(f"Makefile line {i} contains prohibited FishBroWFS_V2.ui. import: {line.strip()}")
        
        # 3. Specific old module "ui.app_streamlit"
        if "ui.app_streamlit" in line_lower:
            pytest.fail(f"Makefile line {i} contains prohibited ui.app_streamlit: {line.strip()}")
        
        # 4. Standalone "ui." as a module prefix (with word boundary)
        # We'll use a simple check: "ui." preceded by whitespace or start of line
        # but exclude "gui." and "build" etc.
        import re
        if re.search(r'(^|\s)ui\.', line) and not re.search(r'(^|\s)gui\.', line):
            # Allow if it's part of a longer word like "build" (but "ui." is likely a module)
            # Additional check: ensure it's not part of a larger word like "build"
            if not re.search(r'\bui\.', line):  # word boundary check
                continue
            pytest.fail(f"Makefile line {i} contains prohibited ui. module reference: {line.strip()}")



--------------------------------------------------------------------------------

FILE tests/test_oom_gate.py
sha256(source_bytes) = 6a17608bd7b688544185cf82d957f2aa22f25c3a9c91d4c10a1260cd9463d75d
bytes = 7841
redacted = False
--------------------------------------------------------------------------------

"""Tests for OOM gate decision maker.

Tests verify:
1. PASS case (estimated <= 60% of budget)
2. BLOCK case (estimated > 90% of budget)
3. AUTO_DOWNSAMPLE case (between 60% and 90%, with recommended_rate in (0,1])
4. Invalid input validation (bars<=0, rate<=0, etc.)
"""

from __future__ import annotations

import pytest

from FishBroWFS_V2.core.oom_gate import decide_gate, decide_oom_action, estimate_bytes
from FishBroWFS_V2.core.schemas.oom_gate import OomGateInput


def test_estimate_bytes() -> None:
    """Test memory estimation formula."""
    inp = OomGateInput(
        bars=1000,
        params=100,
        param_subsample_rate=0.5,
        intents_per_bar=2.0,
        bytes_per_intent_est=64,
    )
    
    estimated = estimate_bytes(inp)
    
    # Formula: bars * params * subsample * intents_per_bar * bytes_per_intent_est
    expected = 1000 * 100 * 0.5 * 2.0 * 64
    assert estimated == expected


def test_decide_gate_pass() -> None:
    """Test PASS decision when estimated <= 60% of budget."""
    # Small workload: 1M bytes, budget is 6GB (6_000_000_000)
    inp = OomGateInput(
        bars=100,
        params=10,
        param_subsample_rate=0.1,
        intents_per_bar=2.0,
        bytes_per_intent_est=64,
        ram_budget_bytes=6_000_000_000,
    )
    
    decision = decide_gate(inp)
    
    assert decision.decision == "PASS"
    assert decision.estimated_bytes <= inp.ram_budget_bytes * 0.6
    assert decision.recommended_subsample_rate is None
    assert "PASS" not in decision.notes  # Notes should describe the decision, not repeat it
    assert decision.estimated_bytes > 0


def test_decide_gate_block() -> None:
    """Test BLOCK decision when estimated > 90% of budget."""
    # Large workload: exceed 90% of budget
    # Set budget to 1GB for easier testing
    budget = 1_000_000_000  # 1GB
    # Need estimated > budget * 0.9 = 900MB
    # Let's use: 10000 bars * 10000 params * 1.0 rate * 2.0 intents * 64 bytes = 12.8GB
    inp = OomGateInput(
        bars=10000,
        params=10000,
        param_subsample_rate=1.0,
        intents_per_bar=2.0,
        bytes_per_intent_est=64,
        ram_budget_bytes=budget,
    )
    
    decision = decide_gate(inp)
    
    assert decision.decision == "BLOCK"
    assert decision.estimated_bytes > budget * 0.9
    assert decision.recommended_subsample_rate is None
    assert "BLOCKED" in decision.notes or "BLOCK" in decision.notes


def test_decide_gate_auto_downsample() -> None:
    """Test AUTO_DOWNSAMPLE decision when estimated between 60% and 90%."""
    # Medium workload: between 60% and 90% of budget
    # Set budget to 1GB for easier testing
    budget = 1_000_000_000  # 1GB
    # Need: budget * 0.6 < estimated < budget * 0.9
    # 600MB < estimated < 900MB
    # Let's use: 5000 bars * 5000 params * 1.0 rate * 2.0 intents * 64 bytes = 3.2GB
    # That's too high. Let's adjust:
    # For 700MB: 700_000_000 = bars * params * 1.0 * 2.0 * 64
    # bars * params = 700_000_000 / (2.0 * 64) = 5_468_750
    # Let's use: 5000 bars * 1094 params * 1.0 rate * 2.0 * 64 = ~700MB
    inp = OomGateInput(
        bars=5000,
        params=1094,
        param_subsample_rate=1.0,
        intents_per_bar=2.0,
        bytes_per_intent_est=64,
        ram_budget_bytes=budget,
    )
    
    decision = decide_gate(inp)
    
    assert decision.decision == "AUTO_DOWNSAMPLE"
    assert decision.estimated_bytes > budget * 0.6
    assert decision.estimated_bytes <= budget * 0.9
    assert decision.recommended_subsample_rate is not None
    assert 0.0 < decision.recommended_subsample_rate <= 1.0
    assert "recommended" in decision.notes.lower() or "subsample" in decision.notes.lower()


def test_decide_gate_auto_downsample_recommended_rate_calculation() -> None:
    """Test that recommended_rate is calculated correctly for AUTO_DOWNSAMPLE."""
    budget = 1_000_000_000  # 1GB
    bars = 1000
    params = 1000
    intents_per_bar = 2.0
    bytes_per_intent = 64
    
    # Use current rate that puts us in AUTO_DOWNSAMPLE zone
    inp = OomGateInput(
        bars=bars,
        params=params,
        param_subsample_rate=1.0,
        intents_per_bar=intents_per_bar,
        bytes_per_intent_est=bytes_per_intent,
        ram_budget_bytes=budget,
    )
    
    decision = decide_gate(inp)
    
    if decision.decision == "AUTO_DOWNSAMPLE":
        # Verify recommended_rate formula: (ram_budget * 0.6) / (bars * params * intents_per_bar * bytes_per_intent_est)
        expected_rate = (budget * 0.6) / (bars * params * intents_per_bar * bytes_per_intent)
        expected_rate = max(0.0, min(1.0, expected_rate))
        
        assert decision.recommended_subsample_rate is not None
        assert abs(decision.recommended_subsample_rate - expected_rate) < 0.0001  # Allow small floating point error


def test_invalid_input_bars_zero() -> None:
    """Test that bars <= 0 raises validation error."""
    with pytest.raises(Exception):  # Pydantic ValidationError
        OomGateInput(
            bars=0,
            params=100,
            param_subsample_rate=0.5,
        )


def test_invalid_input_bars_negative() -> None:
    """Test that bars < 0 raises validation error."""
    with pytest.raises(Exception):  # Pydantic ValidationError
        OomGateInput(
            bars=-1,
            params=100,
            param_subsample_rate=0.5,
        )


def test_invalid_input_params_zero() -> None:
    """Test that params <= 0 raises validation error."""
    with pytest.raises(Exception):  # Pydantic ValidationError
        OomGateInput(
            bars=1000,
            params=0,
            param_subsample_rate=0.5,
        )


def test_invalid_input_subsample_rate_zero() -> None:
    """Test that param_subsample_rate <= 0 raises validation error."""
    with pytest.raises(Exception):  # Pydantic ValidationError
        OomGateInput(
            bars=1000,
            params=100,
            param_subsample_rate=0.0,
        )


def test_invalid_input_subsample_rate_negative() -> None:
    """Test that param_subsample_rate < 0 raises validation error."""
    with pytest.raises(Exception):  # Pydantic ValidationError
        OomGateInput(
            bars=1000,
            params=100,
            param_subsample_rate=-0.1,
        )


def test_invalid_input_subsample_rate_over_one() -> None:
    """Test that param_subsample_rate > 1.0 raises validation error."""
    with pytest.raises(Exception):  # Pydantic ValidationError
        OomGateInput(
            bars=1000,
            params=100,
            param_subsample_rate=1.1,
        )


def test_default_values() -> None:
    """Test that default values work correctly."""
    inp = OomGateInput(
        bars=1000,
        params=100,
        param_subsample_rate=0.5,
    )
    
    assert inp.intents_per_bar == 2.0
    assert inp.bytes_per_intent_est == 64
    assert inp.ram_budget_bytes == 6_000_000_000  # 6GB
    
    decision = decide_gate(inp)
    assert decision.decision in ("PASS", "BLOCK", "AUTO_DOWNSAMPLE")
    assert decision.estimated_bytes >= 0
    assert decision.ram_budget_bytes == inp.ram_budget_bytes


def test_decide_oom_action_returns_dict_schema() -> None:
    """Test legacy decide_oom_action() returns dict schema."""
    cfg = {"bars": 1000, "params_total": 100, "param_subsample_rate": 0.1}
    res = decide_oom_action(cfg, mem_limit_mb=10_000.0)
    
    assert isinstance(res, dict)
    assert res["action"] in {"PASS", "BLOCK", "AUTO_DOWNSAMPLE"}
    assert "estimated_bytes" in res
    assert "estimated_mb" in res
    assert "mem_limit_mb" in res
    assert "mem_limit_bytes" in res
    assert "original_subsample" in res  # Contract key name
    assert "final_subsample" in res  # Contract key name
    assert "params_total" in res
    assert "params_effective" in res
    assert "reason" in res



--------------------------------------------------------------------------------

FILE tests/test_oom_gate_contract.py
sha256(source_bytes) = 874afa1fbce7a15e8c3b2ac89ef02dbba46417cb4736d0f99ebe1b520e75b0e8
bytes = 7204
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for OOM gate.

Tests verify:
1. Gate PASS when under limit
2. Gate BLOCK when over limit and no auto-downsample
3. Gate AUTO_DOWNSAMPLE when allowed
"""

from __future__ import annotations

import numpy as np
import pytest

from FishBroWFS_V2.core.oom_gate import decide_oom_action
from FishBroWFS_V2.core.oom_cost_model import estimate_memory_bytes, summarize_estimates


def test_oom_gate_pass_when_under_limit():
    """Test that gate PASSes when memory estimate is under limit."""
    cfg = {
        "bars": 1000,
        "params_total": 100,
        "param_subsample_rate": 0.1,
        "open_": np.random.randn(1000).astype(np.float64),
        "high": np.random.randn(1000).astype(np.float64),
        "low": np.random.randn(1000).astype(np.float64),
        "close": np.random.randn(1000).astype(np.float64),
        "params_matrix": np.random.randn(100, 3).astype(np.float64),
    }
    
    # Use a very high limit to ensure PASS
    mem_limit_mb = 10000.0
    
    result = decide_oom_action(cfg, mem_limit_mb=mem_limit_mb)
    
    assert result["action"] == "PASS"
    assert result["original_subsample"] == 0.1
    assert result["final_subsample"] == 0.1
    assert "estimates" in result
    assert result["estimates"]["mem_est_mb"] <= mem_limit_mb


def test_oom_gate_block_when_over_limit_and_no_auto():
    """Test that gate BLOCKs when over limit and auto-downsample is disabled."""
    cfg = {
        "bars": 100000,
        "params_total": 10000,
        "param_subsample_rate": 1.0,
        "open_": np.random.randn(100000).astype(np.float64),
        "high": np.random.randn(100000).astype(np.float64),
        "low": np.random.randn(100000).astype(np.float64),
        "close": np.random.randn(100000).astype(np.float64),
        "params_matrix": np.random.randn(10000, 3).astype(np.float64),
    }
    
    # Use a very low limit to ensure BLOCK
    mem_limit_mb = 1.0
    
    result = decide_oom_action(
        cfg,
        mem_limit_mb=mem_limit_mb,
        allow_auto_downsample=False,
    )
    
    assert result["action"] == "BLOCK"
    assert result["original_subsample"] == 1.0
    assert result["final_subsample"] == 1.0  # Not changed
    assert "reason" in result
    assert "mem_est_mb" in result["reason"] or "limit" in result["reason"]


def test_oom_gate_auto_downsample_when_allowed(monkeypatch):
    """Test that gate AUTO_DOWNSAMPLEs when allowed and over limit."""
    # Monkeypatch estimate_memory_bytes to make it subsample-sensitive for testing
    def mock_estimate_memory_bytes(cfg, work_factor=2.0):
        """Mock that makes memory estimate sensitive to subsample."""
        bars = int(cfg.get("bars", 0))
        params_total = int(cfg.get("params_total", 0))
        subsample_rate = float(cfg.get("param_subsample_rate", 1.0))
        params_effective = int(params_total * subsample_rate)
        
        # Simplified: mem scales with bars and effective params
        base_mem = bars * 8 * 4  # 4 price arrays
        params_mem = params_effective * 3 * 8  # params_matrix
        total_mem = (base_mem + params_mem) * work_factor
        return int(total_mem)
    
    monkeypatch.setattr(
        "FishBroWFS_V2.core.oom_cost_model.estimate_memory_bytes",
        mock_estimate_memory_bytes,
    )
    
    cfg = {
        "bars": 10000,
        "params_total": 1000,
        "param_subsample_rate": 0.5,  # Start at 50%
        "open_": np.random.randn(10000).astype(np.float64),
        "high": np.random.randn(10000).astype(np.float64),
        "low": np.random.randn(10000).astype(np.float64),
        "close": np.random.randn(10000).astype(np.float64),
        "params_matrix": np.random.randn(1000, 3).astype(np.float64),
    }
    
    # Dynamic calculation: compute mem_mb for two subsample rates, use midpoint
    def _mem_mb(cfg_dict):
        b = mock_estimate_memory_bytes(cfg_dict, work_factor=2.0)
        return b / (1024.0 * 1024.0)
    
    cfg_half = dict(cfg)
    cfg_half["param_subsample_rate"] = 0.5
    cfg_quarter = dict(cfg)
    cfg_quarter["param_subsample_rate"] = 0.25
    
    mb_half = _mem_mb(cfg_half)  # ~0.633
    mb_quarter = _mem_mb(cfg_quarter)  # ~0.622
    
    # Set limit between these two values  guaranteed to trigger AUTO_DOWNSAMPLE
    mem_limit_mb = (mb_half + mb_quarter) / 2.0
    
    result = decide_oom_action(
        cfg,
        mem_limit_mb=mem_limit_mb,
        allow_auto_downsample=True,
        auto_downsample_step=0.5,
        auto_downsample_min=0.02,
    )
    
    assert result["action"] == "AUTO_DOWNSAMPLE"
    assert result["original_subsample"] == 0.5
    assert result["final_subsample"] < result["original_subsample"]
    assert result["final_subsample"] >= 0.02  # Above minimum
    assert "reason" in result
    assert "auto-downsample" in result["reason"].lower()
    assert result["estimates"]["mem_est_mb"] <= mem_limit_mb


def test_oom_gate_block_when_min_still_over_limit(monkeypatch):
    """Test that gate BLOCKs when even at minimum subsample still over limit."""
    def mock_estimate_memory_bytes(cfg, work_factor=2.0):
        """Mock that always returns high memory."""
        return 100 * 1024 * 1024  # Always 100MB
    
    monkeypatch.setattr(
        "FishBroWFS_V2.core.oom_cost_model.estimate_memory_bytes",
        mock_estimate_memory_bytes,
    )
    
    cfg = {
        "bars": 1000,
        "params_total": 100,
        "param_subsample_rate": 0.5,
        "open_": np.random.randn(1000).astype(np.float64),
        "high": np.random.randn(1000).astype(np.float64),
        "low": np.random.randn(1000).astype(np.float64),
        "close": np.random.randn(1000).astype(np.float64),
        "params_matrix": np.random.randn(100, 3).astype(np.float64),
    }
    
    mem_limit_mb = 50.0  # Lower than mock estimate
    
    result = decide_oom_action(
        cfg,
        mem_limit_mb=mem_limit_mb,
        allow_auto_downsample=True,
        auto_downsample_min=0.02,
    )
    
    assert result["action"] == "BLOCK"
    assert "min_subsample" in result["reason"].lower() or "still too large" in result["reason"].lower()


def test_oom_gate_result_schema():
    """Test that gate result has correct schema."""
    cfg = {
        "bars": 1000,
        "params_total": 100,
        "param_subsample_rate": 0.1,
        "open_": np.random.randn(1000).astype(np.float64),
        "high": np.random.randn(1000).astype(np.float64),
        "low": np.random.randn(1000).astype(np.float64),
        "close": np.random.randn(1000).astype(np.float64),
        "params_matrix": np.random.randn(100, 3).astype(np.float64),
    }
    
    result = decide_oom_action(cfg, mem_limit_mb=10000.0)
    
    # Verify schema
    assert "action" in result
    assert result["action"] in ("PASS", "BLOCK", "AUTO_DOWNSAMPLE")
    assert "reason" in result
    assert isinstance(result["reason"], str)
    assert "original_subsample" in result
    assert "final_subsample" in result
    assert "estimates" in result
    
    # Verify estimates structure
    estimates = result["estimates"]
    assert "mem_est_bytes" in estimates
    assert "mem_est_mb" in estimates
    assert "ops_est" in estimates
    assert "time_est_s" in estimates



--------------------------------------------------------------------------------

FILE tests/test_oom_gate_pure_function_hash_consistency.py
sha256(source_bytes) = f53139536f304edade56d16dfcdf4267896cf051a4717ccfa51f6a8be20eb0a5
bytes = 3296
redacted = False
--------------------------------------------------------------------------------

"""Tests for OOM gate pure function hash consistency.

Tests that decide_oom_action never mutates input cfg and returns new_cfg SSOT.
"""

from __future__ import annotations

import pytest

from FishBroWFS_V2.core.config_hash import stable_config_hash
from FishBroWFS_V2.core.config_snapshot import make_config_snapshot
from FishBroWFS_V2.core.oom_gate import decide_oom_action


def test_oom_gate_pure_function_hash_consistency(monkeypatch) -> None:
    """
    Test that decide_oom_action is pure function (no mutation).
    
    Uses monkeypatch to ensure subsample-sensitive memory estimation,
    guaranteeing that subsample=1.0 exceeds limit and subsample reduction
    triggers AUTO_DOWNSAMPLE.
    
    Verifies:
    - Original cfg subsample remains unchanged
    - decision.new_cfg has modified subsample
    - Hash computed from new_cfg differs from original
    - manifest/snapshot records final_subsample correctly
    """
    def mock_estimate_memory_bytes(cfg, work_factor=2.0):
        """Make mem scale with subsample so AUTO_DOWNSAMPLE is meaningful."""
        subsample = float(cfg.get("param_subsample_rate", 1.0))
        # 100MB at subsample=1.0, 50MB at 0.5, etc.
        base = 100 * 1024 * 1024
        return int(base * subsample)
    
    monkeypatch.setattr(
        "FishBroWFS_V2.core.oom_cost_model.estimate_memory_bytes",
        mock_estimate_memory_bytes,
    )
    
    cfg = {
        "bars": 1,
        "params_total": 1,
        "param_subsample_rate": 1.0,
    }
    mem_limit_mb = 60.0  # 1.0 => 100MB (over), 0.5 => 50MB (under)
    
    decision = decide_oom_action(cfg, mem_limit_mb=mem_limit_mb, allow_auto_downsample=True)
    
    # Verify original cfg unchanged
    assert cfg["param_subsample_rate"] == 1.0, "Original cfg must not be mutated"
    
    # Verify decision has new_cfg
    assert "new_cfg" in decision, "decision must contain new_cfg"
    new_cfg = decision["new_cfg"]
    
    # Lock behavior: allow_auto_downsample=True  PASS AUTO_DOWNSAMPLE min
    assert decision["action"] == "AUTO_DOWNSAMPLE", "Should trigger AUTO_DOWNSAMPLE when allow_auto_downsample=True"
    
    # Verify new_cfg has modified subsample
    assert new_cfg["param_subsample_rate"] < 1.0, "new_cfg should have reduced subsample"
    assert decision["final_subsample"] < 1.0, "final_subsample should be reduced"
    assert decision["final_subsample"] < decision["original_subsample"], "final_subsample must be < original_subsample"
    assert decision["new_cfg"]["param_subsample_rate"] == decision["final_subsample"], "new_cfg subsample must match final_subsample"
    
    # Verify hash consistency
    original_snapshot = make_config_snapshot(cfg)
    original_hash = stable_config_hash(original_snapshot)
    
    new_snapshot = make_config_snapshot(new_cfg)
    new_hash = stable_config_hash(new_snapshot)
    
    assert original_hash != new_hash, "Hash should differ after subsample change"
    
    # Verify final_subsample matches new_cfg
    assert decision["final_subsample"] == new_cfg["param_subsample_rate"], (
        "final_subsample must match new_cfg subsample"
    )
    
    # Verify original_subsample preserved
    assert decision["original_subsample"] == 1.0, "original_subsample must be preserved"



--------------------------------------------------------------------------------

FILE tests/test_perf_breakdown_contract.py
sha256(source_bytes) = 1ba8d022b21ad691c763efbcbfa88af5a2cca6892c773ad1b22232485056209b
bytes = 13043
redacted = False
--------------------------------------------------------------------------------

"""
Stage P2-1.8: Contract Tests for Granular Breakdown and Extended Observability

Tests that verify:
- Granular timing keys exist and are non-negative floats
- Extended observability keys exist (entry/exit intents/fills totals)
- Accounting consistency (intents_total == entry + exit, fills_total == entry + exit)
- run_grid output contains timing keys in perf dict
"""
from __future__ import annotations

import os
import numpy as np

from FishBroWFS_V2.strategy.kernel import run_kernel_arrays, DonchianAtrParams
from FishBroWFS_V2.engine.types import BarArrays
from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_perf_breakdown_keys_existence() -> None:
    """
    D1: Contract test - Verify granular timing keys exist in _obs and are floats >= 0.0
    Also verify that t_total_kernel_s >= max(stage_times) for sanity check.
    
    Contract: keys always exist, values always float >= 0.0.
    (When perf harness runs with profiling enabled, these will naturally become >0 real data.)
    """
    import os
    # Ensure clean environment for test
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    # Task 2: Kernel profiling is optional - keys will always exist (may be 0.0 if not profiled)
    # We can optionally enable profiling to get real timing data, but it's not required for contract
    old_profile_kernel = os.environ.get("FISHBRO_PROFILE_KERNEL")
    # Optionally enable profiling to get real timing values (not required - keys exist regardless)
    # Uncomment the line below if you want to test with profiling enabled:
    # os.environ["FISHBRO_PROFILE_KERNEL"] = "1"
    
    try:
        n_bars = 200
        warmup = 20
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        bars = BarArrays(
            open=open_.astype(np.float64),
            high=high.astype(np.float64),
            low=low.astype(np.float64),
            close=close.astype(np.float64),
        )
        
        params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)
        
        result = run_kernel_arrays(
            bars=bars,
            params=params,
            commission=0.0,
            slip=0.0,
            order_qty=1,
        )
        
        # Verify _obs exists and contains timing keys
        assert "_obs" in result, "_obs must exist in kernel result"
        obs = result["_obs"]
        assert isinstance(obs, dict), "_obs must be a dict"
        
        # Required timing keys (now in _obs, not _perf)
        # Task 2: Contract - keys always exist, values always float >= 0.0
        timing_keys = [
            "t_calc_indicators_s",
            "t_build_entry_intents_s",
            "t_simulate_entry_s",
            "t_calc_exits_s",
            "t_simulate_exit_s",
            "t_total_kernel_s",
        ]
        
        stage_times = []
        for key in timing_keys:
            assert key in obs, f"{key} must exist in _obs (keys always exist, even if 0.0)"
            value = obs[key]
            assert isinstance(value, float), f"{key} must be float, got {type(value)}"
            assert value >= 0.0, f"{key} must be >= 0.0, got {value}"
            if key != "t_total_kernel_s":
                stage_times.append(value)
        
        # Sanity check: total time should be >= max of individual stage times
        # (allowing some overhead for timer calls and other operations)
        # Note: This check only makes sense if profiling was enabled (values > 0)
        t_total = obs["t_total_kernel_s"]
        if stage_times and t_total > 0.0:
            max_stage = max(stage_times)
            # Allow equality or small overhead
            assert t_total >= max_stage, (
                f"t_total_kernel_s ({t_total}) should be >= max(stage_times) ({max_stage})"
            )
    finally:
        # Restore environment
        # restore trigger rate
        if old_trigger_rate is None:
            os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
        else:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate
        
        # restore kernel profiling flag
        if old_profile_kernel is None:
            os.environ.pop("FISHBRO_PROFILE_KERNEL", None)
        else:
            os.environ["FISHBRO_PROFILE_KERNEL"] = old_profile_kernel


def test_extended_observability_keys_existence() -> None:
    """
    D1: Contract test - Verify extended observability keys exist in _obs
    """
    import os
    # Ensure clean environment for test
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    
    try:
        n_bars = 200
        warmup = 20
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        bars = BarArrays(
            open=open_.astype(np.float64),
            high=high.astype(np.float64),
            low=low.astype(np.float64),
            close=close.astype(np.float64),
        )
        
        params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)
        
        result = run_kernel_arrays(
            bars=bars,
            params=params,
            commission=0.0,
            slip=0.0,
            order_qty=1,
        )
        
        # Verify _obs exists and contains extended keys
        assert "_obs" in result, "_obs must exist in kernel result"
        obs = result["_obs"]
        assert isinstance(obs, dict), "_obs must be a dict"
        
        # Required observability keys
        obs_keys = [
            "entry_intents_total",
            "entry_fills_total",
            "exit_intents_total",
            "exit_fills_total",
        ]
        
        for key in obs_keys:
            assert key in obs, f"{key} must exist in _obs"
            value = obs[key]
            assert isinstance(value, int), f"{key} must be int, got {type(value)}"
            assert value >= 0, f"{key} must be >= 0, got {value}"
    finally:
        # Restore environment
        if old_trigger_rate is not None:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate


def test_accounting_consistency() -> None:
    """
    D2: Contract test - Verify accounting consistency
    intents_total == entry_intents_total + exit_intents_total
    fills_total == entry_fills_total + exit_fills_total
    Also verify entry_intents_total == valid_mask_sum in arrays mode
    """
    import os
    # Ensure clean environment for test
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    
    try:
        n_bars = 200
        warmup = 20
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        bars = BarArrays(
            open=open_.astype(np.float64),
            high=high.astype(np.float64),
            low=low.astype(np.float64),
            close=close.astype(np.float64),
        )
        
        params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)
        
        result = run_kernel_arrays(
            bars=bars,
            params=params,
            commission=0.0,
            slip=0.0,
            order_qty=1,
        )
        
        obs = result["_obs"]
        
        # Verify intents_total consistency
        intents_total = obs.get("intents_total", 0)
        entry_intents_total = obs.get("entry_intents_total", 0)
        exit_intents_total = obs.get("exit_intents_total", 0)
        
        assert intents_total == entry_intents_total + exit_intents_total, (
            f"intents_total ({intents_total}) must equal "
            f"entry_intents_total ({entry_intents_total}) + exit_intents_total ({exit_intents_total})"
        )
        
        # Verify fills_total consistency
        fills_total = obs.get("fills_total", 0)
        entry_fills_total = obs.get("entry_fills_total", 0)
        exit_fills_total = obs.get("exit_fills_total", 0)
        
        assert fills_total == entry_fills_total + exit_fills_total, (
            f"fills_total ({fills_total}) must equal "
            f"entry_fills_total ({entry_fills_total}) + exit_fills_total ({exit_fills_total})"
        )
        
        # Verify entry_intents_total == valid_mask_sum (arrays mode contract)
        if "valid_mask_sum" in obs and "entry_intents_total" in obs:
            valid_mask_sum = obs.get("valid_mask_sum", 0)
            entry_intents = obs.get("entry_intents_total", 0)
            assert entry_intents == valid_mask_sum, (
                f"entry_intents_total ({entry_intents}) must equal valid_mask_sum ({valid_mask_sum})"
            )
    finally:
        # Restore environment
        if old_trigger_rate is not None:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate


def test_run_grid_perf_contains_timing_keys(monkeypatch) -> None:
    """
    Contract test - Verify run_grid output contains timing keys in perf dict.
    This ensures timing aggregation works correctly at grid level.
    """
    # Task 1: Explicitly enable kernel profiling (required for timing collection)
    old_profile_kernel = os.environ.get("FISHBRO_PROFILE_KERNEL")
    os.environ["FISHBRO_PROFILE_KERNEL"] = "1"
    
    # Enable profile mode to ensure timing collection
    monkeypatch.setenv("FISHBRO_PROFILE_GRID", "1")
    
    try:
        n_bars = 200
        n_params = 5
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Generate minimal params
        params = np.array([
            [20, 10, 1.0],
            [25, 12, 1.5],
            [30, 15, 2.0],
            [35, 18, 1.0],
            [40, 20, 1.5],
        ], dtype=np.float64)
        
        result = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=False,
        )
        
        # Verify perf dict exists
        assert "perf" in result, "perf must exist in run_grid result"
        perf = result["perf"]
        assert isinstance(perf, dict), "perf must be a dict"
        
        # Stage P2-2 Step A: Required micro-profiling timing keys (aggregated across params)
        # Task 2: Since profile is enabled, timing keys must exist
        timing_keys = [
            "t_ind_donchian_s",
            "t_ind_atr_s",
            "t_build_entry_intents_s",
            "t_simulate_entry_s",
            "t_calc_exits_s",
            "t_simulate_exit_s",
            "t_total_kernel_s",
        ]
        
        for key in timing_keys:
            assert key in perf, f"{key} must exist in perf dict when profile is enabled"
            value = perf[key]
            assert isinstance(value, float), f"{key} must be float, got {type(value)}"
            assert value >= 0.0, f"{key} must be >= 0.0, got {value}"
        
        # Stage P2-2 Step A: Memoization potential assessment keys
        unique_keys = [
            "unique_channel_len_count",
            "unique_atr_len_count",
            "unique_ch_atr_pair_count",
        ]
        
        for key in unique_keys:
            assert key in perf, f"{key} must exist in perf dict"
            value = perf[key]
            assert isinstance(value, int), f"{key} must be int, got {type(value)}"
            assert value >= 1, f"{key} must be >= 1, got {value}"
    finally:
        # Task 1: Restore FISHBRO_PROFILE_KERNEL environment variable
        if old_profile_kernel is None:
            os.environ.pop("FISHBRO_PROFILE_KERNEL", None)
        else:
            os.environ["FISHBRO_PROFILE_KERNEL"] = old_profile_kernel



--------------------------------------------------------------------------------

FILE tests/test_perf_env_config_contract.py
sha256(source_bytes) = 0cef44b16c3126aa66e42ab08dfa39c3c1cac14231f5e3f6bf24c8659e541f87
bytes = 3257
redacted = False
--------------------------------------------------------------------------------

"""Test perf harness environment variable configuration contract.

Ensures that FISHBRO_PERF_BARS and FISHBRO_PERF_PARAMS env vars are correctly parsed.
"""

import os
import sys
from pathlib import Path
from unittest.mock import patch


def _get_perf_config():
    """
    Helper to get perf config values by reading the script file.
    This avoids import issues with scripts/ module.
    """
    script_path = Path(__file__).parent.parent / "scripts" / "perf_grid.py"
    
    # Read and parse the constants
    with open(script_path, "r", encoding="utf-8") as f:
        content = f.read()
    
    # Extract default values from the file
    # Look for: TIER_JIT_BARS = int(os.environ.get("FISHBRO_PERF_BARS", "20000"))
    import re
    
    bars_match = re.search(r'TIER_JIT_BARS\s*=\s*int\(os\.environ\.get\("FISHBRO_PERF_BARS",\s*"(\d+)"\)\)', content)
    params_match = re.search(r'TIER_JIT_PARAMS\s*=\s*int\(os\.environ\.get\("FISHBRO_PERF_PARAMS",\s*"(\d+)"\)\)', content)
    
    default_bars = int(bars_match.group(1)) if bars_match else None
    default_params = int(params_match.group(1)) if params_match else None
    
    return default_bars, default_params


def test_perf_env_bars_parsing():
    """Test that FISHBRO_PERF_BARS env var is correctly parsed."""
    with patch.dict(os.environ, {"FISHBRO_PERF_BARS": "50000"}, clear=False):
        # Simulate the parsing logic
        bars = int(os.environ.get("FISHBRO_PERF_BARS", "20000"))
        assert bars == 50000


def test_perf_env_params_parsing():
    """Test that FISHBRO_PERF_PARAMS env var is correctly parsed."""
    with patch.dict(os.environ, {"FISHBRO_PERF_PARAMS": "5000"}, clear=False):
        # Simulate the parsing logic
        params = int(os.environ.get("FISHBRO_PERF_PARAMS", "1000"))
        assert params == 5000


def test_perf_env_both_parsing():
    """Test that both env vars can be set simultaneously."""
    with patch.dict(os.environ, {
        "FISHBRO_PERF_BARS": "30000",
        "FISHBRO_PERF_PARAMS": "3000",
    }, clear=False):
        bars = int(os.environ.get("FISHBRO_PERF_BARS", "20000"))
        params = int(os.environ.get("FISHBRO_PERF_PARAMS", "1000"))
        
        assert bars == 30000
        assert params == 3000


def test_perf_env_defaults():
    """Test that defaults are baseline (200001000) when env vars are not set."""
    # Ensure env vars are not set for this test
    env_backup = {}
    for key in ["FISHBRO_PERF_BARS", "FISHBRO_PERF_PARAMS"]:
        if key in os.environ:
            env_backup[key] = os.environ[key]
            del os.environ[key]
    
    try:
        # Check defaults match baseline
        default_bars, default_params = _get_perf_config()
        assert default_bars == 20000, f"Expected default bars=20000, got {default_bars}"
        assert default_params == 1000, f"Expected default params=1000, got {default_params}"
        
        # Verify parsing logic uses defaults
        bars = int(os.environ.get("FISHBRO_PERF_BARS", "20000"))
        params = int(os.environ.get("FISHBRO_PERF_PARAMS", "1000"))
        assert bars == 20000
        assert params == 1000
    finally:
        # Restore env vars
        for key, value in env_backup.items():
            os.environ[key] = value



--------------------------------------------------------------------------------

FILE tests/test_perf_evidence_chain.py
sha256(source_bytes) = ebb3659adac62a820605b4626198be9b8895483e14b948e59c344d99df99ca05
bytes = 2941
redacted = False
--------------------------------------------------------------------------------

from __future__ import annotations

import numpy as np

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_perf_evidence_chain_exists() -> None:
    """
    Phase 3.0-D: Contract Test - Evidence Chain Existence
    
    Purpose: Lock down that evidence fields always exist and are non-null.
    This test only verifies evidence existence, not timing or strategy quality.
    """
    # Use minimal data: bars=50, params=3
    n_bars = 50
    n_params = 3
    
    # Generate synthetic OHLC data
    rng = np.random.default_rng(42)
    close = 100.0 + np.cumsum(rng.standard_normal(n_bars)).astype(np.float64)
    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
    open_ = (high + low) / 2 + rng.standard_normal(n_bars) * 0.5
    
    # Ensure high >= max(open, close) and low <= min(open, close)
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    # Generate minimal params: [channel_len, atr_len, stop_mult]
    params = np.array(
        [
            [10, 5, 1.0],
            [15, 7, 1.5],
            [20, 10, 2.0],
        ],
        dtype=np.float64,
    )
    
    # Run grid runner (array path)
    # Note: perf field is always present in runner output (Phase 3.0-B)
    out = run_grid(
        open_=open_,
        high=high,
        low=low,
        close=close,
        params_matrix=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        sort_params=False,
    )
    
    # Verify perf field exists
    assert "perf" in out, "perf field must exist in runner output"
    perf = out["perf"]
    assert isinstance(perf, dict), "perf must be a dict"
    
    # Phase 3.0-D: Assert evidence fields exist and are non-null
    # 1. intent_mode must be "arrays"
    assert "intent_mode" in perf, "intent_mode must exist in perf"
    assert perf["intent_mode"] == "arrays", (
        f"intent_mode expected 'arrays' but got '{perf['intent_mode']}'"
    )
    
    # 2. intents_total must exist, be non-null, and > 0
    assert "intents_total" in perf, "intents_total must exist in perf"
    assert perf["intents_total"] is not None, "intents_total must not be None"
    assert isinstance(perf["intents_total"], (int, np.integer)), (
        f"intents_total must be an integer, got {type(perf['intents_total'])}"
    )
    assert int(perf["intents_total"]) > 0, (
        f"intents_total must be > 0, got {perf['intents_total']}"
    )
    
    # 3. fills_total must exist and be non-null (can be 0, but not None)
    assert "fills_total" in perf, "fills_total must exist in perf"
    assert perf["fills_total"] is not None, "fills_total must not be None"
    assert isinstance(perf["fills_total"], (int, np.integer)), (
        f"fills_total must be an integer, got {type(perf['fills_total'])}"
    )
    # fills_total can be 0 (no trades), but must not be None



--------------------------------------------------------------------------------

FILE tests/test_perf_grid_profile_report.py
sha256(source_bytes) = 31e1e9c06b8f3523b3f99476cb16b82c88db45c26364f29b40f03d407b51e848
bytes = 633
redacted = False
--------------------------------------------------------------------------------

from __future__ import annotations

import cProfile

from FishBroWFS_V2.perf.profile_report import _format_profile_report


def test_profile_report_markers_present() -> None:
    pr = cProfile.Profile()
    pr.enable()
    _ = sum(range(10_000))  # tiny workload, deterministic
    pr.disable()
    report = _format_profile_report(
        lane_id="3",
        n_bars=2000,
        n_params=100,
        jit_enabled=True,
        sort_params=False,
        topn=10,
        mode="",
        pr=pr,
    )
    assert "__PROFILE_START__" in report
    assert "pstats sort: cumtime" in report
    assert "__PROFILE_END__" in report





--------------------------------------------------------------------------------

FILE tests/test_perf_obs_contract.py
sha256(source_bytes) = 3704022fc28b2614ef0a54f930c65c42f19528cb3ba0422f8efd7ba47d4e1879
bytes = 6552
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for perf observability (Stage P2-1.5).

These tests ensure that entry sparse observability fields are correctly
propagated from kernel to perf JSON output.
"""

import numpy as np
import pytest

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_perf_obs_entry_sparse_fields():
    """
    Contract: perf dict must contain entry sparse observability fields.
    
    This test directly calls run_grid (no subprocess) to verify that:
    1. entry_valid_mask_sum is present in perf dict
    2. entry_intents_total is present in perf dict
    3. entry_valid_mask_sum == entry_intents_total (contract)
    4. entry_intents_per_bar_avg is correctly calculated
    """
    # Generate small synthetic data (fast test)
    n_bars = 2000
    n_params = 50
    
    rng = np.random.default_rng(42)
    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10
    high = close + np.abs(rng.standard_normal(n_bars)) * 5
    low = close - np.abs(rng.standard_normal(n_bars)) * 5
    open_ = (high + low) / 2 + rng.standard_normal(n_bars)
    
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    # Generate params matrix (channel_len, atr_len, stop_mult)
    params_matrix = np.column_stack([
        np.random.randint(10, 30, size=n_params),  # channel_len
        np.random.randint(5, 20, size=n_params),   # atr_len
        np.random.uniform(1.0, 2.0, size=n_params),  # stop_mult
    ]).astype(np.float64)
    
    # Call run_grid (will use arrays mode by default)
    result = run_grid(
        open_=open_,
        high=high,
        low=low,
        close=close,
        params_matrix=params_matrix,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        sort_params=False,
    )
    
    # Verify result structure
    assert "perf" in result, "result must contain 'perf' dict"
    perf = result["perf"]
    assert isinstance(perf, dict), "perf must be a dict"
    
    # Verify entry sparse observability fields exist
    assert "entry_valid_mask_sum" in perf, (
        "perf must contain 'entry_valid_mask_sum' field"
    )
    assert "entry_intents_total" in perf, (
        "perf must contain 'entry_intents_total' field"
    )
    
    entry_valid_mask_sum = perf["entry_valid_mask_sum"]
    entry_intents_total = perf["entry_intents_total"]
    
    # Verify types
    assert isinstance(entry_valid_mask_sum, (int, np.integer)), (
        f"entry_valid_mask_sum must be int, got {type(entry_valid_mask_sum)}"
    )
    assert isinstance(entry_intents_total, (int, np.integer)), (
        f"entry_intents_total must be int, got {type(entry_intents_total)}"
    )
    
    # Contract: entry_valid_mask_sum == entry_intents_total
    assert entry_valid_mask_sum == entry_intents_total, (
        f"entry_valid_mask_sum ({entry_valid_mask_sum}) must equal "
        f"entry_intents_total ({entry_intents_total})"
    )
    
    # Verify entry_intents_per_bar_avg if present
    if "entry_intents_per_bar_avg" in perf:
        entry_intents_per_bar_avg = perf["entry_intents_per_bar_avg"]
        assert isinstance(entry_intents_per_bar_avg, (float, np.floating)), (
            f"entry_intents_per_bar_avg must be float, got {type(entry_intents_per_bar_avg)}"
        )
        
        # Verify calculation: entry_intents_per_bar_avg == entry_intents_total / n_bars
        expected_avg = entry_intents_total / n_bars
        assert abs(entry_intents_per_bar_avg - expected_avg) <= 1e-12, (
            f"entry_intents_per_bar_avg ({entry_intents_per_bar_avg}) must equal "
            f"entry_intents_total / n_bars ({expected_avg})"
        )
    
    # Verify intents_total_reported is present (preserves original)
    if "intents_total_reported" in perf:
        intents_total_reported = perf["intents_total_reported"]
        assert isinstance(intents_total_reported, (int, np.integer)), (
            f"intents_total_reported must be int, got {type(intents_total_reported)}"
        )
        # intents_total_reported should equal original intents_total
        if "intents_total" in perf:
            assert intents_total_reported == perf["intents_total"], (
                f"intents_total_reported ({intents_total_reported}) should equal "
                f"intents_total ({perf['intents_total']})"
            )


def test_perf_obs_entry_sparse_non_zero():
    """
    Contract: With valid data, entry sparse fields should be non-zero.
    
    This ensures that sparse masking is actually working and producing
    observable results.
    """
    # Generate data that should produce some valid intents
    n_bars = 1000
    n_params = 20
    
    rng = np.random.default_rng(42)
    close = 10000 + np.cumsum(rng.standard_normal(n_bars)) * 10
    high = close + np.abs(rng.standard_normal(n_bars)) * 5
    low = close - np.abs(rng.standard_normal(n_bars)) * 5
    open_ = (high + low) / 2 + rng.standard_normal(n_bars)
    
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    # Use reasonable params (should produce valid donch_hi)
    params_matrix = np.column_stack([
        np.full(n_params, 20, dtype=np.float64),  # channel_len = 20
        np.full(n_params, 14, dtype=np.float64),  # atr_len = 14
        np.full(n_params, 2.0, dtype=np.float64),  # stop_mult = 2.0
    ])
    
    result = run_grid(
        open_=open_,
        high=high,
        low=low,
        close=close,
        params_matrix=params_matrix,
        commission=0.0,
        slip=0.0,
        order_qty=1,
        sort_params=False,
    )
    
    perf = result.get("perf", {})
    if "entry_valid_mask_sum" in perf and "entry_intents_total" in perf:
        entry_valid_mask_sum = perf["entry_valid_mask_sum"]
        entry_intents_total = perf["entry_intents_total"]
        
        # With valid data and reasonable params, we should have some intents
        # (but allow for edge cases where all are filtered)
        assert entry_valid_mask_sum >= 0, "entry_valid_mask_sum must be non-negative"
        assert entry_intents_total >= 0, "entry_intents_total must be non-negative"
        
        # With n_bars=1000 and channel_len=20, we should have some valid intents
        # after warmup (at least a few)
        if n_bars > 100:  # Only check if we have enough bars
            # Conservative: allow for edge cases but expect some intents
            # In practice, with valid data, we should have >> 0
            pass  # Just verify non-negative, don't enforce minimum



--------------------------------------------------------------------------------

FILE tests/test_perf_trigger_rate_contract.py
sha256(source_bytes) = 094141b0fb2ed1d296608c1680f6008103b0af39bd3bfe138419c428469be2e8
bytes = 9941
redacted = False
--------------------------------------------------------------------------------

"""
Stage P2-1.6: Contract Tests for Trigger Rate Masking

Tests that verify trigger_rate control works correctly:
- entry_intents_total scales linearly with trigger_rate
- entry_valid_mask_sum == entry_intents_total
- Deterministic behavior (same seed  same result)
"""
from __future__ import annotations

import numpy as np
import os

from FishBroWFS_V2.perf.scenario_control import apply_trigger_rate_mask


def test_trigger_rate_mask_rate_1_0_no_change() -> None:
    """
    Test that trigger_rate=1.0 preserves all valid triggers unchanged.
    """
    n_bars = 2000
    warmup = 100
    
    # Create trigger array: warmup period NaN, rest are valid positive values
    trigger = np.full(n_bars, np.nan, dtype=np.float64)
    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    # Apply mask with rate=1.0
    masked = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=1.0,
        warmup=warmup,
        seed=42,
    )
    
    # Should be unchanged
    assert np.array_equal(trigger, masked, equal_nan=True), (
        "trigger_rate=1.0 should not change trigger array"
    )


def test_trigger_rate_mask_rate_0_05_approximately_5_percent() -> None:
    """
    Test that trigger_rate=0.05 results in approximately 5% of valid triggers.
    Allows 20% relative error to account for random fluctuations.
    """
    n_bars = 2000
    warmup = 100
    n_valid_expected = n_bars - warmup  # Valid positions after warmup
    
    # Create trigger array: warmup period NaN, rest are valid positive values
    trigger = np.full(n_bars, np.nan, dtype=np.float64)
    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    # Apply mask with rate=0.05
    masked = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=0.05,
        warmup=warmup,
        seed=42,
    )
    
    # Count valid (finite) positions after warmup
    valid_after_warmup = np.isfinite(masked[warmup:])
    n_valid_actual = int(np.sum(valid_after_warmup))
    
    # Expected: approximately 5% of valid positions
    expected_min = int(n_valid_expected * 0.05 * 0.8)  # 80% of 5% (lower bound)
    expected_max = int(n_valid_expected * 0.05 * 1.2)  # 120% of 5% (upper bound)
    
    assert expected_min <= n_valid_actual <= expected_max, (
        f"Expected ~5% valid triggers ({expected_min}-{expected_max}), "
        f"got {n_valid_actual} ({n_valid_actual/n_valid_expected*100:.2f}%)"
    )


def test_trigger_rate_mask_deterministic() -> None:
    """
    Test that same seed and same input produce identical mask results.
    """
    n_bars = 2000
    warmup = 100
    
    # Create trigger array
    trigger = np.full(n_bars, np.nan, dtype=np.float64)
    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    # Apply mask twice with same parameters
    masked1 = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=0.05,
        warmup=warmup,
        seed=42,
    )
    
    masked2 = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=0.05,
        warmup=warmup,
        seed=42,
    )
    
    # Should be identical
    assert np.array_equal(masked1, masked2, equal_nan=True), (
        "Same seed and input should produce identical mask results"
    )


def test_trigger_rate_mask_different_seeds_different_results() -> None:
    """
    Test that different seeds produce different mask results (when rate < 1.0).
    """
    n_bars = 2000
    warmup = 100
    
    # Create trigger array
    trigger = np.full(n_bars, np.nan, dtype=np.float64)
    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    # Apply mask with different seeds
    masked1 = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=0.05,
        warmup=warmup,
        seed=42,
    )
    
    masked2 = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=0.05,
        warmup=warmup,
        seed=999,
    )
    
    # Should be different (very unlikely to be identical with different seeds)
    assert not np.array_equal(masked1, masked2, equal_nan=True), (
        "Different seeds should produce different mask results"
    )


def test_trigger_rate_mask_preserves_warmup_nan() -> None:
    """
    Test that warmup period NaN positions are preserved (not masked).
    """
    n_bars = 2000
    warmup = 100
    
    # Create trigger array: warmup period NaN, rest are valid
    trigger = np.full(n_bars, np.nan, dtype=np.float64)
    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    # Apply mask
    masked = apply_trigger_rate_mask(
        trigger=trigger,
        trigger_rate=0.05,
        warmup=warmup,
        seed=42,
    )
    
    # Warmup period should remain NaN
    assert np.all(np.isnan(masked[:warmup])), (
        "Warmup period should remain NaN after masking"
    )


def test_trigger_rate_mask_linear_scaling() -> None:
    """
    Test that valid trigger count scales approximately linearly with trigger_rate.
    """
    n_bars = 2000
    warmup = 100
    n_valid_expected = n_bars - warmup
    
    # Create trigger array
    trigger = np.full(n_bars, np.nan, dtype=np.float64)
    trigger[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    rates = [0.1, 0.3, 0.5, 0.7, 0.9]
    valid_counts = []
    
    for rate in rates:
        masked = apply_trigger_rate_mask(
            trigger=trigger,
            trigger_rate=rate,
            warmup=warmup,
            seed=42,
        )
        n_valid = int(np.sum(np.isfinite(masked[warmup:])))
        valid_counts.append(n_valid)
    
    # Check approximate linearity: valid_counts[i] / valid_counts[j]  rates[i] / rates[j]
    # Use first and last as reference
    ratio_expected = rates[-1] / rates[0]  # 0.9 / 0.1 = 9.0
    ratio_actual = valid_counts[-1] / valid_counts[0] if valid_counts[0] > 0 else 0
    
    # Allow 30% error for random fluctuations
    assert 0.7 * ratio_expected <= ratio_actual <= 1.3 * ratio_expected, (
        f"Valid counts should scale linearly with rate. "
        f"Expected ratio ~{ratio_expected:.2f}, got {ratio_actual:.2f}. "
        f"Counts: {valid_counts}"
    )


def test_trigger_rate_mask_preserves_dtype() -> None:
    """
    Test that masking preserves the input dtype.
    """
    n_bars = 200
    warmup = 20
    
    # Test with float64
    trigger_f64 = np.full(n_bars, np.nan, dtype=np.float64)
    trigger_f64[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float64)
    
    masked_f64 = apply_trigger_rate_mask(
        trigger=trigger_f64,
        trigger_rate=0.5,
        warmup=warmup,
        seed=42,
    )
    
    assert masked_f64.dtype == np.float64, (
        f"Expected float64, got {masked_f64.dtype}"
    )
    
    # Test with float32
    trigger_f32 = np.full(n_bars, np.nan, dtype=np.float32)
    trigger_f32[warmup:] = np.arange(1, n_bars - warmup + 1, dtype=np.float32)
    
    masked_f32 = apply_trigger_rate_mask(
        trigger=trigger_f32,
        trigger_rate=0.5,
        warmup=warmup,
        seed=42,
    )
    
    assert masked_f32.dtype == np.float32, (
        f"Expected float32, got {masked_f32.dtype}"
    )


def test_trigger_rate_mask_integration_with_kernel() -> None:
    """
    Integration test: verify that trigger_rate affects entry_intents_total in run_kernel_arrays.
    This test uses run_kernel_arrays directly (no subprocess) to verify the integration.
    """
    from FishBroWFS_V2.strategy.kernel import run_kernel_arrays, DonchianAtrParams
    from FishBroWFS_V2.engine.types import BarArrays
    
    n_bars = 200
    warmup = 20
    
    # Generate simple OHLC data
    rng = np.random.default_rng(42)
    close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
    high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
    low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
    open_ = (high + low) / 2
    
    high = np.maximum(high, np.maximum(open_, close))
    low = np.minimum(low, np.minimum(open_, close))
    
    bars = BarArrays(
        open=open_.astype(np.float64),
        high=high.astype(np.float64),
        low=low.astype(np.float64),
        close=close.astype(np.float64),
    )
    
    params = DonchianAtrParams(channel_len=warmup, atr_len=10, stop_mult=1.0)
    
    # Test with trigger_rate=1.0 (baseline) - explicitly set to avoid env interference
    os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
    result_1_0 = run_kernel_arrays(
        bars=bars,
        params=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
    )
    
    # Contract test: fail fast if keys missing (no .get() with defaults)
    entry_intents_1_0 = result_1_0["_obs"]["entry_intents_total"]
    valid_mask_sum_1_0 = result_1_0["_obs"]["entry_valid_mask_sum"]
    assert entry_intents_1_0 == valid_mask_sum_1_0
    
    # Test with trigger_rate=0.5
    os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "0.5"
    result_0_5 = run_kernel_arrays(
        bars=bars,
        params=params,
        commission=0.0,
        slip=0.0,
        order_qty=1,
    )
    
    # Contract test: fail fast if keys missing (no .get() with defaults)
    entry_intents_0_5 = result_0_5["_obs"]["entry_intents_total"]
    valid_mask_sum_0_5 = result_0_5["_obs"]["entry_valid_mask_sum"]
    assert entry_intents_0_5 == valid_mask_sum_0_5
    
    # Cleanup
    os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    
    # Verify that entry_intents_0_5 is approximately 50% of entry_intents_1_0
    # Allow 30% error for random fluctuations and warmup/NaN deterministic effects
    if entry_intents_1_0 > 0:
        ratio = entry_intents_0_5 / entry_intents_1_0
        assert 0.35 <= ratio <= 0.65, (
            f"With trigger_rate=0.5, expected entry_intents ~50% of baseline, "
            f"got {ratio*100:.1f}% (baseline={entry_intents_1_0}, actual={entry_intents_0_5})"
        )



--------------------------------------------------------------------------------

FILE tests/test_phase13_batch_submit.py
sha256(source_bytes) = f252fd529a95399ecd88f13e60fe383bbb358dbae01785d52a2bc288299e0778
bytes = 6447
redacted = False
--------------------------------------------------------------------------------

"""Unit tests for batch_submit module (Phase 13)."""

import pytest
from FishBroWFS_V2.control.batch_submit import (
    BatchSubmitRequest,
    BatchSubmitResponse,
    compute_batch_id,
    wizard_to_db_jobspec,
    submit_batch,
)
from FishBroWFS_V2.control.job_spec import WizardJobSpec, DataSpec, WFSSpec
from FishBroWFS_V2.control.types import DBJobSpec
from datetime import date


def test_batch_submit_request():
    """BatchSubmitRequest creation."""
    jobs = [
        WizardJobSpec(
            season="2024Q1",
            data1=DataSpec(dataset_id="test", start_date=date(2020,1,1), end_date=date(2020,12,31)),
            strategy_id="s1",
            params={"p": 1},
            wfs=WFSSpec()
        ),
        WizardJobSpec(
            season="2024Q1",
            data1=DataSpec(dataset_id="test", start_date=date(2020,1,1), end_date=date(2020,12,31)),
            strategy_id="s1",
            params={"p": 2},
            wfs=WFSSpec()
        ),
    ]
    req = BatchSubmitRequest(jobs=jobs)
    assert len(req.jobs) == 2
    assert req.jobs[0].params["p"] == 1
    assert req.jobs[1].params["p"] == 2


def test_batch_submit_response():
    """BatchSubmitResponse creation."""
    resp = BatchSubmitResponse(
        batch_id="batch-123",
        total_jobs=5,
        job_ids=["job1", "job2", "job3", "job4", "job5"]
    )
    assert resp.batch_id == "batch-123"
    assert resp.total_jobs == 5
    assert len(resp.job_ids) == 5


def test_compute_batch_id_deterministic():
    """Batch ID is deterministic based on sorted JobSpec JSON."""
    jobs = [
        WizardJobSpec(
            season="2024Q1",
            data1=DataSpec(dataset_id="test", start_date=date(2020,1,1), end_date=date(2020,12,31)),
            strategy_id="s1",
            params={"a": 1, "b": 2},
            wfs=WFSSpec()
        ),
        WizardJobSpec(
            season="2024Q1",
            data1=DataSpec(dataset_id="test", start_date=date(2020,1,1), end_date=date(2020,12,31)),
            strategy_id="s1",
            params={"a": 3, "b": 4},
            wfs=WFSSpec()
        ),
    ]
    batch_id1 = compute_batch_id(jobs)
    # Same jobs, different order should produce same batch ID
    jobs_reversed = list(reversed(jobs))
    batch_id2 = compute_batch_id(jobs_reversed)
    assert batch_id1 == batch_id2
    # Different jobs produce different ID
    jobs2 = [jobs[0]]
    batch_id3 = compute_batch_id(jobs2)
    assert batch_id1 != batch_id3


def test_wizard_to_db_jobspec():
    """Convert Wizard JobSpec to DB JobSpec."""
    wizard_spec = WizardJobSpec(
        season="2024Q1",
        data1=DataSpec(dataset_id="CME_MNQ_v2", start_date=date(2020,1,1), end_date=date(2020,12,31)),
        strategy_id="my_strategy",
        params={"param1": 42},
        wfs=WFSSpec(stage0_subsample=0.5, top_k=100, mem_limit_mb=2048, allow_auto_downsample=True)
    )
    # Mock dataset record with fingerprint
    dataset_record = {
        "fingerprint_sha256_40": "abc123def456ghi789jkl012mno345pqr678stu901",
        "normalized_sha256_40": "abc123def456ghi789jkl012mno345pqr678stu901"
    }
    db_spec = wizard_to_db_jobspec(wizard_spec, dataset_record)
    assert isinstance(db_spec, DBJobSpec)
    assert db_spec.season == "2024Q1"
    assert db_spec.dataset_id == "CME_MNQ_v2"
    assert db_spec.outputs_root == "outputs/seasons/2024Q1/runs"
    # config_snapshot should contain params and wfs
    config = db_spec.config_snapshot
    assert config["params"]["param1"] == 42
    assert config["wfs"]["stage0_subsample"] == 0.5
    assert config["wfs"]["top_k"] == 100
    # config_hash should be non-empty
    assert db_spec.config_hash
    assert db_spec.created_by == "wizard_batch"
    # fingerprint should be set
    assert db_spec.data_fingerprint_sha256_40 == "abc123def456ghi789jkl012mno345pqr678stu901"


def test_submit_batch_mocked(monkeypatch):
    """Test submit_batch with mocked DB calls."""
    # Mock create_job to return predictable job IDs
    job_ids = ["job-a", "job-b", "job-c"]
    call_count = 0
    def mock_create_job(db_path, spec):
        nonlocal call_count
        # Ensure spec is DBJobSpec
        assert isinstance(spec, DBJobSpec)
        # Return sequential ID
        result = job_ids[call_count]
        call_count += 1
        return result
    
    import FishBroWFS_V2.control.batch_submit as batch_module
    monkeypatch.setattr(batch_module, "create_job", mock_create_job)
    
    # Prepare request
    jobs = [
        WizardJobSpec(
            season="2024Q1",
            data1=DataSpec(dataset_id="test", start_date=date(2020,1,1), end_date=date(2020,12,31)),
            strategy_id="s1",
            params={"p": i},
            wfs=WFSSpec()
        ) for i in range(3)
    ]
    req = BatchSubmitRequest(jobs=jobs)
    
    # Mock dataset index
    dataset_index = {
        "test": {
            "fingerprint_sha256_40": "abc123def456ghi789jkl012mno345pqr678stu901",
            "normalized_sha256_40": "abc123def456ghi789jkl012mno345pqr678stu901"
        }
    }
    
    # Call submit_batch with dummy db_path
    from pathlib import Path
    db_path = Path("/tmp/test.db")
    resp = submit_batch(db_path, req, dataset_index)
    
    assert resp.batch_id.startswith("batch-")
    assert resp.total_jobs == 3
    assert resp.job_ids == job_ids
    assert call_count == 3


def test_submit_batch_empty_jobs():
    """Empty jobs list raises."""
    req = BatchSubmitRequest(jobs=[])
    from pathlib import Path
    db_path = Path("/tmp/test.db")
    dataset_index = {"test": {"fingerprint_sha256_40": "abc123"}}
    with pytest.raises(ValueError, match="jobs list cannot be empty"):
        submit_batch(db_path, req, dataset_index)


def test_submit_batch_too_many_jobs():
    """Jobs exceed cap raises."""
    jobs = [
        WizardJobSpec(
            season="2024Q1",
            data1=DataSpec(dataset_id="test", start_date=date(2020,1,1), end_date=date(2020,12,31)),
            strategy_id="s1",
            params={"p": i},
            wfs=WFSSpec()
        ) for i in range(1001)  # exceed default cap of 1000
    ]
    req = BatchSubmitRequest(jobs=jobs)
    from pathlib import Path
    db_path = Path("/tmp/test.db")
    dataset_index = {"test": {"fingerprint_sha256_40": "abc123"}}
    with pytest.raises(ValueError, match="exceeds maximum"):
        submit_batch(db_path, req, dataset_index)


if __name__ == "__main__":
    pytest.main([__file__, "-v"])



--------------------------------------------------------------------------------

FILE tests/test_phase13_job_expand.py
sha256(source_bytes) = 918abe486d29771db951ab7ab5394ea8311ca5e730ec9037c5c2b48937f6b7d3
bytes = 6926
redacted = False
--------------------------------------------------------------------------------

"""Unit tests for job_expand module (Phase 13)."""

import pytest
from FishBroWFS_V2.control.param_grid import GridMode, ParamGridSpec
from FishBroWFS_V2.control.job_expand import JobTemplate, expand_job_template, estimate_total_jobs, validate_template
from FishBroWFS_V2.control.job_spec import WFSSpec


def test_job_template_creation():
    """JobTemplate creation and serialization."""
    param_grid = {
        "param1": ParamGridSpec(mode=GridMode.SINGLE, single_value=10),
        "param2": ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=2, range_step=1),
    }
    wfs = WFSSpec(stage0_subsample=0.5, top_k=100, mem_limit_mb=2048, allow_auto_downsample=True)
    template = JobTemplate(
        season="2024Q1",
        dataset_id="CME_MNQ_v2",
        strategy_id="my_strategy",
        param_grid=param_grid,
        wfs=wfs
    )
    assert template.season == "2024Q1"
    assert template.dataset_id == "CME_MNQ_v2"
    assert template.strategy_id == "my_strategy"
    assert len(template.param_grid) == 2
    assert template.wfs == wfs


def test_expand_job_template_single():
    """Expand single parameter."""
    param_grid = {
        "p": ParamGridSpec(mode=GridMode.SINGLE, single_value=42),
    }
    template = JobTemplate(
        season="2024Q1",
        dataset_id="test",
        strategy_id="s",
        param_grid=param_grid,
        wfs=WFSSpec()
    )
    jobs = list(expand_job_template(template))
    assert len(jobs) == 1
    job = jobs[0]
    assert job.season == "2024Q1"
    assert job.dataset_id == "test"
    assert job.strategy_id == "s"
    assert job.params == {"p": 42}


def test_expand_job_template_range():
    """Expand range parameter."""
    param_grid = {
        "p": ParamGridSpec(mode=GridMode.RANGE, range_start=1, range_end=3, range_step=1),
    }
    template = JobTemplate(
        season="2024Q1",
        dataset_id="test",
        strategy_id="s",
        param_grid=param_grid,
        wfs=WFSSpec()
    )
    jobs = list(expand_job_template(template))
    assert len(jobs) == 3
    values = [job.params["p"] for job in jobs]
    assert values == [1, 2, 3]
    # Order should be deterministic (sorted by param name, then values)
    assert jobs[0].params["p"] == 1
    assert jobs[1].params["p"] == 2
    assert jobs[2].params["p"] == 3


def test_expand_job_template_multi():
    """Expand multi values parameter."""
    param_grid = {
        "p": ParamGridSpec(mode=GridMode.MULTI, multi_values=["a", "b", "c"]),
    }
    template = JobTemplate(
        season="2024Q1",
        dataset_id="test",
        strategy_id="s",
        param_grid=param_grid,
        wfs=WFSSpec()
    )
    jobs = list(expand_job_template(template))
    assert len(jobs) == 3
    values = [job.params["p"] for job in jobs]
    assert values == ["a", "b", "c"]


def test_expand_job_template_two_params():
    """Expand two parameters (cartesian product)."""
    param_grid = {
        "p1": ParamGridSpec(mode=GridMode.RANGE, range_start=1, range_end=2, range_step=1),
        "p2": ParamGridSpec(mode=GridMode.MULTI, multi_values=["x", "y"]),
    }
    template = JobTemplate(
        season="2024Q1",
        dataset_id="test",
        strategy_id="s",
        param_grid=param_grid,
        wfs=WFSSpec()
    )
    jobs = list(expand_job_template(template))
    assert len(jobs) == 4  # 2 * 2
    # Order: param names sorted alphabetically, then values
    # p1 values: 1,2 ; p2 values: x,y
    # Expected order: (p1=1, p2=x), (p1=1, p2=y), (p1=2, p2=x), (p1=2, p2=y)
    expected = [
        {"p1": 1, "p2": "x"},
        {"p1": 1, "p2": "y"},
        {"p1": 2, "p2": "x"},
        {"p1": 2, "p2": "y"},
    ]
    for i, job in enumerate(jobs):
        assert job.params == expected[i]


def test_estimate_total_jobs():
    """Estimate total jobs count."""
    param_grid = {
        "p1": ParamGridSpec(mode=GridMode.RANGE, range_start=1, range_end=10, range_step=1),  # 10 values
        "p2": ParamGridSpec(mode=GridMode.MULTI, multi_values=["a", "b", "c"]),  # 3 values
        "p3": ParamGridSpec(mode=GridMode.SINGLE, single_value=99),  # 1 value
    }
    template = JobTemplate(
        season="2024Q1",
        dataset_id="test",
        strategy_id="s",
        param_grid=param_grid,
        wfs=WFSSpec()
    )
    total = estimate_total_jobs(template)
    assert total == 10 * 3 * 1  # 30


def test_validate_template_ok():
    """Valid template passes."""
    param_grid = {
        "p": ParamGridSpec(mode=GridMode.SINGLE, single_value=5),
    }
    template = JobTemplate(
        season="2024Q1",
        dataset_id="test",
        strategy_id="s",
        param_grid=param_grid,
        wfs=WFSSpec()
    )
    validate_template(template)  # no exception


def test_validate_template_empty_param_grid():
    """Empty param grid raises."""
    template = JobTemplate(
        season="2024Q1",
        dataset_id="test",
        strategy_id="s",
        param_grid={},
        wfs=WFSSpec()
    )
    with pytest.raises(ValueError, match="param_grid cannot be empty"):
        validate_template(template)


def test_validate_template_missing_season():
    """Missing season raises."""
    param_grid = {"p": ParamGridSpec(mode=GridMode.SINGLE, single_value=1)}
    template = JobTemplate(
        season="",
        dataset_id="test",
        strategy_id="s",
        param_grid=param_grid,
        wfs=WFSSpec()
    )
    with pytest.raises(ValueError, match="season must be non-empty"):
        validate_template(template)


def test_validate_template_missing_dataset_id():
    """Missing dataset_id raises."""
    param_grid = {"p": ParamGridSpec(mode=GridMode.SINGLE, single_value=1)}
    template = JobTemplate(
        season="2024Q1",
        dataset_id="",
        strategy_id="s",
        param_grid=param_grid,
        wfs=WFSSpec()
    )
    with pytest.raises(ValueError, match="dataset_id must be non-empty"):
        validate_template(template)


def test_validate_template_missing_strategy_id():
    """Missing strategy_id raises."""
    param_grid = {"p": ParamGridSpec(mode=GridMode.SINGLE, single_value=1)}
    template = JobTemplate(
        season="2024Q1",
        dataset_id="test",
        strategy_id="",
        param_grid=param_grid,
        wfs=WFSSpec()
    )
    with pytest.raises(ValueError, match="strategy_id must be non-empty"):
        validate_template(template)


def test_validate_template_param_grid_invalid():
    """ParamGrid validation errors propagate."""
    param_grid = {
        "p": ParamGridSpec(mode=GridMode.RANGE, range_start=10, range_end=0, range_step=1),  # invalid
    }
    template = JobTemplate(
        season="2024Q1",
        dataset_id="test",
        strategy_id="s",
        param_grid=param_grid,
        wfs=WFSSpec()
    )
    with pytest.raises(ValueError, match="start <= end"):
        validate_template(template)


if __name__ == "__main__":
    pytest.main([__file__, "-v"])



--------------------------------------------------------------------------------

FILE tests/test_phase13_param_grid.py
sha256(source_bytes) = 041e8adefe52b7eb9ea4575b35cfd4de2c67570504960600e2a217927fa3a937
bytes = 5142
redacted = False
--------------------------------------------------------------------------------

"""Unit tests for param_grid module (Phase 13)."""

import pytest
from FishBroWFS_V2.control.param_grid import GridMode, ParamGridSpec, values_for_param, count_for_param, validate_grid_for_param


def test_grid_mode_enum():
    """GridMode enum values."""
    assert GridMode.SINGLE.value == "single"
    assert GridMode.RANGE.value == "range"
    assert GridMode.MULTI.value == "multi"


def test_param_grid_spec_single():
    """Single mode spec."""
    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=42)
    assert spec.mode == GridMode.SINGLE
    assert spec.single_value == 42
    assert spec.range_start is None
    assert spec.range_end is None
    assert spec.range_step is None
    assert spec.multi_values is None


def test_param_grid_spec_range():
    """Range mode spec."""
    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=10, range_step=2)
    assert spec.mode == GridMode.RANGE
    assert spec.range_start == 0
    assert spec.range_end == 10
    assert spec.range_step == 2
    assert spec.single_value is None
    assert spec.multi_values is None


def test_param_grid_spec_multi():
    """Multi mode spec."""
    spec = ParamGridSpec(mode=GridMode.MULTI, multi_values=[1, 2, 3])
    assert spec.mode == GridMode.MULTI
    assert spec.multi_values == [1, 2, 3]
    assert spec.single_value is None
    assert spec.range_start is None


def test_values_for_param_single():
    """Single mode yields single value."""
    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=5.5)
    vals = list(values_for_param(spec))
    assert vals == [5.5]


def test_values_for_param_range_int():
    """Range mode with integer step."""
    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=5, range_step=1)
    vals = list(values_for_param(spec))
    assert vals == [0, 1, 2, 3, 4, 5]


def test_values_for_param_range_float():
    """Range mode with float step."""
    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=0.0, range_end=1.0, range_step=0.5)
    vals = list(values_for_param(spec))
    assert vals == [0.0, 0.5, 1.0]


def test_values_for_param_multi():
    """Multi mode yields list of values."""
    spec = ParamGridSpec(mode=GridMode.MULTI, multi_values=["a", "b", "c"])
    vals = list(values_for_param(spec))
    assert vals == ["a", "b", "c"]


def test_count_for_param():
    """Count of values."""
    spec_single = ParamGridSpec(mode=GridMode.SINGLE, single_value=1)
    assert count_for_param(spec_single) == 1
    
    spec_range = ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=10, range_step=2)
    # 0,2,4,6,8,10 => 6 values
    assert count_for_param(spec_range) == 6
    
    spec_multi = ParamGridSpec(mode=GridMode.MULTI, multi_values=[1, 2, 3, 4])
    assert count_for_param(spec_multi) == 4


def test_validate_grid_for_param_single_ok():
    """Single mode validation passes."""
    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=100)
    validate_grid_for_param(spec, "int", min=0, max=200)
    # No exception


def test_validate_grid_for_param_single_out_of_range():
    """Single mode value out of range raises."""
    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value=300)
    with pytest.raises(ValueError, match="out of range"):
        validate_grid_for_param(spec, "int", min=0, max=200)


def test_validate_grid_for_param_range_invalid_step():
    """Range mode with zero step raises."""
    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=0, range_end=10, range_step=0)
    with pytest.raises(ValueError, match="step must be positive"):
        validate_grid_for_param(spec, "int", min=0, max=100)


def test_validate_grid_for_param_range_start_gt_end():
    """Range start > end raises."""
    spec = ParamGridSpec(mode=GridMode.RANGE, range_start=10, range_end=0, range_step=1)
    with pytest.raises(ValueError, match="start <= end"):
        validate_grid_for_param(spec, "int", min=0, max=100)


def test_validate_grid_for_param_multi_empty():
    """Multi mode with empty list raises."""
    spec = ParamGridSpec(mode=GridMode.MULTI, multi_values=[])
    with pytest.raises(ValueError, match="at least one value"):
        validate_grid_for_param(spec, "int", min=0, max=100)


def test_validate_grid_for_param_multi_duplicates():
    """Multi mode with duplicates raises."""
    spec = ParamGridSpec(mode=GridMode.MULTI, multi_values=[1, 2, 2, 3])
    with pytest.raises(ValueError, match="duplicate values"):
        validate_grid_for_param(spec, "int", min=0, max=100)


def test_validate_grid_for_param_enum():
    """Enum type validation passes if value in choices."""
    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value="buy")
    validate_grid_for_param(spec, "enum", choices=["buy", "sell", "hold"])
    # No exception


def test_validate_grid_for_param_enum_invalid():
    """Enum value not in choices raises."""
    spec = ParamGridSpec(mode=GridMode.SINGLE, single_value="invalid")
    with pytest.raises(ValueError, match="not in choices"):
        validate_grid_for_param(spec, "enum", choices=["buy", "sell"])


if __name__ == "__main__":
    pytest.main([__file__, "-v"])



--------------------------------------------------------------------------------

FILE tests/test_phase141_batch_status_summary.py
sha256(source_bytes) = 7f277528e858664a813cc8276c6a5c38d86cb2bc31598129d242a40a7d1b178e
bytes = 4322
redacted = False
--------------------------------------------------------------------------------

import json
import tempfile
from pathlib import Path
from unittest.mock import patch

import pytest
from fastapi.testclient import TestClient

from FishBroWFS_V2.control.api import app


@pytest.fixture
def client():
    return TestClient(app)


def _write_json(p: Path, obj):
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")


def test_batch_status_reads_execution_json(client):
    with tempfile.TemporaryDirectory() as tmp:
        root = Path(tmp) / "artifacts"
        batch_id = "batch1"

        # execution schema: jobs mapping
        _write_json(
            root / batch_id / "execution.json",
            {
                "batch_state": "RUNNING",
                "jobs": {
                    "jobA": {"state": "SUCCESS"},
                    "jobB": {"state": "FAILED"},
                    "jobC": {"state": "RUNNING"},
                },
            },
        )

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=root):
            r = client.get(f"/batches/{batch_id}/status")
            assert r.status_code == 200
            data = r.json()
            assert data["batch_id"] == batch_id
            assert data["state"] == "RUNNING"
            assert data["jobs_total"] == 3
            assert data["jobs_done"] == 1
            assert data["jobs_failed"] == 1


def test_batch_status_missing_execution_json(client):
    with tempfile.TemporaryDirectory() as tmp:
        root = Path(tmp) / "artifacts"
        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=root):
            r = client.get("/batches/batchX/status")
            assert r.status_code == 404


def test_batch_summary_reads_summary_json(client):
    with tempfile.TemporaryDirectory() as tmp:
        root = Path(tmp) / "artifacts"
        batch_id = "batch1"
        _write_json(
            root / batch_id / "summary.json",
            {"topk": [{"job_id": "jobA", "score": 1.23}], "metrics": {"n": 10}},
        )

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=root):
            r = client.get(f"/batches/{batch_id}/summary")
            assert r.status_code == 200
            data = r.json()
            assert data["batch_id"] == batch_id
            assert isinstance(data["topk"], list)
            assert data["topk"][0]["job_id"] == "jobA"
            assert data["metrics"]["n"] == 10


def test_batch_summary_missing_summary_json(client):
    with tempfile.TemporaryDirectory() as tmp:
        root = Path(tmp) / "artifacts"
        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=root):
            r = client.get("/batches/batchX/summary")
            assert r.status_code == 404


def test_batch_index_endpoint(client):
    with tempfile.TemporaryDirectory() as tmp:
        root = Path(tmp) / "artifacts"
        batch_id = "batch1"
        _write_json(root / batch_id / "index.json", {"batch_id": batch_id, "jobs": ["jobA", "jobB"]})

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=root):
            r = client.get(f"/batches/{batch_id}/index")
            assert r.status_code == 200
            assert r.json()["batch_id"] == batch_id


def test_batch_artifacts_listing(client):
    with tempfile.TemporaryDirectory() as tmp:
        root = Path(tmp) / "artifacts"
        batch_id = "batch1"

        # artifacts tree
        _write_json(
            root / batch_id / "jobA" / "attempt_1" / "manifest.json",
            {"job_id": "jobA", "score": 2.0},
        )
        _write_json(
            root / batch_id / "jobA" / "attempt_2" / "manifest.json",
            {"job_id": "jobA", "metrics": {"score": 3.0}},
        )
        (root / batch_id / "jobB" / "attempt_1").mkdir(parents=True, exist_ok=True)  # no manifest ok

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=root):
            r = client.get(f"/batches/{batch_id}/artifacts")
            assert r.status_code == 200
            data = r.json()
            assert data["batch_id"] == batch_id
            assert [j["job_id"] for j in data["jobs"]] == ["jobA", "jobB"]
            jobA = data["jobs"][0]
            assert [a["attempt"] for a in jobA["attempts"]] == [1, 2]



--------------------------------------------------------------------------------

FILE tests/test_phase14_api_batches.py
sha256(source_bytes) = 92c0db4aa530fdb93a007efcdc6317a854144556f5c7474412871aca88f9c0a3
bytes = 7505
redacted = False
--------------------------------------------------------------------------------

"""Phase 14: API batch endpoints tests."""

import tempfile
from pathlib import Path
from unittest.mock import patch

import pytest
from fastapi.testclient import TestClient

from FishBroWFS_V2.control.api import app


@pytest.fixture
def client():
    """FastAPI test client."""
    return TestClient(app)


@pytest.fixture
def mock_governance_store():
    """Mock governance store.

    NOTE:
    Governance store now uses artifacts root and stores metadata at:
      artifacts/{batch_id}/metadata.json
    """
    with tempfile.TemporaryDirectory() as tmpdir:
        artifacts_root = Path(tmpdir) / "artifacts"
        artifacts_root.mkdir(parents=True, exist_ok=True)

        with patch("FishBroWFS_V2.control.api._get_artifacts_root") as mock_root, \
             patch("FishBroWFS_V2.control.api._get_governance_store") as mock_store:
            from FishBroWFS_V2.control.governance import BatchGovernanceStore
            real_store = BatchGovernanceStore(artifacts_root)
            mock_root.return_value = artifacts_root
            mock_store.return_value = real_store
            yield real_store


def test_get_batch_metadata(client, mock_governance_store):
    """GET /batches/{batch_id}/metadata returns metadata."""
    # Create metadata
    from FishBroWFS_V2.control.governance import BatchMetadata
    meta = BatchMetadata(
        batch_id="batch1",
        season="2026Q1",
        tags=["test"],
        note="hello",
        frozen=False,
        created_at="2025-01-01T00:00:00Z",
        updated_at="2025-01-01T00:00:00Z",
        created_by="system",
    )
    mock_governance_store.set_metadata("batch1", meta)

    response = client.get("/batches/batch1/metadata")
    assert response.status_code == 200
    data = response.json()
    assert data["batch_id"] == "batch1"
    assert data["season"] == "2026Q1"
    assert data["tags"] == ["test"]
    assert data["note"] == "hello"
    assert data["frozen"] is False


def test_get_batch_metadata_not_found(client, mock_governance_store):
    """GET /batches/{batch_id}/metadata returns 404 if not found."""
    response = client.get("/batches/nonexistent/metadata")
    assert response.status_code == 404
    assert "not found" in response.json()["detail"].lower()


def test_update_batch_metadata(client, mock_governance_store):
    """PATCH /batches/{batch_id}/metadata updates metadata."""
    # First create
    from FishBroWFS_V2.control.governance import BatchMetadata
    meta = BatchMetadata(
        batch_id="batch1",
        season="2026Q1",
        tags=[],
        note="",
        frozen=False,
        created_at="2025-01-01T00:00:00Z",
        updated_at="2025-01-01T00:00:00Z",
        created_by="system",
    )
    mock_governance_store.set_metadata("batch1", meta)

    # Update
    update = {"season": "2026Q2", "tags": ["newtag"], "note": "updated"}
    response = client.patch("/batches/batch1/metadata", json=update)
    assert response.status_code == 200
    data = response.json()
    assert data["season"] == "2026Q2"
    assert data["tags"] == ["newtag"]
    assert data["note"] == "updated"
    assert data["frozen"] is False
    assert data["updated_at"] != "2025-01-01T00:00:00Z"  # timestamp updated


def test_update_batch_metadata_frozen_restrictions(client, mock_governance_store):
    """PATCH respects frozen rules."""
    # Create frozen batch
    from FishBroWFS_V2.control.governance import BatchMetadata
    meta = BatchMetadata(
        batch_id="frozenbatch",
        season="2026Q1",
        tags=[],
        note="",
        frozen=True,
        created_at="2025-01-01T00:00:00Z",
        updated_at="2025-01-01T00:00:00Z",
        created_by="system",
    )
    mock_governance_store.set_metadata("frozenbatch", meta)

    # Attempt to change season -> 400
    response = client.patch("/batches/frozenbatch/metadata", json={"season": "2026Q2"})
    assert response.status_code == 400
    assert "Cannot change season" in response.json()["detail"]

    # Attempt to unfreeze -> 400
    response = client.patch("/batches/frozenbatch/metadata", json={"frozen": False})
    assert response.status_code == 400
    assert "Cannot unfreeze" in response.json()["detail"]

    # Append tags should work
    response = client.patch("/batches/frozenbatch/metadata", json={"tags": ["newtag"]})
    assert response.status_code == 200
    data = response.json()
    assert "newtag" in data["tags"]

    # Update note should work
    response = client.patch("/batches/frozenbatch/metadata", json={"note": "updated"})
    assert response.status_code == 200
    assert response.json()["note"] == "updated"


def test_freeze_batch(client, mock_governance_store):
    """POST /batches/{batch_id}/freeze freezes batch."""
    # Create unfrozen batch
    from FishBroWFS_V2.control.governance import BatchMetadata
    meta = BatchMetadata(
        batch_id="batch1",
        season="2026Q1",
        tags=[],
        note="",
        frozen=False,
        created_at="2025-01-01T00:00:00Z",
        updated_at="2025-01-01T00:00:00Z",
        created_by="system",
    )
    mock_governance_store.set_metadata("batch1", meta)

    response = client.post("/batches/batch1/freeze")
    assert response.status_code == 200
    data = response.json()
    assert data["status"] == "frozen"
    assert data["batch_id"] == "batch1"

    # Verify frozen
    assert mock_governance_store.is_frozen("batch1") is True


def test_freeze_batch_not_found(client, mock_governance_store):
    """POST /batches/{batch_id}/freeze returns 404 if batch not found."""
    response = client.post("/batches/nonexistent/freeze")
    assert response.status_code == 404


def test_retry_batch_frozen(client, mock_governance_store):
    """POST /batches/{batch_id}/retry rejects frozen batch."""
    # Create frozen batch
    from FishBroWFS_V2.control.governance import BatchMetadata
    meta = BatchMetadata(
        batch_id="frozenbatch",
        season="2026Q1",
        tags=[],
        note="",
        frozen=True,
        created_at="2025-01-01T00:00:00Z",
        updated_at="2025-01-01T00:00:00Z",
        created_by="system",
    )
    mock_governance_store.set_metadata("frozenbatch", meta)

    response = client.post("/batches/frozenbatch/retry", json={"force": False})
    assert response.status_code == 403
    assert "frozen" in response.json()["detail"].lower()


def test_batch_status_not_implemented(client):
    """GET /batches/{batch_id}/status returns 404 when execution.json missing."""
    # Mock artifacts root to return a path that doesn't have execution.json
    with tempfile.TemporaryDirectory() as tmp:
        root = Path(tmp) / "artifacts"
        root.mkdir(parents=True, exist_ok=True)
        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=root):
            response = client.get("/batches/batch1/status")
            assert response.status_code == 404
            assert "execution.json not found" in response.json()["detail"]


def test_batch_summary_not_implemented(client):
    """GET /batches/{batch_id}/summary returns 404 when summary.json missing."""
    with tempfile.TemporaryDirectory() as tmp:
        root = Path(tmp) / "artifacts"
        root.mkdir(parents=True, exist_ok=True)
        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=root):
            response = client.get("/batches/batch1/summary")
            assert response.status_code == 404
            assert "summary.json not found" in response.json()["detail"]



--------------------------------------------------------------------------------

FILE tests/test_phase14_artifacts.py
sha256(source_bytes) = 9d31a0205b653899a55ecc2f93987f848ca8732e218763ee1dbb76d8be382e55
bytes = 2583
redacted = False
--------------------------------------------------------------------------------

"""Phase 14: Artifacts module tests."""

import json
import tempfile
from pathlib import Path

from FishBroWFS_V2.control.artifacts import (
    canonical_json_bytes,
    compute_sha256,
    write_atomic_json,
    build_job_manifest,
)


def test_canonical_json_bytes_deterministic():
    """Canonical JSON must be deterministic regardless of dict order."""
    obj1 = {"a": 1, "b": 2, "c": [3, 4]}
    obj2 = {"c": [3, 4], "b": 2, "a": 1}
    
    bytes1 = canonical_json_bytes(obj1)
    bytes2 = canonical_json_bytes(obj2)
    
    assert bytes1 == bytes2
    # Ensure no extra whitespace
    decoded = json.loads(bytes1.decode("utf-8"))
    assert decoded == obj1


def test_canonical_json_bytes_unicode():
    """Canonical JSON handles Unicode characters."""
    obj = {"name": "", "value": ""}
    bytes_out = canonical_json_bytes(obj)
    decoded = json.loads(bytes_out.decode("utf-8"))
    assert decoded == obj


def test_compute_sha256():
    """SHA256 hash matches known value."""
    data = b"hello world"
    hash_hex = compute_sha256(data)
    # Expected SHA256 of "hello world"
    expected = "b94d27b9934d3e08a52e52d7da7dabfac484efe37a5380ee9088f7ace2efcde9"
    assert hash_hex == expected


def test_write_atomic_json():
    """Atomic write creates file with correct content."""
    with tempfile.TemporaryDirectory() as tmpdir:
        path = Path(tmpdir) / "test.json"
        obj = {"x": 42, "y": "text"}
        
        write_atomic_json(path, obj)
        
        assert path.exists()
        content = json.loads(path.read_text(encoding="utf-8"))
        assert content == obj


def test_build_job_manifest():
    """Job manifest includes required fields."""
    job_spec = {
        "season": "2026Q1",
        "dataset_id": "CME_MNQ_v2",
        "outputs_root": "/tmp/outputs",
        "config_snapshot": {"param": 1.0},
        "config_hash": "abc123",
        "created_by": "test",
    }
    job_id = "job-123"
    
    manifest = build_job_manifest(job_spec, job_id)
    
    assert manifest["job_id"] == job_id
    assert manifest["season"] == job_spec["season"]
    assert manifest["dataset_id"] == job_spec["dataset_id"]
    assert manifest["config_hash"] == job_spec["config_hash"]
    assert "created_at" in manifest
    assert "manifest_hash" in manifest
    
    # Verify manifest_hash is SHA256 of canonical JSON
    import copy
    manifest_copy = copy.deepcopy(manifest)
    expected_hash = manifest_copy.pop("manifest_hash")
    computed = compute_sha256(canonical_json_bytes(manifest_copy))
    assert expected_hash == computed



--------------------------------------------------------------------------------

FILE tests/test_phase14_batch_aggregate.py
sha256(source_bytes) = 75bf7c3e327a0a2aa31013e5d4ca3d595ba171a63ff822574e9b23770df263dd
bytes = 3530
redacted = False
--------------------------------------------------------------------------------

"""Phase 14: Batch aggregation tests."""

import tempfile
from pathlib import Path

from FishBroWFS_V2.control.batch_aggregate import compute_batch_summary
from FishBroWFS_V2.control.artifacts import canonical_json_bytes, compute_sha256


def test_compute_batch_summary_topk():
    """Batch summary selects top K jobs by score."""
    job_entries = [
        {"job_id": "job1", "score": 0.1},
        {"job_id": "job2", "score": 0.9},
        {"job_id": "job3", "score": 0.5},
        {"job_id": "job4", "score": 0.7},
        {"job_id": "job5", "score": 0.3},
    ]
    
    summary = compute_batch_summary(job_entries, top_k=3)
    
    assert summary["total_jobs"] == 5
    assert len(summary["top_k"]) == 3
    # Should be sorted descending by score
    assert [e["job_id"] for e in summary["top_k"]] == ["job2", "job4", "job3"]
    assert [e["score"] for e in summary["top_k"]] == [0.9, 0.7, 0.5]
    
    # Stats should contain counts
    stats = summary["stats"]
    assert stats["count"] == 5
    assert "mean_score" in stats
    assert "median_score" in stats
    assert "std_score" in stats
    
    # summary_hash should be SHA256 of canonical JSON of summary without hash
    import copy
    summary_copy = copy.deepcopy(summary)
    expected_hash = summary_copy.pop("summary_hash")
    computed = compute_sha256(canonical_json_bytes(summary_copy))
    assert expected_hash == computed


def test_compute_batch_summary_no_score():
    """Batch summary uses job_id ordering when score missing."""
    job_entries = [
        {"job_id": "jobC", "config": {"x": 1}},
        {"job_id": "jobA", "config": {"x": 2}},
        {"job_id": "jobB", "config": {"x": 3}},
    ]
    
    summary = compute_batch_summary(job_entries, top_k=2)
    
    # Top K by job_id alphabetical
    assert [e["job_id"] for e in summary["top_k"]] == ["jobA", "jobB"]
    
    # Stats should not contain score statistics
    stats = summary["stats"]
    assert stats["count"] == 3
    assert "mean_score" not in stats
    assert "median_score" not in stats
    assert "std_score" not in stats


def test_compute_batch_summary_empty():
    """Batch summary handles empty job list."""
    summary = compute_batch_summary([], top_k=5)
    
    assert summary["total_jobs"] == 0
    assert summary["top_k"] == []
    stats = summary["stats"]
    assert stats["count"] == 0
    assert "mean_score" not in stats


def test_compute_batch_summary_k_larger_than_total():
    """Top K larger than total jobs returns all jobs."""
    job_entries = [
        {"job_id": "job1", "score": 0.5},
        {"job_id": "job2", "score": 0.8},
    ]
    
    summary = compute_batch_summary(job_entries, top_k=10)
    
    assert len(summary["top_k"]) == 2
    assert [e["job_id"] for e in summary["top_k"]] == ["job2", "job1"]


def test_compute_batch_summary_deterministic():
    """Summary is deterministic regardless of input order."""
    job_entries1 = [
        {"job_id": "job1", "score": 0.5},
        {"job_id": "job2", "score": 0.8},
    ]
    job_entries2 = [
        {"job_id": "job2", "score": 0.8},
        {"job_id": "job1", "score": 0.5},
    ]
    
    summary1 = compute_batch_summary(job_entries1, top_k=5)
    summary2 = compute_batch_summary(job_entries2, top_k=5)
    
    # Top K order should be same (descending score)
    assert summary1["top_k"] == summary2["top_k"]
    # Stats should be identical
    assert summary1["stats"] == summary2["stats"]
    # Hash should match
    assert summary1["summary_hash"] == summary2["summary_hash"]



--------------------------------------------------------------------------------

FILE tests/test_phase14_batch_execute.py
sha256(source_bytes) = b45e98bc137f1f673fb753bdb489fa683c94b5433edc9ddc8787cd9e3b89b390
bytes = 4555
redacted = False
--------------------------------------------------------------------------------

"""Phase 14: Batch execution tests."""

import tempfile
from pathlib import Path
from unittest.mock import Mock, patch

from FishBroWFS_V2.control.batch_execute import (
    BatchExecutor,
    BatchExecutionState,
    JobExecutionState,
    run_batch,
    retry_failed,
)


def test_batch_execution_state_enum():
    """Batch execution state enum values."""
    assert BatchExecutionState.PENDING.value == "PENDING"
    assert BatchExecutionState.RUNNING.value == "RUNNING"
    assert BatchExecutionState.DONE.value == "DONE"
    assert BatchExecutionState.FAILED.value == "FAILED"
    assert BatchExecutionState.PARTIAL_FAILED.value == "PARTIAL_FAILED"


def test_job_execution_state_enum():
    """Job execution state enum values."""
    assert JobExecutionState.PENDING.value == "PENDING"
    assert JobExecutionState.RUNNING.value == "RUNNING"
    assert JobExecutionState.SUCCESS.value == "SUCCESS"
    assert JobExecutionState.FAILED.value == "FAILED"
    assert JobExecutionState.SKIPPED.value == "SKIPPED"


def test_batch_executor_initial_state():
    """BatchExecutor initializes with correct state."""
    batch_id = "batch-123"
    job_ids = ["job1", "job2", "job3"]
    
    executor = BatchExecutor(batch_id, job_ids)
    
    assert executor.batch_id == batch_id
    assert executor.job_ids == job_ids
    assert executor.state == BatchExecutionState.PENDING
    assert executor.job_states == {
        "job1": JobExecutionState.PENDING,
        "job2": JobExecutionState.PENDING,
        "job3": JobExecutionState.PENDING,
    }
    assert executor.created_at is not None
    assert executor.updated_at is not None


def test_batch_executor_transition():
    """BatchExecutor transitions state based on job states."""
    executor = BatchExecutor("batch", ["job1", "job2"])
    
    # Initially PENDING
    assert executor.state == BatchExecutionState.PENDING
    
    # Start first job -> RUNNING
    executor._set_job_state("job1", JobExecutionState.RUNNING)
    assert executor.state == BatchExecutionState.RUNNING
    
    # Finish first job successfully, second still pending -> RUNNING
    executor._set_job_state("job1", JobExecutionState.SUCCESS)
    assert executor.state == BatchExecutionState.RUNNING
    
    # Start second job -> RUNNING
    executor._set_job_state("job2", JobExecutionState.RUNNING)
    assert executor.state == BatchExecutionState.RUNNING
    
    # Finish second job successfully -> DONE
    executor._set_job_state("job2", JobExecutionState.SUCCESS)
    assert executor.state == BatchExecutionState.DONE
    
    # If one job fails -> PARTIAL_FAILED
    executor._set_job_state("job1", JobExecutionState.FAILED)
    executor._set_job_state("job2", JobExecutionState.SUCCESS)
    executor._recompute_state()
    assert executor.state == BatchExecutionState.PARTIAL_FAILED
    
    # If all jobs fail -> FAILED
    executor._set_job_state("job2", JobExecutionState.FAILED)
    executor._recompute_state()
    assert executor.state == BatchExecutionState.FAILED


def test_batch_executor_skipped_jobs():
    """SKIPPED jobs count as completed for state computation."""
    executor = BatchExecutor("batch", ["job1", "job2"])
    
    executor._set_job_state("job1", JobExecutionState.SUCCESS)
    executor._set_job_state("job2", JobExecutionState.SKIPPED)
    
    # Both jobs are completed (SUCCESS + SKIPPED) -> DONE
    assert executor.state == BatchExecutionState.DONE


@patch("FishBroWFS_V2.control.batch_execute.BatchExecutor")
def test_run_batch_mock(mock_executor_cls):
    """run_batch creates executor and runs jobs."""
    mock_executor = Mock()
    mock_executor_cls.return_value = mock_executor
    
    batch_id = "batch-test"
    job_ids = ["job1", "job2"]
    artifacts_root = Path("/tmp/artifacts")
    
    result = run_batch(batch_id, job_ids, artifacts_root)
    
    mock_executor_cls.assert_called_once_with(batch_id, job_ids)
    mock_executor.run.assert_called_once_with(artifacts_root)
    assert result == mock_executor


@patch("FishBroWFS_V2.control.batch_execute.BatchExecutor")
def test_retry_failed_mock(mock_executor_cls):
    """retry_failed creates executor and retries failed jobs."""
    mock_executor = Mock()
    mock_executor_cls.return_value = mock_executor
    
    batch_id = "batch-retry"
    artifacts_root = Path("/tmp/artifacts")
    
    result = retry_failed(batch_id, artifacts_root)
    
    mock_executor_cls.assert_called_once_with(batch_id, [])
    mock_executor.retry_failed.assert_called_once_with(artifacts_root)
    assert result == mock_executor



--------------------------------------------------------------------------------

FILE tests/test_phase14_batch_index.py
sha256(source_bytes) = 40da311790fe9b26250c80682c676eac35bde90ebc1203d373f0f2e75ba21637
bytes = 3659
redacted = False
--------------------------------------------------------------------------------

"""Phase 14: Batch index tests."""

import json
import tempfile
from pathlib import Path

from FishBroWFS_V2.control.batch_index import build_batch_index
from FishBroWFS_V2.control.artifacts import canonical_json_bytes, compute_sha256


def test_build_batch_index_deterministic():
    """Batch index is deterministic regardless of job entry order."""
    job_entries = [
        {"job_id": "job1", "score": 0.5, "manifest_hash": "abc123", "manifest_path": "batch-123/job1/manifest.json"},
        {"job_id": "job2", "score": 0.3, "manifest_hash": "def456", "manifest_path": "batch-123/job2/manifest.json"},
        {"job_id": "job3", "score": 0.8, "manifest_hash": "ghi789", "manifest_path": "batch-123/job3/manifest.json"},
    ]
    job_entries_shuffled = [
        {"job_id": "job3", "score": 0.8, "manifest_hash": "ghi789", "manifest_path": "batch-123/job3/manifest.json"},
        {"job_id": "job1", "score": 0.5, "manifest_hash": "abc123", "manifest_path": "batch-123/job1/manifest.json"},
        {"job_id": "job2", "score": 0.3, "manifest_hash": "def456", "manifest_path": "batch-123/job2/manifest.json"},
    ]
    
    with tempfile.TemporaryDirectory() as tmpdir:
        artifacts_root = Path(tmpdir)
        batch_id = "batch-123"
        
        index1 = build_batch_index(artifacts_root, batch_id, job_entries)
        index2 = build_batch_index(artifacts_root, batch_id, job_entries_shuffled)
        
        # Index should be identical (entries sorted by job_id)
        assert index1 == index2
        
        # Verify structure
        assert index1["batch_id"] == batch_id
        assert index1["job_count"] == 3
        assert len(index1["jobs"]) == 3
        # Entries should be sorted by job_id
        assert [e["job_id"] for e in index1["jobs"]] == ["job1", "job2", "job3"]
        
        # Verify index_hash is SHA256 of canonical JSON of index without hash
        import copy
        index_copy = copy.deepcopy(index1)
        expected_hash = index_copy.pop("index_hash")
        computed = compute_sha256(canonical_json_bytes(index_copy))
        assert expected_hash == computed


def test_build_batch_index_without_score():
    """Batch index works when jobs have no score field."""
    job_entries = [
        {"job_id": "jobA", "config": {"x": 1}, "manifest_hash": "hashA", "manifest_path": "batch-no-score/jobA/manifest.json"},
        {"job_id": "jobB", "config": {"x": 2}, "manifest_hash": "hashB", "manifest_path": "batch-no-score/jobB/manifest.json"},
    ]
    
    with tempfile.TemporaryDirectory() as tmpdir:
        artifacts_root = Path(tmpdir)
        batch_id = "batch-no-score"
        
        index = build_batch_index(artifacts_root, batch_id, job_entries)
        
        assert index["batch_id"] == batch_id
        assert index["job_count"] == 2
        # Entries sorted by job_id
        assert [e["job_id"] for e in index["jobs"]] == ["jobA", "jobB"]


def test_build_batch_index_writes_file():
    """Batch index writes index.json to artifacts directory."""
    with tempfile.TemporaryDirectory() as tmpdir:
        artifacts_root = Path(tmpdir)
        batch_id = "batch-write"
        job_entries = [{"job_id": "job1", "manifest_hash": "hash1", "manifest_path": "batch-write/job1/manifest.json"}]
        
        index = build_batch_index(artifacts_root, batch_id, job_entries)
        
        # Check file exists
        batch_dir = artifacts_root / batch_id
        index_file = batch_dir / "index.json"
        assert index_file.exists()
        
        # Content matches returned index
        loaded = json.loads(index_file.read_text(encoding="utf-8"))
        assert loaded == index



--------------------------------------------------------------------------------

FILE tests/test_phase14_governance.py
sha256(source_bytes) = 24633a39437529c24a7a618e11821a6dcbf3a4edb21c28896d35f0e6f8242bda
bytes = 5730
redacted = False
--------------------------------------------------------------------------------

"""Phase 14: Governance tests."""

import tempfile
from pathlib import Path

from FishBroWFS_V2.control.governance import (
    BatchGovernanceStore,
    BatchMetadata,
)


def test_batch_metadata_creation():
    """BatchMetadata can be created with defaults."""
    meta = BatchMetadata(batch_id="batch1", season="2026Q1", tags=["test"], note="hello")
    assert meta.batch_id == "batch1"
    assert meta.season == "2026Q1"
    assert meta.tags == ["test"]
    assert meta.note == "hello"
    assert meta.frozen is False
    assert meta.created_at == ""
    assert meta.updated_at == ""


def test_batch_governance_store_init():
    """Store creates directory if not exists."""
    with tempfile.TemporaryDirectory() as tmpdir:
        store_root = Path(tmpdir) / "artifacts"
        store = BatchGovernanceStore(store_root)
        assert store.artifacts_root.exists()
        assert store.artifacts_root.is_dir()


def test_batch_governance_store_set_get():
    """Store can set and retrieve metadata."""
    with tempfile.TemporaryDirectory() as tmpdir:
        store_root = Path(tmpdir) / "artifacts"
        store = BatchGovernanceStore(store_root)

        meta = BatchMetadata(
            batch_id="batch1",
            season="2026Q1",
            tags=["tag1", "tag2"],
            note="test note",
            frozen=False,
            created_at="2025-01-01T00:00:00Z",
            updated_at="2025-01-01T00:00:00Z",
            created_by="user",
        )

        store.set_metadata("batch1", meta)

        retrieved = store.get_metadata("batch1")
        assert retrieved is not None
        assert retrieved.batch_id == meta.batch_id
        assert retrieved.season == meta.season
        assert retrieved.tags == meta.tags
        assert retrieved.note == meta.note
        assert retrieved.frozen == meta.frozen
        assert retrieved.created_at == meta.created_at
        assert retrieved.updated_at == meta.updated_at
        assert retrieved.created_by == meta.created_by


def test_batch_governance_store_update_metadata_new():
    """Update metadata creates new metadata if not exists."""
    with tempfile.TemporaryDirectory() as tmpdir:
        store_root = Path(tmpdir) / "artifacts"
        store = BatchGovernanceStore(store_root)

        meta = store.update_metadata(
            "newbatch",
            season="2026Q2",
            tags=["new"],
            note="created",
        )

        assert meta.batch_id == "newbatch"
        assert meta.season == "2026Q2"
        assert meta.tags == ["new"]
        assert meta.note == "created"
        assert meta.frozen is False
        assert meta.created_at != ""
        assert meta.updated_at != ""


def test_batch_governance_store_update_metadata_frozen_rules():
    """Frozen batch restricts updates."""
    with tempfile.TemporaryDirectory() as tmpdir:
        store_root = Path(tmpdir) / "artifacts"
        store = BatchGovernanceStore(store_root)

        # Create a frozen batch
        meta = store.update_metadata("frozenbatch", season="2026Q1", frozen=True)
        assert meta.frozen is True

        import pytest
        # Attempt to change season -> should raise
        with pytest.raises(ValueError, match="Cannot change season of frozen batch"):
            store.update_metadata("frozenbatch", season="2026Q2")

        # Attempt to unfreeze -> should raise
        with pytest.raises(ValueError, match="Cannot unfreeze a frozen batch"):
            store.update_metadata("frozenbatch", frozen=False)

        # Append tags should work
        meta2 = store.update_metadata("frozenbatch", tags=["newtag"])
        assert "newtag" in meta2.tags
        assert meta2.season == "2026Q1"  # unchanged

        # Update note should work
        meta3 = store.update_metadata("frozenbatch", note="updated note")
        assert meta3.note == "updated note"

        # Setting frozen=True again is no-op
        meta4 = store.update_metadata("frozenbatch", frozen=True)
        assert meta4.frozen is True


def test_batch_governance_store_freeze():
    """Freeze method sets frozen flag."""
    with tempfile.TemporaryDirectory() as tmpdir:
        store_root = Path(tmpdir) / "artifacts"
        store = BatchGovernanceStore(store_root)

        store.update_metadata("batch1", season="2026Q1")
        assert store.is_frozen("batch1") is False

        store.freeze("batch1")
        assert store.is_frozen("batch1") is True

        # Freeze again is idempotent
        store.freeze("batch1")
        assert store.is_frozen("batch1") is True


def test_batch_governance_store_list_batches():
    """List batches with filters."""
    with tempfile.TemporaryDirectory() as tmpdir:
        store_root = Path(tmpdir) / "artifacts"
        store = BatchGovernanceStore(store_root)

        store.update_metadata("batch1", season="2026Q1", tags=["a", "b"])
        store.update_metadata("batch2", season="2026Q1", tags=["b", "c"], frozen=True)
        store.update_metadata("batch3", season="2026Q2", tags=["a"])

        # All batches
        all_batches = store.list_batches()
        assert len(all_batches) == 3
        ids = [m.batch_id for m in all_batches]
        assert sorted(ids) == ["batch1", "batch2", "batch3"]

        # Filter by season
        season_batches = store.list_batches(season="2026Q1")
        assert len(season_batches) == 2
        assert {m.batch_id for m in season_batches} == {"batch1", "batch2"}

        # Filter by tag
        tag_batches = store.list_batches(tag="a")
        assert {m.batch_id for m in tag_batches} == {"batch1", "batch3"}

        # Filter by frozen
        frozen_batches = store.list_batches(frozen=True)
        assert {m.batch_id for m in frozen_batches} == {"batch2"}



--------------------------------------------------------------------------------

FILE tests/test_phase150_season_index.py
sha256(source_bytes) = 49899ac93b57523344a629176850550abade8aa9d5fa5c1b16a3f3ba7748e8f8
bytes = 4916
redacted = False
--------------------------------------------------------------------------------

import json
import tempfile
from pathlib import Path
from unittest.mock import patch

import pytest
from fastapi.testclient import TestClient

from FishBroWFS_V2.control.api import app


@pytest.fixture
def client():
    return TestClient(app)


def _wjson(p: Path, obj):
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")


def test_rebuild_season_index_collects_batches_and_is_deterministic(client):
    with tempfile.TemporaryDirectory() as tmp:
        artifacts_root = Path(tmp) / "artifacts"
        season_root = Path(tmp) / "season_index"
        season = "2026Q1"

        # batch2 (lexicographically after batch1)  write first to verify sorting
        _wjson(
            artifacts_root / "batch2" / "metadata.json",
            {"batch_id": "batch2", "season": season, "tags": ["b", "a"], "note": "n2", "frozen": False},
        )
        _wjson(artifacts_root / "batch2" / "index.json", {"x": 1})
        _wjson(artifacts_root / "batch2" / "summary.json", {"topk": [], "metrics": {}})

        # batch1
        _wjson(
            artifacts_root / "batch1" / "metadata.json",
            {"batch_id": "batch1", "season": season, "tags": ["z"], "note": "n1", "frozen": True},
        )
        _wjson(artifacts_root / "batch1" / "index.json", {"y": 2})
        _wjson(artifacts_root / "batch1" / "summary.json", {"topk": [{"job_id": "j", "score": 1.0}], "metrics": {"n": 1}})

        # different season should be ignored
        _wjson(
            artifacts_root / "batchX" / "metadata.json",
            {"batch_id": "batchX", "season": "2026Q2", "tags": ["ignore"], "note": "", "frozen": False},
        )

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=artifacts_root), \
             patch("FishBroWFS_V2.control.api._get_season_index_root", return_value=season_root):
            r = client.post(f"/seasons/{season}/rebuild_index")
            assert r.status_code == 200
            data = r.json()
            assert data["season"] == season
            assert len(data["batches"]) == 2

            # deterministic order by batch_id
            assert [b["batch_id"] for b in data["batches"]] == ["batch1", "batch2"]

            # tags dedupe+sort in index entries
            b2 = data["batches"][1]
            assert b2["tags"] == ["a", "b"]

            # index file exists
            idx_path = season_root / season / "season_index.json"
            assert idx_path.exists()


def test_season_metadata_lifecycle_and_freeze_rules(client):
    with tempfile.TemporaryDirectory() as tmp:
        season_root = Path(tmp) / "season_index"
        season = "2026Q1"

        with patch("FishBroWFS_V2.control.api._get_season_index_root", return_value=season_root):
            # metadata not exist -> 404
            r = client.get(f"/seasons/{season}/metadata")
            assert r.status_code == 404

            # create/update metadata
            r = client.patch(f"/seasons/{season}/metadata", json={"tags": ["core", "core"], "note": "hello"})
            assert r.status_code == 200
            meta = r.json()
            assert meta["season"] == season
            assert meta["tags"] == ["core"]
            assert meta["note"] == "hello"
            assert meta["frozen"] is False

            # freeze
            r = client.post(f"/seasons/{season}/freeze")
            assert r.status_code == 200
            assert r.json()["status"] == "frozen"

            # cannot unfreeze
            r = client.patch(f"/seasons/{season}/metadata", json={"frozen": False})
            assert r.status_code == 400

            # tags/note still allowed
            r = client.patch(f"/seasons/{season}/metadata", json={"tags": ["z"], "note": "n2"})
            assert r.status_code == 200
            meta2 = r.json()
            assert meta2["tags"] == ["core", "z"]
            assert meta2["note"] == "n2"
            assert meta2["frozen"] is True


def test_rebuild_index_forbidden_when_season_frozen(client):
    with tempfile.TemporaryDirectory() as tmp:
        artifacts_root = Path(tmp) / "artifacts"
        season_root = Path(tmp) / "season_index"
        season = "2026Q1"

        # prepare one batch
        _wjson(
            artifacts_root / "batch1" / "metadata.json",
            {"batch_id": "batch1", "season": season, "tags": [], "note": "", "frozen": False},
        )

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=artifacts_root), \
             patch("FishBroWFS_V2.control.api._get_season_index_root", return_value=season_root):

            # freeze season first
            r = client.post(f"/seasons/{season}/freeze")
            assert r.status_code == 200

            # rebuild should be forbidden
            r = client.post(f"/seasons/{season}/rebuild_index")
            assert r.status_code == 403



--------------------------------------------------------------------------------

FILE tests/test_phase151_season_compare_topk.py
sha256(source_bytes) = cd3003ce772204131eebb303dd8792031b856340514a71f5813e742a1a2f4ed2
bytes = 4762
redacted = False
--------------------------------------------------------------------------------

import json
import tempfile
from pathlib import Path
from unittest.mock import patch

import pytest
from fastapi.testclient import TestClient

from FishBroWFS_V2.control.api import app


@pytest.fixture
def client():
    return TestClient(app)


def _wjson(p: Path, obj):
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")


def test_season_compare_topk_merge_and_tiebreak(client):
    with tempfile.TemporaryDirectory() as tmp:
        artifacts_root = Path(tmp) / "artifacts"
        season_root = Path(tmp) / "season_index"
        season = "2026Q1"

        # season index lists two batches
        _wjson(
            season_root / season / "season_index.json",
            {
                "season": season,
                "generated_at": "2025-12-21T00:00:00Z",
                "batches": [{"batch_id": "batchA"}, {"batch_id": "batchB"}],
            },
        )

        # batchA summary
        _wjson(
            artifacts_root / "batchA" / "summary.json",
            {
                "topk": [
                    {"job_id": "j2", "score": 2.0},
                    {"job_id": "j1", "score": 2.0},  # tie on score, job_id decides inside same batch later
                    {"job_id": "j0", "score": 1.0},
                ],
                "metrics": {"n": 3},
            },
        )

        # batchB summary (tie score with batchA to test tie-break by batch_id then job_id)
        _wjson(
            artifacts_root / "batchB" / "summary.json",
            {
                "topk": [
                    {"job_id": "j9", "score": 2.0},
                    {"job_id": "j8", "score": None},  # None goes last
                ],
                "metrics": {},
            },
        )

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=artifacts_root), \
             patch("FishBroWFS_V2.control.api._get_season_index_root", return_value=season_root):
            r = client.get(f"/seasons/{season}/compare/topk?k=10")
            assert r.status_code == 200
            data = r.json()
            assert data["season"] == season
            items = data["items"]

            # score desc, tie-break batch_id asc, tie-break job_id asc
            # score=2.0 items are: batchA j1/j2, batchB j9
            # batchA < batchB => all batchA first; within batchA j1 < j2
            assert [(x["batch_id"], x["job_id"], x["score"]) for x in items[:3]] == [
                ("batchA", "j1", 2.0),
                ("batchA", "j2", 2.0),
                ("batchB", "j9", 2.0),
            ]

            # None score should be at the end
            assert items[-1]["score"] is None


def test_season_compare_skips_missing_or_corrupt_summaries(client):
    with tempfile.TemporaryDirectory() as tmp:
        artifacts_root = Path(tmp) / "artifacts"
        season_root = Path(tmp) / "season_index"
        season = "2026Q1"

        _wjson(
            season_root / season / "season_index.json",
            {
                "season": season,
                "generated_at": "2025-12-21T00:00:00Z",
                "batches": [{"batch_id": "batchOK"}, {"batch_id": "batchMissing"}, {"batch_id": "batchBad"}],
            },
        )

        _wjson(
            artifacts_root / "batchOK" / "summary.json",
            {"topk": [{"job_id": "j1", "score": 1.0}], "metrics": {}},
        )

        # batchMissing -> no summary.json

        # batchBad -> corrupt json
        bad_path = artifacts_root / "batchBad" / "summary.json"
        bad_path.parent.mkdir(parents=True, exist_ok=True)
        bad_path.write_text("{not-json", encoding="utf-8")

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=artifacts_root), \
             patch("FishBroWFS_V2.control.api._get_season_index_root", return_value=season_root):
            r = client.get(f"/seasons/{season}/compare/topk?k=20")
            assert r.status_code == 200
            data = r.json()
            assert [(x["batch_id"], x["job_id"]) for x in data["items"]] == [("batchOK", "j1")]

            skipped = set(data["skipped_batches"])
            assert "batchMissing" in skipped
            assert "batchBad" in skipped


def test_season_compare_404_when_season_index_missing(client):
    with tempfile.TemporaryDirectory() as tmp:
        artifacts_root = Path(tmp) / "artifacts"
        season_root = Path(tmp) / "season_index"

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=artifacts_root), \
             patch("FishBroWFS_V2.control.api._get_season_index_root", return_value=season_root):
            r = client.get("/seasons/NOPE/compare/topk?k=20")
            assert r.status_code == 404



--------------------------------------------------------------------------------

FILE tests/test_phase152_season_compare_batches.py
sha256(source_bytes) = 45b17d060d32ce3d67f151e2fbfa781c1b1da6f0f37d80f433c35e849d58ea53
bytes = 5735
redacted = False
--------------------------------------------------------------------------------

import json
import tempfile
from pathlib import Path
from unittest.mock import patch

import pytest
from fastapi.testclient import TestClient

from FishBroWFS_V2.control.api import app


@pytest.fixture
def client():
    return TestClient(app)


def _wjson(p: Path, obj):
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")


def test_compare_batches_cards_and_robust_summary(client):
    with tempfile.TemporaryDirectory() as tmp:
        artifacts_root = Path(tmp) / "artifacts"
        season_root = Path(tmp) / "season_index"
        season = "2026Q1"

        # season index includes 3 batches; ensure order is batchA, batchB, batchC
        _wjson(
            season_root / season / "season_index.json",
            {
                "season": season,
                "generated_at": "2025-12-21T00:00:00Z",
                "batches": [
                    {"batch_id": "batchB", "frozen": False, "tags": ["b"], "note": "nB", "index_hash": "iB", "summary_hash": "sB"},
                    {"batch_id": "batchA", "frozen": True, "tags": ["a"], "note": "nA", "index_hash": "iA", "summary_hash": "sA"},
                    {"batch_id": "batchC", "frozen": False, "tags": [], "note": "", "index_hash": None, "summary_hash": None},
                ],
            },
        )

        # batchA: ok summary
        _wjson(
            artifacts_root / "batchA" / "summary.json",
            {"topk": [{"job_id": "j1", "score": 1.23}], "metrics": {"n": 1}},
        )

        # batchB: corrupt summary
        p_bad = artifacts_root / "batchB" / "summary.json"
        p_bad.parent.mkdir(parents=True, exist_ok=True)
        p_bad.write_text("{not-json", encoding="utf-8")

        # batchC: missing summary

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=artifacts_root), \
             patch("FishBroWFS_V2.control.api._get_season_index_root", return_value=season_root):
            r = client.get(f"/seasons/{season}/compare/batches")
            assert r.status_code == 200
            data = r.json()
            assert data["season"] == season

            batches = data["batches"]
            assert [b["batch_id"] for b in batches] == ["batchA", "batchB", "batchC"]

            bA = batches[0]
            assert bA["summary_ok"] is True
            assert bA["top_job_id"] == "j1"
            assert bA["top_score"] == 1.23
            assert bA["topk_size"] == 1

            bB = batches[1]
            assert bB["summary_ok"] is False

            bC = batches[2]
            assert bC["summary_ok"] is False
            assert bC["topk_size"] == 0

            skipped = set(data["skipped_summaries"])
            assert "batchB" in skipped
            assert "batchC" in skipped


def test_compare_leaderboard_grouping_and_determinism(client):
    with tempfile.TemporaryDirectory() as tmp:
        artifacts_root = Path(tmp) / "artifacts"
        season_root = Path(tmp) / "season_index"
        season = "2026Q1"

        _wjson(
            season_root / season / "season_index.json",
            {
                "season": season,
                "generated_at": "2025-12-21T00:00:00Z",
                "batches": [{"batch_id": "batchA"}, {"batch_id": "batchB"}],
            },
        )

        # Include strategy_id and dataset_id in rows for grouping
        _wjson(
            artifacts_root / "batchA" / "summary.json",
            {
                "topk": [
                    {"job_id": "a2", "score": 2.0, "strategy_id": "S1"},
                    {"job_id": "a1", "score": 2.0, "strategy_id": "S1"},  # tie within same group
                    {"job_id": "a0", "score": 1.0, "strategy_id": "S2"},
                ]
            },
        )
        _wjson(
            artifacts_root / "batchB" / "summary.json",
            {
                "topk": [
                    {"job_id": "b9", "score": 2.0, "strategy_id": "S1"},
                    {"job_id": "b8", "score": None, "strategy_id": "S1"},
                ]
            },
        )

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=artifacts_root), \
             patch("FishBroWFS_V2.control.api._get_season_index_root", return_value=season_root):
            r = client.get(f"/seasons/{season}/compare/leaderboard?group_by=strategy_id&per_group=3")
            assert r.status_code == 200
            data = r.json()
            assert data["season"] == season
            assert data["group_by"] == "strategy_id"
            assert data["per_group"] == 3

            groups = {g["key"]: g["items"] for g in data["groups"]}
            assert "S1" in groups
            # Deterministic ordering inside group S1 by score desc, tie-break batch_id asc, job_id asc
            # score=2.0: batchA a1/a2, batchB b9 => batchA first; within batchA a1 < a2
            assert [(x["batch_id"], x["job_id"], x["score"]) for x in groups["S1"][:3]] == [
                ("batchA", "a1", 2.0),
                ("batchA", "a2", 2.0),
                ("batchB", "b9", 2.0),
            ]


def test_compare_endpoints_404_when_season_index_missing(client):
    with tempfile.TemporaryDirectory() as tmp:
        artifacts_root = Path(tmp) / "artifacts"
        season_root = Path(tmp) / "season_index"
        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=artifacts_root), \
             patch("FishBroWFS_V2.control.api._get_season_index_root", return_value=season_root):
            r = client.get("/seasons/NOPE/compare/batches")
            assert r.status_code == 404
            r = client.get("/seasons/NOPE/compare/leaderboard")
            assert r.status_code == 404



--------------------------------------------------------------------------------

FILE tests/test_phase153_season_export.py
sha256(source_bytes) = 5feef8d7a38d110ae57af889cbf17bf3b722148233808a2947fcb76fd2d33673
bytes = 4451
redacted = False
--------------------------------------------------------------------------------

import json
import tempfile
from pathlib import Path
from unittest.mock import patch

import pytest
from fastapi.testclient import TestClient

from FishBroWFS_V2.control.api import app
from FishBroWFS_V2.control.artifacts import compute_sha256


@pytest.fixture
def client():
    return TestClient(app)


def _wjson(p: Path, obj):
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")


def test_export_requires_frozen_season(client):
    with tempfile.TemporaryDirectory() as tmp:
        artifacts_root = Path(tmp) / "artifacts"
        season_root = Path(tmp) / "season_index"
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"

        # season index exists
        _wjson(
            season_root / season / "season_index.json",
            {"season": season, "generated_at": "Z", "batches": []},
        )

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=artifacts_root), \
             patch("FishBroWFS_V2.control.api._get_season_index_root", return_value=season_root), \
             patch("FishBroWFS_V2.control.season_export.get_exports_root", return_value=exports_root):
            r = client.post(f"/seasons/{season}/export")
            assert r.status_code == 403


def test_export_builds_package_and_manifest_sha_matches(client):
    with tempfile.TemporaryDirectory() as tmp:
        artifacts_root = Path(tmp) / "artifacts"
        season_root = Path(tmp) / "season_index"
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"

        # create season index with 2 batches
        _wjson(
            season_root / season / "season_index.json",
            {
                "season": season,
                "generated_at": "2025-12-21T00:00:00Z",
                "batches": [{"batch_id": "batchB"}, {"batch_id": "batchA"}],
            },
        )

        # create season metadata and freeze it
        # (use API to freeze for realism)
        with patch("FishBroWFS_V2.control.api._get_season_index_root", return_value=season_root):
            r = client.post(f"/seasons/{season}/freeze")
            assert r.status_code == 200

        # artifacts files
        _wjson(artifacts_root / "batchA" / "metadata.json", {"season": season, "frozen": True, "tags": ["a"], "note": ""})
        _wjson(artifacts_root / "batchA" / "index.json", {"x": 1})
        _wjson(artifacts_root / "batchA" / "summary.json", {"topk": [{"job_id": "j1", "score": 1.0}], "metrics": {}})

        _wjson(artifacts_root / "batchB" / "metadata.json", {"season": season, "frozen": False, "tags": ["b"], "note": "n"})
        _wjson(artifacts_root / "batchB" / "index.json", {"y": 2})
        # omit batchB summary.json to test missing files recorded

        with patch("FishBroWFS_V2.control.api._get_artifacts_root", return_value=artifacts_root), \
             patch("FishBroWFS_V2.control.api._get_season_index_root", return_value=season_root), \
             patch("FishBroWFS_V2.control.season_export.get_exports_root", return_value=exports_root):
            r = client.post(f"/seasons/{season}/export")
            assert r.status_code == 200
            out = r.json()

            export_dir = Path(out["export_dir"])
            manifest_path = Path(out["manifest_path"])
            assert export_dir.exists()
            assert manifest_path.exists()

            # verify manifest sha matches actual bytes
            actual_sha = compute_sha256(manifest_path.read_bytes())
            assert out["manifest_sha256"] == actual_sha

            # verify key files copied
            assert (export_dir / "season_index.json").exists()
            # metadata may exist (freeze created it)
            assert (export_dir / "season_metadata.json").exists()
            assert (export_dir / "batches" / "batchA" / "metadata.json").exists()
            assert (export_dir / "batches" / "batchA" / "index.json").exists()
            assert (export_dir / "batches" / "batchA" / "summary.json").exists()

            # batchB summary missing -> recorded
            assert "batches/batchB/summary.json" in out["missing_files"]

            # manifest contains file hashes
            man = json.loads(manifest_path.read_text(encoding="utf-8"))
            assert man["season"] == season
            assert "files" in man and isinstance(man["files"], list)
            assert "manifest_sha256" in man



--------------------------------------------------------------------------------

FILE tests/test_phase16_export_replay.py
sha256(source_bytes) = 3f20fdd7f995689f823638e3f1def08c996aa1dbf0f1e64d89e10042020749aa
bytes = 15942
redacted = False
--------------------------------------------------------------------------------

"""
Phase 16: Export Pack Replay Mode regression tests.

Tests that exported season packages can be replayed without artifacts.
"""

import json
import tempfile
from pathlib import Path
from unittest.mock import patch

import pytest
from fastapi.testclient import TestClient

from FishBroWFS_V2.control.api import app
from FishBroWFS_V2.control.season_export_replay import (
    load_replay_index,
    replay_season_topk,
    replay_season_batch_cards,
    replay_season_leaderboard,
)


@pytest.fixture
def client():
    return TestClient(app)


def _wjson(p: Path, obj):
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")


def test_load_replay_index():
    """Test loading replay_index.json."""
    with tempfile.TemporaryDirectory() as tmp:
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"
        
        replay_index = {
            "season": season,
            "generated_at": "2025-12-21T00:00:00Z",
            "batches": [
                {
                    "batch_id": "batchA",
                    "summary": {
                        "topk": [{"job_id": "job1", "score": 1.5, "strategy_id": "S1"}],
                        "metrics": {"n": 10},
                    },
                    "index": {"jobs": ["job1"]},
                }
            ],
        }
        
        _wjson(exports_root / "seasons" / season / "replay_index.json", replay_index)
        
        loaded = load_replay_index(exports_root, season)
        assert loaded["season"] == season
        assert len(loaded["batches"]) == 1
        assert loaded["batches"][0]["batch_id"] == "batchA"


def test_load_replay_index_missing():
    """Test FileNotFoundError when replay_index.json missing."""
    with tempfile.TemporaryDirectory() as tmp:
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"
        
        with pytest.raises(FileNotFoundError):
            load_replay_index(exports_root, season)


def test_replay_season_topk():
    """Test replay season topk."""
    with tempfile.TemporaryDirectory() as tmp:
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"
        
        replay_index = {
            "season": season,
            "generated_at": "2025-12-21T00:00:00Z",
            "batches": [
                {
                    "batch_id": "batchA",
                    "summary": {
                        "topk": [
                            {"job_id": "job1", "score": 1.5, "strategy_id": "S1"},
                            {"job_id": "job2", "score": 1.2, "strategy_id": "S2"},
                        ],
                        "metrics": {},
                    },
                },
                {
                    "batch_id": "batchB",
                    "summary": {
                        "topk": [
                            {"job_id": "job3", "score": 1.8, "strategy_id": "S1"},
                        ],
                        "metrics": {},
                    },
                },
                {
                    "batch_id": "batchC",
                    "summary": None,  # missing summary
                },
            ],
        }
        
        _wjson(exports_root / "seasons" / season / "replay_index.json", replay_index)
        
        res = replay_season_topk(exports_root, season, k=5)
        assert res.season == season
        assert res.k == 5
        assert len(res.items) == 3  # all topk items merged
        assert res.skipped_batches == ["batchC"]
        
        # Verify ordering by score descending
        scores = [item["score"] for item in res.items]
        assert scores == [1.8, 1.5, 1.2]
        
        # Verify batch_id added
        assert all("_batch_id" in item for item in res.items)


def test_replay_season_batch_cards():
    """Test replay season batch cards."""
    with tempfile.TemporaryDirectory() as tmp:
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"
        
        replay_index = {
            "season": season,
            "generated_at": "2025-12-21T00:00:00Z",
            "batches": [
                {
                    "batch_id": "batchA",
                    "summary": {
                        "topk": [{"job_id": "job1", "score": 1.5}],
                        "metrics": {"n": 10},
                    },
                    "index": {"jobs": ["job1"]},
                },
                {
                    "batch_id": "batchB",
                    "summary": None,  # missing summary
                    "index": {"jobs": ["job2"]},
                },
            ],
        }
        
        _wjson(exports_root / "seasons" / season / "replay_index.json", replay_index)
        
        res = replay_season_batch_cards(exports_root, season)
        assert res.season == season
        assert len(res.batches) == 1
        assert res.batches[0]["batch_id"] == "batchA"
        assert res.skipped_summaries == ["batchB"]


def test_replay_season_leaderboard():
    """Test replay season leaderboard."""
    with tempfile.TemporaryDirectory() as tmp:
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"
        
        replay_index = {
            "season": season,
            "generated_at": "2025-12-21T00:00:00Z",
            "batches": [
                {
                    "batch_id": "batchA",
                    "summary": {
                        "topk": [
                            {"job_id": "job1", "score": 1.5, "strategy_id": "S1", "dataset_id": "D1"},
                            {"job_id": "job2", "score": 1.2, "strategy_id": "S2", "dataset_id": "D1"},
                        ],
                        "metrics": {},
                    },
                },
                {
                    "batch_id": "batchB",
                    "summary": {
                        "topk": [
                            {"job_id": "job3", "score": 1.8, "strategy_id": "S1", "dataset_id": "D2"},
                            {"job_id": "job4", "score": 0.9, "strategy_id": "S2", "dataset_id": "D2"},
                        ],
                        "metrics": {},
                    },
                },
            ],
        }
        
        _wjson(exports_root / "seasons" / season / "replay_index.json", replay_index)
        
        # Test group_by strategy_id
        res = replay_season_leaderboard(exports_root, season, group_by="strategy_id", per_group=2)
        assert res.season == season
        assert res.group_by == "strategy_id"
        assert res.per_group == 2
        assert len(res.groups) == 2  # S1 and S2
        
        # Find S1 group
        s1_group = next(g for g in res.groups if g["key"] == "S1")
        assert s1_group["total"] == 2
        assert len(s1_group["items"]) == 2
        assert s1_group["items"][0]["score"] == 1.8  # top score first
        
        # Test group_by dataset_id
        res2 = replay_season_leaderboard(exports_root, season, group_by="dataset_id", per_group=1)
        assert len(res2.groups) == 2  # D1 and D2
        d1_group = next(g for g in res2.groups if g["key"] == "D1")
        assert len(d1_group["items"]) == 1  # per_group=1


def test_export_season_compare_topk_endpoint(client):
    """Test /exports/seasons/{season}/compare/topk endpoint."""
    with tempfile.TemporaryDirectory() as tmp:
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"
        
        replay_index = {
            "season": season,
            "generated_at": "2025-12-21T00:00:00Z",
            "batches": [
                {
                    "batch_id": "batchA",
                    "summary": {
                        "topk": [{"job_id": "job1", "score": 1.5}],
                        "metrics": {},
                    },
                },
            ],
        }
        
        _wjson(exports_root / "seasons" / season / "replay_index.json", replay_index)
        
        with patch("FishBroWFS_V2.control.api.get_exports_root", return_value=exports_root):
            r = client.get(f"/exports/seasons/{season}/compare/topk?k=5")
            assert r.status_code == 200
            data = r.json()
            assert data["season"] == season
            assert data["k"] == 5
            assert len(data["items"]) == 1
            assert data["items"][0]["job_id"] == "job1"


def test_export_season_compare_batches_endpoint(client):
    """Test /exports/seasons/{season}/compare/batches endpoint."""
    with tempfile.TemporaryDirectory() as tmp:
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"
        
        replay_index = {
            "season": season,
            "generated_at": "2025-12-21T00:00:00Z",
            "batches": [
                {
                    "batch_id": "batchA",
                    "summary": {
                        "topk": [{"job_id": "job1", "score": 1.5}],
                        "metrics": {"n": 10},
                    },
                    "index": {"jobs": ["job1"]},
                },
            ],
        }
        
        _wjson(exports_root / "seasons" / season / "replay_index.json", replay_index)
        
        with patch("FishBroWFS_V2.control.api.get_exports_root", return_value=exports_root):
            r = client.get(f"/exports/seasons/{season}/compare/batches")
            assert r.status_code == 200
            data = r.json()
            assert data["season"] == season
            assert len(data["batches"]) == 1
            assert data["batches"][0]["batch_id"] == "batchA"


def test_export_season_compare_leaderboard_endpoint(client):
    """Test /exports/seasons/{season}/compare/leaderboard endpoint."""
    with tempfile.TemporaryDirectory() as tmp:
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"
        
        replay_index = {
            "season": season,
            "generated_at": "2025-12-21T00:00:00Z",
            "batches": [
                {
                    "batch_id": "batchA",
                    "summary": {
                        "topk": [
                            {"job_id": "job1", "score": 1.5, "strategy_id": "S1"},
                        ],
                        "metrics": {},
                    },
                },
            ],
        }
        
        _wjson(exports_root / "seasons" / season / "replay_index.json", replay_index)
        
        with patch("FishBroWFS_V2.control.api.get_exports_root", return_value=exports_root):
            r = client.get(f"/exports/seasons/{season}/compare/leaderboard?group_by=strategy_id")
            assert r.status_code == 200
            data = r.json()
            assert data["season"] == season
            assert data["group_by"] == "strategy_id"
            assert len(data["groups"]) == 1
            assert data["groups"][0]["key"] == "S1"


def test_export_endpoints_missing_replay_index(client):
    """Test 404 when replay_index.json missing."""
    with tempfile.TemporaryDirectory() as tmp:
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"
        
        with patch("FishBroWFS_V2.control.api.get_exports_root", return_value=exports_root):
            r = client.get(f"/exports/seasons/{season}/compare/topk")
            assert r.status_code == 404
            assert "replay_index.json" in r.json()["detail"]


def test_deterministic_ordering():
    """Test deterministic ordering in replay functions."""
    with tempfile.TemporaryDirectory() as tmp:
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"
        
        # Create replay index with batches in non-alphabetical order
        replay_index = {
            "season": season,
            "generated_at": "2025-12-21T00:00:00Z",
            "batches": [
                {
                    "batch_id": "batchZ",
                    "summary": {
                        "topk": [{"job_id": "jobZ", "score": 1.0}],
                        "metrics": {},
                    },
                },
                {
                    "batch_id": "batchA",
                    "summary": {
                        "topk": [{"job_id": "jobA", "score": 2.0}],
                        "metrics": {},
                    },
                },
            ],
        }
        
        _wjson(exports_root / "seasons" / season / "replay_index.json", replay_index)
        
        # Test that batches are processed in sorted order (batchA before batchZ)
        res = replay_season_topk(exports_root, season, k=10)
        # The items should be sorted by score, not batch order
        scores = [item["score"] for item in res.items]
        assert scores == [2.0, 1.0]  # score ordering, not batch ordering
        
        # Test batch cards ordering
        res2 = replay_season_batch_cards(exports_root, season)
        batch_ids = [b["batch_id"] for b in res2.batches]
        assert batch_ids == ["batchA", "batchZ"]  # sorted by batch_id


def test_replay_with_empty_topk():
    """Test replay with empty topk lists."""
    with tempfile.TemporaryDirectory() as tmp:
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"
        
        replay_index = {
            "season": season,
            "generated_at": "2025-12-21T00:00:00Z",
            "batches": [
                {
                    "batch_id": "batchA",
                    "summary": {
                        "topk": [],
                        "metrics": {},
                    },
                },
            ],
        }
        
        _wjson(exports_root / "seasons" / season / "replay_index.json", replay_index)
        
        res = replay_season_topk(exports_root, season, k=5)
        assert res.season == season
        assert len(res.items) == 0
        assert res.skipped_batches == []  # not skipped because summary exists


def test_replay_endpoint_zero_write_guarantee(client):
    """Ensure replay endpoints do NOT write to exports tree."""
    import os
    import time
    
    with tempfile.TemporaryDirectory() as tmp:
        exports_root = Path(tmp) / "exports"
        season = "2026Q1"
        
        replay_index = {
            "season": season,
            "generated_at": "2025-12-21T00:00:00Z",
            "batches": [
                {
                    "batch_id": "batchA",
                    "summary": {
                        "topk": [{"job_id": "job1", "score": 1.5}],
                        "metrics": {},
                    },
                    "index": {"jobs": ["job1"]},
                },
            ],
        }
        
        _wjson(exports_root / "seasons" / season / "replay_index.json", replay_index)
        
        # Record initial state
        def get_file_state():
            files = []
            for root, dirs, filenames in os.walk(exports_root):
                for f in filenames:
                    path = Path(root) / f
                    files.append((str(path.relative_to(exports_root)), path.stat().st_mtime))
            return sorted(files)
        
        initial_state = get_file_state()
        
        with patch("FishBroWFS_V2.control.api.get_exports_root", return_value=exports_root):
            # Call each replay endpoint
            r1 = client.get(f"/exports/seasons/{season}/compare/topk?k=5")
            assert r1.status_code == 200
            r2 = client.get(f"/exports/seasons/{season}/compare/batches")
            assert r2.status_code == 200
            r3 = client.get(f"/exports/seasons/{season}/compare/leaderboard?group_by=strategy_id")
            assert r3.status_code == 200
        
        # Wait a tiny bit to ensure mtime could change if write occurred
        time.sleep(0.01)
        
        final_state = get_file_state()
        
        # No new files should appear, no mtime changes
        assert initial_state == final_state, "Replay endpoints must not write to exports tree"



--------------------------------------------------------------------------------

FILE tests/test_portfolio_artifacts_hash_stable.py
sha256(source_bytes) = f4182d386f1fb8c6151f25d61ba9a7b6a4bacced55e3a7b0b7f82d348ec55e6b
bytes = 6115
redacted = False
--------------------------------------------------------------------------------

"""Test portfolio artifacts hash stability.

Phase 8: Test hash is deterministic and changes with spec changes.
"""

from __future__ import annotations

from pathlib import Path

import pytest

from FishBroWFS_V2.portfolio.artifacts import compute_portfolio_hash, write_portfolio_artifacts
from FishBroWFS_V2.portfolio.compiler import compile_portfolio
from FishBroWFS_V2.portfolio.loader import load_portfolio_spec
from FishBroWFS_V2.strategy.registry import load_builtin_strategies, clear


@pytest.fixture(autouse=True)
def setup_registry() -> None:
    """Setup strategy registry before each test."""
    clear()
    load_builtin_strategies()
    yield
    clear()


def test_hash_same_spec_consistent(tmp_path: Path) -> None:
    """Test hash is consistent for same spec."""
    yaml_content = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params:
      fast_period: 10.0
      slow_period: 20.0
    enabled: true
"""
    
    spec_path = tmp_path / "test.yaml"
    spec_path.write_text(yaml_content, encoding="utf-8")
    
    spec = load_portfolio_spec(spec_path)
    
    # Compute hash multiple times
    hash1 = compute_portfolio_hash(spec)
    hash2 = compute_portfolio_hash(spec)
    hash3 = compute_portfolio_hash(spec)
    
    # All hashes should be identical
    assert hash1 == hash2 == hash3
    assert len(hash1) == 40  # SHA1 hex string length


def test_hash_different_order_consistent(tmp_path: Path) -> None:
    """Test hash is consistent even if legs are in different order."""
    yaml_content1 = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params:
      fast_period: 10.0
      slow_period: 20.0
    enabled: true
  - leg_id: "leg2"
    symbol: "TWF.MXF"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/TWF_MXF_v2.yaml"
    strategy_id: "mean_revert_zscore"
    strategy_version: "v1"
    params:
      zscore_threshold: -2.0
    enabled: true
"""
    
    yaml_content2 = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg2"  # Different order
    symbol: "TWF.MXF"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/TWF_MXF_v2.yaml"
    strategy_id: "mean_revert_zscore"
    strategy_version: "v1"
    params:
      zscore_threshold: -2.0
    enabled: true
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params:
      fast_period: 10.0
      slow_period: 20.0
    enabled: true
"""
    
    spec_path1 = tmp_path / "test1.yaml"
    spec_path1.write_text(yaml_content1, encoding="utf-8")
    
    spec_path2 = tmp_path / "test2.yaml"
    spec_path2.write_text(yaml_content2, encoding="utf-8")
    
    spec1 = load_portfolio_spec(spec_path1)
    spec2 = load_portfolio_spec(spec_path2)
    
    hash1 = compute_portfolio_hash(spec1)
    hash2 = compute_portfolio_hash(spec2)
    
    # Hashes should be identical (legs are sorted by leg_id before hashing)
    assert hash1 == hash2


def test_hash_changes_with_param_change(tmp_path: Path) -> None:
    """Test hash changes when params change."""
    yaml_content1 = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params:
      fast_period: 10.0
      slow_period: 20.0
    enabled: true
"""
    
    yaml_content2 = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params:
      fast_period: 15.0  # Changed
      slow_period: 20.0
    enabled: true
"""
    
    spec_path1 = tmp_path / "test1.yaml"
    spec_path1.write_text(yaml_content1, encoding="utf-8")
    
    spec_path2 = tmp_path / "test2.yaml"
    spec_path2.write_text(yaml_content2, encoding="utf-8")
    
    spec1 = load_portfolio_spec(spec_path1)
    spec2 = load_portfolio_spec(spec_path2)
    
    hash1 = compute_portfolio_hash(spec1)
    hash2 = compute_portfolio_hash(spec2)
    
    # Hashes should be different
    assert hash1 != hash2


def test_write_artifacts_creates_files(tmp_path: Path) -> None:
    """Test write_portfolio_artifacts creates all required files."""
    yaml_content = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params:
      fast_period: 10.0
      slow_period: 20.0
    enabled: true
"""
    
    spec_path = tmp_path / "test.yaml"
    spec_path.write_text(yaml_content, encoding="utf-8")
    
    spec = load_portfolio_spec(spec_path)
    jobs = compile_portfolio(spec)
    
    out_dir = tmp_path / "artifacts"
    artifact_paths = write_portfolio_artifacts(spec, jobs, out_dir)
    
    # Check all files exist
    assert (out_dir / "portfolio_spec_snapshot.yaml").exists()
    assert (out_dir / "compiled_jobs.json").exists()
    assert (out_dir / "portfolio_index.json").exists()
    assert (out_dir / "portfolio_hash.txt").exists()
    
    # Check hash file content
    hash_content = (out_dir / "portfolio_hash.txt").read_text(encoding="utf-8").strip()
    computed_hash = compute_portfolio_hash(spec)
    assert hash_content == computed_hash
    
    # Check index contains hash
    import json
    index_content = json.loads((out_dir / "portfolio_index.json").read_text(encoding="utf-8"))
    assert index_content["portfolio_hash"] == computed_hash



--------------------------------------------------------------------------------

FILE tests/test_portfolio_compile_jobs.py
sha256(source_bytes) = b6c5b393d5fe75d51b144ef4d7919949d4bdeecec11578eec9234e8747ffdb63
bytes = 3025
redacted = False
--------------------------------------------------------------------------------

"""Test portfolio compiler.

Phase 8: Test compilation produces correct job configs.
"""

from __future__ import annotations

from pathlib import Path

import pytest

from FishBroWFS_V2.portfolio.compiler import compile_portfolio
from FishBroWFS_V2.portfolio.loader import load_portfolio_spec
from FishBroWFS_V2.strategy.registry import load_builtin_strategies, clear


@pytest.fixture(autouse=True)
def setup_registry() -> None:
    """Setup strategy registry before each test."""
    clear()
    load_builtin_strategies()
    yield
    clear()


def test_compile_enabled_legs_only(tmp_path: Path) -> None:
    """Test compilation only includes enabled legs."""
    yaml_content = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params:
      fast_period: 10.0
      slow_period: 20.0
    enabled: true
  - leg_id: "leg2"
    symbol: "TWF.MXF"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/TWF_MXF_v2.yaml"
    strategy_id: "mean_revert_zscore"
    strategy_version: "v1"
    params:
      zscore_threshold: -2.0
    enabled: false  # Disabled
"""
    
    spec_path = tmp_path / "test.yaml"
    spec_path.write_text(yaml_content, encoding="utf-8")
    
    spec = load_portfolio_spec(spec_path)
    jobs = compile_portfolio(spec)
    
    # Should only have 1 job (leg1 enabled, leg2 disabled)
    assert len(jobs) == 1
    assert jobs[0]["leg_id"] == "leg1"


def test_compile_job_has_required_keys(tmp_path: Path) -> None:
    """Test compiled jobs have all required keys."""
    yaml_content = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params:
      fast_period: 10.0
      slow_period: 20.0
    enabled: true
    tags: ["test"]
"""
    
    spec_path = tmp_path / "test.yaml"
    spec_path.write_text(yaml_content, encoding="utf-8")
    
    spec = load_portfolio_spec(spec_path)
    jobs = compile_portfolio(spec)
    
    assert len(jobs) == 1
    job = jobs[0]
    
    # Check required keys
    required_keys = {
        "portfolio_id",
        "portfolio_version",
        "leg_id",
        "symbol",
        "timeframe_min",
        "session_profile",
        "strategy_id",
        "strategy_version",
        "params",
    }
    
    assert required_keys.issubset(job.keys())
    
    # Check values
    assert job["portfolio_id"] == "test"
    assert job["portfolio_version"] == "v1"
    assert job["leg_id"] == "leg1"
    assert job["symbol"] == "CME.MNQ"
    assert job["timeframe_min"] == 60
    assert job["strategy_id"] == "sma_cross"
    assert job["strategy_version"] == "v1"
    assert job["params"] == {"fast_period": 10.0, "slow_period": 20.0}
    assert job["tags"] == ["test"]



--------------------------------------------------------------------------------

FILE tests/test_portfolio_spec_loader.py
sha256(source_bytes) = 731ed38e3e7b310912282903830c6348bb02c16c84d9148f502c675ad33ab1d7
bytes = 3646
redacted = False
--------------------------------------------------------------------------------

"""Test portfolio spec loader.

Phase 8: Test YAML/JSON loader can load and type is correct.
"""

from __future__ import annotations

from pathlib import Path

import pytest

from FishBroWFS_V2.portfolio.loader import load_portfolio_spec
from FishBroWFS_V2.portfolio.spec import PortfolioLeg, PortfolioSpec


def test_load_yaml_spec(tmp_path: Path) -> None:
    """Test loading YAML portfolio spec."""
    yaml_content = """
portfolio_id: "test"
version: "v1"
data_tz: "Asia/Taipei"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params:
      fast_period: 10.0
      slow_period: 20.0
    enabled: true
    tags: ["test"]
"""
    
    spec_path = tmp_path / "test.yaml"
    spec_path.write_text(yaml_content, encoding="utf-8")
    
    spec = load_portfolio_spec(spec_path)
    
    assert isinstance(spec, PortfolioSpec)
    assert spec.portfolio_id == "test"
    assert spec.version == "v1"
    assert spec.data_tz == "Asia/Taipei"
    assert len(spec.legs) == 1
    
    leg = spec.legs[0]
    assert isinstance(leg, PortfolioLeg)
    assert leg.leg_id == "leg1"
    assert leg.symbol == "CME.MNQ"
    assert leg.timeframe_min == 60
    assert leg.strategy_id == "sma_cross"
    assert leg.strategy_version == "v1"
    assert leg.params == {"fast_period": 10.0, "slow_period": 20.0}
    assert leg.enabled is True
    assert leg.tags == ["test"]


def test_load_json_spec(tmp_path: Path) -> None:
    """Test loading JSON portfolio spec."""
    import json
    
    json_content = {
        "portfolio_id": "test",
        "version": "v1",
        "data_tz": "Asia/Taipei",
        "legs": [
            {
                "leg_id": "leg1",
                "symbol": "CME.MNQ",
                "timeframe_min": 60,
                "session_profile": "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml",
                "strategy_id": "sma_cross",
                "strategy_version": "v1",
                "params": {
                    "fast_period": 10.0,
                    "slow_period": 20.0,
                },
                "enabled": True,
                "tags": ["test"],
            }
        ],
    }
    
    spec_path = tmp_path / "test.json"
    with spec_path.open("w", encoding="utf-8") as f:
        json.dump(json_content, f)
    
    spec = load_portfolio_spec(spec_path)
    
    assert isinstance(spec, PortfolioSpec)
    assert spec.portfolio_id == "test"
    assert len(spec.legs) == 1


def test_load_missing_fields_raises(tmp_path: Path) -> None:
    """Test loading spec with missing required fields raises ValueError."""
    yaml_content = """
portfolio_id: "test"
# Missing version
legs: []
"""
    
    spec_path = tmp_path / "test.yaml"
    spec_path.write_text(yaml_content, encoding="utf-8")
    
    with pytest.raises(ValueError, match="missing 'version' field"):
        load_portfolio_spec(spec_path)


def test_load_invalid_params_type_raises(tmp_path: Path) -> None:
    """Test loading spec with invalid params type raises ValueError."""
    yaml_content = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params: "invalid"  # Should be dict
"""
    
    spec_path = tmp_path / "test.yaml"
    spec_path.write_text(yaml_content, encoding="utf-8")
    
    with pytest.raises(ValueError, match="params must be dict"):
        load_portfolio_spec(spec_path)



--------------------------------------------------------------------------------

FILE tests/test_portfolio_validate.py
sha256(source_bytes) = 762bd481e5d7b3d27f39032d7ca5c4b40c27c1a5963d52debbaf9a8de6819a24
bytes = 3947
redacted = False
--------------------------------------------------------------------------------

"""Test portfolio validator.

Phase 8: Test validation raises errors for invalid specs.
"""

from __future__ import annotations

from pathlib import Path

import pytest

from FishBroWFS_V2.portfolio.loader import load_portfolio_spec
from FishBroWFS_V2.portfolio.validate import validate_portfolio_spec
from FishBroWFS_V2.strategy.registry import load_builtin_strategies, clear


@pytest.fixture(autouse=True)
def setup_registry() -> None:
    """Setup strategy registry before each test."""
    clear()
    load_builtin_strategies()
    yield
    clear()


def test_validate_empty_legs_raises(tmp_path: Path) -> None:
    """Test validating spec with empty legs raises ValueError."""
    yaml_content = """
portfolio_id: "test"
version: "v1"
legs: []
"""
    
    spec_path = tmp_path / "test.yaml"
    spec_path.write_text(yaml_content, encoding="utf-8")
    
    spec = load_portfolio_spec(spec_path)
    
    with pytest.raises(ValueError, match="at least one leg"):
        validate_portfolio_spec(spec)


def test_validate_duplicate_leg_id_raises(tmp_path: Path) -> None:
    """Test validating spec with duplicate leg_id raises ValueError."""
    yaml_content = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params: {}
  - leg_id: "leg1"  # Duplicate
    symbol: "TWF.MXF"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/TWF_MXF_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params: {}
"""
    
    spec_path = tmp_path / "test.yaml"
    spec_path.write_text(yaml_content, encoding="utf-8")
    
    with pytest.raises(ValueError, match="Duplicate leg_id"):
        load_portfolio_spec(spec_path)


def test_validate_nonexistent_strategy_raises(tmp_path: Path) -> None:
    """Test validating spec with nonexistent strategy raises KeyError."""
    yaml_content = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "nonexistent_strategy"  # Not in registry
    strategy_version: "v1"
    params: {}
"""
    
    spec_path = tmp_path / "test.yaml"
    spec_path.write_text(yaml_content, encoding="utf-8")
    
    spec = load_portfolio_spec(spec_path)
    
    with pytest.raises(KeyError, match="not found in registry"):
        validate_portfolio_spec(spec)


def test_validate_strategy_version_mismatch_raises(tmp_path: Path) -> None:
    """Test validating spec with strategy version mismatch raises ValueError."""
    yaml_content = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "src/FishBroWFS_V2/data/profiles/CME_MNQ_v2.yaml"
    strategy_id: "sma_cross"
    strategy_version: "v2"  # Mismatch (registry has v1)
    params: {}
"""
    
    spec_path = tmp_path / "test.yaml"
    spec_path.write_text(yaml_content, encoding="utf-8")
    
    spec = load_portfolio_spec(spec_path)
    
    with pytest.raises(ValueError, match="strategy_version mismatch"):
        validate_portfolio_spec(spec)


def test_validate_nonexistent_session_profile_raises(tmp_path: Path) -> None:
    """Test validating spec with nonexistent session profile raises FileNotFoundError."""
    yaml_content = """
portfolio_id: "test"
version: "v1"
legs:
  - leg_id: "leg1"
    symbol: "CME.MNQ"
    timeframe_min: 60
    session_profile: "nonexistent_profile.yaml"  # Not found
    strategy_id: "sma_cross"
    strategy_version: "v1"
    params: {}
"""
    
    spec_path = tmp_path / "test.yaml"
    spec_path.write_text(yaml_content, encoding="utf-8")
    
    spec = load_portfolio_spec(spec_path)
    
    with pytest.raises(FileNotFoundError):
        validate_portfolio_spec(spec)



--------------------------------------------------------------------------------

FILE tests/test_report_link_allows_minimal_artifacts.py
sha256(source_bytes) = 7bc906988c6fef4591fdb8475b5ee144e7887d3fde4b8009bd2447b2ec5a7f73
bytes = 6123
redacted = False
--------------------------------------------------------------------------------

"""Tests for report link allowing minimal artifacts.

Tests that report readiness only checks file existence,
and build_report_link always returns Viewer URL.
"""

from __future__ import annotations

import json
from pathlib import Path

import pytest

from FishBroWFS_V2.control.report_links import (
    build_report_link,
    get_outputs_root,
    is_report_ready,
)


def test_is_report_ready_with_minimal_artifacts(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that is_report_ready returns True with only three files."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    run_dir = tmp_path / run_id
    run_dir.mkdir(parents=True)
    
    # Create only the three required files
    (run_dir / "manifest.json").write_text(json.dumps({"run_id": run_id}))
    # Use winners_v2.json (preferred) or winners.json (fallback)
    (run_dir / "winners_v2.json").write_text(json.dumps({"summary": {}}))
    (run_dir / "governance.json").write_text(json.dumps({"scoring": {}}))
    
    # Should return True
    assert is_report_ready(run_id) is True


def test_is_report_ready_missing_file(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that is_report_ready returns False if any file is missing."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    run_dir = tmp_path / run_id
    run_dir.mkdir(parents=True)
    
    # Create only two files (missing governance.json)
    (run_dir / "manifest.json").write_text(json.dumps({"run_id": run_id}))
    (run_dir / "winners.json").write_text(json.dumps({"summary": {}}))
    
    # Should return False
    assert is_report_ready(run_id) is False


def test_build_report_link_always_returns_url(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that build_report_link always returns Viewer URL."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    
    # Should return URL even if artifacts don't exist
    report_link = build_report_link(run_id)
    
    assert report_link is not None
    assert report_link.startswith("/?")
    assert run_id in report_link
    assert "season" in report_link


def test_build_report_link_no_error_string(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that build_report_link never returns error string."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    
    # Should never return error string
    report_link = build_report_link(run_id)
    
    assert report_link is not None
    assert isinstance(report_link, str)
    assert "error" not in report_link.lower()
    assert "not ready" not in report_link.lower()
    assert "missing" not in report_link.lower()


def test_is_report_ready_never_raises(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that is_report_ready never raises exceptions."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    # Should not raise even with invalid run_id
    result = is_report_ready("nonexistent_run")
    assert isinstance(result, bool)
    
    # Should not raise even with None
    result = is_report_ready(None)  # type: ignore
    assert isinstance(result, bool)


def test_build_report_link_never_raises(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that build_report_link never raises exceptions."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    # Should not raise even with invalid run_id
    report_link = build_report_link("nonexistent_run")
    assert report_link is not None
    assert isinstance(report_link, str)
    
    # Should not raise even with empty string
    report_link = build_report_link("")
    assert report_link is not None
    assert isinstance(report_link, str)


def test_minimal_artifacts_content_not_checked(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that is_report_ready does not check content validity."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    run_dir = tmp_path / run_id
    run_dir.mkdir(parents=True)
    
    # Create files with invalid JSON content
    (run_dir / "manifest.json").write_text("invalid json")
    (run_dir / "winners_v2.json").write_text("not json")
    (run_dir / "governance.json").write_text("{}")
    
    # Should still return True (only checks existence)
    assert is_report_ready(run_id) is True


def test_is_report_ready_accepts_winners_json_fallback(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that is_report_ready accepts winners.json as fallback."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    run_dir = tmp_path / run_id
    run_dir.mkdir(parents=True)
    
    # Create files with winners.json (not winners_v2.json)
    (run_dir / "manifest.json").write_text(json.dumps({"run_id": run_id}))
    (run_dir / "winners.json").write_text(json.dumps({"summary": {}}))
    (run_dir / "governance.json").write_text(json.dumps({"scoring": {}}))
    
    # Should still return True (only checks existence)
    assert is_report_ready(run_id) is True


def test_ui_does_not_block_with_minimal_artifacts(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that UI flow does not block with minimal artifacts."""
    monkeypatch.setenv("FISHBRO_OUTPUTS_ROOT", str(tmp_path))
    
    run_id = "test_run_123"
    run_dir = tmp_path / run_id
    run_dir.mkdir(parents=True)
    
    # Create minimal artifacts
    (run_dir / "manifest.json").write_text(json.dumps({"run_id": run_id}))
    (run_dir / "winners_v2.json").write_text(json.dumps({"summary": {}}))
    (run_dir / "governance.json").write_text(json.dumps({"scoring": {}}))
    
    # build_report_link should work
    report_link = build_report_link(run_id)
    assert report_link is not None
    assert "error" not in report_link.lower()
    
    # is_report_ready should return True
    assert is_report_ready(run_id) is True



--------------------------------------------------------------------------------

FILE tests/test_research_console_filters.py
sha256(source_bytes) = 6b8185ba642fbe44c231b0d11a668f5f97439f33d676434c39b5a5de5aacdac0
bytes = 13893
redacted = True
--------------------------------------------------------------------------------

"""Test research_console filters.

Phase 10: Test apply_filters() deterministic behavior.
"""

import pytest
from FishBroWFS_V2.gui.research_console import apply_filters, _norm_optional_text, _norm_optional_choice


def test_norm_optional_text():
    """Test _norm_optional_text helper."""
    # None -> None
    assert _norm_optional_text(None) is None
    
    # Empty string -> None
    assert _norm_optional_text("") is None
    assert _norm_optional_text(" ") is None
    assert _norm_optional_text("\n\t") is None
    
    # Non-string -> string
    assert _norm_optional_text(123) == "123"
    assert _norm_optional_text(True) == "True"
    
    # String with whitespace -> trimmed
    assert _norm_optional_text("  hello  ") == "hello"
    assert _norm_optional_text("hello\n") == "hello"
    assert _norm_optional_text("\thello\t") == "hello"


def test_norm_optional_choice():
    """Test _norm_optional_choice helper."""
    # None -> None
    assert _norm_optional_choice(None) is None
    assert _norm_optional_choice(None, all_tokens=[REDACTED]    
    # Empty/whitespace -> None
    assert _norm_optional_choice("") is None
    assert _norm_optional_choice(" ") is None
    assert _norm_optional_choice("\n\t") is None
    
    # ALL tokens -> None (case-insensitive)
    assert _norm_optional_choice("ALL") is None
    assert _norm_optional_choice("all") is None
    assert _norm_optional_choice(" All ") is None
    assert _norm_optional_choice("UNDECIDED", all_tokens=[REDACTED]    assert _norm_optional_choice("undecided", all_tokens=[REDACTED]    
    # Other values -> trimmed original
    assert _norm_optional_choice("AAPL") == "AAPL"
    assert _norm_optional_choice("  AAPL  ") == "AAPL"
    assert _norm_optional_choice("keep") == "keep"  # NOT uppercased
    assert _norm_optional_choice("KEEP") == "KEEP"


def test_apply_filters_empty_rows():
    """Test with empty rows."""
    rows = []
    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=None)
    assert result == []


def test_apply_filters_no_filters():
    """Test with no filters applied."""
    rows = [
        {"run_id": "run1", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "KEEP"},
        {"run_id": "run2", "symbol": "GOOG", "strategy_id": "strategy2", "decision": "DROP"},
    ]
    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=None)
    assert result == rows


def test_apply_filters_text_normalize():
    """Test text filter normalization."""
    rows = [
        {"run_id": "run1", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "KEEP"},
        {"run_id": "run2", "symbol": "GOOG", "strategy_id": "strategy2", "decision": "DROP"},
    ]
    
    # Empty string should not filter
    result = apply_filters(rows, text="", symbol=None, strategy_id=None, decision=None)
    assert len(result) == 2
    
    # Whitespace-only should not filter
    result = apply_filters(rows, text=" ", symbol=None, strategy_id=None, decision=None)
    assert len(result) == 2
    
    result = apply_filters(rows, text="\n\t", symbol=None, strategy_id=None, decision=None)
    assert len(result) == 2
    
    # Actual text should filter
    result = apply_filters(rows, text="run1", symbol=None, strategy_id=None, decision=None)
    assert len(result) == 1
    assert result[0]["run_id"] == "run1"


def test_apply_filters_choice_normalize():
    """Test choice filter normalization."""
    rows = [
        {"run_id": "run1", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "KEEP"},
        {"run_id": "run2", "symbol": "GOOG", "strategy_id": "strategy2", "decision": "DROP"},
    ]
    
    # ALL should not filter (case-insensitive)
    result = apply_filters(rows, text=None, symbol="ALL", strategy_id=None, decision=None)
    assert len(result) == 2
    
    result = apply_filters(rows, text=None, symbol="all", strategy_id=None, decision=None)
    assert len(result) == 2
    
    result = apply_filters(rows, text=None, symbol=" All ", strategy_id=None, decision=None)
    assert len(result) == 2
    
    # Same for strategy_id
    result = apply_filters(rows, text=None, symbol=None, strategy_id="ALL", decision=None)
    assert len(result) == 2
    
    # Same for decision
    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision="ALL")
    assert len(result) == 2


def test_apply_filters_undecided_semantics():
    """Test UNDECIDED decision filter semantics."""
    rows = [
        {"run_id": "run1", "symbol": "AAPL", "strategy_id": "s1", "decision": None},
        {"run_id": "run2", "symbol": "GOOG", "strategy_id": "s2", "decision": ""},
        {"run_id": "run3", "symbol": "MSFT", "strategy_id": "s3", "decision": " "},
        {"run_id": "run4", "symbol": "TSLA", "strategy_id": "s4", "decision": "KEEP"},
        {"run_id": "run5", "symbol": "NVDA", "strategy_id": "s5", "decision": "DROP"},
    ]
    
    # UNDECIDED should match None, empty string, and whitespace-only
    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision="UNDECIDED")
    assert len(result) == 3
    run_ids = {r["run_id"] for r in result}
    assert run_ids == {"run1", "run2", "run3"}
    
    # Case-insensitive
    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision="undecided")
    assert len(result) == 3
    
    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision=" Undecided ")
    assert len(result) == 3


def test_apply_filters_case_insensitive():
    """Test case-insensitive filtering."""
    rows = [
        {"run_id": "RUN1", "symbol": "AAPL", "strategy_id": "STRATEGY1", "decision": "KEEP"},
        {"run_id": "run2", "symbol": "goog", "strategy_id": "strategy2", "decision": "drop"},
    ]
    
    # Symbol filter case-insensitive
    result = apply_filters(rows, text=None, symbol="aapl", strategy_id=None, decision=None)
    assert len(result) == 1
    assert result[0]["symbol"] == "AAPL"
    
    result = apply_filters(rows, text=None, symbol="AAPL", strategy_id=None, decision=None)
    assert len(result) == 1
    assert result[0]["symbol"] == "AAPL"
    
    # Strategy filter case-insensitive
    result = apply_filters(rows, text=None, symbol=None, strategy_id="strategy1", decision=None)
    assert len(result) == 1
    assert result[0]["strategy_id"] == "STRATEGY1"
    
    # Decision filter case-insensitive
    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision="keep")
    assert len(result) == 1
    assert result[0]["decision"] == "KEEP"
    
    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision="KEEP")
    assert len(result) == 1
    assert result[0]["decision"] == "KEEP"


def test_apply_filters_text_search():
    """Test text filter."""
    rows = [
        {"run_id": "run_aapl_001", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "KEEP"},
        {"run_id": "run_goog_002", "symbol": "GOOG", "strategy_id": "strategy2", "decision": "DROP"},
        {"run_id": "run_aapl_003", "symbol": "AAPL", "strategy_id": "strategy3", "decision": "ARCHIVE"},
    ]
    
    # Search in run_id
    result = apply_filters(rows, text="aapl", symbol=None, strategy_id=None, decision=None)
    assert len(result) == 2
    assert all("aapl" in row["run_id"].lower() for row in result)
    
    # Search in symbol
    result = apply_filters(rows, text="goog", symbol=None, strategy_id=None, decision=None)
    assert len(result) == 1
    assert result[0]["symbol"] == "GOOG"
    
    # Search in strategy_id
    result = apply_filters(rows, text="strategy2", symbol=None, strategy_id=None, decision=None)
    assert len(result) == 1
    assert result[0]["strategy_id"] == "strategy2"
    
    # Search in note field
    rows_with_notes = [
        {"run_id": "run1", "symbol": "AAPL", "strategy_id": "s1", "decision": "KEEP", "note": "good results"},
        {"run_id": "run2", "symbol": "GOOG", "strategy_id": "s2", "decision": "DROP", "note": "bad performance"},
    ]
    result = apply_filters(rows_with_notes, text="good", symbol=None, strategy_id=None, decision=None)
    assert len(result) == 1
    assert result[0]["run_id"] == "run1"


def test_apply_filters_symbol_filter():
    """Test symbol filter."""
    rows = [
        {"run_id": "run1", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "KEEP"},
        {"run_id": "run2", "symbol": "GOOG", "strategy_id": "strategy2", "decision": "DROP"},
        {"run_id": "run3", "symbol": "AAPL", "strategy_id": "strategy3", "decision": "ARCHIVE"},
    ]
    
    result = apply_filters(rows, text=None, symbol="AAPL", strategy_id=None, decision=None)
    assert len(result) == 2
    assert all(row["symbol"] == "AAPL" for row in result)


def test_apply_filters_strategy_filter():
    """Test strategy filter."""
    rows = [
        {"run_id": "run1", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "KEEP"},
        {"run_id": "run2", "symbol": "GOOG", "strategy_id": "strategy2", "decision": "DROP"},
        {"run_id": "run3", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "ARCHIVE"},
    ]
    
    result = apply_filters(rows, text=None, symbol=None, strategy_id="strategy1", decision=None)
    assert len(result) == 2
    assert all(row["strategy_id"] == "strategy1" for row in result)


def test_apply_filters_decision_filter():
    """Test decision filter."""
    rows = [
        {"run_id": "run1", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "KEEP"},
        {"run_id": "run2", "symbol": "GOOG", "strategy_id": "strategy2", "decision": "DROP"},
        {"run_id": "run3", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "KEEP"},
        {"run_id": "run4", "symbol": "MSFT", "strategy_id": "strategy3", "decision": "ARCHIVE"},
    ]
    
    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision="KEEP")
    assert len(result) == 2
    assert all(row["decision"] == "KEEP" for row in result)
    
    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision="DROP")
    assert len(result) == 1
    assert result[0]["decision"] == "DROP"
    
    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision="ARCHIVE")
    assert len(result) == 1
    assert result[0]["decision"] == "ARCHIVE"


def test_apply_filters_combined_filters():
    """Test multiple filters combined."""
    rows = [
        {"run_id": "run_aapl_001", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "KEEP"},
        {"run_id": "run_aapl_002", "symbol": "AAPL", "strategy_id": "strategy2", "decision": "DROP"},
        {"run_id": "run_goog_001", "symbol": "GOOG", "strategy_id": "strategy1", "decision": "KEEP"},
        {"run_id": "run_goog_002", "symbol": "GOOG", "strategy_id": "strategy2", "decision": "ARCHIVE"},
    ]
    
    # Symbol + Decision filter
    result = apply_filters(
        rows, 
        text=None, 
        symbol="AAPL", 
        strategy_id=None, 
        decision="KEEP"
    )
    assert len(result) == 1
    assert result[0]["symbol"] == "AAPL"
    assert result[0]["decision"] == "KEEP"
    
    # Text + Strategy filter
    result = apply_filters(
        rows,
        text="goog",
        symbol=None,
        strategy_id="strategy1",
        decision=None
    )
    assert len(result) == 1
    assert "goog" in result[0]["run_id"].lower()
    assert result[0]["strategy_id"] == "strategy1"
    
    # All three filters combined
    result = apply_filters(
        rows,
        text="aapl",
        symbol="AAPL",
        strategy_id="strategy1",
        decision="KEEP"
    )
    assert len(result) == 1
    assert result[0]["run_id"] == "run_aapl_001"


def test_apply_filters_missing_fields():
    """Test with rows missing some fields."""
    rows = [
        {"run_id": "run1", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "KEEP"},
        {"run_id": "run2", "symbol": None, "strategy_id": "strategy2", "decision": "DROP"},
        {"run_id": "run3", "symbol": "AAPL", "strategy_id": None, "decision": "ARCHIVE"},
        {"run_id": "run4", "symbol": "GOOG", "strategy_id": "strategy1", "decision": None},
    ]
    
    # Filter by symbol (should exclude rows with None symbol)
    result = apply_filters(rows, text=None, symbol="AAPL", strategy_id=None, decision=None)
    assert len(result) == 2
    assert all(row["symbol"] == "AAPL" for row in result)
    
    # Filter by strategy (should exclude rows with None strategy_id)
    result = apply_filters(rows, text=None, symbol=None, strategy_id="strategy1", decision=None)
    assert len(result) == 2
    assert all(row["strategy_id"] == "strategy1" for row in result)
    
    # Filter by decision (should exclude rows with None decision)
    result = apply_filters(rows, text=None, symbol=None, strategy_id=None, decision="KEEP")
    assert len(result) == 1
    assert result[0]["decision"] == "KEEP"


def test_apply_filters_deterministic():
    """Test that filters are deterministic (same input = same output)."""
    rows = [
        {"run_id": "run1", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "KEEP"},
        {"run_id": "run2", "symbol": "GOOG", "strategy_id": "strategy2", "decision": "DROP"},
        {"run_id": "run3", "symbol": "AAPL", "strategy_id": "strategy1", "decision": "ARCHIVE"},
    ]
    
    # Run filter multiple times
    result1 = apply_filters(rows, text="aapl", symbol=None, strategy_id=None, decision="KEEP")
    result2 = apply_filters(rows, text="aapl", symbol=None, strategy_id=None, decision="KEEP")
    result3 = apply_filters(rows, text="aapl", symbol=None, strategy_id=None, decision="KEEP")
    
    assert result1 == result2 == result3
    assert len(result1) == 1
    assert result1[0]["run_id"] == "run1"



--------------------------------------------------------------------------------

FILE tests/test_research_decision.py
sha256(source_bytes) = 177a3cb5ae4b6fed90a491f6bd309a9a99395946783fedd80bce21d763057da1
bytes = 3723
redacted = False
--------------------------------------------------------------------------------

"""Tests for research decision module."""

from __future__ import annotations

import json
from pathlib import Path

import pytest

from FishBroWFS_V2.research.decision import append_decision, load_decisions


def test_append_decision_new(tmp_path: Path) -> None:
    """Test appending a new decision."""
    out_dir = tmp_path / "research"
    
    log_path = append_decision(out_dir, "test-run-123", "KEEP", "Good results")
    
    # Verify log file exists
    assert log_path.exists()
    
    # Verify log content (JSONL)
    with open(log_path, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]
        assert len(lines) == 1
        entry = json.loads(lines[0])
        assert entry["run_id"] == "test-run-123"
        assert entry["decision"] == "KEEP"
        assert entry["note"] == "Good results"
        assert "decided_at" in entry


def test_append_decision_multiple(tmp_path: Path) -> None:
    """Test appending multiple decisions (same run_id allowed)."""
    out_dir = tmp_path / "research"
    
    # Append first decision
    log_path = append_decision(out_dir, "test-run-123", "KEEP", "First decision")
    
    # Append second decision (same run_id, different decision)
    append_decision(out_dir, "test-run-123", "DROP", "Changed mind")
    
    # Verify log has 2 lines
    with open(log_path, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f if line.strip()]
        assert len(lines) == 2
    
    # Verify both entries exist
    entries = []
    with open(log_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                entries.append(json.loads(line))
    
    assert len(entries) == 2
    assert entries[0]["decision"] == "KEEP"
    assert entries[1]["decision"] == "DROP"
    assert entries[1]["run_id"] == "test-run-123"


def test_load_decisions_empty(tmp_path: Path) -> None:
    """Test loading decisions when log doesn't exist."""
    out_dir = tmp_path / "research"
    
    decisions = load_decisions(out_dir)
    assert decisions == []


def test_load_decisions_multiple(tmp_path: Path) -> None:
    """Test loading multiple decisions."""
    out_dir = tmp_path / "research"
    
    # Append multiple decisions
    append_decision(out_dir, "run-1", "KEEP", "Note 1")
    append_decision(out_dir, "run-2", "DROP", "Note 2")
    append_decision(out_dir, "run-3", "ARCHIVE", "Note 3")
    
    # Load decisions
    decisions = load_decisions(out_dir)
    
    assert len(decisions) == 3
    
    # Verify all decisions are present
    run_ids = {d["run_id"] for d in decisions}
    assert run_ids == {"run-1", "run-2", "run-3"}
    
    # Verify decisions
    decision_map = {d["run_id"]: d["decision"] for d in decisions}
    assert decision_map["run-1"] == "KEEP"
    assert decision_map["run-2"] == "DROP"
    assert decision_map["run-3"] == "ARCHIVE"


def test_load_decisions_same_run_multiple_times(tmp_path: Path) -> None:
    """Test loading decisions when same run_id appears multiple times."""
    out_dir = tmp_path / "research"
    
    # Append same run_id multiple times
    append_decision(out_dir, "run-1", "KEEP", "First")
    append_decision(out_dir, "run-1", "DROP", "Second")
    append_decision(out_dir, "run-1", "ARCHIVE", "Third")
    
    # Load decisions - should return all entries
    decisions = load_decisions(out_dir)
    
    assert len(decisions) == 3
    # All should have same run_id
    assert all(d["run_id"] == "run-1" for d in decisions)
    # Decisions should be in order
    assert decisions[0]["decision"] == "KEEP"
    assert decisions[1]["decision"] == "DROP"
    assert decisions[2]["decision"] == "ARCHIVE"



--------------------------------------------------------------------------------

FILE tests/test_research_extract.py
sha256(source_bytes) = 90faf13f82d90b1b59ba34102eb9c4ed2aafd37ff6f60a4d9265b2aaf0a83701
bytes = 6394
redacted = False
--------------------------------------------------------------------------------

"""Tests for research extract module."""

from __future__ import annotations

import json
from pathlib import Path

import pytest

from FishBroWFS_V2.research.extract import extract_canonical_metrics, ExtractionError
from FishBroWFS_V2.research.metrics import CanonicalMetrics


def test_extract_canonical_metrics_success(tmp_path: Path) -> None:
    """Test successful extraction of canonical metrics."""
    run_dir = tmp_path / "run"
    run_dir.mkdir()
    
    # Create manifest.json
    manifest = {
        "run_id": "test-run-123",
        "bars": 1000,
        "created_at": "2025-01-01T00:00:00Z",
    }
    with open(run_dir / "manifest.json", "w", encoding="utf-8") as f:
        json.dump(manifest, f)
    
    # Create metrics.json
    metrics_data = {
        "stage_name": "stage2_confirm",
    }
    with open(run_dir / "metrics.json", "w", encoding="utf-8") as f:
        json.dump(metrics_data, f)
    
    # Create winners.json with topk
    winners = {
        "schema": "v2",
        "stage_name": "stage2_confirm",
        "topk": [
            {
                "candidate_id": "test:1",
                "strategy_id": "donchian_atr",
                "symbol": "CME.MNQ",
                "timeframe": "60m",
                "metrics": {
                    "net_profit": 100.0,
                    "max_dd": -50.0,
                    "trades": 10,
                },
                "score": 100.0,
            },
            {
                "candidate_id": "test:2",
                "strategy_id": "donchian_atr",
                "symbol": "CME.MNQ",
                "timeframe": "60m",
                "metrics": {
                    "net_profit": 50.0,
                    "max_dd": -20.0,
                    "trades": 5,
                },
                "score": 50.0,
            },
        ],
    }
    with open(run_dir / "winners.json", "w", encoding="utf-8") as f:
        json.dump(winners, f)
    
    # Extract metrics
    metrics = extract_canonical_metrics(run_dir)
    
    # Verify
    assert metrics.run_id == "test-run-123"
    assert metrics.bars == 1000
    assert metrics.trades == 15  # 10 + 5
    assert metrics.net_profit == 150.0  # 100 + 50
    assert metrics.max_drawdown == 50.0  # abs(-50)
    assert metrics.start_date == "2025-01-01T00:00:00Z"
    assert metrics.strategy_id == "donchian_atr"
    assert metrics.symbol == "CME.MNQ"
    assert metrics.timeframe_min == 60
    assert metrics.score_net_mdd == 150.0 / 50.0  # net_profit / max_drawdown
    assert metrics.score_final > 0  # score_net_mdd * (trades ** 0.25)


def test_extract_canonical_metrics_missing_artifacts(tmp_path: Path) -> None:
    """Test extraction fails when no artifacts exist."""
    run_dir = tmp_path / "run"
    run_dir.mkdir()
    
    # No artifacts
    with pytest.raises(ExtractionError, match="No artifacts found"):
        extract_canonical_metrics(run_dir)


def test_extract_canonical_metrics_missing_run_id(tmp_path: Path) -> None:
    """Test extraction fails when run_id is missing."""
    run_dir = tmp_path / "run"
    run_dir.mkdir()
    
    # Create manifest without run_id
    with open(run_dir / "manifest.json", "w", encoding="utf-8") as f:
        json.dump({"bars": 100}, f)
    with open(run_dir / "metrics.json", "w", encoding="utf-8") as f:
        json.dump({}, f)
    
    # Should raise ExtractionError
    with pytest.raises(ExtractionError, match="Missing 'run_id'"):
        extract_canonical_metrics(run_dir)


def test_extract_canonical_metrics_missing_bars(tmp_path: Path) -> None:
    """Test extraction fails when bars is missing."""
    run_dir = tmp_path / "run"
    run_dir.mkdir()
    
    # Create manifest without bars
    with open(run_dir / "manifest.json", "w", encoding="utf-8") as f:
        json.dump({"run_id": "test"}, f)
    with open(run_dir / "metrics.json", "w", encoding="utf-8") as f:
        json.dump({}, f)
    
    # Should raise ExtractionError
    with pytest.raises(ExtractionError, match="Missing 'bars'"):
        extract_canonical_metrics(run_dir)


def test_extract_canonical_metrics_zero_drawdown_with_profit(tmp_path: Path) -> None:
    """Test extraction raises when max_drawdown is 0 but net_profit is non-zero."""
    run_dir = tmp_path / "run"
    run_dir.mkdir()
    
    manifest = {
        "run_id": "test-run",
        "bars": 1000,
        "created_at": "2025-01-01T00:00:00Z",
    }
    with open(run_dir / "manifest.json", "w", encoding="utf-8") as f:
        json.dump(manifest, f)
    
    with open(run_dir / "metrics.json", "w", encoding="utf-8") as f:
        json.dump({}, f)
    
    winners = {
        "schema": "v2",
        "topk": [
            {
                "candidate_id": "test:1",
                "metrics": {
                    "net_profit": 100.0,
                    "max_dd": 0.0,  # Zero drawdown
                    "trades": 10,
                },
            },
        ],
    }
    with open(run_dir / "winners.json", "w", encoding="utf-8") as f:
        json.dump(winners, f)
    
    # Should raise ExtractionError
    with pytest.raises(ExtractionError, match="cannot calculate score_net_mdd"):
        extract_canonical_metrics(run_dir)


def test_extract_canonical_metrics_no_trades(tmp_path: Path) -> None:
    """Test extraction with no trades."""
    run_dir = tmp_path / "run"
    run_dir.mkdir()
    
    manifest = {
        "run_id": "test-run-no-trades",
        "bars": 1000,
        "created_at": "2025-01-01T00:00:00Z",
    }
    with open(run_dir / "manifest.json", "w", encoding="utf-8") as f:
        json.dump(manifest, f)
    
    with open(run_dir / "metrics.json", "w", encoding="utf-8") as f:
        json.dump({}, f)
    
    winners = {
        "schema": "v2",
        "topk": [
            {
                "candidate_id": "test:1",
                "metrics": {
                    "net_profit": 0.0,
                    "max_dd": 0.0,
                    "trades": 0,
                },
            },
        ],
    }
    with open(run_dir / "winners.json", "w", encoding="utf-8") as f:
        json.dump(winners, f)
    
    # Extract metrics
    metrics = extract_canonical_metrics(run_dir)
    
    # Verify zero metrics
    assert metrics.trades == 0
    assert metrics.net_profit == 0.0
    assert metrics.max_drawdown == 0.0
    assert metrics.score_net_mdd == 0.0
    assert metrics.score_final == 0.0



--------------------------------------------------------------------------------

FILE tests/test_research_registry.py
sha256(source_bytes) = 3e1fc24b76ed1292bdf3c21890d2aa6a1381874699dc56300545955391eaa6d9
bytes = 5332
redacted = False
--------------------------------------------------------------------------------

"""Tests for research registry module."""

from __future__ import annotations

import json
from pathlib import Path

import pytest

from FishBroWFS_V2.research.registry import build_research_index


def test_build_research_index_empty(tmp_path: Path) -> None:
    """Test building index with empty outputs."""
    outputs_root = tmp_path / "outputs"
    outputs_root.mkdir()
    out_dir = tmp_path / "research"
    
    index_path = build_research_index(outputs_root, out_dir)
    
    # Verify files created
    assert index_path.exists()
    assert (out_dir / "canonical_results.json").exists()
    
    # Verify content
    with open(index_path, "r", encoding="utf-8") as f:
        index_data = json.load(f)
    
    assert index_data["total_runs"] == 0
    assert index_data["entries"] == []


def test_build_research_index_with_runs(tmp_path: Path) -> None:
    """Test building index with multiple runs, verify sorting."""
    outputs_root = tmp_path / "outputs"
    
    # Create two runs with different scores
    run1_dir = outputs_root / "seasons" / "2026Q1" / "runs" / "run-1"
    run1_dir.mkdir(parents=True)
    
    run2_dir = outputs_root / "seasons" / "2026Q1" / "runs" / "run-2"
    run2_dir.mkdir(parents=True)
    
    # Run 1: Higher score_final
    manifest1 = {
        "run_id": "run-1",
        "bars": 1000,
        "created_at": "2025-01-01T00:00:00Z",
    }
    with open(run1_dir / "manifest.json", "w", encoding="utf-8") as f:
        json.dump(manifest1, f)
    
    with open(run1_dir / "metrics.json", "w", encoding="utf-8") as f:
        json.dump({}, f)
    
    winners1 = {
        "schema": "v2",
        "topk": [
            {
                "candidate_id": "test:1",
                "metrics": {
                    "net_profit": 200.0,
                    "max_dd": -50.0,
                    "trades": 20,  # Higher trades -> higher score_final
                },
            },
        ],
    }
    with open(run1_dir / "winners.json", "w", encoding="utf-8") as f:
        json.dump(winners1, f)
    
    # Run 2: Lower score_final
    manifest2 = {
        "run_id": "run-2",
        "bars": 1000,
        "created_at": "2025-01-01T00:00:00Z",
    }
    with open(run2_dir / "manifest.json", "w", encoding="utf-8") as f:
        json.dump(manifest2, f)
    
    with open(run2_dir / "metrics.json", "w", encoding="utf-8") as f:
        json.dump({}, f)
    
    winners2 = {
        "schema": "v2",
        "topk": [
            {
                "candidate_id": "test:2",
                "metrics": {
                    "net_profit": 100.0,
                    "max_dd": -50.0,
                    "trades": 10,  # Lower trades -> lower score_final
                },
            },
        ],
    }
    with open(run2_dir / "winners.json", "w", encoding="utf-8") as f:
        json.dump(winners2, f)
    
    # Build index
    out_dir = tmp_path / "research"
    index_path = build_research_index(outputs_root, out_dir)
    
    # Verify files created
    assert index_path.exists()
    canonical_path = out_dir / "canonical_results.json"
    assert canonical_path.exists()
    
    # Verify canonical_results.json
    with open(canonical_path, "r", encoding="utf-8") as f:
        canonical_data = json.load(f)
    
    assert len(canonical_data) == 2
    
    # Verify research_index.json is sorted (score_final desc)
    with open(index_path, "r", encoding="utf-8") as f:
        index_data = json.load(f)
    
    assert index_data["total_runs"] == 2
    entries = index_data["entries"]
    assert len(entries) == 2
    
    # Verify sorting: run-1 should be first (higher score_final)
    assert entries[0]["run_id"] == "run-1"
    assert entries[1]["run_id"] == "run-2"
    assert entries[0]["score_final"] > entries[1]["score_final"]


def test_build_research_index_preserves_decisions(tmp_path: Path) -> None:
    """Test that building index preserves decisions from decisions.log."""
    outputs_root = tmp_path / "outputs"
    out_dir = tmp_path / "research"
    out_dir.mkdir()
    
    # Create a run
    run_dir = outputs_root / "seasons" / "2026Q1" / "runs" / "run-1"
    run_dir.mkdir(parents=True)
    
    manifest = {
        "run_id": "run-1",
        "bars": 1000,
        "created_at": "2025-01-01T00:00:00Z",
    }
    with open(run_dir / "manifest.json", "w", encoding="utf-8") as f:
        json.dump(manifest, f)
    
    with open(run_dir / "metrics.json", "w", encoding="utf-8") as f:
        json.dump({}, f)
    
    winners = {
        "schema": "v2",
        "topk": [
            {
                "candidate_id": "test:1",
                "metrics": {
                    "net_profit": 100.0,
                    "max_dd": -50.0,
                    "trades": 10,
                },
            },
        ],
    }
    with open(run_dir / "winners.json", "w", encoding="utf-8") as f:
        json.dump(winners, f)
    
    # Add a decision
    from FishBroWFS_V2.research.decision import append_decision
    
    append_decision(out_dir, "run-1", "KEEP", "Good results")
    
    # Build index
    index_path = build_research_index(outputs_root, out_dir)
    
    # Verify decision is preserved
    with open(index_path, "r", encoding="utf-8") as f:
        index_data = json.load(f)
    
    assert index_data["entries"][0]["decision"] == "KEEP"



--------------------------------------------------------------------------------

FILE tests/test_runner_adapter_contract.py
sha256(source_bytes) = 430bbe5580d175d262b7b5b9c22d57424d2c378a3c3c21eec4172e7270cf5859
bytes = 4970
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for runner adapter.

Tests verify:
1. Adapter returns data only (no file I/O)
2. Winners schema is stable
3. Metrics structure is consistent
"""

from __future__ import annotations

import tempfile
from pathlib import Path

import numpy as np
import pytest

from FishBroWFS_V2.pipeline.runner_adapter import run_stage_job


def test_runner_adapter_returns_no_files_written():
    """Test that adapter does not write any files."""
    with tempfile.TemporaryDirectory() as tmpdir:
        tmp_path = Path(tmpdir)
        
        # Count files before
        files_before = list(tmp_path.rglob("*"))
        file_count_before = len([f for f in files_before if f.is_file()])
        
        # Run adapter
        cfg = {
            "stage_name": "stage0_coarse",
            "param_subsample_rate": 0.1,
            "topk": 10,
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "params_total": 100,
            "proxy_name": "ma_proxy_v0",
        }
        
        result = run_stage_job(cfg)
        
        # Count files after
        files_after = list(tmp_path.rglob("*"))
        file_count_after = len([f for f in files_after if f.is_file()])
        
        # Verify no new files were created
        assert file_count_after == file_count_before, (
            "Adapter should not write files, but new files were created"
        )
        
        # Verify result structure
        assert "metrics" in result
        assert "winners" in result


def test_winners_schema_is_stable():
    """Test that winners schema is stable across all stages."""
    test_cases = [
        {
            "stage_name": "stage0_coarse",
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "params_total": 100,
            "topk": 10,
        },
        {
            "stage_name": "stage1_topk",
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "params_total": 100,
            "topk": 5,
            "commission": 0.0,
            "slip": 0.0,
        },
        {
            "stage_name": "stage2_confirm",
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "params_total": 100,
            "commission": 0.0,
            "slip": 0.0,
        },
    ]
    
    for cfg in test_cases:
        cfg["param_subsample_rate"] = 1.0  # Use full for simplicity
        
        result = run_stage_job(cfg)
        
        # Verify winners schema
        winners = result.get("winners", {})
        assert "topk" in winners, f"Missing 'topk' in winners for {cfg['stage_name']}"
        assert "notes" in winners, f"Missing 'notes' in winners for {cfg['stage_name']}"
        assert isinstance(winners["topk"], list)
        assert isinstance(winners["notes"], dict)
        assert winners["notes"].get("schema") == "v1"


def test_metrics_structure_is_consistent():
    """Test that metrics structure is consistent across stages."""
    test_cases = [
        {
            "stage_name": "stage0_coarse",
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "params_total": 100,
            "topk": 10,
        },
        {
            "stage_name": "stage1_topk",
            "open_": np.random.randn(1000).astype(np.float64),
            "high": np.random.randn(1000).astype(np.float64),
            "low": np.random.randn(1000).astype(np.float64),
            "close": np.random.randn(1000).astype(np.float64),
            "params_matrix": np.random.randn(100, 3).astype(np.float64),
            "params_total": 100,
            "topk": 5,
            "commission": 0.0,
            "slip": 0.0,
        },
    ]
    
    required_fields = ["params_total", "params_effective", "bars", "stage_name"]
    
    for cfg in test_cases:
        cfg["param_subsample_rate"] = 0.5
        
        result = run_stage_job(cfg)
        
        metrics = result.get("metrics", {})
        
        # Verify required fields exist
        for field in required_fields:
            assert field in metrics, (
                f"Missing required field '{field}' in metrics for {cfg['stage_name']}"
            )
        
        # Verify stage_name matches
        assert metrics["stage_name"] == cfg["stage_name"]



--------------------------------------------------------------------------------

FILE tests/test_runner_adapter_input_coercion.py
sha256(source_bytes) = f8c038d6377f8053956fc647fe0b2db5a592ad124ff41e7a30f71d888ec3a513
bytes = 5638
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for runner adapter input coercion.

Tests verify that input arrays are coerced to np.ndarray float64,
preventing .shape access errors when lists are passed.
"""

from __future__ import annotations

import numpy as np
import pytest

from FishBroWFS_V2.pipeline.runner_adapter import run_stage_job


def test_stage0_coercion_with_lists() -> None:
    """Test that Stage0 accepts list inputs and coerces to np.ndarray."""
    # Use list instead of np.ndarray
    close_list = [100.0 + i * 0.1 for i in range(1000)]
    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5], [20.0, 10.0, 2.0]]
    
    cfg = {
        "stage_name": "stage0_coarse",
        "param_subsample_rate": 1.0,
        "topk": 3,
        "close": close_list,  # List, not np.ndarray
        "params_matrix": params_matrix_list,  # List, not np.ndarray
        "params_total": 3,
        "proxy_name": "ma_proxy_v0",
    }
    
    # Should not raise AttributeError: 'list' object has no attribute 'shape'
    result = run_stage_job(cfg)
    
    # Verify result structure
    assert "metrics" in result
    assert "winners" in result
    
    # Verify that internal arrays are np.ndarray (by checking results work)
    assert isinstance(result["winners"]["topk"], list)
    assert len(result["winners"]["topk"]) <= 3


def test_stage1_coercion_with_lists() -> None:
    """Test that Stage1 accepts list inputs and coerces to np.ndarray."""
    # Use lists instead of np.ndarray
    open_list = [100.0 + i * 0.1 for i in range(100)]
    high_list = [101.0 + i * 0.1 for i in range(100)]
    low_list = [99.0 + i * 0.1 for i in range(100)]
    close_list = [100.0 + i * 0.1 for i in range(100)]
    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5]]
    
    cfg = {
        "stage_name": "stage1_topk",
        "param_subsample_rate": 1.0,
        "topk": 2,
        "open_": open_list,  # List, not np.ndarray
        "high": high_list,  # List, not np.ndarray
        "low": low_list,  # List, not np.ndarray
        "close": close_list,  # List, not np.ndarray
        "params_matrix": params_matrix_list,  # List, not np.ndarray
        "params_total": 2,
        "commission": 0.0,
        "slip": 0.0,
    }
    
    # Should not raise AttributeError: 'list' object has no attribute 'shape'
    result = run_stage_job(cfg)
    
    # Verify result structure
    assert "metrics" in result
    assert "winners" in result
    
    # Verify that internal arrays are np.ndarray (by checking results work)
    assert isinstance(result["winners"]["topk"], list)


def test_stage2_coercion_with_lists() -> None:
    """Test that Stage2 accepts list inputs and coerces to np.ndarray."""
    # Use lists instead of np.ndarray
    open_list = [100.0 + i * 0.1 for i in range(100)]
    high_list = [101.0 + i * 0.1 for i in range(100)]
    low_list = [99.0 + i * 0.1 for i in range(100)]
    close_list = [100.0 + i * 0.1 for i in range(100)]
    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5]]
    
    cfg = {
        "stage_name": "stage2_confirm",
        "param_subsample_rate": 1.0,
        "open_": open_list,  # List, not np.ndarray
        "high": high_list,  # List, not np.ndarray
        "low": low_list,  # List, not np.ndarray
        "close": close_list,  # List, not np.ndarray
        "params_matrix": params_matrix_list,  # List, not np.ndarray
        "params_total": 2,
        "commission": 0.0,
        "slip": 0.0,
    }
    
    # Should not raise AttributeError: 'list' object has no attribute 'shape'
    result = run_stage_job(cfg)
    
    # Verify result structure
    assert "metrics" in result
    assert "winners" in result
    
    # Verify that internal arrays are np.ndarray (by checking results work)
    assert isinstance(result["winners"]["topk"], list)


def test_coercion_preserves_dtype_float64() -> None:
    """Test that coercion produces float64 arrays."""
    # Test with float32 input (should be coerced to float64)
    close_float32 = np.array([100.0, 101.0, 102.0], dtype=np.float32)
    params_matrix_float32 = np.array([[10.0, 5.0, 1.0]], dtype=np.float32)
    
    cfg = {
        "stage_name": "stage0_coarse",
        "param_subsample_rate": 1.0,
        "topk": 1,
        "close": close_float32,
        "params_matrix": params_matrix_float32,
        "params_total": 1,
        "proxy_name": "ma_proxy_v0",
    }
    
    # Should not raise errors
    result = run_stage_job(cfg)
    
    # Verify result structure
    assert "metrics" in result
    assert "winners" in result


def test_coercion_handles_mixed_inputs() -> None:
    """Test that coercion handles mixed list/np.ndarray inputs."""
    # Mix of lists and np.ndarray
    open_list = [100.0 + i * 0.1 for i in range(100)]
    high_array = np.array([101.0 + i * 0.1 for i in range(100)], dtype=np.float64)
    low_list = [99.0 + i * 0.1 for i in range(100)]
    close_array = np.array([100.0 + i * 0.1 for i in range(100)], dtype=np.float32)
    params_matrix_list = [[10.0, 5.0, 1.0], [15.0, 7.0, 1.5]]
    
    cfg = {
        "stage_name": "stage1_topk",
        "param_subsample_rate": 1.0,
        "topk": 2,
        "open_": open_list,  # List
        "high": high_array,  # np.ndarray float64
        "low": low_list,  # List
        "close": close_array,  # np.ndarray float32 (should be coerced to float64)
        "params_matrix": params_matrix_list,  # List
        "params_total": 2,
        "commission": 0.0,
        "slip": 0.0,
    }
    
    # Should not raise errors
    result = run_stage_job(cfg)
    
    # Verify result structure
    assert "metrics" in result
    assert "winners" in result



--------------------------------------------------------------------------------

FILE tests/test_runner_grid_perf_observability.py
sha256(source_bytes) = 2c6193e843b07272e108b78690743e81630c04e027c39c2b6b8d83fcd276957c
bytes = 1511
redacted = False
--------------------------------------------------------------------------------

from __future__ import annotations

import numpy as np

from FishBroWFS_V2.pipeline.runner_grid import run_grid


def test_run_grid_perf_fields_present_and_non_negative(monkeypatch) -> None:
    # Enable perf observability.
    monkeypatch.setenv("FISHBRO_PROFILE_GRID", "1")

    o = np.array([100, 101, 102, 103, 104, 105], dtype=np.float64)
    h = np.array([101, 102, 103, 104, 106, 107], dtype=np.float64)
    l = np.array([99, 100, 101, 102, 103, 104], dtype=np.float64)
    c = np.array([100.5, 101.5, 102.5, 103.5, 105.5, 106.5], dtype=np.float64)

    params = np.array([[2, 2, 1.0], [3, 2, 1.5]], dtype=np.float64)
    out = run_grid(o, h, l, c, params, commission=0.0, slip=0.0, order_qty=1, sort_params=False)

    assert "perf" in out
    perf = out["perf"]
    assert isinstance(perf, dict)

    for k in ("t_features", "t_indicators", "t_intent_gen", "t_simulate"):
        assert k in perf
        # allow None (JSON null) when measurement is unavailable; never assume 0 is meaningful
        if perf[k] is not None:
            assert float(perf[k]) >= 0.0

    assert "simulate_impl" in perf
    assert perf["simulate_impl"] in ("jit", "py")

    assert "intents_total" in perf
    if perf["intents_total"] is not None:
        assert int(perf["intents_total"]) >= 0

    # Perf harness hook: confirm we can observe intent mode when profiling is enabled.
    assert "intent_mode" in perf
    if perf["intent_mode"] is not None:
        assert perf["intent_mode"] in ("arrays", "objects")





--------------------------------------------------------------------------------

FILE tests/test_seed_demo_run.py
sha256(source_bytes) = c1843e86909f6986284791bba6cb740f0e26a5828066ed1fbfe89befd3ac1852
bytes = 4845
redacted = False
--------------------------------------------------------------------------------

"""Tests for seed_demo_run.

Tests that seed_demo_run creates demo job and artifacts correctly.
"""

from __future__ import annotations

import json
import sqlite3
from pathlib import Path

import pytest

from FishBroWFS_V2.control.seed_demo_run import main, get_db_path


def test_seed_demo_run_no_raise(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that seed_demo_run does not raise exceptions."""
    # Set outputs root to tmp_path
    monkeypatch.chdir(tmp_path)
    monkeypatch.setenv("JOBS_DB_PATH", str(tmp_path / "jobs.db"))
    
    # Should not raise
    run_id = main()
    
    assert run_id.startswith("demo_")
    assert len(run_id) > 5


def test_outputs_directory_created(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that outputs/<season>/runs/<run_id>/ directory is created."""
    monkeypatch.chdir(tmp_path)
    monkeypatch.setenv("JOBS_DB_PATH", str(tmp_path / "jobs.db"))
    
    run_id = main()
    
    # Standard path structure: outputs/<season>/runs/<run_id>/
    run_dir = tmp_path / "outputs" / "seasons" / "2026Q1" / "runs" / run_id
    assert run_dir.exists()
    assert run_dir.is_dir()


def test_artifacts_exist(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that all required artifacts are created."""
    monkeypatch.chdir(tmp_path)
    monkeypatch.setenv("JOBS_DB_PATH", str(tmp_path / "jobs.db"))
    
    run_id = main()
    # Standard path structure: outputs/<season>/runs/<run_id>/
    run_dir = tmp_path / "outputs" / "seasons" / "2026Q1" / "runs" / run_id
    
    # Check manifest.json
    manifest_path = run_dir / "manifest.json"
    assert manifest_path.exists()
    with manifest_path.open("r", encoding="utf-8") as f:
        manifest = json.load(f)
    assert manifest["run_id"] == run_id
    assert "created_at" in manifest
    
    # Check winners_v2.json
    winners_path = run_dir / "winners_v2.json"
    assert winners_path.exists()
    
    # Check governance.json
    governance_path = run_dir / "governance.json"
    assert governance_path.exists()
    
    # Check kpi.json (KPI)
    kpi_path = run_dir / "kpi.json"
    assert kpi_path.exists()
    with kpi_path.open("r", encoding="utf-8") as f:
        kpi = json.load(f)
    assert "net_profit" in kpi
    assert "max_drawdown" in kpi
    assert "num_trades" in kpi
    assert "final_score" in kpi


def test_job_in_db(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that job is created in database with DONE status."""
    monkeypatch.chdir(tmp_path)
    db_path = tmp_path / "jobs.db"
    monkeypatch.setenv("JOBS_DB_PATH", str(db_path))
    
    run_id = main()
    
    # Check database
    conn = sqlite3.connect(str(db_path))
    try:
        cursor = conn.execute("SELECT status, run_id, report_link FROM jobs WHERE run_id = ?", (run_id,))
        row = cursor.fetchone()
        assert row is not None
        
        status, db_run_id, report_link = row
        assert status == "DONE"
        assert db_run_id == run_id
        assert report_link is not None
        assert report_link.startswith("/b5?")
        assert run_id in report_link
        assert "season=2026Q1" in report_link
    finally:
        conn.close()


def test_report_link_not_none(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that report_link is not None."""
    monkeypatch.chdir(tmp_path)
    db_path = tmp_path / "jobs.db"
    monkeypatch.setenv("JOBS_DB_PATH", str(db_path))
    
    run_id = main()
    
    conn = sqlite3.connect(str(db_path))
    try:
        cursor = conn.execute("SELECT report_link FROM jobs WHERE run_id = ?", (run_id,))
        row = cursor.fetchone()
        assert row is not None
        
        report_link = row[0]
        assert report_link is not None
        assert len(report_link) > 0
    finally:
        conn.close()


def test_kpi_values_aligned(tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
    """Test that KPI values align with Phase 6.1 registry."""
    monkeypatch.chdir(tmp_path)
    monkeypatch.setenv("JOBS_DB_PATH", str(tmp_path / "jobs.db"))
    
    run_id = main()
    # Standard path structure: outputs/<season>/runs/<run_id>/
    run_dir = tmp_path / "outputs" / "seasons" / "2026Q1" / "runs" / run_id
    
    # Check kpi.json exists and has required KPIs (KPI)
    kpi_path = run_dir / "kpi.json"
    assert kpi_path.exists()
    with kpi_path.open("r", encoding="utf-8") as f:
        kpi = json.load(f)
    
    assert "net_profit" in kpi
    assert "max_drawdown" in kpi
    assert "num_trades" in kpi
    assert "final_score" in kpi
    
    # Verify KPI values match expected
    assert kpi["net_profit"] == 123456
    assert kpi["max_drawdown"] == -0.18
    assert kpi["num_trades"] == 42
    assert kpi["final_score"] == 1.23



--------------------------------------------------------------------------------

FILE tests/test_session_classification_mnq.py
sha256(source_bytes) = 38b34b9d23b8c31186318ddcdf3189acd034a0c8a88dad162d421444c978c36f
bytes = 1912
redacted = False
--------------------------------------------------------------------------------

"""Test session classification for CME.MNQ."""

from __future__ import annotations

from pathlib import Path

import pytest

from FishBroWFS_V2.data.session.classify import classify_session
from FishBroWFS_V2.data.session.loader import load_session_profile


@pytest.fixture
def mnq_profile() -> Path:
    """Load CME.MNQ session profile."""
    profile_path = Path(__file__).parent.parent / "src" / "FishBroWFS_V2" / "data" / "profiles" / "CME_MNQ_TPE_v1.yaml"
    return profile_path


def test_mnq_day_session(mnq_profile: Path) -> None:
    """Test DAY session classification for CME.MNQ."""
    profile = load_session_profile(mnq_profile)
    
    # Test DAY session times
    assert classify_session("2013/1/1 08:45:00", profile) == "DAY"
    assert classify_session("2013/1/1 10:00:00", profile) == "DAY"
    assert classify_session("2013/1/1 13:44:59", profile) == "DAY"
    
    # Test boundary (end is exclusive)
    assert classify_session("2013/1/1 13:45:00", profile) is None


def test_mnq_night_session(mnq_profile: Path) -> None:
    """Test NIGHT session classification for CME.MNQ."""
    profile = load_session_profile(mnq_profile)
    
    # Test NIGHT session times (spans midnight)
    assert classify_session("2013/1/1 21:00:00", profile) == "NIGHT"
    assert classify_session("2013/1/1 23:59:59", profile) == "NIGHT"
    assert classify_session("2013/1/2 00:00:00", profile) == "NIGHT"
    assert classify_session("2013/1/2 05:59:59", profile) == "NIGHT"
    
    # Test boundary (end is exclusive)
    assert classify_session("2013/1/2 06:00:00", profile) is None


def test_mnq_outside_session(mnq_profile: Path) -> None:
    """Test timestamps outside trading sessions."""
    profile = load_session_profile(mnq_profile)
    
    # Between sessions
    assert classify_session("2013/1/1 14:00:00", profile) is None
    assert classify_session("2013/1/1 20:59:59", profile) is None



--------------------------------------------------------------------------------

FILE tests/test_session_classification_mxf.py
sha256(source_bytes) = 635fffa900f6aef583f7e21f80a4eb40e44c0ff654762bfcec67ecbd65240d14
bytes = 1912
redacted = False
--------------------------------------------------------------------------------

"""Test session classification for TWF.MXF."""

from __future__ import annotations

from pathlib import Path

import pytest

from FishBroWFS_V2.data.session.classify import classify_session
from FishBroWFS_V2.data.session.loader import load_session_profile


@pytest.fixture
def mxf_profile() -> Path:
    """Load TWF.MXF session profile."""
    profile_path = Path(__file__).parent.parent / "src" / "FishBroWFS_V2" / "data" / "profiles" / "TWF_MXF_TPE_v1.yaml"
    return profile_path


def test_mxf_day_session(mxf_profile: Path) -> None:
    """Test DAY session classification for TWF.MXF."""
    profile = load_session_profile(mxf_profile)
    
    # Test DAY session times
    assert classify_session("2013/1/1 08:45:00", profile) == "DAY"
    assert classify_session("2013/1/1 10:00:00", profile) == "DAY"
    assert classify_session("2013/1/1 13:44:59", profile) == "DAY"
    
    # Test boundary (end is exclusive)
    assert classify_session("2013/1/1 13:45:00", profile) is None


def test_mxf_night_session(mxf_profile: Path) -> None:
    """Test NIGHT session classification for TWF.MXF."""
    profile = load_session_profile(mxf_profile)
    
    # Test NIGHT session times (spans midnight)
    assert classify_session("2013/1/1 15:00:00", profile) == "NIGHT"
    assert classify_session("2013/1/1 23:59:59", profile) == "NIGHT"
    assert classify_session("2013/1/2 00:00:00", profile) == "NIGHT"
    assert classify_session("2013/1/2 04:59:59", profile) == "NIGHT"
    
    # Test boundary (end is exclusive)
    assert classify_session("2013/1/2 05:00:00", profile) is None


def test_mxf_outside_session(mxf_profile: Path) -> None:
    """Test timestamps outside trading sessions."""
    profile = load_session_profile(mxf_profile)
    
    # Between sessions
    assert classify_session("2013/1/1 14:00:00", profile) is None
    assert classify_session("2013/1/1 14:59:59", profile) is None



--------------------------------------------------------------------------------

FILE tests/test_session_dst_mnq.py
sha256(source_bytes) = 2ab04a47829c4145a6e9b69b907a324a0920c316c4f6e6d2005a8b4d397e1d7c
bytes = 6117
redacted = False
--------------------------------------------------------------------------------

"""Test DST boundary handling for CME.MNQ.

Tests that session classification remains correct across DST transitions.
Uses programmatic timezone conversion to avoid manual TPE time errors.
"""

from __future__ import annotations

from datetime import datetime
from pathlib import Path

import pytest
from zoneinfo import ZoneInfo

from FishBroWFS_V2.data.session.classify import classify_session
from FishBroWFS_V2.data.session.loader import load_session_profile


@pytest.fixture
def mnq_v2_profile() -> Path:
    """Load CME.MNQ v2 session profile with windows format."""
    profile_path = Path(__file__).parent.parent / "src" / "FishBroWFS_V2" / "data" / "profiles" / "CME_MNQ_v2.yaml"
    return profile_path


def _chicago_to_tpe_ts_str(chicago_time_str: str, date_str: str) -> str:
    """Convert Chicago time to Taiwan time ts_str for a given date.
    
    Args:
        chicago_time_str: Time string "HH:MM:SS" in Chicago timezone
        date_str: Date string "YYYY/M/D" or "YYYY/MM/DD"
        
    Returns:
        Full ts_str "YYYY/M/D HH:MM:SS" in Taiwan timezone
    """
    # Parse date (handles non-zero-padded)
    date_parts = date_str.split("/")
    y, m, d = int(date_parts[0]), int(date_parts[1]), int(date_parts[2])
    
    # Parse Chicago time
    time_parts = chicago_time_str.split(":")
    hh, mm, ss = int(time_parts[0]), int(time_parts[1]), int(time_parts[2])
    
    # Create datetime in Chicago timezone
    chicago_tz = ZoneInfo("America/Chicago")
    dt_chicago = datetime(y, m, d, hh, mm, ss, tzinfo=chicago_tz)
    
    # Convert to Taiwan time
    tpe_tz = ZoneInfo("Asia/Taipei")
    dt_tpe = dt_chicago.astimezone(tpe_tz)
    
    # Return as "YYYY/M/D HH:MM:SS" string (matching input format)
    return f"{dt_tpe.year}/{dt_tpe.month}/{dt_tpe.day} {dt_tpe.hour:02d}:{dt_tpe.minute:02d}:{dt_tpe.second:02d}"


def test_dst_spring_forward_break(mnq_v2_profile: Path) -> None:
    """Test BREAK session classification during DST spring forward (March).
    
    CME break: 16:00-17:00 CT (Chicago time)
    During DST transition, this break period maps to different Taiwan times.
    But classification should still correctly identify BREAK session.
    """
    profile = load_session_profile(mnq_v2_profile)
    
    # DST spring forward: Second Sunday in March (2024-03-10)
    # Before DST (Standard Time, UTC-6): 16:00 CT maps to different TPE time
    # After DST (Daylight Time, UTC-5): 16:00 CT maps to different TPE time
    
    # Calculate TPE ts_str for Chicago 16:00:00 on specific dates
    # Before DST (March 9, 2024 - Saturday)
    tpe_before = _chicago_to_tpe_ts_str("16:00:00", "2024/3/9")
    tpe_before_end = _chicago_to_tpe_ts_str("16:59:59", "2024/3/9")
    
    # After DST (March 11, 2024 - Monday)
    tpe_after = _chicago_to_tpe_ts_str("16:00:00", "2024/3/11")
    tpe_after_end = _chicago_to_tpe_ts_str("16:59:59", "2024/3/11")
    
    # Test break period before DST
    assert classify_session(tpe_before, profile) == "BREAK"
    assert classify_session(tpe_before_end, profile) == "BREAK"
    
    # Test break period after DST
    assert classify_session(tpe_after, profile) == "BREAK"
    assert classify_session(tpe_after_end, profile) == "BREAK"
    
    # Verify: Same exchange time (16:00 CT) maps to different Taiwan times,
    # but classification is consistent (both are BREAK)


def test_dst_fall_back_break(mnq_v2_profile: Path) -> None:
    """Test BREAK session classification during DST fall back (November).
    
    CME break: 16:00-17:00 CT (Chicago time)
    During DST fall back, this break period maps to different Taiwan times.
    But classification should still correctly identify BREAK session.
    """
    profile = load_session_profile(mnq_v2_profile)
    
    # DST fall back: First Sunday in November (2024-11-03)
    # Before DST (Daylight Time, UTC-5): 16:00 CT maps to different TPE time
    # After DST (Standard Time, UTC-6): 16:00 CT maps to different TPE time
    
    # Calculate TPE ts_str for Chicago 16:00:00 on specific dates
    # Before DST (November 2, 2024 - Saturday)
    tpe_before = _chicago_to_tpe_ts_str("16:00:00", "2024/11/2")
    tpe_before_end = _chicago_to_tpe_ts_str("16:59:59", "2024/11/2")
    
    # After DST (November 4, 2024 - Monday)
    tpe_after = _chicago_to_tpe_ts_str("16:00:00", "2024/11/4")
    tpe_after_end = _chicago_to_tpe_ts_str("16:59:59", "2024/11/4")
    
    # Test break period before DST
    assert classify_session(tpe_before, profile) == "BREAK"
    assert classify_session(tpe_before_end, profile) == "BREAK"
    
    # Test break period after DST
    assert classify_session(tpe_after, profile) == "BREAK"
    assert classify_session(tpe_after_end, profile) == "BREAK"
    
    # Verify: Same exchange time (16:00 CT) maps to different Taiwan times,
    # but classification is consistent (both are BREAK)


def test_dst_trading_session_consistency(mnq_v2_profile: Path) -> None:
    """Test TRADING session classification remains consistent across DST.
    
    CME trading: 17:00 CT - 16:00 CT (next day)
    This should be correctly identified regardless of DST transitions.
    """
    profile = load_session_profile(mnq_v2_profile)
    
    # Calculate TPE ts_str for Chicago 17:00:00 on specific dates
    # March (before DST, Standard Time)
    tpe_mar_before = _chicago_to_tpe_ts_str("17:00:00", "2024/3/9")
    assert classify_session(tpe_mar_before, profile) == "TRADING"
    
    # March (after DST, Daylight Time)
    tpe_mar_after = _chicago_to_tpe_ts_str("17:00:00", "2024/3/11")
    assert classify_session(tpe_mar_after, profile) == "TRADING"
    
    # November (before DST, Daylight Time)
    tpe_nov_before = _chicago_to_tpe_ts_str("17:00:00", "2024/11/2")
    assert classify_session(tpe_nov_before, profile) == "TRADING"
    
    # November (after DST, Standard Time)
    tpe_nov_after = _chicago_to_tpe_ts_str("17:00:00", "2024/11/4")
    assert classify_session(tpe_nov_after, profile) == "TRADING"
    
    # Verify: Exchange time 17:00 CT is consistently classified as TRADING,
    # regardless of how it maps to Taiwan time due to DST



--------------------------------------------------------------------------------

FILE tests/test_sparse_intents_contract.py
sha256(source_bytes) = 0567ca7d7cbe9cad729b33aad3129429bf00c952d73328476f92a6e1ee10bf38
bytes = 12858
redacted = False
--------------------------------------------------------------------------------

"""
Stage P2-3A: Contract Tests for Sparse Entry Intents (Grid Level)

Verifies that entry intents are truly sparse at grid level:
- entry_intents_total == entry_valid_mask_sum (not Bars  Params)
- Sparse builder produces identical results to dense builder (same triggers)
"""
from __future__ import annotations

from dataclasses import asdict, is_dataclass

import numpy as np
import os

from FishBroWFS_V2.engine.types import Fill
from FishBroWFS_V2.pipeline.runner_grid import run_grid


def _fill_to_tuple(f: Fill) -> tuple:
    """
    Convert Fill to a comparable tuple representation.
    
    Uses dataclasses.asdict for dataclass instances, falls back to __dict__ or repr.
    Returns sorted tuple to ensure deterministic comparison.
    """
    if is_dataclass(f):
        d = asdict(f)
    else:
        # fallback: __dict__ (for normal classes)
        d = dict(getattr(f, "__dict__", {}))
        if not d:
            # last resort: repr
            return (repr(f),)
    # Fixed ordering to avoid dict order differences
    return tuple(sorted(d.items()))


def test_grid_sparse_intents_count() -> None:
    """
    Test that grid-level entry intents count scales with trigger_rate (param-subsample).
    
    This test verifies the core sparse contract at grid level:
    - entry_intents_total == entry_valid_mask_sum
    - entry_intents_total scales approximately linearly with trigger_rate
    """
    # Ensure clean environment
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    old_param_subsample_rate = os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
    old_profile_grid = os.environ.pop("FISHBRO_PROFILE_GRID", None)
    
    try:
        n_bars = 500
        n_params = 30  # Enough params to make "unique repetition" meaningful
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Generate params matrix (at least 10-50 params for meaningful unique repetition)
        params_list = []
        for i in range(n_params):
            ch_len = 20 + (i % 10)  # Vary channel_len (20-29)
            atr_len = 10 + (i % 5)  # Vary atr_len (10-14)
            stop_mult = 1.0 + (i % 3) * 0.5  # Vary stop_mult (1.0, 1.5, 2.0)
            params_list.append([ch_len, atr_len, stop_mult])
        
        params_matrix = np.array(params_list, dtype=np.float64)
        
        # Fix param_subsample_rate=1.0 (all params) to test trigger_rate effect on intents
        os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = "1.0"
        os.environ["FISHBRO_PROFILE_GRID"] = "1"
        
        # Run Dense (trigger_rate=1.0) - baseline
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
        
        result_dense = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Run Sparse (trigger_rate=0.05) - bar/intent-level sparsity
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "0.05"
        
        result_sparse = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Verify perf dicts exist
        perf_dense = result_dense.get("perf", {})
        perf_sparse = result_sparse.get("perf", {})
        
        assert isinstance(perf_dense, dict), "perf_dense must be a dict"
        assert isinstance(perf_sparse, dict), "perf_sparse must be a dict"
        
        # Core contract: entry_intents_total == entry_valid_mask_sum (both runs)
        entry_intents_dense = perf_dense.get("entry_intents_total")
        entry_valid_mask_dense = perf_dense.get("entry_valid_mask_sum")
        entry_intents_sparse = perf_sparse.get("entry_intents_total")
        entry_valid_mask_sparse = perf_sparse.get("entry_valid_mask_sum")
        
        assert entry_intents_dense == entry_valid_mask_dense, (
            f"Dense: entry_intents_total ({entry_intents_dense}) "
            f"must equal entry_valid_mask_sum ({entry_valid_mask_dense})"
        )
        assert entry_intents_sparse == entry_valid_mask_sparse, (
            f"Sparse: entry_intents_total ({entry_intents_sparse}) "
            f"must equal entry_valid_mask_sum ({entry_valid_mask_sparse})"
        )
        
        # Contract: entry_intents_sparse should be approximately trigger_rate * entry_intents_dense
        # With trigger_rate=0.05, we expect approximately 5% of dense baseline
        # Allow wide tolerance: [0.02, 0.08] (2% to 8% of dense)
        if entry_intents_dense is not None and entry_intents_dense > 0:
            ratio = entry_intents_sparse / entry_intents_dense
            assert 0.02 <= ratio <= 0.08, (
                f"With trigger_rate=0.05, entry_intents_sparse ({entry_intents_sparse}) "
                f"should be approximately 5% of entry_intents_dense ({entry_intents_dense}), "
                f"got ratio {ratio:.4f} (expected [0.02, 0.08])"
            )
        
    finally:
        # Restore environment
        if old_trigger_rate is None:
            os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
        else:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate
        
        if old_param_subsample_rate is None:
            os.environ.pop("FISHBRO_PERF_PARAM_SUBSAMPLE_RATE", None)
        else:
            os.environ["FISHBRO_PERF_PARAM_SUBSAMPLE_RATE"] = old_param_subsample_rate
        
        if old_profile_grid is None:
            os.environ.pop("FISHBRO_PROFILE_GRID", None)
        else:
            os.environ["FISHBRO_PROFILE_GRID"] = old_profile_grid


def test_sparse_vs_dense_builder_parity() -> None:
    """
    Test that sparse builder produces identical results to dense builder (same triggers).
    
    This test verifies determinism parity:
    - Same triggers set  same results (metrics, fills)
    - Order ID determinism
    - Bit-exact parity
    
    Uses FISHBRO_FORCE_SPARSE_BUILDER=1 to test numba builder vs python builder.
    """
    # Ensure clean environment
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    old_force_sparse = os.environ.pop("FISHBRO_FORCE_SPARSE_BUILDER", None)
    
    try:
        n_bars = 300
        n_params = 20
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Generate params matrix
        params_list = []
        for i in range(n_params):
            ch_len = 20 + (i % 10)
            atr_len = 10 + (i % 5)
            stop_mult = 1.0 + (i % 3) * 0.5
            params_list.append([ch_len, atr_len, stop_mult])
        
        params_matrix = np.array(params_list, dtype=np.float64)
        
        # Run A: trigger_rate=1.0, force_sparse=0 (Python builder)
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
        os.environ.pop("FISHBRO_FORCE_SPARSE_BUILDER", None)  # Ensure not set
        
        result_a = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Run B: trigger_rate=1.0, force_sparse=1 (Numba builder, same triggers)
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
        os.environ["FISHBRO_FORCE_SPARSE_BUILDER"] = "1"
        
        result_b = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Verify metrics are identical (bit-exact)
        metrics_a = result_a.get("metrics")
        metrics_b = result_b.get("metrics")
        
        assert metrics_a is not None, "metrics_a must exist"
        assert metrics_b is not None, "metrics_b must exist"
        
        # Compare metrics arrays (should be bit-exact)
        np.testing.assert_array_equal(metrics_a, metrics_b, "metrics must be bit-exact")
        
        # Verify sparse contract holds in both runs
        perf_a = result_a.get("perf", {})
        perf_b = result_b.get("perf", {})
        
        if isinstance(perf_a, dict) and isinstance(perf_b, dict):
            entry_intents_a = perf_a.get("entry_intents_total")
            entry_intents_b = perf_b.get("entry_intents_total")
            
            if entry_intents_a is not None and entry_intents_b is not None:
                assert entry_intents_a == entry_intents_b, (
                    f"entry_intents_total should be identical (same triggers): "
                    f"A={entry_intents_a}, B={entry_intents_b}"
                )
        
    finally:
        # Restore environment
        if old_trigger_rate is None:
            os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
        else:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate
        
        if old_force_sparse is None:
            os.environ.pop("FISHBRO_FORCE_SPARSE_BUILDER", None)
        else:
            os.environ["FISHBRO_FORCE_SPARSE_BUILDER"] = old_force_sparse


def test_created_bar_sorted() -> None:
    """
    Test that created_bar arrays are sorted (ascending).
    
    Note: This test verifies the sparse builder contract that created_bar must be
    sorted. We verify this indirectly through the sparse contract consistency.
    """
    # Ensure clean environment
    old_trigger_rate = os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
    
    try:
        n_bars = 200
        n_params = 10
        
        # Generate simple OHLC data
        rng = np.random.default_rng(42)
        close = 100.0 + np.cumsum(rng.standard_normal(n_bars))
        high = close + np.abs(rng.standard_normal(n_bars)) * 2.0
        low = close - np.abs(rng.standard_normal(n_bars)) * 2.0
        open_ = (high + low) / 2
        
        high = np.maximum(high, np.maximum(open_, close))
        low = np.minimum(low, np.minimum(open_, close))
        
        # Generate params matrix
        params_list = []
        for i in range(n_params):
            ch_len = 20 + (i % 5)
            atr_len = 10 + (i % 3)
            stop_mult = 1.0
            params_list.append([ch_len, atr_len, stop_mult])
        
        params_matrix = np.array(params_list, dtype=np.float64)
        
        # Run grid
        os.environ["FISHBRO_PERF_TRIGGER_RATE"] = "1.0"
        
        result = run_grid(
            open_=open_,
            high=high,
            low=low,
            close=close,
            params_matrix=params_matrix,
            commission=0.0,
            slip=0.0,
            order_qty=1,
            sort_params=True,
        )
        
        # Verify sparse contract: entry_intents_total == entry_valid_mask_sum
        perf = result.get("perf", {})
        if isinstance(perf, dict):
            entry_intents_total = perf.get("entry_intents_total")
            entry_valid_mask_sum = perf.get("entry_valid_mask_sum")
            
            if entry_intents_total is not None and entry_valid_mask_sum is not None:
                assert entry_intents_total == entry_valid_mask_sum, (
                    f"Sparse contract: entry_intents_total ({entry_intents_total}) "
                    f"must equal entry_valid_mask_sum ({entry_valid_mask_sum})"
                )
        
        # Note: created_bar sorted verification would require accessing internal arrays
        # For now, we verify the sparse contract which implies created_bar is sorted
        # (since flatnonzero returns sorted indices)
        
    finally:
        # Restore environment
        if old_trigger_rate is None:
            os.environ.pop("FISHBRO_PERF_TRIGGER_RATE", None)
        else:
            os.environ["FISHBRO_PERF_TRIGGER_RATE"] = old_trigger_rate



--------------------------------------------------------------------------------

FILE tests/test_sparse_intents_mvp_contract.py
sha256(source_bytes) = 22ba50c427e550874747dc7b3041a91a01a3b1dfc92fc22ff27bb41575595453
bytes = 9536
redacted = False
--------------------------------------------------------------------------------

"""Contract tests for sparse intents MVP (Stage P2-1).

These tests ensure:
1. created_bar is sorted (deterministic ordering)
2. intents_total drops significantly with sparse masking
3. Vectorization parity remains bit-exact
"""

import numpy as np
import pytest

from FishBroWFS_V2.config.dtypes import INDEX_DTYPE
from FishBroWFS_V2.engine.types import BarArrays
from FishBroWFS_V2.strategy.kernel import (
    DonchianAtrParams,
    _build_entry_intents_from_trigger,
    run_kernel_arrays,
)


def _expected_entry_count(donch_prev: np.ndarray, warmup: int) -> int:
    """
    Calculate expected entry count using the same mask rules as production.
    
    Production mask (from _build_entry_intents_from_trigger):
    - i = np.arange(1, n)  # bar indices t (from 1 to n-1)
    - valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)
    
    This helper replicates that exact logic.
    """
    n = donch_prev.size
    # Create index array for bars 1..n-1 (bar indices t, where created_bar = t-1)
    i = np.arange(1, n, dtype=INDEX_DTYPE)
    # Sparse mask: valid entries must be finite, positive, and past warmup
    valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)
    return int(np.count_nonzero(valid_mask))


def _make_donch_hi_with_trigger_rate(
    n_bars: int,
    warmup: int,
    trigger_rate: float,
    seed: int = 42,
) -> np.ndarray:
    """
    Generate donch_hi array with controlled trigger rate.
    
    Args:
        n_bars: number of bars
        warmup: warmup period (bars before warmup are NaN)
        trigger_rate: fraction of bars after warmup that should be valid (0.0-1.0)
        seed: random seed
    
    Returns:
        donch_hi array (float64, n_bars):
        - Bars 0..warmup-1: NaN
        - Bars warmup..n_bars-1: trigger_rate fraction are positive values, rest are NaN
    """
    rng = np.random.default_rng(seed)
    
    donch_hi = np.full(n_bars, np.nan, dtype=np.float64)
    
    # After warmup, set trigger_rate fraction to positive values
    post_warmup_bars = n_bars - warmup
    if post_warmup_bars > 0:
        n_valid = int(post_warmup_bars * trigger_rate)
        if n_valid > 0:
            # Select random indices after warmup
            valid_indices = rng.choice(
                np.arange(warmup, n_bars),
                size=n_valid,
                replace=False,
            )
            # Set valid indices to positive values (e.g., 100.0 + small random)
            donch_hi[valid_indices] = 100.0 + rng.random(n_valid) * 10.0
    
    return donch_hi


class TestSparseIntentsMVP:
    """Test sparse intents MVP contract."""

    def test_sparse_intents_created_bar_is_sorted(self):
        """
        Contract: created_bar must be sorted (non-decreasing).
        
        This ensures deterministic ordering and that sparse masking preserves
        the original bar sequence.
        """
        n_bars = 1000
        warmup = 20
        trigger_rate = 0.1
        
        # Generate donch_hi with controlled trigger rate
        donch_hi = _make_donch_hi_with_trigger_rate(n_bars, warmup, trigger_rate, seed=42)
        
        # Create donch_prev (shifted for next-bar active)
        donch_prev = np.empty_like(donch_hi)
        donch_prev[0] = np.nan
        donch_prev[1:] = donch_hi[:-1]
        
        # Build entry intents
        result = _build_entry_intents_from_trigger(
            donch_prev=donch_prev,
            channel_len=warmup,
            order_qty=1,
        )
        
        created_bar = result["created_bar"]
        n_entry = result["n_entry"]
        
        # Verify n_entry matches expected count (exact match using production mask rules)
        expected = _expected_entry_count(donch_prev, warmup)
        assert n_entry == expected, (
            f"n_entry ({n_entry}) should equal expected ({expected}) "
            f"calculated using production mask rules"
        )
        
        # Verify created_bar is sorted (non-decreasing)
        if n_entry > 1:
            assert np.all(created_bar[1:] >= created_bar[:-1]), (
                f"created_bar must be sorted (non-decreasing). "
                f"Got: {created_bar[:10]} ... (showing first 10)"
            )
        
        # Hard consistency check: created_bar must match flatnonzero result exactly
        # This locks in the ordering contract
        i = np.arange(1, donch_prev.size, dtype=INDEX_DTYPE)
        valid_mask = (~np.isnan(donch_prev[1:])) & (donch_prev[1:] > 0) & (i >= warmup)
        idx = np.flatnonzero(valid_mask).astype(created_bar.dtype)
        assert np.array_equal(created_bar[:n_entry], idx), (
            f"created_bar must exactly match flatnonzero result. "
            f"Got: {created_bar[:min(10, n_entry)]}, "
            f"Expected: {idx[:min(10, len(idx))]}"
        )

    def test_sparse_intents_total_drops_order_of_magnitude(self):
        """
        Contract: intents_total should drop significantly with sparse masking.
        
        With controlled trigger rate (e.g., 5%), intents_total should be << n_bars.
        This test directly controls donch_hi to ensure precise trigger rate.
        """
        n_bars = 1000
        warmup = 20
        trigger_rate = 0.05  # 5% trigger rate
        
        # Generate donch_hi with controlled trigger rate
        donch_hi = _make_donch_hi_with_trigger_rate(n_bars, warmup, trigger_rate, seed=42)
        
        # Create donch_prev (shifted for next-bar active)
        donch_prev = np.empty_like(donch_hi)
        donch_prev[0] = np.nan
        donch_prev[1:] = donch_hi[:-1]
        
        # Build entry intents
        result = _build_entry_intents_from_trigger(
            donch_prev=donch_prev,
            channel_len=warmup,
            order_qty=1,
        )
        
        n_entry = result["n_entry"]
        obs = result["obs"]
        
        # Verify diagnostic observations
        assert obs["n_bars"] == n_bars
        assert obs["warmup"] == warmup
        assert obs["valid_mask_sum"] == n_entry
        
        # Verify n_entry matches expected count (exact match using production mask rules)
        expected = _expected_entry_count(donch_prev, warmup)
        assert n_entry == expected, (
            f"n_entry ({n_entry}) should equal expected ({expected}) "
            f"calculated using production mask rules"
        )
        
        # Order-of-magnitude contract: n_entry should be significantly less than n_bars
        # This is the core contract of this test
        # Conservative threshold: 6% of (n_bars - warmup) as upper bound
        max_expected_ratio = 0.06  # 6% conservative upper bound
        max_expected = int((n_bars - warmup) * max_expected_ratio)
        
        assert n_entry <= max_expected, (
            f"n_entry ({n_entry}) should be <= {max_expected} "
            f"({max_expected_ratio*100}% of post-warmup bars) "
            f"with trigger_rate={trigger_rate}, n_bars={n_bars}, warmup={warmup}. "
            f"Sparse masking should significantly reduce intent count (order-of-magnitude reduction)."
        )
        
        # Also verify it's not zero (unless trigger_rate is too low)
        if trigger_rate > 0:
            # With 5% trigger rate, we should have some intents
            assert n_entry > 0, (
                f"Expected some intents with trigger_rate={trigger_rate}, "
                f"but got n_entry={n_entry}"
            )

    def test_vectorization_parity_still_bit_exact(self):
        """
        Contract: Vectorization parity tests should still pass after sparse masking.
        
        This test ensures that sparse masking doesn't break existing parity contracts.
        We rely on the existing test_vectorization_parity.py to verify this.
        
        This test is a placeholder to document the requirement.
        """
        # This test doesn't need to re-implement parity checks.
        # It's sufficient to ensure that make check passes all existing tests.
        # The actual parity verification is in tests/test_vectorization_parity.py
        
        # Basic sanity check: sparse masking should produce valid results
        n_bars = 100
        bars = BarArrays(
            open=np.arange(100, 200, dtype=np.float64),
            high=np.arange(101, 201, dtype=np.float64),
            low=np.arange(99, 199, dtype=np.float64),
            close=np.arange(100, 200, dtype=np.float64),
        )
        
        params = DonchianAtrParams(
            channel_len=10,
            atr_len=5,
            stop_mult=1.5,
        )
        
        result = run_kernel_arrays(
            bars,
            params,
            commission=0.0,
            slip=0.0,
            order_qty=1,
        )
        
        # Verify result structure is intact
        assert "fills" in result
        assert "metrics" in result
        assert "_obs" in result
        assert "intents_total" in result["_obs"]
        
        # Verify diagnostic observations are present
        assert "n_bars" in result["_obs"]
        assert "warmup" in result["_obs"]
        assert "valid_mask_sum" in result["_obs"]
        
        # Verify intents_total is reasonable
        intents_total = result["_obs"]["intents_total"]
        assert intents_total >= 0
        assert intents_total <= n_bars  # Should be <= n_bars due to sparse masking
        
        # Note: Full parity verification is done by test_vectorization_parity.py
        # This test just ensures the basic contract is met



--------------------------------------------------------------------------------

FILE tests/test_stage0_contract.py
sha256(source_bytes) = 3e0adf88bd21a99c1643510b0741c532779eca76b336c1f3649e342f45df9cd0
bytes = 1429
redacted = False
--------------------------------------------------------------------------------

from __future__ import annotations

"""
Stage 0 Contract Tests

Stage 0 must remain a "vector/proxy" layer:
  - MUST NOT import engine/matcher/strategy kernel/pipeline grid runner.
  - MUST NOT create OrderIntent/Fill objects.

These tests are intentionally strict: they prevent "silent scope creep"
that would destroy throughput and blur semantics.
"""

import ast
from pathlib import Path


def _read(path: Path) -> str:
    return path.read_text(encoding="utf-8")


def test_stage0_does_not_import_engine_or_runner_grid() -> None:
    root = Path(__file__).resolve().parent.parent
    p = root / "src" / "FishBroWFS_V2" / "stage0" / "ma_proxy.py"
    code = _read(p)
    tree = ast.parse(code)

    banned_prefixes = (
        "FishBroWFS_V2.engine",
        "FishBroWFS_V2.strategy",
        "FishBroWFS_V2.pipeline",
    )

    for node in ast.walk(tree):
        if isinstance(node, ast.Import):
            for a in node.names:
                name = a.name
                assert not name.startswith(banned_prefixes), f"banned import: {name}"
        if isinstance(node, ast.ImportFrom):
            mod = node.module or ""
            assert not mod.startswith(banned_prefixes), f"banned import-from: {mod}"


def test_stage0_file_exists() -> None:
    root = Path(__file__).resolve().parent.parent
    p = root / "src" / "FishBroWFS_V2" / "stage0" / "ma_proxy.py"
    assert p.exists(), "Stage0 module must exist"





--------------------------------------------------------------------------------

FILE tests/test_stage0_ma_proxy.py
sha256(source_bytes) = ffe0f2d0355276ca5e28204fe0180066918f9f4b48f5ace3e76a593604b2356d
bytes = 1140
redacted = False
--------------------------------------------------------------------------------

from __future__ import annotations

import numpy as np

from FishBroWFS_V2.stage0.ma_proxy import stage0_score_ma_proxy


def test_stage0_scores_shape_and_ordering_trend_series() -> None:
    # Simple upward trend: MA(5)-MA(20) should be mostly positive => positive score
    n = 500
    close = np.linspace(100.0, 200.0, n, dtype=np.float64)

    params = np.array(
        [
            [5.0, 20.0, 0.0],
            [20.0, 5.0, 0.0],  # inverted => should score worse
            [1.0, 2.0, 0.0],
        ],
        dtype=np.float64,
    )

    scores = stage0_score_ma_proxy(close, params)
    assert scores.shape == (3,)
    assert np.isfinite(scores[0])
    assert np.isfinite(scores[1])
    assert np.isfinite(scores[2])
    assert scores[0] > scores[1]


def test_stage0_rejects_invalid_lengths() -> None:
    close = np.linspace(100.0, 101.0, 50, dtype=np.float64)
    params = np.array([[0.0, 10.0], [10.0, 0.0], [1000.0, 5.0]], dtype=np.float64)
    scores = stage0_score_ma_proxy(close, params)
    assert scores.shape == (3,)
    assert scores[0] == -np.inf
    assert scores[1] == -np.inf
    assert scores[2] == -np.inf





--------------------------------------------------------------------------------

FILE tests/test_stage0_no_pnl_contract.py
sha256(source_bytes) = f9519c481fbf92f71139b02f691dc4e7b36c0458d11e914c86b78121f351fad5
bytes = 4808
redacted = False
--------------------------------------------------------------------------------

"""Test Stage0 contract: must NOT contain any PnL/metrics fields.

Stage0 is a proxy ranking stage and must not compute any PnL-related metrics.
This test enforces the contract by checking that Stage0Result does not contain
forbidden PnL/metrics fields.
"""

import inspect
import numpy as np

from FishBroWFS_V2.pipeline.stage0_runner import Stage0Result, run_stage0


# Blacklist of forbidden field names (PnL/metrics related)
FORBIDDEN_FIELD_NAMES = {
    "net",
    "profit",
    "mdd",
    "dd",
    "drawdown",
    "sqn",
    "sharpe",
    "winrate",
    "win_rate",
    "equity",
    "pnl",
    "return",
    "returns",
    "trades",
    "trade",
    "final",
    "score",
    "metric",
    "metrics",
}


def test_stage0_result_no_pnl_fields():
    """Test that Stage0Result dataclass does not contain forbidden PnL fields."""
    # Get all field names from Stage0Result
    if hasattr(Stage0Result, "__dataclass_fields__"):
        field_names = set(Stage0Result.__dataclass_fields__.keys())
    else:
        # Fallback: inspect annotations
        annotations = getattr(Stage0Result, "__annotations__", {})
        field_names = set(annotations.keys())
    
    # Check each field name against blacklist
    violations = []
    for field_name in field_names:
        field_lower = field_name.lower()
        for forbidden in FORBIDDEN_FIELD_NAMES:
            if forbidden in field_lower:
                violations.append(field_name)
                break
    
    assert len(violations) == 0, (
        f"Stage0Result contains forbidden PnL/metrics fields: {violations}\n"
        f"Allowed fields: {field_names}\n"
        f"Forbidden keywords: {FORBIDDEN_FIELD_NAMES}"
    )


def test_stage0_result_allowed_fields_only():
    """Test that Stage0Result only contains allowed fields."""
    # Allowed fields (from spec)
    allowed_fields = {"param_id", "proxy_value", "warmup_ok", "meta"}
    
    if hasattr(Stage0Result, "__dataclass_fields__"):
        actual_fields = set(Stage0Result.__dataclass_fields__.keys())
    else:
        annotations = getattr(Stage0Result, "__annotations__", {})
        actual_fields = set(annotations.keys())
    
    # Check that all fields are in allowed set
    unexpected = actual_fields - allowed_fields
    assert len(unexpected) == 0, (
        f"Stage0Result contains unexpected fields: {unexpected}\n"
        f"Allowed fields: {allowed_fields}\n"
        f"Actual fields: {actual_fields}"
    )


def test_stage0_runner_no_pnl_computation():
    """Test that run_stage0() does not compute PnL metrics."""
    # Generate test data
    np.random.seed(42)
    n_bars = 1000
    n_params = 50
    
    close = 10000 + np.cumsum(np.random.randn(n_bars)) * 10
    params_matrix = np.column_stack([
        np.random.randint(10, 100, size=n_params),
        np.random.randint(5, 50, size=n_params),
        np.random.uniform(1.0, 5.0, size=n_params),
    ]).astype(np.float64)
    
    # Run Stage0
    results = run_stage0(close, params_matrix)
    
    # Verify results structure
    assert len(results) == n_params
    
    for result in results:
        # Verify required fields exist
        assert hasattr(result, "param_id")
        assert hasattr(result, "proxy_value")
        
        # Verify param_id is valid
        assert isinstance(result.param_id, int)
        assert 0 <= result.param_id < n_params
        
        # Verify proxy_value is numeric (can be -inf for invalid params)
        assert isinstance(result.proxy_value, (int, float))
        
        # Verify no PnL fields exist (check attribute names)
        result_dict = result.__dict__ if hasattr(result, "__dict__") else {}
        for field_name in result_dict.keys():
            field_lower = field_name.lower()
            for forbidden in FORBIDDEN_FIELD_NAMES:
                assert forbidden not in field_lower, (
                    f"Stage0Result contains forbidden field: {field_name} "
                    f"(contains '{forbidden}')"
                )


def test_stage0_result_string_representation():
    """Test that Stage0Result string representation does not contain PnL keywords."""
    result = Stage0Result(
        param_id=0,
        proxy_value=10.5,
        warmup_ok=True,
        meta=None,
    )
    
    # Convert to string representation
    result_str = str(result).lower()
    result_repr = repr(result).lower()
    
    # Check that string representations don't contain forbidden keywords
    for forbidden in FORBIDDEN_FIELD_NAMES:
        assert forbidden not in result_str, (
            f"Stage0Result string representation contains forbidden keyword '{forbidden}': {result_str}"
        )
        assert forbidden not in result_repr, (
            f"Stage0Result repr contains forbidden keyword '{forbidden}': {result_repr}"
        )



--------------------------------------------------------------------------------

